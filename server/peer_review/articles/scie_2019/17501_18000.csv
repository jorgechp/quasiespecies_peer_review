PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								A Context Knowledge Map Guided Coarse-to-Fine Action Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Human action recognition; multi-modal fusion; coarse-to-fine classification; knowledge map	FUSION	Human actions involve a wide variety and a large number of categories, which leads to a big challenge in action recognition. However, according to similarities on human body poses, scenes, interactive objects, human actions can be grouped into some semantic groups, i.e. sports, cooking, etc. Therefore, in this paper, we propose a novel approach which recognizes human actions from coarse to fine. Taking full advantage of contributions from high-level semantic contexts, a context knowledge map guided recognition method is designed to realize the coarse-to-fine procedure. In the approach, we define semantic contexts with interactive objects, scenes and body motions in action videos, and build a context knowledge map to automatically define coarse-grained groups. Then fine-grained classifiers are proposed to realize accurate action recognition. The coarse-to-fine procedure narrows action categories in target classifiers, so it is beneficial to improving recognition performance. We evaluate the proposed approach on the CCV, the HMDB-51, and the UCF101 database. Experiments verify its significant effectiveness, on average, improving more than 5 of recognition precisions than current approaches. Compared with the state-of-the-art, it also obtains outstanding performance. The proposed approach achieves higher accuracies of 93.1, 95.4 and 74.5 in the CCV, the UCF-101 and the HMDB51 database, respectively.																	1057-7149	1941-0042					2020	29						2742	2752		10.1109/TIP.2019.2952088													
J								MonoFENet: Monocular 3D Object Detection With Feature Enhancement Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D object detection; monocular images; feature enhancement; neural networks; autonomous driving	DEEP CONVOLUTIONAL NETWORKS	Monocular 3D object detection has the merit of low cost and can be served as an auxiliary module for autonomous driving system, becoming a growing concern in recent years. In this paper, we present a monocular 3D object detection method with feature enhancement networks, which we call MonoFENet. Specifically, with the estimated disparity from the input monocular image, the features of both the 2D and 3D streams can be enhanced and utilized for accurate 3D localization. For the 2D stream, the input image is used to generate 2D region proposals as well as to extract appearance features. For the 3D stream, the estimated disparity is transformed into 3D dense point cloud, which is then enhanced by the associated front view maps. With the RoI Mean Pooling layer, 3D geometric features of RoI point clouds are further enhanced by the proposed point feature enhancement (PointFE) network. The region-wise features of image and point cloud are fused for the final 2D and 3D bounding boxes regression. The experimental results on the KITTI benchmark reveal that our method can achieve state-of-the-art performance for monocular 3D object detection.																	1057-7149	1941-0042					2020	29						2753	2765		10.1109/TIP.2019.2952201													
J								Semi-Supervised Image Dehazing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image dehazing; deep learning; semi-supervised learning		We present an effective semi-supervised learning algorithm for single image dehazing. The proposed algorithm applies a deep Convolutional Neural Network (CNN) containing a supervised learning branch and an unsupervised learning branch. In the supervised branch, the deep neural network is constrained by the supervised loss functions, which are mean squared, perceptual, and adversarial losses. In the unsupervised branch, we exploit the properties of clean images via sparsity of dark channel and gradient priors to constrain the network. We train the proposed network on both the synthetic data and real-world images in an end-to-end manner. Our analysis shows that the proposed semi-supervised learning algorithm is not limited to synthetic training datasets and can be generalized well to real-world images. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art single image dehazing algorithms on both benchmark datasets and real-world images.																	1057-7149	1941-0042					2020	29						2766	2779		10.1109/TIP.2019.2952690													
J								Adaptive Sample-Level Graph Combination for Partial Multiview Clustering	IEEE TRANSACTIONS ON IMAGE PROCESSING										Partial multiview clustering; graph combination; adaptive weights		Multiview clustering explores complementary information among distinct views to enhance clustering performance under the assumption that all samples have complete information in all available views. However, this assumption does not hold in many real applications, where the information of some samples in one or more views may be missing, leading to partial multiview clustering problems. In this case, significant performance degeneration is usually observed. A collection of partial multiview clustering algorithms has been proposed to address this issue and most treat all different views equally during clustering. In fact, because different views provide features collected from different angles/feature spaces, they might play different roles in the clustering process. With the diversity of different views considered, in this study, a novel adaptive method is proposed for partial multiview clustering by automatically adjusting the contributions of different views. The samples are divided into complete and incomplete sets, while a joint learning mechanism is established to facilitate the connection between them and thereby improve clustering performance. More specifically, the method is characterized by a joint optimization model comprising two terms. The first term mines the underlying cluster structure from both complete and incomplete samples by adaptively updating their importance in all available views. The second term is designed to group all data with the aid of the cluster structure modeled in the first term. These two terms seamlessly integrate the complementary information among multiple views and enhance the performance of partial multiview clustering. Experimental results on real-world datasets illustrate the effectiveness and efficiency of our proposed method.																	1057-7149	1941-0042					2020	29						2780	2794		10.1109/TIP.2019.2952696													
J								A Multi-Domain and Multi-Modal Representation Disentangler for Cross-Domain Image Manipulation and Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Representation disentanglement; image translation; domain adaptation; deep learning		Learning interpretable data representation has been an active research topic in deep learning and computer vision. While representation disentanglement is an effective technique for addressing this task, existing works cannot easily handle the problems in which manipulating and recognizing data across multiple domains are desirable. In this paper, we present a unified network architecture of Multi-domain and Multi-modal Representation Disentangler ( $M<^>{2}RD$ ), with the goal of learning domain-invariant content representation with the associated domain-specific representation observed. By advancing adversarial learning and disentanglement techniques, the proposed model is able to perform continuous image manipulation across data domains with multiple modalities. More importantly, the resulting domain-invariant feature representation can be applied for unsupervised domain adaptation. Finally, our quantitative and qualitative results would confirm the effectiveness and robustness of the proposed model over state-of-the-art methods on the above tasks.																	1057-7149	1941-0042					2020	29						2795	2807		10.1109/TIP.2019.2952707													
J								Deep Guided Learning for Fast Multi-Exposure Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-exposure image fusion; convolutional neural networks; guided filtering; computational photography	QUALITY ASSESSMENT	We propose a fast multi-exposure image fusion (MEF) method, namely MEF-Net, for static image sequences of arbitrary spatial resolution and exposure number. We first feed a low-resolution version of the input sequence to a fully convolutional network for weight map prediction. We then jointly upsample the weight maps using a guided filter. The final image is computed by a weighted fusion. Unlike conventional MEF methods, MEF-Net is trained end-to-end by optimizing the perceptually calibrated MEF structural similarity (MEF-SSIM) index over a database of training sequences at full resolution. Across an independent set of test sequences, we find that the optimized MEF-Net achieves consistent improvement in visual quality for most sequences, and runs 10 to 1000 times faster than state-of-the-art methods. The code is made publicly available at https://github.com/makedede/MEFNet.																	1057-7149	1941-0042					2020	29						2808	2819		10.1109/TIP.2019.2952716													
J								Latent Elastic-Net Transfer Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Transfer learning; elastic-net; source domain; target domain; machine learning	DOMAIN ADAPTATION; FACE RECOGNITION; REGULARIZATION	Subspace learning based transfer learning methods commonly find a common subspace where the discrepancy of the source and target domains is reduced. The final classification is also performed in such subspace. However, the minimum discrepancy does not guarantee the best classification performance and thus the common subspace may be not the best discriminative. In this paper, we propose a latent elastic-net transfer learning (LET) method by simultaneously learning a latent subspace and a discriminative subspace. Specifically, the data from different domains can be well interlaced in the latent subspace by minimizing Maximum Mean Discrepancy (MMD). Since the latent subspace decouples inputs and outputs and, thus a more compact data representation is obtained for discriminative subspace learning. Based on the latent subspace, we further propose a low-rank constraint based matrix elastic-net regression to learn another subspace in which the intrinsic intra-class structure correlations of data from different domains is well captured. In doing so, a better discriminative alignment is guaranteed and thus LET finally learns another discriminative subspace for classification. Experiments on visual domains adaptation tasks show the superiority of the proposed LET method.																	1057-7149	1941-0042					2020	29						2820	2833		10.1109/TIP.2019.2952739													
J								Unsupervised Deep Contrast Enhancement With Power Constraint for OLED Displays	IEEE TRANSACTIONS ON IMAGE PROCESSING										Convolutional neural network; deep learning; energy efficiency; image enhancement	IMAGE QUALITY ASSESSMENT	Various power-constrained contrast enhancement (PCCE) techniques have been applied to an organic light emitting diode (OLED) display for reducing the power demands of the display while preserving the image quality. In this paper, we propose a new deep learning-based PCCE scheme that constrains the power consumption of the OLED displays while enhancing the contrast of the displayed image. In the proposed method, the power consumption is constrained by simply reducing the brightness a certain ratio, whereas the perceived visual quality is preserved as much as possible by enhancing the contrast of the image using a convolutional neural network (CNN). Furthermore, our CNN can learn the PCCE technique without a reference image by unsupervised learning. Experimental results show that the proposed method is superior to conventional ones in terms of image quality assessment metrics such as a visual saliency-induced index (VSI) and a measure of enhancement (EME).																	1057-7149	1941-0042					2020	29						2834	2844		10.1109/TIP.2019.2953352													
J								Phase Asymmetry Ultrasound Despeckling With Fractional Anisotropic Diffusion and Total Variation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image edge detection; Ultrasonic imaging; Speckle; Anisotropic magnetoresistance; Feature extraction; TV; Measurement; Ultrasound despeckling; speckle noise; fractional-order diffusion filter; fractional-order TV filter; edge detection; phase congruency; phase asymmetry; image denoising	SPECKLE REDUCTION; KERNEL REGRESSION; IMAGE; NOISE; FILTER; MODEL; ENHANCEMENT; STATISTICS; ALGORITHM	We propose an ultrasound speckle filtering method for not only preserving various edge features but also filtering tissue-dependent complex speckle noises in ultrasound images. The key idea is to detect these various edges using a phase congruence-based edge significance measure called phase asymmetry (PAS), which is invariant to the intensity amplitude of edges and takes 0 in non-edge smooth regions and 1 at the idea step edge, while also taking intermediate values at slowly varying ramp edges. By leveraging the PAS metric in designing weighting coefficients to maintain a balance between fractional-order anisotropic diffusion and total variation (TV) filters in TV cost function, we propose a new fractional TV framework to not only achieve the best despeckling performance with ramp edge preservation but also reduce the staircase effect produced by integral-order filters. Then, we exploit the PAS metric in designing a new fractional-order diffusion coefficient to properly preserve low-contrast edges in diffusion filtering. Finally, different from fixed fractional-order diffusion filters, an adaptive fractional order is introduced based on the PAS metric to enhance various weak edges in the spatially transitional areas between objects. The proposed fractional TV model is minimized using the gradient descent method to obtain the final denoised image. The experimental results and real application of ultrasound breast image segmentation show that the proposed method outperforms other state-of-the-art ultrasound despeckling filters for both speckle reduction and feature preservation in terms of visual evaluation and quantitative indices. The best scores on feature similarity indices have achieved 0.867, 0.844 and 0.834 under three different levels of noise, while the best breast ultrasound segmentation accuracy in terms of the mean and median dice similarity coefficient are 96.25 and 96.15, respectively.																	1057-7149	1941-0042					2020	29						2845	2859		10.1109/TIP.2019.2953361													
J								Context-Interactive CNN for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Reinforcement learning; Feature extraction; Videos; Cameras; Context modeling; Clutter; Person re-identification; multi-task reinforcement learning; context interaction; actor-critic agent; context-critic network	NETWORKS	Despite growing progresses in recent years, cross-scenario person re-identification remains challenging, mainly due to the pedestrians commonly surrounded by highly-complex environment contexts. In reality, the human perception mechanism could adaptively find proper contextualized spatial-temporal clues towards pedestrian recognition. However, conventional methods fall short in adaptively leveraging the long-term spatial-temporal information due to ever-increasing computational cost. Moreover, CNN-based deep learning methods are hard to conduct optimization due to the non-differentiable property of the built-in context search operation. To ameliorate, this paper proposes a novel Context-Interactive CNN (CI-CNN) to dynamically find both spatial and temporal contexts by embedding multi-task Reinforcement Learning (MTRL). The CI-CNN streamlines the multi-task reinforcement learning by using an actor-critic agent to capture the temporal-spatial context simultaneously, which comprises a context-policy network and a context-critic network. The former network learns policies to determine the optimal spatial context region and temporal sequence range. Based on the inferred temporal-spatial cues, the latter one focuses on the identification task and provides feedback for the policy network. Thus, CI-CNN can simultaneously zoom in/out the perception field in spatial and temporal domain for the context interaction with the environment. By fostering the collaborative interaction between the person and context, our method could achieve outstanding performance on various public benchmarks, which confirms the rationality of our hypothesis, and verifies the effectiveness of our CI-CNN framework.																	1057-7149	1941-0042					2020	29						2860	2874		10.1109/TIP.2019.2953587													
J								Discriminative Residual Analysis for Image Set Classification With Posture and Age Variations	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image recognition; Feature extraction; Noise measurement; Training; Task analysis; Databases; Image set recognition; residual analysis; feature extraction; discriminant analysis; regularization	RECOGNITION	Image set recognition has been widely applied in many practical problems like real-time video retrieval and image caption tasks. Due to its superior performance, it has grown into a significant topic in recent years. However, images with complicated variations, e.g., postures and human ages, are difficult to address, as these variations are continuous and gradual with respect to image appearance. Consequently, the crucial point of image set recognition is to mine the intrinsic connection or structural information from the image batches with variations. In this work, a Discriminant Residual Analysis (DRA) method is proposed to improve the classification performance by discovering discriminant features in related and unrelated groups. Specifically, DRA attempts to obtain a powerful projection which casts the residual representations into a discriminant subspace. Such a projection subspace is expected to magnify the useful information of the input space as much as possible, then the relation between the training set and the test set described by the given metric or distance will be more precise in the discriminant subspace. We also propose a nonfeasance strategy by defining another approach to construct the unrelated groups, which help to reduce furthermore the cost of sampling errors. Two regularization approaches are used to deal with the probable small sample size problem. Extensive experiments are conducted on benchmark databases, and the results show superiority and efficiency of the new methods.																	1057-7149	1941-0042					2020	29						2875	2888		10.1109/TIP.2019.2954176													
J								The Structure Transfer Machine Theory and Applications	IEEE TRANSACTIONS ON IMAGE PROCESSING										Manifolds; Deep learning; Training; Probabilistic logic; Object tracking; Task analysis; Transfer learning; convolutional neural networks; manifold loss; learning theory	DIMENSIONALITY REDUCTION; TRACKING	Representation learning is a fundamental but challenging problem, especially when the distribution of data is unknown. In this paper, we propose a new representation learning method, named Structure Transfer Machine (STM), which enables feature learning process to converge at the representation expectation in a probabilistic way. We theoretically show that such an expected value of the representation (mean) is achievable if the manifold structure can be transferred from the data space to the feature space. The resulting structure regularization term, named manifold loss, is incorporated into the loss function of the typical deep learning pipeline. The STM architecture is constructed to enforce the learned deep representation to satisfy the intrinsic manifold structure from the data, which results in robust features that suit various application scenarios, such as digit recognition, image classification and object tracking. Compared with state-of-the-art CNN architectures, we achieve better results on several commonly used public benchmarks.																	1057-7149	1941-0042					2020	29						2889	2902		10.1109/TIP.2019.2954178													
J								Super Diffusion for Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; Feature extraction; Diffusion processes; Laplace equations; Saliency detection; Robustness; Task analysis; Saliency detection; diffusion; spectral clustering	REGION DETECTION; VISUAL-ATTENTION; INTEGRATION; FEATURES	One major branch of saliency object detection methods are diffusion-based which construct a graph model on a given image and diffuse seed saliency values to the whole graph by a diffusion matrix. While their performance is sensitive to specific feature spaces and scales used for the diffusion matrix definition, little work has been published to systematically promote the robustness and accuracy of salient object detection under the generic mechanism of diffusion. In this work, we firstly present a novel view of the working mechanism of the diffusion process based on mathematical analysis, which reveals that the diffusion process is actually computing the similarity of nodes with respect to the seeds based on diffusion maps. Following this analysis, we propose super diffusion, a novel inclusive learning-based framework for salient object detection, which makes the optimum and robust performance by integrating a large pool of feature spaces, scales and even features originally computed for non-diffusion-based salient object detection. A closed-form solution of the optimal parameters for the integration is determined through supervised learning. At the local level, we propose to promote each individual diffusion before the integration. Our mathematical analysis reveals the close relationship between saliency diffusion and spectral clustering. Based on this, we propose to re-synthesize each individual diffusion matrix from the most discriminative eigenvectors and the constant eigenvector (for saliency normalization). The proposed framework is implemented and experimented on prevalently used benchmark datasets, consistently leading to state-of-the-art performance.																	1057-7149	1941-0042					2020	29						2903	2917		10.1109/TIP.2019.2954209													
J								Arbitrarily Shaped Scene Text Detection With a Mask Tightness Text Detector	IEEE TRANSACTIONS ON IMAGE PROCESSING										Proposals; Shape; Text recognition; Training; Detectors; Benchmark testing; Feature extraction; Scene text; arbitrarily shaped text; mask; universal; convolutional neural network; CTW1500 dataset; label; text detection; segmentation	ROBUST; LOCALIZATION; VIDEO; RECOGNITION; COMPETITION	Scene text in the environment is complicated. It can exist in arbitrary text fonts, sizes or shapes. Although scene text detection has witnessed considerable progress in recent years, the detection of text with complex shapes, especially curved text, remains challenging. Datasets with adequate samples to overcome the problem presented by curved text (or other irregularly shaped text) have been introduced only recently; however, the performance of the reported methods on these datasets is unsatisfactory. Therefore, detecting arbitrarily shaped text remains a challenging. This motivated us to propose the Mask Tightness Text Detector (Mask TTD) to improve text detection performance. Mask TTD uses a tightness prior and text frontier learning to enhance pixel-wise mask prediction. In addition, it achieves mutual promotion by integrating a branch for the polygonal boundary of each text region, which significantly improves the detection performance of arbitrarily shaped text. Experiments demonstrate that Mask TTD can achieve state-of-the-art performance on existing curved text datasets (CTW1500, Total-text, and CUTE80) and three common benchmark datasets (RCTW-17, MSRA-TD500, and ICDAR 2015). It is worth mentioning that on CTW1500, our method can outperform previous methods, especially at higher intersection over union (IoU) thresholds (16 higher than the next-best method with an IoU threshold of 0.8), which demonstrates its potential for tight text detection. Moreover, on the largest Chinese-based dataset RCTW-17, Mask TTD outperforms other methods by a large margin in terms of both the Average Precision and F-measure, showing its powerful generalization ability.																	1057-7149	1941-0042					2020	29						2918	2930		10.1109/TIP.2019.2954218													
J								Extended Coding Unit Partitioning for Future Video Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Video coding; Encoding; Copper; Standards; Next generation networking; Decoding; CU partitioning; quad-tree partitioning; asymmetric ternary-tree partitioning; video coding	EFFICIENCY	The flexible partitioning structure such as quad-tree plus binary-tree (QTBT) plays important roles in the next generation video coding standard. This paper investigates new coding unit (CU) partitioning methods to further improve the compression efficiency. In particular, we propose the extended quad-tree partitioning and asymmetric ternary-tree partitioning to further develop the principle behind the existing partitioning structure. First, a central extended quad-tree (CENTRAL-EQT) partitioning is introduced, which extends the traditional QT partitioning with central pattern and generates four elaborately designed sub-CUs with different sizes. Second, we propose a parallel extended quad-tree (PARALLEL-EQT) partitioning that allows the CU to split along a single direction, leading to four identical size sub-blocks. Third, we present an asymmetric ternary-tree (ATT) method, which splits the CU asymmetrically into three sub-blocks. The proposed new partitioning methods allow the interleaving with binary-tree partitioning for enhanced adaptability, and they can be jointly enabled to capture different characteristics of the local content. Simulation results on the JEM7-QTBT-Only platform show that averagely 4.92, 4.81 and 5.08 BD-Rate savings are achieved for the luma component by the proposed methods under random access (RA), low-delay P (LDP) and low-delay B (LDB) configurations, respectively, with around 725 encoding time increase and negligible decoding time increase. Furthermore, experimental results also reveal that the proposed partitioning schemes are effective when cooperating with the upcoming Versatile Video Coding standard.																	1057-7149	1941-0042					2020	29						2931	2946		10.1109/TIP.2019.2955238													
J								Recent Advances in 3D Object Detection in the Era of Deep Neural Networks: A Survey	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Object detection; Cameras; Sensors; Two dimensional displays; Laser radar; Task analysis; 3D object detection; deep neural networks; RGB-D data; LiDAR data; point cloud; deep learning		With the rapid development of deep learning technology and other powerful tools, 3D object detection has made great progress and become one of the fastest growing field in computer vision. Many automated applications such as robotic navigation, autonomous driving, and virtual or augmented reality system require estimation of accurate 3D object location and detection. Under this requirement, many methods have been proposed to improve the performance of 3D object localization and detection. Despite recent efforts, 3D object detection is still a very challenging task due to occlusion, viewpoint variations, scale changes, and limited information in 3D scenes. In this paper, we present a comprehensive review of recent state-of-the-art approaches in 3D object detection technology. We start with some basic concepts, then describe some of the available datasets that are designed to facilitate the performance evaluation of 3D object detection algorithms. Next, we will review the state-of-the-art technologies in this area, highlighting their contributions, importance, and limitations as a guide for future research. Finally, we provide a quantitative comparison of the results of the state-of-the-art methods on the popular public datasets.																	1057-7149	1941-0042					2020	29						2947	2962		10.1109/TIP.2019.2955239													
J								Optical-and-Radar Image Fusion for Dynamic Estimation of Spin Satellites	IEEE TRANSACTIONS ON IMAGE PROCESSING										Satellites; Optical imaging; Radar imaging; Optical sensors; Spaceborne radar; Estimation; Dynamic estimation; spin satellites; optical-and-radar fusion; image interpretation	ATTITUDE ESTIMATION; TARGETS; SAR; ENVISAT; MODEL	As more and more satellites are launched into the space, dynamic estimation of spin satellites has become a critical component of the space situation awareness application. Some explored studies using exterior measurements from different sensors such as optical device and inverse synthetic aperture radar (ISAR) to estimate dynamic parameters of spin satellites. As a single sensor normally provides two-dimensional observation, three-dimensional estimations resulting from these algorithms are strictly related to the prior knowledge of targets characteristics. As a result, it is difficult to expand these methods to other satellites. In order to support the dynamic estimation of most spin satellites, this paper presents a novel dynamic estimation approach which employs synchronized optical-and-radar images. The optical-and-radar fusion strategy has demonstrated its superiority in image analysis field, and breaks down the dynamic estimation of spin satellites into two sub-problems: target attitude estimation and spin parameters estimation. In this work, the proposed algorithm deduces two explicit expressions of target dynamic parameters under the imaging projection model of the joint optical-and-radar observation. Through the particle swarm optimization (PSO), target dynamic parameters are determined in two stages. This paper presents some experiments illustrating the feasibility of the proposed method and subsequent conclusions, which reflect advantages of the joint optical-and-radar observation mode in image interpretation.																	1057-7149	1941-0042					2020	29						2963	2976		10.1109/TIP.2019.2955248													
J								SRHandNet: Real-Time 2D Hand Pose Estimation With Simultaneous Region Localization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Pose estimation; Two dimensional displays; Color; Heating systems; Training; Three-dimensional displays; Machine learning; Real-time hand pose estimation; bounding box representation; inference feedback		This paper introduces a novel method for real-time 2D hand pose estimation from monocular color images, which is named as SRHandNet. Existing methods can not time efficiently obtain appropriate results for small hand. Our key idea is to simultaneously regress the hand region of interests (RoIs) and hand keypoints for a given color image, and iteratively take the hand RoIs as feedback information for boosting the performance of hand keypoints estimation with a single encoder-decoder network architecture. Different from previous region proposal network (RPN), a new lightweight bounding box representation, which is called region map, is proposed. The proposed bounding box representation map together with hand keypoints heatmaps are combined into the unified multi-channel feature maps, which can be easily acquired with only one forward network inference and thus improve the runtime efficiency of the network. Our proposed SRHandNet can run at 40fps for hand bounding box detection and up to 30fps accurate hand keypoints estimation under the desktop environment without implementation optimization. Experiments demonstrate the effectiveness of the proposed method. State-of-the-art results are also achieved out competing all recent methods.																	1057-7149	1941-0042					2020	29						2977	2986		10.1109/TIP.2019.2955280													
J								Realistic Film Noise Generation Based on Experimental Noise Spectra	IEEE TRANSACTIONS ON IMAGE PROCESSING										Correlation; Radiography; Computational modeling; Silver; Optical films; Optical imaging; Two dimensional displays; Image quality; noise simulation; nondestructive testing; radiography	GRANULARITY; MODEL	Generating 2D noise with local, space-varying spectral characteristics is vital where random noise fields with spatially heterogeneous statistical properties are observed and need to be simulated. A realistic, non-stationary noise generator relying on experimental data is presented. That generator is desired in areas such as photography and radiography. For example, before performing actual X-ray imaging in practice, output images are simulated to assess and improve setups. For that purpose, realistic film noise modelling is crucial because noise downgrades the detectability of visual signals. The presented film noise synthesiser improves the realism and value of radiographic simulations significantly, allowing more realistic assessments of radiographic test setups. The method respects space-varying spectral characteristics and probability distributions, locally simulating noise with realistic granularity and contrast. The benefits of this approach are to respect the correlation between noise and image as well as internal correlation, the fast generation of any number of unique noise samples, the exploitation of real experimental data, and its statistical non-stationarity. The combination of these benefits is not available in existing work. Validation of the new technique was undertaken in the field of industrial radiography. While applied to that field here, the technique is general and can also be utilised in any other field where the generation of 2D noise with local, space-varying statistical properties is necessary.																	1057-7149	1941-0042					2020	29						2987	2998		10.1109/TIP.2019.2955284													
J								Selective Spatial Regularization by Reinforcement Learned Decision Making for Object Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Target tracking; Visualization; Correlation; Object tracking; Clutter; Complexity theory; Visual object tracking; correlation filter; selective spatial regularization; MDP; reinforcement learning	VISUAL TRACKING; CORRELATION FILTERS	Spatial regularization (SR) is known as an effective tool to alleviate the boundary effect of correlation filter (CF), a successful visual object tracking scheme, from which a number of state-of-the-art visual object trackers can be stemmed. Nevertheless, SR highly increases the optimization complexity of CF and its target-driven nature makes spatially-regularized CF trackers may easily lose the occluded targets or the targets surrounded by other similar objects. In this paper, we propose selective spatial regularization (SSR) for CF-tracking scheme. It can achieve not only higher accuracy and robustness, but also higher speed compared with spatially-regularized CF trackers. Specifically, rather than simply relying on foreground information, we extend the objective function of CF tracking scheme to learn the target-context-regularized filters using target-context-driven weight maps. We then formulate the online selection of these weight maps as a decision making problem by a Markov Decision Process (MDP), where the learning of weight map selection is equivalent to policy learning of the MDP that is solved by a reinforcement learning strategy. Moreover, by adding a special state, representing not-updating filters, in the MDP, we can learn when to skip unnecessary or erroneous filter updating, thus accelerating the online tracking. Finally, the proposed SSR is used to equip three popular spatially-regularized CF trackers to significantly boost their tracking accuracy, while achieving much faster online tracking speed. Besides, extensive experiments on five benchmarks validate the effectiveness of SSR.																	1057-7149	1941-0042					2020	29						2999	3013		10.1109/TIP.2019.2955292													
J								A Metric for Video Blending Quality Assessment	IEEE TRANSACTIONS ON IMAGE PROCESSING										Lighting; Measurement; Hemorrhaging; Quality assessment; Video recording; Quality of experience; Coherence; Video quality assessment; video blending		We propose an objective approach to assess the quality of video blending. Blending is a fundamental operation in video editing, which can smooth the intensity changes of relevant regions. However blending also generates artefacts such as bleeding and ghosting. To assess the quality of the blended videos, our approach considers the illuminance consistency as a positive aspect while regard the artefacts as a negative aspect. Temporal coherence between frames is also considered. We evaluate our metric on a video blending dataset where the results of subjective evaluation are available. Experimental results validate the effectiveness of our proposed metric, and shows that this metric gives superior performance over existing video quality metrics.																	1057-7149	1941-0042					2020	29						3014	3022		10.1109/TIP.2019.2955294													
J								New Design for Compact Color Screen Sets for High-End Digital Color Press	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Presses; Microcell networks; Printing; Geometry; Complexity theory; Electronic mail; Halftoning; screen; print quality; moire; graininess; color uniformity; color plane mis-registration; regular screens; irregular screens		Digital halftoning is an essential part of the process for printing color, continuous-tone content. Traditionally, the highest quality has been achieved with analog, offset lithographic presses, using color screen sets that yield periodic, clustered-dot halftone patterns. Increasingly, these systems are being supplanted by digital presses that are based on either electrophotographic or inkjet marking processes. Due to the inherent instability of the electrophotographic marking process, periodic, clustered-dot halftone patterns are also widely used with such presses. However, digital presses have much lower resolution than their analog counterparts. Simply mimicking the traditional screen designs used with commercial, offset presses will result in halftone patterns that are more susceptible to moire due to the interaction between the periodic patterns used to render the different color channels. This causes instability in the printed colors. The moire can be reduced by increasing the frequency of the halftone patterns. But this may make the print appear grainier than its analog counterpart. In this paper, we introduce a principled design procedure that allows one to design color screen sets that generate periodic, clustered-dot halftone patterns that improve color stability without increasing graininess. We present experimental results to support the benefits of our new color screen set design framework.																	1057-7149	1941-0042					2020	29						3023	3038		10.1109/TIP.2019.2955295													
J								Bi-Directional Dermoscopic Feature Learning and Multi-Scale Consistent Decision Fusion for Skin Lesion Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Lesions; Skin; Image segmentation; Melanoma; Bidirectional control; Reliability; Feature extraction; Skin lesion segmentation; dermoscopic images; bi-directional dermoscopic feature learning; multi-scale consistent decision fusion	IMAGE SEGMENTATION; NETWORKS; CLASSIFICATION	Accurate segmentation of skin lesion from dermoscopic images is a crucial part of computer-aided diagnosis of melanoma. It is challenging due to the fact that dermoscopic images from different patients have non-negligible lesion variation, which causes difficulties in anatomical structure learning and consistent skin lesion delineation. In this paper, we propose a novel bi-directional dermoscopic feature learning (biDFL) framework to model the complex correlation between skin lesions and their informative context. By controlling feature information passing through two complementary directions, a substantially rich and discriminative feature representation is achieved. Specifically, we place biDFL module on the top of a CNN network to enhance high-level parsing performance. Furthermore, we propose a multi-scale consistent decision fusion (mCDF) that is capable of selectively focusing on the informative decisions generated from multiple classification layers. By analysis of the consistency of the decision at each position, mCDF automatically adjusts the reliability of decisions and thus allows a more insightful skin lesion delineation. The comprehensive experimental results show the effectiveness of the proposed method on skin lesion segmentation, achieving state-of-the-art performance consistently on two publicly available dermoscopic image databases.																	1057-7149	1941-0042					2020	29						3039	3051		10.1109/TIP.2019.2955297													
J								Tangent Fisher Vector on Matrix Manifolds for Action Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Manifolds; Video sequences; Observability; Videos; Covariance matrices; Kernel; Computational modeling; Action recognition; Fisher vector; Grassmann manifold; Hankel matrix; matrix manifold	BINET-CAUCHY KERNELS; DYNAMICAL-SYSTEMS; VIEW; MODELS; VIDEO; IDENTIFICATION; CLASSIFICATION; DESCRIPTORS	In this paper, we address the problem of representing and recognizing human actions from videos on matrix manifolds. For this purpose, we propose a new vector representation method, named tangent Fisher vector, to describe video sequences in the Fisher kernel framework. We first extract dense curved spatio-temporal cuboids from each video sequence. Compared with the traditional 'straight cuboids', the dense curved spatio-temporal cuboids contain much more local motion information. Each cuboid is then described using a linear dynamical system (LDS) to simultaneously capture the local appearance and dynamics. Furthermore, a simple yet efficient algorithm is proposed to learn the LDS parameters and approximate the observability matrix at the same time. Each video sequence is thus represented by a set of LDSs. Considering that each LDS can be viewed as a point in a Grassmann manifold, we propose to learn an intrinsic GMM on the manifold to cluster the LDS points. Finally a tangent Fisher vector is computed by first accumulating all the tangent vectors in each Gaussian component, and then concatenating the normalized results across all the Gaussian components. A kernel is defined to measure the similarity between tangent Fisher vectors for classification and recognition of a video sequence. This approach is evaluated on the state-of-the-art human action benchmark datasets. The recognition performance is competitive when compared with current state-of-the-art results.																	1057-7149	1941-0042					2020	29						3052	3064		10.1109/TIP.2019.2955561													
J								Real-Time Burst Photo Selection Using a Light-Head Adversarial Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Real-time systems; Mobile handsets; Head; Computational modeling; Cameras; Generative adversarial networks; Training; Computational photography; generative adversarial network		We present an automatic moment capture system that runs in real-time on mobile cameras. The system is designed to run in the viewfinder mode and capture a burst sequence of frames before and after the shutter is pressed. For each frame, the system predicts in real-time a goodness score, based on which the best moment in the burst can be selected immediately after the shutter is released. We develop a highly efficient deep neural network ranking model, which implicitly learns a latent relative attribute space to capture subtle visual differences within a sequence of burst images. The overall goodness is computed as a linear aggregation of the goodnesses of all the latent attributes. To obtain a compact model which can run on mobile devices in real-time, we have explored and evaluated a wide range of network design choices, taking into account the constraints of model size, computational cost, and accuracy. Extensive studies show that the best frame predicted by our model hit users' top-1 (out of 11 on average) choice for 64.1 cases and top-3 choices for 86.2 cases. Moreover, the model (only 0.47M Bytes) can run in real time on mobile devices, e.g. 13ms on iPhone 7.																	1057-7149	1941-0042					2020	29						3065	3077		10.1109/TIP.2019.2955563													
J								Self-Enhanced Convolutional Network for Facial Video Hallucination	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spatial resolution; Face; Image reconstruction; Machine learning; Image restoration; Computer science; Facial video hallucination; recurrent frame fusion; sequential feature encoding; deep learning		As a domain-specific super-resolution problem, facial image hallucination has enjoyed a series of breakthroughs thanks to the advances of deep convolutional neural networks. However, the direct migration of existing methods to video is still difficult to achieve good performance due to its lack of alignment and consistency modelling in temporal domain. Taking advantage of high inter-frame dependency in videos, we propose a self-enhanced convolutional network for facial video hallucination. It is implemented by making full usage of preceding super-resolved frames and a temporal window of adjacent low-resolution frames. Specifically, the algorithm first obtains the initial high-resolution inference of each frame by taking into consideration a sequence of consecutive low-resolution inputs through temporal consistency modelling. It further recurrently exploits the reconstructed results and intermediate features of a sequence of preceding frames to improve the initial super-resolution of the current frame by modelling the coherence of structural facial features across frames. Quantitative and qualitative evaluations demonstrate the superiority of the proposed algorithm against state-of-the-art methods. Moreover, our algorithm also achieves excellent performance in the task of general video super-resolution in a single-shot setting.																	1057-7149	1941-0042					2020	29						3078	3090		10.1109/TIP.2019.2955640													
J								HAR-Net: Joint Learning of Hybrid Attention for Single-Stage Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; deep neural networks; hybrid attention mechanism; single-stage detection; joint learning		Object detection has been a challenging task in computer vision. Although significant progress has been made in object detection with deep neural networks, the attention mechanism has yet to be fully developed. In this paper, we propose a hybrid attention mechanism for single-stage object detection. First, we present the modules of spatial attention, channel attention and aligned attention for single-stage object detection. In particular, dilated convolution layers with symmetrically fixed rates are stacked to learn spatial attention. A channel attention mechanism with the cross-level group normalization and squeeze-and-excitation operation is proposed. Aligned attention is constructed with organized deformable filters. Second, the three types of attention are unified to construct the hybrid attention mechanism. We then plug the hybrid attention into Retina-Net and propose the efficient single-stage HAR-Net for object detection. The attention modules and the proposed HAR-Net are evaluated on the COCO detection dataset. The experiments demonstrate that hybrid attention can significantly improve the detection accuracy and that the HAR-Net can achieve a state-of-the-art 45.8 mAP, thus outperforming existing single-stage object detectors.																	1057-7149	1941-0042					2020	29						3092	3103		10.1109/TIP.2019.2957850													
J								IDGCP: Image Dehazing Based on Gamma Correction Prior	IEEE TRANSACTIONS ON IMAGE PROCESSING										Atmospheric scattering theory; gamma correction prior (GCP); global-wise strategy; haze removal; processing time; vision indicator	CONTRAST ENHANCEMENT; SINGLE; VISIBILITY; RESTORATION; WEATHER; MODEL	This paper introduces a novel and effective image prior, i.e., gamma correction prior (GCP), which leads to an efficient image dehazing method, i.e., IDGCP. A step-by-step procedure of the proposed IDGCP is as follows. First, an input hazy image is preprocessed by the proposed GCP, resulting in a homogeneous virtual transformation of the hazy image. Then, from the original input hazy image and its virtual transformation, the depth ratio is extracted based on atmospheric scattering theory. Finally, a "global-wise" strategy and a vision indicator are employed to recover the scene albedo, thus restoring the hazy image. Unlike other image dehazing methods, IDGCP is based on the "global-wise" strategy, and it only needs to determine one unknown constant without any refining process to attain a high-quality restoration, thereby leading to significantly reduced processing time and computation cost. Each step of IDGCP is tested experimentally to validate its robustness. Moreover, a series of experiments are conducted on a number of challenging images with IDGCP and other state-of-the-art technologies, demonstrating the superiority of IDGCP over the others in terms of restoration quality and implementation efficiency.																	1057-7149	1941-0042					2020	29						3104	3118		10.1109/TIP.2019.2957852													
J								Learning Non-Local Spatial Correlations To Restore Sparse 3D Single-Photon Data	IEEE TRANSACTIONS ON IMAGE PROCESSING										Single photon; 3D Lidar imaging; Poisson statistics; image restoration; ADMM; Laplacian regularization; graph; non-uniform sampling; multi-scale analysis	NOISE; WAVELETS	This paper presents a new algorithm for the learning of spatial correlation and non-local restoration of single-photon 3D Lidar images acquired in the photon starved regime (fewer or less than one photon per pixel) or with a reduced number of scanned spatial points (pixels). The algorithm alternates between three steps: (i) extract multi-scale information, (ii) build a robust graph of non-local spatial correlations between pixels, and (iii) the restoration of depth and reflectivity images. A non-uniform sampling approach, which assigns larger patches to homogeneous regions and smaller ones to heterogeneous regions, is adopted to reduce the computational cost associated with the graph. The restoration of the 3D images is achieved by minimizing a cost function accounting for the multi-scale information and the non-local spatial correlation between patches. This minimization problem is efficiently solved using the alternating direction method of multipliers (ADMM) that presents fast convergence properties. Various results based on simulated and real Lidar data show the benefits of the proposed algorithm that improves the quality of the estimated depth and reflectivity images, especially in the photon-starved regime or when containing a reduced number of spatial points.																	1057-7149	1941-0042					2020	29						3119	3131		10.1109/TIP.2019.2957918													
J								Low-Rank Matrix Recovery via Modified Schatten-p Norm Minimization With Convergence Guarantees	IEEE TRANSACTIONS ON IMAGE PROCESSING										Low-rank matrix recovery; modified Schatten-pnorm; iterative singular value thresholding algorithm; Kurdyka-Lojasiewicz property; convergence guarantees	THRESHOLDING ALGORITHM; VARIABLE SELECTION; SPARSE; COMPLETION; RELAXATION; NONCONVEX	In recent years, low-rank matrix recovery problems have attracted much attention in computer vision and machine learning. The corresponding rank minimization problems are both combinational and NP-hard in general, which are mainly solved by both nuclear norm and Schatten-p ( $0 < {p} < 1$ ) norm based optimization algorithms. However, inspired by weighted nuclear norm and Schatten-p norm as the relaxations of rank function, the main merits of this work firstly provide a modified Schatten-p norm in the affine matrix rank minimization problem, denoted as the modified Schatten-p norm minimization (MSpNM). Secondly, its surrogate function is constructed and the equivalence relationship with the MSpNM is further achieved. Thirdly, the iterative singular value thresholding algorithm (ISVTA) is devised to optimize it, and its accelerated version, i.e., AISVTA, is also obtained to reduce the number of iterations through the well-known Nesterov's acceleration strategy. Most importantly, the convergence guarantees and their relationship with objective function, stationary point and variable sequence generated by the proposed algorithms are established under some specific assumptions, e.g., Kurdyka-ojasiewicz (K) property. Finally, numerical experiments demonstrate the effectiveness of the proposed algorithms in the matrix completion problem for image inpainting and recommender systems. It should be noted that the accelerated algorithm has a much faster convergence speed and a very close recovery precision when comparing with the proposed non-accelerated one.																	1057-7149	1941-0042					2020	29						3132	3142		10.1109/TIP.2019.2957925													
J								Taking a Look at Small-Scale Pedestrians and Occluded Pedestrians	IEEE TRANSACTIONS ON IMAGE PROCESSING										Small-scale pedestrians; occluded pedestrians; location bootstrap; semantic transition		Small-scale pedestrian detection and occluded pedestrian detection are two challenging tasks. However, most state-of-the-art methods merely handle one single task each time, thus giving rise to relatively poor performance when the two tasks, in practice, are required simultaneously. In this paper, it is found that small-scale pedestrian detection and occluded pedestrian detection actually have a common problem, i.e., an inaccurate location problem. Therefore, solving this problem enables to improve the performance of both tasks. To this end, we pay more attention to the predicted bounding box with worse location precision and extract more contextual information around objects, where two modules (i.e., location bootstrap and semantic transition) are proposed. The location bootstrap is used to reweight regression loss, where the loss of the predicted bounding box far from the corresponding ground-truth is upweighted and the loss of the predicted bounding box near the corresponding ground-truth is downweighted. Additionally, the semantic transition adds more contextual information and relieves semantic inconsistency of the skip-layer fusion. Since the location bootstrap is not used at the test stage and the semantic transition is lightweight, the proposed method does not add many extra computational costs during inference. Experiments on the challenging CityPersons and Caltech datasets show that the proposed method outperforms the state-of-the-art methods on the small-scale pedestrians and occluded pedestrians (e.g., 5.20 and 4.73 improvements on the Caltech).																	1057-7149	1941-0042					2020	29						3143	3152		10.1109/TIP.2019.2957927													
J								Multi-Scale Deep Residual Learning-Based Single Image Haze Removal via Image Decomposition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Haze removal; single image dehazing; deep learning; deep residual learning; U-Net; convolutional neural networks; image decomposition; image restoration	RAIN STREAKS REMOVAL; VISIBILITY; WEATHER; VISION	Images/videos captured from outdoor visual devices are usually degraded by turbid media, such as haze, smoke, fog, rain, and snow. Haze is the most common one in outdoor scenes due to the atmosphere conditions. In this paper, a novel deep learning-based architecture (denoted by MSRL-DehazeNet) for single image haze removal relying on multi-scale residual learning (MSRL) and image decomposition is proposed. Instead of learning an end-to-end mapping between each pair of hazy image and its corresponding haze-free one adopted by most existing learning-based approaches, we reformulate the problem as restoration of the image base component. Based on the decomposition of a hazy image into the base and the detail components, haze removal (or dehazing) can be achieved by both of our multi-scale deep residual learning and our simplified U-Net learning only for mapping between hazy and haze-free base components, while the detail component is further enhanced via the other learned convolutional neural network (CNN). Moreover, benefited by the basic building block of our deep residual CNN architecture and our simplified U-Net structure, the feature maps (produced by extracting structural and statistical features), and each previous layer can be fully preserved and fed into the next layer. Therefore, possible color distortion in the recovered image would be avoided. As a result, the final haze-removed (or dehazed) image is obtained by integrating the haze-removed base and the enhanced detail image components. Experimental results have demonstrated good effectiveness of the proposed framework, compared with state-of-the-art approaches.																	1057-7149	1941-0042					2020	29						3153	3167		10.1109/TIP.2019.2957929													
J								Deep Image-to-Video Adaptation and Fusion Networks for Action Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Action recognition; adaptation; deep learning; fusion		Existing deep learning methods for action recognition in videos require a large number of labeled videos for training, which is labor-intensive and time-consuming. For the same action, the knowledge learned from different media types, e.g., videos and images, may be related and complementary. However, due to the domain shifts and heterogeneous feature representations between videos and images, the performance of classifiers trained on images may be dramatically degraded when directly deployed to videos. In this paper, we propose a novel method, named Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN), to enhance action recognition in videos by transferring knowledge from images using video keyframes as a bridge. The DIVAFN is a unified deep learning model, which integrates domain-invariant representations learning and cross-modal feature fusion into a unified optimization framework. Specifically, we design an efficient cross-modal similarities metric to reduce the modality shift among images, keyframes and videos. Then, we adopt an autoencoder architecture, whose hidden layer is constrained to be the semantic representations of the action class names. In this way, when the autoencoder is adopted to project the learned features from different domains to the same space, more compact, informative and discriminative representations can be obtained. Finally, the concatenation of the learned semantic feature representations from these three autoencoders are used to train the classifier for action recognition in videos. Comprehensive experiments on four real-world datasets show that our method outperforms some state-of-the-art domain adaptation and action recognition methods.																	1057-7149	1941-0042					2020	29						3168	3182		10.1109/TIP.2019.2957930													
J								Deep Unsupervised Learning of 3D Point Clouds via Graph Topology Inference and Filtering	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D point cloud; deep autoencoder; graph filtering; graph topology inference	SIGNAL; NETWORKS	We propose a deep autoencoder with graph topology inference and filtering to achieve compact representations of unorganized 3D point clouds in an unsupervised manner. Many previous works discretize 3D points to voxels and then use lattice-based methods to process and learn 3D spatial information; however, this leads to inevitable discretization errors. In this work, we try to handle raw 3D points without such compromise. The proposed networks follow the autoencoder framework with a focus on designing the decoder. The encoder of the proposed networks adopts similar architectures as in PointNet, which is a well-acknowledged method for supervised learning of 3D point clouds. The decoder of the proposed networks involves three novel modules: the folding module, the graph-topology-inference module, and the graph-filtering module. The folding module folds a canonical 2D lattice to the underlying surface of a 3D point cloud, achieving coarse reconstruction; the graph-topology-inference module learns a graph topology to represent pairwise relationships between 3D points, pushing the latent code to preserve both coordinates and pairwise relationships of points in 3D point clouds; and the graph-filtering module couples the above two modules, refining the coarse reconstruction through a learnt graph topology to obtain the final reconstruction. The proposed decoder leverages a learnable graph topology to push the codeword to preserve representative features and further improve the unsupervised-learning performance. We further provide theoretical analyses of the proposed architecture. We provide an upper bound for the reconstruction loss and further show the superiority of graph smoothness over spatial smoothness as a prior to model 3D point clouds. In the experiments, we validate the proposed networks in three tasks, including 3D point cloud reconstruction, visualization, and transfer classification. The experimental results show that (1) the proposed networks outperform the state-of-the-art methods in various tasks, including reconstruction and transfer classification; (2) a graph topology can be inferred as auxiliary information without specific supervision on graph topology inference; (3) graph filtering refines the reconstruction, leading to better performances; and (4) designing a powerful decoder could improve the unsupervised-learning performance, just like a powerful encoder.																	1057-7149	1941-0042					2020	29						3183	3198		10.1109/TIP.2019.2957935													
J								Superpixel Embedding Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Superpixel; embedding; convolutional neural networks		Superpixel segmentation is a fundamental computer vision technique that finds application in a multitude of high level computer vision tasks. Most state-of-the-art superpixel segmentation methods are unsupervised in nature and thus cannot fully utilize frequently occurring texture patterns or incorporate multi-scale context. In this paper, we show that superpixel segmentation can be improved by leveraging the superior modeling power of deep convolutional autoencoders in a fully unsupervised manner. We pose the superpixel segmentation problem as one of manifold learning where pixels that belong to similar texture patterns are assigned near identical embedding vectors. The proposed deep network is able to learn image-wide and dataset-wide feature patterns and the relationships between them. This knowledge is used to segment and group pixels in a way that is consistent with a more global definition of pattern coherence. Experiments demonstrate that the superpixels obtained from the embeddings learned by the proposed method outperform the state-of-the-art superpixel segmentation methods for boundary precision and recall values. Additionally, we find that semantic edges obtained from the superpixel embeddings to be significantly better than the contemporary unsupervised approaches.																	1057-7149	1941-0042					2020	29						3199	3212		10.1109/TIP.2019.2957937													
J								Fast Sparse Aperture ISAR Autofocusing and Imaging via ADMM Based Sparse Bayesian Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Inverse synthetic aperture radar (ISAR); sparse aperture; autofocusing; sparse Bayesian learning (SBL); alternating direction Method of multipliers (ADMM)	TARGETS; MOTION; MINIMIZATION; ALGORITHM	Sparse aperture ISAR autofocusing and imaging is generally achieved by methods of compressive sensing (CS), or, sparse signal recovery, because non-uniform sampling of sparse aperture disables fast Fourier transform (FFT)-the core of traditional ISAR imaging algorithms. Note that the CS based ISAR autofocusing methods are often computationally heavy to execute, which limits their applications in real-time ISAR systems. The improvement of computational efficiency of sparse aperture ISAR autofocusing is either necessary or at least highly desirable to promote their practical usage. This paper proposes an efficient sparse aperture ISAR autofocusing algorithm. To eliminate the effect of sparse aperture, the ISAR image is reconstructed by sparse Bayesian learning (SBL), and the phase error is estimated by minimum entropy during the reconstruction of ISAR image. However, the computation of expectation in SBL involves a matrix inversion with an intolerable computational complexity of at least O(L-3) . Here, in the Bayesian inference of SBL, we transform the time-consuming matrix inversion into an element-wise matrix division by the alternating direction method of multipliers (ADMM). An auxiliary variable is introduced to divide the computation of posterior into three simpler subproblems, bringing computational efficiency improvement. Experimental results based on both simulated and measured data validate the effectiveness as well as high efficiency of the proposed algorithm. It is 20-30 times faster than the SBL based sparse aperture ISAR autofocusing approach.																	1057-7149	1941-0042					2020	29						3213	3226		10.1109/TIP.2019.2957939													
J								Parameter-Free Gaussian PSF Model for Extended Depth of Field in Brightfield Microscopy	IEEE TRANSACTIONS ON IMAGE PROCESSING										Blind deconvolution; point spread function; shape from focus; focal stack	FOCUS IMAGE FUSION; DECONVOLUTION; SHAPE	Due to their limited depth of field, conventional brightfield microscopes cannot image thick specimens entirely in focus. A common way to obtain an all-in-focus image is to acquire a z-stack of images by optically sectioning the specimen and then apply a multi-focus fusion method. Unfortunately, for undersampled image stacks, fusion methods cannot remove the blur in regions where the in-focus position is between two optical sections. In this work, we propose a parameter-free Gaussian PSF model in which the all-in-focus image together with both the depth map and sampling distances in image plane are estimated from the image sequence automatically, without knowledge on the z-stack acquisition. In a maximum a posteriori framework, an iteratively reweighted least squares method is used to estimate the image and an adaptive scaled gradient descent method is utilized to estimate the depth map and sampling distances efficiently. Experiments on synthetic and real data demonstrate that the proposed method outperforms the current state-of-the-art, mitigating fusion artifacts and recovering sharper edges.																	1057-7149	1941-0042					2020	29						3227	3238		10.1109/TIP.2019.2957941													
J								A Novel Retinex-Based Fractional-Order Variational Model for Images With Severely Low Light	IEEE TRANSACTIONS ON IMAGE PROCESSING										Retinex; low-light image; fractional-order; variational model; image enhancement; reflectance; illumination	ALGORITHM	In this paper, we propose a novel Retinex-based fractional-order variational model for severely low-light images. The proposed method is more flexible in controlling the regularization extent than the existing integer-order regularization methods. Specifically, we decompose directly in the image domain and perform the fractional-order gradient total variation regularization on both the reflectance component and the illumination component to get more appropriate estimated results. The merits of the proposed method are as follows: 1) small-magnitude details are maintained in the estimated reflectance. 2) illumination components are effectively removed from the estimated reflectance. 3) the estimated illumination is more likely piecewise smooth. We compare the proposed method with other closely related Retinex-based methods. Experimental results demonstrate the effectiveness of the proposed method.																	1057-7149	1941-0042					2020	29						3239	3253		10.1109/TIP.2019.2958144													
J								From Rank Estimation to Rank Approximation: Rank Residual Constraint for Image Restoration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Low-rank; rank residual constraint; nuclear norm minimization; nonlocal self-similarity; group-based sparse representation; image restoration	THRESHOLDING ALGORITHM; SPARSE REPRESENTATION; QUALITY ASSESSMENT; ARTIFACTS; DCT; REGULARIZATION; REDUCTION; NOISE	In this paper, we propose a novel approach to the rank minimization problem, termed rank residual constraint (RRC) model. Different from existing low-rank based approaches, such as the well-known nuclear norm minimization (NNM) and the weighted nuclear norm minimization (WNNM), which estimate the underlying low-rank matrix directly from the corrupted observations, we progressively approximate the underlying low-rank matrix via minimizing the rank residual. Through integrating the image nonlocal self-similarity (NSS) prior with the proposed RRC model, we apply it to image restoration tasks, including image denoising and image compression artifacts reduction. Towards this end, we first obtain a good reference of the original image groups by using the image NSS prior, and then the rank residual of the image groups between this reference and the degraded image is minimized to achieve a better estimate to the desired image. In this manner, both the reference and the estimated image are updated gradually and jointly in each iteration. Based on the group-based sparse representation model, we further provide an analytical investigation on the feasibility of the proposed RRC model. Experimental results demonstrate that the proposed RRC method outperforms many state-of-the-art schemes in both the objective and perceptual quality.																	1057-7149	1941-0042					2020	29						3254	3269		10.1109/TIP.2019.2958309													
J								Ensemble of Deep Convolutional Neural Networks With Gabor Face Representations for Face Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Deep convolutional neural network (DCNN); Gabor face representations; Gabor DCNN (GDCNN) ensemble; face recognition (FR); confidence based majority voting	PATTERNS; HISTOGRAM; MODEL	Most DCNN-based FR approaches typically employ grayscale or RGB color images as input representations of DCNN architectures. However, other effective face representation methods have been developed and incorporated into current practical FR systems. In light of this fact, the focus of our study is to employ Gabor face representations in the design of DCNN-based FR frameworks to improve FR performance. To this end, we develop a novel "Gabor DCNN (GDCNN) ensemble" method that effectively applies different and multiple Gabor face representations as inputs during the training and testing phases of a DCNN for FR applications. The proposed GDCNN ensemble method primarily consists of two parts: 1) GDCNN ensemble construction and 2) GDCNN ensemble combination. The goal of the former part is to build an ensemble of GDCNN members (i.e., base models), each learned with a particular type of Gabor face representation. The objective of the latter part is to adaptively combine multiple FR outputs of individual GDCNN members. We perform extensive experiments to evaluate our proposed method on four public face databases (DBs) using the associated standard evaluation protocols. Experimental results demonstrate that our approach exhibits significantly better FR performance than typical DCNN-based approaches that rely only on grayscale or color face images as input representations. In addition, the feasibility of our proposed GDCNN ensemble has been successfully demonstrated by making comparisons with other state-of-the-art DCNN-based FR methods.																	1057-7149	1941-0042					2020	29						3270	3281		10.1109/TIP.2019.2958404													
J								Prediction and Sampling With Local Graph Transforms for Quasi-Lossless Light Field Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Light fields; energy compaction; transform coding; super-rays; graph Fourier transform; prediction; sampling	FOURIER-TRANSFORMS	Graph-based transforms have been shown to be powerful tools in terms of image energy compaction. However, when the size of the support increases to best capture signal dependencies, the computation of the basis functions becomes rapidly untractable. This problem is in particular compelling for high dimensional imaging data such as light fields. The use of local transforms with limited supports is a way to cope with this computational difficulty. Unfortunately, the locality of the support may not allow us to fully exploit long term signal dependencies present in both the spatial and angular dimensions of light fields. This paper describes sampling and prediction schemes with local graph-based transforms enabling to efficiently compact the signal energy and exploit dependencies beyond the local graph support. The proposed approach is investigated and is shown to be very efficient in the context of spatio-angular transforms for quasi-lossless compression of light fields.																	1057-7149	1941-0042					2020	29						3282	3295		10.1109/TIP.2019.2959215													
J								RIFT: Multi-Modal Image Matching Based on Radiation-Variation Insensitive Feature Transform	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-modal image matching; nonlinear radiation distortions (NRD); feature matching; maximum index map (MIM); phase congruency (PC)	PHASE CONGRUENCY; INVARIANT FEATURE; REGISTRATION; DETECTOR; SIFT	Traditional feature matching methods, such as scale-invariant feature transform (SIFT), usually use image intensity or gradient information to detect and describe feature points; however, both intensity and gradient are sensitive to nonlinear radiation distortions (NRD). To solve this problem, this paper proposes a novel feature matching algorithm that is robust to large NRD. The proposed method is called radiation-variation insensitive feature transform (RIFT). There are three main contributions in RIFT. First, RIFT uses phase congruency (PC) instead of image intensity for feature point detection. RIFT considers both the number and repeatability of feature points and detects both corner points and edge points on the PC map. Second, RIFT originally proposes a maximum index map (MIM) for feature description. The MIM is constructed from the log-Gabor convolution sequence and is much more robust to NRD than traditional gradient map. Thus, RIFT not only largely improves the stability of feature detection but also overcomes the limitation of gradient information for feature description. Third, RIFT analyses the inherent influence of rotations on the values of the MIM and realises rotation invariance. We use six different types of multi-modal image datasets to evaluate RIFT, including optical-optical, infrared-optical, synthetic aperture radar (SAR)-optical, depth-optical, map-optical, and day-night datasets. Experimental results show that RIFT is superior to SIFT and SAR-SIFT on multi-modal images. To the best of our knowledge, RIFT is the first feature matching algorithm that can achieve good performance on all the abovementioned types of multi-modal images. The source code of RIFT and the multi-modal image datasets are publicly available. (1) (1) http://www.escience.cn/people/lijiayuan/index.html																	1057-7149	1941-0042					2020	29						3296	3310		10.1109/TIP.2019.2959244													
J								Visual Object Tracking Via Multi-Stream Deep Similarity Learning Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Deep learning; visual tracking		Visual tracking remains a challenging research problem because of appearance variations of the object over time, changing cluttered background and requirement for real-time speed. In this paper, we investigate the problem of real-time accurate tracking in a instance-level tracking-by-verification mechanism. We propose a multi-stream deep similarity learning network to learn a similarity comparison model purely off-line. Our loss function encourages the distance between a positive patch and the background patches to be larger than that between the positive patch and the target template. Then, the learned model is directly used to determine the patch in each frame that is most distinctive to the background context and similar to the target template. Within the learned feature space, even if the distance between positive patches becomes large caused by the interference of background clutter, impact from hard distractors from the same class or the appearance change of the target, our method can still distinguish the target robustly using the relative distance. Besides, we also propose a complete framework considering the recovery from failures and the template updating to further improve the tracking performance without taking too much computing resource. Experiments on visual tracking benchmarks show the effectiveness of the proposed tracker when comparing with several recent real-time-speed trackers as well as trackers already included in the benchmarks.																	1057-7149	1941-0042					2020	29						3311	3320		10.1109/TIP.2019.2959249													
J								RGB-T Salient Object Detection via Fusing Multi-Level CNN Features	IEEE TRANSACTIONS ON IMAGE PROCESSING										RGB-T salient object detection; adjacent-depth feature combination; multi-branch group fusion; joint attention guided bi-directional message passing	IMAGE; TRACKING; MODEL	RGB-induced salient object detection has recently witnessed substantial progress, which is attributed to the superior feature learning capability of deep convolutional neural networks (CNNs). However, such detections suffer from challenging scenarios characterized by cluttered backgrounds, low-light conditions and variations in illumination. Instead of improving RGB based saliency detection, this paper takes advantage of the complementary benefits of RGB and thermal infrared images. Specifically, we propose a novel end-to-end network for multi-modal salient object detection, which turns the challenge of RGB-T saliency detection to a CNN feature fusion problem. To this end, a backbone network (e.g., VGG-16) is first adopted to extract the coarse features from each RGB or thermal infrared image individually, and then several adjacent-depth feature combination (ADFC) modules are designed to extract multi-level refined features for each single-modal input image, considering that features captured at different depths differ in semantic information and visual details. Subsequently, a multi-branch group fusion (MGF) module is employed to capture the cross-modal features by fusing those features from ADFC modules for a RGB-T image pair at each level. Finally, a joint attention guided bi-directional message passing (JABMP) module undertakes the task of saliency prediction via integrating the multi-level fused features from MGF modules. Experimental results on several public RGB-T salient object detection datasets demonstrate the superiorities of our proposed algorithm over the state-of-the-art approaches, especially under challenging conditions, such as poor illumination, complex background and low contrast.																	1057-7149	1941-0042					2020	29						3321	3335		10.1109/TIP.2019.2959253													
J								A Two-Stage Approach to Few-Shot Learning for Image Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Transfer learning; convolutional neural network; few-shot learning; image classification		This paper proposes a multi-layer neural network structure for few-shot image recognition of novel categories. The proposed multi-layer neural network architecture encodes transferable knowledge extracted from a large annotated dataset of base categories. This architecture is then applied to novel categories containing only a few samples. The transfer of knowledge is carried out at the feature-extraction and the classification levels distributed across the two training stages. In the first-training stage, we introduce the relative feature to capture the structure of the data as well as obtain a low-dimensional discriminative space. Secondly, we account for the variable variance of different categories by using a network to predict the variance of each class. Classification is then performed by computing the Mahalanobis distance to the mean-class representation in contrast to previous approaches that used the Euclidean distance. In the second-training stage, a category-agnostic mapping is learned from the mean-sample representation to its corresponding class-prototype representation. This is because the mean-sample representation may not accurately represent the novel category prototype. Finally, we evaluate the proposed network structure on four standard few-shot image recognition datasets, where our proposed few-shot learning system produces competitive performance compared to previous work. We also extensively studied and analyzed the contribution of each component of our proposed framework.																	1057-7149	1941-0042					2020	29						3336	3350		10.1109/TIP.2019.2959254													
J								Local Semantic Siamese Networks for Fast Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visual object tracking; Siamese deep network; local feature representation	OBJECT TRACKING	Learning a powerful feature representation is critical for constructing a robust Siamese tracker. However, most existing Siamese trackers learn the global appearance features of the entire object, which usually suffers from drift problems caused by partial occlusion or non-rigid appearance deformation. In this paper, we propose a new Local Semantic Siamese (LSSiam) network to extract more robust features for solving these drift problems, since the local semantic features contain more fine-grained and partial information. We learn the semantic features during offline training by adding a classification branch into the classical Siamese framework. To further enhance the representation of features, we design a generally focal logistic loss to mine the hard negative samples. During the online tracking, we remove the classification branch and propose an efficient template updating strategy to avoid aggressive computing load. Thus, the proposed tracker can run at a high-speed of 100 Frame-per-Second (FPS) far beyond real-time requirement. Extensive experiments on popular benchmarks demonstrate the proposed LSSiam tracker achieves the state-of-the-art performance with a high-speed. Our source code is available at https://github.com/shenjianbing/LSSiam.																	1057-7149	1941-0042					2020	29						3351	3364		10.1109/TIP.2019.2959256													
J								A Multi-Scale Spatial-Temporal Attention Model for Person Re-Identification in Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video-based person re-id; spatial-temporal attention; multi-scale pooling		In this paper, we propose a novel deep neural network based attention model to learn the representative local regions from a video sequence for person re-identification. Specifically, we propose a multi-scale spatial-temporal attention (MSTA) model to measure the regions of each frame in different scales from the perspective of whole video sequence. Compared to traditional temporal attention models, MSTA focuses on exploiting the importance of local regions of each frame to the whole video representation in both spatial and temporal domains. A new training strategy is designed for the proposed model by incorporating the image-to-image mode with the video-to-video mode. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed model over state-of-the-art methods.																	1057-7149	1941-0042					2020	29						3365	3373		10.1109/TIP.2019.2959653													
J								A Grassmannian Graph Approach to Affine Invariant Feature Matching	IEEE TRANSACTIONS ON IMAGE PROCESSING										Shape matching; 2D and 3D point registration; affine invariance; invariant coordinates; Grassmann manifold; Laplace-Beltrami operator (LBO); graph Laplacian; singular value decomposition (SVD); feature matching; object recognition	SHAPE; TRANSFORMATION; REGISTRATION; ALGORITHM; 2D	In this work, we present a novel, theoretical approach to address one of the longstanding problems in computer vision: 2D and 3D affine invariant feature matching. Our proposed Grassmannian Graph (GrassGraph) framework employs a two stage procedure that is capable of robustly recovering correspondences between two unorganized, affinely related feature (point) sets. In the ideal case, the first stage maps the feature sets to an affine invariant Grassmannian representation, where the features are mapped into the same subspace. It turns out that coordinate representations extracted from the Grassmannian differ by an arbitrary orthonormal matrix. In the second stage, by approximating the Laplace-Beltrami operator (LBO) on these coordinates, this extra orthonormal factor is nullified, providing true affine invariant coordinates which we then utilize to recover correspondences via simple mutual nearest neighbor relations. Our validation benchmarks use large number of experimental trials performed on 2D and 3D datasets. Experimental results show that the proposed Grass-Graph method successfully recovers large affine transformations.																	1057-7149	1941-0042					2020	29						3374	3387		10.1109/TIP.2019.2959722													
J								Multistage GAN for Fabric Defect Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Fabric defect detection; deep learning; semantic segmentation; generative adversarial network	NETWORKS	Fabric defect detection is an intriguing but challenging topic. Many methods have been proposed for fabric defect detection, but these methods are still suboptimal due to the complex diversity of both fabric textures and defects. In this paper, we propose a generative adversarial network (GAN)-based framework for fabric defect detection. Considering existing challenges in real-world applications, the proposed fabric defect detection system is capable of learning existing fabric defect samples and automatically adapting to different fabric textures during different application periods. Specifically, we customize a deep semantic segmentation network for fabric defect detection that can detect different defect types. Furthermore, we attempted to train a multistage GAN to synthesize reasonable defects in new defect-free samples. First, a texture-conditioned GAN is trained to explore the conditional distribution of defects given different texture backgrounds. Given a novel fabric, we aim to generate reasonable defective patches. Then, a GAN-based fusion network fuses the generated defects to specific locations. Finally, the well-trained multistage GAN continuously updates the existing fabric defect datasets and contributes to the fine-tuning of the semantic segmentation network to better detect defects under different conditions. Comprehensive experiments on various representative fabric samples are conducted to verify the detection performance of our proposed method.																	1057-7149	1941-0042					2020	29						3388	3400		10.1109/TIP.2019.2959741													
J								Grayscale-Thermal Tracking via Inverse Sparse Representation-Based Collaborative Encoding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Grayscale-thermal tracking; inverse sparse representation; discriminant analysis; feature selection	VISUAL TRACKING; FUSION	Grayscale-thermal tracking has attracted a great deal of attention due to its capability of fusing two different yet complementary target observations. Existing methods often consider extracting the discriminative target information and exploring the target correlation among different images as two separate issues, ignoring their interdependence. This may cause tracking drifts in challenging video pairs. This paper presents a collaborative encoding model called joint correlation and discriminant analysis based inver-sparse representation (JCDA-InvSR) to jointly encode the target candidates in the grayscale and thermal video sequences. In particular, we develop a multi-objective programming to integrate the feature selection and the multi-view correlation analysis into a unified optimization problem in JCDA-InvSR, which can simultaneously highlight the special characters of the grayscale and thermal targets through alternately optimizing two aspects: the target discrimination within a given image and the target correlation across different images. For robust grayscale-thermal tracking, we also incorporate the prior knowledge of target candidate codes into the SVM based target classifier to overcome the overfitting caused by limited training labels. Extensive experiments on GTOT and RGBT234 datasets illustrate the promising performance of our tracking framework.																	1057-7149	1941-0042					2020	29						3401	3415		10.1109/TIP.2019.2959912													
J								CDPM: Convolutional Deformable Part Models for Semantically Aligned Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Person re-identification; alignment-robust recognition; part-based model; multi-task learning	SIMILARITY; NETWORK	Part-level representations are essential for robust person re-identification. However, common errors that arise during pedestrian detection frequently result in severe misalignment problems for body parts, which degrade the quality of part representations. Accordingly, to deal with this problem, we propose a novel model named Convolutional Deformable Part Models (CDPM). CDPM works by decoupling the complex part alignment procedure into two easier steps: first, a vertical alignment step detects each body part in the vertical direction, with the help of a multi-task learning model; second, a horizontal refinement step based on attention suppresses the background information around each detected body part. Since these two steps are performed orthogonally and sequentially, the difficulty of part alignment is significantly reduced. In the testing stage, CDPM is able to accurately align flexible body parts without any need for outside information. Extensive experimental results demonstrate the effectiveness of the proposed CDPM for part alignment. Most impressively, CDPM achieves state-of-the-art performance on three large-scale datasets: Market-1501, DukeMTMC-ReID, and CUHK03.																	1057-7149	1941-0042					2020	29						3416	3428		10.1109/TIP.2019.2959923													
J								HMS-Net: Hierarchical Multi-Scale Sparsity-Invariant Network for Sparse Depth Completion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Depth completion; convolutional neural network; sparsity-invariant operations	RECOVERY	Dense depth cues are important and have wide applications in various computer vision tasks. In autonomous driving, LIDAR sensors are adopted to acquire depth measurements around the vehicle to perceive the surrounding environments. However, depth maps obtained by LIDAR are generally sparse because of its hardware limitation. The task of depth completion attracts increasing attention, which aims at generating a dense depth map from an input sparse depth map. To effectively utilize multi-scale features, we propose three novel sparsity-invariant operations, based on which, a sparsity-invariant multi-scale encoder-decoder network (HMS-Net) for handling sparse inputs and sparse feature maps is also proposed. Additional RGB features could be incorporated to further improve the depth completion performance. Our extensive experiments and component analysis on two public benchmarks, KITTI depth completion benchmark and NYU-depth-v2 dataset, demonstrate the effectiveness of the proposed approach. As of Aug. 12th, 2018, on KITTI depth completion leaderboard, our proposed model without RGB guidance ranks 1st among all peer-reviewed methods without using RGB information, and our model with RGB guidance ranks 2nd among all RGB-guided methods.																	1057-7149	1941-0042					2020	29						3429	3441		10.1109/TIP.2019.2960589													
J								End-to-End Optimized ROI Image Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Region of interest; lossy image compression; object segmentation; ROI coding; rate distortion optimization; convolutional neural network		Compressing an image with more bits automatically allocated to the region of interest (ROI) than to the background can both protect key information and reduce substantial redundancy. This paper models ROI image compression as an optimization problem of minimizing a weighted sum of the rate of the image and distortion of the ROI. The traditional framework solves this problem by cascading ROI prediction and ROI coding, through which achieving the optimized solution is impossible. To improve coding performance, we propose a novel deep-learning-based unified framework that can achieve rate distortion optimization for ROI compression. Specifically, the proposed framework includes a pair of ROI encoder and decoder convolutional neural networks and a learned entropy codec. The encoder network simultaneously generates multiscale representations that support efficient rate allocation and an implicit ROI mask that guides rate allocation. The proposed framework can automatically complete ROI image compression, and it can be optimized from data in an end-to-end manner. To effectively train the framework by back propagation, we develop a soft-to-hard ROI prediction scheme to make the entire framework differential. To improve visual quality, we propose a hierarchical distortion loss function to protect both pixel-level fidelity for ROI and structural similarity for the entire image. The proposed framework is implemented in two scenarios: salient-target and face-target ROI compression. Comparative experiments demonstrate the advantages of the proposed framework over the traditional framework, including considerably better subjective visual quality, significantly higher objective ROI compression performance and execution efficiency.																	1057-7149	1941-0042					2020	29						3442	3457		10.1109/TIP.2019.2960869													
J								Structure and Texture-Aware Image Decomposition via Training a Neural Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Structure-texture decomposition; image layer separation; structure descriptor; texture descriptor	ALGORITHMS	Structure-texture image decomposition is a fundamental but challenging topic in computational graphics and image processing. In this paper, we introduce a structure-aware and a texture-aware measures to facilitate the structure-texture decomposition (STD) of images. Edge strengths and spatial scales that have been widely-used in previous STD researches cannot describe the structures and textures of images well. The proposed two measures differentiate image textures from image structures based on their distinctive characteristics. Specifically, the first one aims to measure the anisotropy of local gradients, and the second one is designed to measure the repeatability degree of signal patterns in a neighboring region. Since these two measures describe different properties of image structures and textures, they are complementary to each other. The STD is achieved by optimizing an objective function based on the two new measures. As using traditional optimization methods to solve the optimization problem will require designing different optimizers for different functional spaces, we employ an architecture of deep neural network to optimize the STD cost function in a unified manner. The experimental results demonstrate that, as compared with some state-of-the-art methods, our method can better separate image structure and texture and result in shaper edges in the structural component. Furthermore, to demonstrate the usefulness of the proposed STD method, we have successfully applied it to several applications including detail enhancement, edge detection, and visual quality assessment of super-resolved images.																	1057-7149	1941-0042					2020	29						3458	3473		10.1109/TIP.2019.2961232													
J								3D Point Cloud Denoising Using Graph Laplacian Regularization of a Low Dimensional Manifold Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Graph signal processing; point cloud denoising; low-dimensional manifold		3D point cloud-a new signal representation of volumetric objects-is a discrete collection of triples marking exterior object surface locations in 3D space. Conventional imperfect acquisition processes of 3D point cloud-e.g., stereo-matching from multiple viewpoint images or depth data acquired directly from active light sensors-imply non-negligible noise in the data. In this paper, we extend a previously proposed low-dimensional manifold model for the image patches to surface patches in the point cloud, and seek self-similar patches to denoise them simultaneously using the patch manifold prior. Due to discrete observations of the patches on the manifold, we approximate the manifold dimension computation defined in the continuous domain with a patch-based graph Laplacian regularizer, and propose a new discrete patch distance measure to quantify the similarity between two same-sized surface patches for graph construction that is robust to noise. We show that our graph Laplacian regularizer leads to speedy implementation and has desirable numerical stability properties given its natural graph spectral interpretation. Extensive simulation results show that our proposed denoising scheme outperforms state-of-the-art methods in objective metrics and better preserves visually salient structural features like edges.																	1057-7149	1941-0042					2020	29						3474	3489		10.1109/TIP.2019.2961429													
J								Discriminative Multi-View Privileged Information Learning for Image Re-Ranking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-view re-ranking; privileged information (PI); latent subspaces; multi-view embeddings	RELEVANCE FEEDBACK; RETRIEVAL; FEATURES	Conventional multi-view re-ranking methods usually perform asymmetrical matching between the region of interest (ROI) in the query image and the whole target image for similarity computation. Due to the inconsistency in the visual appearance, this practice tends to degrade the retrieval accuracy particularly when the image ROI, which is usually interpreted as the image objectness, accounts for a smaller region in the image. Since Privileged Information (PI), which can be viewed as the image prior, is able to characterize well the image objectness, we are aiming at leveraging PI for further improving the performance of multi-view re-ranking in this paper. Towards this end, we propose a discriminative multi-view re-ranking approach in which both the original global image visual contents and the local auxiliary PI features are simultaneously integrated into a unified training framework for generating the latent subspaces with sufficient discriminating power. For the on-the-fly re-ranking, since the multi-view PI features are unavailable, we only project the original multi-view image representations onto the latent subspace, and thus the re-ranking can be achieved by computing and sorting the distances from the multi-view embeddings to the separating hyperplane. Extensive experimental evaluations on the two public benchmarks, Oxford5k and Paris6k, reveal that our approach provides further performance boost for accurate image re-ranking, whilst the comparative study demonstrates the advantage of our method against other multi-view re-ranking methods.																	1057-7149	1941-0042					2020	29						3490	3505		10.1109/TIP.2019.2962667													
J								Image Feature Correspondence Selection: A Comparative Study and a New Contribution	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image feature correspondence; feature matching; correspondence selection; inliers	PERFORMANCE EVALUATION; SCALE; FRAMEWORK; RANSAC	Image feature correspondence selection is pivotal to many computer vision tasks from object recognition to 3D reconstruction. Although many correspondence selection algorithms have been developed in the past decade, there still lacks an in-depth evaluation and comparison in the open literature, which makes it difficult to choose the appropriate algorithm for a specific application. This paper attempts to fill this gap by evaluating eight competing correspondence selection algorithms including both classical methods and current state-of-the-art ones. In addition to preselected correspondences, we have compared different combinations of detector and descriptor on four standard datasets. The diversity of those datasets cover a wide range of uncertainty factors including zoom, rotation, blur, viewpoint change, JPEG compression, light change, different rendering styles and multiple structures. We have measured the quality of competing correspondence selection algorithms in terms of four performance metrics - i.e., precision, recall, F-measure and efficiency. Moreover, we propose to combine the strengths of eight competing methods by combining their correspondence selection results. Extensive experimental results are reported to demonstrate the superiority of several fusion strategies to individual methods, which suggests the possibility of adaptively combining those methods for even better performance.																	1057-7149	1941-0042					2020	29						3506	3519		10.1109/TIP.2019.2962678													
J								Semantic Segmentation With Context Encoding and Multi-Path Decoding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantic segmentation; context encoding; gated sum; boundary delineation refinement; deep learning; CGBNet; convolutional neural networks	SCENE; FEATURES	Semantic image segmentation aims to classify every pixel of a scene image to one of many classes. It implicitly involves object recognition, localization, and boundary delineation. In this paper, we propose a segmentation network called CGBNet to enhance the segmentation performance by context encoding and multi-path decoding. We first propose a context encoding module that generates context-contrasted local feature to make use of the informative context and the discriminative local information. This context encoding module greatly improves the segmentation performance, especially for inconspicuous objects. Furthermore, we propose a scale-selection scheme to selectively fuse the segmentation results from different-scales of features at every spatial position. It adaptively selects appropriate score maps from rich scales of features. To improve the segmentation performance results at boundary, we further propose a boundary delineation module that encourages the location-specific very-low-level features near the boundaries to take part in the final prediction and suppresses them far from the boundaries. The proposed segmentation network achieves very competitive performance in terms of all three different evaluation metrics consistently on the six popular scene segmentation datasets, Pascal Context, SUN-RGBD, Sift Flow, COCO Stuff, ADE20K, and Cityscapes.																	1057-7149	1941-0042					2020	29						3520	3533		10.1109/TIP.2019.2962685													
J								A Multistage Refinement Network for Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Salient object detection; stagewise refinement; pyramid pooling module; layerwise recurrent; channel attention; global average pooling	DRIVEN; MODEL	Deep convolutional neural networks (CNNs) have been successfully applied to a wide variety of problems in computer vision, including salient object detection. To accurately detect and segment salient objects, it is necessary to extract and combine high-level semantic features with low-level fine details simultaneously. This is challenging for CNNs because repeated subsampling operations such as pooling and convolution lead to a significant decrease in the feature resolution, which results in the loss of spatial details and finer structures. Therefore, we propose augmenting feedforward neural networks by using the multistage refinement mechanism. In the first stage, a master net is built to generate a coarse prediction map in which most detailed structures are missing. In the following stages, the refinement net with layerwise recurrent connections to the master net is equipped to progressively combine local context information across stages to refine the preceding saliency maps in a stagewise manner. Furthermore, the pyramid pooling module and channel attention module are applied to aggregate different-region-based global contexts. Extensive evaluations over six benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches.																	1057-7149	1941-0042					2020	29						3534	3545		10.1109/TIP.2019.2962688													
J								SITUP: Scale Invariant Tracking Using Average Peak-to-Correlation Energy	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visual object tracking; discriminative correlation filter; scale estimation; average peak-to-correlation energy	VISUAL TRACKING; OBJECT TRACKING; RECOGNITION; SHAPE	Robust and accurate scale estimation of a target object is a challenging task in visual object tracking. Most existing tracking methods cannot accommodate large scale variation in complex image sequences and thus result in inferior performance. In this paper, we propose to incorporate a novel criterion called the average peak-to-correlation energy into the multi-resolution translation filter framework to obtain robust and accurate scale estimation. The resulting system is named SITUP: Scale Invariant Tracking using Average Peak-to-Correlation Energy. SITUP effectively tackles the problem of fixed template size in standard discriminative correlation filter based trackers. Extensive empirical evaluation on the publicly available tracking benchmark datasets demonstrates that the proposed scale searching framework meets the demands of scale variation challenges effectively while providing superior performance over other scale adaptive variants of standard discriminative correlation filter based trackers. Also, SITUP obtains favorable performance compared to state-of-the-art trackers for various scenarios while operating in real-time on a single CPU.																	1057-7149	1941-0042					2020	29						3546	3557		10.1109/TIP.2019.2962694													
J								Compressive Spectral Light Field Image Reconstruction via Online Tensor Representation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spectral light field; compressive imaging; online tensor representation; tucker decomposition	DECOMPOSITIONS	In recent years there has been an increasing interest in sensing devices that capture multidimensional information such as the spectral light field (SLF) images, which are 5-dimensional (5D) representations of a scene including 2D spatial, 2D angular and 1D spectral information. Spatio-spectral and angular information plays an important role in modern applications spanning from microscopy to computer vision. However, SLF sensors use expensive beam-splitters or cameras arrays placed in tandem, which split the sensing problem in two time consuming and independent tasks: spectral and light field imaging tasks. This work proposes a compressive spectral light field imaging architecture that builds on the principles of the compressive imaging framework, to capture multiplexed representations of the multidimensional information, so that, less measurements are required to capture the SLF data cube. Alongside, we propose a computational algorithm to recover the 5D information from the compressed measurements, exploiting the inherent high correlations within the SLF by treating them as 3D tensors. Furthermore, exploiting the geometry properties of the proposed optical architecture, the Tucker decomposition is applied to the set of compressed measurements, so that, an ad-hoc dictionary-like image representation basis is calculated online. This in turn, entails a more accurate reconstruction of the SLF since the dictionary fits the specific characteristics of the image itself. We demonstrate through simulations over three SLF datasets captured in our laboratory, and an experimental proof-of-concept implementation, that the proposed compressive imaging device together with the proposed computational algorithm represent an efficient alternative to capture SLF, compared to conventional methods that employ either side-information or multiple sensors. Also, we show that the tensor-based proposed algorithm exhibits a lower computational complexity than the matrix-based state of the art counterparts, thus enabling fast processing of multidimensional images.																	1057-7149	1941-0042					2020	29						3558	3568		10.1109/TIP.2019.2963376													
J								Deepzzle: Solving Visual Jigsaw Puzzles With Deep Learning and Shortest Path Optimization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reassembly; jigsaw puzzle; deep learning; graph; branch-cut; cultural heritage		We tackle the image reassembly problem with wide space between the fragments, in such a way that the patterns and colors continuity is mostly unusable. The spacing emulates the erosion of which the archaeological fragments suffer. We crop-square the fragments borders to compel our algorithm to learn from the content of the fragments. We also complicate the image reassembly by removing fragments and adding pieces from other sources. We use a two-step method to obtain the reassemblies: 1) a neural network predicts the positions of the fragments despite the gaps between them; 2) a graph that leads to the best reassemblies is made from these predictions. In this paper, we notably investigate the effect of branch-cut in the graph of reassemblies. We also provide a comparison with the literature, solve complex images reassemblies, explore at length the dataset, and propose a new metric that suits its specificities.																	1057-7149	1941-0042					2020	29						3569	3581		10.1109/TIP.2019.2963378													
J								PWStableNet: Learning Pixel-Wise Warping Maps for Video Stabilization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video stabilization; pixel-wise warping; cascade networks		As the videos captured by hand-held cameras are often perturbed by high-frequency jitters, stabilization of these videos is an essential task. Many video stabilization methods have been proposed to stabilize shaky videos. However, most methods estimate one global homography or several homographies based on fixed meshes to warp the shaky frames into their stabilized views. Due to the existence of parallax, such single or a few homographies can not well handle the depth variation. In contrast to these traditional methods, we propose a novel video stabilization network, called PWStableNet, which comes up pixel-wise warping maps, i.e., potentially different warping for different pixels, and stabilizes each pixel to its stabilized view. To our best knowledge, this is the first deep learning based pixel-wise video stabilization. The proposed method is built upon a multi-stage cascade encoder-decoder architecture and learns pixel-wise warping maps from consecutive unstable frames. Inter-stage connections are also introduced to add feature maps of a former stage to the corresponding feature maps at a latter stage, which enables the latter stage to learn the residual from the feature maps of former stages. This cascade architecture can produce more precise warping maps at latter stages. To ensure the correct learning of pixel-wise warping maps, we use a well-designed loss function to guide the training procedure of the proposed PWStableNet. The proposed stabilization method achieves comparable performance with traditional methods, but stronger robustness and much faster processing speed. Moreover, the proposed stabilization method outperforms some typical CNN-based stabilization methods, especially in videos with strong parallax. Codes will be provided at https://github.com/mindazhao/pix-pix-warping-video-stabilization.																	1057-7149	1941-0042					2020	29						3582	3595		10.1109/TIP.2019.2963380													
J								Strongly Constrained Discrete Hashing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Learning to hash; image retrieval; discrete optimization	ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH	Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH $_{K}$ . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH $_{K}$ achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.																	1057-7149	1941-0042					2020	29						3596	3611		10.1109/TIP.2020.2963952													
J								Learning a Single Tucker Decomposition Network for Lossy Image Compression With Multiple Bits-per-Pixel Rates	IEEE TRANSACTIONS ON IMAGE PROCESSING										Lossy image compression; convolutional neural networks; tucker decomposition	QUANTIZATION; SIMILARITY	Lossy image compression (LIC), which aims to utilize inexact approximations to represent an image more compactly, is a classical problem in image processing. Recently, deep convolutional neural networks (CNNs) have achieved interesting results in LIC by learning an encoder-quantizer-decoder network from a large amount of data. However, existing CNN-based LIC methods generally train a network for a specific bits-per-pixel (bpp). Such a "one-network-per-bpp" problem limits the generality and flexibility of CNNs to practical LIC applications. In this paper, we propose to learn a single CNN which can perform LIC at multiple bpp rates. A simple yet effective Tucker Decomposition Network (TDNet) is developed, where there is a novel tucker decomposition layer (TDL) to decompose a latent image representation into a set of projection matrices and a core tensor. By changing the rank of core tensor and its quantization, we can easily adjust the bpp rate of latent image representation within a single CNN. Furthermore, an iterative non-uniform quantization scheme is presented to optimize the quantizer, and a coarse-to-fine training strategy is introduced to reconstruct the decompressed images. Extensive experiments demonstrate the state-of-the-art compression performance of TDNet in terms of both PSNR and MS-SSIM indices.																	1057-7149	1941-0042					2020	29						3612	3625		10.1109/TIP.2020.2963956													
J								Multi-Task Consistency-Preserving Adversarial Hashing for Cross-Modal Retrieval	IEEE TRANSACTIONS ON IMAGE PROCESSING										Cross-modal retrieval; hashing; consistency-preserving; adversarial; multi-task		Owing to the advantages of low storage cost and high query efficiency, cross-modal hashing has received increasing attention recently. As failing to bridge the inherent modality gap between modalities, most existing cross-modal hashing methods have limited capability to explore the semantic consistency information between different modality data, leading to unsatisfactory search performance. To address this problem, we propose a novel deep hashing method named Multi-Task Consistency-Preserving Adversarial Hashing (CPAH) to fully explore the semantic consistency and correlation between different modalities for efficient cross-modal retrieval. First, we design a consistency refined module (CR) to divide the representations of different modality into two irrelevant parts, i.e., modality-common and modality-private representations. Then, a multi-task adversarial learning module (MA) is presented, which can make the modality-common representation of different modalities close to each other on feature distribution and semantic consistency. Finally, the compact and powerful hash codes can be generated from modality-common representation. Comprehensive evaluations conducted on three representative cross-modal benchmark datasets illustrate our method is superior to the state-of-the-art cross-modal hashing methods.																	1057-7149	1941-0042					2020	29						3626	3637		10.1109/TIP.2020.2963957													
J								A Data Dependent Multiscale Model for Hyperspectral Unmixing With Spectral Variability	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hyperspectral data; spectral variability; spatial regularization; multiscale; superpixels	SPATIAL REGULARIZATION; COMPONENT ANALYSIS; OPTIMIZATION; INFORMATION; IMAGES	Spectral variability in hyperspectral images can result from factors including environmental, illumination, atmospheric and temporal changes. Its occurrence may lead to the propagation of significant estimation errors in the unmixing process. To address this issue, extended linear mixing models have been proposed which lead to large scale nonsmooth ill-posed inverse problems. Furthermore, the regularization strategies used to obtain meaningful results have introduced interdependencies among abundance solutions that further increase the complexity of the resulting optimization problem. In this paper we present a novel data dependent multiscale model for hyperspectral unmixing accounting for spectral variability. The new method incorporates spatial contextual information to the abundances in extended linear mixing models by using a multiscale transform based on superpixels. The proposed method results in a fast algorithm that solves the abundance estimation problem only once in each scale during each iteration. Simulation results using synthetic and real images compare the performances, both in accuracy and execution time, of the proposed algorithm and other state-of-the-art solutions.																	1057-7149	1941-0042					2020	29						3638	3651		10.1109/TIP.2020.2963959													
J								Illumination Invariant Hyperspectral Image Unmixing Based on a Digital Surface Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spectral unmixing; endmember; illumination; shadow; spectral variability	SPECTRAL MIXTURE ANALYSIS; ENDMEMBER VARIABILITY; DATA FUSION; ALGORITHM; SPARSE	Although many spectral unmixing models have been developed to address spectral variability caused by variable incident illuminations, the mechanism of the spectral variability is still unclear. This paper proposes an unmixing model, named illumination invariant spectral unmixing (IISU). IISU makes the first attempt to use the radiance hyperspectral data and a LiDAR-derived digital surface model (DSM) in order to physically explain variable illuminations and shadows in the unmixing framework. Incident angles, sky factors, visibility from the sun derived from the LiDAR-derived DSM support the explicit explanation of endmember variability in the unmixing process from radiance perspective. The proposed model was efficiently solved by a straightforward optimization procedure. The unmixing results showed that the other state-of-the-art unmixing models did not work well especially in the shaded pixels. On the other hand, the proposed model estimated more accurate abundances and shadow compensated reflectance than the existing models.																	1057-7149	1941-0042					2020	29						3652	3664		10.1109/TIP.2020.2963961													
J								Zero-VAE-GAN: Generating Unseen Features for Generalized and Transductive Zero-Shot Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Zero-shot learning; generative model; self-training		Zero-shot learning (ZSL) is a challenging task due to the lack of unseen class data during training. Existing works attempt to establish a mapping between the visual and class spaces through a common intermediate semantic space. The main limitation of existing methods is the strong bias towards seen class, known as the domain shift problem, which leads to unsatisfactory performance in both conventional and generalized ZSL tasks. To tackle this challenge, we propose to convert ZSL to the conventional supervised learning by generating features for unseen classes. To this end, a joint generative model that couples variational autoencoder (VAE) and generative adversarial network (GAN), called Zero-VAE-GAN, is proposed to generate high-quality unseen features. To enhance the class-level discriminability, an adversarial categorization network is incorporated into the joint framework. Besides, we propose two self-training strategies to augment unlabeled unseen features for the transductive extension of our model, addressing the domain shift problem to a large extent. Experimental results on five standard benchmarks and a large-scale dataset demonstrate the superiority of our generative model over the state-of-the-art methods for conventional, especially generalized ZSL tasks. Moreover, the further improvement of the transductive setting demonstrates the effectiveness of the proposed self-training strategies.																	1057-7149	1941-0042					2020	29						3665	3680		10.1109/TIP.2020.2964429													
J								A Recurrent Neural Network for Particle Tracking in Microscopy Images Using Future Information, Track Hypotheses, and Multiple Detections	IEEE TRANSACTIONS ON IMAGE PROCESSING										Biomedical imaging; microscopy images; particle tracking; deep learning	CELL; ENDOCYTOSIS; ALGORITHM; PLATFORM; DENSE	Automatic tracking of particles in time-lapse fluorescence microscopy images is essential for quantifying the dynamic behavior of subcellular structures and virus structures. We introduce a novel particle tracking approach based on a deep recurrent neural network architecture that exploits past and future information in both forward and backward direction. Assignment probabilities are determined jointly across multiple detections, and the probability of missing detections is computed. In addition, existence probabilities are determined by the network to handle track initiation and termination. For correspondence finding, track hypotheses are propagated to future time points so that information at later time points can be used to resolve ambiguities. A handcrafted similarity measure and handcrafted motion features are not necessary. Manually labeled data is not required for network training. We evaluated the performance of our approach using image data of the Particle Tracking Challenge as well as real fluorescence microscopy image sequences of virus structures. It turned out that the proposed approach outperforms previous methods.																	1057-7149	1941-0042					2020	29						3681	3694		10.1109/TIP.2020.2964515													
J								Connecting Image Denoising and High-Level Vision Tasks via Deep Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Deep learning; neural network; image denoising; high-level vision	SPARSE REPRESENTATION	Image denoising and high-level vision tasks are usually handled independently in the conventional practice of computer vision, and their connection is fragile. In this paper, we cope with the two jointly and explore the mutual influence between them with the focus on two questions, namely (1) how image denoising can help improving high-level vision tasks, and (2) how the semantic information from high-level vision tasks can be used to guide image denoising. First for image denoising we propose a convolutional neural network in which convolutions are conducted in various spatial resolutions via downsampling and upsampling operations in order to fuse and exploit contextual information on different scales. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We experimentally show that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network produces more visually appealing results. Extensive experiments demonstrate the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online: https://github.com/Ding-Liu/DeepDenoising																	1057-7149	1941-0042					2020	29						3695	3706		10.1109/TIP.2020.2964518													
J								Model-Free Distortion Rectification Framework Bridged by Distortion Distribution Map	IEEE TRANSACTIONS ON IMAGE PROCESSING										Distortion rectification; model-free framework; dual-stream feature learning; deep learning	CALIBRATION; CAMERA; IMAGES	Recently, learning-based distortion rectification schemes have shown high efficiency. However, most of these methods only focus on a specific camera model with fixed parameters, thus failing to be extended to other models. To avoid such a disadvantage, we propose a model-free distortion rectification framework for the single-shot case, bridged by the distortion distribution map (DDM). Our framework is based on an observation that the pixel-wise distortion information is explicitly regular in a distorted image, despite different models having different types and numbers of distortion parameters. Motivated by this observation, instead of estimating the heterogeneous distortion parameters, we construct a proposed distortion distribution map that intuitively indicates the global distortion features of a distorted image. In addition, we develop a dual-stream feature learning module, benefitting from both the advantages of traditional methods that leverage the local handcrafted feature and learning-based methods that focus on the global semantic feature perception. Due to the sparsity of handcrafted features, we discrete the features into a 2D point map and learn the structure inspired by PointNet. Finally, a multimodal attention fusion module is designed to attentively fuse the local structural and global semantic features, providing the hybrid features for the more reasonable scene recovery. The experimental results demonstrate the excellent generalization ability and more significant performance of our method in both quantitative and qualitative evaluations, compared with the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						3707	3718		10.1109/TIP.2020.2964523													
J								Material Based Object Tracking in Hyperspectral Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object tracking; hyperspectral imaging; material unmixing	SPARSE REGRESSION	Traditional color images only depict color intensities in red, green and blue channels, often making object trackers fail in challenging scenarios, e.g., background clutter and rapid changes of target appearance. Alternatively, material information of targets contained in large amount of bands of hyperspectral images (HSI) is more robust to these difficult conditions. In this paper, we conduct a comprehensive study on how material information can be utilized to boost object tracking from three aspects: dataset, material feature representation and material based tracking. In terms of dataset, we construct a dataset of fully-annotated videos, which contain both hyperspectral and color sequences of the same scene. Material information is represented by spectral-spatial histogram of multidimensional gradients, which describes the 3D local spectral-spatial structure in an HSI, and fractional abundances of constituted material components which encode the underlying material distribution. These two types of features are embedded into correlation filters, yielding material based tracking. Experimental results on the collected dataset show the potentials and advantages of material based object tracking.																	1057-7149	1941-0042					2020	29						3719	3733		10.1109/TIP.2020.2965302													
J								Joint Coding of Local and Global Deep Features in Videos for Visual Search	IEEE TRANSACTIONS ON IMAGE PROCESSING										Local deep feature; joint coding; visual search; inter-feature correlation	EFFICIENT APPROACH; DESCRIPTORS; RETRIEVAL	Practically, it is more feasible to collect compact visual features rather than the video streams from hundreds of thousands of cameras into the cloud for big data analysis and retrieval. Then the problem becomes which kinds of features should be extracted, compressed and transmitted so as to meet the requirements of various visual tasks. Recently, many studies have indicated that the activations from the convolutional layers in convolutional neural networks (CNNs) can be treated as local deep features describing particular details inside an image region, which are then aggregated (e.g., using Fisher Vectors) as a powerful global descriptor. Combination of local and global features can satisfy those various needs effectively. It has also been validated that, if only local deep features are coded and transmitted to the cloud while the global features are recovered using the decoded local features, the aggregated global features should be lossy and consequently would degrade the overall performance. Therefore, this paper proposes a joint coding framework for local and global deep features (DFJC) extracted from videos. In this framework, we introduce a coding scheme for real-valued local and global deep features with intra-frame lossy coding and inter-frame reference coding. The theoretical analysis is performed to understand how the number of inliers varies with the number of local features. Moreover, the inter-feature correlations are exploited in our framework. That is, local feature coding can be accelerated by making use of the frame types determined with global features, while the lossy global features aggregated with the decoded local features can be used as a reference for global feature coding. Extensive experimental results under three metrics show that our DFJC framework can significantly reduce the bitrate of local and global deep features from videos while maintaining the retrieval performance.																	1057-7149	1941-0042					2020	29						3734	3749		10.1109/TIP.2020.2965306													
J								Moment Retrieval via Cross-Modal Interaction Networks With Query Reconstruction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Moment retrieval; syntactic GCN; multi-head self-attention; multi-stage cross-modal interaction; query reconstruction		Moment retrieval aims to localize the most relevant moment in an untrimmed video according to the given natural language query. Existing works often only focus on one aspect of this emerging task, such as the query representation learning, video context modeling or multi-modal fusion, thus fail to develop a comprehensive system for further performance improvement. In this paper, we introduce a novel Cross-Modal Interaction Network (CMIN) to consider multiple crucial factors for this challenging task, including the syntactic dependencies of natural language queries, long-range semantic dependencies in video context and the sufficient cross-modal interaction. Specifically, we devise a syntactic GCN to leverage the syntactic structure of queries for fine-grained representation learning and propose a multi-head self-attention to capture long-range semantic dependencies from video context. Next, we employ a multi-stage cross-modal interaction to explore the potential relations of video and query contents, and we also consider query reconstruction from the cross-modal representations of target moment as an auxiliary task to strengthen the cross-modal representations. The extensive experiments on ActivityNet Captions and TACoS demonstrate the effectiveness of our proposed method.																	1057-7149	1941-0042					2020	29						3750	3762		10.1109/TIP.2020.2965987													
J								Reverse Attention-Based Residual Network for Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Salient object detection; reverse attention; side-output residual learning; saliency prior	MODEL	Benefiting from the quick development of deep convolutional neural networks, especially fully convolutional neural networks (FCNs), remarkable progresses have been achieved on salient object detection recently. Nevertheless, these FCNs based methods are still challenging to generate high resolution saliency maps, and also not applicable for subsequent applications due to their heavy model weights. In this paper, we propose a compact and efficient deep network with high accuracy for salient object detection. Firstly, we propose two strategies for initial prediction, one is a new designed multi-scale context module, the other is incorporating hand-crafted saliency priors. Secondly, we employ residual learning to refine it progressively by only learning the residual in each side-output, which can be achieved with few convolutional parameters, therefore leads to high compactness and high efficiency. Finally, we further design a novel top-down reverse attention block to guide the above side-output residual learning. Specifically, the current predicted salient regions are used to erase its side-output feature, thus the missing object parts and details can be efficiently learned from these unerased regions, which results in more complete detection and high accuracy. Extensive experimental results on seven benchmark datasets demonstrate that the proposed network performs favorably against the state-of-the-art approaches, and shows advantages in simplicity, compactness and efficiency.																	1057-7149	1941-0042					2020	29						3763	3776		10.1109/TIP.2020.2965989													
J								Satisfied-User-Ratio Modeling for Compressed Video	IEEE TRANSACTIONS ON IMAGE PROCESSING										Satisfied-user-ratio; just-noticeable difference; video quality; rate estimation; visual masking	IMAGE QUALITY ASSESSMENT	With explosive increase of internet video services, perceptual modeling for video quality has attracted more attentions to provide high quality-of-experience (QoE) for end-users subject to bandwidth constraints, especially for compressed video quality. In this paper, a novel perceptual model for satisfied-user-ratio (SUR) on compressed video quality is proposed by exploiting compressed video bitrate changes and spatial-temporal statistical characteristics extracted from both uncompressed original video and reference video. In the proposed method, an efficient video feature set is explored and established to model SUR curves against bitrate variations by leveraging the Gaussian Processes Regression (GPR) framework. In particular, the proposed model is based on the recently released large-scale video quality dataset, VideoSet, and takes both spatial and temporal masking effects into consideration. To make it more practical, we further optimize the proposed method from three aspects including feature source simplification, computation complexity reduction and video codec adaption. Based on experimental results on VideoSet, the proposed method can accurately model SUR curves for various video contents and predict their required bitrates at given SUR values. Subjective experiments are conducted to further verify the generalization ability of the proposed SUR model.																	1057-7149	1941-0042					2020	29						3777	3789		10.1109/TIP.2020.2965994													
J								A Metric for Light Field Reconstruction, Compression, and Display Quality Evaluation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Light field imaging; light field processing; quality assessment; quality metric; spatial quality; angular quality	SIMILARITY; IMAGES; INDEX	Owning to the recorded light ray distributions, light field contains much richer information and provides possibilities of some enlightening applications, and it has becoming more and more popular. To facilitate the relevant applications, many light field processing techniques have been proposed recently. These operations also bring the loss of visual quality, and thus there is need of a light field quality metric to quantify the visual quality loss. To reduce the processing complexity and resource consumption, light fields are generally sparsely sampled, compressed, and finally reconstructed and displayed to the users. We consider the distortions introduced in this typical light field processing chain, and propose a full-reference light field quality metric. Specifically, we measure the light field quality from three aspects: global spatial quality based on view structure matching, local spatial quality based on near-edge mean square error, and angular quality based on multi-view quality analysis. These three aspects have captured the most common distortions introduced in light field processing, including global distortions like blur and blocking, local geometric distortions like ghosting and stretching, and angular distortions like flickering and sampling. Experimental results show that the proposed method can estimate light field quality accurately, and it outperforms the state-of-the-art quality metrics which may be effective for light field.																	1057-7149	1941-0042					2020	29						3790	3804		10.1109/TIP.2020.2966081													
J								A Multimodal Saliency Model for Videos With High Audio-Visual Correspondence	IEEE TRANSACTIONS ON IMAGE PROCESSING										Audio-visual attention; visual attention; multimodal; saliency; attention fusion	FREE-ENERGY PRINCIPLE; BLIND QUALITY ASSESSMENT; SEGMENTATION; ATTENTION	Audio information has been bypassed by most of current visual attention prediction studies. However, sound could have influence on visual attention and such influence has been widely investigated and proofed by many psychological studies. In this paper, we propose a novel multi-modal saliency (MMS) model for videos containing scenes with high audio-visual correspondence. In such scenes, humans tend to be attracted by the sound sources and it is also possible to localize the sound sources via cross-modal analysis. Specifically, we first detect the spatial and temporal saliency maps from the visual modality by using a novel free energy principle. Then we propose to detect the audio saliency map from both audio and visual modalities by localizing the moving-sounding objects using cross-modal kernel canonical correlation analysis, which is first of its kind in the literature. Finally we propose a new two-stage adaptive audiovisual saliency fusion method to integrate the spatial, temporal and audio saliency maps to our audio-visual saliency map. The proposed MMS model has captured the influence of audio, which is not considered in the latest deep learning based saliency models. To take advantages of both deep saliency modeling and audio-visual saliency modeling, we propose to combine deep saliency models and the MMS model via a later fusion, and we find that an average of 5 performance gain is obtained. Experimental results on audio-visual attention databases show that the introduced models incorporating audio cues have significant superiority over state-of-the-art image and video saliency models which utilize a single visual modality.																	1057-7149	1941-0042					2020	29						3805	3819		10.1109/TIP.2020.2966082													
J								Graininess-Aware Deep Feature Learning for Robust Pedestrian Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Pedestrian detection; attention; deep learning; graininess	OCCLUSION; NETWORK	In this paper, we propose a graininess-aware deep feature learning method for pedestrian detection. Unlike most existing methods which utilize the convolutional features without explicit distinction, we appropriately exploit multiple convolutional layers and dynamically select most informative features. Specifically, we train a multi-scale pedestrian attention via pixel-wise segmentation supervision to efficiently identify the pedestrian of particular scales. We encodes the fine-grained attention map into the feature maps of the detection layers to guide them to highlight the pedestrians of specific scale and avoid the background interference. The graininess-aware feature maps generated with our attention mechanism are more focused on pedestrians, and in particular on the small-scale and occluded targets. We further introduce a zoom-in-zoom-out module to enhances the features by incorporating local details and context information. Extensive experimental results on five challenging pedestrian detection benchmarks show that our method achieves very competitive or even better performance with the state-of-the-arts and is faster than most existing approaches.																	1057-7149	1941-0042					2020	29						3820	3834		10.1109/TIP.2020.2966371													
J								View-Invariant Deep Architecture for Human Action Recognition Using Two-Stream Motion and Shape Temporal Dynamics	IEEE TRANSACTIONS ON IMAGE PROCESSING										Human action recognition; spatial temporal dynamics; human pose model; late fusion		Human action Recognition for unknown views, is a challenging task. We propose a deep view-invariant human action recognition framework, which is a novel integration of two important action cues: motion and shape temporal dynamics (STD). The motion stream encapsulates the motion content of action as RGB Dynamic Images (RGB-DIs), which are generated by Approximate Rank Pooling (ARP) and processed by using fine-tuned InceptionV3 model. The STD stream learns long-term view-invariant shape dynamics of action using a sequence of LSTM and Bi-LSTM learning models. Human Pose Model (HPM) generates view-invariant features of structural similarity index matrix (SSIM) based key depth human pose frames. The final prediction of the action is made on the basis of three types of late fusion techniques i.e. maximum (max), average (avg) and multiply (mul), applied on individual stream scores. To validate the performance of the proposed novel framework, the experiments are performed using both cross-subject and cross-view validation schemes on three publically available benchmarks-NUCLA multi-view dataset, UWA3D-II Activity dataset and NTU RGB-D Activity dataset. Our algorithm outperforms existing state-of-the-arts significantly, which is measured in terms of recognition accuracy, receiver operating characteristic (ROC) curve and area under the curve (AUC).																	1057-7149	1941-0042					2020	29						3835	3844		10.1109/TIP.2020.2965299													
J								Unsupervised Deep Image Fusion With Structure Tensor Representations	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image fusion; image contrast; structure tensor; convolutional neural network; and unsupervised learning	PERFORMANCE; GRADIENT; INFORMATION; FRAMEWORK; CURVELET; NETWORK	Convolutional neural networks (CNNs) have facilitated substantial progress on various problems in computer vision and image processing. However, applying them to image fusion has remained challenging due to the lack of the labelled data for supervised learning. This paper introduces a deep image fusion network (DIF-Net), an unsupervised deep learning framework for image fusion. The DIF-Net parameterizes the entire processes of image fusion, comprising of feature extraction, feature fusion, and image reconstruction, using a CNN. The purpose of DIF-Net is to generate an output image which has an identical contrast to high-dimensional input images. To realize this, we propose an unsupervised loss function using the structure tensor representation of the multi-channel image contrasts. Different from traditional fusion methods that involve time-consuming optimization or iterative procedures to obtain the results, our loss function is minimized by a stochastic deep learning solver with large-scale examples. Consequently, the proposed method can produce fused images that preserve source image details through a single forward network trained without reference ground-truth labels. The proposed method has broad applicability to various image fusion problems, including multi-spectral, multi-focus, and multi-exposure image fusions. Quantitative and qualitative evaluations show that the proposed technique outperforms existing state-of-the-art approaches for various applications.																	1057-7149	1941-0042					2020	29						3845	3858		10.1109/TIP.2020.2966075													
J								Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning	JOURNAL OF MACHINE LEARNING RESEARCH											HESSIAN EIGENMAPS; LOCALIZATION; CONVERGENCE; CONSISTENCY; LAPLACIAN; MATRICES; GRAPH	One of the common tasks in unsupervised learning is dimensionality reduction, where the goal is to find meaningful low-dimensional structures hidden in high-dimensional data. Sometimes referred to as manifold learning, this problem is closely related to the problem of localization, which aims at embedding a weighted graph into a low-dimensional Euclidean space. Several methods have been proposed for localization, and also manifold learning. Nonetheless, the robustness property of most of them is little understood. In this paper, we obtain perturbation bounds for classical scaling and trilateration, which are then applied to derive performance bounds for Isomap, Landmark Isomap, and Maximum Variance Unfolding. A new perturbation bound for procrustes analysis plays a key role.																	1532-4435						2020	21																						
J								Practical Locally Private Heavy Hitters	JOURNAL OF MACHINE LEARNING RESEARCH										Differential privacy; local differential privacy; heavy hitters; histograms; sketching		We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time - TreeHist and Bitstogram. In both algorithms, server running time is (O) over tilde (n) and user running time is (O) over tilde (1), hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring O(n(5/2)) server time and O(n(3/2)) user time. With a typically large number of participants in local algorithms (n in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.																	1532-4435						2020	21																						
J								On Mahalanobis Distance in Functional Settings	JOURNAL OF MACHINE LEARNING RESEARCH										Functional data; Mahalanobis distance; reproducing kernel Hilbert spaces; kernel methods in statistics; square root operator	OUTLIER	Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.																	1532-4435						2020	21																						
J								Lower Bounds for Testing Graphical Models: Colorings and Antiferromagnetic Ising Models	JOURNAL OF MACHINE LEARNING RESEARCH										distribution testing; structure learning; graphical models; Ising model; colorings	COMPLEXITY; ALGORITHMS; HARDNESS; FIELDS	We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model M and a sampling oracle for the distribution mu(M*) of an unknown model M*, can we efficiently determine if the two models M and M* are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the Ising model and proper colorings, and explore whether identity testing is any easier than structure learning. For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018) presented a polynomial time algorithm for identity testing. We prove hardness results in the anti-ferromagnetic (repulsive) setting in the same regime of parameters where structure learning is known to require a super-polynomial number of samples. Specifically, for n-vertex graphs of maximum degree d, we prove that if vertical bar beta vertical bar d = omega (log n) (where beta is the inverse temperature parameter), then there is no polynomial running time identity testing algorithm unless RP = NP. In the hard-constraint setting, we present hardness results for identity testing for proper colorings. Our results are based on the presumed hardness of #BIS, the problem of (approximately) counting independent sets in bipartite graphs.																	1532-4435						2020	21																						
J								On l(p)-Support Vector Machines and Multidimensional Kernels	JOURNAL OF MACHINE LEARNING RESEARCH										Support Vector Machines; Kernel functions; l(p)-norms; Mathematical Optimization	SUPPORT; TENSOR; CLASSIFICATION; PREDICTION; NORMS; SET	In this paper, we extend the methodology developed for Support Vector Machines (SVM) using the l(2)-norm (l(2)-SVM) to the more general case of l(p)-norms with p > 1 (l(p)-SVM). We derive second order cone formulations for the resulting dual and primal problems. The concept of kernel function, widely applied in l(2)-SVM, is extended to the more general case of l(p)-norms with p > 1 by defining a new operator called multidimensional kernel. This object gives rise to reformulations of dual problems, in a transformed space of the original data, where the dependence on the original data always appear as homogeneous polynomials. We adapt known solution algorithms to efficiently solve the primal and dual resulting problems and some computational experiments on real-world datasets are presented showing rather good behavior in terms of the accuracy of l(p)-SVM with p > 1.																	1532-4435						2020	21																						
J								Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information	JOURNAL OF MACHINE LEARNING RESEARCH										semi-supervised learning; general stochastic block models; misclassification; weighted message passing; minimum energy flow; statistical inference	COMMUNITY DETECTION; TREES; TIME	We study the misclassification error for community detection in general heterogeneous stochastic block models (SBM) with noisy or partial label information. We establish a connection between the misclassification rate and the notion of minimum energy on the local neighborhood of the SBM. We develop an optimally weighted message passing algorithm to reconstruct labels for SBM based on the minimum energy flow and the eigenvectors of a certain Markov transition matrix. The general SBM considered in this paper allows for unequal-size communities, degree heterogeneity, and different connection probabilities among blocks. We focus on how to optimally weigh the message passing to improve misclassification.																	1532-4435						2020	21																						
J								Online Sufficient Dimension Reduction Through Sliced Inverse Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Dimension reduction; online learning; perturbation; singular value decomposition; sliced inverse regression; gradient descent	STOCHASTIC-APPROXIMATION; PCA	Sliced inverse regression is an effective paradigm that achieves the goal of dimension reduction through replacing high dimensional covariates with a small number of linear combinations. It does not impose parametric assumptions on the dependence structure. More importantly, such a reduction of dimension is sufficient in that it does not cause loss of information. In this paper, we adapt the stationary sliced inverse regression to cope with the rapidly changing environments. We propose to implement sliced inverse regression in an online fashion. This online learner consists of two steps. In the first step we construct an online estimate for the kernel matrix; in the second step we propose two online algorithms, one is motivated by the perturbation method and the other is originated from the gradient descent optimization, to perform online singular value decomposition. The theoretical properties of this online learner are established. We demonstrate the numerical performance of this online learner through simulations and real world applications. All numerical studies confirm that this online learner performs as well as the batch learner.																	1532-4435						2020	21																						
J								DESlib: A Dynamic ensemble selection library in Python	JOURNAL OF MACHINE LEARNING RESEARCH										Multiple classifier systems; Ensemble of Classifiers; Dynamic classifier selection; Dynamic ensemble selection; Machine learning; Python	CLASSIFIER SELECTION; BASE CLASSIFIERS; COMPETENCE; DESIGN	DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) dcs, containing the implementation of dynamic classifier selection methods (DCS); (ii) des, containing the implementation of dynamic ensemble selection methods (DES); (iii) static, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page:																	1532-4435						2020	21																						
J								Lower Bounds for Parallel and Randomized Convex Optimization	JOURNAL OF MACHINE LEARNING RESEARCH										lower bounds; convex optimization; parallel algorithms; randomized algorithms; non-Euclidean optimization		We study the question of whether parallelization in the exploration of the feasible set can be used to speed up convex optimization, in the local oracle model of computation and in the high-dimensional regime. We show that the answer is negative for both deterministic and randomized algorithms applied to essentially any of the interesting geometries and nonsmooth, weakly-smooth, or smooth objective functions. In particular, we show that it is not possible to obtain a polylogarithmic (in the sequential complexity of the problem) number of parallel rounds with a polynomial (in the dimension) number of queries per round. In the majority of these settings and when the dimension of the space is polynomial in the inverse target accuracy, our lower bounds match the oracle complexity of sequential convex optimization, up to at most a logarithmic factor in the dimension, which makes them (nearly) tight. Another conceptual contribution of our work is in providing a general and streamlined framework for proving lower bounds in the setting of parallel convex optimization. Prior to our work, lower bounds for parallel convex optimization algorithms were only known in a small fraction of the settings considered in this paper, mainly applying to Euclidean (l(2)) and l(infinity) spaces.																	1532-4435						2020	21																						
J								A Statistical Learning Approach to Modal Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Nonparametric modal regression; empirical risk minimization; generalization bounds; kernel density estimation; statistical learning theory	ROBUST VARIABLE SELECTION; NONPARAMETRIC-ESTIMATION; DENSITY-FUNCTION; CORRENTROPY; PREDICTION; CLASSIFICATION; CONSISTENCY; ESTIMATORS; RATES; MODEL	This paper studies the nonparametric modal regression problem systematically from a statistical learning viewpoint. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that the nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its Bayes rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols such as mean regression, median regression, and quantile regression. On the practical side, the implementation issues of modal regression including the computational algorithm and the selection of the tuning parameters are discussed. Numerical validations on modal regression are also conducted to verify our findings.																	1532-4435						2020	21																						
J								Generalized probabilistic principal component analysis of correlated data	JOURNAL OF MACHINE LEARNING RESEARCH										Gaussian process; maximum marginal likelihood estimator; kernel method; principal component analysis; Stiefel manifold	GAUSSIAN STOCHASTIC-PROCESS; COMPUTER-MODELS; PROCESS EMULATION; KERNEL PCA; CALIBRATION; OPTIMIZATION; NUMBER	Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated. In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA) by providing the closed-form maximum marginal likelihood estimator of the factor loadings and other parameters. Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience. Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach.																	1532-4435						2020	21																						
J								Connecting Spectral Clustering to Maximum Margins and Level Sets	JOURNAL OF MACHINE LEARNING RESEARCH										spectral clustering; maximum margin clustering; density clustering; level sets; convergence; asymptotics; consistency	CONSISTENCY	We study the connections between spectral clustering and the problems of maximum margin clustering, and estimation of the components of level sets of a density function. Specifically, we obtain bounds on the eigenvectors of graph Laplacian matrices in terms of the between cluster separation, and within cluster connectivity. These bounds ensure that the spectral clustering solution converges to the maximum margin clustering solution as the scaling parameter is reduced towards zero. The sensitivity of maximum margin clustering solutions to outlying points is well known, but can be mitigated by first removing such outliers, and applying maximum margin clustering to the remaining points. If outliers are identified using an estimate of the underlying probability density, then the remaining points may be seen as an estimate of a level set of this density function. We show that such an approach can be used to consistently estimate the components of the level sets of a density function under very mild assumptions.																	1532-4435						2020	21																						
J								A Unified Framework for Structured Graph Learning via Spectral Constraints	JOURNAL OF MACHINE LEARNING RESEARCH										Structured graph learning; spectral graph theory; Markov random field; Gaussian graphical model; Laplacian matrix; clustering; adjacency matrix; bipartite structure; spectral similarity	INVERSE COVARIANCE ESTIMATION; ISOTONIC REGRESSION; MODEL SELECTION; MINIMIZATION METHODS; TOPOLOGY INFERENCE; SIGNAL; ALGORITHMS; MATRIX; LASSO; REGULARIZATION	Graph learning from data is a canonical problem that has received substantial attention in the literature. Learning a structured graph is essential for interpretability and identification of the relationships among data. In general, learning a graph with a specific structure is an NP-hard combinatorial problem and thus designing a general tractable algorithm is challenging. Some useful structured graphs include connected, sparse, multi-component, bipartite, and regular graphs. In this paper, we introduce a unified framework for structured graph learning that combines Gaussian graphical model and spectral graph theory. We propose to convert combinatorial structural constraints into spectral constraints on graph matrices and develop an optimization framework based on block majorization-minimization to solve structured graph learning problem. The proposed algorithms are provably convergent and practically amenable for a number of graph based applications such as data clustering. Extensive numerical experiments with both synthetic and real data sets illustrate the effectiveness of the proposed algorithms. An open source R package containing the code for all the experiments is available at https://CRAN.R-project.org/package=spectralGraphTopology																	1532-4435						2020	21																						
J								A model of fake data in data-driven analysis	JOURNAL OF MACHINE LEARNING RESEARCH										data-driven analysis; fake data; game theory; point process		Data-driven analysis has been increasingly used in various decision making processes. With more sources, including reviews, news, and pictures, can now be used for data analysis, the authenticity of data sources is in doubt. While previous literature attempted to detect fake data piece by piece, in the current work, we try to capture the fake data sender's strategic behavior to detect the fake data source. Specifically, we model the tension between a data receiver who makes data-driven decisions and a fake data sender who benefits from misleading the receiver. We propose a potentially infinite horizon continuous time game-theoretic model with asymmetric information to capture the fact that the receiver does not initially know the existence of fake data and learns about it during the course of the game. We use point processes to model the data traffic, where each piece of data can occur at any discrete moment in a continuous time flow. We fully solve the model and employ numerical examples to illustrate the players' strategies and payoffs for insights. Specifically, our results show that maintaining some suspicion about the data sources and understanding that the sender can be strategic are very helpful to the data receiver. In addition, based on our model, we propose a methodology of detecting fake data that is complementary to the previous studies on this topic, which suggested various approaches on analyzing the data piece by piece. We show that after analyzing each piece of data, understanding a source by looking at the its whole history of pushing data can be helpful.																	1532-4435						2020	21																						
J								Distributed Feature Screening via Componentwise Debiasing	JOURNAL OF MACHINE LEARNING RESEARCH										Feature screening; Big data; Divide-and-conquer; Aggregated correlation; Sure screening property	MODEL	Feature screening is a powerful tool in processing high-dimensional data. When the sample size N and the number of features p are both large, the implementation of classic screening methods can be numerically challenging. In this paper, we propose a distributed screening framework for big data setup. In the spirit of "divide-and-conquer", the proposed framework expresses a correlation measure as a function of several component parameters, each of which can be distributively estimated using a natural U-statistic from data segments. With the component estimates aggregated, we obtain a final correlation estimate that can be readily used for screening features. This framework enables distributed storage and parallel computing and thus is computationally attractive. Due to the unbiased distributive estimation of the component parameters, the final aggregated estimate achieves a high accuracy that is insensitive to the number of data segments m. Under mild conditions, we show that the aggregated correlation estimator is as efficient as the centralized estimator in terms of the probability convergence bound and the mean squared error rate; the corresponding screening procedure enjoys sure screening property for a wide range of correlation measures. The promising performances of the new method are supported by extensive numerical examples.																	1532-4435						2020	21																						
J								Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections	JOURNAL OF MACHINE LEARNING RESEARCH										kernel methods; regularized algorithms; stochastic gradient methods; random projection; sketching	REGRESSION; INEQUALITIES; BOUNDS	We study the least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space as a special case. We first investigate regularized algorithms adapted to a projection operator on a closed subspace of the Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function. As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nystrom regularized algorithms. Our results provide optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nystrom regularized algorithms, considering both the attainable and non-attainable cases, in the well-conditioned regimes. We then study stochastic gradient methods with projection over the subspace, allowing multi-pass over the data and minibatches, and we derive similar optimal statistical convergence results.																	1532-4435						2020	21																						
J								Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms	JOURNAL OF MACHINE LEARNING RESEARCH										unsupervised learning; spectral clustering; manifold learning; fast algorithms; shortest path distance	MAXIMUM CAPACITY; VERTEX DEGREE; LONGEST EDGE; GRAPH; SEARCH	We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of high-dimensional outliers. By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets, and prove guarantees on the labeling accuracy of the proposed algorithm. Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric. We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points.																	1532-4435						2020	21																						
J								Universal Latent Space Model Fitting for Large Networks with Edge Covariates	JOURNAL OF MACHINE LEARNING RESEARCH										community detection; network with covariates; non-convex optimization; projected gradient descent	COMMUNITY DETECTION; MATRIX COMPLETION; VERTEX CLASSIFICATION; RECOVERY; RECONSTRUCTION; ROBUST; RATES	Latent space models are effective tools for statistical modeling and visualization of network data. Due to their close connection to generalized linear models, it is also natural to incorporate covariate information in them. The current paper presents two universal fitting algorithms for networks with edge covariates: one based on nuclear norm penalization and the other based on projected gradient descent. Both algorithms are motivated by maximizing the likelihood function for an existing class of inner-product models, and we establish their statistical rates of convergence for these models. In addition, the theory informs us that both methods work simultaneously for a wide range of different latent space models that allow latent positions to affect edge formation in flexible ways, such as distance models. Furthermore, the effectiveness of the methods is demonstrated on a number of real world network data sets for different statistical tasks, including community detection with and without edge covariates, and network assisted learning.																	1532-4435						2020	21																						
J								Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems	JOURNAL OF MACHINE LEARNING RESEARCH										Derivative-Free Optimization; Linear Quadratic Control; Non-Convex Optimization	TAIL PROBABILITIES; FORMS	We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of these methods when applied to linear-quadratic systems, and study various settings of driving noise and reward feedback. Our main theoretical result provides an explicit bound on the sample or evaluation complexity: we show that these methods are guaranteed to converge to within any pre-specified tolerance of the optimal policy with a number of zero-order evaluations that is an explicit polynomial of the error tolerance, dimension, and curvature properties of the problem. Our analysis reveals some interesting differences between the settings of additive driving noise and random initialization, as well as the settings of one-point and two-point reward feedback. Our theory is corroborated by simulations of derivative-free methods in application to these systems. Along the way, we derive convergence rates for stochastic zero-order optimization algorithms when applied to a certain class of non-convex problems.																	1532-4435						2020	21																						
J								Target Propagation in Recurrent Neural Networks	JOURNAL OF MACHINE LEARNING RESEARCH										recurrent neural networks; target propagation; biological plausibility	ORGANIZATION; PROJECTIONS; AREA-V2	Recurrent Neural Networks have been widely used to process sequence data, but have long been criticized for their biological implausibility and training difficulties related to vanishing and exploding gradients. This paper presents a novel algorithm for training recurrent networks, target propagation through time (TPTT), that outperforms standard backpropagation through time (BPTT) on four out of the five problems used for testing. The proposed algorithm is initially tested and compared to BPTT on four synthetic time lag tasks, and its performance is also measured using the sequential MNIST data set. In addition, as TPTT uses target propagation, it allows for discrete nonlinearities and could potentially mitigate the credit assignment problem in more complex recurrent architectures.																	1532-4435						2020	21																						
J								High-Dimensional Interactions Detection with Sparse Principal Hessian Matrix	JOURNAL OF MACHINE LEARNING RESEARCH										Interaction Detection; Principal Hessian Matrix; ADMM; Sparse M-Estimator	ALTERNATING DIRECTION METHOD; VARIABLE SELECTION; REGRESSION; MULTIPLIERS	In statistical learning framework with regressions, interactions are the contributions to the response variable from the products of the explanatory variables. In high-dimensional problems, detecting interactions is challenging due to combinatorial complexity and limited data information. We consider detecting interactions by exploring their connections with the principal Hessian matrix. Specifically, we propose a one-step synthetic approach for estimating the principal Hessian matrix by a penalized M-estimator. An alternating direction method of multipliers (ADMM) is proposed to efficiently solve the encountered regularized optimization problem. Based on the sparse estimator, we detect the interactions by identifying its nonzero components. Our method directly targets at the interactions, and it requires no structural assumption on the hierarchy of the interactions effects. We show that our estimator is theoretically valid, computationally efficient, and practically useful for detecting the interactions in a broad spectrum of scenarios.																	1532-4435						2020	21																						
J								Neyman-Pearson classification: parametrics and sample size requirement	JOURNAL OF MACHINE LEARNING RESEARCH										classification; asymmetric error; Neyman-Pearson (NP) paradigm; NP oracle inequalities; minimum sample size requirement; linear discriminant analysis (LDA); NP umbrella algorithm; adaptive splitting	DISCRIMINANT-ANALYSIS; MICROARRAY DATA; CANCER	The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level alpha. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong et al. (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class 0 observation as class 1 under the 0-1 coding) upper bound alpha with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class 0, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class 0 observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class 0 observations, we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers. The proposed NP classifiers are implemented in the R package nproc.																	1532-4435						2020	21																						
J								Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian computation; data partitioning; expectation propagation; hierarchical models; statistical computing	PRECISION MATRIX; REGRESSION; MODELS; CLASSIFICATION; LIKELIHOOD	A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.																	1532-4435						2020	21																						
J								A Low Complexity Algorithm with O(root T) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints	JOURNAL OF MACHINE LEARNING RESEARCH										online convex optimization; long term constraints; regret bounds; constraint violation bounds; low complexity		This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve O(root T) regret and O(T-3/4) constraint violations for general problems and another algorithm to achieve an O(T-2/3) bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve O(T-max{theta,T-1-theta}) regret and O (T1-theta/2) constraint violations where theta is an element of(0, 1). The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an O (root T) regret bound with O(1) constraint violations.																	1532-4435						2020	21																						
J								On the Use of Biased-Randomized Algorithms for Solving Non-Smooth Optimization Problems	ALGORITHMS										non-smooth optimization; biased-randomized algorithms; heuristics; soft constraints	VEHICLE-ROUTING PROBLEM; KEY GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; OPTIMAL POWER-FLOW; FACILITY-LOCATION; MULTI-DEPOT; MANAGEMENT; DISPATCH; SEARCH	Soft constraints are quite common in real-life applications. For example, in freight transportation, the fleet size can be enlarged by outsourcing part of the distribution service and some deliveries to customers can be postponed as well; in inventory management, it is possible to consider stock-outs generated by unexpected demands; and in manufacturing processes and project management, it is frequent that some deadlines cannot be met due to delays in critical steps of the supply chain. However, capacity-, size-, and time-related limitations are included in many optimization problems as hard constraints, while it would be usually more realistic to consider them as soft ones, i.e., they can be violated to some extent by incurring a penalty cost. Most of the times, this penalty cost will be nonlinear and even noncontinuous, which might transform the objective function into a non-smooth one. Despite its many practical applications, non-smooth optimization problems are quite challenging, especially when the underlying optimization problem is NP-hard in nature. In this paper, we propose the use of biased-randomized algorithms as an effective methodology to cope with NP-hard and non-smooth optimization problems in many practical applications. Biased-randomized algorithms extend constructive heuristics by introducing a nonuniform randomization pattern into them. Hence, they can be used to explore promising areas of the solution space without the limitations of gradient-based approaches, which assume the existence of smooth objective functions. Moreover, biased-randomized algorithms can be easily parallelized, thus employing short computing times while exploring a large number of promising regions. This paper discusses these concepts in detail, reviews existing work in different application areas, and highlights current trends and open research lines.																		1999-4893				JAN	2020	13	1							8	10.3390/a13010008													
J								Optimal Prefix Free Codes with Partial Sorting dagger	ALGORITHMS										deferred data structure; Huffman; median; optimal prefix free codes; partial sum; van Leeuwen	CONSTRUCTION; IMPLEMENTATION	We describe an algorithm computing an optimal prefix free code for n unsorted positive weights in time within O(n(1+lg alpha))subset of O(nlgn), where the alternation alpha is an element of[1..n-1] approximates the minimal amount of sorting required by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision tree computational model, in the worst case over all instances of size n and alternation alpha. Such results refine the state of the art complexity of Theta(nlgn) in the worst case over instances of size n in the same computational model, a landmark in compression and coding since 1952. Beside the new analysis technique, such improvement is obtained by combining a new algorithm, inspired by van Leeuwen's algorithm to compute optimal prefix free codes from sorted weights (known since 1976), with a relatively minor extension of Karp et al.'s deferred data structure to partially sort a multiset accordingly to the queries performed on it (known since 1988). Preliminary experimental results on text compression by words show alpha to be polynomially smaller than n, which suggests improvements by at most a constant multiplicative factor in the running time for such applications.																		1999-4893				JAN	2020	13	1							12	10.3390/a13010012													
J								An Effective and Efficient Genetic-Fuzzy Algorithm for Supporting Advanced Human-Machine Interfaces in Big Data Settings	ALGORITHMS										genetic optimization; fuzzy algorithms; advanced human-machine interfaces; humanoid robotics	MIRROR NEURONS; SPEECH; DIFFERENTIATION	In this paper we describe a novel algorithm, inspired by the mirror neuron discovery, to support automatic learning oriented to advanced man-machine interfaces. The algorithm introduces several points of innovation, based on complex metrics of similarity that involve different characteristics of the entire learning process. In more detail, the proposed approach deals with an humanoid robot algorithm suited for automatic vocalization acquisition from a human tutor. The learned vocalization can be used to multi-modal reproduction of speech, as the articulatory and acoustic parameters that compose the vocalization database can be used to synthesize unrestricted speech utterances and reproduce the articulatory and facial movements of the humanoid talking face automatically synchronized. The algorithm uses fuzzy articulatory rules, which describe transitions between phonemes derived from the International Phonetic Alphabet (IPA), to allow simpler adaptation to different languages, and genetic optimization of the membership degrees. Large experimental evaluation and analysis of the proposed algorithm on synthetic and real data sets confirms the benefits of our proposal. Indeed, experimental results show that the vocalization acquired respects the basic phonetic rules of Italian languages and that subjective results show the effectiveness of multi-modal speech production with automatic synchronization between facial movements and speech emissions. The algorithm has been applied to a virtual speaking face but it may also be used in mechanical vocalization systems as well.																		1999-4893				JAN	2020	13	1							13	10.3390/a13010013													
J								A Matheuristic for Joint Optimal Power and Scheduling Assignment in DVB-T2 Networks dagger	ALGORITHMS										telecommunications; DVB-T; antenna design; network optimization; mixed integer linear programming; tight linear relaxations; matheuristics; ILP heuristics	BASE STATION LOCATION; GENETIC ALGORITHMS; OPTIMIZATION; DESIGN; MODEL	Because of the introduction and spread of the second generation of the Digital Video Broadcasting-Terrestrial standard (DVB-T2), already active television broadcasters and new broadcasters that have entered in the market will be required to (re)design their networks. This is generating a new interest for effective and efficient DVB optimization software tools. In this work, we propose a strengthened binary linear programming model for representing the optimal DVB design problem, including power and scheduling configuration, and propose a new matheuristic for its solution. The matheuristic combines a genetic algorithm, adopted to efficiently explore the solution space of power emissions of DVB stations, with relaxation-guided variable fixing and exact large neighborhood searches formulated as integer linear programming (ILP) problems solved exactly. Computational tests on realistic instances show that the new matheuristic performs much better than a state-of-the-art optimization solver, identifying solutions associated with much higher user coverage.																		1999-4893				JAN	2020	13	1							27	10.3390/a13010027													
J								Journey Planning Algorithms for Massive Delay-Prone Transit Networks dagger	ALGORITHMS										journey planning; transit networks; dynamic graph algorithms; algorithms engineering; massive datasets; experimental algorithmics	DISTANCE QUERIES	This paper studies the journey planning problem in the context of transit networks. Given the timetable of a schedule-based transportation system (consisting, e.g., of trains, buses, etc.), the problem seeks journeys optimizing some criteria. Specifically, it seeks to answer natural queries such as, for example, "find a journey starting from a source stop and arriving at a target stop as early as possible". The fastest approach for answering to these queries, yielding the smallest average query time even on very large networks, is the Public Transit Labeling framework, proposed for the first time in Delling et al., SEA 2015. This method combines three main ingredients: (i) a graph-based representation of the schedule of the transit network; (ii) a labeling of such graph encoding its transitive closure (computed via a time-consuming pre-processing); (iii) an efficient query algorithm exploiting both (i) and (ii) to answer quickly to queries of interest at runtime. Unfortunately, while transit networks' timetables are inherently dynamic (they are often subject to delays or disruptions), ptl is not natively designed to handle updates in the schedule-even after a single change, precomputed data may become outdated and queries can return incorrect results. This is a major limitation, especially when dealing with massively sized inputs (e.g., metropolitan or continental sized networks), as recomputing the labeling from scratch, after each change, yields unsustainable time overheads that are not compatible with interactive applications. In this work, we introduce a new framework that extends ptl to function in delay-prone transit networks. In particular, we provide a new set of algorithms able to update both the graph and the precomputed labeling whenever a delay affects the network, without performing any recomputation from scratch. We demonstrate the effectiveness of our solution through an extensive experimental evaluation conducted on real-world networks. Our experiments show that: (i) the update time required by the new algorithms is, on average, orders of magnitude smaller than that required by the recomputation from scratch via ptl; (ii) the updated graph and labeling induce both query time performance and space overhead that are equivalent to those that are obtained by the recomputation from scratch via ptl. This suggests that our new solution is an effective approach to handling the journey planning problem in delay-prone transit networks.																		1999-4893				JAN	2020	13	1							2	10.3390/a13010002													
J								A Numerical Approach for the Filtered Generalized Cech Complex	ALGORITHMS										disk system; generalized Cech complex; Cech scale; generalized Vietoris-Rips Lemma; miniball problem	PERSISTENT; TOPOLOGY; SPHERE	In this paper, we present an algorithm to compute the filtered generalized Cech complex for a finite collection of disks in the plane, which do not necessarily have the same radius. The key step behind the algorithm is to calculate the minimum scale factor needed to ensure rescaled disks have a nonempty intersection, through a numerical approach, whose convergence is guaranteed by a generalization of the well-known Vietoris-Rips Lemma, which we also prove in an alternative way, using elementary geometric arguments. We give an algorithm for computing the 2-dimensional filtered generalized Cech complex of a finite collection of d-dimensional disks in Rd, and we show the performance of our algorithm.																		1999-4893				JAN	2020	13	1							11	10.3390/a13010011													
J								Image Completion with Large or Edge-Missing Areas	ALGORITHMS										image completion; generative adversarial networks; large missing area; edge-missing area		Existing image completion methods are mostly based on missing regions that are small or located in the middle of the images. When regions to be completed are large or near the edge of the images, due to the lack of context information, the completion results tend to be blurred or distorted, and there will be a large blank area in the final results. In addition, the unstable training of the generative adversarial network is also prone to cause pseudo-color in the completion results. Aiming at the two above-mentioned problems, a method of image completion with large or edge-missing areas is proposed; also, the network structures have been improved. On the one hand, it overcomes the problem of lacking context information, which thereby ensures the reality of generated texture details; on the other hand, it suppresses the generation of pseudo-color, which guarantees the consistency of the whole image both in vision and content. The experimental results show that the proposed method achieves better completion results in completing large or edge-missing areas.																		1999-4893				JAN	2020	13	1							14	10.3390/a13010014													
J								A Soft-Voting Ensemble Based Co-Training Scheme Using Static Selection for Binary Classification Problems	ALGORITHMS										binary classification; co-training; ensemble methods; feature views; dynamic ensemble selection; Soft-Voting	REGRESSION; PERFORMANCE; STRATEGIES; FUSION	In recent years, a forward-looking subfield of machine learning has emerged with important applications in a variety of scientific fields. Semi-supervised learning is increasingly being recognized as a burgeoning area embracing a plethora of efficient methods and algorithms seeking to exploit a small pool of labeled examples together with a large pool of unlabeled ones in the most efficient way. Co-training is a representative semi-supervised classification algorithm originally based on the assumption that each example can be described by two distinct feature sets, usually referred to as views. Since such an assumption can hardly be met in real world problems, several variants of the co-training algorithm have been proposed dealing with the absence or existence of a naturally two-view feature split. In this context, a Static Selection Ensemble-based co-training scheme operating under a random feature split strategy is outlined regarding binary classification problems, where the type of the base ensemble learner is a soft-Voting one composed of two participants. Ensemble methods are commonly used to boost the predictive performance of learning models by using a set of different classifiers, while the Static Ensemble Selection approach seeks to find the most suitable structure of ensemble classifier based on a specific criterion through a pool of candidate classifiers. The efficacy of the proposed scheme is verified through several experiments on a plethora of benchmark datasets as statistically confirmed by the Friedman Aligned Ranks non-parametric test over the behavior of classification accuracy, F-1-score, and Area Under Curve metrics.																		1999-4893				JAN	2020	13	1							26	10.3390/a13010026													
J								Unstructured Uncertainty Based Modeling and Robust Stability Analysis of Textile-Reinforced Composites with Embedded Shape Memory Alloys	ALGORITHMS										textile reinforced composite; shape memory alloy; robust stability	ACTUATORS	This paper develops the mathematical modeling and deflection control of a textile-reinforced composite integrated with shape memory actuators. The mathematical model of the system is derived using the identification method and an unstructured uncertainty approach. Based on this model and a robust stability analysis, a robust proportional-integral controller is designed for controlling the deflection of the composite. We showed that the robust controller depends significantly on the modeling of the uncertainty. The performance of the proposed controller is compared with a classical one through experimental analysis. Experimental results show that the proposed controller has a better performance as it reduces the overshoot and provide robustness to uncertainty. Due to the robust design, the controller also has a wide operating range, which is advantageous for practical applications.																		1999-4893				JAN	2020	13	1							24	10.3390/a13010024													
J								Image Restoration Using a Fixed-Point Method for a TVL2 Regularization Problem	ALGORITHMS										image restoration; total variation; fixed-point method; non-expansive operator; proximity operator	CONVERGENCE; ALGORITHMS; RECOVERY	In this paper, we first propose a new TVL2 regularization model for image restoration, and then we propose two iterative methods, which are fixed-point and fixed-point-like methods, using CGLS (Conjugate Gradient Least Squares method) for solving the new proposed TVL2 problem. We also provide convergence analysis for the fixed-point method. Lastly, numerical experiments for several test problems are provided to evaluate the effectiveness of the proposed two iterative methods. Numerical results show that the new proposed TVL2 model is preferred over an existing TVL2 model and the proposed fixed-point-like method is well suited for the new TVL2 model.																		1999-4893				JAN	2020	13	1							1	10.3390/a13010001													
J								Non Data-Aided SNR Estimation for UAV OFDM Systems	ALGORITHMS										unmanned aerial vehicle; orthogonal frequency division multiplexing; signal-to-noise ratio; parameter estimation; non data-aided	CHANNEL ESTIMATION; NOISE VARIANCE; NETWORKS	Signal-to-noise ratio (SNR) estimation is essential in the unmanned aerial vehicle (UAV) orthogonal frequency division multiplexing (OFDM) system for getting accurate channel estimation. In this paper, we propose a novel non-data-aided (NDA) SNR estimation method for UAV OFDM system to overcome the carrier interference caused by the frequency offset. First, an absolute value series is achieved which is based on the sampled received sequence, where each sampling point is validated by the data length apart. Second, by dividing absolute value series into the different series according to the total length of symbol, we obtain an output series by stacking each part. Third, the root mean squares of noise power and total power are estimated by utilizing the maximum and minimum platform in the characteristic curve of the output series after the wavelet denoising. Simulation results show that the proposed method performs better than other methods, especially in the low synchronization precision, and it has low computation complexity.																		1999-4893				JAN	2020	13	1							22	10.3390/a13010022													
J								Computing Persistent Homology of Directed Flag Complexes	ALGORITHMS										neural networks; topology; directed graphs; directed flag complexes; persistent homology; computational software		We present a new computing package Flagser, designed to construct the directed flag complex of a finite directed graph, and compute persistent homology for flexibly defined filtrations on the graph and the resulting complex. The persistent homology computation part of Flagser is based on the program Ripser by U. Bauer, but is optimised specifically for large computations. The construction of the directed flag complex is done in a way that allows easy parallelisation by arbitrarily many cores. Flagser also has the option of working with undirected graphs. For homology computations Flagser has an approximate option, which shortens compute time with remarkable accuracy. We demonstrate the power of Flagser by applying it to the construction of the directed flag complex of digital reconstructions of brain microcircuitry by the Blue Brain Project and several other examples. In some instances we perform computation of homology. For a more complete performance analysis, we also apply Flagser to some other data collections. In all cases the hardware used in the computation, the use of memory and the compute time are recorded.																		1999-4893				JAN	2020	13	1							19	10.3390/a13010019													
J								Parameterized Optimization in Uncertain Graphs-A Survey and Some Results	ALGORITHMS										probabilistic graphs; uncertain graphs; influence maximization; cascade failure; linear reliable ordering; expected coverage; probabilistic core	LINEAR-TIME ALGORITHM; FACILITY LOCATION; RELIABLE SOURCE; SHORTEST PATHS; NETWORK; COMPLEXITY; MODEL; TREE	We present a detailed survey of results and two new results on graphical models of uncertainty and associated optimization problems. We focus on two well-studied models, namely, the Random Failure (RF) model and the Linear Reliability Ordering (LRO) model. We present an FPT algorithm parameterized by the product of treewidth and max-degree for maximizing expected coverage in an uncertain graph under the RF model. We then consider the problem of finding the maximal core in a graph, which is known to be polynomial time solvable. We show that the Probabilistic-Core problem is polynomial time solvable in uncertain graphs under the LRO model. On the other hand, under the RF model, we show that the Probabilistic-Core problem is W[1]-hard for the parameter d, where d is the minimum degree of the core. We then design an FPT algorithm for the parameter treewidth.																		1999-4893				JAN	2020	13	1							3	10.3390/a13010003													
J								A Comparative Study of Four Metaheuristic Algorithms, AMOSA, MOABC, MSPSO, and NSGA-II for Evacuation Planning	ALGORITHMS										evacuation planning; multi-objective optimization; meta-heuristic algorithms; AMOSA; MOABC; MSPSO; NSGA-II	MULTIOBJECTIVE GENETIC ALGORITHM; LOCATION-ALLOCATION; EVOLUTIONARY OPTIMIZATION; MODEL; OPERATIONS; BERTH	Evacuation planning is an important activity in disaster management to reduce the effects of disasters on urban communities. It is regarded as a multi-objective optimization problem that involves conflicting spatial objectives and constraints in a decision-making process. Such problems are difficult to solve by traditional methods. However, metaheuristics methods have been shown to be proper solutions. Well-known classical metaheuristic algorithms-such as simulated annealing (SA), artificial bee colony (ABC), standard particle swarm optimization (SPSO), genetic algorithm (GA), and multi-objective versions of them-have been used in the spatial optimization domain. However, few types of research have applied these classical methods, and their performance has not always been well evaluated, specifically not on evacuation planning problems. This research applies the multi-objective versions of four classical metaheuristic algorithms (AMOSA, MOABC, NSGA-II, and MSPSO) on an urban evacuation problem in Rwanda in order to compare the performances of the four algorithms. The performances of the algorithms have been evaluated based on the effectiveness, efficiency, repeatability, and computational time of each algorithm. The results showed that in terms of effectiveness, AMOSA and MOABC achieve good quality solutions that satisfy the objective functions. NSGA-II and MSPSO showed third and fourth-best effectiveness. For efficiency, NSGA-II is the fastest algorithm in terms of execution time and convergence speed followed by AMOSA, MOABC, and MSPSO. AMOSA, MOABC, and MSPSO showed a high level of repeatability compared to NSGA-II. It seems that by modifying MOABC and increasing its effectiveness, it could be a proper algorithm for evacuation planning.																		1999-4893				JAN	2020	13	1							16	10.3390/a13010016													
J								Simple Constructive, Insertion, and Improvement Heuristics Based on the Girding Polygon for the Euclidean Traveling Salesman Problem	ALGORITHMS										heuristic algorithm; traveling salesman problem; computational experiment; time complexity	ANT COLONY OPTIMIZATION; ALGORITHM	The Traveling Salesman Problem (TSP) aims at finding the shortest trip for a salesman, who has to visit each of the locations from a given set exactly once, starting and ending at the same location. Here, we consider the Euclidean version of the problem, in which the locations are points in the two-dimensional Euclidean space and the distances are correspondingly Euclidean distances. We propose simple, fast, and easily implementable heuristics that work well, in practice, for large real-life problem instances. The algorithm works on three phases, the constructive, the insertion, and the improvement phases. The first two phases run in time O(n2) and the number of repetitions in the improvement phase, in practice, is bounded by a small constant. We have tested the practical behavior of our heuristics on the available benchmark problem instances. The approximation provided by our algorithm for the tested benchmark problem instances did not beat best known results. At the same time, comparing the CPU time used by our algorithm with that of the earlier known ones, in about 92% of the cases our algorithm has required less computational time. Our algorithm is also memory efficient: for the largest tested problem instance with 744,710 cities, it has used about 50 MiB, whereas the average memory usage for the remained 217 instances was 1.6 MiB.																		1999-4893				JAN	2020	13	1							5	10.3390/a13010005													
J								A Grey-Box Ensemble Model Exploiting Black-Box Accuracy and White-Box Intrinsic Interpretability	ALGORITHMS										explainable machine learning; interpretable machine learning; semi-supervised learning; self-training algorithms; ensemble learning; black; white and grey box models		Machine learning has emerged as a key factor in many technological and scientific advances and applications. Much research has been devoted to developing high performance machine learning models, which are able to make very accurate predictions and decisions on a wide range of applications. Nevertheless, we still seek to understand and explain how these models work and make decisions. Explainability and interpretability in machine learning is a significant issue, since in most of real-world problems it is considered essential to understand and explain the model's prediction mechanism in order to trust it and make decisions on critical issues. In this study, we developed a Grey-Box model based on semi-supervised methodology utilizing a self-training framework. The main objective of this work is the development of a both interpretable and accurate machine learning model, although this is a complex and challenging task. The proposed model was evaluated on a variety of real world datasets from the crucial application domains of education, finance and medicine. Our results demonstrate the efficiency of the proposed model performing comparable to a Black-Box and considerably outperforming single White-Box models, while at the same time remains as interpretable as a White-Box model.																		1999-4893				JAN	2020	13	1							17	10.3390/a13010017													
J								Comparative Analysis of Different Model-Based Controllers Using Active Vehicle Suspension System	ALGORITHMS										higher-order sliding mode control; quarter-car; super twisting algorithm; active suspension system	ORDER; OBSERVER; PENDULUM; ACTUATOR; SKYHOOK	This paper deals with the active vibration control of a quarter-vehicle suspension system. Damping control methods investigated in this paper are: higher-order sliding mode control (HOSMC) based on super twisting algorithm (STA), first-order sliding mode control (FOSMC), integral sliding mode control (ISMC), proportional integral derivative (PID), linear quadratic regulator (LQR) and passive suspension system. Performance comparison of different active controllers are analyzed in terms of vertical displacement, suspension travel and wheel deflection. The theoretical, quantitative and qualitative analysis verify that the STA-based HOSMC exhibits better performance as well as negate the undesired disturbances with respect to FOSMC, ISMC, PID, LQR and passive suspension system. Furthermore, it is also robust to intrinsic bounded uncertain dynamics of the model.																		1999-4893				JAN	2020	13	1							10	10.3390/a13010010													
J								Local Convergence of an Efficient Multipoint Iterative Method in Banach Space	ALGORITHMS										Banach space; divided difference; system of equations; order of convergence	SEMILOCAL CONVERGENCE; SOLVING SYSTEMS; ORDER	We discuss the local convergence of a derivative-free eighth order method in a Banach space setting. The present study provides the radius of convergence and bounds on errors under the hypothesis based on the first Frechet-derivative only. The approaches of using Taylor expansions, containing higher order derivatives, do not provide such estimates since the derivatives may be nonexistent or costly to compute. By using only first derivative, the method can be applied to a wider class of functions and hence its applications are expanded. Numerical experiments show that the present results are applicable to the cases wherein previous results cannot be applied.																		1999-4893				JAN	2020	13	1							25	10.3390/a13010025													
J								Optimal Learning and Self-Awareness Versus PDI	ALGORITHMS										control systems; feedforward; feedback; learning systems; deterministic artificial intelligence; Luenberger; proportional-derivative-integral; PDI; virtual zero reference; dead-beat control inspired		This manuscript will explore and analyze the effects of different paradigms for the control of rigid body motion mechanics. The experimental setup will include deterministic artificial intelligence composed of optimal self-awareness statements together with a novel, optimal learning algorithm, and these will be re-parameterized as ideal nonlinear feedforward and feedback evaluated within a Simulink simulation. Comparison is made to a custom proportional, derivative, integral controller (modified versions of classical proportional-integral-derivative control) implemented as a feedback control with a specific term to account for the nonlinear coupled motion. Consistent proportional, derivative, and integral gains were used throughout the duration of the experiments. The simulation results will show that akin feedforward control, deterministic self-awareness statements lack an error correction mechanism, relying on learning (which stands in place of feedback control), and the proposed combination of optimal self-awareness statements and a newly demonstrated analytically optimal learning yielded the highest accuracy with the lowest execution time. This highlights the potential effectiveness of a learning control system.																		1999-4893				JAN	2020	13	1							23	10.3390/a13010023													
J								Two-Machine Job-Shop Scheduling Problem to Minimize the Makespan with Uncertain Job Durations	ALGORITHMS										scheduling; uncertain duration; flow-shop; job-shop; makespan criterion	PROCESSING TIMES; COMPLETION-TIME; FLOWSHOP; BOUNDS	We study two-machine shop-scheduling problems provided that lower and upper bounds on durations of n jobs are given before scheduling. An exact value of the job duration remains unknown until completing the job. The objective is to minimize the makespan (schedule length). We address the issue of how to best execute a schedule if the job duration may take any real value from the given segment. Scheduling decisions may consist of two phases: an off-line phase and an on-line phase. Using information on the lower and upper bounds for each job duration available at the off-line phase, a scheduler can determine a minimal dominant set of schedules (DS) based on sufficient conditions for schedule domination. The DS optimally covers all possible realizations (scenarios) of the uncertain job durations in the sense that, for each possible scenario, there exists at least one schedule in the DS which is optimal. The DS enables a scheduler to quickly make an on-line scheduling decision whenever additional information on completing jobs is available. A scheduler can choose a schedule which is optimal for the most possible scenarios. We developed algorithms for testing a set of conditions for a schedule dominance. These algorithms are polynomial in the number of jobs. Their time complexity does not exceed O(n2). Computational experiments have shown the effectiveness of the developed algorithms. If there were no more than 600 jobs, then all 1000 instances in each tested series were solved in one second at most. An instance with 10,000 jobs was solved in 0.4 s on average. The most instances from nine tested classes were optimally solved. If the maximum relative error of the job duration was not greater than 20%, then more than 80% of the tested instances were optimally solved. If the maximum relative error was equal to 50%, then 45% of the tested instances from the nine classes were optimally solved.																		1999-4893				JAN	2020	13	1							4	10.3390/a13010004													
J								Detection of Suicide Ideation in Social Media Forums Using Deep Learning	ALGORITHMS										suicide ideation; early suicide detection; linguistic metadata; word embedding; machine learning; deep learning; Reddit social media	TWITTER; USERS; BEHAVIOR	Suicide ideation expressed in social media has an impact on language usage. Many at-risk individuals use social forum platforms to discuss their problems or get access to information on similar tasks. The key objective of our study is to present ongoing work on automatic recognition of suicidal posts. We address the early detection of suicide ideation through deep learning and machine learning-based classification approaches applied to Reddit social media. For such purpose, we employ an LSTM-CNN combined model to evaluate and compare to other classification models. Our experiment shows the combined neural network architecture with word embedding techniques can achieve the best relevance classification results. Additionally, our results support the strength and ability of deep learning architectures to build an effective model for a suicide risk assessment in various text classification tasks.																		1999-4893				JAN	2020	13	1							7	10.3390/a13010007													
J								A Generalized MILP Formulation for the Period-Aggregated Resource Leveling Problem with Variable Job Duration	ALGORITHMS										resource leveling problem; project scheduling		We study a resource leveling problem with variable job duration. The considered problem includes both scheduling and resource management decisions. The planning horizon is fixed and separated into a set of time periods of equal length. There are several types of resources and their amount varies from one period to another. There is a set of jobs. For each job, a fixed volume of work has to be completed without any preemption while using different resources. If necessary, extra resources can be used at additional costs during each time period. The optimization goal is to minimize the total overload costs required for the execution of all jobs by the given deadline. The decision variables specify the starting time of each job, the duration of the job and the resource amount assigned to the job during each period (it may vary over periods). We propose a new generalized mathematical formulation for this optimization problem. The formulation is compared with existing approaches from the literature. Theoretical study and computational experiments show that our approach provides more flexible resource allocation resulting in better final solutions.																		1999-4893				JAN	2020	13	1							6	10.3390/a13010006													
J								Top Position Sensitive Ordinal Relation Preserving Bitwise Weight for Image Retrieval	ALGORITHMS										image retrieval; binary code; hash algorithm; bitwise weights; top-rank-sensitive; ordinal-relation-preserving		In recent years, binary coding methods have become increasingly popular for tasks of searching approximate nearest neighbors (ANNs). High-dimensional data can be quantized into binary codes to give an efficient similarity approximation via a Hamming distance. However, most of existing schemes consider the importance of each binary bit as the same and treat training samples at different positions equally, which causes many data pairs to share the same Hamming distance and a larger retrieval loss at the top position. To handle these problems, we propose a novel method dubbed by the top-position-sensitive ordinal-relation-preserving bitwise weight (TORBW) method. The core idea is to penalize data points without preserving an ordinal relation at the top position of a ranking list more than those at the bottom and assign different weight values to their binary bits according to the distribution of query data. Specifically, we design an iterative optimization mechanism to simultaneously learn binary codes and bitwise weights, which makes their learning processes related to each other. When the iterative procedure converges, the binary codes and bitwise weights are effectively adapted to each other. To reduce the training complexity, we relax the discrete constraints of both the binary codes and the indicator function. Furthermore, we pretrain a tensor ordinal graph to decrease the time consumption of computing a relative similarity relationship among data points. Experimental results on three large-scale ANN search benchmark datasets, i.e., SIFT1M, GIST1M, and Cifar10, show that the proposed TORBW method can achieve superior performance over state-of-the-art approaches.																		1999-4893				JAN	2020	13	1							18	10.3390/a13010018													
J								Markov Chain Monte Carlo Based Energy Use Behaviors Prediction of Office Occupants	ALGORITHMS										MCMC; energy use behavior; time series; electrical equipment	ENVIRONMENTAL CONTROLS; ACTIVITY RECOGNITION; THERMAL COMFORT; MCMC; APARTMENTS; MODELS	Prediction of energy use behaviors is a necessary prerequisite for designing personalized and scalable energy efficiency programs. The energy use behaviors of office occupants are different from those of residential occupants and have not yet been studied as intensively as residential occupants. This paper proposes a method based on Markov chain Monte Carlo (MCMC) to predict the energy use behaviors of office occupants. Firstly, an indoor electrical Internet of Things system (IEIoTS) for the office scenario is developed to collect the switching state time series data of selected user electrical equipment (desktop computer, water dispenser, light) and the historical environment parameters. Then, the Metropolis-Hastings (MH) algorithm is used to sample and obtain the optimal solution of the parameters for the office occupants' behavior function, the model of which includes the energy action model, energy working hours model, and air-conditioner energy use behavior model. Finally, comparative experiments are carried out to evaluate the performance of the proposed method. The experimental results show that while the mean value performs similarly in estimating the energy use model, the proposed method outperforms the Maximum Likelihood Estimation (MLE) method on uncertainty quantification with relatively narrower confidence intervals.																		1999-4893				JAN	2020	13	1							21	10.3390/a13010021													
J								Citywide Cellular Traffic Prediction Based on a Hybrid Spatiotemporal Network	ALGORITHMS										communication traffic prediction; intelligent traffic management; deformable convolution; attention mechanism	5G NETWORKS; TECHNOLOGIES; INTELLIGENCE	With the arrival of 5G networks, cellular networks are moving in the direction of diversified, broadband, integrated, and intelligent networks. At the same time, the popularity of various smart terminals has led to an explosive growth in cellular traffic. Accurate network traffic prediction has become an important part of cellular network intelligence. In this context, this paper proposes a deep learning method for space-time modeling and prediction of cellular network communication traffic. First, we analyze the temporal and spatial characteristics of cellular network traffic from Telecom Italia. On this basis, we propose a hybrid spatiotemporal network (HSTNet), which is a deep learning method that uses convolutional neural networks to capture the spatiotemporal characteristics of communication traffic. This work adds deformable convolution to the convolution model to improve predictive performance. The time attribute is introduced as auxiliary information. An attention mechanism based on historical data for weight adjustment is proposed to improve the robustness of the module. We use the dataset of Telecom Italia to evaluate the performance of the proposed model. Experimental results show that compared with the existing statistics methods and machine learning algorithms, HSTNet significantly improved the prediction accuracy based on MAE and RMSE.																		1999-4893				JAN	2020	13	1							20	10.3390/a13010020													
J								A Visual Object Tracking Algorithm Based on Improved TLD	ALGORITHMS										object tracking; tracking-learning-detection; kernelized correlation filters; histogram of oriented gradient; real time; robustness		Visual object tracking is an important research topic in the field of computer vision. Tracking-learning-detection (TLD) decomposes the tracking problem into three modules-tracking, learning, and detection-which provides effective ideas for solving the tracking problem. In order to improve the tracking performance of the TLD tracker, three improvements are proposed in this paper. The built-in tracking module is replaced with a kernelized correlation filter (KCF) algorithm based on the histogram of oriented gradient (HOG) descriptor in the tracking module. Failure detection is added for the response of KCF to identify whether KCF loses the target. A more specific detection area of the detection module is obtained through the estimated location provided by the tracking module. With the above operations, the scanning area of object detection is reduced, and a full frame search is required in the detection module if objects fails to be tracked in the tracking module. Comparative experiments were conducted on the object tracking benchmark (OTB) and the results showed that the tracking speed and accuracy was improved. Further, the TLD tracker performed better in different challenging scenarios with the proposed method, such as motion blur, occlusion, and environmental changes. Moreover, the improved TLD achieved outstanding tracking performance compared with common tracking algorithms.																		1999-4893				JAN	2020	13	1							15	10.3390/a13010015													
J								Improving search engine optimization (SEO) by using hybrid modified MCDM models	ARTIFICIAL INTELLIGENCE REVIEW										SEO; MCDM; DEMATEL; DANP; VIKOR	MULTICRITERIA ANALYSIS; DEMATEL	Search engine optimization (SEO) has been considered one of the most important techniques in internet marketing. This study establishes a decision model of search engine ranking for administrators to improve the performances of websites that satisfy users' needs. To probe into the interrelationship and influential weights among criteria of SEO and evaluate the gaps of performance to achieve the aspiration level in real world, this research utilizes hybrid modified multiple criteria decision-making models, including decision-making trial and evaluation laboratory (DEMATEL), DEMATEL-based analytic network process (called DANP), and VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR). The empirical findings discover that the criteria of SEO possessed a self-effect relationship based on DEMATEL technique. According to the influential network relation map (INRM), external website optimization is the top priority dimension that needs to be improved when implementing SEO. Among the six criteria for evaluation, meta tags is the most significant criterion influencing search engine ranking, followed by keywords and website design. The evaluation of search engine ranking reveals that the website with lowest gap would be the optimal example for administrators of websites to make high ranking website during the time that this study is executed.																	0269-2821	1573-7462				JAN	2020	53	1					1	16		10.1007/s10462-018-9644-0													
J								40 years of cognitive architectures: core cognitive abilities and practical applications	ARTIFICIAL INTELLIGENCE REVIEW										Survey; Cognitive architectures; Perception; Attention; Cognitive abilities; Practical applications	GLOBAL WORKSPACE THEORY; BRAIN-BASED DEVICES; REAL-TIME; DECISION-MAKING; NEURAL-NETWORK; ARTIFICIAL-INTELLIGENCE; VISUAL-ATTENTION; META-COGNITION; HUMANOID ROBOT; UNIFIED THEORY	In this paper we present a broad overview of the last 40 years of research on cognitive architectures. To date, the number of existing architectures has reached several hundred, but most of the existing surveys do not reflect this growth and instead focus on a handful of well-established architectures. In this survey we aim to provide a more inclusive and high-level overview of the research on cognitive architectures. Our final set of 84 architectures includes 49 that are still actively developed, and borrow from a diverse set of disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities, such as perception, attention mechanisms, action selection, memory, learning, reasoning and metareasoning. In order to assess the breadth of practical applications of cognitive architectures we present information on over 900 practical projects implemented using the cognitive architectures in our list. We use various visualization techniques to highlight the overall trends in the development of the field. In addition to summarizing the current state-of-the-art in the cognitive architecture research, this survey describes a variety of methods and ideas that have been tried and their relative success in modeling human cognitive abilities, as well as which aspects of cognitive behavior need more research with respect to their mechanistic counterparts and thus can further inform how cognitive science might progress.																	0269-2821	1573-7462				JAN	2020	53	1					17	94		10.1007/s10462-018-9646-y													
J								Social Book Search: a survey	ARTIFICIAL INTELLIGENCE REVIEW										Social Book Search; Book recommendation; Language modeling; Vector Space model; Boolean Retrieval model; Information Retrieval	MODEL	Social Book Search is a new area of social search. In the modern world everything is going to be amenable due to the web and social media. The web and social media give us access to a wealth of information, not only different in quantity but also in character. Traditional descriptions from professionals are now supplemented with user generated content. Although books have been the predominant source of information for centuries, the way we acquire, share, and publish information has changed and has been changing in fundamental ways due to the web. In the modern era, in order to purchase a book, the users are not only depend on the title, author name, publisher etc of the book available as controlled metadata but also on other aspects which include the reviews, editorial reviews etc available in different social media. Our primary focus in this survey is to describe the features of different social cataloging book sites as well as their recommendation based on books. How the online searching of books is useful to the user and up to what extent, and what are their aim are some of the issues we shall deal with. We will also discuss evolution of those techniques for Social Book Search presented over years at the Initiative for the Evaluation of XML Retrieval (INEX) and Conference and Labs of the Evaluation Forum (CLEF). To what extent the features and functionality of a social sharing platform influence the user behavior, is also discussed in the paper. This survey provides an overview of research done in the area of Social Book Search from perspective of Information Retrieval.																	0269-2821	1573-7462				JAN	2020	53	1					95	139		10.1007/s10462-018-9647-x													
J								Two types of coverings based multigranulation rough fuzzy sets and applications to decision making	ARTIFICIAL INTELLIGENCE REVIEW										Multigranulation rough set; Covering based (optimistic; pessimistic and variable precision) multigranulation rough fuzzy set; Neighborhood; Covering based rough fuzzy set; Multiple criteria group decision making	NEIGHBORHOOD OPERATORS; APPROXIMATION OPERATORS; GRANULATION; MODEL; REDUCTION	Covering based multigranulation rough fuzzy set, as a generalization of granular computing and covering based rough fuzzy set theory, is a vital tool for dealing with the vagueness and multigranularity in artificial intelligence and management sciences. By means of neighborhoods, we introduce two types of coverings based (optimistic, pessimistic and variable precision) multigranulation rough fuzzy set models, respectively. Some axiomatic systems are also obtained. The relationships between two types of coverings based (optimistic, pessimistic and variable precision) multigranulation rough fuzzy set models are established. Based on the theoretical discussion for the covering based multigranulation rough fuzzy set models, we present an approach to multiple criteria group decision making problem. These two types of basic models and the procedure of decision making methods as well as the algorithm for the new approach are given in detail. By comparative analysis, the ranking results based on two different models have a highly consensus. Although there exist some different ranking results on these two methods, the optimal selected alternative is the same.																	0269-2821	1573-7462				JAN	2020	53	1					167	198		10.1007/s10462-018-9649-8													
J								Real-time control of ball balancer using neural integrated fuzzy controller	ARTIFICIAL INTELLIGENCE REVIEW										Ball balancer system; Neural integrated fuzzy control; Neural integrated fuzzy with PID control; Proportional-integral-derivative control	FUTURE-DIRECTIONS; PLATE SYSTEM; TRACKING; DESIGN; OBSERVER; DYNAMICS; SCHEME	This paper presents the design, control, and validation of two degrees of freedom Ball Balancer system. The ball and plate system is a nonlinear, electromechanical, multivariable, closed-loop unstable system on which study is carried out to control the position of ball and plate angle. The model of the system is developed using MATLAB/Simulink, and neural integrated fuzzy and its hybridization with PID have been implemented. The performance of each controller is evaluated in terms of time response analysis and steady-state error. Comparative study of simulation and real-time control results show that by using the neural integrated fuzzy controller and neural integrated fuzzy with proportional-integral-derivative Controller, the peak overshoot is reduced as compared with the PID controller and lead the system prone to appropriate balancing. These control techniques provide a stable and controlled output to the system for ball balancing and plate angle control.																	0269-2821	1573-7462				JAN	2020	53	1					351	368		10.1007/s10462-018-9658-7													
J								Reflective agents for personalisation in collaborative games	ARTIFICIAL INTELLIGENCE REVIEW										Personalisation; Profiling; Collaborative games; Decision-making games; Reflective agents; Player engagement; Player performance	DECISION-MAKING; DESIGN; CUSTOMIZATION; OPTIMIZATION; SYSTEM; MODEL; PLAY	The collaborative aspect of games has been shown to potentially increase player performance and engagement over time. However, collaborating players need to perform well for the team as a whole to benefit and thus teams often end up performing no better than a strong player would have performed individually. Personalisation offers a means for improving overall performance and engagement, but in collaborative games, personalisation is seldom implemented, and when it is, it is overwhelmingly passive such that the player is not guided to goal states and the effectiveness of the personalisation is not evaluated and adapted accordingly. In this paper, we propose and apply the use of reflective agents to personalisation ('reflective personalisation') in collaborative gaming for individual players within collaborative teams via a combination of individual player and team profiling in order to improve player and thus team performance and engagement. The reflective agents self-evaluate, dynamically adapting their personalisation techniques to most effectively guide players towards specific goal states, match players and form teams. We incorporate this agent-based approach within a microservices architecture, which itself is a set of collaborating services, to facilitate a scalable and portable approach that enables both player and team profiles to persist across multiple games. An experiment involving 90 players over a two-month period was used to comparatively assess three versions of a collaborative game that implemented reflective, guided, and passive personalisation for individual players within teams. Our results suggest that the proposed reflective personalisation approach improves team player performance and engagement within collaborative games over guided or passive personalisation approaches, but that it is especially effective for improving engagement.																	0269-2821	1573-7462				JAN	2020	53	1					429	474		10.1007/s10462-018-9665-8													
J								Independence test and canonical correlation analysis based on the alignment between kernel matrices for multivariate functional data	ARTIFICIAL INTELLIGENCE REVIEW										Multivariate functional data; Functional data analysis; Correlation analysis; Canonical correlation analysis	DEPENDENCE; ALGORITHMS	In the case of vector data, Gretton et al. (Algorithmic learning theory. Springer, Berlin, pp 63-77, 2005) defined Hilbert-Schmidt independence criterion, and next Cortes et al. (J Mach Learn Res 13:795-828, 2012) introduced concept of the centered kernel target alignment (KTA). In this paper we generalize these measures of dependence to the case of multivariate functional data. In addition, based on these measures between two kernel matrices (we use the Gaussian kernel), we constructed independence test and nonlinear canonical variables for multivariate functional data. We show that it is enough to work only on the coefficients of a series expansion of the underlying processes. In order to provide a comprehensive comparison, we conducted a set of experiments, testing effectiveness on two real examples and artificial data. Our experiments show that using functional variants of the proposed measures, we obtain much better results in recognizing nonlinear dependence.																	0269-2821	1573-7462				JAN	2020	53	1					475	499		10.1007/s10462-018-9666-7													
J								A novel exponential distance and its based TOPSIS method for interval-valued intuitionistic fuzzy sets using connection number of SPA theory	ARTIFICIAL INTELLIGENCE REVIEW										Multiattribute decision-making; Exponential distance measure; Information measures; Connection numbers; IVIFS; TOPSIS method	MULTIATTRIBUTE DECISION-MAKING; AGGREGATION OPERATORS; PROGRAMMING METHODOLOGY; SIMILARITY MEASURES; LAWS	The objective of this work is to present a novel multi-attribute decision making (MADM) method under interval-valued intuitionistic fuzzy (IVIF) set environment by integrating a Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) method. Set pair analysis (SPA) theory is the modern uncertainty theory which is composed by the three components, namely "identity", "discrepancy" and "contrary" degrees of the connection number (CN) and overlap with the various existing theories for handling the uncertainties in the data. Thus, motivated by this, in the present work, an attempt is made to enrich the theory of information measure by presented some exponential based distance measures using CNs of the IVIF sets. The supremacy of the proposed measure is also discussed. Afterward, a TOPSIS method based on the proposed distance measures is developed to solve MADM problem under IVIF environment where each of the element is characteristics by IVIF numbers. The utility, as well as supremacy of the approach, is confirmed through a real-life numerical example and validate it by comparing their results with the several existing approaches results.																	0269-2821	1573-7462				JAN	2020	53	1					595	624		10.1007/s10462-018-9668-5													
J								Detecting anomalies in sequential data augmented with new features	ARTIFICIAL INTELLIGENCE REVIEW										Anomaly detection; Sequential data; Feature extraction; Weighted local outlier factor	OUTLIER DETECTION	This paper presents a new weighted local outlier factor method for anomaly detection, which is underpinned with three novel components: (1) a piecewise linear representation defined on the basis of the important points that consist of extreme points and additional points; (2) a set of new features which are used to identify anomalies given the new piecewise linear representation; (3) a weighting schema, assigning different weights to different features by accounting for the discriminant power of the features. The underlying idea of the proposed method is to characterize a time series with a set of four features and then discover abnormal changes by taking account of the closeness of any data points augmented with the new features. The comparative experiments demonstrate that the proposed piecewise representation method has performed well in sequential time series data, and the weighted local outlier factor method has achieved better accuracy and RankPower in detecting anomalies from the same data sets in comparison with the conventional local outlier factor, normalized local outlier factor and HOT symbolic aggregate approximation methods.																	0269-2821	1573-7462				JAN	2020	53	1					625	652		10.1007/s10462-018-9671-x													
J								Robust visual line-following navigation system for humanoid robots	ARTIFICIAL INTELLIGENCE REVIEW										Line-following; Humanoid robot; Computer vision; Rectangle search; Angle compensation; PID controller; Visual navigation	DESIGN	This paper implements a novel line-following system for humanoid robots. Camera embedded on the robot's head captures the image and then extracts the line using a high-speed and high-accuracy rectangular search method. This method divides the search location into three sides of rectangle and performs image convolution by edge detection matrix. The extracted line is used to calculate relative parameters, including forward velocity, lateral velocity and angular velocity that drive line-following walking. A proposed path curvature estimation method generates the forward velocity and guidance reference point of the robot. A classical PID controller and a PID controller with angle compensation are then used to set the lateral velocity and angular velocity of the robot, improving the performance in tracking a curved line. Line-following experiments for various shapes were conducted using humanoid robot NAO. Experimental results demonstrate the robot can follow different line shapes with the tracking error remaining at a low level. This is a significant improvement from existing biped robot visual navigation systems.																	0269-2821	1573-7462				JAN	2020	53	1					653	670		10.1007/s10462-018-9672-9													
J								Covering-based intuitionistic fuzzy rough sets and applications in multi-attribute decision-making	ARTIFICIAL INTELLIGENCE REVIEW										Covering based IF rough sets; IF beta-neighborhood; IFC beta-neighborhood; IF-TOPSIS methodology; MADM	APPROXIMATION OPERATORS; NEIGHBORHOOD OPERATORS; EXTENSIONS; MODELS; TOPSIS	Covering based intuitionistic fuzzy (IF) rough set is a generalization of granular computing and covering based rough sets. By combining covering based rough sets, IF sets and fuzzy rough sets, we introduce three classes of coverings based IF rough set models via IF beta-neighborhoods and IF complementary beta-neighborhood (IFC beta-neighborhood). The corresponding axiomatic systems are investigated, respectively. In particular, the rough and precision degrees of covering based IF rough set models are discussed. The relationships among these types of coverings based IF rough set models and covering based IF rough set models proposed by Huang et al. (Knowl Based Syst 107:155-178, 2016). Based on the theoretical analysis for coverings based IF rough set models, we put forward intuitionistic fuzzy TOPSIS (IF-TOPSIS) methodology to multi-attribute decision-making (MADM) problem with the evaluation of IF information problem. An effective example is to illustrate the proposed methodology. Finally, we deal with MADM problem with the evaluation of fuzzy information based on CFRS models. By comparative analysis, we find that it is more effective to deal with MADM problem with the evaluation of IF information based on CIFRS models than the one with the evaluation of fuzzy information based on CFRS models.																	0269-2821	1573-7462				JAN	2020	53	1					671	701		10.1007/s10462-018-9674-7													
J								Marketing campaign targeting using bridge extraction in multiplex social network	ARTIFICIAL INTELLIGENCE REVIEW										Social marketing; Influence metric; Link prediction; Graph mining; Sentiment analysis	LINK-PREDICTION; RECOMMENDATION; MODEL	In this paper, we introduce a methodology for improving the targeting of marketing campaigns using bridge prediction in communities based on the information of multilayer online social networks. The campaign strategy involves the identification of nodes with high brand loyalty and top-ranking nodes in terms of participation in bridges that will be involved in the evolution of the graph. Our approach is based on an efficient classification model combining topological characteristics of crawled social graphs with sentiment and linguistic traits of user-nodes, popularity in social media as well as meta path-based features of multilayer networks. To validate our approach we present a set of experimental results using a well-defined dataset from Twitter and Foursquare. Our methodology is useful to recommendation systems as well as to marketers who are interested to use social influence and run effective marketing campaigns.																	0269-2821	1573-7462				JAN	2020	53	1					703	724		10.1007/s10462-018-9675-6													
J								Robust multi-criteria decision making methodology for real life logistics center location problem	ARTIFICIAL INTELLIGENCE REVIEW										City logistics centers; Location selection; BWM; EDAS; Sensitivity analysis; Distance measures	SELECTION	A logistics center is the hub of a specific area, within various logistics-related activities (distribution, storage, transportation, consolidation, handling, customs clearance, imports, exports, transit processes, infrastructural services, insurance, banking, etc.) that are performed on a commercial basis. Determining the location of the logistics center is an important decision regarding cost and benefit analysis. A three-stage methodology has been applied for presenting a framework for logistics center location selection in the context of Kayseri's logistics development plan. The first stage includes the determination of criteria through literature review and interviews with experts. The second stage includes the weighting of determined criteria using linear BWM (best-worst method). The third stage includes the ranking of locations using the evaluation based on distance from average solution (EDAS) method with different distance measures. Our proposed methodology BWM-EDAS and also EDAS with different distance measures, which are applied for the first time in the literature, provides helpful findings to rank the logistics center locations. Lastly, sensitivity analysis is conducted to validate the robustness.																	0269-2821	1573-7462				JAN	2020	53	1					725	751		10.1007/s10462-019-09763-y													
J								From ants to whales: metaheuristics for all tastes	ARTIFICIAL INTELLIGENCE REVIEW										Nature-inspired metaheuristics; Bio-inspired algorithms; Optimization; review	PARTICLE SWARM OPTIMIZATION; SIMULATED ANNEALING ALGORITHM; MOTH-FLAME OPTIMIZATION; GREY WOLF OPTIMIZER; ELECTROMAGNETISM-LIKE MECHANISM; FLOWER POLLINATION ALGORITHM; VEHICLE-ROUTING PROBLEM; TEXT FEATURE-SELECTION; KRILL HERD ALGORITHM; DIFFERENTIAL EVOLUTION	Nature-inspired metaheuristics comprise a compelling family of optimization techniques. These algorithms are designed with the idea of emulating some kind natural phenomena (such as the theory of evolution, the collective behavior of groups of animals, the laws of physics or the behavior and lifestyle of human beings) and applying them to solve complex problems. Nature-inspired methods have taken the area of mathematical optimization by storm. Only in the last few years, literature related to the development of this kind of techniques and their applications has experienced an unprecedented increase, with hundreds of new papers being published every single year. In this paper, we analyze some of the most popular nature-inspired optimization methods currently reported on the literature, while also discussing their applications for solving real-world problems and their impact on the current literature. Furthermore, we open discussion on several research gaps and areas of opportunity that are yet to be explored within this promising area of science.																	0269-2821	1573-7462				JAN	2020	53	1					753	810		10.1007/s10462-018-09676-2													
J								Applying Max-sum to asymmetric distributed constraint optimization problems	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Distributed constraint optimization; Incomplete inference distributed algorithms; Max-sum	DECENTRALIZED COORDINATION; BELIEF PROPAGATION; INFERENCE; ADOPT	We study the adjustment and use of the Max-sum algorithm for solving Asymmetric Distributed Constraint Optimization Problems (ADCOPs). First, we formalize asymmetric factor-graphs and apply the different versions of Max-sum to them. Apparently, in contrast to local search algorithms, most Max-sum versions perform similarly when solving symmetric and asymmetric problems and some even perform better on asymmetric problems. Second, we prove that the convergence properties of Max-sum_ADVP (an algorithm that was previously found to outperform standard Max-sum and Bounded Max-sum) and the quality of the solutions it produces, are dependent on the order between nodes involved in each constraint, i.e., the inner constraint order (ICO). A standard ICO allows to reproduce the properties achieved for symmetric problems. Third, we demonstrate that a non-standard ICO can be used to balance exploration and exploitation. Our results indicate that Max-sum_ADVP with non-standard ICO and Damped Max-sum, when solving asymmetric problems, both outperform other versions of Max-sum, as well as local search algorithms specifically designed for solving ADCOPs.																	1387-2532	1573-7454				JAN 1	2020	34	1							13	10.1007/s10458-019-09436-8													
J								Transmission of images by unmanned underwater vehicles	AUTONOMOUS ROBOTS										Underwater communications; Sonar image transmissions; Unmanned underwater vehicle; Naval mine counter-measures	QUALITY ASSESSMENT; COMPRESSION	As an acoustic communications medium, water is characterized by frequency dependent attenuation, short range, very low bandwidth, scattering, and multi-path. It is generally difficult to acoustically communicate even terse messages underwater much less images. For the naval mine counter-measures mission, there is value in transmitting images of mine-like objects, acquired by side-scan sonar on-board unmanned underwater vehicles, to the above-water operator for review. The contribution of this paper is a methodology and implementation, based on vector quantization, to compress and transmit snippets of side-scan sonar images from underway unmanned underwater vehicles to an operator. The work has been validated through controlled indoor tank tests and several at-sea trials. The fidelity of the received images is such that trained operators can recognize targets in the received images as well as they would have in the original images. Future work investigates machine learning to improve the compression basis and psycho-visual studies for the specialized skill of feature recognition in sonar images.																	0929-5593	1573-7527				JAN	2020	44	1			SI		3	24		10.1007/s10514-019-09866-z													
J								Statistics of the distance traveled until successful connectivity for unmanned vehicles	AUTONOMOUS ROBOTS										First passage distance; Connectivity; Mobile robots; Gauss-Markov process; Realistic communication	COMMUNICATION; UAV	In this paper, we consider a scenario where a robot needs to establish connectivity with a remote operator or another robot, as it moves along a path. We are interested in answering the following question: what is the distance traveled by the robot along the path before it finds a connected spot? More specifically, we are interested in characterizing the statistics of the distance traveled along the path before it gets connected, in realistic channel environments experiencing path loss, shadowing and multipath effects. We develop an exact mathematical analysis of these statistics for straight-line paths and also mathematically characterize a more general space of loop-free paths (beyond straight paths) for which the analysis holds, based on the properties of the path such as its curvature. Finally, we confirm our theoretical analysis using extensive numerical results with real channel parameters from downtown San Francisco.																	0929-5593	1573-7527				JAN	2020	44	1			SI		25	42		10.1007/s10514-019-09850-7													
J								Optimizing multi-robot communication under bandwidth constraints	AUTONOMOUS ROBOTS										Communication decision-making; Multi-robot systems; Multi-robot planning	SYSTEMS	Robots working collaboratively can share observations with others to improve team performance, but communication bandwidth is limited. Recognizing this, an agent must decide which observations to communicate to best serve the team. Accurately estimating the value of a single communication is expensive; finding an optimal combination of observations to put in the message is intractable. In this paper, we present OCBC, an algorithm for Optimizing Communication under Bandwidth Constraints. OCBC uses forward simulation to evaluate communications and applies a bandit-based combinatorial optimization algorithm to select what to include in a message. We evaluate OCBC's performance in a simulated multi-robot navigation task. We show that OCBC achieves better task performance than a state-of-the-art method while communicating up to an order of magnitude less.																	0929-5593	1573-7527				JAN	2020	44	1			SI		43	55		10.1007/s10514-019-09849-0													
J								Distributed assignment with limited communication for multi-robot multi-target tracking	AUTONOMOUS ROBOTS										Multi-robot system; Task assignment; Distributed algorithm	CONNECTIVITY CONTROL; TASK ALLOCATION; MOVING TARGETS; ALGORITHMS; TAXONOMY; COVERAGE; SWARM	We study the problem of tracking multiple moving targets using a team of mobile robots. Each robot has a set of motion primitives to choose from in order to collectively maximize the number of targets tracked or the total quality of tracking. Our focus is on scenarios where communication is limited and the robots have limited time to share information with their neighbors. As a result, we seek distributed algorithms that can find solutions in a bounded amount of time. We present two algorithms: (1) a greedy algorithm that is guaranteed to find a 2-approximation to the optimal (centralized) solution but requiring |R| communication rounds in the worst case, where |R| denotes the number of robots, and (2) a local algorithm that finds a are parameters that allow the user to trade-off the solution quality with communication time. In addition to theoretical results, we present empirical evaluation including comparisons with centralized optimal solutions.																	0929-5593	1573-7527				JAN	2020	44	1			SI		57	73		10.1007/s10514-019-09856-1													
J								Live multicast video streaming from drones: an experimental study	AUTONOMOUS ROBOTS										Multicast video streaming; Rate-adaptation; IEEE 802.11; Drones	NETWORKS; QOE	We present and evaluate a multicast framework for point-to-multipoint and multipoint-to-point-to-multipoint video streaming that is applicable if both source and receiver nodes are mobile. Receiver nodes can join a multicast group by selecting a particular video stream and are dynamically elected as designated nodes based on their signal quality to provide feedback about packet reception. We evaluate the proposed application-layer rate-adaptive multicast video streaming over an aerial ad-hoc network that uses IEEE 802.11, a desirable protocol that, however, does not support a reliable multicast mechanism due to its inability to provide feedback from the receivers. Our rate-adaptive approach outperforms legacy multicast in terms of goodput, delay, and packet loss. Moreover, we obtain a gain in video quality (PSNR) of 30%\for point-to-multipoint and of for multipoint-to-point-to-multipoint streaming.																	0929-5593	1573-7527				JAN	2020	44	1			SI		75	91		10.1007/s10514-019-09851-6													
J								SwarmCom: an infra-red-based mobile ad-hoc network for severely constrained robots	AUTONOMOUS ROBOTS										Swarm robotics; MANET; Infra-red communication; Channel model; e-puck; libIrcom	LOCAL COMMUNICATION; E-PUCK; ARCHITECTURE; EVOLUTION; SYSTEMS; TDMA	Swarm robotics investigates groups of relatively simple robots that use decentralized control to achieve a common goal. While the robots of many swarm systems communicate via optical links, the underlying channels and their impact on swarm performance are poorly understood. This paper models the optical channel of a widely used robotic platform, the e-puck. It proposes SwarmCom, a mobile ad-hoc network for mobile robots. SwarmCom has a detector that, with the help of the channel model, was designed to adapt to the environment and nearby robots. Experiments with groups of up to 30 physical e-pucks show that (i) SwarmCom outperforms the state-of-the-art infra-red communication software-libIrcom-in range (up to 3 times further), bit error rate (between 50 and 63% lower), or throughput (up to 8 times higher) and that (ii) the maximum number of communication channels per robot is relatively low, which limits the load per robot even for high-density swarms. Using channel coding, the bit error rate can be further reduced at the expense of throughput. SwarmCom could have profound implications for swarm robotics, contributing to system understanding and reproducibility, while paving the way for novel applications.																	0929-5593	1573-7527				JAN	2020	44	1			SI		93	114		10.1007/s10514-019-09873-0													
J								Automatic Design of Deep Networks with Neural Blocks	COGNITIVE COMPUTATION										Automatic deep networks design; Reinforcement learning; Deep convolutional neural networks; Neural blocks; Image classification		In recent years, deep neural networks (DNNs) have achieved great successes in many areas, such as cognitive computation, pattern recognition, and computer vision. Although many hand-crafted deep networks have been proposed in the literature, designing a well-behaved neural network for a specific application requires high-level expertise yet. Hence, the automatic architecture design of DNNs has become a challenging and important problem. In this paper, we propose a new reinforcement learning method, whose action policy is to select neural blocks and construct deep networks. We define the action search space with three types of neural blocks, i.e., dense block, residual block, and inception-like block. Additionally, we have also designed several variants for the residual and inception-like blocks. The optimal network is automatically learned by a Q-learning agent, which is iteratively trained to generate well-performed deep networks. To evaluate the proposed method, we have conducted experiments on three datasets, MNIST, SVHN, and CIFAR-10, for image classification applications. Compared with existing hand-crafted and auto-generated neural networks, our auto-designed neural network delivers promising results. Moreover, the proposed reinforcement learning algorithm for deep networks design only runs on one GPU, demonstrating much higher efficiency than most of the previous deep network search approaches.																	1866-9956	1866-9964				JAN	2020	12	1					1	12		10.1007/s12559-019-09677-5													
J								Use of Neural Signals to Evaluate the Quality of Generative Adversarial Network Performance in Facial Image Generation	COGNITIVE COMPUTATION										Generative adversarial networks; Rapid serial visual presentation; Human judgments; Brain-computer interface; Neuro-AI interface		There is a growing interest in using generative adversarial networks (GANs) to produce image content that is indistinguishable from real images as judged by a typical person. A number of GAN variants for this purpose have been proposed; however, evaluating GAN performance is inherently difficult because current methods for measuring the quality of their output are not always consistent with what a human perceives. We propose a novel approach that combines a brain-computer interface (BCI) with GANs to generate a measure we call Neuroscore, which closely mirrors the behavioral ground truth measured from participants tasked with discerning real from synthetic images. This technique we call a neuro-AI interface, as it provides an interface between a human's neural systems and an AI process. In this paper, we first compare the three most widely used metrics in the literature for evaluating GANs in terms of visual quality and compare their outputs with human judgments. Secondly, we propose and demonstrate a novel approach using neural signals and rapid serial visual presentation (RSVP) that directly measures a human perceptual response to facial production quality, independent of a behavioral response measurement. The correlation between our proposed Neuroscore and human perceptual judgments has Pearson correlation statistics: r(48) = - 0.767, p = 2.089e - 10. We also present the bootstrap result for the correlation i.e., p <= 0.0001. Results show that our Neuroscore is more consistent with human judgment compared with the conventional metrics we evaluated. We conclude that neural signals have potential applications for high-quality, rapid evaluation of GANs in the context of visual image synthesis.																	1866-9956	1866-9964				JAN	2020	12	1					13	24		10.1007/s12559-019-09670-y													
J								A Multiple Attribute Group Decision-making Method Based on the Partitioned Bonferroni Mean of Linguistic Intuitionistic Fuzzy Numbers	COGNITIVE COMPUTATION										Partition Bonferroni mean operator; Linguistic intuitionistic fuzzy sets; MAGDM	AGGREGATION OPERATORS; 2-TUPLE	As a more effective linguistic information representation model, the linguistic intuitionistic fuzzy number (LIFN), made up of the linguistic membership degree (LMD) and the linguistic non-membership degree (LNMD), plays an important role in describing uncertain-decision information in cognitive activity. The partitioned Bonferroni mean (PBM) operator is constructed based on real conditions, considering that some factors considered by decision-makers are interrelated and some are independent. The PBM operator can aggregate evaluation information under various attributes, which are divided into several independent groups; the factors interact with each other within the same group, but the factors are independent of each other when located in different groups. The classical PBM operators and their extensions can handle many decision-making problems but are unable to handle decision-making problems under a linguistic intuitionistic environment. To take full advantage of the PBM operator and LIFNs, this paper combines the PBM operator with LIFNs to propose several PBM operators for aggregating LIFNs. First, the relative theories about LIFNs are reviewed in brief. Then, linguistic intuitionistic fuzzy partitioned BM aggregation operators and partitioned geometric BM aggregation operators are discussed, including the linguistic intuitionistic fuzzy partitioned Bonferroni mean (LIFPBM) operator, the linguistic intuitionistic fuzzy weighted partitioned Bonferroni mean (LIFWPBM) operator, the linguistic intuitionistic fuzzy partitioned geometric Bonferroni mean (LIFPGBM) operator, and the linguistic intuitionistic fuzzy weighted partitioned geometric Bonferroni mean (LIFWPGBM) operator. Moreover, a novel method constructed on the developed operators is presented to address multiple attribute group decision-making (MAGDM) problems with the LIFNs. The feasibility of the proposed method is given by a numeric example, and the comparative analysis of our proposed method and other methods shows some advantages of these new methods. The developed method is constructed based on LIFNs and the PBM operator, and it is more practical and general than other existing methods and easily describes qualitative information that stems from a decision maker's cognition. In addition, it can more effectively handle the complex relationship among multiple attributes in MAGDM problems.																	1866-9956	1866-9964				JAN	2020	12	1					49	70		10.1007/s12559-019-09676-6													
J								Using the Instance-Based Learning Paradigm to Model Energy-Relevant Occupant Behaviors in Buildings	COGNITIVE COMPUTATION										Cognitive modeling; Energy-relevant behavior; Instance-based learning; Prediction; Social simulation	INDOOR AIR-QUALITY; THERMAL COMFORT; OFFICE; WINDOWS; SIMULATION; PROBABILITY; PERFORMANCE; ADAPTATION; OPERATION; PREDICT	Human interactive behavior is accountable for most of the variance between the observed and predicted energy consumption of buildings, and is accordingly acknowledged as a major field of research into limiting building-related energy consumption. A thorough understanding of occupant behavior is critical to facilitate a more reliable prediction of energy consumption and identifying means by which pro-environmental behaviors can be promoted. Insights and models from psychology and sociology appear to be best suited to improving such understanding, and this article contributes to this end by developing and testing a cognitive model that serves as the core of a numerical human-building interaction model. The proposed implementation builds on instance-based learning, a well-established cognitive modeling paradigm, is integrated into a thermodynamic building model, and complemented by perception models for the approximation of the thermal and olfactory perception of the environment. The model successfully learns to interact plausibly with a set of elements of a model room-a heating system, a window, and the actor's clothing-in order to establish predefined room conditions. Accumulation of context-specific instances in the declarative memory, which are retrieved and blended in a decision situation, provide the model with the flexibility to adapt its actions to very different climatic contexts, represented by the locations Stuttgart, Madrid, Stockholm, and Melbourne. Moreover, the model manages to find appropriate compromises if need satisfaction requires contradictory actions, such as in situations where satisfaction of the olfactory need requires opening the window and satisfaction of the thermal need requires keeping it closed. Despite its obvious complexity, the model must be considered to be a basic model, which restricts the immediate comparability of its results to human behavior data. However, the successfully applied plausibility checks clearly indicate the value of the cognitive approach to modeling human-building interaction.																	1866-9956	1866-9964				JAN	2020	12	1					71	99		10.1007/s12559-019-09672-w													
J								An Integrated Method with PROMETHEE and Conflict Analysis for Qualitative and Quantitative Decision-Making: Case Study of Site Selection for Wind Power Plants	COGNITIVE COMPUTATION										Multiple-criteria decision-making; PROMETHEE; Probabilistic linguistic term set; Conflict analysis; Site selection	LINGUISTIC TERM SETS; CRITERIA; RANKING; SUPPLIERS; ELECTRE	Multiple-criteria decision-making is common in our daily life. The probabilistic linguistic term set is an effective tool to represent both simple and cognitive complex linguistic expressions given by individuals and groups completely. In this paper, the PROMETHEE (Preference Ranking Organization METHod for Enrichment Evaluations) is enhanced by integrating with a conflict analysis to solve general multiple-criteria decision-making problems with both quantitative and qualitative criteria. Firstly, to capture the inherent uncertainty of evaluations, interval numbers are used to expresses the values of quantitative criteria while probability linguistic term sets are used to scale the qualitative criteria. Then, a preference function for both quantitative and qualitative criteria is proposed. In addition, a conflict analysis is presented and added to the PROMETHEE, which can derive the preference, indifference, and incomparability (PIR) relations of alternatives. A reference point is given to select the thresholds for the PIR relations. Finally, the improved PROMETHEE is highlighted by a case study concerning site selection of the wind power plant.																	1866-9956	1866-9964				JAN	2020	12	1					100	114		10.1007/s12559-019-09675-7													
J								Saliency Subtraction Inspired Automated Event Detection in Underwater Environments	COGNITIVE COMPUTATION										Adaptive Saliency Subtraction; Event detection; Ground truth generation; Local Patch Saliency	VISUAL-ATTENTION; OBJECTS; MODEL	Unmanned underwater exploration in unconstrained environments is a challenging problem. Analysis of the large volumes of images/videos captured by underwater stations/vehicles manually is a major bottleneck for further research. Existing computer vision methods either do not target unconstrained underwater environments or they only aim to detect static or moving entities. In this paper, we present a novel method for analyzing underwater videos and detecting events. Entry/exit of an object in scene is treated as an event independent of the other objects present therein. The method is applied on underwater videos with no prior knowledge, thus aiding in automated underwater exploration. The method is inspired by the fact that saliency of objects in the scene is invariant of the surrounding environment. The proposed method is composed of three main steps: Local Patch Saliency, Adaptive Saliency Subtraction, and event generation for analyzing underwater imagery from the videos. The method is aimed at detecting overlapping events containing man-made as well as natural objects including those containing multiple objects in the unconstrained underwater conditions. The performance of the method is evaluated on publicly available videos obtained from Ocean Networks Canada and Fish4Knowledge datasets. Ground truth for Ocean Networks Canada videos is not available; hence, a method for generating the same for varied sources is also presented. The algorithm achieves a precision of 98% for event detection with 20% misclassification rate. The results show the robustness of the method that performs even in complex and varying underwater conditions.																	1866-9956	1866-9964				JAN	2020	12	1					115	127		10.1007/s12559-019-09671-x													
J								An Air Combat Decision Learning System Based on a Brain-Like Cognitive Mechanism	COGNITIVE COMPUTATION										Autonomous air combat; Learning system; Brain-like cognitive mechanism; Knowledge; Multi-level memory; Biological neural mechanism	SEQUENTIAL MANEUVERING DECISIONS; THEORETICAL APPROACH; PREFRONTAL CORTEX; INFLUENCE DIAGRAM; PREDICTION	The unmanned aerial vehicle (UAV) has emerged unexpectedly as a new force in the recent local wars. To occupy the high ground of military technology in the future, research on autonomous air combat using UAVs is extremely important. In this paper, we propose an intelligent air combat learning system inspired by the cognitive mechanism of the human brain. The air combat capability was divided into two parts: declarative and procedural memory. Imitating the updating and storage mechanism of human knowledge, a long- and short-term hierarchical asynchronous learning principle was designed. We adopted the basic idea of using the error signal to drive learning according to neurophysiological research. Drawing lessons from the working memory mechanism, the error signal for driving the short-term learning was created in the absence of labelled data, using only the interactive information. Then, we proved that the learning mechanism could ensure that system performance advances steadily and gradually. The experiments illustrated that the learning system designed in this paper can achieve some inferior confrontation ability through self-learning without human prior knowledge. Action strategies formed by learning are analogous to the classical tactical manoeuvres of human fighter pilots. We compared our work with related works and found that our method could improve its performance continuously and finally defeat its opponent.																	1866-9956	1866-9964				JAN	2020	12	1					128	139		10.1007/s12559-019-09683-7													
J								A Cognitively Inspired System Architecture for the Mengshi Cognitive Vehicle	COGNITIVE COMPUTATION										Cognitive vehicle; Autonomous driving; Driving scene construction; Cross-modal transfer		This paper introduces the functional system architecture of the Mengshi intelligent vehicle, winner of the 2018 World Intelligent Driving Challenge (WIDC). Different from traditional smart vehicles, a cognitive module is introduced in the system architecture to realise the transition from perception to decision-making. This is shown to enhance the practical utility of the smart vehicle, enabling safe and robust driving in different scenes. The collaborative work of hardware and software systems is achieved through multi-sensor fusion and artificial intelligence (AI) technologies, including novel use of deep machine learning and context-aware scene analysis to select optimal driving strategies. Experimental results using both robustness tests and road tests confirm that the Mengshi intelligent vehicle is reliable and robust in challenging environments. This paper describes the major components of this cognitively inspired architecture and discusses the results of the 2018 WIDC.																	1866-9956	1866-9964				JAN	2020	12	1					140	149		10.1007/s12559-019-09692-6													
J								Efficient Hybrid Nature-Inspired Binary Optimizers for Feature Selection	COGNITIVE COMPUTATION										Whale optimization algorithm; Grey wolf optimizer; Optimization; Feature selection; Metaheuristics	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; ALGORITHM; SEARCH	The process of dimensionality reduction is a crucial solution to deal with the dimensionality problem that may be faced when dealing with the majority of machine learning techniques. This paper proposes an enhanced hybrid metaheuristic approach using grey wolf optimizer (GWO) and whale optimization algorithm (WOA) to develop a wrapper-based feature selection method. The main objective of the proposed technique is to alleviate the drawbacks of both algorithms, including immature convergence and stagnation to local optima (LO). The hybridization is done with improvements in the mechanisms of both algorithms. To confirm the stability of the proposed approach, 18 well-known datasets are employed from the UCI repository. Furthermore, the classification accuracy, number of selected features, fitness values, and run time matrices are collected and compared with a set of well-known feature selection approaches in the literature. The results show the superiority of the proposed approach compared with both GWO and WOA. The results also show that the proposed hybrid technique outperforms other state-of-the-art approaches, significantly.																	1866-9956	1866-9964				JAN	2020	12	1					150	175		10.1007/s12559-019-09668-6													
J								EPF: A General Framework for Supporting Continuous Top-k Queries Over Streaming Data	COGNITIVE COMPUTATION										ELM stream classification top-k	EXTREME LEARNING-MACHINE; APPROXIMATION	Continuous top-k query over sliding window is a fundamental problem in the domain of streaming data management, which monitors the query window and retrieves k objects with the highest scores when the window slides. The key of supporting this query is maintaining a subset of objects in the window, and try to retrieve answers from them when the window slides. The state-of-the-art approach called SAP utilizes the partition technique to support top-k searches. Its key idea is using, as few as possible, high-quality candidates to support the query via finding a proper partition. However, it has to waste relatively high computation cost in evaluating whether the partition is proper and re-scanning the widow. In this paper, we propose an ELM-based framework named EPF, which improves SAP via learning the nature of streaming data. If we learn that the distribution of streaming data is predictable, we could construct a suitable prediction model for a more efficient partition of the window. Furthermore, we propose a novel algorithm to reduce the re-scanning cost. We conduct a thorough experimental study of this technique on real and synthetic datasets and show the significant performance improvement when applying the technique in existing algorithms.																	1866-9956	1866-9964				JAN	2020	12	1					176	194		10.1007/s12559-019-09661-z													
J								Extracting New Dispatching Rules for Multi-objective Dynamic Flexible Job Shop Scheduling with Limited Buffer Spaces	COGNITIVE COMPUTATION										Dynamic flexible job shop scheduling; Dispatching rules; Buffer conditions; Simulation; Gene expression programming; Nature-inspired approaches	FLOW-SHOP; COMPUTATIONAL INTELLIGENCE; OPTIMIZATION; ALGORITHM; SEARCH	Dispatching rules are among the most widely applied and practical methods for solving dynamic flexible job shop scheduling problems in manufacturing systems. Hence, the design of applicable and effective rules is always an important subject in the scheduling literature. The aim of this study is to propose a practical approach for extracting efficient rules for a more general type of dynamic job shop scheduling problem in which jobs arrive at the shop at different times and machine breakdowns occur stochastically. Limited-buffer conditions are also considered, increasing the problem complexity. Benchmarks are selected from the literature, with some modifications. Gene expression programming combined with a simulation model is used for the design of scheduling policies. The extracted rules are compared with several classic dispatching rules from the literature based on a multi-objective function. The new rules are found to be superior to the classic ones. They are robust and can be used for similar complex scheduling problems. The results prove the efficiency of gene expression programming as a nature-inspired method for dispatching rule extraction.																	1866-9956	1866-9964				JAN	2020	12	1					195	205		10.1007/s12559-018-9595-4													
J								Computational Imaging Method with a Learned Plug-and-Play Prior for Electrical Capacitance Tomography	COGNITIVE COMPUTATION										Image reconstruction; Deep convolutional neural network; Image prior; Data-driven prior; Inverse problem; Electrical capacitance tomography	CONVOLUTIONAL NEURAL-NETWORKS; RECONSTRUCTION ALGORITHM; INVERSION ALGORITHM; CLASSIFICATION; ITERATION; SELECTION; SPARSITY; MACHINE	Electrical capacitance tomography (ECT) is a potent image-based measurement technology for monitoring industrial processes, but low-quality images generally limit its application scope and measurement reliability. To increase the precision of reconstruction, in this study, a data-driven plug-and-play prior abstracted by a deep convolutional neural network (DCNN) and the sparseness prior of imaging objects, in form of regularizers, are jointly leveraged to generate a potent imaging model, in which the L-1 norm of the mismatch error acts as a data fidelity term (DFT) to weaken the sensitivity of estimation result to noisy input data. The DCNN is embedded into the split Bregman (SB) technique to generate a powerful computing scheme for solving the built imaging model and the fast iterative shrinkage-thresholding algorithm (FISTA) is applied to solve the sub-problems efficiently. Extensive numerical results verify that the proposed imaging technique has competitive reconstruction ability and better robustness in comparison with the state-of-the-art methods. This study demonstrates the validity and efficacy of the proposed algorithm in reducing reconstruction error. Most importantly, the research outcomes verify that the data-driven plug-and-play prior and the sparseness prior can be jointly embedded into the imaging model, leading to a remarkable decline in reconstruction error.																	1866-9956	1866-9964				JAN	2020	12	1					206	223		10.1007/s12559-019-09682-8													
J								A Multiscale Hierarchical Threshold-Based Completed Local Entropy Binary Pattern for Texture Classification	COGNITIVE COMPUTATION										Feature extraction; Multiscale thresholding; Completed local entropy binary pattern; Texture classification	ROTATION-INVARIANT; RECOGNITION; DESCRIPTOR; COUNT	Over the year, visual texture analysis has come to be recognized as one of the most important methods in the area of medical image analysis and understanding, face description and detection, and so on. The goal of texture descriptors is to capture the general characteristic of textures such as dependency as well as invariance properties. Among all the texture descriptors, the binary pattern family of algorithms achieves a great trade of representation efficiency and complexity. This work introduces an efficient discriminative texture descriptor for visual texture classification. Its main contribution is twofold: a multiscale thresholding framework based on hierarchical adaptive local partition to binary encoding and an efficient completed local entropy binary pattern (CLEBP) descriptor. The basic completed local entropy binary pattern is extended by multiscale thresholding framework with hierarchical thresholding to capture not only microstructure local patterns but also macrostructure texture information. Such extension improves the quality and discriminative factor of texture classification. Extensive experiments on three widely used benchmark texture databases (Outex, UIUC, and KTH-TIPS) proof the efficiency of the proposed visual texture descriptor and hierarchical thresholding strategy. Compared with some classical local binary pattern variants and many state-of-the-art methods, the proposed descriptor achieves competitive and superior texture classification performance. The results prove that the proposed method is a powerful and effective texture descriptor for visual texture classification.																	1866-9956	1866-9964				JAN	2020	12	1					224	237		10.1007/s12559-019-09673-9													
J								Optimal Feature Selection for Learning-Based Algorithms for Sentiment Classification	COGNITIVE COMPUTATION										Machine learning; Feature selection; Optimal feature selection; Relationship analysis; Sentiment classification; Social media; Text analysis	MACHINE	Sentiment classification is an important branch of cognitive computation-thus the further studies of properties of sentiment analysis is important. Sentiment classification on text data has been an active topic for the last two decades and learning-based methods are very popular and widely used in various applications. For learning-based methods, a lot of enhanced technical strategies have been used to improve the performance of the methods. Feature selection is one of these strategies and it has been studied by many researchers. However, an existing unsolved difficult problem is the choice of a suitable number of features for obtaining the best sentiment classification performance of the learning-based methods. Therefore, we investigate the relationship between the number of features selected and the sentiment classification performance of the learning-based methods. A new method for the selection of a suitable number of features is proposed in which the Chi Square feature selection algorithm is employed and the features are selected using a preset score threshold. It is discovered that there is a relationship between the logarithm of the number of features selected and the sentiment classification performance of the learning-based method, and it is also found that this relationship is independent of the learning-based method involved. The new findings in this research indicate that it is always possible for researchers to select the appropriate number of features for learning-based methods to obtain the best sentiment classification performance. This can guide researchers to select the proper features for optimizing the performance of learning-based algorithms. (A preliminary version of this paper received a Best Paper Award at the International Conference on Extreme Learning Machines 2018.)																	1866-9956	1866-9964				JAN	2020	12	1					238	248		10.1007/s12559-019-09669-5													
J								A Collaborative-Filtering-Based Data Collection Strategy for Friedreich's Ataxia	COGNITIVE COMPUTATION										Recommendation system; Collaborative filtering; Friedreich's ataxia; Data mining; Data collection	SYSTEMS; IDEBENONE; SCALE	Friedreich's ataxia (FRDA) is an inherited neurodegenerative disorder with the prevalence of 2-4 in every 100,000 Caucasian population. Since 2010, the European Friedreich's Ataxia Consortium for Translational Studies (EFACTS) has endeavored to define and characterize FRDA by recruiting over 940 FRDA patients to provide baseline data in 19 study sites distributed in 9 European countries. It is challenging to collect primary data at EFACTS' study sites because of physical/psychological difficulties in recruiting new patients and collecting follow-up assessment data. To overcome such challenges, in this paper, we propose a novel data collection strategy for the FRDA baseline data by using the collaborative filtering (CF) approaches. This strategy is motivated by the popularity of the nowadays "Recommendation System" whose central idea is based on the fact that similar patients have similar symptoms on each test item. By doing so, instead of having no data at all, the FRDA researchers would be provided with certain predicted baseline data on patients who cannot attend the assessments for physical/psychological reasons, thereby helping with the data analysis from the researchers' perspective. It is shown that the CF approaches are capable of predicting baseline data based on the similarity in test items of the patients, where the prediction accuracy is evaluated based on three rating scales selected from the EFACTS database. Experimental results demonstrate the validity and efficiency of the proposed strategy.																	1866-9956	1866-9964				JAN	2020	12	1					249	260		10.1007/s12559-019-09674-8													
J								Triangular Fuzzy Neutrosophic Preference Relations and Their Application in Enterprise Resource Planning Software Selection	COGNITIVE COMPUTATION										ERP software selection; Group decision-making; Triangular fuzzy neutrosophic preference relation; Multiplicative consistency; Consensus	GROUP DECISION-MAKING; SYSTEM; CONSISTENCY; SETS	Enterprise resource planning (ERP) system selection is one of the most important topics in an ERP implementation program that ensures the success of the system. Because of the inherent complexity of ERP systems, it is difficult and time consuming for the organization to select the suitable ERP software. This paper employs triangular fuzzy neutrosophic preference relations (TFNPRs) to express the recognitions of decision-makers (DMs) for the choice of ERP software. Preference relation is a powerful tool to express complex decision problems, and the triangular fuzzy neutrosophic number (TFNN) is a good choice to represent the recognitions of the DMs; this paper combines preference relation with TFNN to define the concept of triangular fuzzy neutrosophic preference relations (TFNPRs). To rank the evaluated ERP systems logically, a multiplicative consistency concept for TFNPRs is defined. Then, several multiplicative consistency-based 0-1 mixed programming models are established for estimating missing values in incomplete TFNPRs and for deriving multiplicatively consistent TFNPRs from inconsistent ones, respectively. For group decision-making (GDM), a Manhattan distance measure-based consensus index is defined to measure the agreement degrees of the DMs' opinions. A multiplicative consistency and consensus-based algorithm to GDM with TFNPRs is provided that can cope with incomplete and inconsistent TFNPRs. Meanwhile, an illustrative example about the selection of ERP software is offered to show the utilization of the new method, and comparison analysis is performed with several previous methods about ERP software selection. The new method adopts TFNPRs that can express the fuzzy truth-membership degree, the fuzzy indeterminacy-membership degree and the fuzzy falsity-membership degree of the recognitions of the DMs. It extends the application of preference relations and endows the DMs with more flexibility to denote their recognitions. Furthermore, the new method is based on the multiplicative consistency and consensus analysis that ensures the rational and representative ranking of the considered objects.																	1866-9956	1866-9964				JAN	2020	12	1					261	295		10.1007/s12559-019-09640-4													
J								Rising Star Evaluation Based on Extreme Learning Machine in Geo-Social Networks	COGNITIVE COMPUTATION										Extreme learning machine; Geo-social network; Rising star		In social networks, rising stars are junior individuals who may be not so charming at first but turn out to be outstanding over time. Recently, rising star evaluation has become a popular research topic in the field of social analysis, which is helpful for decision support, cognitive computation, and other practical problems. In this paper, we study the problem of rising star evaluation in geo-social networks. Specifically, given a topic keyword Q and a time point t, we aim at evaluating the latent influence of users to find rising stars, which refer to experts who have few activities and little impact currently on the underlying geo-social network but may become influential experts in the future. To efficiently evaluate future stars, we propose a novel processing framework based on extreme learning machine (ELM) called FS-ELM. FS-ELM consists of three key components. The first component constructs features by incorporating social topology and user behavior patterns. The second component extracts supervised information by discovering topic experts of Q at time (t + Delta t); that is, excluding those detected at time t, topic experts obtained at time (t + Delta t) can be regarded as rising stars at time t. The third component is ELM-based future star classification that leverages ELM as a departure point to evaluate whether a user is a rising star. Our experimental studies conducted on real-world datasets show that (1) FS-ELM can effectively discover rising stars with a query topic at time t and outperform other traditional methods and (2) user social characteristics have an important impact on the rising star evaluation. This paper studies a novel problem, namely, rising star evaluation in geo-social networks. We propose an advanced processing framework based on ELM by exploiting social topology characteristics and user behavior patterns. The experimental results encouragingly demonstrate the efficiency and effectiveness of the proposed approach.																	1866-9956	1866-9964				JAN	2020	12	1					296	308		10.1007/s12559-019-09680-w													
J								A New Risk-Based Fuzzy Cognitive Model and Its Application to Decision-Making	COGNITIVE COMPUTATION										Fuzzy cognitive model; fuzzy sets; decision-making; information risk; RFC-TOPSIS	INTUITIONISTIC FUZZY; NEUTROSOPHIC SETS; SELECTION; DEFUZZIFICATION; NUMBERS; FUSION	Cognitive information in real-world decision-making problems is usually associated with all sorts of ambiguities and uncertainties. Fuzzy sets have been proposed as a general workaround for such information representation. Notwithstanding, there are cases in which the fuzzy sets and fuzzy numbers have some degree of uncertainty when available data either come from unreliable sources or refer to events in the future. These situations result in some unreliability of the obtained fuzzy information. For the modeling of the possible future-event effects on the fuzzy information credibility, the present research presents a novel risk-based fuzzy cognitive methodology by investigating all possible cases to risk modeling of fuzzy sets and the governing mathematical equations. The new fuzzy cognitive model is used to develop a multi-criteria decision-making method based on a fuzzy TOPSIS method so-called RFC-TOPSIS, and the proposed approach was tested on a case study of failure modes and effects analysis problem. Based on the results, robust outcomes were obtained when the proposed methodology was used, highlighting the flexibility and the efficiency of the proposed methodology. The present concept can be used to deal with any problems, where membership function is associated with some risks and errors due to risk factors.																	1866-9956	1866-9964				JAN	2020	12	1					309	326		10.1007/s12559-019-09701-8													
J								A multi-objective ant colony optimization algorithm for community detection in complex networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Community detection; Multi-objective optimization; Complex network; Ant colony; Modularity; Pareto-optimal front	GENETIC ALGORITHM; EVOLUTIONARY ALGORITHM; MODULARITY; GA	Studying the structure of the evolutionary communities in complex networks is essential for detecting the relationships between their structures and functions. Recent community detection algorithms often use the single-objective optimization criterion. One such criterion is modularity which has fundamental problems and disadvantages and does not illustrate complex networks' structures. In this study, a novel multi-objective optimization algorithm based on ant colony algorithm (ACO) is recommended to solve the community detection problem in complex networks. In the proposed method, a Pareto archive is considered to store non-dominated solutions found during the algorithm's process. The proposed method maximizes both goals of community fitness and community score in a trade-off manner to solve community detection problem. In the proposed approach, updating the pheromone in ACO has been changed through Pareto concept and Pareto Archive. So, only non-dominated solutions that have entered the Pareto archive after each iteration are updated and strengthened through global updating. In contrast, the dominated solutions are weakened and forgotten through local updating. This method of updating the Pheromone will improve algorithm exploration space, and therefore, the algorithm will search and find new solutions in the optimal space. In comparison to other algorithms, the results of the experiments show that this algorithm successfully detects network structures and is competitive with the popular state-of-the-art approaches.																	1868-5137	1868-5145				JAN	2020	11	1			SI		5	21		10.1007/s12652-018-1159-7													
J								Hybrid microaggregation for privacy preserving data mining	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hybrid micoaggregation; Information loss; Identity disclosure risk; Attribute disclosure risk; Fuzzy and possibilistic clustering	DISCLOSURE RISK; K-ANONYMITY; PERTURBATION; ALGORITHM	k-Anonymity by microaggregation is one of the most commonly used anonymization techniques. This success is owe to the achievement of a worth of interest trade-off between information loss and identity disclosure risk. However, this method may have some drawbacks. On the disclosure limitation side, there is a lack of protection against attribute disclosure. On the data utility side, dealing with a real datasets is a challenging task to achieve. Indeed, the latter are characterized by their large number of attributes and the presence of noisy data, such that outliers or, even, data with missing values. Generating an anonymous individual data useful for data mining tasks, while decreasing the influence of noisy data is a compelling task to achieve. In this paper, we introduce a new microaggregation method, called HM-pfsom, based on fuzzy possibilistic clustering. Our proposed method operates through an hybrid manner. This means that the anonymization process is applied per block of similar data. Thus, we can help to decrease the information loss during the anonymization process. The HM-pfsom approach proposes to study the distribution of confidential attributes within each sub-dataset. Then, according to the latter distribution, the privacy parameter k is determined, in such a way to preserve the diversity of confidential attributes within the anonymized microdata. This allows to decrease the disclosure risk of confidential information.																	1868-5137	1868-5145				JAN	2020	11	1			SI		23	38		10.1007/s12652-018-1122-7													
J								What do people think about this monument? Understanding negative reviews via deep learning, clustering and descriptive rules	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sentiment analysis; Deep learning; Aspect clustering; Subgroup discovery	SUBGROUP DISCOVERY; EMERGING PATTERN; PREFERENCES; TOURISM; SET	Aspect-based sentiment analysis enables the extraction of fine-grained information, as it connects specific aspects that appear in reviews with a polarity. Although we detect that the information from these algorithms is very accurate at local level, it does not contribute to obtain an overall understanding of reviews. To fill this gap, we propose a methodology to portray opinions through the most relevant associations between aspects and polarities. Our methodology combines three off-the-shelf algorithms: (1) deep learning for extracting aspects, (2) clustering for joining together similar aspects, and (3) subgroup discovery for obtaining descriptive rules that summarize the polarity information of set of reviews. Concretely, we aim at depicting negative opinions from three cultural monuments in order to detect those features that need to be improved. Experimental results show that our approach clearly gives an overview of negative aspects, therefore it will be able to attain a better comprehension of opinions.																	1868-5137	1868-5145				JAN	2020	11	1			SI		39	52		10.1007/s12652-018-1150-3													
J								A password creation and validation system for social media platforms based on big data analytics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social media platforms; Big data analytics; Passwords; Password composition; Password generation; Text mining	SECURITY; MEMORABILITY	In the era of social web, the number of accounts that users need to maintain and consequently the number of associated passwords has increased. This usually constraints users to select very weak passwords that can be easily remembered, yet easily compromised. Indeed, recent data breaches in major social networking platforms have shown that users are still using naive passwords such as 123456. To remedy this serious problem and taking advantage of the availability of real password datasets, we propose a novel methodology that can detect all frequent and non-frequent patterns. The results are used to develop a novel password creation and validation system that could improve the strength of a password.																	1868-5137	1868-5145				JAN	2020	11	1			SI		53	73		10.1007/s12652-019-01172-x													
J								A novel network virtualization based on data analytics in connected environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data analytics; Network virtualization; Heterogeneous network; Connected environment; Network interference; Data-driven networking; Machine learning techniques	BIG DATA; INTERNET; WIRELESS; THINGS; FRAMEWORK; COMMUNICATION; OPPORTUNITIES; VEHICLES; SYSTEMS; IOT	Big data analytics is a growing trend for network and service management. Some approaches such as statistical analysis, data mining and machine learning have become promising techniques to improve operations and management of information technology systems and networks. In this paper, we introduce a novel approach for network management in terms of abnormality detection based on data analytics. Particularly, the main research focuses on how the network configuration can be automatically and adaptively decided, given various dynamic contexts (e.g., network interference, heterogeneity and so on). Specifically, we design a context-based data-driven framework for network operation in connected environment which includes three layer architecture: (i) network entity layer; (ii) complex semantic analytics layer and (iii) action provisioning layer. A case study on interference-based abnormal detection for connected vehicle explains more detail about our work.																	1868-5137	1868-5145				JAN	2020	11	1			SI		75	86		10.1007/s12652-018-1083-x													
J								Eventfully Safapp: hybrid approach to event detection for social media mining	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Semantic analysis; Event extraction; Online recruitment; Social media analysis; Radicalization detection; Foreign fighter recruitment		Safapp is a tool that crawls and analyses data from social media (specially Twitter and blogs) in the context of radicalization detection. This app has been developed in the context of SAFFRON European project. This paper focuses on the description of the semantic module of Safapp which is dedicated to the analysis of textual content of social networks and blogs, and more specifically on a newly developed module: the event extractor. With this module we expect to go one step further in the development of useful tools to track and analyze online propaganda.																	1868-5137	1868-5145				JAN	2020	11	1			SI		87	95		10.1007/s12652-018-1078-7													
J								An evolutionary clustering approach based on temporal aspects for context-aware service recommendation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Web service recommendation; Context-aware clustering; Multi-swarm optimization; K-means; Slope One	PARTICLE SWARM OPTIMIZATION; TIME-AWARE; SYSTEMS	Over the last years, recommendation techniques have emerged to cope with the challenging task of optimal service selection, and to help consumers satisfy their needs and preferences. However, most existing models on service recommendation only consider the traditional user-service relation, while in the real world, the perception and popularity of Web services may depend on several conditions including temporal, spatial and social constraints. Such additional factors in recommender systems influence users' preferences to a large extent. In this paper, we propose a context-aware Web service recommendation approach with a specific focus on time dimension. First, K-means clustering method is hybridized with a multi-population variant of the well-known Particle Swarm Optimization (PSO) in order to exclude the less similar users which share few common Web services with the active user in specific contexts. Slope One method is, then, applied to predict the missing ratings in the current context of user. Finally, a recommendation algorithm is proposed in order to return the top-rated services. Experimental studies confirmed the accuracy of our recommendation approach when compared to three existing solutions.																	1868-5137	1868-5145				JAN	2020	11	1			SI		119	138		10.1007/s12652-018-1079-6													
J								Enhance sentiment analysis on social networks with social influence analytics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social networks; Sentiment analysis; Homophily; Social influence		Sentiment analysis on social networks has attracted increasing research attention. Most previous works rely on text mining and the phenomenon of Homophily reflected by explicit friendship relations, which are a weak assumption for modeling sentiment and opinion similarities. In this paper we show that competitive results can be achieved with consideration of implicit influence relationships. In particular, we use heterogeneous graphs to infer sentiment polarities at user-level. We show that information about social influence processes can be used to improve sentiment analysis. Our transductive learning results reveal that incorporating such information can indeed lead to statistically significant sentiment classification improvements.																	1868-5137	1868-5145				JAN	2020	11	1			SI		139	149		10.1007/s12652-019-01234-0													
J								Entropy-aware ambient IoT analytics on humanized music information fusion	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of Things; Big data; Edge and cloud computing; Entropy; Latency; Power dissipation; Efficiency; Music fusion	SOUND SOURCE LOCALIZATION; BIG DATA; CLOUD; ALGORITHMS; INTERNET; RECOMMENDATION	Musical information fusion in the era of Internet is participatory multi-sensor based heterogeneous musical data recognition and computing. Participatory devices enhance the progression of intelligent multimedia data fusion and analytics in the participated edge computing devices in the context of ambient Internet of Things. Sensed data streams coming from multi-sensors encounter the conventional methodologies for data analytics and are further transmitted to emerging big data archetype. The proposed contribution analyses, validates and evaluates a set of qualitative music data collected from wearable sound sensors. The authors present system architecture with three committed layers of participated devices for music fusion in the Internet of Things environment. Besides, an analytical case study on music fusion challenges is discussed along with the elucidation of their unique features in terms of Big data V-Scheme, followed by the demonstration of edge-cloud computing paradigm with deliberate evaluations. In this work, the system requirements in terms of data transmission latency and relevant power dissipation are visualized. The information and proposed system entropy of stochastic source of music data are evaluated in order to measure system efficiency and stability for performing multimedia communication. Quantitative evaluations are studied for comparison of heterogeneous system architectures in terms of system entropy that illustrate significant improvement in music fusion efficiency upon employing the proposed system archetype.																	1868-5137	1868-5145				JAN	2020	11	1			SI		151	171		10.1007/s12652-019-01261-x													
J								A decomposition-based ant colony optimization algorithm for the multi-objective community detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ant colony optimization; Decompose; Multi-objective optimization problems; Community detection; Complex network	EVOLUTIONARY ALGORITHM; GENETIC ALGORITHM; NETWORKS; MOEA/D	Community structure is an important feature of complex network and detecting community can help us understand the function of networks very well. Community detection can be considered as a multi-objective optimization problem and the heuristic operators have shown promising results in dealing with this problem. In this paper, a multi-objective community detection algorithm named MOCD-ACO is proposed by combining the heuristic operator of ant colony optimization (ACO) and the multi-objective evolutionary algorithmbased on decomposition (MOEA/D). MOCD-ACO can simultaneously decompose two objective functions, i.e., Negative Ratio Association and Ratio Cut, into a number of single-objective optimization problems. Each ant is responsible for searching for a solution to a sub-problem. All ants are divided into some groups, each group sharing a pheromone matrix. The ants use pseudo-random probability selection models to construct solutions. An ant updates its current solution if it has found a better one in terms of its own objective. To make the algorithm not easy to fall into the local optimal solution, the weighted simulated annealing local search operator is integrated into the framework to expand the search range. In the experiments, synthetic network datasets and real network datasets are used to evaluate the performance of MOCD-ACO. Compared with five state-of-the-art methods, our algorithm proves to be effective in terms of normalized mutual information and modularity.																	1868-5137	1868-5145				JAN	2020	11	1			SI		173	188		10.1007/s12652-019-01241-1													
J								Evaluating fusion of RGB-D and inertial sensors for multimodal human action recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human action recognition; Deep learning; Convolutional neural network; Recurrent neural network; Multimodal fusion		Fusion of multiple modalities from different sensors is an important area of research for multimodal human action recognition. In this paper, we conduct an in-depth study to investigate the effect of different parameters like input preprocessing, data augmentation, network architectures and model fusion so as to come up with a practical guideline for multimodal action recognition using deep learning paradigm. First, for RGB videos, we propose a novel image-based descriptor called stacked dense flow difference image (SDFDI), capable of capturing the spatio-temporal information present in a video sequence. A variety of deep 2D convolutional neural networks (CNN) are then trained to compare our SDFDI against state-of-the-art image-based representations. Second, for skeleton stream, we propose data augmentation technique based on 3D transformations so as to facilitate training a deep neural network on small datasets. We also propose a bidirectional gated recurrent unit (BiGRU) based recurrent neural network (RNN) to model skeleton data. Third, for inertial sensor data, we propose data augmentation based on jittering with white Gaussian noise along with deep a 1D-CNN network for action classification. The outputs of all these three heterogeneous networks (1D-CNN, 2D-CNN and BiGRU) are combined by a variety of model fusion approach based on score and feature fusion. Finally, in order to illustrate the efficacy of the proposed framework, we test our model on a publicly available UTD-MHAD dataset, and achieved an overall accuracy of 97.91%, which is about 4% higher than using each modality individually. We hope that the discussions and conclusions from this work will provide a deeper insight to the researchers in the related fields, and provide avenues for further studies for different multi-sensor based fusion architectures.																	1868-5137	1868-5145				JAN	2020	11	1			SI		189	208		10.1007/s12652-019-01239-9													
J								A fog based load forecasting strategy based on multi-ensemble classification for smart grids	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; FOG; Cloud; Load forecasting	ENTROPY WEIGHT; TOPSIS METHOD; ALGORITHM; SELECTION; PREDICTION; FRAMEWORK; NETWORKS; INTERNET; MODEL; IOT	Internet of things (IoT) improves the development and operation of smart electrical grids (SEGs). Overcoming the cloud challenges, 2-tier architecture is replaced by 3-tier one for including a fog computing tier that acts as a bridge between the IoT devices embedded in SEG and cloud. Load forecasting is an essential process for the electrical system operation and planning as it provides intelligence to energy management. This paper completes the electrical load forecasting (ELF) strategy introduced (Rabie et al. in Cluster Comput 22(1):241-270, 2019). ELF consists of two phases which are; (1) data pre-processing Phase (DP2) and (2) load prediction phase (LP2). Both phases can be performed in the cloud tier on the stored data which is sent from fogs to cloud at cloud servers. DP2 aims to perform feature selection and outlier rejection using data mining techniques. The main contribution of this paper focuses on LP2 providing multi-ensemble load prediction (MELP) method which can be learned by the filtered data from DP2 to give fast and accurate predictions. MELP can deal with big electrical data based on Map-Reduce method. It mainly consists of two levels which are; (1) local ensemble level (LEL) in map phase and (2) global ensemble level (GEL) in reduce phase. In LEL, the ensemble classification principle is applied at every device in map phase. In GEL, the perfect and final decision for load prediction is taken in reduce phase based on global judger (GJ) method from many local predictions which are the results of all devices in map phase. The conducted experimental results have shown that the proposed MELP outperforms recent prediction methods in terms of accuracy, precision, recall, F1-measure, and run time. It is concluded that the proposed MELP method can deal with big electrical data. It has a good impact in maximizing system reliability, resilience, and stability as it introduces fast and accurate load predictions.																	1868-5137	1868-5145				JAN	2020	11	1			SI		209	236		10.1007/s12652-019-01299-x													
J								Deep learning-based face analysis system for monitoring customer interest	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Facial expression recognition; Head pose estimation; Facial analysis; Customer monitoring; Convolutional neural network	FACIAL EXPRESSION RECOGNITION; CONVOLUTIONAL NEURAL-NETWORKS; HEAD POSE ESTIMATION; PARALLEL FRAMEWORK; VISUAL FOCUS; EMOTIONS; ATTENTION; PATTERNS; BEHAVIOR; RECALL	In marketing research, one of the most exciting, innovative, and promising trends is quantification of customer interest. This paper presents a deep learning-based system for monitoring customer behavior specifically for detection of interest. The proposed system first measures customer attention through head pose estimation. For those customers whose heads are oriented toward the advertisement or the product of interest, the system further analyzes the facial expressions and reports customers' interest. The proposed system starts by detecting frontal face poses; facial components important for facial expression recognition are then segmented and an iconized face image is generated; finally, facial expressions are analyzed using the confidence values of obtained iconized face image combined with the raw facial images. This approach fuses local part-based features with holistic facial information for robust facial expression recognition. With the proposed processing pipeline, using a basic imaging device, such as a webcam, head pose estimation, and facial expression recognition is possible. The proposed pipeline can be used to monitor emotional response of focus groups to various ideas, pictures, sounds, words, and other stimuli.																	1868-5137	1868-5145				JAN	2020	11	1			SI		237	248		10.1007/s12652-019-01310-5													
J								An algorithm for detecting SQL injection vulnerability using black-box testing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Black box testing; SQL injection; SQL injection vulnerability; SQL injection attack; SQLI vulnerability scanner	FRAMEWORK	SQL Injection Attack (SQLIA) is one of the most severe attack that can be used against web database-driven applications. Attackers use SQLIA to obtain unauthorized access and perform unauthorized data modifications due to initial improper input validation by the web application developer. Various studies have shown that, on average, 64% of web applications worldwide are vulnerable to SQLIA due to improper input. To mitigate the devastating problem of SQLIA, this research proposes an automatic black box testing for SQL Injection Vulnerability (SQLIV). This acts to automate an SQLIV assessment in SQLIA. In addition, recent studies have shown that there is a need for improving the effectiveness of existing SQLIVS in order to reduce the cost of manual inspection of vulnerabilities and the risk of being attacked due to inaccurate false negative and false positive results. This research focuses on improving the effectiveness of SQLIVS by proposing an object-oriented approach in its development in order to help and minimize the incidence of false positive and false negative results, as well as to provide room for improving a proposed scanner by potential researchers. To test and validate the accuracy of research work, three vulnerable web applications were developed. Each possesses a different type of vulnerabilities and an experimental evaluation was used to validate the proposed scanner. In addition, an analytical evaluation is used to compare the proposed scanner with the existing academic scanners. The result of the experimental analysis shows significant improvement by achieving high accuracy compared to existing studies. Similarly, the analytical evaluations showed that the proposed scanner is capable of analyzing attacked page response using four different techniques.																	1868-5137	1868-5145				JAN	2020	11	1			SI		249	266		10.1007/s12652-019-01235-z													
J								Artificial bee colony with enhanced food locations for solving mechanical engineering design problems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial bee colony; Engineering design problems; Constrained Optimization; Convergence; Opposition based learning	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; ALGORITHM; SELECTION; PSO	Artificial Bee colony (ABC) simulates the intelligent foraging behavior of bees. ABC consists of three kinds of bees: employed, onlooker and scout. Employed bees perform exploration and onlooker bees perform exploitation whereas scout bees are responsible for randomly searching the food source in the feasible region. Being simple and having fewer control parameters ABC has been widely used to solve complex multifaceted optimization problems. ABC performs well at exploration than exploitation. The success of any nontraditional algorithm depends on these two antagonist factors. Focusing on this limitation of ABC, in this study the food locations in basic ABC are enhanced using Opposition based learning (OBL) concept. This variant is improved by incorporating greediness in searching behavior and named as I-ABC greedy. The modifications help in maintaining population diversity as well as enhance exploitation. The proposal is validated on seven mechanical engineering design problems. The simulated results have been noticed competent with that of state-of-art algorithms.																	1868-5137	1868-5145				JAN	2020	11	1			SI		267	290		10.1007/s12652-019-01265-7													
J								A comprehensive survey on trajectory schemes for data collection using mobile elements in WSNs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network; Mobile elements; Path planning; Trajectory; Energy-efficient; Mules	WIRELESS SENSOR NETWORKS; EFFICIENT DATA-COLLECTION; HOT-SPOT PROBLEM; POWER TRANSMISSION; GENETIC ALGORITHM; TOPOLOGY CONTROL; DATA MULE; SINK; LIFETIME; PROTOCOL	Mobile elements trajectory optimization is one of the most important and efficient ways to enhance the performance of wireless sensor networks (WSNs). In the last 15 years, extensive research has been done in this area, but less effort has been devoted to providing a concise review of the broader area. This article surveys the role of mobile elements trajectory optimization in the performance improvement of WSNs. The complete survey has been done based on three major aspects: applications, trajectory techniques and domains used to formulate the trajectory. Under these three aspects, large numbers of schemes are discussed, along with their sub-aspects. A comparative analysis using eight important parameters, like trajectory pattern, number of mobile elements, speed, mobile element type, etc., is presented in a chronological fashion. The paper also points out the merits and demerits of each scheme described. Based on the current research, we have identified some research domains in this area that need more attention and further exploration.																	1868-5137	1868-5145				JAN	2020	11	1			SI		291	312		10.1007/s12652-019-01268-4													
J								A trust model based batch verification of digital signatures in IoT	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Batch verification; Digital signature; Trust model	WIRELESS SENSOR NETWORKS; AUTHENTICATION SCHEME; AGGREGATE	In the modern day world, the Internet of things (IoT) is not a new concept. IoT is getting deployed in various applications and fields. Hence with this fast-growing trend, it is essential to maintain the security in the IoT network. Digital Signature is one of the important ways to authenticate an electronic document or a message during communication. Multiple digital signatures are verified at once through the concept of batch verification. Batch verification of multiple digital signatures reduces the computation load and time. Hence this concept is beneficial in IoT environment where nodes have low computation power and operate in a real-time environment. In this paper, we have developed a Trust Model for IoT which helps the Gateway node to identify the trusted sensor nodes which perform batch verification. The sensor nodes receive a batch of signatures from the Gateway node and verify signatures through batch verification and accordingly send back the results. The trust model that we have developed in this paper significantly reduces the probability of selecting unreliable nodes for verification and also reduces the computation load at Gateway node. We have implemented our trust model and presented the results for batch verification of digital signatures.																	1868-5137	1868-5145				JAN	2020	11	1			SI		313	327		10.1007/s12652-019-01289-z													
J								Speech/music classification using visual and spectral chromagram features	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Speech; music classification; Chromagram visual features; Chroma spectral features; Eigenvector centrality; Uniform LBP descriptors; SVM classifier	DISCRIMINATION	Automatic speech/music classification is an important tool in multimedia content analysis and retrieval which efficiently categorizes input audio and store it into relevant classes. This article proposes use of chromagram textural and spectral features for speech and music classification. Chromagram textural feature set is based on transforming the input audio into a chromagram image representation and then extracting uniform local binary pattern textural descriptors. Chroma spectral features involves novel chroma bin features which exploits music tonality present in the music signal. The optimal feature subset from the original feature set is selected using eigenvector centrality based feature selection, removing the redundant and irrelevant features and further enhancing the prediction performance. The performance of the algorithm is evaluated using S&S, GTZAN and MUSAN databases providing the advantage and suitability of both chroma spectral and visual features for the classification task. Extensive experiments performed using support vector machine classifier shows that the chromagram textural descriptors outperform other state-of-the-art approaches. Besides, good results are also achieved in the mismatched training and testing.																	1868-5137	1868-5145				JAN	2020	11	1			SI		329	347		10.1007/s12652-019-01303-4													
J								Fall detection and human activity classification using wearable sensors and compressed sensing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fall detection; Human activity; Wearable sensors; Compressed sensing; Classification	DETECTION SYSTEM; ACTIVITY RECOGNITION	The fall of elderly patients is still a critical medical issue since it can cause irreversible bone injuries due to the elderly bones weakness. To mitigate the likelihood of the occurrence of a fall, continuously tracking the patients with balance and health issues has been envisaged, despite being unpractical. To address this problem, we propose an efficient automatic fall detection system which is also fitted for the detection of different activities of daily living (ADL). The system relies on a wearable Shimmer device, to transmit some inertial signals via a wireless connection to a computer. Aiming at reducing the size of the transmitted data and minimizing the energy consumption, a compressive sensing (CS) method is applied. In this perspective, we started by creating our dataset from 17 subjects performing a set of movements, then three distinct systems were investigated: one which detects the presence or the absence of the fall, a second which detects static or dynamic movements including the fall, and a third which recognizes the fall and six other ADL activities. In the acquisition and classification steps, first only the data collected by the accelerometer are exploited, then a mixture of the accelerometer and gyroscope measurements are taken into consideration. The two configurations are compared and the resulting system incorporating CS capabilities is shown to achieve up to 99.8% of accuracy.																	1868-5137	1868-5145				JAN	2020	11	1			SI		349	361		10.1007/s12652-019-01214-4													
J								Using entropy for similarity measures in collaborative filtering	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender system; Similarity measure; Collaborative filtering; Information entropy	USER SIMILARITY; MODEL	Collaborative filtering has been successfully implemented in many commercial recommender systems. These systems recommend items favored by other users with similar preference history to the current user. As finding similar users is critical to the performance of the system, various techniques have been suggested to develop similarity measures. However, there are still much to be improved, because existing similarity measures simply utilize additional heuristic information and seldom reflect the global rating behaviors on items. This paper aims to improve the previous similarity measures by employing the information entropy of user ratings so that the user's global rating behavior on items can be reflected. The efficiency of the proposed method is examined through extensive experiments to demonstrate its superior performance over the previous similarity measures especially in small-scaled and sparse datasets.																	1868-5137	1868-5145				JAN	2020	11	1			SI		363	374		10.1007/s12652-019-01226-0													
J								Maclaurin symmetric mean aggregation operators based on t-norm operations for the dual hesitant fuzzy soft set	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Maclaurin symmetric mean; Aggregation operator; Multicriteria decision-making; Dual hesitant fuzzy soft set	GROUP DECISION-MAKING; SIMILARITY MEASURES; DISTANCE	The objective of this paper is to present a Maclaurin symmetric mean (MSM) operator to aggregate dual hesitant fuzzy (DHF) soft numbers. The salient feature of MSM operators is that it can reflect the interrelationship between the multi-input arguments. Under DHF soft set environment, we develop some aggregation operators named as DHF soft MSM averaging (DHFSMSMA) operator, the weighted DHF soft MSM averaging (WDHFSMSMA) operator, DHF soft MSM geometric (DHFSMSMG) operator, and the weighted DHF soft MSM geometric (WDHFSMSMG) operator. Further, some properties and the special cases of these operators are discussed. Then, by utilizing these operators, we develop an approach for solving the multicriteria decision-making problem and illustrate it with a numerical example. Finally, a comparison analysis has been done to analyze the advantages of the proposed operators.																	1868-5137	1868-5145				JAN	2020	11	1			SI		375	410		10.1007/s12652-019-01238-w													
J								MLP modeling for search advertising price prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Prediction modeling; Deep learning; MLP; Keyword ad		As the use of online and various smart devices spread, the use of online search engines became more active. As Internet shopping has evolved through online search engines, competition is under way to launch its link at the top of search engines to expose its links to prospective shoppers. This trend has contributed to the increase in advertising costs in the search advertising market. In this case, the value of the search keyword is generally calculated based on the frequency of the search keyword, however the search engine configures the price of the search keyword through the private auction method without disclosing the price in real time. Finally, it is difficult to reach the exact price and position by passive statistical method in order to predict the price of the search keyword. There is a growing demand for automation methodologies to perform this process quickly and efficiently. In this paper, we propose a Multi-Layer Perceptron (MLP) Neural Network modeling method that estimates bid prices of search keywords by collecting search keywords. MLP is used because it uses generalized delta learning rules and easily gets trained in less number of iterations. In this paper, we propose a MLP based prediction modeling to predict optimal bidding price of the keyword in a specific ranking of search engine.																	1868-5137	1868-5145				JAN	2020	11	1			SI		411	417		10.1007/s12652-019-01298-y													
J								Skeleton-based comparison of throwing motion for handball players	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Performance evaluation; Kinect V2; Skeleton; Dynamic time warping; Handball	TO-DISTAL SEQUENCE; VIDEO; VALIDITY; KINECT; RECOGNITION; RELIABILITY; KINEMATICS; CAPTURE; OBJECTS	The main goal of this work is to design an automated solution based on RGB-D data for quantitative analysis, perceptible evaluation and comparison of handball player's performance. To that end, we introduced a new RGB-D dataset that can be used for an objective comparison and evaluation of handball player's performance during throws. We filmed 62 handball players (44 beginners and 18 experts), who performed the same type of action, using a Kinect V2 sensor that provides RGB data, depth data and skeletons. Moreover, using skeleton data simulating 3D joint connections, we examined the main angles responsible for throwing performance in order to analyze individual skills of handball players (beginners against model and experts) relatively to throw actions. The comparison was performed statically (using only one frame) as well as dynamically during the entire throwing action. In particular, given the temporal sequence of 25 joints of each handball player, we adopted the dynamic time warping technique to compare the throwing motion between two athletes. The obtained results were found to be promising. Thus, the suggested markless solution would help handball coaches to optimize beginners' movements during throwing actions.																	1868-5137	1868-5145				JAN	2020	11	1			SI		419	431		10.1007/s12652-019-01301-6													
J								A new Genetic Algorithm based fusion scheme in monaural CASA system to improve the performance of the speech	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Speech segregation; GA fusion scheme; T-F binary mask; Voiced Mask; Unvoiced Mask	SOURCE SEPARATION; INTELLIGIBILITY; NOISE; ENHANCEMENT; SOUNDS; PHASE	This research work proposes a new Genetic Algorithm (GA) based fusion scheme to effectively fuse the Time-Frequency (T-F) binary mask of voiced and unvoiced speech. The perceptual cues such as correlogram, cross-correlogram and pitch are commonly used to obtain the T-F binary mask of voiced speech. Recently, researchers use speech onset and offset to segment the unvoiced speech from the noisy speech mixture. Most of the research work which uses speech onset and offset to represent the unvoiced speech, combine the segments of unvoiced speech with the segments of voiced speech to obtain the T-F binary mask. This research work effectively fuses the T-F binary mask of voiced and unvoiced speech, instead of combining the segments of voiced and unvoiced speech using a Genetic Algorithm (GA). Moreover, a new method is proposed in this research work to obtain a T-F binary mask from the segments of unvoiced speech. The performance of the proposed GA based fusion scheme is evaluated using measures such as quality and intelligibility. The experimental results show that the proposed system enhances the speech quality by increasing the SNR with an average value of 10.74 dB and decreases the noise residue with an average value of 26.15% when compared with noisy speech mixture and enhances the speech intelligibility by increasing the CSII, NCM and STOI with an average value of 0.22, 0.20 and 0.17 as compared with the conventional speech segregation systems.																	1868-5137	1868-5145				JAN	2020	11	1			SI		433	446		10.1007/s12652-019-01309-y													
J								A data-driven cyber-physical approach for personalised smart, connected product co-development in a cloud-based environment	JOURNAL OF INTELLIGENT MANUFACTURING										Data-driven design; Open architecture product; Smart product; Cyber physical system; Cloud; Mass personalisation	PLATFORM DESIGN; CONFIGURATION; TOOLKITS; CUSTOMIZATION; FRAMEWORK; SYSTEM	The rapid development of information and communication technology enables a promising market of information densely product, i.e. smart, connected product (SCP), and also changes the way of user-designer interaction in the product development process. For SCP, massive data generated by users drives its design innovation and somehow determines its final success. Nevertheless, most existing works only look at the new functionalities or values that are derived in the one-way communication by introducing novel data analytics methods. Few work discusses about an effective and systematic approach to enable individual user innovation in such context, i.e. co-development process, which sets the fundamental basis of the prevailing concept of data-driven design. Aiming to fill this gap, this paper proposes a generic data-driven cyber-physical approach for personalised SCP co-development in a cloud-based environment. A novel concept of smart, connected, open architecture product is hence introduced with a generic cyber-physical model established in a cloud-based environment, of which the interaction processes are enabled by co-development toolkits with smartness and connectedness. Both the personalized SCP modelling method and the establishment of its cyber-physical product model are described in details. To further demonstrate the proposed approach, a case study of a smart wearable device (i.e. i-BRE respiratory mask) development process is given with general discussions.																	0956-5515	1572-8145				JAN	2020	31	1					3	18		10.1007/s10845-018-1430-y													
J								Multi-objective optimization of an engine mount design by means of memetic genetic programming and a local exploration approach	JOURNAL OF INTELLIGENT MANUFACTURING										Structural optimization; Multi-objective optimization; Genetic programming; Finite element analysis; Decision making	FINITE-ELEMENT MODEL; MECHANICAL-PROPERTIES; DECISION-MAKING; SIMPLEX-METHOD; PARAMETERS; MINIMIZATION; PREDICTION; CONCRETE; SYSTEM; VALUES	This work addresses the optimization of an engine mount design from a multi-objective scenario. Our methodology is divided into three phases: phase one focuses on data collection through computer simulations. The objectives considered during the analyses are: total mass, first natural frequency and maximum von Mises stress. In phase two, a surrogate model by means of genetic programming is generated for each one of the objectives. Moreover, a local search procedure is incorporated into the overall genetic programming algorithm for improving its performance. Finally, in phase three, instead of steering the search to finding the approximate Pareto front, a local exploration approach based on a change in the weight space is used to lead a search into user defined directions turning the decision making more intuitive.																	0956-5515	1572-8145				JAN	2020	31	1					19	32		10.1007/s10845-018-1432-9													
J								A multi-objective hybrid evolutionary approach for buffer allocation in open serial production lines	JOURNAL OF INTELLIGENT MANUFACTURING										Buffer allocation problem; Multi-objective optimization; Hybrid meta-heuristics; Production lines	FACILITY LAYOUT PROBLEM; MODIFIED NSGA-II; UNRELIABLE PRODUCTION; GENETIC ALGORITHM; DESIGN; MODELS	The buffer allocation problem is of particular interest for operations management since buffers have a considerable impact on capacity improvement in production systems. In this study, the buffer allocation is solved to optimize two conflicting objectives of maximizing the average system production rate and minimizing total buffer size. A hybrid evolutionary algorithm-based simulation optimization approach is proposed for the multi-objective buffer allocation problem (MOBAP) in open serial production lines. As a search methodology, the Pareto optimal set is derived by hybrid approach using elitist non-dominated sorting genetic algorithm (NSGA-II) and a special version of a multi-objective simulated annealing. As an evaluative tool, discrete event simulation modeling is used to estimate the performance measures for the production systems. To demonstrate the efficacy of the proposed hybrid approach, a comparative study is provided for the MOBAP in various serial line configurations. The comparative results show that the hybrid method has a considerable potential to minimize the total buffer space by appropriately allocating space to each buffer while maximizing average production rate.																	0956-5515	1572-8145				JAN	2020	31	1					33	51		10.1007/s10845-018-1435-6													
J								Establishment of maintenance inspection intervals: an application of process mining techniques in manufacturing	JOURNAL OF INTELLIGENT MANUFACTURING										Maintenance management; Preventive maintenance; Inspection intervals; Shop-floor data; Process mining; Predictive and probabilistic models; Bayesian Networks	DECISION-MAKING; KNOWLEDGE FORMALIZATION; SELECTION; SYSTEMS; MODEL; STRATEGY	Reducing costs and increasing equipment availability (uptime) are among the main goals of industrial ventures. Well defined interval durations between maintenance inspections provide major support in achieving these targets. However, in order to establish the best interval length, process behavior, cycle times and related costs must be clearly known, and future estimates for these parameters must be established. This paper applies process mining techniques in developing a probabilistic model in Bayesian Networks integrated to predictive models. The probability of a given activity occurring in the probabilistic model output establishes the forecast boundaries for predictive models, responsible for estimating process cycle times. Availability (uptime) and cost functions are mathematically defined and an iterative process is performed in the length of intervals between maintenance inspections until the time and costs wasted are minimized and the best interval duration is found. The probabilistic model enables simulating changes in the event occurrence probability, allowing a number of different scenarios to be visualized and providing better support to managers in scheduling maintenance activities. The results show that production losses can be further reduced through optimally defined intervals between maintenance inspections.																	0956-5515	1572-8145				JAN	2020	31	1					53	72		10.1007/s10845-018-1434-7													
J								Recurrent feature-incorporated convolutional neural network for virtual metrology of the chemical mechanical planarization process	JOURNAL OF INTELLIGENT MANUFACTURING										Chemical mechanical planarization; Advanced process control; Virtual metrology; Recurrent neural network; Convolutional neural network	SEMICONDUCTOR; PREDICTION; REGRESSION; QUALITY	In semiconductor manufacturing, the chemical mechanical planarization (CMP) process produces higher thickness variability in the edge area of the wafer than that in the center area due to the characteristics of the polishing operation. To address this problem, advanced CMP equipment includes a function that controls the removal rate of each area. However, to take full advantage of this capability, effective advanced process control systems must be implemented with a virtual metrology (VM) model. However, the prediction performance of the VM model often deteriorates due to process drift. Here, we present a deep learning-based VM model that demonstrates high performance in the presence of nonlinear process drift. The proposed model combines a recurrent neural network and a convolutional neural network to extract time-dependent and time-independent features. A two-stage model training method is proposed that alternately updates the weights of each network to improve prediction performance. In the experiments using on-site CMP process data, the performance of the deep learning models exceeded that of standard machine learning models. The proposed model showed an 8.48% decrease in process variability relative to the best-performing machine learning model, which was elastic nets.																	0956-5515	1572-8145				JAN	2020	31	1					73	86		10.1007/s10845-018-1437-4													
J								A flexible machine vision system for small part inspection based on a hybrid SVM/ANN approach	JOURNAL OF INTELLIGENT MANUFACTURING										Machine vision (MV); Part classification; Support vector machines; Artificial neural networks; Flexible inspection; Identification; Sorting	VISUAL INSPECTION; CLASSIFICATION; RECOGNITION; SELECTION; FEATURES; SIZE	Machine vision inspection systems are often used for part classification applications to confirm that correct parts are available in manufacturing or assembly operations. Support vector machines (SVMs) and artificial neural networks (ANNs) are popular choices for classifiers. These supervised classifiers perform well when developed for specific applications and trained with known class images. Their drawback is that they cannot be easily applied to different applications without extensive retuning. Moreover, for the same application, they do not perform well if there are unknown class images. This paper proposes a novel solution to the above limitations of SVMs and ANNs, with the development of a hybrid approach that combines supervised and semi-supervised layers. To illustrate its performance, the system is applied to three different small part identification and sorting applications: (1) solid plastic gears, (2) clear plastic wire connectors and (3) metallic Indian coins. The ability of the system to work with different applications with minimal tuning and user inputs illustrates its flexibility. The robustness of the system is demonstrated by its ability to reject unknown class images. Four hybrid classification methods were developed and tested: (1) SSVM-USVM, (2) USVM-SSVM, (3) USVM-SANN and (4) SANN-USVM. It was found that SANN-USVM gave the best results with an accuracy of over 95% for all three applications. A software package known as FlexMVS for flexible machine vision system was written to illustrate the hybrid approach that enabled easy execution of the image conditioning, feature extraction and classification steps. The image library and database used in this study is available at .																	0956-5515	1572-8145				JAN	2020	31	1					103	125		10.1007/s10845-018-1438-3													
J								Smart recovery decision-making of used industrial equipment for sustainable manufacturing: belt lifter case study	JOURNAL OF INTELLIGENT MANUFACTURING										Sustainable manufacturing; Product recovery; End-of-life management; Smart manufacturing; Multi-objective optimization	PRODUCT RECOVERY; REVERSE LOGISTICS; INTEGRATED METHOD; OPTIMIZATION; MANAGEMENT; MODEL; STRATEGIES; COMPONENTS; ALGORITHM; SELECTION	End-of-Life (EOL) product recovery is proved to be an attractive way to achieve sustainable manufacturing while extending the producer's responsibility to closed-loop product service. However, it is still a challenge to provide flexible and smart recovery plans for industrial equipment at different periods of product service. In this paper, we investigate the smart recovery decision-making problem. We propose a system framework for the implementation of smart EOL management based on product condition monitoring. Different product-level EOL business strategies and component-level recovery options are suggested in this recovery decision support system. Then, multi-objective optimization models are formulated to identify the age-dependent recovery roadmap that best matches the product condition and meets the business goals. In order to achieve environmentally friendly recovery, both recovery profits and energy performances are optimized in our models. We conduct a case study of belt lifter used in the automobile assembly line. The Non-dominated Sorting Genetic Algorithm II is used to solve the proposed model. Numerical experiments validate our models and provide practical insights into flexible recovery business.																	0956-5515	1572-8145				JAN	2020	31	1					183	197		10.1007/s10845-018-1439-2													
J								An application to Stereolithography of a feature recognition algorithm for manufacturability evaluation	JOURNAL OF INTELLIGENT MANUFACTURING										Feature recognition (FR); Knowledge based engineering (KBE); Design for manufacturing (DfM); Additive manufacturing (AM); Stereolithography (SLA)	DESIGN	Additive manufacturing processes are experiencing extraordinary growth in present years. Concerning the production of goods by using this technology, expertise and know-how are today relevant while process simulation needs to be extensively validated before acquiring the necessary reliability, which is already achieved and established for a number of manufacturing processes. The objective of the present work is the development of a new algorithm for feature recognition, which is the first step towards an application of rules for manufacturability to digital models. The proposed approach was specifically conceived for design for additive manufacturing (DfAM). The method starts from a graph-based representation of geometric models that is the base for the definition of new and original geometrical entities. Then, an algorithm-based process has been identified and proposed for their detection. Eventually, these geometrical entities have been used for comparison with rules and constraints of DfAM in order to point out possible critical issues for manufacturability. A self-developed plugin software was implemented for the application of proposed procedure in Computer Aided Design systems. Several applications of a set of DfAM rules are provided and tested to validate the method by means of case studies. As a conclusion, such an application demonstrated the suitability of the approach for detections of features that are relevant to an early investigation into Stereolithography manufacturability. Presented approach could be helpful during early phases of product development for detecting critical manufacturing issues and thus for realising an assistant-tool that can help designers by displaying potential solutions to overcome them. Since the very first steps of product design, this integration of manufacturing knowledge allows for a reduction of a number of potential errors occurring during product fabrication and then for a decrease of required time for product development.																	0956-5515	1572-8145				JAN	2020	31	1					199	214		10.1007/s10845-018-1441-8													
J								Point-by-point prediction of cutting force in 3-axis CNC milling machines through voxel framework in digital manufacturing	JOURNAL OF INTELLIGENT MANUFACTURING										Cutting force prediction; Digital manufacturing; Voxel-based framework	SURFACE-ROUGHNESS; SELECTION	A new digital-based model is presented for the prediction of cutting forces in 3-axis CNC milling of surfaces. The model uses an algorithm to detect the work-piece/cutter intersection domain automatically for given cutter location, cutter and work-piece geometries. The algorithm uses a voxel-based representation for the workpiece and rasterized tool slice to detect the tool engagement. Furthermore, an analytical approach is used to calculate the cutting forces based on the discretized model. The results of model validation experiments on machining PMMA, Aluminum 6061 and 304 Stainless Steel are presented. Comparisons of the predicted and measured forces show that this digital approach can be used to accurately predict forces during machining.																	0956-5515	1572-8145				JAN	2020	31	1					215	226		10.1007/s10845-018-1442-7													
J								Intelligent approach for process modelling and optimization on electrical discharge machining of polycrystalline diamond	JOURNAL OF INTELLIGENT MANUFACTURING										Artificial neural network; Electrical discharge machining; Electrode wear rate; Material removal rate; Polycrystalline diamond; Radial basis function neural network	ARTIFICIAL NEURAL-NETWORK; GREY RELATIONAL ANALYSIS; EDM PROCESS PARAMETERS; MATERIAL REMOVAL RATE; MICRO-EDM; PERFORMANCE; PREDICTION; ALGORITHM; WEAR; SIZE	Polycrystalline diamond (PCD) is increasingly becomes an important material used in the industry for cutting tools of difficult-to-machine materials due to its excellent characteristics such as hardness, toughness and wear resistance. However, its applications are restricted because of the PCD material is difficult to machine. Therefore, electrical discharge machining (EDM) is an ideal method suitable for PCD materials due to its non-contact process nature. The performance of EDM, however, is significantly influenced by its process parameters and type of electrode. In this study, soft computing technique was utilized to optimize the performance of the EDM in roughing condition for eroding PCD with copper tungsten or copper nickel electrode. Central composite design with five levels of three machining parameters viz. peak current, pulse interval and pulse duration has been used to design the experimental matrix. The EDM experiment was conducted based on the design experimental matrix. Subsequently, the effectiveness of EDM on shaping PCD with copper tungsten and copper nickel was evaluated in terms of material removal rate (MRR) and electrode wear rate (EWR). It was found that copper tungsten electrode gave lower EWR, in comparison with the copper nickel electrode. The predictive model of radial basis function neural network (RBFNN) was developed to predict the MRR and EWR of the EDM process. The prominent predictive ability of RBFNN was confirmed as the prediction errors in terms of mean-squared error were found within the range of 6.47E-05 to 7.29E-06. Response surface plot was drawn to study the influences of machining parameters of EDM for shaping PCD with copper tungsten and copper nickel. Subsequently, moth search algorithm (MSA) was used to determine the optimal machining parameters, such that the MRR was maximized and EWR was minimized. Based on the obtained optimal parameters, confirmation test with the absolute error within the range of 1.41E-06 to 5.10E-05 validated the optimization capability of MSA.																	0956-5515	1572-8145				JAN	2020	31	1					227	247		10.1007/s10845-018-1443-6													
J								A data-driven approach for constructing the component-failure mode matrix for FMEA	JOURNAL OF INTELLIGENT MANUFACTURING										Failure mode and effects analysis; Component-failure mode matrix; Data mining; System reliability; Automotive industry	IDENTIFICATION; TEXT	Failure mode and effects analysis (FMEA) is one of the typical structured, systematic and proactive approaches for product or system failure analysis. A critical step in FMEA is identifying potential failure modes for product sub-systems, components, and processes, for which component-failure mode (CF) knowledge is necessarily needed as an important source of knowledge. However, this knowledge is usually acquired manually based on historical documents such as bills of material and failure analysis reports, which is a labor-intensive and time-consuming task, incurring inefficiency and plenty of mistakes. Nevertheless, few existing studies have developed an effective and intelligent approach to acquiring accurate CF knowledge automatically. To fill the gap, this paper proposes a method to construct the CF matrix automatically by mining unstructured and short quality problem texts and mapping as well as representing them as CF knowledge. Starting with mining the frequent itemsets of failure modes through Apriori algorithm, the method uses the semantic dictionary WordNet to find synonyms in the set of failure modes, based on which the standard set of failure modes is finally built. Subsequently, upon the previous work and components set, we design the component-failure mode matrix mining (CFMM) algorithm and apply it to establish the CF matrix from unstructured quality problem texts. Lastly, we examine the quality data of the seat module of an automobile company as a case study in order to validate the proposed method. The result shows that the failure mode extraction method with standardized features can extract failure modes more effectively than the FP-growth and K-means clustering methods. Meanwhile, the devised CFMM algorithm can extract more combinations of CF than the FP-growth method and build a richer CF matrix. Although different industries have distinct domain characteristics, our proposed method can be applicable not only to manufacturing but also to other fields needing FMEA to enhance product and system reliability.																	0956-5515	1572-8145				JAN	2020	31	1					249	265		10.1007/s10845-019-01466-z													
J								A Unified Approach to Consensus Control of Three-Link Manipulators	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Three-link manipulators; Consensus algorithms; Graph Laplacian; Complex Hurwitz polynomials; Consensus rate	MULTIAGENT SYSTEMS; SUFFICIENT CONDITIONS; SYNCHRONIZATION; STABILITY	We consider a consensus control problem for a set of three-link manipulators connected by digraphs. Assume that the control inputs of each manipulator are the torques on its links and they are generated by adjusting the weighted difference between the manipulator's states and those of its neighbor agents. Then, we propose a condition for adjusting the weighting coefficients in the control inputs, so that full consensus is achieved among the manipulators. By designing complex Hurwitz polynomials, we obtain a necessary and sufficient condition for achieving the consensus. Moreover, the discussion is extended to the case of designing convergence rate of consensus. Numerical examples are provided to illustrate the condition and the design conditions.																	0921-0296	1573-0409				JAN	2020	97	1					3	15		10.1007/s10846-019-01032-y													
J								Safety Control Method of Robot-Assisted Cataract Surgery with Virtual Fixture and Virtual Force Feedback	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Cataract surgery; Master-slave robot; Virtual fixture; Force feedback; Remote centre of motion; Safety control	LENS	Surgery is an effective means of treating cataracts and restoring vision. However, cataract surgery rate (CSR) in developing countries and regions is relatively low due to the lack of experienced high-level surgeons. In this paper, to reduce the reliance of surgery on physician experience and thereby increase CSR, a master-slave robotic system and safety control strategies with a virtual fixture and virtual force feedback are proposed to assist cataract surgery. First, the surgery is divided into four different stages with different robot control modes. Secondly, the virtual constraint area with virtual spring model in the operating stage is established, so that the doctor can distinguish the operation area where the end of the surgical instrument is located by feedback force. Thirdly, safety control algorithm guarantees that the surgical instrument strictly moves around the surgical incision point, which is regarded as a remote centre of motion, so that the cornea outside the incision point is not injured. Finally, the experimental results show that the proposed safety control strategy allows the robotic system to perform the procedure safely.																	0921-0296	1573-0409				JAN	2020	97	1					17	32		10.1007/s10846-019-01012-2													
J								Delay-Dependent Stability Analysis in Haptic Rendering	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Haptic devices; Stability; Linear matrix inequality	FUZZY-SYSTEMS; TIME; PASSIVITY; QUANTIZATION; INTERFACE; COULOMB; DEVICE	Nowadays haptic devices have lots of applications in virtual reality systems. While using a haptic device, one of the main requirements is the stable behavior of the system. An unstable behavior of a haptic device may damage itself and even may hurt its operator. Stability of haptic devices in the presence of inevitable time delay in addition to a suitable zero-order hold is studied in the presented paper, using two different methods. Both presented methods are based on Lyapunov-Krazuvskii functional. In the first method, a model transform is performed to determine the stability boundary, while the second approach is based on Free Weighing Matrices (FWMs). Delay-dependent stability criteria are determined by solving Linear Matrix Inequalities (LMIs). Results of these two methods are compared with each other and verified by simulations as well as experiments on a KUKA Light Weight Robot 4 (LWR4). It is concluded that using free weighing matrices leads to more unknown parameters and needs more calculation, but its results are less conservative.																	0921-0296	1573-0409				JAN	2020	97	1					33	45		10.1007/s10846-019-01017-x													
J								Development and Clinical Evaluation of a Posterior Active Walker for Disabled Children	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robotic walking aid; Gait intention from position of the trunk; Posterior active walker	GAIT; ANTERIOR; KINEMATICS; MOBILITY; ROBOT	This paper presents a novel posterior active walker designed for disabled children. The active walker adapts an existing posterior passive walker by motorising its back wheels and propelling them in accordance with intention of walking gait of the user. The gait intention of the user is determined from the position of user's trunk with respect to the walker frame using an array of sensors. The principle of working of the active walker is explained in this paper. A mathematical model for the active walker is developed and the same has been simulated for a control algorithm developed, which controls the walker in different possible conditions to achieve a comfortable and efficient propulsion. The simulation results present performance of the active walker. The walker has been practically implemented and the details of the same have been presented. The experimental functional evaluation of the walker has been done in the lab which matches the simulation result. Later the developed walker was assessed for its ability to track the user. Three healthy children are made to walk with the active walker on different surfaces and the test results show that the walker is able to effectively track the children. The developed walker was subjected to clinical evaluation which involved ten disabled children who were prescribed use of walker. Clinical study involved relative evaluation of effectiveness of passive and the active walkers. It was concluded from the clinical trials that the use of the developed active walker significantly reduces the energy spent by the user compared to the conventional passive walker.																	0921-0296	1573-0409				JAN	2020	97	1					47	65		10.1007/s10846-019-01009-x													
J								A Vision-Based Coordinated Motion Scheme for Dual-Arm Robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Dual-arm robots; Coordinated motion; NAO robot; Visual positioning	INVERSE KINEMATICS	With the rise of service robots, research on cooperation between two-arm robots has become increasingly important. In this paper, two NAO two-armed robots are used as the experimental platform and are combined with projective geometry, vision, robotics and other knowledge to carry out theoretical derivation and experiments on the coordinated movements of dual-arm robots. From the aspect of visual information processing, we analyse and solve the detailed target recognition process. Then, on this basis, we propose a set of complete coordinated motion control schemes. For object recognition, in this paper, we propose a highly adaptable linear stick recognition method. To solve the control flow of coordinated movement, we calculate the inverse kinematics of the unreachable pose of a single NAO manipulator by ignoring the degree of freedom of rotation around an end axis, and propose a trajectory planning method for the vertical constraint relationship between the tool and the workpiece plane in the coordinated manipulator movement. A comparison of the results of a simulation and a real experiment reveals that the trajectories of a workpiece clamped at the ends of the two robots' mechanical arms are roughly the same; consequently, the coordinated control scheme proposed in this paper is feasible. Moreover, the scheme proposed in this paper is sufficiently accurate to meet service robot applications in daily life. Because the joint active clearance of the NAO robot arm is large and its sensor sensitivity is high, clearance change can be used in the future to replace the force sensor for hybrid control.																	0921-0296	1573-0409				JAN	2020	97	1					67	79		10.1007/s10846-019-01035-9													
J								Coverage Path Planning for 2D Convex Regions	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Coverage path planning; Unmanned aerial vehicle; Drone survey; Computational geometry; Optimal path	UAV; SYSTEM	The number of two-dimensional surveying missions with unmanned aerial vehicles has dramatically increased in the last years. To fully automatize the surveying missions it is essential to solve the coverage path planning problem defined as the task of computing a path for a robot so that all the points of a region of interest will be observed. State-of-the-art planners define as the optimal path the one with the minimum number of flight lines. However, the connection path, composed by the path from the starting point to the region of interest plus the path from it to the ending point, is underestimated. We propose an efficient planner for computing the optimal edge-vertex back-and-forth path. Unlike previous approaches, we take into account the starting and ending points. In this article, we demonstrate the vertex-edge path optimality along with in-field experiments using a multirotor vehicle validating the applicability of the planner.																	0921-0296	1573-0409				JAN	2020	97	1					81	94		10.1007/s10846-019-01024-y													
J								Deep-Learning-Based Human Intention Prediction Using RGB Images and Optical Flow	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Human intention prediction; Deep learning; RGB image; Optical flow		A key technical issue for human intention prediction from observed human actions is how to discover and utilize the spatio-temporal patterns behind those actions. Inspired by the well-known two-stream architecture for action recognition, this paper proposes an approach for human intention prediction based on a two-stream architecture using RGB images and optical flow. Firstly, the action-start frame of each trial of a human action is determined by calculating the L2 distance of the positions of the human joints between frames of the skeleton data. Secondly, a spatial network and a temporal network are trained separately to predict human intentions using RGB images and optical flow, respectively. Both early concatenation fusion methods in the spatial network and sampling methods in the temporal network are optimized based on experiments. Finally, average fusion is used to fuse the prediction results from the spatial network and the temporal network. To verify the effectiveness of the proposed approach, a new dataset of human intentions behind human actions is introduced. This dataset contains RGB images, RGB-D images, and skeleton data of human actions of pitching a ball. Experiments show that the proposed approach can predict human intentions behind human actions with a prediction accuracy of 74% on the proposed dataset. The proposed approach is further evaluated on the Intention from Motion (IfM) dataset, a dataset of human intentions behind human actions of grasping a bottle. The proposed approach achieves a prediction accuracy of 77% on the IfM dataset. The proposed approach is effective in predicting human intentions behind human actions in different applications.																	0921-0296	1573-0409				JAN	2020	97	1					95	107		10.1007/s10846-019-01049-3													
J								Coordinate Descent Optimization for Winged-UAV Design	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Coordinate descent method; Optimization based design; UAV	MULTIDISCIPLINARY OPTIMIZATION; AIRCRAFT	In this paper, a powerful optimization framework is proposed to design highly efficient winged unmanned aerial vehicle (UAV) that is powered by electric motors. In the proposed approach, the design of key UAV parameters including both aerodynamic configurations, (e.g. wing span, sweep angle, chord, taper ratio, cruise speed and angle of attack) and the propulsion systems (e.g. propeller, motor and battery) are cast into an unified optimization problem, where the optimization objective is the design goal (e.g. flight range, endurance). Moreover, practical constraints are naturally incorporated into the design procedures as constraints of the optimization problem. These constraints may arise from the preliminary UAV shape and layout determined by industrial design, weight constraints, etc. The backend of the optimization based UAV design framework are highly accurate aerodynamic models and propulsion system models proposed in this paper and verified by actual experiment data. The optimization framework is inherently non-convex and involves both continuous variables (e.g. the aerodynamic configuration parameters) and discrete variables (e.g. propulsion system combinations). To solve this problem, a novel coordinate descent method is proposed. Trial designs show that the proposed method works rather efficiently, converging in a few iterations. And the returned solution is rather stable with different initial conditions. Finally, the entire approach is applied to design a quadrotor tail-sitter VTOL UAV. The designed UAV is validated by both CFD simulations and intensive real-world flight tests.																	0921-0296	1573-0409				JAN	2020	97	1					109	124		10.1007/s10846-019-01020-2													
J								Energy-Constrained Multi-UAV Coverage Path Planning for an Aerial Imagery Mission Using Column Generation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Coverage path planning; Multi-UAV missions; Column generation; Energy-constrained optimization	LINEAR-PROGRAMMING APPROACH; AREA COVERAGE; VEHICLES; ALGORITHM	This paper presents a new Coverage Path Planning (CPP) method for an aerial imaging mission with multiple Unmanned Aerial Vehicles (UAVs). In order to solve a CPP problem with multicopters, a typical mission profile can be defined with five mission segments: takeoff, cruise, hovering, turning, and landing. The traditional arc-based optimization approaches for the CPP problem cannot accurately estimate actual energy consumption to complete a given mission because they cannot account for turning phases in their model, which may cause non-feasible routes. To solve the limitation of the traditional approaches, this paper introduces a new route-based optimization model with column generation that can trace the amount of energy required for all different mission phases. This paper executes numerical simulations to demonstrate the effectiveness of the proposed method for both a single UAV and multiple UAV scenarios for CPP problems.																	0921-0296	1573-0409				JAN	2020	97	1					125	139		10.1007/s10846-019-01010-4													
J								Multi-UAV Trajectory Optimization Utilizing a NURBS-Based Terrain Model for an Aerial Imaging Mission	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-UAV trajectory optimization; Aerial imaging; Non-Uniform Rational B-Spline (NURBS); Terrain modeling; Vehicle routing problem	COVERAGE	Trajectory optimization precisely scanning an irregular terrain is a challenging problem since the trajectory optimizer needs to handle complex geometry topology, vehicle performance, and a sensor specification. To address these problems, this paper introduces a novel framework of a multi-UAV trajectory optimization method for an aerial imaging mission in an irregular terrain environment. The proposed framework consists of terrain modeling and multi-UAV trajectory optimization. The terrain modeling process employs a Non-Uniform Rational B-Spline (NURBS) surface fitting method based on point cloud information resulting from an airborne LiDAR sensor or other sensor systems. The NURBS-based surface model represents a computationally efficient terrain topology. In the trajectory optimization method, the framework introduces a multi-UAV vehicle routing problem enabling UAV to scan an entire area of interest, and obtains feasible trajectories based on given vehicle performance characteristics, and sensor specifications, and the approximated terrain model. The proposed multi-UAV trajectory optimization algorithm is tested by representative numerical simulations in a realistic aerial imaging environment, namely, San Diego and Death Valley, California.																	0921-0296	1573-0409				JAN	2020	97	1					141	154		10.1007/s10846-019-01027-9													
J								Decentralized MPC for UAVs Formation Deployment and Reconfiguration with Multiple Outgoing Agents	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-agent systems; Unmanned aerial vehicles; Decentralized model predictive control; Voronoi tessellation; Formation reconfiguration	AERIAL VEHICLE UAV; STRATEGY; DESIGN	This paper presents a new decentralized algorithm for the deployment and reconfiguration of a multi-agent formation in a convex bounded polygonal area when considering several outgoing agents. The system is deployed over a two-dimensional convex bounded area, each agent being driven by its own linear model predictive controller. At each time instant, the area is partitioned into Voronoi cells associated with each agent. Due to the movement of the agents, this partition is time-varying. The objective of the proposed algorithm is to drive the agents into a static configuration based on the Chebyshev center of each Voronoi cell. When some agents present a non-cooperating behavior (e.g. agents required for a different mission, faulty agents, etc.), they have to leave the formation by tracking a reference outside the system's workspace. The outgoing agents and their objective positions partition the convex bounded polygonal area into working regions. Each remaining agent will track a new objective point allowing it to avoid the trajectory of the outgoing agents. The computation of this objective position is based on the agent's safety region (i.e. the intersection of the contracted Voronoi cell and the contracted working region). When the outgoing agents have left the workspace, the remaining agents resume their deployment objective. Simulation results on a formation of a team of unmanned aerial vehicles are finally presented to validate the algorithm proposed in this paper when several agents leave the formation.																	0921-0296	1573-0409				JAN	2020	97	1					155	170		10.1007/s10846-019-01025-x													
J								Cooperative Aerial Manipulation with Decentralized Adaptive Force-Consensus Control	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Mobile manipulation; Adaptive force control; Multi-robot coordination; Multirotors	LOAD TRANSPORT; GEOMETRIC CONTROL; QUADROTOR UAVS	In this paper, we consider multiple quacopter aerial robots and develop a decentralized adaptive controller to cooperatively manipulate a payload. We assume that the mass of the payload is not available to the controller. The developed decentralized adaptive controller employs a consensus algorithm based on connected graphs to ensure that the estimated mass from every agent adds up-to the actual mass of the payload and each agent gets an equal share of the payload's mass. Our controller ensures that all quadcopters asymptotically converge to a constant reference velocity. It also ensures that all of the forces applied to the payload converges to desired set-points. Desired thrusts and attitude angles are computed from the control algorithms and a low-level PD controller is implemented to track the desired commands for each quadcopter. We validate the effectiveness of the controllers in numerical simulations.																	0921-0296	1573-0409				JAN	2020	97	1					171	183		10.1007/s10846-019-01048-4													
J								Distributed Reactive Model Predictive Control for Collision Avoidance of Unmanned Aerial Vehicles in Civil Airspace	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Collision avoidance; Distributed model predictive control; ICAO right of way	CONFLICT-RESOLUTION; AIRCRAFT; ALGORITHM	Safety in the operations of UAVs (Unmanned Aerial Vehicles) depends on the current and future reduction of technical barriers and on the improvements related to their autonomous capabilities. Since the early stages, aviation has been based on pilots and Air Traffic Controllers that take decisions to make aircraft follow their routes while avoiding collisions. RPA (Remotely Piloted Aircraft) can still involve pilots as they are UAVs controlled from ground, but need the definition of common rules, of a dedicated Traffic Controller and exit strategies in the case of lack of communication between the Ground Control Station and the aircraft. On the other hand, completely autonomous aircraft are currently banned from civil airspace, but researchers and engineers are spending great effort in developing methodologies and technologies to increase the reliability of fully autonomous flight in view of a safe and efficient integration of UAVs in the civil airspace. This paper deals with the design of a collision avoidance system based on a Distributed Model Predictive Controller (DMPC) for trajectory tracking, where anticollision constraints are defined in accordance with the Right of Way rules, as prescribed by the International Civil Aviation Organization (ICAO) for human piloted flights. To reduce the computational burden, the DMPC is formulated as a Mixed Integer Quadratic Programming optimization problem. Simulation results are shown to prove the effectiveness of the approach, also in the presence of a densely populated airspace.																	0921-0296	1573-0409				JAN	2020	97	1					185	203		10.1007/s10846-019-01047-5													
J								Control of Multi-Agent Collaborative Fixed-Wing UASs in Unstructured Environment	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Validation and verification; Multi-agent systems; Formation flight	COOPERATIVE CONTROL; AGENTS SUBJECT; SYSTEMS; TEAM; ALGORITHMS; AVOIDANCE; GUIDANCE; VEHICLES; GRAPHS	In recent years, the study of dynamics and control of swarming robots and aircraft has been an active research topic. Many multi-agent collaborative control algorithms have been developed and have been validated in simulations, however the technological and logistic complexity involved in validation of these algorithms in actual flight tests has been a major hurdle impeding more frequent and wider applications. This work presents robust navigation algorithms for multi-agent fixed-wing aircraft based on an adaptive moving mesh partial differential equations controlled by the free energy heat flow equation. Guidance, navigation, and control algorithms for control of multi-agent unmanned aerial system (UASs) were validated through actual flight tests, and the robustness of these algorithms were also investigated using different aircraft platforms. The verification and validation flight tests were conducted using two different fixed-wing platforms: A DG808 sail-plane with a 4m wingspan T-tail configuration and a Skyhunter aircraft utilizing a 2.4m wingspan and a twin-boom configuration. The developed swarm navigation algorithm uses a virtual leader guidance scheme and has been implemented and optimized using optimal control theory. Multi-scale moving point guidance has been developed and complimented by a linear quadratic regulator controller. Several flight tests have been successfully conducted and a system of systems including software and hardware was successfully validated and verified.																	0921-0296	1573-0409				JAN	2020	97	1					205	225		10.1007/s10846-019-01057-3													
J								Three Dimensional Collision Avoidance for Multi Unmanned Aerial Vehicles Using Velocity Obstacle	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Three-dimensional collision avoidance; Three-dimensional velocity obstacle method; Unmanned aerial vehicles	MANEUVERS; PROTOCOL	Recently, multi-UAV systems are attracting growing interests. It offers wide range of applications in civilian and military environment, carrying out dangerous missions that manned aircraft cannot offer. Currently, an important challenge is the collision avoidance algorithm. The idea is to use the collision avoidance algorithm to control the multi-UAV systems. This will guarantee the safety of the UAVs. The UAVs will complete the missions without colliding with any moving or static obstacles. This topic has motivated the development of various collision avoidance algorithms. In this paper, we proposed some improvements on the three-dimensional velocity obstacle algorithm proposed in Jenie et al. (J. Guid. Control Dyn. 39(10), 2312-2323 25). Our improvements are threefold. First, we indicate the limitations of the original 3D collision avoidance method and present the modifications on the algorithm. Second, we develop the velocity obstacle method to be capable of handling cube obstacles in 3D space. Third, a real flight test is conducted to verify the effectiveness of the proposed three-dimensional velocity obstacle method.																	0921-0296	1573-0409				JAN	2020	97	1					227	248		10.1007/s10846-019-01055-5													
J								Cloud-SPHERE: Towards Secure UAV Service Provision	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										UAV; IoT; Security; Safety; Service; HAMSTER; Cloud-SPHERE	INTERNET; THINGS; PRIVACY; NETWORKS	Unmanned Aerial Vehicles (UAVs) are gaining popularity in many fields and have now increased connectivity. Among others, security is considered one of the greatest challenges for UAV technology acceptance from the general public, more so when the UAV is connected to other vehicles and/or systems. In this paper, the first steps towards secure UAV service provision are presented through the Cloud-SPHERE platform, a security approach for integrating UAVs into the IoT ecosystem. UAV security and service provision are discussed and the prototype outlined. Preliminary service provision by UAVs using the MQTT protocol are presented.																	0921-0296	1573-0409				JAN	2020	97	1					249	268		10.1007/s10846-019-01046-6													
J								Analysis of Hannan consistent selection for Monte Carlo tree search in simultaneous move games	MACHINE LEARNING										Nash equilibrium; Extensive form games; Simultaneous moves; Zero sum; Hannan consistency		Hannan consistency, or no external regret, is a key concept for learning in games. An action selection algorithm is Hannan consistent (HC) if its performance is eventually as good as selecting the best fixed action in hindsight. If both players in a zero-sum normal form game use a Hannan consistent algorithm, their average behavior converges to a Nash equilibrium of the game. A similar result is known about extensive form games, but the played strategies need to be Hannan consistent with respect to the counterfactual values, which are often difficult to obtain. We study zero-sum extensive form games with simultaneous moves, but otherwise perfect information. These games generalize normal form games and they are a special case of extensive form games. We study whether applying HC algorithms in each decision point of these games directly to the observed payoffs leads to convergence to a Nash equilibrium. This learning process corresponds to a class of Monte Carlo Tree Search algorithms, which are popular for playing simultaneous-move games but do not have any known performance guarantees. We show that using HC algorithms directly on the observed payoffs is not sufficient to guarantee the convergence. With an additional averaging over joint actions, the convergence is guaranteed, but empirically slower. We further define an additional property of HC algorithms, which is sufficient to guarantee the convergence without the averaging and we empirically show that commonly used HC algorithms have this property.																	0885-6125	1573-0565				JAN	2020	109	1					1	50		10.1007/s10994-019-05832-z													
J								Rankboost plus : an improvement to Rankboost	MACHINE LEARNING										Ranking; Boosting; Ensemble methods; Rankboost		Rankboost is a well-known algorithm that iteratively creates and aggregates a collection of "weak rankers" to build an effective ranking procedure. Initial work on Rankboost proposed two variants. One variant, that we call Rb-d and which is designed for the scenario where all weak rankers have the binary range {0,1} has good theoretical properties, but does not perform well in practice. The other, that we call Rb-c, has good empirical behavior and is the recommended variation for this binary weak ranker scenario but lacks a theoretical grounding. In this paper, we rectify this situation by proposing an improved Rankboost algorithm for the binary weak ranker scenario that we call Rankboost+. We prove that this approach is theoretically sound and also show empirically that it outperforms both Rankboost variants in practice. Further, the theory behind Rankboost+ helps us to explain why Rb-d may not perform well in practice, and why Rb-c is better behaved in the binary weak ranker scenario, as has been observed in prior work.																	0885-6125	1573-0565				JAN	2020	109	1					51	78		10.1007/s10994-019-05826-x													
J								Combining Bayesian optimization and Lipschitz optimization	MACHINE LEARNING										Bayesian optimization; Global optimization; Lipschitz optimzation; Optimization	EFFICIENT GLOBAL OPTIMIZATION	Bayesian optimization and Lipschitz optimization have developed alternative techniques for optimizing black-box functions. They each exploit a different form of prior about the function. In this work, we explore strategies to combine these techniques for better global optimization. In particular, we propose ways to use the Lipschitz continuity assumption within traditional BO algorithms, which we call Lipschitz Bayesian optimization (LBO). This approach does not increase the asymptotic runtime and in some cases drastically improves the performance (while in the worst case the performance is similar). Indeed, in a particular setting, we prove that using the Lipschitz information yields the same or a better bound on the regret compared to using Bayesian optimization on its own. Moreover, we propose a simple heuristics to estimate the Lipschitz constant, and prove that a growing estimate of the Lipschitz constant is in some sense "harmless". Our experiments on 15 datasets with 4 acquisition functions show that in the worst case LBO performs similar to the underlying BO method while in some cases it performs substantially better. Thompson sampling in particular typically saw drastic improvements (as the Lipschitz information corrected for its well-known "over-exploration" pheonemon) and its LBO variant often outperformed other acquisition functions.																	0885-6125	1573-0565				JAN	2020	109	1					79	102		10.1007/s10994-019-05833-y													
J								Provable accelerated gradient method for nonconvex low rank optimization	MACHINE LEARNING											MATRIX COMPLETION; OPTIMAL RATES; BOUNDS	Optimization over low rank matrices has broad applications in machine learning. For large-scale problems, an attractive heuristic is to factorize the low rank matrix to a product of two much smaller matrices. In this paper, we study the nonconvex problem minU R-nxr g(U) = f (UUT) under the assumptions that f (X) is restricted mu-strongly convex and L-smooth on the set {X : X greater than or similar to 0, rank(X) <= r}. We propose an accelerated gradient method with alternating constraint that operates directly on the U factors and show that the method has local linear convergence rate with the optimal dependence on the condition number of root L/mu. Globally, our method converges to the critical point with zero gradient from any initializer. Our method also applies to the problem with the asymmetric factorization of X = UVT and the same convergence result can be obtained. Extensive experimental results verify the advantage of our method.																	0885-6125	1573-0565				JAN	2020	109	1					103	134		10.1007/s10994-019-05819-w													
J								Sum-product graphical models	MACHINE LEARNING										Sum product networks; Probabilistic graphical models; Density estimation; Deep learning; Exact inference; Density estimation	ALGORITHM; MIXTURES	This paper introduces a probabilistic architecture called sum-product graphical model (SPGM). SPGMs represent a class of probability distributions that combines, for the first time, the semantics of probabilistic graphical models (GMs) with the evaluation efficiency of sum-product networks (SPNs): Like SPNs, SPGMs always enable tractable inference using a class of models that incorporate context specific independence. Like GMs, SPGMs provide a high-level model interpretation in terms of conditional independence assumptions and corresponding factorizations. Thus, this approach provides new connections between the fields of SPNs and GMs, and enables a high-level interpretation of the family of distributions encoded by SPNs. We provide two applications of SPGMs in density estimation with empirical results close to or surpassing state-of-the-art models. The theoretical and practical results demonstrate that jointly exploiting properties of SPNs and GMs is an interesting direction of future research.																	0885-6125	1573-0565				JAN	2020	109	1					135	173		10.1007/s10994-019-05813-2													
J								Comparison of export and outward foreign direct investment models of Chinese enterprises based on quantitative algorithm	NEURAL COMPUTING & APPLICATIONS										Export trade; Outward foreign direct investment; Chinese enterprises	MULTINATIONAL-ENTERPRISES; TRADE	Since the beginning of the new millennium, the productivity differences among different enterprises within the industry, namely the heterogeneity of enterprises, have been incorporated into the general equilibrium trade model. At the same time, it brings new focus to trade theory: By analyzing the characteristics of individual enterprises, it analyzes the choice of organizational structure of individual enterprises. Outward foreign direct investment (OFDI) and international trade are the two most important international economic activities nowadays. The development of OFDI will have certain impact on export trade. The relationship between OFDI and export trade is different due to the specific national conditions of different countries. Against this background, we study the impact of China's expanding OFDI on export trade. We should make better use of OFDI to promote the development of China's export trade and improve the export-oriented economic mode that relies solely on exports to expand the international market. It not only has theoretical value, but also has practical guiding significance for the formulation of China's "going out" economic and trade policy. This study introduces heterogeneity into the international expansion behavior model of Chinese enterprises in the context of global value chain specialization. Taking the position of Chinese enterprises in the value chain as the location dimension, we have integrated and expanded a new concept of enterprise advantage based on the value chain status from the research category of international trade theory. In this paper, we introduce the behavior of competitors into the dimension of enterprise decision space and construct an endogenous dynamic equilibrium model of Chinese enterprises' export trade and OFDI. The purpose is to provide theoretical explanations and practical guidance for Chinese enterprises' export trade and endogenous optimization decision making in the process of deepening the division of labor in the global value chain.																	0941-0643	1433-3058				JAN	2020	32	1			SI		1	9		10.1007/s00521-018-3699-3													
J								Nested rings: a simple scalable ring-based ROADM structure for neural application computing in mega datacenters	NEURAL COMPUTING & APPLICATIONS										Networks; Architecture; Circuit-switched; Optical interconnects		A simple scalable reconfigurable optical add/drop multiplexer was developed for neural application computing in mega datacenters (DCs). Operations are simplified significantly using a new routing algorithm, while maintaining the contention-free nature. It can accommodate 4096 ports for future DCs.																	0941-0643	1433-3058				JAN	2020	32	1			SI		11	21		10.1007/s00521-018-3700-1													
J								PID controller optimized by genetic algorithm for direct-drive servo system	NEURAL COMPUTING & APPLICATIONS										Genetic algorithm; PID controller; Servo system; Parameters optimization	MODEL	The latest development trend of direct-drive electro-hydraulic servo technology is discussed. The working principle, system model and system control theory of an electro-hydraulic servo system are studied. The dynamic behavior of the direct-drive electro-hydraulic servo system is highly nonlinear, structure uncertainty. Considering that the standard PID controller cannot fulfill all the demands, it is necessary to use advanced means for compensation. A PID controller optimized by genetic algorithm for an electro-hydraulic servo system direct driven by a permanent magnet synchronous motor is proposed. The genetic algorithm is applied to optimize the parameters of the PID controller. The simulation and experiment research of one direct-drive electro-hydraulic servo system are carried out to verify the response properties of the proposed controller. The step signal tracking responses of the servo system with different parameters of PID controller are, respectively, reported. In addition, a feedforward PID controller using genetic algorithm optimization is also designed for the direct-drive servo system. The simulation and experiment results show that the feedforward PID controller using genetic algorithm optimization has good dynamic response characteristics in the electro-hydraulic servo system based on a direct-drive permanent magnet synchronous motor.																	0941-0643	1433-3058				JAN	2020	32	1			SI		23	30		10.1007/s00521-018-3739-z													
J								Research on structural dynamics in Chinese automobile standard citation network	NEURAL COMPUTING & APPLICATIONS										Standard citation network; Dynamics; Complex network theory; Topological evolution		China, who owns the largest automobile consuming market, is becoming the largest automobile manufacturing country. However, a gap still lies between China and other traditional automobile manufacturing countries in terms of design capability and production efficiency. Standardization can effectively promote technological innovation and industrial upgradation, which enhances the overall performance of the automobile industry. This paper aims at identifying the structural problem in the automobile standard citation network in China, since the citation relationship reflects the transmission and development of knowledge or technologies. To this end, a dynamic standard citation network model is developed for ease of extraction of standard data at any time points. A set of complex network metrics at both node and network level are chosen with rational explanations in the context of automobile industry. With the data collected from publicized Web sites, the topological evolution of this network is analyzed as well. We significantly show that the standard citation networks at different time periods are generally loosely connected and contain too many isolated nodes. Meanwhile, the critical nodes in the standard citation network also change dynamically. We suggest that these isolated standards should be integrated into the citation network through revision activities.																	0941-0643	1433-3058				JAN	2020	32	1			SI		31	39		10.1007/s00521-018-3740-6													
J								Research on the LSTM Mongolian and Chinese machine translation based on morpheme encoding	NEURAL COMPUTING & APPLICATIONS										Mongolian and Chinese machine translation; GRU-CRF algorithm; LSTM neural network; Neural machine translation		The neural machine translation model based on long short-term memory (LSTM) has become the mainstream in machine translation with its unique coding-decoding structure and semantic mining features. However, there are few studies on the Mongolian and Chinese neural machine translation combined with LSTM. This paper mainly studies the preprocessing of Mongolian and Chinese bilingual corpus and the construction of the LSTM model of Mongolian morpheme coding. In the corpus preprocessing stage, this paper presents a hybrid algorithm for the construction of word segmentation modules. The sequence that has not been annotated is treated semantically and labeled by a combination of gated recurrent unit and conditional random field. In order to learn more grammar and semantic knowledge from Mongolian corpus, in the model construction stage, this paper presents the LSTM neural network model based on morpheme coding to construct the encoder. This paper also constructs the LSTM neural network decoder to predict the Chinese decode. Experimental comparisons of sentences of different lengths according to the construction model show that the model has improved translation performance in dealing with long-term dependence problems.																	0941-0643	1433-3058				JAN	2020	32	1			SI		41	49		10.1007/s00521-018-3741-5													
J								The attribute reduction method modeling and evaluation based on flight parameter data	NEURAL COMPUTING & APPLICATIONS										Attribute reduction; Flight parameter data; Neighbourhood rough set; Factor analysis	ROUGH SET; QAR DATA	This article focuses on the flight parameter data attribute reduction modelling and evaluation problem. From a structural perspective, flight parameter data analysis has two mainly problems, dimensions and measures. To handle the problems, the attribute of the flight parameter should be reduced. The processed parameter data can be modeled to analyze the flight safety problems. This paper proposes an attribute reduction method with the flight parameter data of the landing phase, which is period the security incidents occurred most frequently. The study applies the neighbourhood rough set to attribute reduction. The proposed attribute reduction method was evaluated and compared with the attribute reduction of factor analysis. The result suggests that the proposed method has higher prediction accuracy.																	0941-0643	1433-3058				JAN	2020	32	1			SI		51	60		10.1007/s00521-018-3742-4													
J								Mutual authentication for vehicular network in complex and uncertain driving	NEURAL COMPUTING & APPLICATIONS										Mutual authentication; LTE-V; Vehicular network; Cloud service; Privacy preserving	PROTOCOL; SCHEME; SECURE; LTE; MANAGEMENT	With the rapid development of big data and cloud computing, vehicular is connected to the Internet in the complex and uncertain driving environment. The rapid growth of the types of services used by vehicles has made the problem of inefficient of traditional driving environment architecture more and more obvious. The vehicle has to register and remember a large number of usernames and passwords to each server. Authentication schemes for multi-server architectures have been proposed and applied to a wide range of areas, but there has been little research on the Internet of vehicles. The long-term evolution for vehicle (LTE-V) is a wireless network architecture and can be used for cooperative communication in vehicular network. Communications and authentication for LTE-V have the high request in complex and uncertain driving environment. To meet the needs of complex and uncertain driving environments, this paper proposes a novel mutual authentication and the key agreement scheme (LEANDER) under multi-server architecture. In this scheme, elliptic curve is used to reduce the computational complexity, and a more concise authentication method is constructed. Random anonymity supports multi-server for two-way authentication and key agreement, so as to effectively protect the privacy of the vehicle. Moreover, it can be use BAN logic to prove and analyze the effectiveness of this scheme. The performance analysis results show that the proposed mutual authentication scheme is effective and more secure than other state-of-the-art methods.																	0941-0643	1433-3058				JAN	2020	32	1			SI		61	72		10.1007/s00521-018-3743-3													
J								Research on topic discovery technology for Web news	NEURAL COMPUTING & APPLICATIONS										Topic discovery; Weight computation; Latent semantic analysis; Similarity		With the development of information technology, Web news has become the main way of information dissemination. Web news topic discovery is useful for users to quickly find valuable information and its research is constantly improved. Traditional topic discovery research is based on vector space model, but it has the defects such as high dimension and data sparsity. However, the latent semantic analysis can map the high-dimensional and sparse words to k-dimensional semantic space and improve the similarity of the news of the same topic by the semantic correlation between words. In this paper, Web news topic discovery is studied. First, the set of Web news text is vectored and the weight of each feature in the texts is calculated by improved TFIDF. After the original text vector set is analysed by latent semantic analysis, the semantic relation is fully exploited between the texts and the words, and the news topics are extracted by clustering approach. For the extraction of sub-topics, the co-occurrence of words is used to display the sub-topics. In essence, the sub-topic vector is established through these co-occurrence words. The experimental results show that the proposed method can effectively capture the current hot topics of Web news and related sub-topics. It is meaningful for the technology of information retrieval and data mining.																	0941-0643	1433-3058				JAN	2020	32	1			SI		73	83		10.1007/s00521-018-3744-2													
J								Research on algorithm for solving maximum independent set of semi-external data of large graph data	NEURAL COMPUTING & APPLICATIONS										Large image data; Semi-external storage; Independent set	SIMULATIONS	The maximum independent set algorithm for large-scale data semi-existing data is studied and the solving method of the largest independent set problem in large-map data is mainly analyzed in this paper. The specific research contents are mainly divided into semi-external map algorithm based on Greedy heuristic strategy, semi-external map algorithm based on swap and design, and implementation of semi-external graph algorithm processing function library. Experiments on a large number of real and artificially generated data sets show that the algorithm in this paper is very efficient both in time and in space. The largest independent set obtained by the algorithm can reach more than 96% of its theoretical upper bound for most of the data.																	0941-0643	1433-3058				JAN	2020	32	1			SI		85	91		10.1007/s00521-018-3779-4													
J								Research on large data set clustering method based on MapReduce	NEURAL COMPUTING & APPLICATIONS										MapReduce; Large data; Set clustering method	ALGORITHM	The similarities and differences between the K-means algorithm and the Canopy algorithm's MapReduce implementation are described in detail, and the possibility of combining the two to design a better algorithm suitable for clustering analysis of large data sets is analyzed in this paper. Different from the previous literature's improvement ideas for K-means algorithm, it proposes new ideas for sampling and analyzes the selection of relevant thresholds in this paper. Finally, it introduces the MapReduce implementation framework based on Canopy partitioning and filtering K-means algorithm and analyzes some pseudocode in this chapter. Finally, it briefly analyzes the time complexity of the algorithm in this paper.																	0941-0643	1433-3058				JAN	2020	32	1			SI		93	99		10.1007/s00521-018-3780-y													
J								Engineering vehicle management system based on the internet of things	NEURAL COMPUTING & APPLICATIONS										Transportation cost; Engineering vehicle; B; S architecture; Database management		To improve the efficiency of engineering vehicles to save cost and accelerate production operation, the Browser/Server (B/S) architecture is used, ASP.NET is used as a development tool, and Oracle is regarded as the background database management. The management system has good scalability and maintainability. The implementation of this system simplifies the engineering vehicle management process. The background and significance of the system development are described, and the technology used in the development system is explained. Moreover, the system is analysed and designed in detail, and the system is finally realized and tested. In the design and implementation of the system, seven functional modules of the system are completed, and the function of each functional module is designed and tested.																	0941-0643	1433-3058				JAN	2020	32	1			SI		101	107		10.1007/s00521-018-3781-x													
J								Optimization of stepwise clustering algorithm in backward trajectory analysis	NEURAL COMPUTING & APPLICATIONS										Backward trajectory; Clustering; Algorithm; Optimization	SOURCE IDENTIFICATION	In recent years, the backward trajectory model has been widely used in the research of meteorological and atmospheric environmental quality. This paper presents a comprehensive study on a stepwise clustering analysis algorithm in the clustering process of backward trajectory model and an application of the clustering analysis of single-particle backward trajectory in 2016 in Changchun City. This study starts with an analysis of the original stepwise clustering algorithm and its application to a clustering process of 8784 backward trajectories during 48 h in Changchun City as a benchmark test case. Then, two improvements are made in the algorithm: First, in the process of finding the optimal classification, the algorithm complexity is improved from original O(n(3)) to O(log(n)*n(2)) through algorithm improvement. The algorithm performance is enhanced by log(n) times. Second, in the process of re-establishing the classification, the algorithm complexity is improved from the original O(m*n(2)) to O(m*log(n)*n), that is another algorithm performance improvement by a factor of log(n). Therefore, the accumulative execution efficiency improvement through the algorithm optimization is 2*log(n) times, which has been further verified in the practical application in Changchun City.																	0941-0643	1433-3058				JAN	2020	32	1			SI		109	115		10.1007/s00521-018-3782-9													
J								Study of the impact mechanism of inter-organizational learning on alliance performance-with relationship capital as the mediator	NEURAL COMPUTING & APPLICATIONS										Inter-organizational learning; Relationship capital; Alliance performance; Intermediary effect	INNOVATION	In previous studies, most scholars merely focused on the direct effect of inter-organizational learning on alliance performance and the impact of relationship capital on the learning between alliance organizations, while ignoring the fact that inter-organizational learning may also exert a significant positive effect on relationship capital and further influence alliance performance through the intermediary of relationship capital. This article constructs a theoretical model of the indirect impact mechanism of the learning between alliance organizations on alliance performance with relationship capital as the mediator. Based on the questionnaire data of 210 alliance enterprises in China, this article empirically researches into the relationship among inter-organizational learning, relationship capital (trust and relationship commitment) and alliance performance. The findings suggest that in the specific management situations in China, the learning between alliance organizations can affect alliance performance not only in a direct manner, but also in an indirect manner through the intermediary of relationship capital. By summing up the empirical results, the article puts forward some suggestions on management practices to further enrich the understanding of the value of learning between alliance organizations.																	0941-0643	1433-3058				JAN	2020	32	1			SI		117	126		10.1007/s00521-018-3783-8													
J								Database resource integration of shared cloud platform based on RAC architecture	NEURAL COMPUTING & APPLICATIONS										RAC architecture; Cloud platform; Database; Resource integration		This paper focuses on the core problem of database resource integration mode. The theme of this paper is the construction and implementation of database resource integration model, and some legal and commercial constraints related to it, so the content of the research involves the integration of technology, integration system, the integration of the status analysis and other aspects. The basic concept, classification, structure system and basic principles of Oracle RAC are discussed. Combined with business needs, a solution based on Oracle RAC is proposed. We implemented the planning and deployment of shared storage, file storage and network setup based on Oracle RAC database. The automatic failover without user intervention is realized, which greatly improves the reliability of the database, and has a certain guiding significance for the application and construction of the enterprise database. First, the research status of database resources integration at home and abroad is expounded through literature analysis. Secondly, the model of database resource integration is put forward, and its realization is discussed. Thirdly, the status of database resource integration is analyzed. Finally, the problems to be solved are put forward in the end. Through the research of this topic, it provides some reference and basis for the database resource integration model. At the same time, it also makes a paving for the maximum elimination of information isolated island and promoting the sharing of database resources.																	0941-0643	1433-3058				JAN	2020	32	1			SI		127	138		10.1007/s00521-018-3784-7													
J								Demand-side management for smart grid networks using stochastic linear programming game	NEURAL COMPUTING & APPLICATIONS										Smart grid; Distributed demand-side management; Scheduling scheme; Stochastic linear programming game	SYSTEM; ARCHITECTURE	This paper analyzes the mode provisioning and scheduling, in light of the aggregation over distributed energy storage system for improving the interactions and energy trading decisions under the smart grid networks. Further a new smart power system equipped with energy storage devices yields efficiency and robustness in a novel structure, which can identify and react on the energy market equilibrium in a timely manner. An energy consumption and stochastic linear programming game in the distributed structure is proposed for the energy payments, so that scheduling for appliances and storage devices can be used here as well. Furthermore, it is easy to implement a proposed two-phase DSLPM (distributed stochastic linear programming management) algorithm to bring about optimality with both energy provider and users to approach payoff sharing under uncertainty. With the incomplete information, a price equilibrium scheme is proposed. Experimental results are shown to verify the consumed energy, payment, and convergence properties of the proposed models.																	0941-0643	1433-3058				JAN	2020	32	1			SI		139	149		10.1007/s00521-018-3787-4													
J								Development of secured data transmission using machine learning-based discrete-time partially observed Markov model and energy optimization in cognitive radio networks	NEURAL COMPUTING & APPLICATIONS										Machine learning; Wireless communication; Cognitive radio networks; Byzantine attack; eclat algorithm	ACCESS	The cognitive radio network (CR) is a primary and promising technology to distribute the spectrum assignment to an unlicensed user (secondary users) which is not utilized by the licensed user (primary user).The cognitive radio network frames a reactive security policy to enhance the energy monitoring while using the CR network primary channels. The CR network has a good amount of energy capacity using battery resource and accesses the data communication via the time-slotted channel. The data communication with moderate energy-level utilization during transmission is a great challenge in CR network security monitoring, since intruders may often attack the network in reducing the energy level of the PU or SU. The framework used to secure the communication is using the discrete-time partially observed Markov decision process. This system proposes a modern data communication-secured scheme using private key encryption with the sensing results, and eclat algorithm has been proposed for energy detection and Byzantine attack prediction. The data communication is secured using the AES algorithm at the CR network, and the simulation provides the best effort-efficient energy usage and security.																	0941-0643	1433-3058				JAN	2020	32	1			SI		151	161		10.1007/s00521-018-3788-3													
J								An approach of recursive timing deep belief network for algal bloom forecasting	NEURAL COMPUTING & APPLICATIONS										Algal bloom; Recursive time series deep belief network; Forecasting; Dynamic nonlinear process	WATER-QUALITY MODEL; PREDICTION	The forecasting methods of water bloom in existence are hard to reflect nonlinear dynamic change in algal bloom formation mechanism, leading to poor forecasting accuracy of bloom. To solve this problem, this paper deeply analyzes the generation process of algal bloom, introduces the recursive time series algorithm into the deep belief network model and improves the model structure and training algorithm, and proposes a forecasting method based on the recursive timed deep belief network model. The model introduces the current moments and historical time values of the characterization factors and influencing factors at the input layer, and increases the connection between the input layer and the hidden layer of the deep belief network. A recursive algorithm is used to establish the relationship between the current time value of the characterization factor and the historical time value of the characterization factor, and the connection between the current time value of the hidden layer and the influencing factor is increased. By re-extracting the characteristics of the hidden layer at each moment, and then fine tuning the network parameters by the BP neural network, a recursive timing deep belief network model is finally constructed. The results show that compared with the existing forecasting methods, this method can extract the characteristics of time series data more accurately and completely to deal with the dynamic nonlinear process and can further improve the forecast accuracy of algal blooms.																	0941-0643	1433-3058				JAN	2020	32	1			SI		163	171		10.1007/s00521-018-3790-9													
J								Subspace projection semi-real-valued MVDR algorithm based on vector sensors array processing	NEURAL COMPUTING & APPLICATIONS										DOA; Vector sensors array; SRV-MVDR; Half spectrum search	OF-ARRIVAL ESTIMATION; SPECTRUM; ESPRIT	The existing SRV-MVDR (semi-real-valued MVDR) algorithm is only applicable to the pressure sensors array and cannot distinguish between mirror radiation sources and real source (DOA fuzzy), and this paper presents a method of vector sensors array SRV-MVDR based on subspace projection. Compared with the existing SRV-MVDR, only half spectrum search is needed to solve the DOA fuzzy problem. No subsequent discrimination is needed. The data collected by vector sensors array are processed jointly by using the idea of dimensionality reduction so that it satisfies the processing condition of SRV-MVDR method. Theoretical analysis and computer simulation show that this method has robustness. At the same time, it is more suitable for low SNR and small snapshots and has broad prospects in practical engineering.																	0941-0643	1433-3058				JAN	2020	32	1			SI		173	181		10.1007/s00521-018-3791-8													
J								Parameter selection method for support vector machine based on adaptive fusion of multiple kernel functions and its application in fault diagnosis	NEURAL COMPUTING & APPLICATIONS										Support vector regression machine; Kernel function characteristics; Parameter selection; Fifth-degree cubature Kalman filter		A new model parameter selection method for support vector machine based on adaptive fusion of multiple kernel functions is proposed in this paper. Characteristics of local kernels, global kernels, mixtures of kernels and multiple kernels were analyzed. Fusion coefficients of the multiple kernel function, kernel function parameters and regression parameters are combined to form the parameters of the state vector. Thus, the model selection problem is transformed into a nonlinear system state estimation problem. Then, we use a fifth-degree cubature Kalman filter to estimate the parameters. In this way, we realize adaptive selection of the multiple kernel function weighted coefficient, the kernel parameters and the regression parameters. A simulation experiment was performed to interpret the PE process for fault diagnosis.																	0941-0643	1433-3058				JAN	2020	32	1			SI		183	193		10.1007/s00521-018-3792-7													
J								Research on evolutionary model of urban rail transit vulnerability based on computer simulation	NEURAL COMPUTING & APPLICATIONS										Passenger flow; Subway; Vulnerability; Evolutionary; Simulation	ADAPTIVE CAPACITY; ACCESSIBILITY	In order to overcome the vulnerability of the effect of large passenger flow, an improved method based on the vulnerability of large passenger flow was proposed, and 5-year (2013-2017) passenger flow techniques are applied. The urban rail transit safety vulnerability simulation model mainly has three modules. Module 1: Urban rail transit can respond to disturbances in time and make corresponding adjustments and adaptations. Module 2: Urban rail traffic can be restored to a completely normal state for certain disturbances. Module 3: Urban rail transit can be completed within a limited self-recovery and adjustment time, and the fragile state after disturbance can be restored to normal state in time. This section of urban rail transit safety vulnerability evolution model is the core of the algorithm is studied, and according to the model to design the best algorithm procedures, specific algorithm to run the program is shown in Fig. 1. The results of this paper can be used as a basis for solving safety problems. It can in turn help to avoid or reduce the occurrence of disasters and to ensure the safe, fast and efficient operation of subway. This work has significance in theory and practice.																	0941-0643	1433-3058				JAN	2020	32	1			SI		195	204		10.1007/s00521-018-3793-6													
J								Analysis of students' learning and psychological features by contrast frequent patterns mining on academic performance	NEURAL COMPUTING & APPLICATIONS										Contrast frequent patterns; Data mining; Learning feature; Psychological feature	ANALYTICS; RULES	In recent years, data mining techniques have been widely applied in education. However, studies on analyzing the similarity or difference of the same learning pattern in different student groups are still rare. In this study, a data mining method which combines the concepts of contrast sets mining and association rules mining is introduced. It could provide quantitative analysis for the similarity and difference of association rules obtained from the academic records datasets of multiple grades. On this basis, student psychological features are deduced without being sensitive to privacy. The work in this study can help educators understand the learning and psychological states of students in different grades, so as to formulate teaching plans that are more targeted to improve their academic performance.																	0941-0643	1433-3058				JAN	2020	32	1			SI		205	211		10.1007/s00521-018-3802-9													
J								Research on cold chain logistic service pricing-based on tripartite Stackelberg game	NEURAL COMPUTING & APPLICATIONS										Cold chain logistics; Stackelberg game; Three-phase pricing model; Non-cooperative game; Cooperative game	SUPPLY CHAIN; MANAGEMENT; QUALITY; CHOICE	Fresh e-commerce cannot be separated from cold chain logistics as a guarantee when supplying fresh agricultural products in different places. On the one hand, the high cost of cold chain logistics requires the cold chain logistic enterprises to price their services provided by cold chain logistic enterprises. On the other hand, it requires fresh e-commerce to reprice their products considering cold chain logistic cost. Whether the pricing strategies of both are proper affects the income of both sides, and also affects the consumers' willingness to pay. Based on Steinberg game model and benefit equilibrium analysis, a three-stage pricing model with third-party cold chain logistic enterprise as leader, fresh e-commerce company as follower and consumer as secondary follower is established. Through the analysis of cooperative game and non-cooperative game, the optimal pricing and the best income of the cold chain logistic enterprises and the fresh e-commerce enterprises in the process of using cold chain logistics are obtained. Taking two different types of fresh products as an example, this paper simulates two kinds of fresh products based on pricing model, compares the two strategies of cooperative game and non-cooperative game, probes into the change of profit between fresh e-commerce and cold chain enterprises in different price ranges and selects pricing strategy.																	0941-0643	1433-3058				JAN	2020	32	1			SI		213	222		10.1007/s00521-018-3803-8													
J								Design and feasibility study of roots-type power machine rotor based on numerical simulation	NEURAL COMPUTING & APPLICATIONS										Roots-type power machine; Rotor; Profile lines; Numerical simulation; Test		In this paper, a new type of low-pressure steam generator, roots-type power machine, is proposed to solve the shortcomings of medium- and low-temperature waste heat energy recovery. The rotor is systematically studied by means of theoretical analysis, numerical simulation and experimental verification. Firstly, the motion law of roots-type power machine is analyzed theoretically. According to the rotor profile characteristics, the rotor profile structure is divided and the parameter equation is established. In order to verify the feasibility of the designed rotor, the pressure field, velocity field and mass flow field inside the roots-type power machine where the rotor is located are analyzed by numerical simulation method. Finally, a test platform is built to test the power output characteristics, load characteristics and flow characteristics of the roots-type power machine. The feasibility of the rotor profile design method and the reliability of the rotor mathematical model are verified. The results show that the design of the rotor profile can meet the power generation requirements of the roots-type power machine. The rotor profile design method provides a new idea and strong theoretical guidance and basis for the design of the rotor of the subsequent roots-type power machine. Moreover, the development of the roots-type power machine will have a profound impact on the recycling and energy saving of low-temperature waste heat resources.																	0941-0643	1433-3058				JAN	2020	32	1			SI		223	234		10.1007/s00521-018-3804-7													
J								Multi-source knowledge integration based on machine learning algorithms for domain ontology	NEURAL COMPUTING & APPLICATIONS										Domain ontology; Thesaurus; Online encyclopedia; Similarity computing	EXTRACTION	In this paper, a new approach of automatic building for domain ontology based on machine learning algorithm is proposed, and by which the large-scale e-Gov ontology is built automatically. The advent of the knowledge graph era puts forward higher requirements for semantic search and analysis. Since traditional manual ontology construction requires the participation of domain experts in large-scale ontology construction, which will take time and considerable resources, and the ontology scale is also limited. The approach proposed in this paper not only makes up for the shortage of thesaurus description of the semantic relation between terms, but also takes advantage of the massive online encyclopedia knowledge and typical similarity algorithm in machine learning to fill the domain ontology automatically, so that the advantages of the two different knowledge sources are fully utilized and the system as a whole is gained. Ultimately, this may provide the foundation and support for the construction of knowledge graph and the semantic-oriented applications.																	0941-0643	1433-3058				JAN	2020	32	1			SI		235	245		10.1007/s00521-018-3806-5													
J								A new image encryption algorithm based on two-dimensional spatiotemporal chaotic system	NEURAL COMPUTING & APPLICATIONS										Spatiotemporal chaos; Coupled map lattices; Bit-level; Image encryption	SCHEME; SYNCHRONIZATION; CIPHERS; PERMUTATION; NETWORK	In this paper, a new image encryption algorithm based on the two-dimensional spatiotemporal chaotic system is proposed.This system mixes linear neighborhood coupling and the nonlinear chaotic map coupling of lattices, and it has more cryptographic features in dynamics than the system of coupled map lattices does. The two-dimensional coupled map lattices (2DCML) system is only a special case in this chaotic system. In addition, bit-level permutation is employed to strengthen security of the cryptosystem. Simulations have been carried out, and the results demonstrate that the proposed algorithm has properties of large key space, high sensitivity to key, strong resisting attack. So, it is more secure and effective algorithm for encryption of digital images.																	0941-0643	1433-3058				JAN	2020	32	1			SI		247	260		10.1007/s00521-018-3577-z													
J								Linear and nonlinear stochastic distribution for consensus problem in multi-agent systems	NEURAL COMPUTING & APPLICATIONS										Consensus problem; Multi-agent systems; Linear stochastic distribution; Nonlinear stochastic distribution	FINITE-TIME CONSENSUS; NEURAL-NETWORK; PROTOCOLS; COORDINATION; OPERATORS; AGENTS	This paper presents a linear and nonlinear stochastic distribution for the interactions in multi-agent systems (MAS). The interactions are considered for the agents to reach a consensus using hetero-homogeneous transition stochastic matrices. The states of the agents are presented as variables sharing information in the MAS dynamically. The paper studies the interaction among agents for the attainment of consensus by limit behavior from their initial states' trajectories. The paper provides a linear distribution of DeGroot model compared with a nonlinear distribution of change stochastic quadratic operators (CSQOs), doubly stochastic quadratic operators (DSQOs) and extreme doubly stochastic quadratic operators (EDSQOs) for a consensus problem in MAS. The comparison study is considered for stochastic matrix (SM) and doubly stochastic matrix (DSM) cases of the hetero-homogeneous transition stochastic matrices. In the case of SM, the work's results show that the DeGroot linear model converges to the same unknown limit while CSQOs, DSQOs and EDSQOs converge to the center. However, the results show that the linear of DeGroot and nonlinear distributions of CSQOs, DSQOs and EDSQOs converge to the center with DSM. Additionally, the case of DSM is observed to converge faster compared to that of SM in the case of nonlinear distribution of CSQOs, DSQOs and EDSQOs. In general, the novelty of this study is in showing that the nonlinear stochastic distribution reaches a consensus faster than all cases. In fact, the EDSQO is a very simple system compared to other nonlinear distributions.																	0941-0643	1433-3058				JAN	2020	32	1			SI		261	277		10.1007/s00521-018-3615-x													
J								Improved inception-residual convolutional neural network for object recognition	NEURAL COMPUTING & APPLICATIONS										DCNN; RCNN; Inception network; Residual network; Deep learning		Machine learning and computer vision have driven many of the greatest advances in the modeling of Deep Convolutional Neural Networks (DCNNs). Nowadays, most of the research has been focused on improving recognition accuracy with better DCNN models and learning approaches. The recurrent convolutional approach is not applied very much, other than in a few DCNN architectures. On the other hand, Inception-v4 and Residual networks have promptly become popular among computer the vision community. In this paper, we introduce a new DCNN model called the Inception Recurrent Residual Convolutional Neural Network (IRRCNN), which utilizes the power of the Recurrent Convolutional Neural Network (RCNN), the Inception network, and the Residual network. This approach improves the recognition accuracy of the Inception-residual network with same number of network parameters. In addition, this proposed architecture generalizes the Inception network, the RCNN, and the Residual network with significantly improved training accuracy. We have empirically evaluated the performance of the IRRCNN model on different benchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The experimental results show higher recognition accuracy against most of the popular DCNN models including the RCNN. We have also investigated the performance of the IRRCNN approach against the Equivalent Inception Network (EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the CIFAR-100 dataset. We report around 4.53, 4.49 and 3.56% improvement in classification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100 dataset respectively. Furthermore, the experiment has been conducted on the TinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing accuracy compared to the Inception Recurrent CNN, the EIN, the EIRN, Inception-v3, and Wide Residual Networks.																	0941-0643	1433-3058				JAN	2020	32	1			SI		279	293		10.1007/s00521-018-3627-6													
J								Evaluation of mechanical properties of concretes containing coarse recycled concrete aggregates using multivariate adaptive regression splines (MARS), M5 model tree (M5Tree), and least squares support vector regression (LSSVR) models	NEURAL COMPUTING & APPLICATIONS										Recycled aggregate concrete (RAC); Mechanical properties; Least squares support vector regression (LSSVR); M5 model tree (M5Tree); Multivariate adaptive regression splines (MARS)	COMPRESSIVE STRENGTH; DURABILITY PROPERTIES; PERFORMANCE; PREDICTION; CEMENT; BEHAVIOR; MACHINE; MODULUS; FUME	This paper investigates the application of three artificial intelligence methods, including multivariate adaptive regression splines (MARS), M5 model tree (M5Tree), and least squares support vector regression (LSSVR) for the prediction of the mechanical behavior of recycled aggregate concrete (RAC). A large and reliable experimental test database containing the results of 650 compressive strength, 421 elastic modulus, 152 flexural strength, and 346 splitting tensile strength tests of RACs with no pozzolanic admixtures assembled from the published literature was used to train, test, and validate the three data-driven-based models. The results of the model assessment show that the LSSVR model provides improved accuracy over the existing models in the prediction of the compressive strength of RACs. The results also indicate that, although all three models provide higher accuracy than the existing models in the prediction of the splitting tensile strength of RACs, only the performance of the LSSVR model exceeds those of the best-performing existing models for the flexural strength of RACs. The results of this study indicate that MARS, M5Tree, and LSSVR models can provide close predictions of the mechanical properties of RACs by accurately capturing the influences of the key parameters. This points to the possibility of the application of these three models in the pre-design and modeling of structures manufactured with RACs.																	0941-0643	1433-3058				JAN	2020	32	1			SI		295	308		10.1007/s00521-018-3630-y													
J								Analysis on the potential of an EA-surrogate modelling tandem for deep learning parametrization: an example for cancer classification from medical images	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Parametrization; Surrogate model; Evolutionary algorithms		The paper introduces a novel modality to efficiently tune the convolutional layers of a deep neural network (CNN) and an approach to also rank the importance of the involved hyperparameters. Evolutionary algorithms (EA) offer a flexible solution to this twofold issue, while the expensive simulations of the deep learner with the generated configurations are resolved by surrogate modelling. Three models have been used and evaluated as surrogates: random forests (RF), support vector machines (SVM) and Kriging. Sample convolutional configurations are generated by Latin hypercube sampling and have attached computed accuracy outcomes from real CNN runs. For the hyperparameter estimation task, the fitness of an individual from the EA associated with a surrogate model is subsequently derived from the CNN accuracy estimation on those variable values. With respect to the ranking and variable selection task, RF includes implicit variable selection, the SVM can be straightforwardly supported by a second EA, and Kriging offers a ranking based on the corresponding theta values. The estimated accuracy of the found hyperparameter values is compared with the true validation accuracy, and they are next used for the prediction on the test cases. The ranking of the variables for each of the three surrogate models is compared, and their influence is also revealed by response surface methodology. The experimental testing of the proposed EA-surrogate approaches is conducted on a real-world scenario of histopathological image interpretation in colorectal cancer diagnosis.																	0941-0643	1433-3058				JAN	2020	32	2			SI		313	322		10.1007/s00521-018-3709-5													
J								Combining feature engineering and feature selection to improve the prediction of methionine oxidation sites in proteins	NEURAL COMPUTING & APPLICATIONS										Protein prediction; Post-translational modification; Methionine oxidation; Predictive computational model	REDUCTION; MODELS	Methionine is a proteinogenic amino acid that can be post-translationally modified. It is now well established that reactive oxygen species can oxidise methionine residues within living cells. For a long time, it has been thought that such a modification represents merely an inevitable damage derived from aerobic metabolism. However, several authors have begun to contemplate a possible role for this methionine modification in cell signalling. During the last years, a number of proteomic studies have been carried out with the purpose of detecting proteins containing oxidised methionines. Although these proteomic works allow to pinpoint those methionines being oxidised, they are also arduous, expensive and time-consuming. For these reasons, computational approaches aimed at predicting methionine oxidation sites in proteins become an appealing alternative. In the current work, we address methionine oxidation prediction by combining computational intelligence methods with feature engineering and feature selection techniques to improve the efficacy of several machine learning models, while reducing the number of input characteristics needed to get high accuracy rates. We compare random forests, support vector machines, neural networks and flexible discriminant analysis models. Random forests give the best AUC (0.8124 +/- 0.0334)) and accuracy rates (0.7590 +/- 0.0551\documentclass[12pt]{minimal} by using only a reduced set of 16 characteristics. These results surpass the outcomes of previous works. In addition, we present an end-user script that has been developed to take a protein ID as an input and return a list with the oxidation state of all the methionine residues found in the analysed protein. Finally, to illustrate the applicability of this tool, we have selected the human alpha 1-antitrypsin protein as a case study. This protein was selected because it was not present among the set of proteins used to build up the predictive models but the protein has been well characterised experimentally in terms of methionine oxidation. The prediction returned by our script fully matches the empirical evidence. Out of the nine methionine residues found in this protein, our model predicts the oxidation of only two of them, M351 and M358, which have been reported, on the base of mass spectrometry analyses, to be particularly susceptible to oxidation.																	0941-0643	1433-3058				JAN	2020	32	2			SI		323	334		10.1007/s00521-018-3655-2													
J								Posture transition analysis with barometers: contribution to accelerometer-based algorithms	NEURAL COMPUTING & APPLICATIONS										Accelerometer; Barometer; Human activity recognition	PHYSICAL-ACTIVITY; PRESSURE SENSORS; RECOGNITION; GAIT; IDENTIFICATION; MOVEMENT; DISEASE; SYSTEM	Posture transitions are one of the most mechanically demanding tasks and are useful to evaluate the motor status of patients with motor impairments, frail individuals or the elderly, among others. So far, wearable inertial systems have been one of the most employed tools in the study of these movements due to their suitable size and weight, being non-invasive systems. These devices are mainly composed of accelerometers and, to a lesser extent, gyroscopes, magnetometers or barometers. Although accelerometers provide the most reliable measurement, detecting activities where a change of altitude is observed, such as some posture transitions, may require additional sensors to reliably detect these activities. In this work, we present an algorithm that combines the information of a barometer and an accelerometer to detect posture transitions and falls. In contrast to other works, we test different activities (where altitude is involved) in order to achieve a reliable classifier against false positives. Furthermore, by means of feature selection methods, we obtain optimal subsets of features for the accelerometer and barometer sensors to contextualise these activities. The selected features are tested through several machine learning classifiers, which are assessed with an evaluation data set. Results show that the inclusion of barometer features in addition to those obtained for an accelerometer clearly enhances the detection accuracy up to a 11%, in terms of geometric mean between sensitivity and specificity, compared to algorithms where only the accelerometer is used. Finally, we have also analysed the computer burden; in this sense, the usage of barometers, in addition to increase the accuracy, also reduces the computational resources required to classify a new pattern, as shown by a reduction in the number of support vectors.																	0941-0643	1433-3058				JAN	2020	32	2			SI		335	349		10.1007/s00521-018-3759-8													
J								Monte Carlo uncertainty analysis of an ANN-based spectral analysis method	NEURAL COMPUTING & APPLICATIONS										Sine-fitting methods; Spectral analysis; ADALINE; Digital measurement; Uncertainty; Monte Carlo		This work presents the uncertainty analysis of an artificial neural network (ANN)-based method, called multiharmonic ANN fitting method (MANNFM), which is able to obtain, at a metrological level, the spectrum of asynchronously sampled periodical signals. For sinusoidal and harmonic content signals, jitter and quantization noise contributions to uncertainty are considered in order to obtain amplitude and phase uncertainties using Monte Carlo method. The analysis performed identifies also both contributions to uncertainty for different parameters laboratory configurations. The analysis is performed simultaneously with our method and two others: discrete Fourier transform (DFT), for synchronously sampled signals, and multiharmonic sine-fitting method (MSFM), for asynchronously sampled signals, in order to compare them in terms of uncertainty. Regarding asynchronous methods, results show that MANNFM provides the same uncertainties than MSFM, with the advantage of a simpler implementation. Regarding asynchronous and synchronous methods comparison, results for sinusoidal signals show that MANNFM has the same uncertainty as DFT for amplitude and higher uncertainty for phase values; for signals with harmonic content, amplitude conclusions maintain but, regarding phase, both MANNFM and DFT uncertainties become closer as the frequency increases, which implies, in fact, that when synchronous sampling is not possible, spectrum analysis can be performed with asynchronous methods without incurring in uncertainty increases.																	0941-0643	1433-3058				JAN	2020	32	2			SI		351	368		10.1007/s00521-019-04169-x													
J								State estimation of zinc air batteries using neural networks	NEURAL COMPUTING & APPLICATIONS										Electrochemical impedance spectroscopy; State of health; State of charge; Artificial neural network		The main task of battery management systems is to keep the working area of the battery in a safe state. Estimation of the state of charge and the state of health is therefore essential. The traditional way uses the voltage level of a battery to determine those values. Modern metal air batteries provide a flat voltage characteristic which necessitates new approaches. One promising technique is the electrochemical impedance spectroscopy, which measures the AC resistance for a set of different frequencies. Previous approaches match the measured impedances with a nonlinear equivalent circuit, which needs a lot of time to solve a nonlinear least-squares problem. This paper combines the electrochemical impedance spectroscopy with neural networks to speed up the state estimation using the example of zinc air batteries. Moreover, these networks are trained with different subsets of the spectra as input data in order to determine the required number of frequencies.																	0941-0643	1433-3058				JAN	2020	32	2			SI		369	377		10.1007/s00521-018-3705-9													
J								A quadratic boundedness approach to a neural network-based simultaneous estimation of actuator and sensor faults	NEURAL COMPUTING & APPLICATIONS										Neural network; Fault diagnosis; Actuator fault; Sensor fault; Multiple fault estimation	CONTROL DESIGN; SYSTEMS; OBSERVER; DIAGNOSIS; INPUT; FTC	The paper is devoted to the problem of a neural network-based robust simultaneous actuator and sensor faults estimator design for the purpose of the fault diagnosis of nonlinear systems. In particular, the methodology of designing a neural network-based fault estimator is developed. The main novelty of the approach is associated with possibly simultaneous sensor and actuator faults under imprecise measurements. For this purpose, a linear parameter-varying description of a recurrent neural network is exploited. The proposed approach guaranties a predefined disturbance attenuation level and convergence of the estimator. In particular, it uses the quadratic boundedness approach to provide uncertainty intervals of the achieved estimates. The final part of the paper presents an illustrative example concerning the application of the proposed approach to the multitank system fault diagnosis.																	0941-0643	1433-3058				JAN	2020	32	2			SI		379	389		10.1007/s00521-018-3706-8													
J								Wind power ramp event detection with a hybrid neuro-evolutionary approach	NEURAL COMPUTING & APPLICATIONS										Wind power ramp events; Prediction; Neuro-evolutionary algorithms; Unbalanced classification problems	EXTREME LEARNING-MACHINE; SUPPORT VECTOR MACHINES; FEATURE-SELECTION; PREDICTION SYSTEMS; UNCERTAINTY; SPEED	In this paper, a hybrid system for wind power ramp events (WPREs) detection is proposed. The system is based on modeling the detection problem as a binary classification problem from atmospheric reanalysis data inputs. Specifically, a hybrid neuro-evolutionary algorithm is proposed, which combines artificial neural networks such as extreme learning machine (ELM), with evolutionary algorithms to optimize the trained models and carry out a feature selection on the input variables. The phenomenon under study occurs with a low probability, and for this reason the classification problem is quite unbalanced. Therefore, is necessary to resort to techniques focused on providing a balance in the classes, such as the synthetic minority over-sampling technique approach, the model applied in this work. The final model obtained is evaluated by a test set using both ELM and support vector machine algorithms, and its accuracy performance is analyzed. The proposed approach has been tested in a real problem of WPREs detection in three wind farms located in different areas of Spain, in order to see the spatial generalization of the method.																	0941-0643	1433-3058				JAN	2020	32	2			SI		391	402		10.1007/s00521-018-3707-7													
J								An Intelligent Transportation System to control air pollution and road traffic in cities integrating CEP and Colored Petri Nets	NEURAL COMPUTING & APPLICATIONS										Intelligent control systems; Complex Event Processing; Event processing languages; Formal methods; Petri Nets	MODELS	Air pollution generated by road traffic in large cities is a great concern in today's society since pollution has an important impact on human health, even causing premature deaths. To address the problem, this paper presents an Intelligent Transportation System model based on Complex Event Processing technology and Colored Petri Nets (CPNs). It takes into consideration the levels of environmental pollution and road traffic, according to the air quality levels accepted by the international recommendations as well as the handbook emission factors for road transport methodology. This proposal, therefore, tackles a common problem in today's large cities, where traffic restrictions must be applied due to environmental pollution. CPNs are used in this work as a tool to make decisions about traffic regulations, so as to reduce pollution levels.																	0941-0643	1433-3058				JAN	2020	32	2			SI		405	426		10.1007/s00521-018-3850-1													
J								Pooling spike neural network for fast rendering in global illumination	NEURAL COMPUTING & APPLICATIONS										Clustering-based dynamic learning; Global illumination; Sparse coding; Pooling spike neural network	MACHINE; NOISE; MODEL	The generation of photo-realistic images is a major topic in computer graphics. By using the principles of physical light propagation, images that are indistinguishable from real photographs can be generated. However, this computation is a very time-consuming task. When simulating the real behavior of light, images can take hours to be of sufficient quality. This paper proposes a bio-inspired architecture with spiking neurons for fast rendering in global illumination. The objective is to find the number of paths that are required for each image in order to be perceived identical to the visually converged one computed by the path tracing algorithm. The challenge is that the visually converged image is unknown so that we start from a very noisy image to converge toward the less noisy image. This architecture with functional parts of sparse encoding, dynamic learning, and decoding consists of a robust convergence measure on blocks. Different pooling strategies are performed in order to separate noise from signal in a deep learning process. The learning algorithm selects the most pertinent images using clustering dynamic learning. The system dynamic computes a learning parameter for each image based on its level of noise. The experiments are conducted on a global illumination set which contains a large number of images with different resolutions and noise levels computed using diffuse and specular rendering. With respect to the scenes with 512x512resolution, 3232 different images are used for learning and 9696 images are used for testing. For the scenes with 800x800resolution, the training and the testing data contain, respectively, 3760 and 6320 images. The result is a system composed from only two spike pattern association neurons that accurately predict the quality of images with respect to human psycho-visual scores. The pooling spike neural network has been compared with the support vector and fast relevance vector machines. The obtained results show that the proposed method gives promising efficiency in terms of accuracy (which is calculated as the mean square error on each block of the scenes and the variation of the actual thresholds of the perception models and the desired human psycho-visual scores) and less number of parameters.																	0941-0643	1433-3058				JAN	2020	32	2			SI		427	446		10.1007/s00521-018-3941-z													
J								Solving cloud vendor selection problem using intuitionistic fuzzy decision framework	NEURAL COMPUTING & APPLICATIONS										Cloud vendor; Decision making; Intuitionistic fuzzy set; Standard variance; Three-way VIKOR	VIKOR METHOD; MODEL; SET	This paper presents a new decision-making framework called cloud vendor selector (CVS) for effective selection of cloud vendors by mitigating the challenge of unreasonable criteria weight assignment and improper management of uncertainty. The CVS comprises of two stages where, in the first stage, decision-makers' intuitionistic fuzzy-valued preferences are aggregated using newly proposed extended simple Atanassov's intuitionistic weighted geometry operator. Further, in the second stage, criteria weights are estimated by using newly proposed intuitionistic fuzzy statistical variance method and finally, ranking of cloud vendor (CV) is done using newly proposed three-way VIKOR method under intuitionistic fuzzy environment which introduces neutral category along with cost and benefit for better understanding the nature of criteria. An illustrative example of CV selection is demonstrated to show the practicality and usefulness of the proposed framework. Finally, the strength and weakness of the proposal are realized from both theoretic and numeric context by comparison with other methods.																	0941-0643	1433-3058				JAN	2020	32	2			SI		589	602		10.1007/s00521-018-3648-1													
J								A Conflict-Driven Solving Procedure for Poly-Power Constraints	JOURNAL OF AUTOMATED REASONING										Constraint solving; Computer algebra; Root isolation; Satisfiability modulo theories; Conflict-driven learning	ALGORITHM	This paper studies the satisfiability problem of poly-power constraints (conjunctions of poly-power equations and inequalities), in which poly-powers are univariate nonlinear functions that extend integer exponents of polynomials to real algebraic exponents. To solve the poly-power constraint, we present a sound and complete procedure that incorporates conflict-driven learning with the exclusion algorithm for isolating positive roots of poly-powers. Furthermore, we introduce a kind of optimal interval-splitting, based on the Stern-Brocot tree and on binary rational numbers respectively, so that the operands occurring in the execution are chosen to be as simple as possible. The solving procedure, thereby, turns out to be promisingly efficient on randomly generated examples.																	0168-7433	1573-0670				JAN	2020	64	1					1	20		10.1007/s10817-018-09501-z													
J								ExpTime Tableaux with Global Caching for Hybrid PDL	JOURNAL OF AUTOMATED REASONING										Modal logic; Propositional dynamic logic; Tableau-based decision procedures; Global caching; Complexity-optimal tableaux	DECISION PROCEDURE; LOGIC	We present the first direct tableau decision procedure with the ExpTime complexity for HPDL (Hybrid Propositional Dynamic Logic). It checks whether a given ABox (a finite set of assertions) in HPDL is satisfiable. Technically, it combines global caching with checking fulfillment of eventualities and dealing with nominals. Our procedure contains enough details for direct implementation and has been implemented for the TGC2 (Tableaux with Global Caching) system. As HPDL can be used as a description logic for representing and reasoning about terminological knowledge, our procedure is useful for practical applications.																	0168-7433	1573-0670				JAN	2020	64	1					21	52		10.1007/s10817-018-09506-8													
J								Automating Free Logic in HOL, with an Experimental Application in Category Theory	JOURNAL OF AUTOMATED REASONING										Free logic; Classical higher-order logic; Category theory; Interactive and automated theorem proving		A shallow semantical embedding of free logic in classical higher-order logic is presented, which enables the off-the-shelf application of higher-order interactive and automated theorem provers for the formalisation and verification of free logic theories. Subsequently, this approach is applied to a selected domain of mathematics: starting from a generalization of the standard axioms for a monoid we present a stepwise development of various, mutually equivalent foundational axiom systems for category theory. As a side-effect of this work some (minor) issues in a prominent category theory textbook have been revealed. The purpose of this article is not to claim any novel results in category theory, but to demonstrate an elegant way to "implement" and utilize interactive and automated reasoning in free logic, and to present illustrative experiments.																	0168-7433	1573-0670				JAN	2020	64	1					53	72		10.1007/s10817-018-09507-7													
J								Priority Inheritance Protocol Proved Correct	JOURNAL OF AUTOMATED REASONING										Priority Inheritance Protocol; Formal correctness proof; Real-time systems; Isabelle; HOL		In real-time systems with threads, resource locking and priority scheduling, one faces the problem of Priority Inversion. This problem can make the behaviour of threads unpredictable and the resulting bugs can be hard to find. The Priority Inheritance Protocol is one solution implemented in many systems for solving this problem, but the correctness of this solution has never been formally verified in a theorem prover. As already pointed out in the literature, the original informal investigation of the Property Inheritance Protocol presents a correctness "proof" for an incorrect algorithm. In this paper we fix the problem of this proof by making all notions precise and implementing a variant of a solution proposed earlier. We also generalise the scheduling problem to the practically relevant case where critical sections can overlap. Our formalisation in Isabelle/HOL is based on Paulson's inductive approach to protocol verification. The formalisation not only uncovers facts overlooked in the literature, but also helps with an efficient implementation of this protocol. Earlier implementations were criticised as too inefficient. Our implementation builds on top of the small PINTOS operating system used for teaching.																	0168-7433	1573-0670				JAN	2020	64	1					73	95		10.1007/s10817-019-09511-5													
J								Politeness and Combination Methods for Theories with Bridging Functions	JOURNAL OF AUTOMATED REASONING										Satisfiability modulo theories; Combination method; Non-disjoint theories; Bridging functions	UNIONS	The Nelson-Oppen combination method is ubiquitous in Satisfiability Modulo Theories solvers. However, one of its major drawbacks is to be restricted to disjoint unions of theories. We investigate the problem of extending this combination method to particular non-disjoint unions of theories defined by connecting disjoint theories via bridging functions. A possible application is to solve verification problems expressed in a combination of data structures connected to arithmetic with bridging functions such as the length of lists and the size of trees. We present a sound and complete combination method a la Nelson-Oppen for the theory of absolutely free data structures, including lists and trees. This combination procedure is then refined for standard interpretations. The resulting theory has a nice politeness property, enabling combinations with arbitrary decidable theories of elements. In addition, we have identified a class of polite data structure theories for which the combination method remains sound and complete. This class includes all the subtheories of absolutely free data structures (e.g, the empty theory, injectivity, projection). Again, the politeness property holds for any theory in this class, which can thus be combined with bridging functions and arbitrary decidable theories of elements. This illustrates the significance of politeness in the context of non-disjoint combinations of theories.																	0168-7433	1573-0670				JAN	2020	64	1					97	134		10.1007/s10817-019-09512-4													
J								A Prover Dealing with Nominals, Binders, Transitivity and Relation Hierarchies	JOURNAL OF AUTOMATED REASONING										Automated proof systems; Tableaux; Modal logic; Hybrid logic	DECISION PROCEDURE; DESCRIPTION LOGIC; HYBRID LOGICS; COMPLEXITY; FRAGMENT	This work describes the Sibyl prover, an implementation of a tableau based proof procedure for multi-modal hybrid logic with the converse, graded and global modalities, and enriched with features largely used in description logics: transitivity and relation hierarchies. The proof procedure is provably terminating when the input problem belongs to an expressive decidable fragment of hybrid logic. After a description of the implemented proof procedure, the way how the implementation deals with the most delicate aspects of the calculus is explained. Some experimental results, run on sets of randomly generated problems as well as some hand-tailored ones, show only a moderate deterioration in the performances of the prover when the number of transitivity and inclusion axioms increase. Sibyl is compared with other provers (HTab, the hybrid logic prover whose expressive power is closer to Sibyl's one, and the first-order prover SPASS). The obtained results show that Sibyl has reasonable performances.																	0168-7433	1573-0670				JAN	2020	64	1					135	165		10.1007/s10817-019-09513-3													
J								Energy efficiency in CMOS power amplifier designs for ultralow power mobile wireless communication systems	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Efficiency; radio frequency; power amplifier; CMOS; wireless; 4G; 5G; system-on-chip	COMBINING TRANSFORMER; ENHANCEMENT; 1W	Wireless communication standards keep evolving so that the requirement for high data rate operation can be fulfilled. This leads to the efforts in designing high linearity and low power consumption radio frequency power amplifier (RFPA) to support high data rate signal transmission and preserving battery life. The percentage of the DC power of the transceiver utilized by the power amplifier (PA) depends on the efficiency of the PA, user data rate, propagation conditions, signal modulations, and communication protocols. For example, the PA of a WLAN transceiver consumes 49% of the overall efficiency from the transmitter. Hence, operating the PA with minimum power consumption without trading-off the linearity is vital in order to achieve the goal of fully integrated system-on-chip (SoC) solution for 4G and 5G transceivers. In this paper, the efficiency in CMOS PA is discussed through the review of multifarious efficiency enhancement techniques in CMOS PA design. This is categorized into the review of efficiency in fundamental classes of PA in which Class E achieves the highest efficiency of 67%, followed by complex architectures utilized to enhance the efficiency level of the PA in which the outphasing architecture achieved the highest efficiency of 60.7%.																	1300-0632	1303-6203					2020	28	1					1	16		10.3906/elk-1903-47													
J								Simulation and analysis of wind turbine radar echo based on 3-D scattering point model	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Experimental measurement; scattering point model; wind turbine radar echo	INTERFERENCE	Wind turbine (WT) arrays in wind farms can cause serious interference on nearby radar stations. This interference could be filtered out if wind turbine radar echo (WTRE) can be obtained accurately. Considering the singleness of in-field experiments, numerical simulation became the majority among such works, but few of them reached necessary accuracy. Therefore, we propose a solution method of WTRE based on three-dimensional (3-D) scattering point model. Firstly, we use the nonuniform rational B-spline to build the 3-D model of WT. Secondly, based on the method of moments (MoM), the Rao-Wilton-Gisson (RWG) basis function is adopted to discretize the integral area of WTRE into triangular elements, which ensures the continuity of induced current on the surface of WT. Taking the centers of triangular elements as scattering point source, we obtain the 3-D point scattering model of WT, which is then used to derive the echo equation of WT and eventually time-frequency domain waveform of WTRE. The result presents high accuracy comparing with the traditional method and scaled model experiments in an anechoic chamber. Further analysis indicates that the proposed method can be used to estimate important parameters of an operating WT accurately.																	1300-0632	1303-6203					2020	28	1					34	44		10.3906/elk-1901-134													
J								Design of a high performance narrowband low noise amplifier using an on-chip orthogonal series stacked differential fractal inductor for 5G applications	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Quality factor; noise figure; input matching; gain; inductance; source degeneration	COMMON-GATE; CMOS; LNA	Inductors play a crucial role in the design of radio frequency integrated circuits (RFICs) and they typically consume a considerably large area and have a low-quality factor at high frequencies. The employment of fractal structure in on-chip inductors helps in improving the quality factor and also reduces the overall area besides improving the inductance value. In this paper, an orthogonal series stacked differential fractal inductor is proposed and the same is used to design a low noise amplifier (LNA) for 5G band (27-30 GHz) applications. The proposed inductor is fabricated on a multilayer printed circuit board and the measurement results demonstrate twice the enhancement in inductance, 56% improvement in quality factor, and 33% reduction in series resistance when compared to the conventional series stacked fractal inductor for the equivalent on-chip area. The LNA using cascode topology with inductive source degeneration is designed and simulated at a center frequency of 28 GHz in 90 nm CMOS technology using the tool Advanced Design System. The inductors in the LNA are replaced with the proposed on-chip inductor for different layers, which contributes to high gain, better input matching, and low noise figure compared to the state-of-the-art LNAs.																	1300-0632	1303-6203					2020	28	1					45	60		10.3906/elk-elk-1903-185													
J								A hybrid model based on the convolutional neural network model and artificial bee colony or particle swarm optimization-based iterative thresholding for the detection of bruised apples	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Bruised apple; stem-calyx; convolutional neural network; artificial bee colony; particle swarm optimization	PERFORMANCE; ALGORITHMS	In this study, apple images taken with near-infrared (NIR) cameras were classified as bruised and healthy objects using iterative thresholding approaches based on artificial bee colony (ABC) and particle swarm optimization (PSO) algorithms supported by a convolutional neural network (CNN) deep learning model. The proposed model includes the following stages: image acquisition, image preprocessing, the segmentation of anatomical regions (stem-calyx regions) to be discarded, the detection of bruised areas on the apple images, and their classification. For this aim, by using the image acquisition platform with a NIR camera, a total of 1200 images at 6 different angles were taken from 200 apples, of which 100 were bruised and 100 healthy. In order to increase the success of detection and classification, adaptive histogram equalization (AHE), edge detection, and morphological operations were applied to the images in the preprocessing stage, respectively. First, in order to segment and discard the stem-calyx anatomical regions of the images, the CNN model was trained by using the preprocessed images. Second, the threshold value was determined by means of the ABC/PSO-based iterative thresholding approach on the images whose stem-calyx regions were discarded, and then the bruised areas on the images with no stem-calyx anatomical regions were detected by using the determined threshold value. Finally, the apple images were classified as bruised and healthy objects by using this threshold value. In order to illustrate the classification success of our approaches, the same classification experiments were reimplemented by directly using the CNN model alone on the preprocessed images with no ABC and PSO approaches. Experimental results showed that the hybrid model proposed in this paper was more successful than the CNN model in which ABC-and PSO-based iterative threshold approaches were not used.																	1300-0632	1303-6203					2020	28	1					61	79		10.3906/elk-1904-180													
J								A novel semisupervised classification method via membership and polyhedral conic functions	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Semisupervised classification; multiclass classification; membership functions; polyhedral conic functions		In real-world problems, finding sufficient labeled data for defining classification rules is very difficult. This paper suggests a new semisupervised multiclass classification method. In the initialization, new membership functions are defined by utilizing the labeled data's medoids and means. Then the unlabeled points are labeled with the class of the highest membership value. In the supervised learning phase, separation via the polyhedral conic functions (PCFs) approach is improved by using defined membership values in the linear programming problem. The suggested algorithm is tested on real-world datasets and compared with the state-of-the-art semisupervised methods. The results obtained indicate that the suggested algorithm is effective in classification and is worth studying.																	1300-0632	1303-6203					2020	28	1					80	92		10.3906/elk-1905-45													
J								Retinal vessel segmentation using modified symmetrical local threshold	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Retinal vessel segmentation; feature extraction; symmetrical local threshold; retinopathy	MATCHED-FILTER; BLOOD-VESSELS; IMAGES	Retinal vessel segmentation is important for the identification of many diseases including glaucoma, hypertensive retinopathy, diabetes, and hypertension. Moreover, retinal vessel diameter is associated with cardiovascular mortality. Accurate detection of blood vessels improves the detection of exudates in color fundus images, as well as detection of the retinal nerve, optic disc, or fovea. A retinal vessel is a darker stripe on a lighter background. Thus, the objective is very similar to the lane detection task for intelligent vehicles. A lane on a road is a light stripe on a darker background (i.e. asphalt). For lane detection, the symmetrical local threshold (SLT) is found to be the most robust feature extractor among the tested algorithms in the road marking (ROMA) dataset. Unfortunately, the SLT cannot be applied directly for retinal vessel segmentation. The SLT is a 1D filter and is designed for detecting vertical or close to vertical light stripes with predictable width. In this paper, the SLT is modified to detect dark stripes and four kernels, instead of one, are designed to detect both vertical and horizontal features of a retinal vessel with variable thickness. The proposed algorithm is tested using the High Resolution Fundus (HRF) image database and the accuracy is estimated to be 95.53%. Furthermore, when tested with the Digital Retinal Images for Vessel Segmentation (DRIVE) database, the accuracy is estimated to be 93.69%.																	1300-0632	1303-6203					2020	28	1					93	106		10.3906/elk-1902-81													
J								Detailed modeling of a thermoelectric generator for maximum power point tracking	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Thermoelectric generator; modeling; MPPT; MATLAB/Simulink	DC-DC CONVERTER; FUZZY-LOGIC; PHOTOVOLTAIC SYSTEM; PERFORMANCE; SIMULATION; DESIGN; MPPT	Thermoelectric generators (TEGs) are used in small power applications to generate electrical energy from waste heats. Maximum power is obtained when the connected load to the ends of TEGs matches their internal resistance. However, impedance matching cannot always be ensured. Therefore, TEGs operate at lower efficiency. For this reason, maximum power point tracking (MPPT) algorithms are utilized. In this study, both TEGs and a boost converter with MPPT were modeled together. Detailed modeling, simulation, and verification of TEGs depending on the Seebeck coefficient, the hot/cold side temperatures, and the number of modules in MATLAB/Simulink were carried out. In addition, a boost converter having a perturb and observation (P&O) MPPT algorithm was added to the TEG modeling. After the TEG output equations were determined, the TEG modeling was performed based on manufacturer data sheets. Thanks to the TEG model and the boost converter with P&O MPPT, the maximum power was tracked with a value of 98.64% and the power derived from the TEG was nearly unaffected by the load changes. The power outputs obtained from the system with and without MPPT were compared to emphasize the importance of MPPT. These simulation values were verified by using an experimental setup. Ultimately, the proposed modeling provides a system of TEGs and a boost converter having P&O MPPT.																	1300-0632	1303-6203					2020	28	1					124	139		10.3906/elk-1907-166													
J								Fuzzy c-Means Directional Clustering (FCMDC) algorithm using trigonometric approximation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Directional data; fuzzy directional clustering; trigonometric mean; angular distance	VON-MISES DISTRIBUTIONS; SIMULATION; MIXTURES; MODEL	Cluster analysis is widely used in data analysis. Statistical data analysis is generally performed on the linear data. If the data has directional structure, classical statistical methods cannot be applied directly to it. This study aims to improve a new directional clustering algorithm which is based on trigonometric approximation. The trigonometric approximation is used for both descriptive statistics and clustering of directional data. In this paper, the fuzzy clustering algorithms (FCD and FCM4DD) improved for directional data and the proposed method are carried out on some numerical and real data examples, and the simulation results are presented. Consequently, these results indicate that the fuzzy c-means directional clustering algorithm gives the better results from the points of the mean square error and the standard deviation for cluster centers.																	1300-0632	1303-6203					2020	28	1					140	152		10.3906/elk-1903-118													
J								Context-aware system for glycemic control in diabetic patients using neural networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Neural networks; diabetes; closed-loop control; insulin; convolutional neural networks	GLUCOSE; ENVIRONMENT; PREDICTION; ALGORITHM	Diabetic patients are quite hesitant in engaging in normal physiological activities due to difficulties associated with diabetes management. Over the last few decades, there have been advancements in the computational power of embedded systems and glucose sensing technologies. These advancements have attracted the attention of researchers around the globe developing automatic insulin delivery systems. In this paper, a method of closed-loop control of diabetes based on neural networks is proposed. These neural networks are used for making predictions based on the clinical data of a patient. A neural network feedback controller is also designed to provide a glycemic response by regulating the insulin infusion rate. An activity recognition model based on convolutional neural networks is also proposed for predicting the patient's current physical activity. Predictions from this model are transformed into a six-level code and are fed as input to the neural network glucose prediction model. Experimental results of the proposed system show good performance in keeping blood glucose levels in the nondiabetic range.																	1300-0632	1303-6203					2020	28	1					153	166		10.3906/elk-1903-137													
J								Consumer loans' first payment default detection: a predictive model	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Machine learning; default loan; first payment default; imbalanced class problem; oversampling; undersampling	CREDIT; RISK	A default loan (also called nonperforming loan) occurs when there is a failure to meet bank conditions and repayment cannot be made in accordance with the terms of the loan which has reached its maturity. In this study, we provide a predictive analysis of the consumer behavior concerning a loan's first payment default (FPD) using a real dataset of consumer loans with approximately 600,000 records from a bank. We use logistic regression, naive Bayes, support vector machine, and random forest on oversampled and undersampled data to build eight different models to predict FPD loans. A two-class random forest using undersampling yielded more than 86% on all performance measures: accuracy, precision, recall, and F1-score. The corresponding scores are even as high as 96% for oversampling. However, when tested on the real and balanced dataset, the performance of oversampling deteriorates as generating synthetic data for an extremely imbalanced dataset harms the training procedure of the algorithms. The study also provides an understanding of the reasons for nonperforming loans and helps to manage credit risks more consciously.																	1300-0632	1303-6203					2020	28	1					167	+		10.3906/elk-1809-190													
J								Fast texture classification of denoised SAR image patches using GLCM on Spark	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Classification; machine learning; synthetic aperture radar; cluster computing; naive Bayes; decision tree; random forest		Classification of a synthetic aperture radar (SAR) image is an essential process for SAR image analysis and interpretation. Recent advances in imaging technologies have allowed data sizes to grow, and a large number of applications in many areas have been generated. However, analysis of high-resolution SAR images, such as classification, is a time-consuming process and high-speed algorithms are needed. In this study, classification of high-speed denoised SAR image patches by using Apache Spark clustering framework is presented. Spark is preferred due to its powerful open-source cluster-computing framework with fast, easy-to-use, and in-memory analytics. Classification of SAR images is realized on patch level by using the supervised learning algorithms embedded in the Spark machine learning library. The feature vectors used as the classifier input are obtained using gray-level cooccurrence matrix which is chosen to quantitatively evaluate textural parameters and representations. SAR image patches used to construct the feature vectors are first applied to the noise reduction algorithm to obtain a more accurate classification accuracy. Experimental studies were carried out using naive Bayes, decision tree, and random forest algorithms to provide comparative results, and significant accuracies were achieved. The results were also compared with a state-of-the-art deep learning method. TerraSAR-X images of high-resolution real-world SAR images were used as data.																	1300-0632	1303-6203					2020	28	1					182	195		10.3906/elk-1904-7													
J								Applying deep learning models to structural MRI for stage prediction of Alzheimer's disease	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Alzheimer's disease diagnosis; dementia diagnosis; convolutional neural networks; deep learning; structural MRI	CLASSIFICATION; DIAGNOSIS; DEMENTIA	Alzheimer's disease is a brain disease that causes impaired cognitive abilities in memory, concentration, planning, and speaking. Alzheimer's disease is defined as the most common cause of dementia and changes different parts of the brain. Neuroimaging, cerebrospinal fluid, and some protein abnormalities are commonly used as clinical diagnostic biomarkers. In this study, neuroimaging biomarkers were applied for the diagnosis of Alzheimer's disease and dementia as a noninvasive method. Structural magnetic resonance (MR) brain images were used as input of the predictive model. T1 weighted volumetric MR images were reduced to two-dimensional space by several preprocessing methods for three different projections. Convolutional neural network (CNN) models took preprocessed brain images, and the training and testing of the CNN models were carried out with two different data sets. The CNN models achieved accuracy values around 0.8 for diagnosis of both Alzheimer's disease and mild cognitive impairment. The experimental results revealed that the diagnosis of patients with mild cognitive impairment was more difficult than that of patients with Alzheimer's disease. The proposed deep learning-based model might serve as an efficient and practical diagnostic tool when MRI data are integrated with other clinical tests.																	1300-0632	1303-6203					2020	28	1					196	210		10.3906/elk-1904-172													
J								Time series forecasting on multivariate solar radiation data using deep learning (LSTM)	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Deep learning; LSTM; solar radiation; time series	PREDICTION; NETWORK	Energy management is an emerging problem nowadays and utilization of renewable energy sources is an efficient solution. Solar radiation is an important source for electricity generation. For effective utilization, it is important to know precisely the amount from different sources and at different horizons: minutes, hours, and days. Depending on the horizon, two main classes of methods can be used to forecast the solar radiation: statistical time series forecasting methods for short to midterm horizons and numerical weather prediction methods for medium- to long-term horizons. Although statistical time series forecasting methods are utilized in the literature, there are a limited number of studies that utilize deep artificial neural networks. In this study, we focus on statistical time series forecasting methods for short-term horizons (1 h). The aim of this study is to discover the effect of using multivariate data on solar radiation forecasting using a deep learning approach. In this context, we propose a multivariate forecast model that uses a combination of different meteorological variables, such as temperature, humidity, and nebulosity. In the proposed model, recurrent neural network (RNN) variation, namely a long short-term memory (LSTM) unit is used. With an experimental approach, the effect of each meteorological variable is investigated. By hyperparameter tuning, optimal parameters are found in order to construct the best models that fit the global solar radiation data. We compared the results with those of previous studies and we found that the multivariate approach performed better than the previous univariate models did. In further experiments, the effect of combining the most effective parameters was investigated and, as a result, we observed that temperature and nebulosity are the most effective parameters for predicting future solar radiance.																	1300-0632	1303-6203					2020	28	1					211	223		10.3906/elk-1907-218													
J								An index-based joint multilingual/cross-lingual text categorization using topic expansion via BabelNet	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Topic expansion; topic model; distributional semantic model; word sense disambiguation		The majority of the state-of-the-art text categorization algorithms are supervised and therefore require prior training. Besides the rigor involved in developing training datasets and the requirement for repetition of training for different texts, working with multilingual texts poses additional unique challenges. One of these challenges is that the developer is required to have many different languages involved. Term expansion such as query expansion has been applied in numerous applications; however, a major drawback of most of these applications is that the actual meaning of terms is not usually taken into consideration. Considering the semantics of terms is necessary because of the polysemous nature of most natural language words. In this paper, as a specific contribution to the document index approach for text categorization, we present a joint multilingual/cross-lingual text categorization algorithm (JointMC) based on semantic term expansion of class topic terms through an optimized knowledge-based word sense disambiguation. The lexical knowledge in BabelNet is used for the word sense disambiguation and expansion of the topics' terms. The categorization algorithm computes the distributed semantic similarity between the expanded class topics and the text documents in the test corpus. We evaluate our categorization algorithm using a multilabel text categorization problem. The multilabel categorization task uses the JRC-Acquis dataset. The JRC-Acquis dataset is based on subject domain classification of the European Commission's EuroVoc microthesaurus. We compare the performance of the classifier with a model of it using the original class topics. Furthermore, we compare the performance of our classifier with two state-of-the-art supervised algorithms (each for multilingual and cross-lingual tasks) using the same dataset. Empirical results obtained on five experimental languages show that categorization with expanded topics shows a very wide performance margin when compared to usage of the original topics. Our algorithm outperforms the existing supervised technique, which used the same dataset. Cross-language categorization surprisingly shows similar performance and is marginally better for some of the languages.																	1300-0632	1303-6203					2020	28	1					224	237		10.3906/elk-1901-140													
J								A new biometric identity recognition system based on a combination of superior features in finger knuckle print images	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Entropy-based pattern; texture feature; genetic algorithm; biometric; finger knuckle print	LOCAL BINARY PATTERNS; FACE RECOGNITION; IRIS; IDENTIFICATION; ORIENTATION	Biometric methods are among the safest and most secure solutions for identity recognition and verification. One of the biometric features with sufficient uniqueness for identity recognition is the finger knuckle print (FKP). This paper presents a new method of identity recognition and verification based on FKP features, where feature extraction is combined with an entropy-based pattern histogram and a set of statistical texture features. The genetic algorithm (GA) is then used to find the superior features among those extracted. After extracting superior features, a support vector machine-based feedback scheme is used to improve the performance of the biometric system. Two datasets called Poly-U FKP and FKP are used for performance evaluation. The proposed method managed to achieve 94.91% and 98.5% recognition rates on the Poly-U FKP and FKP datasets and outperformed all of the existing methods in this respect. These results demonstrate the potential of this method as a simple yet effective solution for FKP-based identity recognition.																	1300-0632	1303-6203					2020	28	1					238	252		10.3906/elk-1906-12													
J								Automatic characterization of copy number polymorphism using high throughput sequencing	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Genomics; copy number polymorphism; whole genome sequencing; containers	STRUCTURAL VARIATION; SEGMENTAL DUPLICATIONS; HUMAN-GENOME; COMBINATORIAL ALGORITHMS; DIVERSITY; MAP; INSERTIONS; STRATEGIES; EVOLUTION; DISCOVERY	Genome structural variation, broadly defined as alterations longer than 50 bp, are important sources for genetic variation among humans, including those that cause complex diseases such as autism, developmental delay, and schizophrenia. Although there has been considerable progress in characterizing structural variation since the beginnings of the 1000 Genomes Project, one form of structural variation called segmental duplications (SDs) remained largely understudied in large cohorts. This is mostly because SDs cannot be accurately discovered using the alignment files generated with standard read mapping tools. Instead, they can only be found when multiple map locations are considered. There is still a single algorithm available for SD discovery, which includes various tools and scripts that are not portable and are difficult to use. Additionally, this algorithm relies on a priori information for regions where no structural variations are discovered in large number of genomes. Therefore, there is a need for fully automated, portable, and user-friendly tools to make SD characterization a part of genome analyses. Here we introduce such an algorithm and efficient implementation, called mrCaNaVaR, that aims to fill this gap in genome analysis toolbox.																	1300-0632	1303-6203					2020	28	1					253	261		10.3906/elk-1903-135													
J								Lung cancer subtype differentiation from positron emission tomography images	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Machine learning; PET; lung cancer; texture analysis	FDG-PET; CLASSIFICATION; RADIOMICS; PROGNOSIS; FEATURES	Lung cancer is one of the deadly cancer types, and almost 85% of lung cancers are nonsmall cell lung cancer (NSCLC). In the present study we investigated classification and feature selection methods for the differentiation of two subtypes of NSCLC, namely adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). The major advances in understanding the effects of therapy agents suggest that future targeted therapies will be increasingly subtype specific. We obtained positron emission tomography (PET) images of 93 patients with NSCLC, 39 of which had ADC while the rest had SqCC. Random walk segmentation was applied to delineate three-dimensional tumor volume, and 39 texture features were extracted to grade the tumor subtypes. We examined 11 classifiers with two different feature selection methods and the effect of normalization on accuracy. The classifiers we used were the k-nearest-neighbor, logistic regression, support vector machine, Bayesian network, decision tree, radial basis function network, random forest, AdaBoostM1, and three stacking methods. To evaluate the prediction accuracy we performed a leave-one-out cross-validation experiment on the dataset. We also considered optimizing certain hyperparameters of these models by performing 10-fold cross-validation separately on each training set. We found that the stacking ensemble classifier, which combines a decision tree, AdaBoostM1, and logistic regression methods by a metalearner, was the most accurate method for detecting subtypes of NSCLC, and normalization of feature sets improved the accuracy of the classification method.																	1300-0632	1303-6203					2020	28	1					262	274		10.3906/elk-1810-154													
J								On the automorphisms and isomorphisms of MDS matrices and their efficient implementations	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										MDS matrix; branch number; block cipher	GENERATE	In this paper, we explicitly define the automorphisms of MDS matrices over the same binary extension field. By extending this idea, we present the isomorphisms between MDS matrices over F-2m and MDS matrices over F-2mt, where t >= 1 and m > 1, which preserves the software implementation properties in view of XOR operations and table lookups of any given MDS matrix over F-2m. Then we propose a novel method to obtain distinct functions related to these automorphisms and isomorphisms to be used in generating isomorphic MDS matrices (new MDS matrices in view of implementation properties) using the existing ones. The comparison with the MDS matrices used in AES, ANUBIS, and subfield-Hadamard construction shows that we generate an involutory 4 x 4 MDS matrix over F-28 (from an involutory 4 x 4 MDS matrix over F-24) whose required number of XOR operations is the same as that of ANUBIS and the subfield-Hadamard construction, and better than that of AES. The proposed method, due to its ground field structure, is intended to be a complementary method for the current construction methods in the literature.																	1300-0632	1303-6203					2020	28	1					275	287		10.3906/elk-1906-151													
J								A novel S-box-based postprocessing method for true random number generation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Ring oscillator; random number; postprocessing; S-box; statistical test	FPGA	The quality of randomness in numbers generated by true random number generators (TRNGs) depends on the source of entropy. However, in TRNGs, sources of entropy are affected by environmental changes and this creates a correlation between the generated bit sequences. Postprocessing is required to remove the problem created by this correlation in TRNGs. In this study, an S-box-based postprocessing structure is proposed as an alternative to the postprocessing structures seen in the published literature. A ring oscillator (RO)-based TRNG is used to demonstrate the use of an S-box for postprocessing and the removal of correlations between number sequences. The statistical properties of the numbers generated through postprocessing are obtained according to the entropy, autocorrelation, statistical complexity measure, and the NIST 800.22 test suite. According to the results, the postprocessing successfully removed the correlation. Moreover, the data rate of the bit sequence generated by the proposed postprocessing is reduced to 2/3 of its original value at the output.																	1300-0632	1303-6203					2020	28	1					288	301		10.3906/elk-1906-194													
J								Filter design for small target detection on infrared imagery using normalized-cross-correlation layer	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Small target detection; filter design; normalized-cross-correlation; convolutional neural networks	TRACKING	In this paper, we introduce a machine learning approach to the problem of infrared small target detection filter design. For this purpose, similar to a convolutional layer of a neural network, the normalized-cross-correlational (NCC) layer, which we utilize for designing a target detection/recognition filter bank, is proposed. By employing the NCC layer in a neural network structure, we introduce a framework, in which supervised training is used to calculate the optimal filter shape and the optimum number of filters required for a specific target detection/recognition task on infrared images. We also propose the mean-absolute-deviation NCC (MAD-NCC) layer, an efficient implementation of the proposed NCC layer, designed especially for FPGA systems, in which square root operations are avoided for real-time computation. As a case study we work on dim-target detection on midwave infrared imagery and obtain the filters that can discriminate a dim target from various types of background clutter, specific to our operational concept.																	1300-0632	1303-6203					2020	28	1					302	317		10.3906/elk-1807-287													
J								Nonlocal means estimation of intrinsic mode functions for speech enhancement	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Speech enhancement; denoising; nonlocal means; empirical mode decomposition	SPECTRUM; NOISE	The main aim of this paper is to introduce a new approach to enhance speech signals by exploring the advantages of nonlocal means (NLM) estimation and empirical mode decomposition. NLM, a patch-based denoising method, is extensively used for two-dimensional signals like images. However, its use for one-dimensional signals has been attracting more attention recently. The NLM-based approach is quite useful for removing low-frequency noises based on nonlocal similarities present among samples of the signal. However, there is an issue of under averaging in the high-frequency regions. The temporal and spectral characteristics of the speech signal are changing markedly over time. Thus NLM is conventionally not effective to remove the noise components from the speech signal, unlike image denoising. To address this issue, initially, the speech signal is decomposed into oscillatory components called intrinsic mode functions (IMFs) by using a temporal decomposition technique known as the sifting process. Each IMF represents signal information at a certain scale or frequency band. The IMFs do not have abrupt power spectral changes over time. The decomposed IMFs are processed using NLM estimation based on nonlocal similarities for better speech enhancement. The simulation result shows that the proposed method gives better performance in terms of subjective and objective quality measures. Its performance is evaluated for white, factory, and babble noises at different signal to noise ratios.																	1300-0632	1303-6203					2020	28	1					318	330		10.3906/elk-1901-86													
J								Sensor anomaly detection in the industrial internet of things based on edge computing	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Industrial internet of things; sensors anomaly detection; edge computing	CLOUD	In the industrial internet of things (IIoT), because thousands of pieces of hardware, instruments, and various controllers are involved, the core problem is the sensors. Detection using sensors is the bottom line of the IIoT, directly affecting the detection accuracy and control indicators of the IIoT system. However, when a large number of real-time data generated by IIoT devices are transferred to cloud computing centers, large-scale data will inevitably bring computing load, which will affect the computing speed of cloud computing centers and increase the computing load of cloud computing data centers. These factors directly lead to instability and delay in sensor data collected in real time in the IIoT. In this paper, a sensor outlier detection algorithm based on edge calculation is proposed. Firstly, focusing on the problem of the large amount of data in terminal equipment of the IIoT, the edge technology method of data compression is used to optimize the compression of sensor data, and different thresholds are set according to different industrial process requirements, so as to ensure the real-time aspect and authenticity of the data. Then, using the K-means clustering algorithm, the compressed test data sets are analyzed and the abnormal sensor detection values and labels are obtained. Finally, the effectiveness of such an approach is evaluated through a sample case study involving a temperature control system.																	1300-0632	1303-6203					2020	28	1					331	346		10.3906/elk-1906-55													
J								A symmetric-based framework for securing cloud data at rest	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Cloud computing security; encryption; decryption; ASCII; cloud storage		Cloud computing is the umbrella term for delivering services via the Internet. It enables enterprises and individuals to access services such as virtual machines, storage, or applications on demand. It allows them to achieve more by paying less, and it removes the barrier of installing physical infrastructure. However, due to its openness and availability over the Internet, the issue of ensuring security and privacy arises. This requires careful consideration from enterprises and individuals before the adoption of cloud computing. In order to overcome security issues, cloud service providers are required to use strong security measures to secure their storage and protect cloud data from unauthorized access. In this paper, a novel framework and symmetric-based encryption scheme for securing cloud data at rest is introduced. The performance evaluation of the new framework shows that it has a high level of efficiency, feasibility, and scalability.																	1300-0632	1303-6203					2020	28	1					347	361		10.3906/elk-1902-114													
J								On the asymptotic analysis of the high-order statistics of the channel capacity over generalized fading channels	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Higher-order ergodic capacity; higher-order amount of fading; generalized fading channels	DIVERSITY	In this article, we provide further asymptotic analysis to the higher-order statistics (HOS) of the channel capacity over generalized fading channels, especially by proposing simple and closed-form expressions each of which can be easily computed as a tight bound revealing the existence of constant gap between the actual and asymptotic HOS of the channel capacity in the limit of both high and low signal-to-noise ratios. As such, we show that these closed-form asymptotic expressions are insightful enough to comprehend the diversity gains. The mathematical formalism we followed in this article is illustrated with some selected numerical examples that validate the correctness of our newly derived asymptotic results.																	1300-0632	1303-6203					2020	28	1					362	379		10.3906/elk-1809-66													
J								Reliability comparisons of mobile network operators: an experimental case study from a crowdsourced dataset	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Data analytics; MNOs; statistics; reliability; KPIs		It is of great interest for Mobile Network Operators (MNOs) to know how well their network infrastructure performance behaves in different geographical regions of their operating country compared to their horizontal competitors. However, traditional network monitoring and measurement methods of network infrastructure use limited numbers of measurement points that are insufficient for detailed analysis and expensive to scale using an internal workforce. On the other hand, the abundance of crowdsourced content can engender various unforeseen opportunities for MNOs to cope with this scaling problem. This paper investigates end-to-end reliability and packet loss (PL) performance comparisons of MNOs using a previously collected real-world proprietary crowdsourced dataset from a user application for 13 months' duration in Turkey. More particularly, a unified crowdsourced data-aided statistical MNO comparison framework is proposed, which consists of data collection and network performance analysis steps. Our results are statistically supported using confidence interval analysis for the mean difference of PL ratios and reliability levels of MNOs using unpaired number of observations statistical analysis. The network performance results indicate that significant performance differences in MNOs depending on different regions of the country exist. Moreover, we observe that the overall comparative ordered list of MNOs' reliability performance does not differ when both PL and latency requirements vary.																	1300-0632	1303-6203					2020	28	1					380	393		10.3906/elk-1904-143													
J								A compact wideband series linear dielectric resonator array antenna	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Antenna array; dielectric resonator antenna; gain; modelling; wideband	FREQUENCY; DESIGN	This communication presents a miniaturised series linear wideband array of notched rectangular dielectric resonator antennas that operate in the band IEEE 802.11a. Three dielectric resonators (DRs) were excited through the aperture slots coupled with a microstrip feed. To improve the array gain, the aperture slots were placed based on the attributes related to the standing-wave ratio on a short-ended microstrip feeder to obtain optimal joint power for the DRs, while the bandwidth was improved using the notched rectangular DRs. An equivalent impedance model of the proposed array was postulated to provide physical insight into the array resonance behaviour. The impedance model was simulated using the Agilent Advanced Design System software and optimised to determine the DRs' dimensions. Then the array prototype was simulated and experimentally implemented. The maximum measured gain across a 1.68-GHz bandwidth was found to be 8.28 dBi. The antenna structure measured approximately 60 x 40 mm, thereby making it a good component for wireless communication systems.																	1300-0632	1303-6203					2020	28	1					394	403		10.3906/elk-1905-41													
J								Design and fabrication of an eight-port binary Wilkinson power splitter	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Power splitter; passive circuit; binary; integrated circuit	MICROSTRIP	An eight-port X-band power splitter based on binary Wilkinson power splitter topology is presented in this paper. The proposed power splitter consists of three stages, with one, two, and four equal power splitters in the first, second, and third stages, respectively. The input power is divided equally between the eight outputs to provide an equal-split power splitter. Evaluation of the structure is carried out using even- and odd-mode techniques. An epoxy laminate FR4 substrate with thickness of 0.5 mm is chosen, while the designed power splitter is simulated using Keysight Advanced Design System 2019 and ANSYS High Frequency Structure Simulation. As the results indicate, excellent performance is obtained for the power splitter, with a reflection coefficient of -36.5 dB and an insertion loss of -9.32 dB at the 9.65 GHz center frequency. The proposed power splitter has the potential to be employed in radar and microwave systems, specifically in phased-array structures.																	1300-0632	1303-6203					2020	28	1					404	413		10.3906/elk-1906-186													
J								Design and implementation of a bandpass Wilkinson power divider with wide bandwidth and harmonic suppression	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Bandpass filter; harmonic suppression; wide operation band; Wilkinson power divider	LINE	In this paper a Wilkinson power divider (WPD) is presented with ultrawide-band operation and harmonic suppression. This WPD is designed using coupling lines and meandered open stubs at main branches. The center frequency of the presented WPD is 4.25 GHz, which is fabricated and measured on RT/Duroid substrate with dielectric constant of 2.2. The proposed WPD provides good filtering band with high attenuation level. The 15 dB return loss operational bandwidth (BW) of the WPD is obtained between 3.2 GHz and 5.3 GHz, which shows 50% operational bandwidth.																	1300-0632	1303-6203					2020	28	1					414	422		10.3906/elk-1806-111													
J								A wide angle multiple beam lens for convex conformal arrays	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Convex curved array; multiple beam; multifocal lens; perfect focal point	DESIGN; PERFORMANCE	Multifocal lenses have been widely used as multiple beam forming networks for linear and planar arrays, but are not suitable for large convex conformal arrays as they are not physically realizable for wide angle beams. In this study, a new lens design especially suitable for convex conformal arrays is introduced. The new lens has no perfect focal points, but there are a certain number of correct phases in each of the beam directions. The directional patterns of the radiating elements are also taken into account to improve radiation patterns for desired directions. It has been shown that the new lens can be designed for circular arc arrays with satisfactory radiation performance for parameters where the design of realizable multifocal lenses is not possible.																	1300-0632	1303-6203					2020	28	1					423	431		10.3906/elk-1907-53													
J								RMS frequency error performance and spurious signals in two-point modulators due to path imbalances	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Frequency error; phase-locked loop; pulse width modulation; two-point modulator	RF	In this study the authors introduce an analysis of rms frequency error performance and spurious signals generated by two-point modulators. The analysis is not limited to a constant delay and magnitude imbalance between modulation paths but allows frequency-dependent group delay and amplitude variations as well. Moreover, a discrete time phase frequency detector model is incorporated in Laplace domain analysis that takes into account the sampling nature of a phase-locked loop (PLL). Using the spectrum of pulse width modulated charge pump pulses, the spurious signals at the output of the PLL are evaluated. The proposed formulae are tested on a practical setup and quite accurate results are obtained.																	1300-0632	1303-6203					2020	28	1					432	442		10.3906/elk-1804-141													
J								A power and area efficient approximate carry skip adder for error resilient applications	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Approximate computing; carry skip adder; low-power design; error resilient applications	DESIGN	The compute-intensive multimedia applications on portable devices require power and area efficient arithmetic units. The adder is a prime building block of these arithmetic units and limits the overall performance. Therefore, this paper analyzes the logic operations of the state-of-the-art adders and presents a novel low complexity adder segment with new carry prediction logic by removing the redundant logic and sharing the common operations. Further, a new power and area efficient approximate carry skip (PAEA-CSK) adder is proposed using the novel adder segment. The effectiveness of the proposed PAEA-CSK adder is evaluated and compared over the existing adders by implementing them in VHDL and synthesizing using the Synopsys Design Compiler with the 65nm TSMC CMOS Library. The synthesis result shows that the proposed PAEA-CSK adder requires 27.28% and 18.03% less area and power, respectively, over the existing carry skip-based approximate adder with the same accuracy. Further, the Sobel edge detector (SED) embedded with the proposed adder improves PSNR by a minimum of 16.94 dB over the SED embedded with a nonzeroing bit-truncation adder.																	1300-0632	1303-6203					2020	28	1					443	457		10.3906/elk-1907-72													
J								Controlling waveguide modes using PT transformation media	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Complex transformation optics; waveguides; PT symmetry	METAMATERIAL BLUEPRINTS; MINIATURIZATION; ABSORPTION; ABSORBERS; LAYERS	We study rectangular waveguide modes loaded with parity-time (PT) transformation media derived by complex transformation optics (CTO) approach. PT transformation media are obtained through mirror symmetric complex coordinate transformations resulting in a balanced loss/gain media. It is shown that waveguide modes can be controlled by simply changing the imaginary part of the complex coordinate transformation while not affecting any other characteristic of the waveguide. The field distribution inside the waveguide can either be stretched towards the sides or squeezed at the center of the waveguide by employing different loading configurations.																	1300-0632	1303-6203					2020	28	1					458	467		10.3906/elk-1904-43													
J								Operation scheme for MMC-based STATCOM using modified instantaneous symmetrical components	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										MMC; STATCOM; modified instantaneous symmetrical components; unbalance compensation	POWER; VOLTAGE	Modular multilevel converters (MMCs) are characterized by modularization and multielectric equality, and the application of this structure to a high-voltage large-capacity static synchronous compensator (STATCOM) shows good potential. In this paper, a modified instantaneous symmetrical component method is proposed for positive- and negative-sequence decomposition, i.e. the critical part of the control device, which can accurately detect the instantaneous value of each sequence component in three-phase asymmetrical phasors in real time. Then, based on this method, the paper proposes a control method for MMC-based STATCOMs that solves the problems of multi-DC (direct current) voltage balance, grid-connected current control, and circulation suppression. Finally, the proposed detection and control methods are verified by performing simulations and experiments. The results show that the proposed method can quickly and accurately detect the positive- and negative-sequence reactive component that should be compensated by the device, thus realizing real-time reactive compensation for MMC-based STATCOMs.																	1300-0632	1303-6203					2020	28	1					468	484		10.3906/elk-1904-198													
J								Optimal fractional-order PID controller of inverter-based power plants for power systems LFO damping	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Inverter-based power plant; large-scale PV power plant; low-frequency oscillation; fractional-order proportional-integral-derivative; power oscillation damper	SCALE PV PLANT; DESIGN; PENETRATION; ALGORITHMS; MODEL; AGC	The penetration of inverter-based power plants (IBPPs), such as large-scale photovoltaic (PV) power plants (LPPPs), is ever increasing considering the merits of renewable energy power plants (REPPs). Given that IBPPs are added to power systems or replaced by conventional power plants, they should undertake the most common tasks of synchronous generators. The low-frequency oscillation (LFO) damping through the power system stabilizers (PSSs) of synchronous generators is regarded as one of the common tasks in power plants. This paper proposes an optimal fractional-order proportional-integral-derivative (FOPID) controller implemented in the control loop of IBPPs for LFO damping in power systems. For this purpose, the last version of the generic dynamic model for renewable technologies (GDMRT) is used, which was released by the Western Electricity Coordinating Council (WECC) and Electric Power Research Institute (EPRI). In addition, an LPPP is studied as a case study. The FOPID controller is optimally tuned using the particle swarm optimization (PSO) algorithm in order to produce effective LFO damping. Finally, the performance of this controller is simulated and investigated in a two-area test system, showing the better performance of the LPPP for LFO damping by using the proposed optimal FOPID controller compared to the optimal lead-lag controller and optimal PID controller.																	1300-0632	1303-6203					2020	28	1					485	499		10.3906/elk-1907-31													
J								A low dropout voltage regulator with a transient voltage spikes reducer and improved figure of merit	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Dynamic biasing; adaptive biasing; source bulk modulation; output capacitorless; figure of merit; low dropout voltage regulator	LOW QUIESCENT CURRENT; LDO REGULATOR	An area efficient output capacitor-free low dropout [LDO] voltage regulator with an improved figure of merit is presented in this paper. The proposed LDO regulator consists of a novel, dynamically biased error amplifier that reduces overshoot and undershoot voltage spikes arising from abrupt load changes. Source bulk modulation is employed to enhance the current driving capability of the pass transistor. An adaptive biasing scheme is also used along with dynamic biasing to improve the current efficiency of the system. The on-chip capacitor required for proper working of the LDO regulator is only 35 pF. The proposed LDO regulator is designed and simulated in 180 nm standard CMOS technology. The LDO regulator exhibits a line regulation of 1.67 mV/V and a load regulation of 100 mu V/mA. When load changes from 0 mA to 100 mA in 1 mu s, an undershoot of 148 mV and an overshoot of 172 mV are observed. The measured power supply rejection ratio is 25 dB at 100 kHz. The working of the proposed LDO regulator has been tested under all process corners and Monte-Carlo statistical analysis reveals that it is robust against process variations and local mismatch.																	1300-0632	1303-6203					2020	28	1					500	508		10.3906/elk-1904-203													
J								Pulse width modulation control of fifteen-switch inverter for four AC loads	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Common frequency mode; fifteen-switch inverter; sinusoidal pulse width modulation; space vector pulse width modulation	MATRIX CONVERTER; INDUCTION-MOTOR; DRIVE; COUNT	In many studies in the literature, various topologies with reduced switch count are proposed. With the use of these topologies, a lower number of semiconductor switches are required to produce a desired set of voltage. This in turn reduces the size and cost of the inverter. This paper proposes a new reduced switch count topology named "fifteen-switch inverter (FSI)", which is experimentally verified. The FSI has five switches in one leg and have three legs for three phases. It is capable of controlling four three-phase ac loads. In this proposed inverter topology, fifteen switches are used against the twenty four switches in conventional two-level inverter to control four ac loads. The proposed control is implemented using modified sinusoidal and space vector pulse width modulation techniques. A comparative performance of FSI with conventional sinusoidal pulse width modulation and space vector pulse width modulation are presented in linear operating region. The structure and the principle of operation of the proposed inverter are introduced and verified using simulation. The inverter prototype was built and the proposed inverter has been verified experimentally using digital signal controller. The experimental results verify the applicability of the proposed inverter and the employed pulse generation technique.																	1300-0632	1303-6203					2020	28	1					509	524		10.3906/elk-1905-26													
J								Performance improvement of induction motor drives with model-based predictive torque control	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Variable speed drive; predictive control; induction motor; machine vector control; direct torque control	CONTROL STRATEGY; DTC; REDUCTION; RIPPLE; SVM	One of the most important advantages of using modeling and simulation software in design and control engineering is the ability to predict system behavior within specified conditions. This paper presents a novel error vector-based control algorithm that aims to reduce torque ripples predicting flux and torque errors in a conventional vector-controlled induction motor. For this purpose, a new control model has been developed that envisages flux change by applying probabilistic space vectors' torque and flux control. In the proposed predictive control algorithm, flux and torque errors are calculated for each candidate voltage vector. Thus, the optimal output voltage vector that minimizes the error is determined for the next sampling time. A MATLAB/Simulink model of the proposed control algorithm was formed and experimental studies were conducted to test the applicability and effectiveness of the proposed method using a dSpace 1104 controller board. The results obtained by comparing the effects of the conventional torque control and the proposed predictive direct torque control methods on the motor performance are presented under various speed and loading conditions. The experimental results prove that the proposed algorithm reduced the torque ripples of the motor remarkably when compared to the conventional torque control. The proposed method stands out with its simple structure due to simplified cost function with the flux model prediction approach and easy applicability with satisfactory results.																	1300-0632	1303-6203					2020	28	1					525	539		10.3906/elk-1804-124													
J								Estimation of distribution-based multiobjective design space exploration for energy and throughput-optimized MPSoCs	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Estimation of distribution algorithm; multiobjective optimization; design space exploration; energy/throughput; multiprocessal system on chip	DISTRIBUTION ALGORITHM; HYBRID ESTIMATION; RECONFIGURATION; TIME; FRAMEWORK; SYSTEM	Modern multicore architectures comprise a large set of components and parameters that require being matched to achieve the best balance between power consumption and throughput performance for a particular application domain. The exploration of design space for finding the best power-throughput trade-off is a combinatorial optimization problem with a large number of combinations, and. in general, its solution is prohibitively difficult to be explored exhaustively. However, fortunately, evolutionary algorithms (EAs) have the potential to efficiently solve this problem with reasonable computational complexity. In this paper, we consider a multiobjective design space exploration (DSE) problem with two conflicting objectives. The first objective corresponds to power consumption minimization while the second objective relates to throughput maximization. We approach this problem by employing the estimation of distribution algorithm (EDA), which belongs to the family of EAs. The proposed EDA-based DSE (EDA-DSE) scheme efficiently selects the design parameters (i.e. cache size, number of cores, and operating frequency) with an efficient power-throughput ratio. The proposed scheme is verified using cycle-accurate simulations over a set of benchmarks and the simulation results show a significant reduction in energy-delay product for all benchmark applications when compared to the default baseline configuration and genetic algorithm.																	1300-0632	1303-6203					2020	28	1					540	555		10.3906/elk-1812-59													
J								Dynamically updated diversified ensemble-based approach for handling concept drift	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Concept drift; ensemble learning; classification; diversity; data streams; machine learning	WEIGHTED-MAJORITY	Concept drift is the phenomenon where underlying data distribution changes over time unexpectedly. Examining such drifts and getting insight into the executing processes at that instance of time is a big challenge. Prediction models should be capable of handling drifts in scenarios where statistical properties show abrupt changes. Various strategies exist in the literature to deal with such challenging scenarios but the majority of them are limited to the identification of a particular kind of drift pattern. The proposed approach uses online drift detection in a diversified adaptive setting with pruning techniques to formulate a concept drift handling approach, named ensemble-based online diversified drift detection (En-ODDD), with an aim to identify the majority of drifts including abrupt, gradual, recurring, mixed, etc. in a single model. En-ODDD is equipped with a dynamically updated ensemble to speed up the adaptability to changing distributions. Unlike prevalent approaches, which do not consider correlations between experts, En-ODDD entails experts using varying randomized subsets of input data. Different levels of sampling having been applied for diversity generation to promote generalization. Prediction accuracy has been used to evaluate the effectiveness of the proposed approach using Massive Online Analysis software and compared with ten state-of-the-art algorithms. Experimental results on fifteen benchmark datasets (artificial and real-world) having up to one million instances depict that En-ODDD outperforms the existing approaches irrespective of nature of drift.																	1300-0632	1303-6203					2020	28	1					556	574		10.3906/elk-1810-122													
J								A preliminary survey on software testing practices in Khyber PakhtunKhwa region of Pakistan	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Empirical software testing; Software testing practices; software quality; Khyber PakhtunKhwa Pakistan; software development verification and validation process		Conducted to ensure the quality of software products, the software testing process has a great significance in the software development and is the vital step of the verification and validation process. For conforming a software feature to the end user requirements, organizations rely on extensive testing procedures. Despite being the key factor, many of the software development industries/companies do not define/follow a systematic testing process. In this paper, we analyze/learn from the conducted surveys in the past and formulate a questionnaire for a survey in the northern region of Pakistan. To the best of our knowledge, no such survey has ever been conducted in Pakistan. Thus, our work can be used as a baseline for such surveys in other regions or for a survey at the country level. This survey focuses on various aspects of testing methodologies and techniques. With the analyses of the survey findings, we conclude that a testing "maturity" model (based on the cultural/organizational/political values/constraints) should be devised for the improvement of current testing practices to ensure the production of quality software. The survey is regional but the results and knowledge obtained are in line with the related surveys conducted in other regions of the globe.																	1300-0632	1303-6203					2020	28	1					575	+		10.3906/elk-1903-6													
J								Small traffic sign detection from large image	APPLIED INTELLIGENCE										Small traffic sign; Loss function; Deconvolution; Hard negative samples; Tsinghua-Tencent 100K	RECOGNITION	Automatic traffic sign detection has great potential for intelligent vehicles. The ability to detect small traffic signs in large traffic scenes enhances the safety of intelligent devices. However, small object detection is a challenging problem in computer vision; the main problem involved in accurate traffic sign detection is the small size of the signs. In this paper, we present a deconvolution region-based convolutional neural network (DR-CNN) to cope with this problem. This method first adds a deconvolution layer and a normalization layer to the output of the convolution layer. It concatenates the features of the different layers into a fused feature map to provide sufficient information for small traffic sign detection. To improve training effectiveness and distinguish hard negative samples from easy positive ones, we propose a two-stage adaptive classification loss function for region proposal networks (RPN) and fully connected neural networks within DR-CNN. Finally, we evaluate our proposed method on the new and challenging Tsinghua-Tencent 100K dataset. We further conduct ablation experiments and analyse the effectiveness of the fused feature map and the two-stage classification loss function. The final experimental results demonstrate the superiority of the proposed method for detecting small traffic signs.																	0924-669X	1573-7497				JAN	2020	50	1					1	13		10.1007/s10489-019-01511-7													
J								How to add new knowledge to already trained deep learning models applied to semantic localization	APPLIED INTELLIGENCE										Semantic localization; Deep learning; Retraining strategies; Machine learning		The capacity of a robot to automatically adapt to new environments is crucial, especially in social robotics. Often, when these robots are deployed in home or office environments, they tend to fail because they lack the ability to adapt to new and continuously changing scenarios. In order to accomplish this task, robots must obtain new information from the environment, and then add it to their already learned knowledge. Deep learning techniques are often used to tackle this problem successfully. However, these approaches, complete retraining of the models, which is highly time-consuming. In this work, several strategies are tested to find the best way to include new knowledge in an already learned model in a deep learning pipeline, putting the spotlight on the time spent for this training. We tackle the localization problem in the long term with a deep learning approach and testing several retraining strategies. The results of the experiments are discussed and, finally, the best approach is deployed on a Pepper robot.																	0924-669X	1573-7497				JAN	2020	50	1					14	28		10.1007/s10489-019-01517-1													
J								A high-speed D-CART online fault diagnosis algorithm for rotor systems	APPLIED INTELLIGENCE										Data mining; CART; Fault diagnosis; Intelligent manufacturing	REFRIGERANT FLOW SYSTEM; FEATURE-EXTRACTION; CLASSIFICATION; PERFORMANCE; TREE; IDENTIFICATION; MAINTENANCE	Intelligent manufacturing poses a challenge for fault diagnosis of rotor systems to meet the three tasks: whether exists faults, faults location and quantitative diagnosis. Traditional methods hardly meet all the three tasks in online fault diagnosis. This paper proposes a modified classification and regression tree (CART) algorithm named D-CART algorithm to provide much faster fault classification by reducing the iteration times in computation while still ensuring accuracy. Experiments are carried on to achieve a comprehensive online fault diagnosis for rotor systems such as faults location, faults types and quantitative analysis of unbalanced mass in this paper. In comparison with the other 4 novel CART-based algorithms, the experimental results indicate that the speed of D-CART algorithm is improved by a factor of 23.92 compared to the fastest improved algorithm (Adaboost-CART) and a model accuracy of up to 96.77%. Thus demonstrating the speed superiority of D-CART algorithm in both diagnosing locations of different faults types and determining the loading masses of unbalanced faults. The proposed method has the potential to realize high-accuracy online fault diagnosis for rotor systems.																	0924-669X	1573-7497				JAN	2020	50	1					29	41		10.1007/s10489-019-01516-2													
J								Recognising innovative companies by using a diversified stacked generalisation method for website classification	APPLIED INTELLIGENCE										Innovative detection; Text classification; Ensemble learning; Multiple views; Multi-view learning	REGULARIZATION PATHS; MODEL; CLASSIFIERS; ENSEMBLE	In this paper, we propose a classification system which is able to decide whether a company is innovative or not, based only on its public website available on the internet. As innovativeness plays a crucial role in the development of myriad branches of the modern economy, an increasing number of entities are expending effort to be innovative. Thus, a new issue has appeared: how can we recognise them? Not only is grasping the idea of innovativeness challenging for humans, but also impossible for any known machine learning algorithm. Therefore, we propose a new indirect technique: a diversified stacked generalisation method, which is based on a combination of a multi-view approach and a genetic algorithm. The proposed approach achieves better performance than all other classification methods which include: (i) models trained on single datasets; or (ii) a simple voting method on these models. Furthermore, in this study, we check if unaligned feature space improves classification results. The proposed solution has been extensively evaluated on real data collected from companies' websites. The experimental results verify that the proposed method improves the classification quality of websites which might represent innovative companies.																	0924-669X	1573-7497				JAN	2020	50	1					42	60		10.1007/s10489-019-01509-1													
J								Transfer Naive Bayes algorithm with group probabilities	APPLIED INTELLIGENCE										Transfer learning; Naive Bayes; Group probabilities; Classification	INTRUSION DETECTION; MACHINE	In order to protect data privacy, a new transfer group probability Naive Bayes algorithm TrGNB is proposed. TrGNB is applied to scenarios in which the source domain contains a large amount of labeled data and only a small amount of unlabeled data group probability information in the target domain. TrGNB integrates the ideology of transfer learning and group probability information into the Naive Bayes model, which not only improves the classification effect of the learning task in the target domain but also protects the data privacy. The TrGNB was verified on the 20-Newsgroups, Reuters-21578 and Email spam datasets. The experimental results show that TrGNB significantly improves the classification accuracy compared with the benchmark algorithms.																	0924-669X	1573-7497				JAN	2020	50	1					61	73		10.1007/s10489-019-01512-6													
J								Enhancing data analysis: uncertainty-resistance method for handling incomplete data	APPLIED INTELLIGENCE										Incomplete data; Missing values; Belief function theory; Mapped data; Classification	MISSING DATA IMPUTATION; K-NEAREST NEIGHBORS; FUZZY C-MEANS; QUANTILE REGRESSION; ALGORITHM; COMBINATION	In data analysis, incomplete data commonly occurs and can have significant effects on the conclusions that can be drawn from the data. Incomplete data cause another problem, so-called uncertainty which leads to producing unreliable results. Hence, developing effective techniques to impute these missing values is crucial. Missing or incomplete data and noise are two common sources of uncertainty. In this paper, an effective method for imputing missing values is introduced which is robust to uncertainties that are arising from incompleteness and noise. A kernel-based method for removing the noise is designed. Using the belief function theory, the class of incomplete data is determined. Finally, every missing dimension is imputed considering the mean value of the same dimension of the members belonging to the determined class. The performance has been evaluated on real-world data sets from UCI repository. The results of the experiments have been compared with state-of-the-art methods, which show the superiority of the proposed method regarding classification accuracy.																	0924-669X	1573-7497				JAN	2020	50	1					74	86		10.1007/s10489-019-01514-4													
J								A jigsaw puzzle inspired algorithm for solving large-scale no-wait flow shop scheduling problems	APPLIED INTELLIGENCE										No-wait flow shop scheduling; Makespan; Heuristic algorithm; Jigsaw puzzle	FLEXIBLE FLOWSHOP; TIME	The no-wait flow shop scheduling problem (NWFSP), as a typical NP-hard problem, has important ramifications in the modern industry. In this paper, a jigsaw puzzle inspired heuristic (JPA) is proposed for solving NWFSP with the objective of minimizing makespan. The core idea behind JPA is to find the best match for each job until all the jobs are scheduled in the set of process. In JPA, a waiting time matrix is constructed to measure the gap between two jobs. Then, a matching matrix based on the waiting time matrix is obtained. Finally, the optimal scheduling sequence is built by using the matching matrix. Experimental results on large-scale benchmark instances show that JPA is superior to the state-of-the-art heuristics.																	0924-669X	1573-7497				JAN	2020	50	1					87	100		10.1007/s10489-019-01497-2													
J								Feature selection with Symmetrical Complementary Coefficient for quantifying feature interactions	APPLIED INTELLIGENCE										Feature selection; ReliefF; Sequential Forward Selection; Feature interaction; Random Forest; Symmetrical Complementary Coefficient	MUTUAL INFORMATION; CLASSIFICATION; RELEVANCE	In the field of machine learning and data mining, feature interaction is a ubiquitous issue that cannot be ignored and has attracted more attention in recent years. In this paper, we proposed the Symmetrical Complementary Coefficient which can quantify feature interactions very well. Based on it, we improved the Sequential Forward Selection (SFS) algorithm and proposed a new feature subset searching algorithm called SCom-SFS which only needs to consider the feature interactions between adjacent features on a given sequence instead of all of them. Moreover, discovered feature interactions can speed up the process of searching for the optimal feature subset. In addition, we have improved the ReliefF algorithm by screening out representative samples from the original data set, and need not to sample the samples. The improved ReliefF algorithm has been proved to be more efficient and reliable. An effective and complete feature selection algorithm RRSS is obtained through the combination of the two modified algorithms. According to the experimental results, the proposed algorithm RRSS outperformed five classic and two latest feature selection algorithms in terms of size of resulting feature subset, Accuracy, Kappa coefficient, and adjusted Mean-Square Error (MSE).																	0924-669X	1573-7497				JAN	2020	50	1					101	118		10.1007/s10489-019-01518-0													
J								MOSHEPO: a hybrid multi-objective approach to solve economic load dispatch and micro grid problems	APPLIED INTELLIGENCE										Multi-objective optimization; Spotted hyena optimizer; Emperor penguin optimizer; Economic dispatch; Micro grid	PARTICLE SWARM OPTIMIZATION; SPOTTED HYENA OPTIMIZER; GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION; EMISSION DISPATCH; FLOW	This paper proposes a novel hybrid multi-objective algorithm named Multi-objective Spotted Hyena and Emperor Penguin Optimizer (MOSHEPO) for solving both convex and non-convex economic dispatch and micro grid power dispatch problems. The proposed algorithm combines two newly developed bio-inspired optimization algorithms namely Multi-objective Spotted Hyena Optimizer (MOSHO) and Emperor Penguin Optimizer (EPO). MOSHEPO contemplates many non-linear characteristics of power generators such as transmission losses, multiple fuels, valve-point loading, and prohibited operating zones along with their operational constraints, for practical operation. To evaluate the effectiveness of MOSHEPO, the proposed algorithm has been tested on various benchmark test systems and its performance is compared with other well-known approaches. The experimental results demonstrate that the proposed algorithm outperforms other algorithms with low computational efforts while solving economic and micro grid power dispatch problems.																	0924-669X	1573-7497				JAN	2020	50	1					119	137		10.1007/s10489-019-01522-4													
J								Aggregated topic models for increasing social media topic coherence	APPLIED INTELLIGENCE										Topic models; Data fusion; Topic coherence; Ensemble methods; Social media		This research presents a novel aggregating method for constructing an aggregated topic model that is composed of the topics with greater coherence than individual models. When generating a topic model, a number of parameters have to be specified. The resulting topics can be very general or very specific, which depend on the chosen parameters. In this study we investigate the process of aggregating multiple topic models generated using different parameters with a focus on whether combining the general and specific topics is able to increase topic coherence. We employ cosine similarity and Jensen-Shannon divergence to compute the similarity among topics and combine them into an aggregated model when their similarity scores exceed a predefined threshold. The model is evaluated against the standard topics models generated by the latent Dirichlet allocation and Non-negative Matrix Factorisation. Specifically we use the coherence of topics to compare the individual models that create aggregated models against those of the aggregated model and models generated by Non-negative Matrix Factorisation, respectively. The results demonstrate that the aggregated model outperforms those topic models at a statistically significant level in terms of topic coherence over an external corpus. We also make use of the aggregated topic model on social media data to validate the method in a realistic scenario and find that again it outperforms individual topic models.																	0924-669X	1573-7497				JAN	2020	50	1					138	156		10.1007/s10489-019-01438-z													
J								Fuzzy risk analysis under influence of non-homogeneous preferences elicitation in fiber industry	APPLIED INTELLIGENCE										Fuzzy risk analysis; Grey numbers; Non-homogeneous preferences elicitation; Fiber industry	DECISION-MAKING MODEL; REASONABLE PROPERTIES; TENSILE-STRENGTH; RANKING	Fuzzy risk analysis plays an important role in mitigating the levels of harm of a risk. In real world scenarios, it is a big challenge for risk analysts to make a proper and comprehensive decision when coping with risks that are incomplete, vague and fuzzy. Many established fuzzy risk analysis approaches do not have the flexibility to deal with knowledge in the form of preferences elicitation which lead to incorrect risk decision. The inefficiency is reflected when they consider only risk analyst preferences elicitation that is partially known. Nonetheless, the preferences elicited by the risk analyst are often non-homogeneous in nature such that they can be completely known, completely unknown, partially known and partially unknown. In this case, established fuzzy risk analysis methods are considered as inefficient in handling risk, hence an appropriate fuzzy risk analysis method that can deal with the non-homogeneous nature of risk analyst's preferences elicitation is worth developing. Therefore, this paper proposes a novel fuzzy risk analysis method that is capable to deal with the non-homogeneous risk analyst's preferences elicitation based on grey numbers. The proposed method aims at resolving the uncertain interactions between homogeneous and non-homogeneous natures of risk analyst's preferences elicitation by using a novel consensus reaching approach that involves transformation of grey numbers into grey parametric fuzzy numbers. Later on, a novel fuzzy risk assessment score approach is presented to correctly evaluate and distinguish the levels of harm of the risks faced, such that these evaluations are consistent with preferences elicitation of the risk analyst. A real world risk analysis problem in fiber industry is then carried out to demonstrate the novelty, validity and feasibility of the proposed method.																	0924-669X	1573-7497				JAN	2020	50	1					157	168		10.1007/s10489-019-01508-2													
J								Effective sanitization approaches to protect sensitive knowledge in high-utility itemset mining	APPLIED INTELLIGENCE										Privacy-preserving utility mining; Sensitive knowledge; Sanitization process; Side effects	ASSOCIATION RULES; EFFICIENT ALGORITHM; FREQUENT; PATTERNS	For mutual benefit, data is shared among business organizations. However, this may result in privacy and security threats. To address this issue, privacy-preserving data mining is presented to sanitize the original database to hide all sensitive knowledge. Privacy-preserving utility mining is an extension of privacy-preserving data mining, the objective of which is to hide all sensitive high-utility itemsets and minimize the side effects on non-sensitive knowledge caused by the sanitization process. In this paper, three heuristic algorithms for privacy-preserving utility mining are proposed, namely, Selecting Maximum Utility item first (SMAU), Selecting Minimum Utility item first (SMIU) and Selecting Minimum Side Effects item first (SMSE). The quality of the database is well maintained because all of the proposed algorithms consider the side effects on the non-sensitive itemsets. Furthermore, to avoid performing multiple database scans, two table structures, T-table and HUI-table, are adopted to accelerate the hiding process by only scanning the database twice. The experimental results show that the proposed approaches successfully conceal all sensitive itemsets with fewer distortions of non-sensitive knowledge. Moreover, the influence of the database density on the proposed approaches is observed.																	0924-669X	1573-7497				JAN	2020	50	1					169	191		10.1007/s10489-019-01524-2													
J								Non-convex approximation based l(0)-norm multiple indefinite kernel feature selection	APPLIED INTELLIGENCE										l(0)-norm; Feature selection; Multiple kernel learning; DC programming	VARIABLE SELECTION; REGULARIZATION; SPARSITY	Multiple kernel learning (MKL) for feature selection utilizes kernels to explore complex properties of features, which has been shown to be among the most effective for feature selection. To perform feature selection, a natural way is to use the l(0)-norm to get sparse solutions. However, the optimization problem involving l(0)-norm is NP-hard. Therefore, previous MKL methods typically utilize a l(1)-norm to get sparse kernel combinations. However, the l(1)-norm, as a convex approximation of l(0)-norm, sometimes cannot attain the desired solution of the l(0)-norm regularizer problem and may lead to prediction accuracy loss. In contrast, various non-convex approximations of l(0)-norm have been proposed and perform better in many linear feature selection methods. In this paper, we propose a novel l(0)-norm based MKL method (l(0)-MKL) for feature selection with non-convex approximations constraint on kernel combination coefficients to select features automatically. Considering the better empirical performance of indefinite kernels than positive kernels, our l(0)-MKL is built on the primal form of multiple indefinite kernel learning for feature selection. The non-convex optimization problem of l(0)-MKL is further refumated as a difference of convex functions (DC) programming and solved by DC algorithm (DCA). Experiments on real-world datasets demonstrate that l(0)-MKL is superior to some related state-of-the-art methods in both feature selection and classification performance.																	0924-669X	1573-7497				JAN	2020	50	1					192	202		10.1007/s10489-018-01407-y													
J								Bag of contour fragments for improvement of object segmentation	APPLIED INTELLIGENCE										Object recognition; Object segmentation; Semantic segmentation; Shape feature	SHAPE; RECOGNITION; REPRESENTATION; TEXTONBOOST; ALGORITHM; FEATURES; CUTS	Many state-of-the-art shape features have been proposed for the shape recognition task. In this paper, to explore whether a shape feature influences object segmentation, we propose a specific shape feature, Fisher shape (a form of bag of contour fragments), and we combine this with the appearance feature with multiple kernel learning to create a pipeline of object segmentation system. The experimental results on benchmark datasets clearly demonstrate that the pipeline of object segmentation is effective and that the Fisher shape can improve object segmentation with only the appearance feature.																	0924-669X	1573-7497				JAN	2020	50	1					203	221		10.1007/s10489-019-01525-1													
J								Multi-stage optimization model for hesitant qualitative decision making with hesitant fuzzy linguistic preference relations	APPLIED INTELLIGENCE										Decision making; Hesitant fuzzy linguistic preference relations; Multiplicative consistency; Priority vector; Integer programming model	TERM SETS; CONSISTENCY MEASURES; MULTIPLICATIVE CONSISTENCY; ADDITIVE CONSISTENCY; NUMERICAL SCALE; COMPATIBILITY; AGGREGATION	Hesitant fuzzy linguistic preference relation (HFLPR) as a new preference relation is introduced to express the decision makers' (DMs') hesitant preference information for each pairwise comparison between different alternatives or criteria. In this paper, the priority vector and consistency of HFLPR are discussed based on a two-stage optimization and multiplicative consistency. Based on the original hesitant preference information, the multiplicative consistency index of an HFLPR is defined to measure the consistency level of the HFLPR. For an unacceptable multiplicative consistent HFLPR, a goal programming model, which is an integer optimization model, is developed to derive an acceptable, multiplicative, consistent HFLPR. According to probability sampling, a linguistic preference relation (LPR) with the best consistency level and an LPR with the worst consistency level with regard to an HFLPR are defined. Combining the two LPRs, a two-stage optimization framework is constructed to obtain the HFLPR's priority vector, which considers the DM's risk preference. A multi-stage optimization approach is proposed to solve decision-making problems by integrating the goal programming model and the two-stage optimization framework. Two real life problems are analyzed to show the feasibility of the proposed approach.																	0924-669X	1573-7497				JAN	2020	50	1					222	240		10.1007/s10489-019-01502-8													
J								Dynamic uncertain causality graph based on Intuitionistic fuzzy sets and its application to root cause analysis	APPLIED INTELLIGENCE										Dynamic uncertain causality graph; Knowledge representation and reasoning; Uncertainty; IFDUCG; Intuitionistic fuzzy set	GROUP DECISION-MAKING; ARTIFICIAL NEURAL-NETWORK; KNOWLEDGE REPRESENTATION; BAYESIAN NETWORK; FAULT-DETECTION; DIAGNOSIS; OPERATORS	Dynamic uncertain causality graph (DUCG), which is based on probability theory, is used for uncertain knowledge representation and reasoning. However, the traditional DUCG has difficulty expressing the causality of the events with crisp numbers. Therefore, an intuitionistic fuzzy set based dynamic uncertain causality graph (IFDUCG) model is proposed in this paper. The model focuses on describing the uncertain event in the form of intuitionistic fuzzy sets, which can handle with the problem of describing vagueness and uncertainty of an event in the traditional model. Then the technique for order preference by similarity to an ideal solution (TOPSIS) method is combined with IFDUCG for knowledge representation and reasoning so as to integrate more abundant experienced knowledge into the model to make the model more reliable. Then some examples are used to validate the proposed method. The experimental results prove that the proposed method is effective and flexible in dealing with the difficulty of the fuzzy event of knowledge representation and reasoning. Furthermore, we make a practical application to root cause analysis of aluminum electrolysis and the results show that the proposed method is available for workers to make decisions.																	0924-669X	1573-7497				JAN	2020	50	1					241	255		10.1007/s10489-019-01520-6													
J								Multi-objective particle swarm optimization based on cooperative hybrid strategy	APPLIED INTELLIGENCE										Cooperative hybrid strategy; Dynamic clustering; Life; Lottery probability	EVOLUTIONARY ALGORITHMS; REFERENCE POINTS; DECOMPOSITION; RANKING	A multi-objective particle swarm optimization based on cooperative hybrid strategy (CHSPSO) is presented in this paper to solve complex multi-objective problems. Most algorithms usually contain only one strategy, which makes them unable to trade off the convergence and diversity when solving the complex multi-objective problems. The proposed cooperative hybrid strategy can effectively guarantee the convergence and the diversity of the algorithm. The multi-population strategy and the dynamic clustering strategy are employed to improve the convergence and the diversity. At the same time, the life strategy and lottery probability selection strategy are used to further ensure the diversity of the population. A series of test functions are used to verify the effectiveness of CHSPSO. The performance of the proposed algorithm is compared with other evolutionary algorithms. The results show that CHSPSO can obtain a better convergence and diversity for the complex multi-objective problems.																	0924-669X	1573-7497				JAN	2020	50	1					256	269		10.1007/s10489-019-01496-3													
J								Scalability and sparsity issues in recommender datasets: a survey	KNOWLEDGE AND INFORMATION SYSTEMS										Recommender systems; Collaborative filtering; Memory-based technique; Model-based techniques; Hard clustering; Biclustering; Matrix factorization; Graph-theoretic technique; Fuzzy recommender systems; Scalability; Sparsity	RESEARCH RESOURCES; SYSTEM; ALGORITHMS; SIMILARITY; STRATEGIES; CONSTANT; GRAPH	Recommender systems have been widely used in various domains including movies, news, music with an aim to provide the most relevant proposals to users from a variety of available options. Recommender systems are designed using techniques from many fields, some of which are: machine learning, information retrieval, data mining, linear algebra and artificial intelligence. Though in-memory nearest-neighbor computation is a typical approach for collaborative filtering due to its high recommendation accuracy; its performance on scalability is still poor given a huge user and item base and availability of only few ratings (i.e., data sparsity) in archetypal merchandising applications. In order to alleviate scalability and sparsity issues in recommender systems, several model-based approaches were proposed in the past. However, if research in recommender system is to achieve its potential, there is a need to understand the prominent techniques used directly to build recommender systems or for preprocessing recommender datasets, along with its strengths and weaknesses. In this work, we present an overview of some of the prominent traditional as well as advanced techniques that can effectively handle data dimensionality and data sparsity. The focus of this survey is to present an overview of the applicability of some advanced techniques, particularly clustering, biclustering, matrix factorization, graph-theoretic, and fuzzy techniques in recommender systems. In addition, it highlights the applicability and recent research works done using each technique.																	0219-1377	0219-3116				JAN	2020	62	1					1	43		10.1007/s10115-018-1254-2													
J								A scalable privacy-preserving framework for temporal record linkage	KNOWLEDGE AND INFORMATION SYSTEMS										Secure multiparty computation; Encryption; Temporal records		Record linkage (RL) is the process of identifying matching records from different databases that refer to the same entity. In many applications, it is common that the attribute values of records that belong to the same entity evolve over time, for example people can change their surname or address. Therefore, to identify the records that refer to the same entity over time, RL should make use of temporal information such as the time-stamp of when a record was created and/or update last. However, if RL needs to be conducted on information about people, due to privacy and confidentiality concerns organisations are often not willing or allowed to share sensitive data in their databases, such as personal medical records or location and financial details, with other organisations. This paper proposes a scalable framework for privacy-preserving temporal record linkage that can link different databases while ensuring the privacy of sensitive data in these databases. We propose two protocols that can be used in different linkage scenarios with and without a third party. Our protocols use Bloom filter encoding which incorporates the temporal information available in records during the linkage process. Our approaches first securely calculate the probabilities of entities changing attribute values in their records over a period of time. Based on these probabilities, we then generate a set of masking Bloom filters to adjust the similarities between record pairs. We provide a theoretical analysis of the complexity and privacy of our techniques and conduct an empirical study on large real databases containing several millions of records. The experimental results show that our approaches can achieve better linkage quality compared to non-temporal PPRL while providing privacy to individuals in the databases that are being linked.																	0219-1377	0219-3116				JAN	2020	62	1					45	78		10.1007/s10115-019-01370-1													
J								An integrated trust establishment model for the internet of agents	KNOWLEDGE AND INFORMATION SYSTEMS										Trust establishment; Internet of agents; Agent modeling	MULTIAGENT SYSTEMS; POWER; MANAGEMENT	The Internet of Agents (IoA) is an emerging field of research that extends the concept of Internet of Things (IoT) by augmenting internal reasoning and intelligence capability to, traditionally, naive things used in IoT. In a fully automated IoA environment, agents tend to choose trustworthy partners to rely on for fulfilling their objectives, particularly when the agents cannot guarantee that potential interacting partners share the same core beliefs or make accurate statements regarding their competence and abilities. Trust establishment mechanisms can be used to direct trustees, rather than trustors, to build a higher level of trust and have greater impact on the results of their interactions when such interactions are based on trust. This paper presents ITE, a trust establishment model that integrates the two major sources of information to produce a comprehensive assessment of a trustor's needs in IoA. Specifically, ITE attempts to incorporate explicit and implicit feedbacks to provide trust establishment under a wider range of circumstances.																	0219-1377	0219-3116				JAN	2020	62	1					79	105		10.1007/s10115-019-01348-z													
J								Resolution-based rewriting for Horn-SHIQ ontologies	KNOWLEDGE AND INFORMATION SYSTEMS										Query rewriting; Description logics; Ontologies; Resolution	DESCRIPTION LOGICS; INFERENCE ENGINE; FAMILY	An important approach to query answering over description logic (DL) ontologies is via rewriting the input ontology and query into languages such as (disjunctive) datalog, for which scalable data saturation systems exist. This approach has been studied for DLs of different expressivities such as DL-Lite, ELHI and Horn-SHIQ. When it comes to expressive languages resolution is an important technique that can be applied to obtain the rewritings. This is mainly because it allows for the design of general-purpose algorithms that can be easily lifted to support languages of high expressivity. In the current work we present an efficient resolution-based rewriting algorithm tailor-made for the expressive DL language Horn-SHIQ. Our algorithm avoids performing many unnecessary inferences, which is one of the main problems of resolution-based algorithms. This is achieved by careful analysis of the complex axioms structure supported in Horn-SHIQ. Moreover, we have implemented the proposed algorithm and obtained very encouraging results when conducting extensive experimental evaluation.																	0219-1377	0219-3116				JAN	2020	62	1					107	143		10.1007/s10115-019-01345-2													
J								Mutual clustering on comparative texts via heterogeneous information networks	KNOWLEDGE AND INFORMATION SYSTEMS										Mutual clustering; Text mining; Heterogeneous information network	PREDICTION; SEARCH	Currently, many intelligence systems contain the texts from multi-sources, e.g., bulletin board system posts, tweets and news. These texts can be "comparative" since they may be semantically correlated and thus provide us with different perspectives toward the same topics or events. To better organize the multi-sourced texts and obtain more comprehensive knowledge, we propose to study the novel problem of Mutual Clustering on Comparative Texts (MCCT), which aims to cluster the comparative texts simultaneously and collaboratively. The MCCT problem is difficult to address because 1) comparative texts usually present different data formats and structures and thus, they are hard to organize and 2) there lacks an effective method to connect the semantically correlated comparative texts to facilitate clustering them in an unified way. To this aim, in this paper, we propose a Heterogeneous Information Network-based Text clustering framework HINT. HINT first models multi-sourced texts (e.g. news and tweets) as heterogeneous information networks by introducing the shared "anchor texts" to connect the comparative texts. Next, two similarity matrices based on HINT as well as a transition matrix for cross-text-source knowledge transfer are constructed. Comparative texts clustering are then conducted by utilizing the constructed matrices. Finally, a mutual clustering algorithm is also proposed to further unify the separate clustering results of the comparative texts by introducing a clustering consistency constraint. We conduct extensive experimental on three tweets-news datasets, and the results demonstrate the effectiveness and robustness of the proposed method in addressing the MCCT problem.																	0219-1377	0219-3116				JAN	2020	62	1					175	202		10.1007/s10115-019-01356-z													
J								Histogram-based clustering of multiple data streams	KNOWLEDGE AND INFORMATION SYSTEMS										Data stream mining; Histogram data; Clustering; Sensor data streams	TIME-SERIES; EVOLUTION	This paper introduces a strategy for clustering online multiple data streams. We assume that several sources are used for recording, over time, data about some physical phenomena. Each source provides repeated measurements at a very high frequency so that it is not possible to store the whole amount of data into some easy-to-access media, but data are available only in batches. Our aim is to discover a partition of the sources (e.g. sensors) into homogeneous clusters, analysing the incoming streams of data. The proposed strategy is based on processing the incoming data batches independently, through an initial summarization of the data batches by histograms and, then, by means of a local clustering performed on the histograms which provides a further data summarization. To keep track of the data proximities among the data streams over time, we use local clustering outputs for updating a proximity matrix. The final partitioning of the streams is obtained by a clustering based on such proximity matrix. Through an application on real and simulated data, we show the effectiveness of our strategy in finding homogeneous groups of sources of data streams.																	0219-1377	0219-3116				JAN	2020	62	1					203	238		10.1007/s10115-019-01350-5													
J								Rate of change analysis for interestingness measures	KNOWLEDGE AND INFORMATION SYSTEMS										Association rule mining; Objective measures; Properties of measures; Rate of change analysis		The use of association rulemining techniques in diverse contexts and domains has resulted in the creation of numerous interestingness measures. This, in turn, has motivated researchers to come up with various classification schemes for these measures. One popular approach to classify the objective measures is to assess the set of mathematical properties they satisfy in order to help practitioners select the right measure for a given problem. In this research, we discuss the insufficiency of the existing properties in the literature to capture certain behaviors of interestingness measures. This motivates us to adopt an approach where a measure is described by how it varies if there is a unit change in the frequency count (f(11), f(10), f(01), f(00)), at different preexisting states of the counts. This rate of change analysis is formally defined as the first partial derivative of the measure with respect to the various frequency counts. We use this analysis to define two novel properties, unit-null asymptotic invariance (UNAI) and unit-null zero rate (UNZR). UNAI looks at the asymptotic effect of adding frequency patterns, while UNZR looks at the initial effect of adding frequency patterns when they do not preexist in the dataset. We present a comprehensive analysis of 50 interestingness measures and classify them in accordance with the two properties. We also present multiple empirical studies, involving both synthetic and real-world datasets, which are used to cluster various measures according to the rule ranking patterns of the measures. The study concludes with the observation that classification of measures using the empirical clusters shares significant similarities to the classification of measures done through the properties presented in this research.																	0219-1377	0219-3116				JAN	2020	62	1					239	258		10.1007/s10115-019-01352-3													
J								Mining maritime traffic conflict trajectories from a massive AIS data	KNOWLEDGE AND INFORMATION SYSTEMS										Maritime traffic data; AIS data; Conflict trajectory; Trajectory data mining	COLLISION RISK; MODEL	The growing volume of maritime traffic is proving a hindrance to navigational safety. Researchers have sought to improve the safety of maritime transportation by conducting statistical analysis on historical collision data in order to identify the causes of maritime collisions. However, this approach is hindered by the limited number of incidents that can be collected in a given area over a given period of time. Automatic Identification System (AIS) has made available enormous quantities of maritime traffic data. Trajectory data are collected through the electronic exchange of navigational data among ships and terrestrial and satellite base stations. Due to a massive AIS data of recording ship movement, such data provide great opportunity to discover maritime traffic knowledge of movement behavior analysis, route estimation, and the detection of anomalous behaviors. Our objective in this paper was to identify potential between-ship traffic conflicts through the discovery of AIS data. Traffic conflict refers to trajectories that could lead to a collision if the ships do not take any evasive action. In other words, conflicting trajectories can be treated as a near-collision cases for analysis. The prevention of collisions requires an efficient method by which to extract conflicting trajectories from a massive collection of AIS data. To this end, we developed a framework CCT Discovery that allows the automated identification of clusters of conflicting trajectories (CCTs) from AIS data without supervision. Experiments based on real-world data demonstrate the efficacy of the proposed framework in terms of accuracy and efficiency. For improvement in the navigational traffic safety, the discovered data of conflict trajectory can contribute to numerous applications, such as collision situation awareness and prediction, anti-collision behaviors modeling and recommendation, and conflict area analysis for maritime traffic flow management.																	0219-1377	0219-3116				JAN	2020	62	1					259	285		10.1007/s10115-019-01355-0													
J								An intelligence approach for group stock portfolio optimization with a trading mechanism	KNOWLEDGE AND INFORMATION SYSTEMS										Grouping genetic algorithms; Genetic algorithms; Group stock portfolio; Grouping problem; Portfolio optimization	GENETIC ALGORITHMS; TRANSACTION COSTS; SELECTION; MODEL; RETURNS; SYSTEM	Optimizing a stock portfolio from a given financial dataset is always a very attractive task, as various factors should be considered. Hence, many methods based on evolutionary algorithms have been developed in the past decades to deal with the portfolio optimization problem. To provide a more flexible stock portfolio, we propose an algorithm to optimize a group stock portfolio by using a grouping genetic algorithm. In accordance with the optimized group stock portfolio, many stock portfolios can be generated and provided to investors. Each chromosome in the genetic algorithm is composed of a grouping part, a stock part and a stock portfolio part. The grouping and stock parts are used to indicate how to divide stocks into groups. The stock portfolio part is used to represent how many stocks should be selected from groups to form a portfolio and what units should be purchased. Four fitness functions are designed to evaluate each individual. Each of them is composed of the group balance, the unit balance, the stock price balance and the portfolio satisfaction. Genetic operations, including crossover, mutation and inversion, are then executed to obtain new offspring to find the best solution. Furthermore, the proposed approach with a trading mechanism is designed to get a more useful group stock portfolio. Experiments on 31 stocks in accordance with four scenarios were conducted to show the merits and effectiveness of the proposed approach.																	0219-1377	0219-3116				JAN	2020	62	1					287	316		10.1007/s10115-019-01353-2													
J								Constructing biomedical domain-specific knowledge graph with minimum supervision	KNOWLEDGE AND INFORMATION SYSTEMS										Knowledge graph construction; Biomedical; Domain-specific; Minimum supervision		Domain-specific knowledge graph is an effective way to represent complex domain knowledge in a structured format and has shown great success in real-world applications. Most existing work on knowledge graph construction and completion shares several limitations in that sufficient external resources such as large-scale knowledge graphs and concept ontologies are required as the starting point. However, such extensive domain-specific labeling is highly time-consuming and requires special expertise, especially in biomedical domains. Therefore, knowledge extraction from unstructured contexts with minimum supervision is crucial in biomedical fields. In this paper, we propose a versatile approach for knowledge graph construction with minimum supervision based on unstructured biomedical domain-specific contexts including the steps of entity recognition, unsupervised entity and relation embedding, latent relation generation via clustering, relation refinement and relation assignment to assign cluster-level labels. The experimental results based on 24,687 unstructured biomedical science abstracts show that the proposed framework can effectively extract 16,192 structured facts with high precision. Moreover, we demonstrate that the constructed knowledge graph is a sufficient resource for the task of knowledge graph completion and new knowledge inference from unseen contexts.																	0219-1377	0219-3116				JAN	2020	62	1					317	336		10.1007/s10115-019-01351-4													
J								Analysis of loss functions for fast single-class classification	KNOWLEDGE AND INFORMATION SYSTEMS										Neural networks; Classification; Extreme classification		We consider neural network training, in applications in which there are many possible classes, but at test time, the task is a binary classification task of determining whether the given example belongs to a specific class. We define the single logit classification (SLC) task: training the network so that at test time, it would be possible to accurately identify whether the example belongs to a given class in a computationally efficient manner, based only on the output logit for this class. We propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC task. We show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. Our experiments show that indeed in almost all cases, losses that are aligned with the Principle of Logit Separation obtain at least 20% relative accuracy improvement in the SLC task compared to losses that are not aligned with it, and sometimes considerably more. Furthermore, we show that fast SLC does not cause any drop in binary classification accuracy, compared to standard classification in which all logits are computed, and yields a speedup which grows with the number of classes.																	0219-1377	0219-3116				JAN	2020	62	1					337	358		10.1007/s10115-019-01395-6													
J								Exploiting block co-occurrence to control block sizes for entity resolution	KNOWLEDGE AND INFORMATION SYSTEMS										Deduplication; Entity resolution; Heuristics; Data quality	RECORD LINKAGE; ADAPTIVE BLOCKING	The problem of identifying duplicated entities in a dataset has gained increasing importance during the last decades. Due to the large size of the datasets, this problem can be very costly to be solved due to its intrinsic quadratic complexity. Both researchers and practitioners have developed a variety of techniques aiming to speed up a solution to this problem. One of these techniques is called blocking, an indexing technique that splits the dataset into a set of blocks, such that each block contains entities that share a common property evaluated by a blocking key function. In order to improve the efficacy of the blocking technique, multiple blocking keys may be used, and thus, a set of blocking results is generated. In this paper, we investigate how to control the size of the blocks generated by the use of multiple blocking keys and maintain reasonable quality results, which is measured by the quality of the produced blocks. By controlling the size of the blocks, we can reduce the overall cost of solving an entity resolution problem and facilitate the execution of a variety of tasks (e.g., real-time and privacy-preserving entity resolution). For doing so, we propose many heuristics which exploit the co-occurrence of entities among the generated blocks for pruning, splitting and merging blocks. The experimental results we carry out using four datasets confirm the adequacy of the proposed heuristics for generating block sizes within a predefined range threshold as well as maintaining reasonable blocking quality results.																	0219-1377	0219-3116				JAN	2020	62	1					359	400		10.1007/s10115-019-01347-0													
J								Multi-agent system application for music features extraction, meta-classification and context analysis	KNOWLEDGE AND INFORMATION SYSTEMS										Music classification; Multi-agent system; Multi-label classification; Meta-classifiers; Musical genre; Musical emotions; Social networks	MOOD	Manual music classification is a slow and costly process. Most recent works about music auto-classification such as genre or emotions make this process easier, but are focused on a single task. In this work, a music multi-classification platform is presented. This platform is based on multi-agent systems, allowing to distribute the extraction, classification, and service tasks among agents. The platform performs a musical genre and emotional classification and provides context information of songs from social networks such as Twitter and Last.fm. The methods chosen based on meta-classifiers to perform single-label and multi-label classification obtain great results. In the case of multi-label classification, better results are obtained than in other previous works.																	0219-1377	0219-3116				JAN	2020	62	1					401	422		10.1007/s10115-018-1319-2													
J								Sequence Pattern Mining with Variables	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Data mining; Digital forensics; Printers; Production; Databases; Printing; Government; Sequence pattern mining; variables; item attributes; interleaving; digital forensics; temporal event abstraction		Sequence pattern mining (SPM) seeks to find multiple items that commonly occur together in a specific order. One common assumption is that the relevant differences between items are captured through creating distinct items. In some domains, this leads to an exponential increase in the number of items. This paper presents a new SPM, Sequence Mining of Temporal Clusters (SMTC), that allows item differentiation through attribute variables for domains with large numbers of items. It also provides a new technique for addressing interleaving, a phenomena that occurs when two sequences occur simultaneously resulting in their items alternating. By first clustering items temporally and only focusing on sequences after the temporal clusters are established, it sidesteps the traditional interleaving issues. SMTC is evaluated on a digital forensics dataset, a domain with a large number of items and frequent interleaving. Its results are compared with Discontinuous Varied Order Sequence Mining (DVSM) with variables added (DVSM-V). By adding variables, both algorithms reduce the data by 96 percent, and identify 100 percent of the events while keeping the false positive rate below 0.03 percent. SMTC mines the data in 20 percent of the time it takes DVSM-V and provides a lower false positive rate even at higher similarity thresholds.																	1041-4347	1558-2191				JAN 1	2020	32	1					177	187		10.1109/TKDE.2018.2881675													
J								A semi-supervised model for knowledge graph embedding	DATA MINING AND KNOWLEDGE DISCOVERY										Knowledge graph; Deep learning; Graph convolutional networks		Knowledge graphs have shown increasing importance in broad applications such as question answering, web search, and recommendation systems. The objective of knowledge graph embedding is to encode both entities and relations of knowledge graphs into continuous low-dimensional vector spaces to perform various machine learning tasks. Most of the existing works only focused on the local structure of knowledge graphs when utilizing structural information of entities, which may not sincerely preserve the global structure of knowledge graphs.In this paper, we propose a semi-supervised model by adopting graph convolutional networks to utilize both local and global structural information of entities. Specifically, our model takes textual information of each entity into consideration as entity attributes in the process of learning. We show the effectiveness of our model by applying it to two traditional tasks for knowledge graph: entity classification and link prediction. Experimental results on two well-known corpora reveal the advantages of this model compared to state-of-the-art methods on both tasks. Moreover, the results show that even with only 1% labeled data to train, our model can still achieve good performance.																	1384-5810	1573-756X				JAN	2020	34	1					1	20		10.1007/s10618-019-00653-z													
J								FastEE: Fast Ensembles of Elastic Distances for time series classification	DATA MINING AND KNOWLEDGE DISCOVERY										Time series classification; Scalable; Similarity measures; Ensembles	RETRIEVAL	In recent years, many new ensemble-based time series classification (TSC) algorithms have been proposed. Each of them is significantly more accurate than their predecessors. The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is currently the most accurate TSC algorithm when assessed on the UCR repository. It is a meta-ensemble of 5 state-of-the-art ensemble-based classifiers. The time complexity of HIVE-COTE-particularly for training-is prohibitive for most datasets. There is thus a critical need to speed up the classifiers that compose HIVE-COTE. This paper focuses on speeding up one of its components: Ensembles of Elastic Distances (EE), which is the classifier that leverages on the decades of research into the development of time-dedicated measures. Training EE can be prohibitive for many datasets. For example, it takes a month on the ElectricDevices dataset with 9000 instances. This is because EE needs to cross-validate the hyper-parameters used for the 11 similarity measures it encompasses. In this work, Fast Ensembles of Elastic Distances is proposed to train EE faster. There are two versions to it. The exact version makes it possible to train EE 10 times faster. The approximate version is 40 times faster than EE without significantly impacting the classification accuracy. This translates to being able to train EE on ElectricDevices in 13 h.																	1384-5810	1573-756X				JAN	2020	34	1					231	272		10.1007/s10618-019-00663-x													
J								ARBEE: Towards Automated Recognition of Bodily Expression of Emotion in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION										Body language; Emotional expression; Computer vision; Crowdsourcing; Video analysis; Perception; Statistical modeling	FACIAL EXPRESSION; BODY; HISTOGRAMS; FACE	Humans are arguably innately prepared to comprehend others' emotional expressions from subtle body movements. If robots or computers can be empowered with this capability, a number of robotic applications become possible. Automatically recognizing human bodily expression in unconstrained situations, however, is daunting given the incomplete understanding of the relationship between emotional expressions and body movements. The current research, as a multidisciplinary effort among computer and information sciences, psychology, and statistics, proposes a scalable and reliable crowdsourcing approach for collecting in-the-wild perceived emotion data for computers to learn to recognize body languages of humans. To accomplish this task, a large and growing annotated dataset with 9876 video clips of body movements and 13,239 human characters, named Body Language Dataset (BoLD), has been created. Comprehensive statistical analysis of the dataset revealed many interesting insights. A system to model the emotional expressions based on bodily movements, named Automated Recognition of Bodily Expression of Emotion (ARBEE), has also been developed and evaluated. Our analysis shows the effectiveness of Laban Movement Analysis (LMA) features in characterizing arousal, and our experiments using LMA features further demonstrate computability of bodily expression. We report and compare results of several other baseline methods which were developed for action recognition based on two different modalities, body skeleton and raw image. The dataset and findings presented in this work will likely serve as a launchpad for future discoveries in body language understanding that will enable future robots to interact and collaborate more effectively with humans.																	0920-5691	1573-1405				JAN	2020	128	1					1	25		10.1007/s11263-019-01215-y													
J								Synchronization Problems in Computer Vision with Closed-Form Solutions	INTERNATIONAL JOURNAL OF COMPUTER VISION										Synchronization; Averaging; Graph optimization; Multiple point-set registration; Structure from motion; Multi-view matching	MOTION; REGISTRATION; VIEWS; EIGENVECTORS; LOCALIZATION; ALGORITHM; NETWORKS	In this paper we survey and put in a common framework several works that have been developed in different contexts, all dealing with the same abstract problem, called synchronization by some authors, or averaging, or graph optimization by others. The problem consists in recovering some variables from a set of pairwise relation measurements. In particular, we concentrate on instances where the variables and the measures belong to a (semi-)group and the measures are their mutual differences (or ratios, depending on how the group operation is called). The groups we deal with have a matrix representation, which leads to an elegant theory and closed-form solutions.																	0920-5691	1573-1405				JAN	2020	128	1					26	52		10.1007/s11263-019-01240-x													
J								Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION										Robust attention model; Deep learning on sets; Multi-view 3D reconstruction		We study the problem of recovering an underlying 3D shape from a set of images. Existing learning based approaches usually resort to recurrent neural nets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to fuse multiple deep features encoded from input images. However, GRU based approaches are unable to consistently estimate 3D shapes given different permutations of the same set of input images as the recurrent unit is permutation variant. It is also unlikely to refine the 3D shape given more images due to the long-term memory loss of GRU. Commonly used pooling approaches are limited to capturing partial information, e.g., max/mean values, ignoring other valuable features. In this paper, we present a new feed-forward neural module, named AttSets, together with a dedicated training algorithm, named FASet, to attentively aggregate an arbitrarily sized deep feature set for multi-view 3D reconstruction. The AttSets module is permutation invariant, computationally efficient and flexible to implement, while the FASet algorithm enables the AttSets based network to be remarkably robust and generalize to an arbitrary number of input images. We thoroughly evaluate FASet and the properties of AttSets on multiple large public datasets. Extensive experiments show that AttSets together with FASet algorithm significantly outperforms existing aggregation approaches.																	0920-5691	1573-1405				JAN	2020	128	1					53	73		10.1007/s11263-019-01217-w													
J								Temporal Action Detection with Structured Segment Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Temporal action detection; Temporal action localization; Temporal action proposals; Human action recognition		This paper addresses an important and challenging task, namely detecting the temporal intervals of actions in untrimmed videos. Specifically, we present a framework called structured segment network (SSN). It is built on temporal proposals of actions. SSN models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and precise localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end manner. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping is devised to generate high quality action proposals. We further study the importance of the decomposed discriminative model and discover a way to achieve similar accuracy using a single classifier, which is also complementary with the original SSN design. On two challenging benchmarks, THUMOS'14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.																	0920-5691	1573-1405				JAN	2020	128	1					74	95		10.1007/s11263-019-01211-2													
J								Tracking Persons-of-Interest via Unsupervised Representation Adaptation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Face tracking; Transfer learning; Convolutional neural networks; Triplet loss	MULTITARGET	Multi-face tracking in unconstrained videos is a challenging problem as faces of one person often can appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Existing multi-target tracking methods often use low-level features which are not sufficiently discriminative for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face representations using convolutional neural networks (CNNs). Unlike existing CNN-based approaches which are only trained on large-scale face image datasets offline, we automatically generate a large number of training samples using the contextual constraints for a given video, and further adapt the pre-trained face CNN to the characters in the specific videos using discovered training samples. The embedding feature space is fine-tuned so that the Euclidean distance in the space corresponds to the semantic face similarity. To this end, we devise a symmetric triplet loss function which optimizes the network more effectively than the conventional triplet loss. With the learned discriminative features, we apply an EM clustering algorithm to link tracklets across multiple shots to generate the final trajectories. We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques.																	0920-5691	1573-1405				JAN	2020	128	1					96	120		10.1007/s11263-019-01212-1													
J								Shape-From-Template with Curves	INTERNATIONAL JOURNAL OF COMPUTER VISION										Curve reconstruction; Shape-from-Template; Isometry; Discrete Hidden Markov Model; Partial differential equations	RECONSTRUCTION; DEFORMATION; MODELS	Shape-from-Template (SfT) is the problem of using a shape template to infer the shape of a deformable object observed in an image. The usual case of SfT is 'Surface' SfT, where the shape is a 2D surface embedded in 3D, and the image is a 2D perspective projection. We introduce 'Curve' SfT, comprising two new cases of SfT where the shape is a 1D curve. The first new case is when the curve is embedded in 2D and the image a 1D perspective projection. The second new case is when the curve is embedded in 3D and the image a 2D perspective projection. We present a thorough theoretical study of these new cases for isometric deformations, which are a good approximation of ropes, cables and wires. Unlike Surface SfT, we show that Curve SfT is only ever solvable up to discrete ambiguities. We present the necessary and sufficient conditions for solvability with critical point analysis. We further show that unlike Surface SfT, Curve SfT cannot be solved locally using exact non-holonomic Partial Differential Equations. Our main technical contributions are two-fold. First, we give a stable, global reconstruction method that models the problem as a discrete Hidden Markov Model. This can generate all candidate solutions. Second, we give a non-convex refinement method using a novel angle-based deformation parameterization. We present quantitative and qualitative results showing that real curve shaped objects such as a necklace can be successfully reconstructed with Curve SfT.																	0920-5691	1573-1405				JAN	2020	128	1					121	165		10.1007/s11263-019-01214-z													
J								Classifier and Exemplar Synthesis for Zero-Shot Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION										Zero-shot learning; Generalized zero-shot learning; Transfer learning; Object recognition; Semantic embeddings; Evaluation metrics	DATABASE	Zero-shot learning (ZSL) enables solving a task without the need to see its examples. In this paper, we propose two ZSL frameworks that learn to synthesize parameters for novel unseen classes. First, we propose to cast the problem of ZSL as learning manifold embeddings from graphs composed of object classes, leading to a flexible approach that synthesizes "classifiers" for the unseen classes. Then, we define an auxiliary task of synthesizing "exemplars" for the unseen classes to be used as an automatic denoising mechanism for any existing ZSL approaches or as an effective ZSL model by itself. On five visual recognition benchmark datasets, we demonstrate the superior performances of our proposed frameworks in various scenarios of both conventional and generalized ZSL. Finally, we provide valuable insights through a series of empirical analyses, among which are a comparison of semantic representations on the full ImageNet benchmark as well as a comparison of metrics used in generalized ZSL. Our code and data are publicly available at .																	0920-5691	1573-1405				JAN	2020	128	1					166	201		10.1007/s11263-019-01193-1													
J								Bi-Real Net: Binarizing Deep Network Towards Real-Network Performance	INTERNATIONAL JOURNAL OF COMPUTER VISION										1-Bit CNNs; Binary convolution; Shortcut; 1-Layer-per-block	TRACKING	In this paper, we study 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the lacking of a representational capability and the training difficulty impede 1-bit CNNs from performing as well as real-valued networks. To this end, we propose Bi-Real net with a novel training algorithm to tackle these two challenges. To enhance the representational capability, we propagate the real-valued activations generated by each 1-bit convolution via a parameter-free shortcut. To address the training difficulty, we propose a training algorithm using a tighter approximation to the derivative of the sign function, a magnitude-aware binarization for weight updating, a better initialization method, and a two-step scheme for training a deep network. Experiments on ImageNet show that an 18-layer Bi-Real net with the proposed training algorithm achieves 56.4% top-1 classification accuracy, which is 10% higher than the state-of-the-arts (e.g., XNOR-Net), with a greater memory saving and a lower computational cost. Bi-Real net is also the first to scale up 1-bit CNNs to an ultra-deep network with 152 layers, and achieves 64.5% top-1 accuracy on ImageNet. A 50-layer Bi-Real net shows comparable performance to a real-valued network on the depth estimation task with merely a 0.3% accuracy gap.																	0920-5691	1573-1405				JAN	2020	128	1					202	219		10.1007/s11263-019-01227-8													
J								Predicting Intentions from Motion: The Subject-Adversarial Adaptation Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION										Action recognition and prediction; Human intentions; Grasping; Kinematic analysis; Adversarial domain adaptation	ACTION RECOGNITION; DOMAIN ADAPTATION	This paper aims at investigating the action prediction problem from a pure kinematic perspective. Specifically, we address the problem of recognizing future actions, indeed human intentions, underlying a same initial (and apparently unrelated) motor act. This study is inspired by neuroscientific findings asserting that motor acts at the very onset are embedding information about the intention with which are performed, even when different intentions originate from a same class of movements. To demonstrate this claim in computational and empirical terms, we designed an ad hoc experiment and built a new 3D and 2D dataset where, in both training and testing, we analyze a same class of grasping movements underlying different intentions. We investigate how much the intention discriminants generalize across subjects, discovering that each subject tends to affect the prediction by his/her own bias. Inspired by the domain adaptation problem, we propose to interpret each subject as a domain, leading to a novel subject adversarial paradigm. The proposed approach favorably copes with our new problem, boosting the considered baseline features encoding 2D and 3D information and which do not exploit the subject information.																	0920-5691	1573-1405				JAN	2020	128	1					220	239		10.1007/s11263-019-01234-9													
J								Single Image Dehazing via Multi-scale Convolutional Neural Networks with Holistic Edges	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image dehazing; Image defogging; Convolutional neural network; Transmission map	VISIBILITY; FRAMEWORK; WEATHER; VISION	Single image dehazing has been a challenging problem which aims to recover clear images from hazy ones. The performance of existing image dehazing methods is limited by hand-designed features and priors. In this paper, we propose a multi-scale deep neural network for single image dehazing by learning the mapping between hazy images and their transmission maps. The proposed algorithm consists of a coarse-scale net which predicts a holistic transmission map based on the entire image, and a fine-scale net which refines dehazed results locally. To train the multi-scale deep network, we synthesize a dataset comprised of hazy images and corresponding transmission maps based on the NYU Depth dataset. In addition, we propose a holistic edge guided network to refine edges of the estimated transmission map. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of quality and speed.																	0920-5691	1573-1405				JAN	2020	128	1					240	259		10.1007/s11263-019-01235-8													
J								Detecting Coherent Groups in Crowd Scenes by Multiview Clustering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Feature extraction; Clustering methods; Optical imaging; Electronic mail; Videos; Computer science; Correlation; Crowd analysis; group detection; context descriptor; multiview clustering; graph clustering	TRACKING	Detecting coherent groups is fundamentally important for crowd behavior analysis. In the past few decades, plenty of works have been conducted on this topic, but most of them have limitations due to the insufficient utilization of crowd properties and the arbitrary processing of individuals. In this study, a Multiview-based Parameter Free framework (MPF) is proposed. Based on the L1-norm and L2-norm, we design two versions of the multiview clustering method, which is the main part of the proposed framework. This paper presents the contributions on three aspects: (1) a new structural context descriptor is designed to characterize the structural properties of individuals in crowd scenes; (2) a self-weighted multiview clustering method is proposed to cluster feature points by incorporating their orientation and context similarities; and (3) a novel framework is introduced for group detection, which is able to determine the group number automatically without any parameter or threshold to be tuned. The effectiveness of the proposed framework is evaluated on real-world crowd videos, and the experimental results show its promising performance on group detection. In addition, the proposed multiview clustering method is also evaluated on a synthetic dataset and several standard benchmarks, and its superiority over the state-of-the-art competitors is demonstrated.																	0162-8828	1939-3539				JAN 1	2020	42	1					46	58		10.1109/TPAMI.2018.2875002													
J								QUALITY OF SERVICE PROVISIONING USING MULTICRITERIA HANDOVER STRATEGY IN OVERLAID NETWORKS	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Macrocells; Femtocells; Handovers; Packet loss ratio; Throughput; Jitter; Delay; Signalling cost	DECISION ALGORITHM; FEMTOCELL	The poor coverage problem inside the buildings and the existence of disparate wireless technologies have led to the development of integrated macro femtocellular networks. Correct and efficient vertical handover decision is of utmost importance in such integrated macro femtocellular networks. Initially, Received Signal Strength (RSS) used to be the sole criterion for taking a decision for vertical handover. Such decision leads to inaccurate results and therefore unnecessary handovers. In this paper, a multicriteria vertical handover strategy based on received signal strength, user velocity, number of users in a cell and data rate of the cell is proposed. This proposed strategy and the traditional algorithm are simulated and the results are analyzed quantitatively for different QoS parameters namely ping pong rate, packet loss ratio, throughput, packet delay, jitter and signalling cost. The results show significant improvement in the QoS parameters when the proposed strategy is used as compared to traditional algorithm.																	0127-9084						2020	33	1					1	21		10.22452/mjcs.vol33no1.1													
J								INVESTIGATION OF IMAGE STITCHING REFINEMENT WITH ENHANCED CORRELATION COEFFICIENT	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Image registration; Image stitching	ALIGNMENT	Image stitching combines multiple overlapping scenes into a panorama. It can be applied in video stitching, super-resolution, and medical imaging. Post-processing routines such as exposure compensation, seam line adjustment, and blending are often performed to increase its visual appeal. Our aim is to increase the registration accuracy between adjacent image pairs in a stitching algorithm. This is done by adding an area-based registration step, namely Enhanced Correlation Coefficient (ECC), which refines the original feature-based image registration. The performance of registrations in the stitching algorithm is evaluated using root-mean-squared error of control points, structural similarity index, and universal image quality index. The Boat and Graffiti datasets from the Oxford Robotics Database are used for experiments. It is found that ECC largely improves registrations in stitching except for a slight increase in root-mean-squared error of control points in the Boat dataset. In addition, ECC's enhancement makes the registration outperform the ground truth in one image of both datasets.																	0127-9084						2020	33	1					22	34		10.22452/mjcs.vol33no1.2													
J								AUTOMATIC MUSIC COMPOSITION USING GENETIC ALGORITHM AND ARTIFICIAL NEURAL NETWORKS	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Genetic algorithm; Neural network; Automatic music composition	SELECTION	The aim of this paper is to automatically compose new pleasing music from randomly generated notes without human intervention. To achieve this goal, Genetic Algorithm was implemented to generate random notes. The Neural Network was trained on a set of melodies to learn their regularity of patterns and then it is used as a fitness evaluator for the generated music from the Genetic Algorithm. Four Genetic Algorithms (using different combinations of tournament, roulette-wheel selections and one-point, two-point crossovers) were used in generating music to compare them according to which one is the most suitable for music composition. The experiments show that using tournament selection and two-point crossover produces better music patterns than using other combinations by 57%. The experiments show that the generated music was good and the results were promising. For evaluation, 10 music experts were asked to listen and evaluate four samples of the generated music; two of them were evaluated high from the Neural Network and two were evaluated low. Then we compared their results with the results from the Neural Network. The results show that the error rate for Neural Network was 16.7% and accuracy was 83.3%.																	0127-9084						2020	33	1					35	51		10.22452/mjcs.vol33no1.3													
J								REAL-TIME AUTOMATED CONTOUR BASED MOTION TRACKING USING A SINGLE-CAMERA FOR UPPER LIMB ANGULAR MOTION MEASUREMENT	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Angular Measurement; Stroke Rehabilitation; Home-Based Rehabilitation; Upper Limb; Motion Tracking; Skin Segmentation; Contour-based Measurement	DATA FUSION; INTEGRATION	Upper limb angular motion measurement is particularly a subject of interest in the field of rehabilitation. It is commonly measured manually by physiotherapist via goniometer to monitor stroke patient's progress. However, manual measurement has many inherent drawbacks such as the need to have both patient and physiotherapist to be at the same place. In this paper, an automated real-time single-camera upper limb angular motion measuring system is proposed for home-based rehabilitation to aid physiotherapist to monitor patient's progress. The measurement of concern are angle measurement of elbow extension, elbow flexion, wrist flexion and wrist extension. To do this, we propose a method that utilized predefined coordinate points extracted from the contours of the object named as contour based motion tracking. The proposed method overcome problems of prior target tracking techniques such as Kalman filter, Optical flow and Cam-shift. The proposed method includes skin region segmentation and arm modelling for motion tracking. Prior skin segmentation techniques suffer from fixed threshold value set by the user. Therefore, we introduce dynamic threshold based on the lower and upper threshold boundary of isolated skin regions from the background. To ensure the reliability of our skin segmentation method, we compare them with four different related algorithms. The result shows that our skin segmentation method outperformed the prior method with 93% detection accuracy. Following the segmentation, we model the arm motion tracking by formulating mathematical equation of various points and motion velocities to track the arm. We then model the wrist and elbow position to estimate arm angular motion. The method is put together and tested with real human subject and other test settings. The result shows that our proposed method able to produce an accurate and reliable reading of +-1.25 average range of error from actual physiotherapist reading.																	0127-9084						2020	33	1					52	77		10.22452/mjcs.vol33no1.4													
J								COMPRESSIVE SENSING BASED CONVOLUTIONAL NEURAL NETWORK FOR OBJECT DETECTION	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Deep compression; Convolutional neural network; Discrete cosine transform; Compressive sensing; YOLO; Object detection		Deep neural networks (DNN) have shown significant performance in several domains including computer vision and machine learning. Convolutional Neural Networks (CNN), known as a particular type of DNN, have shown their promising potentials in discovering vision-based patterns from quantity of labeled images. Many CNN-based algorithms are thus proposed to solve the problem of object detection and object recognition. However, CNN-based systems are hard to deploy on embedded systems due to their computationally and storage intensive. In this paper, we propose a method to compress convolutional neural network to decreases its computation and storage cost by exploiting inherent redundancy property of parameters in different kinds of layers of CNN architecture. During the compression, we firstly construct parameter matrices from different kinds of layers and convert parameter matrices to frequency domain through discrete cosine transform (DCT). Due to the smooth property of parameters when processing images, the resulting frequency matrices are dominated by low-frequency components. We thus prune high-frequency part to emphasize the dominating part of frequency matrix and make the frequency matrix sparse. Then, the sparse frequency matrices are sampled with distributed random Gaussian matrix under the guiding of compress sensing. Finally, we retrain the network with the sampling matrices to fine-tune the remaining parameters. We evaluate the proposed method on several typical convolutional neural network and show it outperforms one latest compression approach.																	0127-9084						2020	33	1					78	89		10.22452/mjcs.vol33no1.5													
J								Fluctuation Scaling of Neuronal Firing and Bursting in Spontaneously Active Brain Circuits	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Microelectrode array; spontaneous neural activity; neuronal bursting; power law; scale invariance; primary somatosensory cortex	SPIKING; PATTERNS; OSCILLATIONS; INHIBITION; DYNAMICS; MODEL; CONNECTIVITY; STIMULATION; INFORMATION; VARIABILITY	We employed high-density microelectrode arrays to investigate spontaneous firing patterns of neurons in brain circuits of the primary somatosensory cortex (S1) in mice. We recorded from over 150 neurons for 10 min in each of eight different experiments, identified their location in Si, sorted their action potentials (spikes), and computed their power spectra and inter-spike interval (ISI) statistics. Of all persistently active neurons, 92% fired with a single dominant frequency - regularly firing neurons (RNs) - from 1 to 8Hz while 8% fired in burst with two dominant frequencies - bursting neurons (BNs) - corresponding to the inter-burst (2-6 Hz) and intra-burst intervals (20-160Hz). RNs were predominantly located in layers 2/3 and 5/6 while BNs localized to layers 4 and 5. Across neurons, the standard deviation of ISI was a power law of its mean, a property known as fluctuation scaling, with a power law exponent of 1 for RNs and 1.25 for BNs. The power law implies that firing and bursting patterns are scale invariant: the firing pattern of a given RN or BN resembles that of another RN or BN, respectively, after a time contraction or dilation. An explanation for this scale invariance is discussed in the context of previous computational studies as well as its potential role in information processing.																	0129-0657	1793-6462				JAN	2020	30	1							1950017	10.1142/S0129065719500175													
J								Exploring the Brain Responses to Driving Fatigue Through Simultaneous EEG and fNIRS Measurements	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										EEG; fNIRS; fatigue driving; oxygenated hemoglobin	NEAR-INFRARED SPECTROSCOPY; MENTAL FATIGUE; FMRI; ATTENTION; ELECTROENCEPHALOGRAPHY	Fatigue is one problem with driving as it can lead to difficulties with sustaining attention, behavioral lapses, and a tendency to ignore vital information or operations. In this research, we explore multimodal physiological phenomena in response to driving fatigue through simultaneous functional near-infrared spectroscopy (fNIRS) and electroencephalography (EEG) recordings with the aim of investigating the relationships between hemodynamic and electrical features and driving performance. Sixteen subjects participated in an event-related lane-deviation driving task while measuring their brain dynamics through fNIRS and EEGs. Three performance groups, classified as Optimal, Suboptimal, and Poor, were defined for comparison. From our analysis, we find that tonic variations occur before a deviation, and phasic variations occur afterward. The tonic results show an increased concentration of oxygenated hemoglobin (HbO(2)) and power changes in the EEG theta, alpha, and beta bands. Both dynamics are significantly correlated with deteriorated driving performance. The phasic EEG results demonstrate event-related desynchronization associated with the onset of steering vehicle in all power bands. The concentration of phasic HbO(2) decreased as performance worsened. Further, the negative correlations between tonic EEG delta and alpha power and HbO(2) oscillations suggest that activations in HbO(2) are related to mental fatigue. In summary, combined hemodynamic and electrodynamic activities can provide complete knowledge of the brain's responses as evidence of state changes during fatigue driving.																	0129-0657	1793-6462				JAN	2020	30	1							1950018	10.1142/S0129065719500187													
J								Reachability Analysis of Neural Masses and Seizure Control Based on Combination Convolutional Neural Network	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Epilepsy; EEG; neural masses model; combination convolutional neural network	EEG-BASED DIAGNOSIS; MATHEMATICAL-MODEL; SYNCHRONIZATION; METHODOLOGY; EPILEPSY; CLASSIFICATION; STIMULATION; TRANSITION; COHERENCE	Epileptic seizures arise from synchronous firing of multiple spatially separated neural masses; therefore, many synchrony measures are used for seizure detection and characterization. However, synchrony measures reflect only the overall interaction strength among populations of neurons but cannot reveal the coupling strengths among individual populations, which is more important for seizure control. The concepts of reachability and reachable cluster were proposed to denote the coupling strengths of a set of neural masses. Here, we describe a seizure control method based on coupling strengths using combination convolutional neural network (CCNN) modeling. The neurophysiologically based neural mass model (NMM), which can bridge signal processing and neurophysiology, was used to simulate the proposed controller. Although the adjacency matrix and reachability matrix could not be identified perfectly, the vast majority of adjacency values were identified, reaching 95.64% using the CCNN with an optimal threshold. For cases of discrete and continuous coupling strengths, the proposed controller maintained the average reachable cluster strengths at about 0.1, indicating effective seizure control.																	0129-0657	1793-6462				JAN	2020	30	1							1950023	10.1142/S0129065719500230													
J								Spatiotemporal Oscillatory Patterns During Working Memory Maintenance in Mild Cognitive Impairment and Subjective Cognitive Decline	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Induced oscillatory activity; magnetoencephalography (MEC); mild cognitive impairment (MCI); subjective cognitive decline (SCD); Alzheimer's disease (AD); working memory (WM)	MEDIAL TEMPORAL-LOBE; LONG-TERM-MEMORY; THETA-OSCILLATIONS; ALZHEIMERS-DISEASE; PREFRONTAL CORTEX; FUNCTIONAL CONNECTIVITY; GAMMA-ACTIVITY; ALPHA-ACTIVITY; OLDER-ADULTS; LIFE-STYLE	Working memory (WM) is a crucial cognitive process and its disruption is among the earliest symptoms of Alzheimer's disease. While alterations of the neuronal processes underlying WM have been evidenced in mild cognitive impairment (MCI), scarce literature is available in subjective cognitive decline (SCD). We used rnagnetoencephalography during a WM task performed by MCI (n = 45), SCD (n = 49) and healthy elders (n 49) to examine group differences during the maintenance period (0-4000 ms). Data were analyzed using time-frequency analysis and significant oscillatory differences were localized at the source level. Our results indicated significant differences between groups, mainly during the early maintenance (250-1250 ms) in the theta, alpha and beta bands and in the late maintenance (2750-3750 ms) in the theta band. MCI showed lower local synchronization in fronto-temporal cortical regions in the early theta-alpha window relative to controls (p = 2 x 10(-03)) and SCD (p = 4 x 10(-03)), and in the late theta window relative to controls (p =1 x 10(03)) and SCD (p = 0.01). Early theta-alpha power was significantly correlated with memory scores (rho = 0.24, p = 0.02) and late theta power was correlated with task performance (rho = 0.24, p = 0.03) and functional activity scores (rho = -0.23, p = 0.02). In the early beta window, MCI showed reduced power in ternporo-posterior regions relative to controls (p = 3 x 10(-03)) and SCD (p = 0.02). Our results may suggest that these alterations would reflect that memory-related networks are damaged.																	0129-0657	1793-6462				JAN	2020	30	1							1950019	10.1142/S0129065719500199													
J								A Cerebellum-Inspired Learning Approach for Adaptive and Anticipatory Control	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										internal forward model; cerebellum; motor control; adaptive learning; Smith predictor; bio-inspired; recurrent	INTERNAL-MODELS; MOTOR CONTROL	The cerebellum, which is responsible for motor control and learning, has been suggested to act as a Smith predictor for compensation of time-delays by means of internal forward models. However, insights about how forward model predictions are integrated in the Smith predictor have not yet been unveiled. To fill this gap, a novel bio-inspired modular control architecture that merges a recurrent cerebellar-like loop for adaptive control and a Smith predictor controller is proposed. The goal is to provide accurate anticipatory corrections to the generation of the motor commands in spite of sensory delays and to validate the robustness of the proposed control method to input and physical dynamic changes. The outcome of the proposed architecture with other two control schemes that do not include the Smith control strategy or the cerebellar-like corrections are compared. The results obtained on four sets of experiments confirm that the cerebellum-like circuit provides more effective corrections when only the Smith strategy is adopted and that minor tuning in the parameters, fast adaptation and reproducible configuration are enabled.																	0129-0657	1793-6462				JAN	2020	30	1							1950028	10.1142/S012906571950028X													
J								Human Gait Recognition Based on Frame-by-Frame Gait Energy Images and Convolutional Long Short-Term Memory	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Gait classification; deep learning; Long Short-Term Memory (LSTM); frame-by-frame GEI (ff-GEI)	DEEP; NETWORK; PERFORMANCE	Human gait recognition is one of the most promising biometric technologies, especially for unobtrusive video surveillance and human identification from a distance. Aiming at improving recognition rate, in this paper we study gait recognition using deep learning and propose a novel method based on convolutional Long Short-Term Memory (Conv-LSTM). First, we present a variation of Gait Energy Images, i.e. frame-by-frame GEI (ff-GEI), to expand the volume of available Gait Energy Images (GEI) data and relax the constraints of gait cycle segmentation required by existing gait recognition methods. Second, we demonstrate the effectiveness of ff-GEI by analyzing the cross-covariance of one person's gait data. Then, making use of the temporality of our human gait, we design a novel gait recognition model using Conv-LSTM. Finally, the proposed method is evaluated extensively based on the CASIA Dataset B for cross-view gait recognition, furthermore the OU-ISIR Large Population Dataset is employed to verify its generalization ability. Our experimental results show that the proposed method outperforms other algorithms based on these two datasets. The results indicate that the proposed ff-GEI model using Conv-LSTM, coupled with the new gait representation, can effectively solve the problems related to cross-view gait recognition.																	0129-0657	1793-6462				JAN	2020	30	1							1950027	10.1142/S0129065719500278													
J								An intelligent framework for monitoring dengue fever risk using LDA-ANFIS	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Dew computing; cloud computing; coronary heart disease; Linear Discriminant Analysis; Internet of Things; Adaptive Neuro Fuzzy Inference System	CORONARY-HEART-DISEASE; HEALTH; SYSTEM; MODEL	Dengue fever (DENF) is considered as the world's fastest-growing vector-borne disease. Its timely and proper treatment possesses great significance, as it affects vital body organs and causes morbidity and mortality. In this paper, a dew-cloud assisted DENF-based cyber physical system (CPS) is proposed. This system diagnoses DENF, and monitors the effects of DENF infection on vital body organs. This system uses Linear Discriminant Analysis-Adaptive Neuro Fuzzy Inference System (LDA-ANFIS) classification at the dew space to diagnose the DENF, and monitor the coronary heart disease (CHD) risk of the DENF infected users in real-time. Based on the real-time diagnosis and monitoring, the system immediately alerts the concerned stakeholders and provides timely medical support in case of any dengue-related emergency. The proposed CPS also assesses the Temporal Network Analysis (TNA)-based DENF outbreak risk, at the cloud space to control and prevent the outbreak of DENF by identifying the critical areas and individuals, which contributes in spreading DENF infection. The experimental evaluation of the proposed system acknowledges performance efficiency and utilization through various experimental and statistical approaches.																	1876-1364	1876-1372					2020	12	1					5	20		10.3233/AIS-200547													
J								A deep learning model to predict lower temperatures in agriculture	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Deep Learning; LSTM; precision agriculture; IoT	PRECISION AGRICULTURE; TECHNOLOGIES; LSTM	Deep learning techniques provide a novel framework for prediction and classification in decision-making procedures that are widely applied in different fields. Precision agriculture is one of these fields where the use of decision-making technologies provides better production with better costs and a greater benefit for farmers. This paper develops an intelligent framework based on a deep learning model for early prediction of crop frost to help farmers activate anti-frost techniques to save the crop. This model is based on a long short-term memory (LSTM) model and it is designed to predict low temperatures. The model is based on information from an IoT infrastructure deployed on two plots in Murcia (Southeast of Spain). Three experiments are performed; a cross validation to validate the model from the most pessimistic point of view, a validation of 24 consecutive hours of temperatures, in order to know 24 hours before the possible temperature drop and a comparison with two traditional time series prediction techniques, namely Auto Regressive Integrated Moving Average and the Gaussian process. The results obtained are satisfactory, being better the results of the LSTM, obtaining an average quadratic error of less than a Celsius degree and a determination coefficient R-2 greater than 0.95.																	1876-1364	1876-1372					2020	12	1					21	34		10.3233/AIS-200546													
J								Deployment framework for the Internet of water meters using computer vision on ARM platform	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										ARM platform; computer vision; internet of things; machine learning; water meter		This article presents the conception of a new method developed mainly in Python to automate the reading process of water meters with an analog display using computer vision and machine learning. A camera captures the consumption value in the water meter, and the yielded image undergoes image processing until the digits are detected and isolated. Then the digits are passed into an SVM machine-learning model that carries out a high accuracy OCR. The software is executed over an ARM platform running Linux. The data resultant from the automated metering, such as the device identification number, event date and time in UTC, consumption value, volume and time variations, flow, and display image, are locally stored and transmitted to a cloud server through VPN in a Wi-Fi and cellular network connection, or by SMS, enabling a remote supervision. Thereby, the automatic metering method features a new way to perform predictive analysis and management of water and meters proactively and can be replicated for digital-display water meters, as well as extended to handle automatic metering on electricity and gas meters as well.																	1876-1364	1876-1372					2020	12	1					35	60		10.3233/AIS-200544													
J								Intention mining: A deep learning-based approach for smart devices	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Smart environment; IoT devices; intelligent wearable devices; smartwatches; machine learning; deep learning; intention mining	PHYSICAL-ACTIVITY INTERVENTION; FITBIT; ACCURACY; VALIDATION; ACCEPTANCE	Smartwatches have become increasingly popular due to their ability to track human activities. The tracked information can be shared with other devices, such as smartphones, and used for scheduling, time management, and health management. Although several studies have focused on developing techniques for natural language text, users intention-to-recommend smartwatches have never been investigated. Consequently, the manufacturers, as well as potential buyers cannot get a holistic view of users' perception of the smart device of their interest. Also, the non-availability of publicly available benchmark corpus has thwarted the development of intention mining techniques. Retrospectively, this study has proposed an approach for mining users' intention to recommend smartwatches. In particular, we have employed an innovative approach, involving a screening processing and annotation guidelines, to develop the first-ever manually annotated corpus for mining intention-to-recommend smartwatches. Furthermore, we have performed experiments using two deep-learning techniques and five types of word embeddings to evaluate their effectiveness for intention mining Finally, the recommendation sentences are synthesized to develop a deeper understanding of the user feedback on the selected products.																	1876-1364	1876-1372					2020	12	1					61	73		10.3233/AIS-200545													
J								A blind and robust color images water marking method based on block transform and secured by modified 3-dimensional Henon map	SOFT COMPUTING										Watermarking; Color image; 3D Henon chaotic map; Integer wavelet transform; Discrete wavelet transform; Contourlet transform	WATERMARKING SCHEME; WAVELET TRANSFORM; CHAOTIC MAP; SVD; ALGORITHM; GENERATOR; FRAGILE; DCT; DWT	This paper proposes a blind and robust color image watermarking method based on a new three-dimensional Henon chaotic map and uses integer wavelet transform, discrete wavelet transform and contourlet transform in embedding and extracting processes. In the presented approach, color images are divided into 4x4main nonoverlapping parts, and one of the transforms is applied to these parts. Then the low-low sub-band of transform is selected. The suggested map is used to find 2x2 blocks in the embedding process. The bits of watermark are embedded in the parts of images to increase the robustness of the proposed watermarking scheme. To improve the quality of the final watermark, the suggested technique uses a correction process in the extracting process. In this paper, the bifurcation diagram, Lyapunov exponent, cobweb plot and trajectory diagram are used to show the chaotic behavior of the proposed map. Based on DIEHARD, ENT and NIST test suites, the suggested map can be used as a pseudo-random number generator. The simulation results show that the proposed watermarking algorithm is robust against most image processing attacks like salt & pepper, cropping, low-pass filter, wiener filter, blurring, etc. The comparison results between the suggested watermarking scheme, and some similar methods show that the presented technique has good performance, imperceptibility, acceptable robustness and outperforms most related methods.																	1432-7643	1433-7479				JAN	2020	24	2					771	794		10.1007/s00500-019-04524-z													
J								Filters and ideals in the generalization of pseudo-BL algebras	SOFT COMPUTING										Filters; Ideals; Pseudo-BL algebras; Quasi-pseudo-BL algebras; Quasi-pseudo-MV algebras		In this paper, we introduce the notion of quasi-pseudo-BL algebras as the generalization of pseudo-BL algebras and quasi-pseudo-MV algebras. First, we investigate the properties of quasi-pseudo-BL algebras and show the subdirect product composition of any quasi-pseudo-BL algebra. Especially, some properties of good quasi-pseudo-BL algebras are presented. Second, we discuss the filters of quasi-pseudo-BL algebras and prove that there exists a bijective correspondence between normal filters and filter congruences on a quasi-pseudo-BL algebra. The properties of some special filters are also discussed. Finally, we study the ideals of quasi-pseudo-BL algebras and investigate some connections between ideals and filters of a quasi-pseudo-BL algebra.																	1432-7643	1433-7479				JAN	2020	24	2					795	812		10.1007/s00500-019-04528-9													
J								Algebraic semantics of the {->, square}-fragment of Propositional Lax Logic	SOFT COMPUTING										Hilbert algebras; Modal operators; Topological representation	HILBERT-ALGEBRAS	In this paper, we will study a particular subvariety of Hilbert algebras with a modal operator square, called Lax Hilbert algebras. These algebras are the algebraic semantic of the {square, ->}-fragment of a particular intuitionistic modal logic, called Propositional Lax Logic (PLL), which has applications to the formal verification of computer hardware. These algebras turn to be a generalization of the variety of Heyting algebras with a modal operator studied, under different names, by Macnab (Algebra Univ 12:5-29, 1981), Goldblatt (Math Logic Q 27(31-35):495-529, 1981; J Logic Comput 21(6):1035-1063, 2010) and by Bezhanishvili and Ghilardi (Ann Pure Appl Logic 147:84-100, 2007). We shall prove that the set of fixpoints of a Lax Hilbert algebra < A, square > is a Hilbert algebra such that its dual space is homeomorphic to the subspace of reflexive elements of the dual space of A. We will define the notion of subframe of a Hilbert space < X, K >, and we will prove that there is a 1-1 correspondence between subframes of < X, K > and binary relations Q. X x X such that < X, K, Q > is a Lax Hilbert space. In addition, we will define the notion of subframe variety and we will prove that any variety of Hilbert algebras is a subframe variety.																	1432-7643	1433-7479				JAN	2020	24	2					813	823		10.1007/s00500-019-04536-9													
J								Spectral resolutions and observables in n-perfect MV-algebras	SOFT COMPUTING										Spectral resolution; Observable; MV-algebra; Perfect MV-algebra; n-perfect MV-algebra; Principal radical; Rad-Dedekind sigma-completeness; State; Categorical equivalence; Loomis-Sikorski theorem		We define an observable as a kind of a sigma-homomorphism from the Borel sigma-algebra of the real line into a Rad-Dedekind sigma-complete n-perfect MV-algebra with principal radical preserving disjoint unions. We show that there is a one-to-one correspondence between spectral resolutions (defined now with four properties and not with three ones). This correspondence allows us to define a partial order on the set of observables, called the Olson order, as well as a sum of observables which converts the set of observables into a commutative lattice-ordered semigroup with respect to the Olson order and with the sum of observables.																	1432-7643	1433-7479				JAN	2020	24	2					843	860		10.1007/s00500-019-04543-w													
J								A note on the algebraicity of L-fuzzy subalgebras in universal algebra	SOFT COMPUTING										Universal algebra; Algebraic lattice; Compact element; L-fuzzy subalgebra	LATTICE	Given a universal algebra A := ( A; F-A) of type F, it is well known that the lattice Sub(A) of subuniverses of A is algebraic and its compact elements are exactly finitely generated subuniverses of A. In this paper, under a distributive algebraic lattice L := (L; boolean AND, boolean OR; 0, 1), we characterize the compact elements of the lattice Fs(A, L) of L-fuzzy subalgebras of A, which is an extension of Sub(A) and show that the latter is algebraic.																	1432-7643	1433-7479				JAN	2020	24	2					895	899		10.1007/s00500-019-04594-z													
J								learning with policy prediction in continuous state-action multi-agent decision processes	SOFT COMPUTING										Fuzzy policy iteration; Reinforcement learning; Multi-agent learning; Fuzzy systems	COMPREHENSIVE SURVEY	Inspired by recent attention to multi-agent reinforcement learning (MARL), the effort to provide efficient methods in this field is increasing. But, there are many issues which make this field challenging. Decision making of an agent depends on the other agents' behavior while sharing information is not always possible. On the other hand, predicting other agents' policies while they are also learning is a difficult task. Also, some agents in a multi-agent environment may not behave rationally. In such cases, achieving Nash equilibrium, as a target in a system with ideal behavior, is not possible and the best policy is the best response to the other agents' policies. In addition, many real-world multi-agent problems have a continuous nature in their state and action spaces. This induces complexity in MARL scenarios. In order to overcome these challenges, we propose a new multi-agent learning method based on fuzzy least-square policy iteration. The proposed method consists of two parts: an Inner Model as one other agent policy approximation method and a multi-agent method to learn a near-optimal policy based on the others agents' policies. Both of the proposed algorithms are applicable to problems with continuous state and action spaces. These methods can be used independently or in combination with each other. They are defined to perfectly suit each other so that the outputs of Inner Model are entirely consistent with the nature of inputs of the multi-agent method. In problems with no possibility of explicit communication, combinations of the proposed methods are recommended. In addition, theoretical analysis proves the near optimality of the policies learned by these methods. We evaluate the learning methods in problems with continuous state-action spaces: the well-known predator-prey problem and the unit commitment problem in the smart power grid. The results are satisfactory and show acceptable performance of our methods.																	1432-7643	1433-7479				JAN	2020	24	2					901	918		10.1007/s00500-019-04600-4													
J								Solving fuzzy regression equation and its approximation for random fuzzy variable and their application	SOFT COMPUTING										Fuzzy quantity; Fuzzy numbers; Random fuzzy quantity; Regression equation; Approximation of regression equation	TEMPERATURE PREDICTION; LINEAR-REGRESSION; LOGICAL RELATIONSHIPS; ELLIPSOID NUMBERS; MODEL; RANKING	In this paper, the problem of linear regression is studied for random fuzzy quantity U which has some statistical linear relationship with another random real variable T. First, we turn the problem of fuzzy number linear regression model into the problem of usual real linear regression models, introduce the conception of r-linear regression equation of random fuzzy number, and give two results on what conditions we can determine the fuzzy number coefficients of the random fuzzy quantity linear regression equation by using these real number coefficients of r-linear regression equations. Then, we give specific method and steps to determine the fuzzy number coefficients of the random fuzzy number linear regression equation from a set of statistic of random fuzzy quantity U and random real variable T. And then, for convenience of application, we propose conception of alpha-approximation fuzzy number regression equations of random fuzzy number U with respect to random real variable T through further discussion to the fuzzy coefficients of the random fuzzy number linear regression equation and obtain the specific expressions of membership functions of the fuzzy coefficients of the alpha-approximation fuzzy number regression equations. At last, we give the specific method of solving the alpha-approximation fuzzy number regression equation and use a specific example to show the application and rationality of the proposed method.																	1432-7643	1433-7479				JAN	2020	24	2					919	933		10.1007/s00500-019-04612-0													
J								A page replacement algorithm based on a fuzzy approach to improve cache memory performance	SOFT COMPUTING										Operating system; Memory management; Page replacement algorithms; Cache; Fuzzy c-means	CLASSIFICATION; PATTERN; LRU	The memory management in the operating system includes a part called the page replacement algorithms. Replacement algorithms in environments that require high-performance computing are considered as an important issue. For example, these algorithms are very important in cache management in microprocessors, web caching, replication strategies in distributed information systems and so on. Due to the important role of replacement algorithms in overcoming the problem of performance caused by the difference in processor speeds and memory, many algorithms were proposed. Most of them are the developed schemes of the least frequently used (LFU) and least recently used (LRU). Although most proposed designs can solve the LRU and LFU defects, they are implemented in a difficult way. The most important advantage of LRU and LFU is their simple implementation. This research proposes a page replacement algorithm that is simple to implement. The algorithm, which uses three parameters to cluster cache pages, is called the fuzzy page replacement algorithm. Whenever a miss occurs, it selects a page of the cluster with the lowest priority which has the smallest Euclidean distance with its center and then exits the cache. The most significant advantage of the algorithm is using the FCM (fuzzy c-means) algorithm to cluster pages, resulting in better replacement and hence higher memory performance.																	1432-7643	1433-7479				JAN	2020	24	2					955	963		10.1007/s00500-019-04624-w													
J								KNN search-based trajectory cloaking against the Cell-ID tracking in cellular network	SOFT COMPUTING										Cellular network; Cell-ID positioning; Hausdorff distance; Trajectory privacy; KNN search	LOCATION PRIVACY; K-ANONYMITY	The widely used smartphone with powerful positioning capability makes it easy for a user to find his precise physical location. However, this may reveal a user's geo-location information, making the real-time tracking of the user possible. For example, on the basis of a sequence of numbers (i.e., Cell-IDs) received in the Cell-ID positioning, an entity can gain access to a person's movement routes without his consent. We argue that if the trajectory of a person is traced, then all his visits may be exposed. Therefore, trajectory cloaking against the mobile positioning is urgently necessary. In this paper, we propose a dummy base station replacement (DBSR) algorithm. It mainly uses the idea of dummy trajectory anonymity and is achieved by replacing the true Cell-ID provided by the network with a fake but nearby Cell-ID. We also implement our DBSR algorithm on an Android-based smartphone to evaluate its performance. Experimental results show that the DBSR algorithm can efficiently tackle the privacy breach caused by the single-base-station positioning in cellular network.																	1432-7643	1433-7479				JAN	2020	24	2					965	980		10.1007/s00500-019-03935-2													
J								Multi-objective redundancy hardening with optimal task mapping for independent tasks on multi-cores	SOFT COMPUTING										Fault tolerance; Multi-cores; Task hardening; Task mapping; Multi-objective optimization; Memetic algorithms	TOLERANT EMBEDDED SYSTEMS; EVOLUTIONARY ALGORITHMS; FITNESS APPROXIMATION; MODULAR REDUNDANCY; MEMETIC ALGORITHM; OPTIMIZATION; RELIABILITY; PERFORMANCE; LEVEL	The rate of transient faults has increased significantly as the technology scales up. The tolerance of transient faults has become an important issue in the system design. Dual modular redundancy (DMR) and triple modular redundancy (TMR) are two commonly used techniques that can achieve fault detection and masking through executing redundant tasks. As DMR and TMR have different time and cost overheads, we must carefully determine which one should be used for each task (i.e., task hardening) to achieve the optimal system design. Furthermore, for multi-core systems, the system-level design includes the allocation of cores for the tasks (i.e., task mapping) as well. This paper aims at task hardening and mapping simultaneously for independent tasks on multi-cores with heterogeneous performances, in order to minimize the maximum completion time of all tasks (i.e., makespan). We demonstrate that once task hardening is given, task mapping of independent tasks can be achieved by employing min-max-weight perfect matching with a polynomial time complexity. Besides, as there is a trade-off between cost and time performance, we propose a multi-objective memetic algorithm (MOMA)-based task hardening method to obtain a set of solutions with different numbers of cores (i.e., costs), so the designer can choose different solutions according to different requirements. The key idea of the MOMA is to incorporate problem-specific knowledge into the global search of evolutionary algorithms. Our experimental studies have demonstrated the effectiveness of the proposed method and have shown that by combining the results of MOMA and MOEA we can provide a designer with a highly accurate set of solutions within a reasonable amount of time.																	1432-7643	1433-7479				JAN	2020	24	2					981	995		10.1007/s00500-019-03937-0													
J								An efficient and robust grey wolf optimizer algorithm for large-scale numerical optimization	SOFT COMPUTING										Grey wolf optimizer; Large-scale optimization; Particle swarm optimization; Efficient; Robust	PARTICLE SWARM OPTIMIZATION; ARTIFICIAL BEE COLONY; SINE COSINE ALGORITHM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; MEMETIC ALGORITHM; LEVY FLIGHT; SEARCH; STRATEGY	Meta-heuristic algorithms are widely viewed as feasible techniques to solve continuous large-scale numerical optimization problems. Grey wolf optimizer (GWO) is a relatively new stochastic algorithm with only a few parameters to adjust that can be easily used for global optimization. This paper presents an efficient and robust GWO (ERGWO) variant to solve large-scale numerical optimization problems. Inspired by particle swarm optimization, a nonlinearly adjustment strategy for parameter control is designed to balance exploration and exploitation. Additionally, a modified position-updating equation is presented to improve convergence speed. The performance of ERGWO is verified on 18 benchmark large-scale numerical optimization problems with dimensions ranging from 30 to 10,000, 30 benchmarks from CEC 2014, 30 functions in CEC 2017, respectively. Numerical experiments are performed to compare ERGWO to the basic GWO algorithm, other GWO variants, and other well-known meta-heuristic search techniques. Simulations demonstrate that the proposed ERGWO algorithm can find high quality solutions with low computational cost and very fast convergence.																	1432-7643	1433-7479				JAN	2020	24	2					997	1026		10.1007/s00500-019-03939-y													
J								Deep learning for effective Android malware detection using API call graph embeddings	SOFT COMPUTING										Android malware; Deep learning; Graph embedding; Hyper-parameter tuning; API call graph		High penetration of Android applications along with their malicious variants requires efficient and effective malware detection methods to build mobile platform security. API call sequence derived from API call graph structure can be used to model application behavior accurately. Behaviors are extracted by following the API call graph, its branching, and order of calls. But identification of similarities in graphs and graph matching algorithms for classification is slow, complicated to be adopted to a new domain, and their results may be inaccurate. In this study, the authors use the API call graph as a graph representation of all possible execution paths that a malware can track during its runtime. The embedding of API call graphs transformed into a low dimension numeric vector feature set is introduced to the deep neural network. Then, similarity detection for each binary function is trained and tested effectively. This study is also focused on maximizing the performance of the network by evaluating different embedding algorithms and tuning various network configuration parameters to assure the best combination of the hyper-parameters and to reach at the highest statistical metric value. Experimental results show that the presented malware classification is reached at 98.86% level in accuracy, 98.65% in F-measure, 98.47% in recall and 98.84% in precision, respectively.																	1432-7643	1433-7479				JAN	2020	24	2					1027	1043		10.1007/s00500-019-03940-5													
J								A variable-level automated defect identification model based on machine learning	SOFT COMPUTING										Machine learning; Static analysis; Automated defect identification; Alarm classification; Model evaluation	STATIC ANALYSIS; ANALYZER; ALARMS	Static analysis tools, automatically detecting potential source code defects at an early phase during the software development process, are diffusely applied in safety-critical software fields. However, alarms reported by the tools need to be inspected manually by developers, which is inevitable and costly, whereas a large proportion of them are found to be false positives. Aiming at automatically classifying the reported alarms into true defects and false positives, we propose a defect identification model based on machine learning. We design a set of novel features at variable level, called variable characteristics, for building the classification model, which is more fine-grained than the existing traditional features. We select 13 base classifiers and two ensemble learning methods for model building based on our proposed approach, and the reported alarms classified as unactionable (false positives) are pruned for the purpose of mitigating the effort of manual inspection. In this paper, we firstly evaluate the approach on four open-source C projects, and the classification results show that the proposed model achieves high performance and reliability in practice. Then, we conduct a baseline experiment to evaluate the effectiveness of our proposed model in contrast to traditional features, indicating that features at variable level improve the performance significantly in defect identification. Additionally, we use machine learning techniques to rank the variable characteristics in order to identify the contribution of each feature to our proposed model.																	1432-7643	1433-7479				JAN	2020	24	2					1045	1061		10.1007/s00500-019-03942-3													
J								A novel extension to VIKOR method under intuitionistic fuzzy context for solving personnel selection problem	SOFT COMPUTING										Personnel selection problem; Intuitionistic fuzzy set; Interval numbers; Multi-criteria decision making; VIKOR method	GROUP DECISION-MAKING; AGGREGATION OPERATORS; SUPPLIER SELECTION; PREDICTORS; RANKING	Personnel selection is a challenging problem for any organization. The success of a project is determined by the human resources that handle the project. To make better personnel selections, researchers have adopted multi-criteria decision-making (MCDM) approaches. Among these, fuzzy-based MCDM methods are most frequently used, as they handle vagueness and imprecision better. Intuitionistic fuzzy set (IFS) is a popular MCDM context which provides degree of membership and non-membership for preference elicitation. In this work, we propose a novel decision-making framework that consists of two stages. In the first stage, a new extension to the popular VIKOR method is presented under IFS context. The positive and negative ideal solutions are determined, and VIKOR parameters are calculated using transformation procedure. The proposed method combines the strength of both interval-valued fuzzy set and IFS that is more effective in handling vagueness with a simple formulation setup. In the second stage, a personnel selection problem is used to validate the proposed framework. Finally, the superiority and weakness of the proposed framework are discussed by comparison with other methods.																	1432-7643	1433-7479				JAN	2020	24	2					1063	1081		10.1007/s00500-019-03943-2													
J								Neural network algorithm based on Legendre improved extreme learning machine for solving elliptic partial differential equations	SOFT COMPUTING										Legendre polynomials; Legendre neural network; PDEs; IELM	NUMERICAL-SOLUTION; FEEDFORWARD NETWORKS; TIME-SERIES; OPTIMIZATION; APPROXIMATION; MODEL; CLASSIFICATION	To provide a new numerical algorithm for solving elliptic partial differential equations (PDEs), the Legendre neural network (LNN) and improved extreme learning machine (IELM) algorithm are introduced to propose a Legendre improved extreme learning machine (L-IELM) method, which is applied to solving elliptic PDEs in this paper. The product of two Legendre polynomials is chosen as basis functions of hidden neurons. Single hidden layer LNN is used to construct approximate solutions and its derivatives of differential equations. IELM algorithm is used for network weights training, and the algorithm steps of the proposed L-IELM method are summarized. Finally, in order to evaluate the present algorithm, various test examples are selected and solved by the proposed approach to validate the calculation accuracy. Comparative study with the earlier methods in literature is described to verify the superiority of the presented L-IELM method. Experiment results show that the proposed L-IELM algorithm can perform well in terms of accuracy and execution time, which in addition provides a new algorithm for solving elliptic PDEs.																	1432-7643	1433-7479				JAN	2020	24	2					1083	1096		10.1007/s00500-019-03944-1													
J								Improved grey wolf optimization based on the two-stage search of hybrid CMA-ES	SOFT COMPUTING										Grey wolf optimization; CMA-ES; Function optimization; Hybrid algorithm; Two-stage search	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; ALGORITHM; DISPATCH; STRATEGY	Hybrid algorithms with different features are an important trend in algorithm improvement. In this paper, an improved grey wolf optimization based on the two-stage search of hybrid covariance matrix adaptation-evolution strategy (CMA-ES) is proposed to overcome the shortcomings of the original grey wolf optimization that easily falls into the local minima when solving complex optimization problems. First, the improved algorithm divides the whole search process into two stages. In the first stage, the improved algorithm makes full use of the global search ability of grey wolf optimization on a large scale and thoroughly explores the location of the optimal solution. In the second stage, due to CMA-ES having a strong local search capability, the three CMA-ES instances use the alpha wolf, beta wolf and delta wolf as the starting points. In addition, these instances have different step size for parallel local exploitations. Second, in order to make full use of the global search ability of the grey wolf algorithm, the Beta distribution is used to generate as much of an initial population as possible in the non-edge region of the solution space. Third, the new algorithm improves the hunting formula of the grey wolf algorithm, which increases the diversity of the population through the interference of other individuals and reduces the use of the head wolf's guidance to the population. Finally, the new algorithm is quantitatively evaluated by fifteen standard benchmark functions, five test functions of CEC 2014 suite and two engineering design cases. The results show that the improved algorithm significantly improves the convergence, robustness and efficiency for solving complex optimization problems compared with other six well-known optimization algorithms.																	1432-7643	1433-7479				JAN	2020	24	2					1097	1115		10.1007/s00500-019-03948-x													
J								A novel meta-heuristic optimization method based on golden ratio in nature	SOFT COMPUTING										Meta-heuristic; Golden ratio optimization method; Optimization algorithm; Constrained optimization; Optimization	SINE-COSINE ALGORITHM; COLLIDING BODIES OPTIMIZATION; LEARNING-BASED OPTIMIZATION; SYMBIOTIC ORGANISMS SEARCH; BEE COLONY ALGORITHM; GREY WOLF OPTIMIZER; ENGINEERING OPTIMIZATION; STRUCTURAL OPTIMIZATION; OPTIMAL-DESIGN; EVOLUTION	A novel parameter-free meta-heuristic optimization algorithm known as the golden ratio optimization method (GROM) is proposed. The proposed algorithm is inspired by the golden ratio of plant and animal growth which is formulated by the well-known mathematician Fibonacci. He introduced a series of numbers in which a number (except the first two numbers) is equal to the sum of the two previous numbers. In this series, the ratio of two consecutive numbers is almost the same for all the numbers and is known as golden ratio. This ratio can be extensively found in nature such as snail lacquer part and foliage growth of trees. The proposed approach employed this golden ratio to update the solutions in an optimization algorithm. In the proposed method, the solutions are updated in two different phases to achieve the global best answer. There is no need for any parameter tuning, and the implementation of the proposed method is very simple. In order to evaluate the proposed method, 29 well-known benchmark test functions and also 5 classical engineering optimization problems including 4 mechanical engineering problems and 1 electrical engineering problem are employed. Using several test functions, the performance of the proposed method in solving different problems including discrete, continuous, high dimension, and high constraints problems is testified. The results of the proposed method are compared with those of 11 well-regarded state-of-the-art optimization algorithms. The comparisons are made from different aspects such as the final obtained answer, the speed and behavior of convergence, and CPU time consumption. Superiority of the purposed method from different points of views can be concluded by means of comparisons.																	1432-7643	1433-7479				JAN	2020	24	2					1117	1151		10.1007/s00500-019-03949-w													
J								A genetic algorithm with local search for solving single-source single-sink nonlinear non-convex minimum cost flow problems	SOFT COMPUTING										Minimum cost flow problem; Non-convex cost function; Genetic algorithm; Local search	DYNAMIC-PROGRAMMING APPROACH; FIXED-CHARGE; TRANSPORTATION PROBLEM; APPROXIMATIONS; OPTIMIZATION	Network models are widely used for solving difficult real-world problems. The minimum cost flow problem (MCFP) is one of the fundamental network optimisation problems with many practical applications. The difficulty of MCFP depends heavily on the shape of its cost function. A common approach to tackle MCFPs is to relax the non-convex, mixed-integer, nonlinear programme (MINLP) by introducing linearity or convexity to its cost function as an approximation to the original problem. However, this sort of simplification is often unable to sufficiently capture the characteristics of the original problem. How to handle MCFPs with non-convex and nonlinear cost functions is one of the most challenging issues. Considering that mathematical approaches (or solvers) are often sensitive to the shape of the cost function of non-convex MINLPs, this paper proposes a hybrid genetic algorithm with local search (namely GALS) for solving single-source single-sink nonlinear non-convex MCFPs. Our experimental results demonstrate that GALS offers highly competitive performances as compared to those of the mathematical solvers and a standard genetic algorithm.																	1432-7643	1433-7479				JAN	2020	24	2					1153	1169		10.1007/s00500-019-03951-2													
J								ALO-optimized artificial neural network-controlled dynamic voltage restorer for compensation of voltage issues in distribution system	SOFT COMPUTING										Power distribution system; Dynamic voltage restorer (DVR); ALO-ANN; Voltage sag/swell; Harmonics; Control scheme and PSO		The major concern in power distribution system is the power quality concerns specifically voltage sag/swell and harmonics. To compensate for such voltage disturbances, an effective device called dynamic voltage restorer (DVR) is utilized in the electric power distribution system. Accordingly, in this paper, the DVR is designed to shield the sensitive load from source-side voltage disturbances under nonlinear load conditions. Further, in the proposed work, an ant lion optimizer-optimized artificial neural network (ALO-ANN) is used to control DVR, thus compensating the balanced and the unbalanced voltage sag/swell and distortions in the load-side voltage. Simulation results using MATLAB/Simulink software are analysed to confirm the efficiency of the suggested ALO-ANN regulator scheme of DVR system at critical loads by comparing with the existing PSO (particle swarm optimization)-centred control scheme.																	1432-7643	1433-7479				JAN	2020	24	2					1171	1184		10.1007/s00500-019-03952-1													
J								A core firework updating information guided dynamic fireworks algorithm for global optimization	SOFT COMPUTING										Fireworks algorithm; Updating information; Core firework; Swarm intelligence algorithm; Evolutionary computing	SYSTEMS	As a new variant of swarm intelligence algorithm, fireworks algorithm (FWA) exhibits promising performance on a wide set of optimization problems, for which the fireworks algorithm has been concentrated on and investigated by researchers recently. This paper aims to improve the performance of the FWA by exploiting updating information of the core firework to guide the algorithm's searching process. Based on this mentality, this paper ameliorated the explosion strategy of core firework of dynamic fireworks algorithm (dynFWA). The proposed algorithm, named dynPgFWA in this paper, improved FWA from two aspects: amplifying the explosion amplitude on the direction on which core firework is updated, and making more sparks which are generated by core firework distributed on this direction to enhance the algorithm's searching ability on updating direction. A numerical experiment on CEC2015 and CEC2017 test suite was implemented to verify the performance of the proposed algorithm. Results of the experiment indicated that dynPgFWA outperformed the compared evolutionary algorithms in the quality of solutions.																	1432-7643	1433-7479				JAN	2020	24	2					1185	1211		10.1007/s00500-019-03953-0													
J								On shadowed hypergraphs	SOFT COMPUTING										Fuzzy sets; Shadow sets; Fuzzy hypergraphs	AGGREGATION OPERATORS; 3-WAY APPROXIMATIONS; FUZZY HYPERGRAPHS; SETS	Fuzzy hypergraphs are useful tools for representing granular structures when describing the relations between objects and set of hyperedges, at minute detail, in a specific granularity. Their merit over classical hypergraphs lies in their ability to model uncertainty that may arise with objects and/or granules incident to each other. Many previous studies on fuzzy hypergraphs seek to exploit their strong descriptive potential to analyze n-ary relations in several contexts. However, due to the very detailed numeric membership grades of their objects, fuzzy hypergraphs are sensitive to noise, which may be overwhelming in their general interpretation. To address this issue, a principle of least commitment to certain membership grades is sort by embracing a framework of shadowed sets. The specific concern of this paper is to study a noise-tolerable framework, viz. shadowed hypergraph. Our goal is to capture and quantify noisy objects in clearly marked out zones. We discuss some characteristic properties of shadowed hypergraph and describe an algorithm for transforming a given fuzzy hypergraph into its resulting shadowed hypergraph. Finally, some illustrative examples are provided to demonstrate the essence of shadowed hypergraphs.																	1432-7643	1433-7479				JAN	2020	24	2					1213	1224		10.1007/s00500-019-03955-y													
J								SVM Hyper-parameters optimization using quantized multi-PSO in dynamic environment	SOFT COMPUTING										Support vector machine; Dynamic environment; Model selection problem; Multi-swarm optimization; Exclusion; Anti-convergence	MODEL SELECTION; MULTIOBJECTIVE OPTIMIZATION; VECTOR	Support vector machine (SVM) is considered as one of the most powerful classifiers. They are parameterized models build upon the support vectors extracted during the training phase. One of the crucial tasks in the modeling of SVM is to select optimal values for its hyper-parameters, because the effectiveness and efficiency of SVM depend upon these parameters. This task of selecting optimal values for the SVM hyper-parameters is often called as the SVM model selection problem. Till now a lot of methods have been proposed to deal with this SVM model selection problem, but most of these methods consider the model selection problem in static environment only, where the knowledge about a problem does not change over time. In this paper we have proposed a framework to deal with SVM model selection problem in dynamic environment. In dynamic environment, knowledge about a problem changes over time due to which static optimum values for yper-parameters may degrade the performance of the classifier. For this there should be one efficient mechanism which can re-evaluate the optimal values of hyper-parameters when the knowledge about a problem changes. Our proposed framework uses multi-swarm-based optimization with exclusion and anti-convergence theory to select the optimal values for the SVM hyper-parameters in dynamic environment. The experiments performed using the proposed framework have shown better results in comparison with other techniques like traditional gird search, first grid search, PSO, chained PSO and dynamic model selection in terms of effectiveness and efficiency.																	1432-7643	1433-7479				JAN	2020	24	2					1225	1241		10.1007/s00500-019-03957-w													
J								An intelligent credit card fraud detection approach based on semantic fusion of two classifiers	SOFT COMPUTING										Online fraud detection; Optimization algorithm; Multi-level classification; E-banking		The increased usage of credit cards for online and regular purchases in E-banking communication systems is vulnerable to credit card fraud. Data imbalance also poses a huge challenge in the fraud detection process. The efficiency of the current fraud detection system (FDS) is in question only because they detect the fraudulent activity after the suspicious transaction is done. This paper proposes an intelligent two-level credit card fraud detection model from highly imbalanced datasets, relying on the semantic fusion of k-means and artificial bee colony algorithm (ABC) to enhance the classification accuracy and speed up detection convergence. ABC as a second classification level performs a kind of neighborhood search combined with the global search to handle the inability the k-means classifier to discover the real cluster if the same data is inputted in a different order it may produce different cluster. Besides, the k-means classifier may be surrounded by the local optimum as it is sensitive to the initial condition. The advised system filters the dataset' features using a built-in rule engine to analyze whether the transaction is genuine or fraudulent based on many customer behavior (profile) parameters such geographical locations, usage frequency, and book balance. Experimental results indicate that the proposed model can enhance the classification accuracy against the risk coming from suspicious transactions, and gives higher accuracy compared to traditional methods.																	1432-7643	1433-7479				JAN	2020	24	2					1243	1253		10.1007/s00500-019-03958-9													
J								Monte Carlo method for the real and complex fuzzy system of linear algebraic equations	SOFT COMPUTING										System of linear algebraic equations; Complex fuzzy number; Transition probability matrix; Hadamard product; Markov chain Monte Carlo; Embedding method	HADAMARD PRODUCT; FAN PRODUCT; EIGENVALUES; BOUNDS	In this paper, we apply the Monte Carlo method to solve the real and complex fuzzy system of linear algebraic equations via new techniques. At first, we determine the specified and simpler computing condition for convergence of the Monte Carlo method using Hadamard product related to select the transition probability matrix. Then, we employ the new strategy based on the exclusive characteristic of the Monte Carlo method to find the solution of the real and complex fuzzy system of linear algebraic equations. Finally, some numerical examples are proposed to demonstrate the validity and efficiency of the discussed theoretical concepts.																	1432-7643	1433-7479				JAN	2020	24	2					1255	1270		10.1007/s00500-019-03960-1													
J								Improving the one-position inheritance artificial bee colony algorithm using heuristic search mechanisms	SOFT COMPUTING										Meta-heuristic; Swarm intelligence; Artificial bee colony algorithm; Continuous optimization	OPTIMIZATION	The artificial bee colony algorithm with one-position inheritance (OPIABC) has shown good performance for large-scale problems. But, the improvement in its performance for some other type test problem is not obvious, since the onlookers in this algorithm use the foraging strategy that randomly selects a neighbor to produce a new candidate. Moreover, the scout foraging behavior in this algorithm is completely random, which would sometimes make it consume more search efforts to discover some promising area and hamper its convergent speed especially for large-scale optimization. To further improve its performance, a running information-guided onlooker foraging strategy and a heuristic scout search mechanism are designed and combined with it. The improved OPIABC algorithm has been tested on a set of test functions with dimensions D = 30, 100 and 1000. Experimental results show that after using the heuristic search mechanisms, the performance of the OPIABC algorithm is significantly improved for most test problems.																	1432-7643	1433-7479				JAN	2020	24	2					1271	1281		10.1007/s00500-019-03964-x													
J								A high accurate vehicle speed estimation method	SOFT COMPUTING										Vehicle speed estimation; Camera calibration; Projection histogram; Speed enumeration		In this paper, we present a novel approach for accurate vehicle speed estimation from video sequences. Common methods usually track sets of distinguishing features; however, feature extraction is a difficult task in dynamic environments. Herein, we propose a novel analysis method without feature extraction. Initially, a frame difference method is applied to a region of interest, from which projection histograms are obtained and a group of key bins are selected to represent the vehicle motion. Then, all the possible speeds are tested one by one, and the extreme value of the testing function is selected for the corresponding speed. The proposed system was tested on three data sets containing 2054 vehicles, where the ground truth of speed is obtained by a radar speed detector. The experiment results show that the proposed system has an average error of 0.3 km/h, with 99.4% of the estimation speed within the error of range (- 2 km/h, 2 km/h). The system turns out to be robust, accurate and real time for practical use.																	1432-7643	1433-7479				JAN	2020	24	2					1283	1291		10.1007/s00500-019-03965-w													
J								EPL models with fuzzy imperfect production system including carbon emission: a fuzzy differential equation approach	SOFT COMPUTING										Fuzzy imperfect production; Carbon emission; Fuzzy differential equation; Intuitionistic fuzzy optimization technique	DETERIORATING PRODUCTION PROCESSES; CAP-AND-TRADE; INVENTORY MODEL; SUPPLY CHAIN; LOT-SIZE; DEMAND; POLICY; QUANTITY; QUALITY; LENGTH	The paper outlines the production policies for maximum profit of a firm producing imperfect economic lot size with time-dependent fuzzy defective rate under the respective country's carbon emission rules. Generally in economic production lot-size models, defective production starts after the passage of some time from production commencement. So the starting time of producing defective units is normally uncertain and imprecise. Thus, produced defective units are fuzzy, partially reworked instantly and sold as fresh units. As a result, the inventory level at any time becomes fuzzy and the relation between the production, demand and inventory level becomes a fuzzy differential equation (FDE). Nowadays, different governments have made environmental regulations following the United Nations Framework Convention on Climate Change to reduce carbon emission. Some governments use cape and trade policy on emission. Due to this, firms are in fix how to optimize the production. If the firms produce more, the profit increases along with more emission and corresponding tax. Here, models are formulated as profit maximization problems using FDE, and the corresponding inventory and environmental costs are calculated using fuzzy Riemann integration. An a-cut of average profits is obtained and the reduced multi-objective crisp problems are solved using intuitionistic fuzzy optimization technique. The models are illustrated numerically and results are presented graphically. Considering different carbon regulations, an algorithm for a firm management is presented to achieve the maximum profit. Real-life production problems for the firms in Annex I and developing countries are solved.																	1432-7643	1433-7479				JAN	2020	24	2					1293	1313		10.1007/s00500-019-03967-8													
J								Neural collision avoidance system for biomimetic autonomous underwater vehicle	SOFT COMPUTING										Neural networks; Underwater vehicle; Biomimetic robot; Collision avoidance; Autonomy	BEHAVIORAL FUSION; STRATEGIES; NAVIGATION; ALGORITHM; NETWORKS	Autonomous underwater vehicles (AUVs) are underwater robots which are able to perform certain tasks without the help of a human operator. The key skill of each AUV is the capability to avoid collisions. To this end, appropriate devices and software are necessary with the potential to detect obstacles and to take proper decisions from the point of view of both the task and safety of the vehicle. The paper presents a neural collision avoidance system (NCAS) designed for the biomimetic autonomous underwater vehicle (BAUV). The NCAS is a component of the path following and collision avoidance system (PFCAS), which as the name implies is responsible for safely leading the vehicle along a desired path with collision avoidance. The task of NCAS is to make decisions regarding vehicle maneuvers in the horizontal plane, but only in the close proximity of the obstacles. It is implemented as an evolutionary artificial neural network designed by means of a neuro-evolutionary technique called assembler encoding with evolvable operations (AEEO). The paper outlines operation and construction of the BAUV as well as the PFCAS, the role of the NCAS in the entire system, and briefly presents AEEO as well as reporting on the experiments performed in simulation.																	1432-7643	1433-7479				JAN	2020	24	2					1315	1333		10.1007/s00500-019-03969-6													
J								Scheduling parallel machine problem under general effects of deterioration and learning with past-sequence-dependent setup time: heuristic and meta-heuristic approaches	SOFT COMPUTING										Scheduling; Deteriorating jobs; Learning effect; Setup times; Genetic algorithm; Ant colony optimization	GENETIC ALGORITHM; PROCESSING-TIMES; FLOW-SHOP; COMPLETION-TIME; JOBS; MINIMIZE; MAINTENANCE; OPTIMIZATION; COLONY; MODEL	This study investigates an identical parallel machine scheduling problem with past-sequence-dependent setup times and general effects of deteriorating and learning. The actual job processing time on each machine is defined by a two-element function of the normal processing times of the preprocessed jobs and its scheduled position on the same machine. Moreover, the job setup time on each machine is a function of the actual processing times of the preprocessed jobs on the same machine. A novel mixed-integer programming model is developed to satisfy the goal of minimizing total completion time. Due to the NP-hard characteristic and intractability of the problem, three efficient methodologies including a heuristic algorithm (HA), a genetic algorithm (GA) with an enhanced exploration ability and an ant colony optimization (ACO) combined with a new stochastic elitism strategy are designed to find optimal/near-optimal solutions within an appropriate period of time. The effectiveness and efficiency of the presented model and the proposed algorithms are verified by computational experiments. The computational results indicate that the suggested algorithms are effective and executable approaches to generate solutions as good as optimal solution in the small-sized problems. Also, the ACO statistically outperformed the HA and GA in the medium- and large-sized problems.																	1432-7643	1433-7479				JAN	2020	24	2					1335	1355		10.1007/s00500-019-03970-z													
J								Hierarchical granular hotspots detection	SOFT COMPUTING										Hotspot; EFCM; wEFCM; Information granule		We present a hierarchical model based on the extended fuzzy C-means (EFCM) clustering algorithm to develop a granular view of hotspots on a geographic map. The objective is to establish an overview of the spatial distribution of a phenomenon when the relevant data are partitioned into different datasets. The EFCM algorithm is applied to each dataset to detect local hotspots, represented as circles, on the map. The local hotspots constitute information granules at lower level of abstraction in the model. A weighted EFCM algorithm is then applied to a dataset formed by the centers of all the local hotspots to extract circular prototypes, defined as global hotspots, which constitute information granules at the higher level, and hence, they deliver a global overview of the spatial distribution of the phenomenon on the map. Two indices related to the essential criteria of the principle of justifiable granularity are used. The results demonstrate that the most justifiable overview is obtained by using the radius of the local hotspot as weight. Comparisons with a hierarchical model based on FCM algorithm show that our algorithm gives a better granular view of the phenomenon with respect to the latter.																	1432-7643	1433-7479				JAN	2020	24	2					1357	1376		10.1007/s00500-019-03971-y													
J								Novel model to integrate word embeddings and syntactic trees for automatic caption generation from images	SOFT COMPUTING										Caption generation from images; Word embeddings; Syntactic trees; Kernel descriptors; Tag refinement; Natural language generation; N-gram phrases; BLEU	FEATURES; SCALE	Automatic caption generation from images is an interesting and mainstream direction in the field of machine learning. This method enables us to build a powerful computer model that can interpret the implicit semantic information of images. However, the current state of research faces significant challenges such as those related to extracting robust image features, suppressing noisy words, and improving a caption's coherence. For the first problem, a novel computer vision system is presented to create a new image feature called MK-KDES-1 (MK-KDES represents Multiple Kernel-Kernel Descriptors) after extracting three KDES features and fusing them by MKL (Multiple Kernel Learning) model. The MK-KDES-1 feature captures both textural characteristics and shape characteristics of images, which contribute to improving the BLEU_1 (BLEU represents Bilingual Evaluation Understudy) scores of captions. For the second problem, an effective newly designed two-layer TR (Tag Refinement) strategy is integrated into our NLG (Natural Language Generation) algorithm. Words that are most relevant semantically to images are summarized to generate N-gram phrases. Noisy words are suppressed using the innovative TR strategy. For the last problem, on the one hand, a pop WE (Word Embeddings) model and a novel metric called PDI (Positive Distance Information) are introduced together to generate N-gram phrases. The phrases are evaluated by the AWSC (Accumulated Word Semantic Correlation) metric. On the other hand, the phrases are fused to generate captions by the ST (Syntactic Trees). Experimental results demonstrate that informative captions with high BLEU_3 scores can be obtained to describe images.																	1432-7643	1433-7479				JAN	2020	24	2					1377	1397		10.1007/s00500-019-03973-w													
J								Equilibrium strategy for human resource management with limited effort: in-house versus outsourcing	SOFT COMPUTING										Human resource outsourcing; Limited effort; Double moral hazard; Contracts	MORAL HAZARD; INFORMATION	When outsourcing human resource activities to a vendor, a firm with limited effort may want to save energy to focus on core competencies. However, the firm could also lose control over these activities by outsourcing, especially in a situation with unobservable action, and this could in turn increase cost. In this paper, we consider a firm with limited effort performing core and non-core human resource activities and aiming to choose an appropriate human resource management strategy, in-house or outsourcing within a framework of agency theory. We show that only when the effort ceiling is low enough, will the firm completely commit to its core activities. Surprisingly, under outsourcing strategy with double moral hazard, we find that both of the vendor and the firm distort their efforts. Moreover, the outsourcing strategy is optimal if the effort ceiling is low enough or if the cost of the non-core activities is high and the positive interaction between the two activities is low. Otherwise, the firm prefers the in-house strategy, especially in the case with a high effort ceiling and unobservable action. In addition, we also find that a high revenue generation capacity for the core activities may increase the motivation of the firm for outsourcing.																	1432-7643	1433-7479				JAN	2020	24	2					1399	1422		10.1007/s00500-019-03974-9													
J								Ensemble classification from deep predictions with test data augmentation	SOFT COMPUTING										Convolutional neural networks; Data augmentation; Ensemble classification	NEURAL-NETWORKS; PERTURBATION; RECOGNITION	Data augmentation has become a standard step to improve the predictive power and robustness of convolutional neural networks by means of the synthetic generation of new samples depicting different deformations. This step has been traditionally considered to improve the network at the training stage. In this work, however, we study the use of data augmentation at classification time. That is, the test sample is augmented, following the same procedure considered for training, and the decision is taken with an ensemble prediction over all these samples. We present comprehensive experimentation with several datasets and ensemble decisions, considering a rather generic data augmentation procedure. Our results show that performing this step is able to boost the original classification, even when the room for improvement is limited.																	1432-7643	1433-7479				JAN	2020	24	2					1423	1433		10.1007/s00500-019-03976-7													
J								Multi-person and multi-criteria decision making with the induced probabilistic ordered weighted average distance	SOFT COMPUTING										Fuzzy logic; Multi-criteria decision making; OWA operator; Fuzzy distances	FUZZY MCDM APPROACH; AGGREGATION OPERATORS; MOVING AVERAGES; OWA OPERATORS; RISK	This paper presents a new approach for selecting suppliers of products or services, specifically with respect to complex decisions that require evaluating different business characteristics to ensure their suitability and to meet the conditions defined in the recruitment process. To address this type of problem, this study presents the multi-person multi-criteria induced ordered weighted average distance (MP-MC-IOWAD) operator, which is an extension of the OWA operators that includes the notion of distances to multiple criteria and expert valuations. Thus, this work introduces new distance measures that can aggregate the information with probabilistic information and consider the attitudinal character of the decision maker. Further extensions are developed using probabilities to form the induced probabilistic ordered weighted average distance (IPOWAD) operator. An example in the management of insurance policies is presented, where the selection of insurance companies is very complex and requires the consideration of subjective criteria by experts in decision making.																	1432-7643	1433-7479				JAN	2020	24	2					1435	1446		10.1007/s00500-019-03977-6													
J								Structural representation-based off-line Tamil handwritten character recognition	SOFT COMPUTING										PM-Quad tree; Strip tree; Z-ordering; SVM; Chain code	BANGLA CHARACTER	Tamil handwritten character recognition system enormously depends on its character features. This paper deals with the feature extraction and the three ways of feature predictions that are experimented in order to grasp features from various Tamil characters possessing variations in style and shape. Shape, shape ordering and location-based instances are the features predicted from the characters. The key features of this paper are the strip tree-based hierarchical formation which deals with the shape features of the characters, the implementation of the Z-ordering algorithm for addressing the structure ordering and finally the representation of PM-Quad tree that deals with extracting locations of the character features. A hierarchical classification algorithm based on support vector machine is used for predicting the character from its character features using divide-and-conquer procedure. Proof of this work shows that this work can address more characters and its varied shapes.																	1432-7643	1433-7479				JAN	2020	24	2					1447	1472		10.1007/s00500-019-03978-5													
J								Performance comparison using firefly and PSO algorithms on congestion management of deregulated power market involving renewable energy sources	SOFT COMPUTING										Congestion management; Deregulated power market; Locational marginal price; Rescheduling of generators; Renewable energy sources; Firefly algorithm; Particle swarm optimization	DISTRIBUTION-SYSTEM; VOLTAGE STABILITY; OPTIMAL PLACEMENT; GENERATORS; LOCATION; SOLAR; WIND	The power industry across the globe is subjected to a sweeping change in its business as well as in an operating model where the monopoly utilities are being liberalized and opened up for competition with private players. As an outcome of this, the transmission corridors evacuating the power of inexpensive generators would be burdened if all such transactions are admitted. One of the most proficient techniques for congestion management is rescheduling the generators. This research paper suggests a framework to regulate the power flows of the transmission lines within the stipulated limit in a deregulated electricity market environment through rescheduling with and without renewable energy sources (RES). The problem of rescheduling is framed with the intention of lessening the congestion cost. Unlike the traditional method, the best location for the placement of RES is established utilizing a novel weighted locational marginal price (LMP)-based method. The firefly algorithm (FA) and particle swarm optimization (PSO) algorithm are employed in order to get optimized results. The realistic cases are considered, and the results obtained with and without RES using FA and PSO are compared to prove the research study. The efficacy of the method is explored with IEEE 30-bus system.																	1432-7643	1433-7479				JAN	2020	24	2					1473	1482		10.1007/s00500-019-03979-4													
J								Modeling reverse thinking for machine learning	SOFT COMPUTING										Machine learning; Inertial thinking model; Modeling reverse thinking	BIG DATA; INTEGRATION; DIVERGENT	Human inertial thinking schemes can be formed through learning, which are then applied to quickly solve similar problems later. However, when problems are significantly different, inertial thinking generally presents the solutions that are definitely imperfect. In such cases, people will apply creative thinking, such as reverse thinking, to solve problems. Similarly, machine learning methods also form inertial thinking schemes through learning the knowledge from a large amount of data. However, when the testing samples are vastly different, the formed inertial thinking schemes will inevitably generate errors. This kind of inertial thinking is called illusion inertial thinking. Because all machine learning methods do not consider the illusion inertial thinking, in this paper we propose a new method that uses the reverse thinking to correct the illusion inertial thinking, which increases the generalization ability of machine learning methods. Experimental results on benchmark data sets validated the proposed method.																	1432-7643	1433-7479				JAN	2020	24	2					1483	1496		10.1007/s00500-019-03980-x													
J								Role of honesty and confined interpersonal influence in modelling predilections	SOFT COMPUTING										Honesty; Group decision-making; Social network analysis; Confined influence; Predilection	DECISION; QUANTIFIERS; OPERATORS	Classical models of decision-making do not incorporate for the role of influence and honesty that affects the process. This paper develops on the theory of influence in social network analysis. We study the role of influence and honesty of individual experts on collective outcomes. It is assumed that experts have the tendency to improve their initial predilection for an alternative, over the rest, if they interact with one another. It is suggested that this revised predilection may not be proposed with complete honesty by the expert. Degree of honesty is computed from the preference relation provided by the experts. This measure is dependent on average fuzziness in the relation and its disparity from an additive reciprocal relation. Moreover, an algorithm is introduced to cater for incompleteness in the adjacency matrix of interpersonal influences. This is done by analysing the information on how the expert has influenced others and how others have influenced the expert.																	1432-7643	1433-7479				JAN	2020	24	2					1497	1509		10.1007/s00500-019-03981-w													
J								Linguistic summarization of fuzzy social and economic networks: an application on the international trade network	SOFT COMPUTING										Fuzzy sets; Social and economic networks; Linguistic summarization; Semi-fuzzy quantifiers; International trade network	TIME-SERIES; COMMUNITY STRUCTURE; QUANTIFIERS; GENERATION; MODELS; REPRESENTATION; PREDICTION; DISCOVERY	Although plenty of techniques such as link prediction, clustering, and position analysis have been proposed to analyze social and economic networks and patterns of social and economic relationships in various fields, few studies have addressed the transformation of social and economic network data into the knowledge in the form of linguistic summaries. In this study, we propose, for the first time in the literature, iteration, reciprocal and branching-based linguistic summary forms taking into account both the attributes (features) of social and economic actors and the relations between them. We then develop methods for evaluating the degree of truth of the suggested linguistic summary forms by leveraging generalized quantifiers, specifically semi-fuzzy and polyadic quantifiers. The advantages and applicability of the proposed linguistic summary forms are illustrated on the international trade network.																	1432-7643	1433-7479				JAN	2020	24	2					1511	1527		10.1007/s00500-019-03982-9													
J								A new cipher system using semi-natural composition in Indian raga	SOFT COMPUTING										Cryptography; Indian raga; Semi-natural composition		Cryptographic algorithms are the basic of care-free transactions over the Internet today. Confidential information of a government or private agency or department is secured through the use of Cryptography. From doing secure communication to transferring information of national importance, Cryptographic algorithms play the sole role in hiding the confidentiality. Musical attributes such as notes of which the music is composed are not constant and vary from composition to composition. Same tune played by different composers shows a variation in the sequence of notes used along with other attributes of a musical composition such as duration of each note and the frequency at which each note is played. Such a variation can be employed for the purpose of encrypting the message. In this paper, we have incorporated the use of Hindustani (North Indian) musical notes to encrypt messages. We have used a semi-natural composition process to generate note sequences of Indian music which can then be used as a tool for message hiding. This at first place ensures that the message is hidden from the intruder and second it gives a new random sequence of notes every time same message is sent. So the very purpose of a Cryptographic algorithm is served. The encrypted message in the form of musical notes is then sent to the intended recipient in the form of a musical composition which helps in defying the intruder of sensing any confidential information that is being sent over the communication channel.																	1432-7643	1433-7479				JAN	2020	24	2					1529	1537		10.1007/s00500-019-03983-8													
J								Application of a new accelerated algorithm to regression problems	SOFT COMPUTING										Nonexpansive mapping; S-iteration method; Regression; Composite minimization problems	MAXIMAL MONOTONE-OPERATORS; PROXIMAL POINT ALGORITHM; CONVERGENCE THEOREMS; NONEXPANSIVE-MAPPINGS; FIXED-POINTS; MINIMIZATION; SHRINKAGE	Many iterative algorithms like Picard, Mann, Ishikawa are very useful to solve fixed point problems of nonlinear operators in real Hilbert spaces. The recent trend is to enhance their convergence rate abruptly by using inertial terms. The purpose of this paper is to investigate a new inertial iterative algorithm for finding the fixed points of nonexpansive operators in the framework of Hilbert spaces. We study the weak convergence of the proposed algorithm under mild assumptions. We apply our algorithm to design a new accelerated proximal gradient method. This new proximal gradient technique is applied to regression problems. Numerical experiments have been conducted for regression problems with several publicly available high-dimensional datasets and compare the proposed algorithm with already existing algorithms on the basis of their performance for accuracy and objective function values. Results show that the performance of our proposed algorithm overreaches the other algorithms, while keeping the iteration parameters unchanged.																	1432-7643	1433-7479				JAN	2020	24	2					1539	1552		10.1007/s00500-019-03984-7													
J								Senti-NSetPSO: large-sized document-level sentiment analysis using Neutrosophic Set and particle swarm optimization	SOFT COMPUTING										Evolutionary algorithm; Neutrosophic Set; Opinion mining; Particle swarm optimization; Sentiment analysis	FUZZY-LOGIC; CLASSIFICATION; MACHINE; MODEL	In the last decade, opinion mining has been explored by using various machine learning methods. In the literature, document-level sentiment analysis has been majorly dealt with short-sized text only. For large-sized text, document-level sentiment analysis has never been dealt. In this paper, a hybrid framework named as "Senti-NSetPSO'' is proposed to analyse large-sized text. Senti-NSetPSO comprises of two classifiers: binary and ternary based on hybridization of particle swarm optimization (PSO) with Neutrosophic Set. This method is suitable to classify large-sized text having more than 25 kb of size. Swarm size generated from large text can give a suitable measurement for implementation of PSO convergence. The proposed approach is trained and tested for large-sized text collected from Blitzer, aclIMDb, Polarity and Subjective Dataset. The proposed method establishes a co-relation between sentiment analysis and Neutrosophic Set. On Blitzer, aclIMDb and Polarity dataset, the model acquires satisfactory accuracy by ternary classifier. The accuracy of ternary classifier of the proposed framework shows significant improvement than review paper classifier present in the literature.																	1432-7643	1433-7479				JAN	2020	24	1			SI		3	15		10.1007/s00500-019-04209-7													
J								Dynamic adaptation of the PID's gains via Interval type-1 non-singleton type-2 fuzzy logic systems whose parameters are adapted using the backpropagation learning algorithm	SOFT COMPUTING										IT2 fuzzy logic systems; PID control algorithm; Singleton numbers; Type-1 non-singleton numbers; PID IT2 fuzzy self-tuning; IT2 NSFLS-1	DERIVATIVE FILTER; CONTROLLER; DESIGN; ROBOT; AGC	This work presents a new design to dynamically adapt the proportional, the integral and the derivative (PID) controllers gains using three interval type-1 non-singleton type-2 fuzzy logic systems (IT2 NSFLS-1), one fuzzy system for each gain of the PID, being the first main contribution of this proposal. This assembly is named as hybrid IT2 NSFLS-1 PID. Each IT2 NSFLS-1 system requires two non-singleton input values each period of discrete time (k), (1) the error e(k) and its standard deviation sigma e(k), and (2) the change of error Delta e(k) and its standard deviation sigma Delta e(k), to calculate the corresponding adjustment Delta KP(k), Delta KI(k), and Delta KD(k) for the PID controllers gains K-p(k), K-i(k), and K-d(k). The second main contribution of this proposal is that the parameters of each IT2 NSFLS-1 system are tuned each period of discrete time (k) by the non-singleton backpropagation (BP) algorithm using the plant output error and its standard deviation, which are processed as non-singleton values together with its non-singleton partial derivatives with respect to each IT2 fuzzy system parameter. Then these updated gains are used by the PID controller to calculate the best control signal for the plant under control. The uncertainty and the mean value of the measurement are used to calculate the non-singleton error which is processed as (a) input and (b) as gradient vector by each of the three IT2 NSFLS-1 systems. Simulation results show that the proposed hybrid assembly presents the better performance than the next five benchmarking control systems (a) the classic Zeigler-Nichols PID controller, and (b) four hybrid assemblies using PID controller and fuzzy systems with fixed fuzzy rule bases (T1 SFLS, T1 NSFLS, IT2 SFLS, IT2 NSFLS-1). The proposed assembly produces the better performance in a shortest period of time and it maintains a stable behavior on the output of the second-order plant model subject to variations and noise.																	1432-7643	1433-7479				JAN	2020	24	1			SI		17	40		10.1007/s00500-019-04360-1													
J								Qualitative model optimization of almond (Terminalia catappa) oil using soxhlet extraction in type-2 fuzzy environment	SOFT COMPUTING										Almond seeds; Soxhlet extraction; Interval type-2 fuzzy logic; Fuzzy membership function; Fuzzy optimization	TRANSPORTATION PROBLEM; SENSITIVITY-ANALYSIS; LOGIC SYSTEMS; KERNEL OIL; L.; TOCOPHEROL; AMYGDALUS; WILD	An investigation into solid-liquid-based soxhlet extraction and drying pretreatment was conducted for the oil extraction from almond seed powder. The best possible combination of extraction parameters was obtained with interval type-2 fuzzy logic. Four major parameters: extraction time, temperature, moisture content and solvent-to-sample ratio were taken as input variables, and oil recovery and stability index were taken as output variables to optimize the extraction process based on their selected linguistic nature. Using these four input and two output parameters, eight Mamdani fuzzy inference systems were formed depending on the different membership functions of the variables. Finally, a statistical analysis has been performed using type-2 fuzzy data set to improve the control of process parameters that can be easily determined in type-2 fuzzy environment to get high yield as well as prominent quality.																	1432-7643	1433-7479				JAN	2020	24	1			SI		41	51		10.1007/s00500-019-04158-1													
J								Intuitionistic fuzzy adaptive sliding mode control of nonlinear systems	SOFT COMPUTING										Intuitionistic fuzzy; Adaptive sliding mode; Robust control	INTERVAL TYPE-2; INVERTED PENDULUM; LOGIC SYSTEMS; REPRESENTATION; AGGREGATION; OPERATOR	In this paper, Takagi-Sugeno intuitionistic fuzzy adaptive sliding mode control system (TS-IFASMC) is designed for nonlinear systems. We propose an intuitionistic fuzzy method to determine the parameters of the adaptive sliding mode control method which is used to control nonlinear systems. Intuitionistic fuzzy systems are considered as a skilled tool to model uncertainty in systems so they can transfer expert knowledge to control schemes better than the other classical methods, and real-world problems can be handled more effectively with this control method. In the proposed system, control parameters are defined by the intuitionistic fuzzy membership, non-membership, hesitation degrees and an integral sliding mode surface for a robust control performance. The novelty of this study is the use of Takagi-Sugeno type intuitionistic fuzzy system in adaptive sliding mode control method and comparison of performance of this new system with other classical methods. Thus, adaptive sliding mode controller based on the Takagi-Sugeno intuitionistic fuzzy system is obtained to provide robust control performance. Finally, the results support the effectiveness of the presented control scheme.																	1432-7643	1433-7479				JAN	2020	24	1			SI		53	64		10.1007/s00500-019-04286-8													
J								An optimal redistribution plan considering aftermath disruption in disaster management	SOFT COMPUTING										Disaster management; Solid transportation problem; Trapezoidal fuzzy number; Trapezoidal neutrosophic number; Redistribution; Disruption	FACILITY LOCATION; DECISION-MAKING; NETWORK DESIGN; OR/MS RESEARCH; MODEL; LOGISTICS; OPTIMIZATION; UNCERTAINTY; EVACUATION; ALLOCATION	Unpredictable occurrence of any disaster emerges immeasurable demand in an affected society. Importance of immediate response in the aftermath of disaster is a crucial part of humanitarian logistic. Resource redistribution among the affected areas makes the optimal allocation in this chaotic situation. The research work has introduced a transportation plan considering the redistribution of resources from those areas which has already acquired relief and restored the normal condition to those areas still not being recovered from the effect of calamities. This research plan is developed to minimize the total cost of the relief operation as well as optimal allocation of the resources. The optimal allocation amidst the disruption of some resource storing points in the aftermath attack of disaster is also one of the key factors of the research. This research work has a great impact for decision-maker to derive an appropriate decision-making in such an anarchic situation of critical humanitarian supply chain. Due to the complexity of disaster, the model is considered in mixed uncertain environment. A numerical study is also performed to show the smooth functioning of the mathematical model assuming the uncertainty by trapezoidal neutrosophic number. Also, trapezoidal fuzzy number is implemented for uncertain parameters of the mathematical model and hereby compared with trapezoidal neutrosophic number.																	1432-7643	1433-7479				JAN	2020	24	1			SI		65	82		10.1007/s00500-019-04287-7													
J								Toward a development of general type-2 fuzzy classifiers applied in diagnosis problems through embedded type-1 fuzzy classifiers	SOFT COMPUTING										General type-2 fuzzy logic; Fuzzy classifier; Footprint of uncertainty	LOGIC SYSTEMS; CLASSIFICATION; SVM; OPTIMIZATION; REDUCTION; ALGORITHM; NETWORK; IMPACT; SETS	Nowadays, with the emergence of computer-aided systems, diagnosis problems are one of the most important application areas of artificial intelligence. The present paper is focused on a specific kind of computer-aided diagnosis system based on General Type-2 Fuzzy Logic. The main goal is the generation of General Type-2 Fuzzy Classifiers that can handle the data uncertainty. The concept of embedded Type-1 Fuzzy membership functions has been proposed to be used in the design of General Type-2 Fuzzy Classifiers. A methodology for generating the embedded Type-1 fuzzy membership functions is introduced, and the subsequent approach for developing the Footprint of Uncertainty of the General Type-2 Fuzzy Classifier is presented. On the other hand, the proposed approach performance is evaluated by the experimentation with different diagnosis benchmark problems. In addition, a statistical comparison with respect to another existing approach of General Type-2 Fuzzy classifiers is presented.																	1432-7643	1433-7479				JAN	2020	24	1			SI		83	99		10.1007/s00500-019-04157-2													
J								Improved grey wolf optimization-based feature subset selection with fuzzy neural classifier for financial crisis prediction	SOFT COMPUTING										Financial crisis prediction; Feature selection; Fuzzy neural classifier; Grey wolf optimization	ALGORITHM; NETWORKS; DISTRESS	In present days, prediction of financial crisis of a company is a hot research area. The use of data mining and machine learning algorithms assists to resolve the financial crisis prediction (FCP) problem. Since financial data contain more demographical and unwanted information, it might decrease the classification performance significantly. So, feature selection (FS) process is applied to choose useful data and remove the irrelevant repetitive data. This paper introduces a novel predictive framework for FCP model by the incorporation of improved grey wolf optimization (IGWO) and fuzzy neural classifier (FNC). An IGWO algorithm is derived by the integration of GWO algorithm and tumbling effect. The presented IGWO-based FS method is employed to discover the optimal features from the financial data. For classification purposes, FNC is employed. The proposed method is experimented on two benchmark data sets, namely Australian Credit and German data set under several of performance metrics. The experimental values verified the superior nature of the proposed FCP model over the compared methods.																	1432-7643	1433-7479				JAN	2020	24	1			SI		101	110		10.1007/s00500-019-04323-6													
J								A novel parameter estimation in dynamic model via fuzzy swarm intelligence and chaos theory for faults in wastewater treatment plant	SOFT COMPUTING										Fault detection; Whale optimization algorithm; Wastewater treatment; Chaos theory; Fuzzy c-means algorithm	OPTIMIZATION; ALGORITHMS; DIAGNOSIS; SYSTEMS	Faults during a wastewater treatment for plant (WWTP) are critical issue for social and biological. Poorly treated wastewater may achieve dangerous effect for human as well as nature. This paper proposed a novel model based on a binary version of whale optimization algorithm (WOA), chaos theory and fuzzy logic, namely (CF-BWOA). CF-BWOA is applied in the application of WWTP to find out the more relevant attributes from the whole dataset, reducing cost and validation of decision rules, and helping to identify a non-well-structured domain. CF-BWOA attempts to reduce the whole feature set without loss of significant information to the classification process. Fast fuzzy c-means is used as a cost function to measure the fuzzification and uncertainty of data. Ten different chaos sequence maps are used to estimate and tune WOA parameters. Experiments are applied on a complex real-time dataset with various uncertainty features and missing values. The overall result indicates that the CWOA with the Sine chaos map shows the better performance, lower error, higher convergence speed and shorter execution time. In addition, the proposed model is capable of detecting sensor process faults in WWTP with high accuracy and can guide the operators of these systems to control decisions.																	1432-7643	1433-7479				JAN	2020	24	1			SI		111	129		10.1007/s00500-019-04225-7													
J								A ranking method based on interval type-2 fuzzy sets for multiple attribute group decision making	SOFT COMPUTING										Multi-attribute group decision making; Interval type-2 fuzzy set; Ranking method; Centroid point	TRANSPORTATION PROBLEM; MCDM METHOD; NUMBERS; SELECTION; RISK; WASPAS; TOPSIS; WORDS; LOGIC	Ranking of fuzzy numbers has become an important research direction for decision-making problems due to its role to find the best objects under uncertainty. In this paper, we propose a new approach to perform multiple attribute group decision-making (MAGDM) problems using the ranking of interval type-2 fuzzy sets. Initially, a new ranking method for interval type-2 fuzzy numbers based on centroid and rank index has been proposed. Next, we present a comparative study to analyze the ranking values of the proposed method with the existing approaches, where we explore the necessity of the proposed ranking method. After that, a new MAGDM approach has been developed using the proposed ranking procedure to solve uncertain MAGDM problems. Finally, the applicability of the proposed approach has been illustrated using two numerical examples and a case study related to car-sharing problems. The proposed study exhibits a useful way to solve fuzzy MAGDM problems with much efficient manner since it applies interval type-2 fuzzy sets compared to type-1 fuzzy sets to signify the evaluating values and weights of the attributes.																	1432-7643	1433-7479				JAN	2020	24	1			SI		131	154		10.1007/s00500-019-04285-9													
J								Multi-objective optimization of cost-effective and customer-centric closed-loop supply chain management model in T-environment	SOFT COMPUTING										Customer satisfaction; Environmental concerns; Closed-loop supply chain; T-set; Subsidy; Multi-objective optimization	DECISION-MAKING; CONCEPTUAL-FRAMEWORK; MEDIATING ROLE; SOCIAL MEDIA; PERFORMANCE; SATISFACTION; QUALITY; IMPACT; CRM; SUSTAINABILITY	This article presents one real-life-based cost-effective and customer-centric closed-loop supply chain management model. The review of the existing literature identifies the classical performance indicators to any supply chain management model as the aggregate revenue, the customer satisfaction and the environmental concern. However, this review fails to find a single optimization-based supply chain management model that considers these three indicators, simultaneously. In this article, the proposed model maximizes the customer-satisfaction index and the aggregate revenue both under the environmental considerations via the reverse chain, whereas many existing studies took the reverse chain and the associated subsidies into account; this is the first mathematical model that optimizes the customer-satisfaction index, at the same time. This article employs the T-set that represents the inherent impreciseness to objective functions to the proposed model. The corresponding optimal values are superior than stipulated goals to both the objective functions in T-environment. The managerial insights extracted from sensitivity analysis of parameters suggest the managers to stabilize the environmental concern and the customer satisfaction, while ensuring the cost-effectiveness in real-life-based T-environment. Also, this analysis finds that the subsidy assists any supply chain to sustain, only if it is offered without any break and within the optimally determined bounds.																	1432-7643	1433-7479				JAN	2020	24	1			SI		155	178		10.1007/s00500-019-04289-5													
J								Dynamic parameter adaptation in the harmony search algorithm for the optimization of interval type-2 fuzzy logic controllers	SOFT COMPUTING										Dynamic parameter adaptation; Harmony search algorithm; Fuzzy controller; Uncertainty; Interval type-2 fuzzy logic	SYSTEMS; DESIGN	At the present time there are several types of metaheuristics which have been used to solve various types of problems in the real world. These metaheuristics contain parameters that are usually fixed throughout the iterations. However, various techniques exist to adjust the parameters of an algorithm such as probabilistic, fuzzy logic, among others. This work describes the methodology and equations for building Triangular and Gaussian interval type-2 membership functions, and this methodology was applied to the optimization of a benchmark control problem with an interval type-2 fuzzy logic controller. To validate in the best way the effect of uncertainty we perform experiments using noise (Pulse generator) and without noise. Also, a statistical z-test is presented to verify the effectiveness of the proposed method. The main contribution of this article is the proposed use of the theory of interval type-2 fuzzy logic to the dynamic adjustment of parameters for the harmony search algorithm and then its application to the optimal design of interval type-2 fuzzy logic controller.																	1432-7643	1433-7479				JAN	2020	24	1			SI		179	192		10.1007/s00500-019-04124-x													
J								Optimization of fuzzy controller design using a Differential Evolution algorithm with dynamic parameter adaptation based on Type-1 and Interval Type-2 fuzzy systems	SOFT COMPUTING										Differential Evolution; Problem control; Fuzzy control; Parameters	MOBILE ROBOTS; LOGIC SYSTEMS	This paper proposes the use of the Differential Evolution algorithm with fuzzy logic for parameter adaptation in the optimal design of fuzzy controllers for nonlinear plants. The Differential Evolution algorithm is enhanced using Type-1 and Interval Type-2 fuzzy systems for achieving dynamic adaptation of the mutation parameter. In this paper, four control optimization problems in which the Differential Evolution algorithm optimizes the membership functions of the fuzzy controllers are presented. First, the experiments were performed with the original algorithm, second the experiments were performed with the Fuzzy Differential Evolution (in this case the mutation parameter is dynamic), and last, experiments were performed applying noise to the control plant by using Fuzzy Differential Evolution.																	1432-7643	1433-7479				JAN	2020	24	1			SI		193	214		10.1007/s00500-019-04156-3													
J								Monadic Boolean algebras with an automorphism and their relation to Df(2)-algebras	SOFT COMPUTING										Monadic Boolean algebra; Df(2)-algebra; Congruence; Subdirectly irreducible algebra; Discriminator variety		In this work, we initiate an investigation of the class B-Tkm of monadic Boolean algebras endowed with a monadic automorphism of period k. These algebras constitute a generalization of monadic symmetric Boolean algebras. We determine the congruences on these algebras and we characterize the subdirectly irreducible algebras. This last result allows us to prove that B-Tkm is a discriminator variety and as a consequence, the principal congruences are characterized. Finally, we explore, in the finite case, the relationship between this class and the class Df(2) of diagonal-free two-dimensional cylindric algebras.																	1432-7643	1433-7479				JAN	2020	24	1			SI		227	236		10.1007/s00500-019-04317-4													
J								Classification of non-local rings with genus two zero-divisor graphs	SOFT COMPUTING										Non-local rings; Zero-divisor graphs; Genus of a graph; Double torus	UNITARY CAYLEY; PLANAR	The zero-divisor graph of a commutative ring R is a simple graph whose vertices are the nonzero zero divisors of R and two distinct vertices are adjacent if their product is zero. In this article, we determine precisely all non-local commutative rings whose zero-divisor graphs have genus two.																	1432-7643	1433-7479				JAN	2020	24	1			SI		237	245		10.1007/s00500-019-04345-0													
J								n-Normal residuated lattices	SOFT COMPUTING										Residuated lattice; omega-filter; Coannihilator; Coannulet; Normal residuated lattice; n-normal residuated lattice; Reticulation	REPRESENTATIONS	The notion of n-normal residuated lattice, as a subclass of residuated lattices in which every prime filter contains at most n minimal prime filters, is introduced and investigated. Before that, the notion of.-filter is introduced and it is observed that the set of omega-filters in a residuated lattice forms a distributive lattice on its own, which includes the set of coannulets as a sublattice. The class of n-normal residuated lattices is characterized in terms of their prime filters, minimal prime filters, coannulets and omega-filters. It is shown that a residuated lattice is normal if and only if its reticulation is conormal. Finally, the existence of the greatest omega-filters contained in a given filter of a normal residuated lattice is obtained.																	1432-7643	1433-7479				JAN	2020	24	1			SI		247	258		10.1007/s00500-019-04346-z													
J								Indefinite LQ optimal control for discrete-time uncertain systems	SOFT COMPUTING										LQ optimal control; Indefinite weighting matrices; Discrete-time uncertain systems; Recurrence equation; Generalized inverse	PORTFOLIO SELECTION; RICCATI-EQUATIONS; STATE	This paper is concerned with a linear quadratic (LQ) optimal control for discrete-time uncertain systems, with indefinite state and control weighting matrices in the cost function. Firstly, a recurrence equation of general optimal control problem for discrete-time uncertain systems is obtained by applying Bellman's principle of optimality. Then, the optimal state feedback control is obtained based on the recurrence equation. Moreover, a sufficient condition of well-posedness for the LQ problem is proposed and a general expression for the optimal control set is given. Furthermore, a numerical example is presented by using the obtained results. Finally, as an application of the indefinite LQ optimal control, an optimal production inventory problem of uncertain environment is solved.																	1432-7643	1433-7479				JAN	2020	24	1			SI		267	279		10.1007/s00500-019-04350-3													
J								Anonymous certificateless multi-receiver encryption scheme for smart community management systems	SOFT COMPUTING										Certificateless cryptography; Encryption; Multi-receiver; Pairing; Security; Standard model	PUBLIC-KEY ENCRYPTION; IDENTITY-BASED ENCRYPTION; BROADCAST ENCRYPTION; MULTIRECEIVER ENCRYPTION; STRONGLY SECURE; PROTOCOL; CRYPTOSYSTEMS; CRYPTANALYSIS; ENCAPSULATION; GDLP	In community management services, it is a common requirement for management centers to send the same encrypted message to some units and individuals in the community, while avoiding the leakage of personal information of the user. In order to achieve this goal safely and efficiently, the multi-receiver encryption is a good option. In the setting, a sender generates the ciphertext for a designed group of receivers. Any receiver in the group can obtain the plaintext by decrypting the ciphertext using his own private key, and the true identity of the receiver is kept secret to anyone including other receivers. Recently, several certificateless multi-receiver encryption (CLMRE) schemes have been introduced, and all of them are proved to be secure in the random oracles model (ROM). ROM is a simulation of the hash function and can not replace the real hash function computation. In this paper, a new CLMRE scheme is constructed and it is proved to be secure based on decision bilinear Diffie-Hellman problem in the standard model (SM). It achieves the anonymity of the receivers and is suitable for smart community management systems.																	1432-7643	1433-7479				JAN	2020	24	1			SI		281	292		10.1007/s00500-019-04375-8													
J								Quantile fuzzy regression based on fuzzy outputs and fuzzy parameters	SOFT COMPUTING										Fuzzy number; Goodness of fit; Loss function; Fuzzy parameter; Quantile fuzzy regression	GOAL PROGRAMMING APPROACH; LEAST-SQUARES ESTIMATION; LINEAR-REGRESSION; MODEL	A new approach is investigated to the problem of quantile regression modeling based on the fuzzy response variable and the fuzzy parameters. In this approach, we first introduce a loss function between fuzzy numbers which it can present some quantiles of fuzzy data. Then, we fit a quantile regression model between the available data based on proposed loss function. To evaluate the goodness of fit of the optimal quantile fuzzy regression models, we introduce two indices. Inside, we study the application of the proposed approach in modeling some soil characteristics, based on a real data set.																	1432-7643	1433-7479				JAN	2020	24	1			SI		311	320		10.1007/s00500-019-04424-2													
J								Spatial-domain steganalytic feature selection based on three-way interaction information and KS test	SOFT COMPUTING										Spatial-domain steganalytic features; Three-way interaction information; Feature selection; Steganalysis; KS test	AGGREGATION OPERATORS; MUTUAL INFORMATION	To select informative features from steganalytic features, a spatial-domain steganalytic feature selection method based on three-way interaction information and Kolmogorov-Smirnov (KS) test is proposed. Three-way interaction information is employed to rank all the features, and KS test is exploited to remove redundant features. Feature selection process of the proposed method is presented as follows: It calculates mutual information between features and the class label and selects the feature with the maximum value. Then, it loops to calculate three-way interaction information among each candidate feature, the previously selected feature and the class label and select the candidate feature with the maximum value. Following that, it calculates KS test between features and compares an obtained parameter with the predefined significance level for eliminating redundant features. To validate the performance of the proposed method, several typical feature ranking methods based on information measure and spatial-domain steganalytic feature selection methods are adopted for performance comparisons. Experimental results demonstrate that the proposed method can achieve better feature selection performance.																	1432-7643	1433-7479				JAN	2020	24	1			SI		333	340		10.1007/s00500-019-03910-x													
J								Optimal feature selection in industrial foam injection processes using hybrid binary Particle Swarm Optimization and Gravitational Search Algorithm in the Mahalanobis-Taguchi System	SOFT COMPUTING										Feature selection; Mahalanobis-Taguchi System; Binary Particle Swarm Optimization; Binary Gravitational Search Algorithm	TEXT FEATURE-SELECTION; TOOL	The detection of variables that contribute to the variation of a system is one of the most important considerations in the industrial manufacturing processes. This work presents the combination of Mahalanobis-Taguchi system and a hybrid binary metaheuristic based on particle swarm optimization and gravitational search algorithm (BPSOGSA) to perform an optimal feature selection in order to detect the relevant variables in a real process of foam injection in automotive industry. The proposed method is compared with other feature selection approach based in binary PSO algorithm. The experimental results revealed that BPSOGSA is faster and successfully converge selecting a smallest subset of features than BPSO. Moreover, the feature selection effect is validated through other widely used machine learning algorithms which improve their accuracy performance when they are trained with the subset of detected variables by the proposed system.																	1432-7643	1433-7479				JAN	2020	24	1			SI		341	349		10.1007/s00500-019-03911-w													
J								Scheduling multi-component maintenance with a greedy heuristic local search algorithm	SOFT COMPUTING										Maintenance scheduling; Downtime; Repair	IMPERIALIST COMPETITIVE ALGORITHM; PREVENTIVE MAINTENANCE; SYSTEM; OPTIMIZATION; POLICY; MODEL	As many large-scale systems age, and due to budgetary and performance efficiency concerns, there is a need to improve the decision-making process for system sustainment, including maintenance, repair, and overhaul (MRO) operations and the acquisition of MRO parts. To help address the link between sustainment policies and acquisition, this work develops a greedy heuristic-based local search algorithm (GHLSA) to provide a system maintenance schedule for multi-component systems, coordinating recommended component maintenance times to reduce system downtime costs, thereby enabling effective acquisition. The proposed iterative algorithm aims to minimize the sum of downtime, earliness and tardiness costs of scheduling, which contains three phases: (1) the construction phase, which uses a heuristic to construct an initial partial solution, (2) an improvement phase, which aims to improve the partial solution generated in the construction phase, and finally, (3) a local search phase, which performs a local search technique to the partial solution found in the improvement phase. The proposed algorithm makes a trade-off between exploration and exploitation of solutions. The experimental results for small (10 jobs) and large size (50 jobs) problems indicate that GHLSA outperforms both genetic algorithm and simulated annealing approaches in terms of solution quality and is similar in terms of efficiency.																	1432-7643	1433-7479				JAN	2020	24	1			SI		351	366		10.1007/s00500-019-03914-7													
J								Logarithmic means of sequences of fuzzy numbers and a Tauberian theorem	SOFT COMPUTING										Sequences of fuzzy numbers; Tauberian theorems; Logarithmic means; Slow oscillation	SUMMABLE DOUBLE SEQUENCES; AGGREGATION OPERATORS; CESARO SUMMABILITY	A sequence (x(n)) of fuzzy numbers is said to be summable to a fuzzy number L by the logarithmic mean method (l, 2) if lim(n ->infinity) 1/l(n)((2)) Sigma(n)(k=1) x(k)/kl(k) = L where l(n)((2)) = Sigma(n)(k=1) 1/kl(k) similar to log(log n). We prove that the ordinary convergence of (x(n)) implies its (l, 2) summability. The converse implication is not necessarily true. Namely, the (l, 2) summability of (x(n)) may not imply the convergence of (x(n)). However, under certain additional conditions the converse may hold. Such conditions are called Tauberian conditions, and the resulting theorem is said to be a Tauberian theorem. In this paper, we provide necessary and sufficient Tauberian conditions to transform (l, 2) summable sequences of fuzzy numbers into convergent sequences of fuzzy numbers with preserving the limit.																	1432-7643	1433-7479				JAN	2020	24	1			SI		367	374		10.1007/s00500-019-03915-6													
J								Optimal replacement policy with minimal repair and preventive maintenance of an aircraft structure subjected to corrosion	SOFT COMPUTING										Replacement policy; Preventive maintenance; Minimal repair; Reliability; Corrosion	SYSTEMS	This study focuses on a replacement policy using preventive maintenance and periodic inspections with minimal repairs for a deteriorating aircraft structure suffering from corrosion damage. The type of structure component operates normally when its cumulative corroded depth that does not exceed the maintenance allowance threshold and satisfies the reliability requirement. We assume that the number of minimal repairs and preventive maintenance time are both stochastic variables with a independent geometric distribution, and the lifetime has a Weibull distribution. An optimal replacement model is formulated to minimize the expected cost rate with two constraints: corrosion threshold and reliability level. The optimal time interval T* between the two successive periodic inspections with minimal repairs and the number of preventive maintenance N* can be obtained by an improved iteration algorithm using the golden section method and quadratic interpolation method based on stochastic simulation. Finally, a numerical example using parameters sensitivity analysis is illustrated to verify the proposed model and algorithm.																	1432-7643	1433-7479				JAN	2020	24	1			SI		375	384		10.1007/s00500-019-03919-2													
J								A topological method for reduction in digital information uncertainty	SOFT COMPUTING										Topological concepts; Rough set; Reduction knowledge; Decision making	ROUGH; APPROXIMATION; RULES	An attribute reduct, an important concept of rough set theory, is a subset that is sufficient and individually necessary for preserving a particular property of the given information system. In this study, we present a proposed method to calculate the accuracy of data by using the concepts of pre-open and semi-open. We also compared the results of accuracies in the proposed method with the accuracies in Yao and Pawlak methods. Our study revealed that the new model calculating the degree of accuracy was better than the previous models. Additionally, we provided a new insight into the application of the attribute reduction and we used MATLAB programming to obtain the result.																	1432-7643	1433-7479				JAN	2020	24	1			SI		385	396		10.1007/s00500-019-03920-9													
J								Utilization of trapezoidal intuitionistic fuzzy numbers and extended fuzzy preference relation for multi-criteria group decision-making based on individual differentiation of decision-makers	SOFT COMPUTING										Extended fuzzy preference relation; Individual differentiation; Intuitionistic fuzzy numbers; MCGDM; TOPSIS	AGGREGATION OPERATORS; MODEL	In 2015, Li and Chen proposed a multi-criteria group decision-making (MCGDM) method, as similar as technique for order preference by similarity to ideal solution (TOPSIS), with trapezoidal intuitionistic fuzzy information to derive preference values (i.e., alternative ratings) based on individual differentiation of decision-makers. The individual differentiation consideration of decision-makers was useful because alternative ratings and criteria weights in group were generally derived by mean computation in the past. Additionally, MCGDM with trapezoidal intuitionistic fuzzy information is regarded to be the extension of multi-criteria decision-making (MCDM) with group decision-making under fuzzy environment. Practically, fuzzy extension of MCDM is commonly complicated based on the characteristics of fuzzy numbers, especially for intuitionistic fuzzy numbers that may be the most complex one for all kinds of fuzzy numbers. Obviously, Li and Chen's contribution was extending MCGDM based on individual differentiation of decision-makers under trapezoidal intuitionistic fuzzy environment. Unfortunately, Li and Chen's method merely expressed the importance difference of decision-makers for yielding preference values, but they derived criteria weights by mean computation. Therefore, the computations of preference values and criteria weights are inconsistent on considering the importance of decision-makers. Besides, their fuzzy extension was complicated and hard for MCGDM with trapezoidal intuitionistic fuzzy information. To resolve the importance inconsistent of yielding ratings and weights as well as fuzzy operation complicated ties, we utilize trapezoidal intuitionistic fuzzy numbers and extended fuzzy preference relation for MCGDM based on individual differentiation of decision-makers in this paper. By utilization of extended fuzzy preference relation, both alternative ratings and criteria weights of MCGDM under intuitionistic fuzzy environment are yielded based on individual differentiation of decision-makers, and decision-making problems are easily and reasonably solved. Furthermore, we also use the simplified version of the MCGDM with trapezoidal intuitionistic fuzzy numbers to evaluate alternatives in the illustrating example of Li and Chen, and compare their evaluating result with the result of proposed method based on individual differentiation of decision-makers.																	1432-7643	1433-7479				JAN	2020	24	1			SI		397	407		10.1007/s00500-019-03921-8													
J								Efficient compression of volumetric medical images using Legendre moments and differential evolution	SOFT COMPUTING										Volumetric medical images; Compression; Legendre moments; Differential evolution	COMPUTATION	Volumetric medical images are widely used in diagnosing and detecting health problems of patients. Large datasets of volumetric medical images required huge storage space and high network capabilities to transmit these medical images from one location to another especially in the applications of telemedicine and teleradiology. In addition, the quality of medical images plays an important role in successful diagnoses. Therefore, an efficient compression algorithm must achieve significant reduction in the size of these volumetric medical images by using high compression ratio and preserve the quality of these images for successful diagnosis. In this paper, a novel optimized compression algorithm for volumetric medical images is proposed. In this algorithm, the volumetric medical images are divided into two-dimensional (2D) slices where each slice is divided into a group of 8 x 8 nonoverlapped blocks. The Legendre moments are computed for each block where the differential evolution optimization algorithm is utilized to select the optimum moments according to minimization of the cost function. Volumetric medical images from different medical imaging modalities are used in testing and evaluating the proposed compression algorithm. The performance of the proposed algorithm is compared with the existing volumetric medical images compression algorithms where the comparison clearly shows that the proposed algorithm outperforms the existing compression algorithms in terms of mean square error, peak signal-to-noise ratio, normalized correlation coefficient, and structural similarity index.																	1432-7643	1433-7479				JAN	2020	24	1			SI		409	427		10.1007/s00500-019-03922-7													
J								An intuitionistic fuzzy projection-based approach and application to software quality evaluation	SOFT COMPUTING										Normalized projection measure; Group decision-making; Intuitionistic fuzzy vector; Interval-valued intuitionistic fuzzy vector; Software quality evaluation	GROUP DECISION-MAKING; RISK-EVALUATION; CRISP VALUES; INFORMATION; PERFORMANCE; MODEL; METHODOLOGY; WEIGHTS; MAKERS; TOPSIS	Projection is a very important measure in decision science. However, this research finds that the existing projection measures are not always reasonable in intuitionistic fuzzy settings. To solve this problem, this work provides a new normalized projection measure. And this work establishes a new group decision-making model based on new normalized projection measure and TOPSIS (technique for order preference by similarity to ideal solution) technique. This paper also introduces a practical application to the software quality evaluation. An experimental analysis shows the practicability, feasibility and validity of method introduced in this paper. In a word, this article contributes to knowledge domain a new decision-making technique and tool.																	1432-7643	1433-7479				JAN	2020	24	1			SI		429	443		10.1007/s00500-019-03923-6													
J								An improved watermarking algorithm for color image using Schur decomposition	SOFT COMPUTING										Schur decomposition; Color image; Watermarking; Invisibility	ROBUST WATERMARKING	In order to protect the color image copyright protection of the multimedia big data, it is necessary to design a color image watermarking algorithm. To achieve this purpose, an improved color image watermarking algorithm based on Schur decomposition is proposed in this paper. First, the watermark information is, respectively, embedded into the upper triangular matrix and the unitary matrix of Schur decomposition by two different methods, and two temporary watermarked image blocks are obtained. Then, the proposed improved method is used to select the final watermarked image block from these temporary watermarked image blocks. The highlight of the proposed method is that the final watermarked block has less visual distortion. Meanwhile, the embedded flag is created and uploaded to the cloud service provider with the watermarked image. When extracting watermark, the original host image or the watermark image is not needed. Experimental results show that the proposed watermarking algorithm has better performance; in particular, the watermark invisibility has been obviously improved than other methods considered in this paper.																	1432-7643	1433-7479				JAN	2020	24	1			SI		445	460		10.1007/s00500-019-03924-5													
J								An improved scatter search algorithm for the corridor allocation problem considering corridor width	SOFT COMPUTING										Corridor allocation problem; Facility layout; Scatter search algorithm; Simulated annealing operation	FACILITY LAYOUT; OPTIMIZATION; DESIGN	In the existing literature on the corridor allocation problem (CAP), the corridor width is not taken into consideration. But in the actual production, the corridor width plays a very important role in logistics transportation inside factories. To study the effect of the corridor width in a CAP problem, the corridor width is considered by a mixed-integer programming model proposed in this paper. Subsequently, an improved scatter search (ISS) algorithm is proposed to handle the CAP. Several improvement mechanisms have been applied to the ISS according to the special characteristics of the problem, such as the adoption of a simulated annealing operation, a dynamic reference set update method, and an improved subset generation method. The proposed ISS is evaluated on test instances of various sizes ranging from 9 to 49 facilities. Computational results demonstrate the validity of the ISS. Specifically, for small-sized instances, the acquired best solutions by the ISS are identical to the optimal solutions obtained by the exact solution given by GUROBI, while for moderate and large-sized instances, the objective values by the ISS are better than those solved by the method in GUROBI. Furthermore, the proposed algorithm shows better performance in solution quality and stability by comparing to the simulated annealing algorithm and the scatter search algorithm.																	1432-7643	1433-7479				JAN	2020	24	1			SI		461	481		10.1007/s00500-019-03925-4													
J								Some solvingmethods for a fuzzy multi-point boundary value problem	SOFT COMPUTING										Fuzzy generalized derivatives; The fuzzy second-order differential equations; The multi-point boundary value problems; The real Green's function method	EQUATIONS	In this paper, we consider a fuzzy multi-point boundary value problem-FMBVP [or a multi-point boundary value problem (MBVP) for fuzzy second-order differential equations (FSDEs) under generalized Hukuhara differentiability]. We present solving methods for a FMBVP in the space of fuzzy numbers E-1, such that we have shown the ability to and methods to find solution of the MBVP for FSDEs in the form of (FHgi - FHgj)-solutions. In addition, we provide with a new idea to develop the real Green's function method and give two examples being simple illustration of this FMBVP.																	1432-7643	1433-7479				JAN	2020	24	1			SI		483	499		10.1007/s00500-019-03926-3													
J								Partial divergence measure of uncertain random variables and its application	SOFT COMPUTING										Chance theory; Uncertain random variable; Partial entropy; Partial divergence measure; Portfolio selection	CROSS-ENTROPY; LARGE NUMBERS; MODELS; LAW	Cross-entropy (divergence measure) of two uncertain random variables characterizes the difference of two chance distributions. Sometimes, we occur with a complex system as a mixture of uncertain variables and controllable random variables; in order to characterize the difference in these situations, this paper introduces the concept of partial divergence measure of two uncertain random variables and investigates several properties of this concept. Furthermore, some formulas are derived to calculate the partial divergence measure. And how to use these formulas, several examples are provided. Finally, as an application of partial divergence measure, the concept is used to portfolio selection with uncertain random returns as a mixture of new markets and controllable historical markets.																	1432-7643	1433-7479				JAN	2020	24	1			SI		501	512		10.1007/s00500-019-03929-0													
J								Perception based performance analysis of higher education institutions: a soft computing approach	SOFT COMPUTING										Ranking; Perception; Fuzzy logic; DEA; TOPSIS; Entropy method	AUSTRALIAN UNIVERSITIES; EFFICIENCY ANALYSIS; DEPARTMENTS; TECHNOLOGY; ECONOMICS	In the tertiary education institutions, rankings have started gaining ample attention all over the world. This has created a profound impact on the indian higher education system. As a result of that, in 2015, the government of India announced National institutional ranking framework (NIRF) to rank the indian institutions. NIRF is based on multiple parameters which are evaluated by standardised survey. In this work, a mathematical model, which can handle such multiple parameters to rank higher education institutions (HEIs), has been proposed. In this model, six criteria, named as, student intake, faculty strength, expenditure of the institution, research paper published per faculty, placements and perception, are considered. Since the criterion perception is a qualitative criterion and can not be modelled by classical mathematics, fuzzy rule based inference system is proposed to determine its precise value. Then DEA-Entropy-TOPSIS approach has been employed to rank HEIs. To emphasize the applicability of the proposed method, a numerical illustration is provided. It is asserted that this proposed mathematical model is a unique HEIs ranking approach involving human perception.																	1432-7643	1433-7479				JAN	2020	24	1			SI		513	521		10.1007/s00500-019-03931-6													
J								Distance measures on intuitionistic fuzzy sets based on intuitionistic fuzzy dissimilarity functions	SOFT COMPUTING										Intuitionistic fuzzy sets; Distance measures; Dissimilarity functions; Fuzzy equivalencies; Intuitionistic fuzzy dissimilarity functions; Pattern recognition	SIMILARITY MEASURES; VAGUE SETS; DIVERGENCE; ENTROPY	One of the significant topics in intuitionistic fuzzy set (IFS) is the measure of the distance between IFSs. Although distance measures on IFSs have been widely studied in previous studies, there are few studies about the generation of them. In this paper, a quaternary function called intuitionistic fuzzy dissimilarity function is proposed to construct distance measures on IFSs. Two methods for building intuitionistic fuzzy dissimilarity functions are presented. The first one is obtained by combining dissimilarity functions and fuzzy equivalencies. The second one is obtained based on constructing new intuitionistic fuzzy dissimilarity functions through other existing ones. We also examine and compare some properties of intuitionistic fuzzy dissimilarity functions, through which we obtain some properties of distance measures on IFSs. Some examples of pattern recognition are applied to illustrate the effectiveness of the proposed distance measures on IFSs.																	1432-7643	1433-7479				JAN	2020	24	1			SI		523	541		10.1007/s00500-019-03932-5													
J								A multiple pheromone ant colony optimization scheme for energy-efficient wireless sensor networks	SOFT COMPUTING										Energy-aware routing; Wireless sensor networks; Swarm intelligence; Ant colony optimization	ROUTING PROTOCOL; CLUSTERING-ALGORITHM; ARCHITECTURE	Ant colony optimization (ACO) is a well-applied technique to solve the real-time problem of discovering the energy-efficient routes to transmit the sensing information to the base station (BS). Traditionally, ACO incorporated wireless sensor networks used only one pheromone, i.e., minimum distance between the sensor nodes to discover the optimum route to the BS. The authors illustrated a multiple pheromone-based ACO technique known as multiple pheromone ant colony optimization (MPACO), for instance, distance between sensing nodes, their residual energy and number of neighbor nodes to ascertain an efficient route. MPACO enables the sensing nodes to transmit the sensing data to BS over optimal routes with economical energy consumption to achieve a prolonged network life span. The comprehensive evaluation reveals that MPACO proffers 20% more network lifetime than the current existing ACO technique, i.e., improved ACO. Moreover, MPACO shows a significant improvement of 300% in network life span than another existing fuzzy-based strategy, i.e., multi-objective fuzzy clustering algorithm.																	1432-7643	1433-7479				JAN	2020	24	1			SI		543	553		10.1007/s00500-019-03933-4													
J								A nifty collaborative analysis to predicting a novel tool (DRFLLS) for missing values estimation	SOFT COMPUTING										Intelligent data analysis; Missing values; Imputation methods; Random forest; Local least squares	TEXT FEATURE-SELECTION; GENE-EXPRESSION DATA; VALUE IMPUTATION; RANDOM FORESTS; ALGORITHMS; OPTIMIZATION	One of the important trends in an intelligent data analysis will be the growing importance of data processing. But this point faces problems similar to those of data mining (i.e., high-dimensional data, missing value imputation and data integration); one of the challenges in estimation missing value methods is how to select the optimal number of nearest neighbors of those values. This paper, attempting to search the capability of building a novel tool to estimate missing values of various datasets called developed random forest and local least squares (DRFLLS). By developing random forest algorithm, seven categories of similarity measures were defined. These categories are person similarity coefficient, simple similarity, and fuzzy similarity (M1, M2, M3, M4 and M5). They are sufficient to estimate the optimal number of neighborhoods of missing values in this application. Hereafter, local least squares (LLS) has been used to estimate the missing values. Imputation accuracy can be measured in different ways: Pearson correlation (PC) and NRMSE. Then, the optimal number of neighborhoods is associated with the highest value of PC and a smaller value of NRMSE. The experimental results were carried out on six datasets obtained from different disciplines, and DRFLLS proves the dataset which has a small rate of missing values gave the best estimation to the number of nearest neighbors by DRFPC and in the second degree by DRFFSM1 when r = 4, while if the dataset has high rate of missing values, then it gave the best estimation to number of nearest neighbors by DRFFSM5 and in the second degree by DRFFSM3. After that, the missing value was estimated by LLS, and the results accuracy was measured by NRMSE and Pearson correlation. The smallest value of NRMSE for a given dataset is corresponding to DRF correlation function which is a better function for a given dataset. The highest value of PC for a given dataset is corresponding to DRF correlation function which is a better function for a given dataset.																	1432-7643	1433-7479				JAN	2020	24	1			SI		555	569		10.1007/s00500-019-03972-x													
J								Modeling of EHD inkjet printing performance using soft computing-based approaches	SOFT COMPUTING										Electrohydrodynamic (EHD) inkjet printing; Design of experiment (DOE); Statistical regression analysis; ANOVA; Artificial neural networks; Genetic algorithm	PARTICLE SWARM OPTIMIZATION; ARTIFICIAL NEURAL-NETWORKS; KRILL HERD ALGORITHM; ELECTROHYDRODYNAMIC JET; PULSED VOLTAGE; DESIGN; INTELLIGENCE; CAPABILITIES; PATTERNS; PROTEIN	Nature-inspired heuristic and/or metaheuristic algorithms have been used for solving complex real-world problems in recent years. Electrohydrodynamic (EHD) inkjet printing is a microadditive manufacturing process in which high-resolution jets of polarizable functional materials were deposited on the defined spot of a substrate at the appointed time. The quality of the printed features is derived by the complex physics of the system. Parameter modeling of this process was carried out by using regression analysis, a feed-forward neural network trained with backpropagation (BPNN) and a neural network trained with a genetic algorithm (GA-NN) separately. This study emphasizes the droplet diameter prediction of an EHD inkjet printing system and explores the applicability of the soft computing-based methods for this new emerging technology. Soft computing-based approaches have been developed for the first time in this area to model the EHD inkjet process. Five hundred data were produced through the conventional regression analysis to train the neural network-based models. Output droplet diameter was predicted for different combinations of input parameters such as standoff height (SH), applied voltage (AV) and ink flow rate (FR) using the above three approaches, and their performances were analyzed through some randomly created real experimental test cases. All three models gave good prediction accuracy with less than 10% error in the prediction of the droplet diameter. Furthermore, it had been observed that the performance of GA-NN surpasses both the regression- and BPNN-based approaches in most of the test cases. It achieved quite satisfactory average absolute percentage deviation value of 2.51% between the target and predicted output using GA-NN model, which also showed an improvement over the regression or BPNN model.																	1432-7643	1433-7479				JAN	2020	24	1			SI		571	589		10.1007/s00500-019-04202-0													
J								Zombie politics: evolutionary algorithms to counteract the spread of negative opinions	SOFT COMPUTING										Dynamic optimization; Opinion propagation; Epidemiology; Evolutionary computing	DISTRIBUTIONS; DYNAMICS	This paper is about simulating the spread of opinions in a society and about finding ways to counteract that spread. To abstract away from potentially emotionally laden opinions, we instead simulate the spread of a zombie outbreak in a society. The virus causing this outbreak is different from traditional approaches: It not only causes a binary outcome (healthy vs. infected) but rather a continuous outcome. To counteract the outbreak, a discrete number of infection-level-specific treatments are available. This corresponds to acts of mild persuasion or the threats of legal action in the opinion spreading use case. This paper offers a genetic and a cultural algorithm that find the optimal mixture of treatments during the run of the simulation. They are assessed in a number of different scenarios. It is shown that albeit far from being perfect, the cultural algorithm delivers superior performance at lower computational expense.																	1432-7643	1433-7479				JAN	2020	24	1			SI		591	601		10.1007/s00500-019-04251-5													
J								Graph coloring: a novel heuristic based on trailing path-properties, perspective and applications in structured networks	SOFT COMPUTING										Chromatic number; Graph partitioning; NP to P; Motif identifier; Protein design	INTELLIGENCE OPTIMIZATION ALGORITHM; PROTEINS; COMPLEMENTARITY; SEARCH; MAP	Graph coloring is a manifestation of graph partitioning, wherein a graph is partitioned based on the adjacency of its elements. The fact that there is no general efficient solution to this problem that may work unequivocally for all graphs opens up the realistic scope for combinatorial optimization algorithms to be invoked. The algorithmic complexity of graph coloring is non-deterministic in polynomial time and hard. To the best of our knowledge, there is no algorithm as yet that procures an exact solution of the chromatic number comprehensively for any and all graphs within the polynomial (P) time domain. Here, we present a novel heuristic, namely the 'trailing path', which returns an approximate solution of the chromatic number within P time, and with a better accuracy than most existing algorithms. The 'trailing path' algorithm is effectively a subtle combination of the search patterns of two existing heuristics (DSATUR and largest first) and operates along a trailing path of consecutively connected nodes (and thereby effectively maps to the problem of finding spanning tree(s) of the graph) during the entire course of coloring, where essentially lies both the novelty and the apt of the current approach. The study also suggests that the judicious implementation of randomness is one of the keys toward rendering an improved accuracy in such combinatorial optimization algorithms. Apart from the algorithmic attributes, essential properties of graph partitioning in random and different structured networks have also been surveyed, followed by a comparative study. The study reveals the remarkable stability and absorptive property of chromatic number across a wide array of graphs. Finally, a case study is presented to demonstrate the potential use of graph coloring in protein design-yet another hard problem in structural and evolutionary biology.																	1432-7643	1433-7479				JAN	2020	24	1			SI		603	625		10.1007/s00500-019-04278-8													
J								Improvement in Hadoop performance using integrated feature extraction and machine learning algorithms	SOFT COMPUTING										Big Data; Hadoop system; MapReduce; Feature selection; Correlation-based feature selection (CFS); Mutual information (MI); AdaBoost and support vector machine (SVM)	CLASSIFIER	Big Data has been a term used in datasets which are complex and large in such a way there are some traditional technologies of data processing which are not adequate. Big Data can revolutionize most aspects in society such as collection or management of data from Big Data which is challenging and also very complex. The Hadoop has been designed for processing a large amount of unstructured and complex data. It has provided with a large amount of storage for data along with the ability to be able to tackle unlimited and concurrent tasks or jobs. The selection of features is an extremely powerful technique in the reduction of dimensionality and is also the most important step in machine learning applications. In recent decades, data is getting larger in a progressive manner in terms of instances and numbers making it very hard to deal with the problem of feature selection. In order to cope with such an epoch of Big Data, there are some more new techniques that are required to address the problem in a more efficient manner. At the same time, the suitability of the algorithms currently used may not be applicable especially when the size of data is above hundreds of gigabytes. For the purpose of this work, the correlation-based feature selection along with mutual information-based methods of feature selection was used for improving the performance. The AdaBoost and the support vector machine based classifiers have been used for improving their accuracy. The results of the experiment prove that the method proposed was able to achieve better performance compared to that of the other methods.																	1432-7643	1433-7479				JAN	2020	24	1			SI		627	636		10.1007/s00500-019-04453-x													
J								A new method for prediction of air pollution based on intelligent computation	SOFT COMPUTING										Air pollutants; Big data; Prediction; Analytical solution; Long short-term memory; Particle swarm algorithm; Intelligent computation	NEURAL-NETWORK; QUALITY; SYSTEM; OPTIMIZATION; PSO	The detection and treatment of increasing air pollution due to technological developments represent some of the most important challenges facing the world today. Indeed, there has been a significant increase in levels of environmental pollution in recent years. The aim of the work presented herein is to design an intelligent predictor for the concentrations of air pollutants over the next 2 days based on deep learning techniques using a recurrent neural network (RNN). The best structure for its operation is then determined using a particle swarm optimization (PSO) algorithm. The new predictor based on intelligent computation relying on unsupervised learning, i.e., long short-term memory (LSTM) and optimization (i.e., PSO), is called the smart air quality prediction model (SAQPM). The main goal is to predict six the concentrations of six types of air pollution, viz. PM2.5 particulate matter, PM10, particulate matter, nitrogen dioxide (NO2), carbon monoxide (CO), ozone (O-3), and sulfur dioxide (SO2). SAQPM consists of four stages. The first stage involves data collection from multiple stations (35 in this case). The second stage involves preprocessing of the data, including (a) separation of each station with an independent focus, (b) handle missing values, and (c) normalization of the dataset to the range of (0, 1) using the MinMaxScalar method. The third stage relates to building the predictor based on the LSTM method by identifying the best structure and parameter values (weight, bias, number of hidden layers, number of nodes in each hidden layer, and activation function) for the network using the functional PSO algorithm to achieve a goal. Thereafter, the dataset is split into training and testing parts based on the ten cross-validation principle. The training dataset is then used to build the predictor. In the fourth stage, evaluation results for each station are obtained by reading the concentration of each pollutant each hour for at most 30 days then taking the average of the symmetric mean absolute percentage error (SMAPE) for 25 days only.																	1432-7643	1433-7479				JAN	2020	24	1			SI		661	680		10.1007/s00500-019-04495-1													
J								Modified fuzzy TOPSIS plus TFNs ranking model for candidate selection using the qualifying criteria	SOFT COMPUTING										Election systems; Voting; Candidate selection; Fuzzy MCDM; Ranking triangular fuzzy numbers; Positional ranking	MCDM APPROACH	Currently, globalization process significantly impacts not only technological, economical, but also social, political and cultural fields. Ongoing social, economic and political processes demonstrate their impacts, and countries are governed by different regimes and government forms. From this standpoint, there is a need for qualified, competent staff for operation of the regimes and governments. In the article researches, which criteria or factors must be taken into account for selection of competent candidates that are suitable for relevant positions during the election process in contrast to traditional voting. Criteria for candidates' selection include adoption of democratic principles, age, education, government agency experience, professional competence, global culture and value acknowledgement, influence in voting area, leadership skills, activity in social media, etc. In the article implemented multi-criteria evaluation approach for candidate selection. Candidates are ranked based on criteria selected using modified fuzzy TOPSIS and triangular fuzzy numbers ranking methods and different aggregation operators. Candidates are ranked by applying both methods in a numeral experiment, and obtained results are compared. Proposed fuzzy multi-criteria decision-making model allows determining a compromise solution in candidate selection.																	1432-7643	1433-7479				JAN	2020	24	1			SI		681	695		10.1007/s00500-019-04521-2													
J								The O-PLS methodology for orthogonal signal correction-is it correcting or confusing?	JOURNAL OF CHEMOMETRICS										NAP; O-PLS; OSC; TP; PLS plus ST	REGRESSION; MODELS; OPLS	The separation of predictive and nonpredictive (or orthogonal) information in linear regression problems is considered to be an important issue in chemometrics. Approaches including net analyte preprocessing methods and various orthogonal signal correction (OSC) methods have been studied in a considerable number of publications. In the present paper, we focus on the simplest single response versions of some of the early OSC approaches including Fearns OSC, the orthogonal projections to latent structures, the target projection (TP), and the projections to latent structures (PLS) postprocessing by similarity transformation. These methods are claimed to yield improved model building and interpretation alternatives compared with ordinary PLS, by filtering "off" the response-orthogonal parts of the samples in a dataset. We point out at some fundamental misconceptions that were made in the justification of the PLS-related OSC algorithms and explain the key properties of the resulting modelling.																	0886-9383	1099-128X				JAN	2020	34	1							e2884	10.1002/cem.2884													
J								Multivariate patent analysis-Using chemometrics to analyze collections of chemical and pharmaceutical patents	JOURNAL OF CHEMOMETRICS										text analytics; OnPLS; principal component analysis; orthogonal projections to latent structures; feature engineering	LATENT; O2-PLS; ONPLS	Patents are an important source of technological knowledge, but the amount of existing patents is vast and quickly growing. This makes development of tools and methodologies for quickly revealing patterns in patent collections important. In this paper, we describe how structured chemometric principles of multivariate data analysis can be applied in the context of text analysis in a novel combination with common machine learning preprocessing methodologies. We demonstrate our methodology in 2 case studies. Using principal component analysis (PCA) on a collection of 12338 patent abstracts from 25 companies in big pharma revealed sub-fields which the companies are active in. Using PCA on a smaller collection of patents retrieved by searching for a specific term proved useful to quickly understand how patent classifications relate to the search term. By using orthogonal projections to latent structures (O-PLS) on patent classification schemes, we were able to separate patents on a more detailed level than using PCA. Lastly, we performed multi-block modeling using OnPLS on bag-of-words representations of abstracts, claims, and detailed descriptions, respectively, showing that semantic variation relating to patent classification is consistent across multiple text blocks, represented as globally joint variation. We conclude that using machine learning to transform unstructured data into structured data provide a good preprocessing tool for subsequent chemometric multivariate data analysis and provides an easily interpretable and novel workflow to understand large collections of patents. We demonstrate this on collections of chemical and pharmaceutical patents.																	0886-9383	1099-128X				JAN	2020	34	1							e3041	10.1002/cem.3041													
J								Visualization of descriptive multiblock analysis	JOURNAL OF CHEMOMETRICS										data fusion; descriptive analytics; multiblock analysis; OnPLS; visualization	ORTHOGONAL PROJECTIONS; INTEGRATED ANALYSIS; PLS-REGRESSION; CHEMOMETRICS; INFORMATION; COMMON; ONPLS; SCA	Understanding and making the most of complex data collected from multiple sources is a challenging task. Data integration is the procedure of describing the main features in multiple data blocks, and several methods for multiblock analysis have been previously developed, including OnPLS and JIVE. One of the main challenges is how to visualize and interpret the results of multiblock analyses because of the increased model complexity and sheer size of data. In this paper, we present novel visualization tools that simplify interpretation and overview of multiblock analysis. We introduce a correlation matrix plot that provides an overview of the relationships between blocks found by multiblock models. We also present a multiblock scatter plot, a metadata correlation plot, and a variation distribution plot, that simplify the interpretation of multiblock models. We demonstrate our visualizations on an industrial case study in vibration spectroscopy (NIR, UV, and Raman datasets) as well as a multiomics integration study (transcript, metabolite, and protein datasets). We conclude that our visualizations provide useful tools to harness the complexity of multiblock analysis and enable better understanding of the investigated system.																	0886-9383	1099-128X				JAN	2020	34	1							e3071	10.1002/cem.3071													
J								Exploring the latent variable space of PLS2 by post-transformation of the score matrix (ptLV)	JOURNAL OF CHEMOMETRICS										post-transformation of PLS2; predictive and non-predictive latent variables; Projection to Latent Structures regression	ORTHOGONAL SIGNAL CORRECTION; LEAST-SQUARES REGRESSION; PROJECTIONS; ALGORITHMS	Projection to Latent Structures (PLS) regression is largely applied in chemometrics. The most used algorithm for performing PLS is probably PLS2. PLS2 solves the problem of redundancy and collinearity in complex data sets and produces a small set of latent variables that can be used to investigate complex phenomena. However, the presence of specific cluster structures or trends in the data can drive PLS2 towards wrong directions and a redundant number of latent variables is generated. To overcome this unexpected behaviour, OSC-based methods were developed. The main idea was to use the concept of orthogonality to identified two different type of sources of structured variation which are modeled into two different subspaces: the non-predictive subspace described by latent variables orthogonal to the Y-response and the predictive subspace related to the Y-response. OSC-based methods work on the variable space producing suitable weight vectors to project the data. In this study, a new post-transformation method, called post-transformation of the Latent Variable space (ptLV), is introduced. The method generates a latent space isomorphic to that discovered by PLS2 where the non-predictive data variation is separated from the predictive one. It works on the score space and can be applied also to kernel-PLS2 (KPLS2). The relationships with post-transformation of PLS2 (ptPLS2) are investigated and a real and two simulated data sets are used to illustrate how ptLV works in practice.																	0886-9383	1099-128X				JAN	2020	34	1							e3079	10.1002/cem.3079													
J								Study of chemical compound spatial distribution in biodegradable active films using NIR hyperspectral imaging and multivariate curve resolution	JOURNAL OF CHEMOMETRICS										biodegradable film; cellulose acetate; near-infrared hyperspectral imaging; multivariate curve resolution; plasticizer	NEAR-INFRARED SPECTROSCOPY; NANOCOMPOSITE FILMS; CELLULOSE; PEDIOCIN; ACID	A study of spatial distribution of the four different plasticizers and sorbic acid incorporated in cellulose acetate biodegradable films using near-infrared hyperspectral imaging (NIR-HSI) and multivariate curve resolution-alternating least squares (MCR-ALS) is presented. A NIR-HSI was acquired for each film. MCR-ALS was applied to generate pure component distribution maps. A repeatability study was performed. The proposed method was able to recover the pure spectra of each film component accurately. The relative concentration vectors obtained by the MCR-ALS were rebuilt in matrices, and it was possible to analyze the homogeneity of the film constituents based on macropixel analysis and homogeneity index. The NIR-HSI imaging showed excellent repeatability. For the first time, a study detailing the distribution of chemical compounds incorporated into entire biodegradable films was possible by using NIR hyperspectral imaging combined with the MCR-ALS method.																	0886-9383	1099-128X				JAN	2020	34	1							e3193	10.1002/cem.3193													
J								A Volumetric Approach to Point Cloud Compression-Part I: Attribute Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Image coding; Geometry; Transforms; Octrees; Splines (mathematics); Hilbert space; Bezier volumes; B-splines; wavelets; point cloud compression; color coding; attribute coding; multiresolution representations; graph signal processing; counting measure	GRIDS	Compression of point clouds has so far been confined to coding the positions of a discrete set of points in space and the attributes of those discrete points. We introduce an alternative approach based on volumetric functions that are functions defined not just on a finite set of points but throughout space. As in regression analysis, volumetric functions are continuous functions that are able to interpolate values on a finite set of points as linear combinations of continuous basis functions. Using a B-spline wavelet basis, we are able to code volumetric functions representing both geometry and attributes. Geometry compression is addressed in Part II of this paper, while attribute compression is addressed in Part I. Attributes are represented by a volumetric function whose coefficients can be regarded as a critically sampled orthonormal transform that generalizes the recent successful Region-Adaptive Hierarchical (or Haar) Transform to higher orders. Experimental results show that attribute compression using higher order volumetric functions is an improvement over the first-order functions used in the emerging MPEG point cloud compression standard.																	1057-7149	1941-0042					2020	29						2203	2216		10.1109/TIP.2019.2908095													
J								A Volumetric Approach to Point Cloud Compression-Part II: Geometry Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Bezier volumes; B-splines; wavelets; point cloud compression; geometry coding; shape coding; multiresolution representations; signed distance function; graph signal processing		Compression of point clouds has so far been confined to coding the positions of a discrete set of points in space and the attributes of those discrete points. We introduce an alternative approach based on volumetric functions, which are functions defined not just on a finite set of points, but throughout space. As in regression analysis, volumetric functions are continuous functions that are able to interpolate values on a finite set of points as linear combinations of continuous basis functions. Using a B-spline wavelet basis, we are able to code volumetric functions representing both geometry and attributes. Attribute compression is addressed in Part I of this paper, while geometry compression is addressed in Part II. Geometry is represented implicitly as the level set of a volumetric function (the signed distance function or similar). Experimental results show that geometry compression using volumetric functions improves over the methods used in the emerging MPEG Point Cloud Compression (G-PCC) standard.																	1057-7149	1941-0042					2020	29						2217	2229		10.1109/TIP.2019.2957853													
J								Toward Intelligent Sensing: Intermediate Deep Feature Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Image coding; Task analysis; Feature extraction; Deep learning; Video coding; Standardization; Deep learning; intelligent front-end; feature compression		The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features with high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the transmitted intermediate layer features. We also present the results for evaluations of both lossless and lossy deep feature compression, which provide meaningful investigations and baselines for future research and standardization activities.																	1057-7149	1941-0042					2020	29						2230	2243		10.1109/TIP.2019.2941660													
J								Low-Rank Approximation via Generalized Reweighted Iterative Nuclear and Frobenius Norms	IEEE TRANSACTIONS ON IMAGE PROCESSING										Low-rank approximation problem; matrix completion (MC); robust principal component analysis (RPCA); image decomposition; generalized iterative reweighted nuclear norm (GIRNN); generalized iterative reweighted Frobenius norm (GIRFN)	CONVOLUTIONAL SPARSE; MATRIX COMPLETION; MINIMIZATION; ALGORITHMS	The low-rank approximation problem has recently attracted wide concern due to its excellent performance in real-world applications such as image restoration, traffic monitoring, and face recognition. Compared with the classic nuclear norm, the Schatten- p norm is stated to be a closer approximation to restrain the singular values for practical applications in the real world. However, Schatten- p norm minimization is a challenging non-convex, non-smooth, and non-Lipschitz problem. In this paper, inspired by the reweighted l(1) and l(2) norm for compressive sensing, the generalized iterative reweighted nuclear norm (GIRNN) and the generalized iterative reweighted Frobenius norm (GIRFN) algorithms are proposed to approximate Schatten- p norm minimization. By involving the proposed algorithms, the problem becomes more tractable and the closed solutions are derived from the iteratively reweighted subproblems. In addition, we prove that both proposed algorithms converge at a linear rate to a bounded optimum. Numerical experiments for the practical matrix completion (MC), robust principal component analysis (RPCA), and image decomposition problems are illustrated to validate the superior performance of both algorithms over some common state-of-the-art methods.																	1057-7149	1941-0042					2020	29						2244	2257		10.1109/TIP.2019.2949383													
J								Visual Saliency Detection via Kernelized Subspace Ranking With Active Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Saliency detection; active learning; subspace ranking; support vector machines; feature projection	OBJECT DETECTION; REGION DETECTION; DEEP; ATTENTION	Saliency detection task has witnessed a booming interest for years, due to the growth of the computer vision community. In this paper, we introduce a new saliency model that performs active learning with kernelized subspace ranker (KSR) referred to as KSR-AL. This pool-based active learning algorithm ranks the informativeness of unlabeled data by considering both uncertainty sampling and information density, thereby minimizing the cost of labeling. The informative images are selected to train the KSR iteratively and incrementally. The learning model of this algorithm is designed on object-level proposals and region-based convolutional neural network (R-CNN) features, by jointly learning a Rank-SVM classifier and a subspace projection. When the active learning process meets its stopping criteria, the saliency map of each image is generated by a weight fusion of its top-ranked proposals, whose ranking scores are graded by the learned ranker. We show that the KSR-AL achieves a reduction in annotation, as well as improvement in performance, compared with the supervised learning scheme. Besides, the proposed algorithm also outperforms the state-of-the-art methods. These improvements are demonstrated by extensive experiments on six publicly available benchmark datasets.																	1057-7149	1941-0042					2020	29						2258	2270		10.1109/TIP.2019.2945679													
J								2D Quaternion Sparse Discriminant Analysis	IEEE TRANSACTIONS ON IMAGE PROCESSING										Quaternions; Feature extraction; Dimensionality reduction; Training; Correlation; Covariance matrices; Linear discriminant analysis; 2D-QSDA; dimension reduction; sparse feature extraction; RGB image; RGB-D image	FOURIER-TRANSFORM; COLOR; MATRICES; REPRESENTATION; EIGENFACES; SELECTION; PCA	Linear discriminant analysis has been incorporated with various representations and measurements for dimension reduction and feature extraction. In this paper, we propose two-dimensional quaternion sparse discriminant analysis (2D-QSDA) that meets the requirements of representing RGB and RGB-D images. 2D-QSDA advances in three aspects: 1) including sparse regularization, 2D-QSDA relies only on the important variables, and thus shows good generalization ability to the out-of-sample data which are unseen during the training phase; 2) benefited from quaternion representation, 2D-QSDA well preserves the high order correlation among different image channels and provides a unified approach to extract features from RGB and RGB-D images; 3) the spatial structure of the input images is also retained via the matrix-based processing. We tackle the constrained trace ratio problem of 2D-QSDA by solving a corresponding constrained trace difference problem, which is then transformed into a quaternion sparse regression (QSR) model. Afterward, we reformulate the QSR model to an equivalent complex form to avoid the processing of the complicated structure of quaternions. A nested iterative algorithm is designed to learn the solution of 2D-QSDA in the complex space and then we convert this solution back to the quaternion domain. To improve the separability of 2D-QSDA, we further propose 2D-QSDA(w) using the weighted pairwise between-class distances. Extensive experiments on RGB and RGB-D databases demonstrate the effectiveness of 2D-QSDA and 2D-QSDA(w) compared with peer competitors.																	1057-7149	1941-0042					2020	29						2271	2286		10.1109/TIP.2019.2947775													
J								How is Gaze Influenced by Image Transformations? Dataset and Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Data models; Observers; Image resolution; Visualization; Mathematical model; Semantics; Robustness; Human gaze; saliency prediction; data augmentation; generative adversarial networks; model robustness	VISUAL-ATTENTION; SALIENCY	Data size is the bottleneck for developing deep saliency models, because collecting eye-movement data is very time-consuming and expensive. Most of current studies on human attention and saliency modeling have used high-quality stereotype stimuli. In real world, however, captured images undergo various types of transformations. Can we use these transformations to augment existing saliency datasets? Here, we first create a novel saliency dataset including fixations of 10 observers over 1900 images degraded by 19 types of transformations. Second, by analyzing eye movements, we find that observers look at different locations over transformed versus original images. Third, we utilize the new data over transformed images, called data augmentation transformation (DAT), to train deep saliency models. We find that label-preserving DATs with negligible impact on human gaze boost saliency prediction, whereas some other DATs that severely impact human gaze degrade the performance. These label-preserving valid augmentation transformations provide a solution to enlarge existing saliency datasets. Finally, we introduce a novel saliency model based on generative adversarial networks (dubbed GazeGAN). A modified U-Net is utilized as the generator of the GazeGAN, which combines classic "skip connection" with a novel "center-surround connection" (CSC) module. Our proposed CSC module mitigates trivial artifacts while emphasizing semantic salient regions, and increases model nonlinearity, thus demonstrating better robustness against transformations. Extensive experiments and comparisons indicate that GazeGAN achieves state-of-the-art performance over multiple datasets. We also provide a comprehensive comparison of 22 saliency models on various transformed scenes, which contributes a new robustness benchmark to saliency community. Our code and dataset are available at: https://github.com/CZHQuality/Sal-CFS-GAN.																	1057-7149	1941-0042					2020	29						2287	2300		10.1109/TIP.2019.2945857													
J								A Wave-Shaped Deep Neural Network for Smoke Density Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Estimation; Image segmentation; Feature extraction; Semantics; Image color analysis; Decoding; Visualization; Deep neural network; W-Net; smoke density estimation; smoke segmentation; smoke simulation	VIDEO; IMAGE; SEPARATION	Smoke density estimation from a single image is a totally new but highly ill-posed problem. To solve the problem, we stack several convolutional encoder-decoder structures together to propose a wave-shaped neural network, termed W-Net. Stacking encoder-decoders directly increases the network depth, leading to the enlargement of receptive fields for encoding more semantic information. To maximize the degrees of feature re-usage, we copy and resize the outputs of encoding layers to corresponding decoding layers, and then concatenate them to implement short-cut connections for improving spatial accuracy. The crests and troughs of W-Net are special structures containing abundant localization and semantic information, so we also use short-cut connections between these structures and decoding layers. Estimated smoke density is useful in many applications, such as smoke segmentation, smoke detection, disaster simulation. Experimental results show that our method outperforms existing methods on both smoke density estimation and segmentation. It also achieves satisfying results in visual detection of auto exhausts.																	1057-7149	1941-0042					2020	29						2301	2313		10.1109/TIP.2019.2946126													
J								Robust Low-Rank Tensor Minimization via a New Tensor Spectral k-Support Norm	IEEE TRANSACTIONS ON IMAGE PROCESSING										Robust low-rank tensor minimization; tensor robust principal component analysis; tensor singular value decomposition (t-SVD); alternating direction method of multipliers; proximal algorithm; conditional gradient descent	ALGORITHMS; FRAMEWORK	Recently, based on a new tensor algebraic framework for third-order tensors, the tensor singular value decomposition (t-SVD) and its associated tubal rank definition have shed new light on low-rank tensor modeling. Its applications to robust image/video recovery and background modeling show promising performance due to its superior capability in modeling cross-channel/frame information. Under the t-SVD framework, we propose a new tensor norm called tensor spectral k-support norm (TSP-k) by an alternative convex relaxation. As an interpolation between the existing tensor nuclear norm (TNN) and tensor Frobenius norm (TFN), it is able to simultaneously drive minor singular values to zero to induce low-rankness, and to capture more global information for better preserving intrinsic structure. We provide the proximal operator and the polar operator for the TSP-k norm as key optimization blocks, along with two showcase optimization algorithms for medium- and large-size tensors. Experiments on synthetic, image and video datasets in medium and large sizes, all verify the superiority of the TSP-k norm and the effectiveness of both optimization methods in comparison with the existing counterparts.																	1057-7149	1941-0042					2020	29						2314	2327		10.1109/TIP.2019.2946445													
J								Low Cost Gaze Estimation: Knowledge-Based Solutions	IEEE TRANSACTIONS ON IMAGE PROCESSING										Estimation; Head; Image resolution; Gaze tracking; Databases; Cameras; Gaze estimation methods; low resolution; eye tracking	TRACKING; POSE	Eye tracking technology in low resolution scenarios is not a completely solved issue to date. The possibility of using eye tracking in a mobile gadget is a challenging objective that would permit to spread this technology to non-explored fields. In this paper, a knowledge based approach is presented to solve gaze estimation in low resolution settings. The understanding of the high resolution paradigm permits to propose alternative models to solve gaze estimation. In this manner, three models are presented: a geometrical model, an interpolation model and a compound model, as solutions for gaze estimation for remote low resolution systems. Since this work considers head position essential to improve gaze accuracy, a method for head pose estimation is also proposed. The methods are validated in an optimal framework, I2Head database, which combines head and gaze data. The experimental validation of the models demonstrates their sensitivity to image processing inaccuracies, critical in the case of the geometrical model. Static and extreme movement scenarios are analyzed showing the higher robustness of compound and geometrical models in the presence of user's displacement. Accuracy values of about 3 degrees have been obtained, increasing to values close to 5 degrees in extreme displacement settings, results fully comparable with the state-of-the-art.																	1057-7149	1941-0042					2020	29						2328	2343		10.1109/TIP.2019.2946452													
J								Deep Portrait Image Completion and Extrapolation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face; Extrapolation; Pose estimation; Deep learning; Semantics; Heating systems; Three-dimensional displays; Image completion; portrait extrapolation; human parsing; deep learning		General image completion and extrapolation methods often fail on portrait images where parts of the human body need to be recovered - a task that requires accurate human body structure and appearance synthesis. We present a two-stage deep learning framework for tackling this problem. In the first stage, given a portrait image with an incomplete human body, we extract a complete, coherent human body structure through a human parsing network, which focuses on structure recovery inside the unknown region with the help of full-body pose estimation. In the second stage, we use an image completion network to fill the unknown region, guided by the structure map recovered in the first stage. For realistic synthesis the completion network is trained with both perceptual loss and conditional adversarial loss. We further propose a face refinement network to improve the fidelity of the synthesized face region. We evaluate our method on publicly-available portrait image datasets, and show that it outperforms other state-of-the-art general image completion methods. Our method enables new portrait image editing applications such as occlusion removal and portrait extrapolation. We further show that the proposed general learning framework can be applied to other types of images, e.g. animal images.																	1057-7149	1941-0042					2020	29						2344	2355		10.1109/TIP.2019.2945866													
J								Local-Adaptive Image Alignment Based on Triangular Facet Approximation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Cameras; Data models; Distortion; Strain; Technological innovation; Computational efficiency; Computer vision; image stitching; image alignment; parallax handling; triangular meshing	VIEW	Accurate and efficient image alignment is the core problem in the research of panoramic stitching nowadays. This paper proposes a local-adaptive image alignment method based on triangular facet approximation, which directly manipulates the matching data in the camera coordinates, and therefore rises superior to the imaging model of cameras. A more robust planar transformation model is proposed and extended to be local-adaptive via combining it with two weighting strategies. By approximating the scene as a combination of adjacent triangular facets, the planar and spherical triangulation strategies are introduced to more efficiently align normal and fisheye images respectively. The efficiency of the proposed method are verified through the comparative experiments on several challenging cases both qualitatively and quantitatively.																	1057-7149	1941-0042					2020	29						2356	2369		10.1109/TIP.2019.2949424													
J								Repeated Look-Up Tables	IEEE TRANSACTIONS ON IMAGE PROCESSING										Table lookup; Optimization; Interpolation; Dynamic range; Hardware; Decoding; Standards; Look-up tables		Efficient hardware implementations routinely approximate mathematical functions with look-up tables, while keeping the error of the approximation under control. For a certain class of commonly occurring 1D functions, namely monotonically increasing or decreasing functions, we found that it is possible to approximate such functions by repeated application of a very low resolution 1D look-up table. There are many advantages to cascading multiple identical LUTs, including the promise of a very simple hardware design and the use of standard linear interpolation. Further, the complexity associated with unequal bin sizes can be avoided. We show that for realistic applications, including gamma correction, high dynamic range encoding and decoding curves, as well as tone mapping and inverse tone mapping applications, multiple cascaded look-up tables can reduce the approximation error by more than 50% compared to a single look-up table with the same total memory footprint.																	1057-7149	1941-0042					2020	29						2370	2379		10.1109/TIP.2019.2949245													
J								Deep Active Shape Model for Robust Object Fitting	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; convolutional neural networks; generalized expectation-maximization; HOG; SIFT; active shape model	SEGMENTATION	Object recognition and localization is still a very challenging problem, despite recent advances in deep learning (DL) approaches, especially for objects with varying shapes and appearances. Statistical models, such as an Active Shape Model (ASM), rely on a parametric model of the object, allowing an easy incorporation of prior knowledge about shape and appearance in a principled way. To take advantage of these benefits, this paper proposes a new ASM framework that addresses two tasks: (i) comparing the performance of several image features used to extract observations from an input image; and (ii) improving the performance of the model fitting by relying on a probabilistic framework that allows the use of multiple observations and is robust to the presence of outliers. The goal in (i) is to maximize the quality of the observations by exploring a wide set of handcrafted features (HOG, SIFT, and texture templates) and more recent DL-based features. Regarding (ii), we use the Generalized Expectation-Maximization algorithm to deal with outliers and to extend the fitting process to multiple observations. The proposed framework is evaluated in the context of facial landmark fitting and the segmentation of the endocardium of the left ventricle in cardiac magnetic resonance volumes. We experimentally observe that the proposed approach is robust not only to outliers, but also to adverse initialization conditions and to large search regions (from where the observations are extracted from the image). Furthermore, the results of the proposed combination of the ASM with DL-based features are competitive with more recent DL approaches (e.g. FCN [1], U-Net [2] and CNN Cascade [3]), showing that it is possible to combine the benefits of statistical models and DL into a new deep ASM probabilistic framework.																	1057-7149	1941-0042					2020	29						2380	2394		10.1109/TIP.2019.2948728													
J								BMAN: Bidirectional Multi-Scale Aggregation Networks for Abnormal Event Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Event detection; Feature extraction; Encoding; Detectors; Task analysis; Heuristic algorithms; Deep learning; Video analysis; abnormal event detection; normal pattern encoding; multi-scale	VIDEO ANOMALY DETECTION; NEURAL-NETWORKS; LOCALIZATION; HISTOGRAMS	Abnormal event detection is an important task in video surveillance systems. In this paper, we propose novel bidirectional multi-scale aggregation networks (BMAN) for abnormal event detection. The proposed BMAN learns spatio-temporal patterns of normal events to detect deviations from the learned normal patterns as abnormalities. The BMAN consists of two main parts: an inter-frame predictor and an appearance-motion joint detector. The inter-frame predictor is devised to encode normal patterns, which generates an inter-frame using bidirectional multi-scale aggregation based on attention. With the feature aggregation, robustness for object scale variations and complex motions is achieved in normal pattern encoding. Based on the encoded normal patterns, abnormal events are detected by the appearance-motion joint detector in which both appearance and motion characteristics of scenes are considered. Comprehensive experiments are performed, and the results show that the proposed method outperforms the existing state-of-the-art methods. The resulting abnormal event detection is interpretable on the visual basis of where the detected events occur. Further, we validate the effectiveness of the proposed network designs by conducting ablation study and feature visualization.																	1057-7149	1941-0042					2020	29						2395	2408		10.1109/TIP.2019.2948286													
J								RhythmNet: End-to-End Heart Rate Estimation From Face via Spatial-Temporal Representation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Heart rate; Estimation; Webcams; Databases; Skin; Image color analysis; Head; Remote heart rate estimation; rPPG; spatial-temporal representation; end-to-end learning	REMOTE-PPG; NONCONTACT	Heart rate (HR) is an important physiological signal that reflects the physical and emotional status of a person. Traditional HR measurements usually rely on contact monitors, which may cause inconvenience and discomfort. Recently, some methods have been proposed for remote HR estimation from face videos; however, most of them focus on well-controlled scenarios, their generalization ability into less-constrained scenarios (e.g., with head movement, and bad illumination) are not known. At the same time, lacking large-scale HR databases has limited the use of deep models for remote HR estimation. In this paper, we propose an end-to-end RhythmNet for remote HR estimation from the face. In RyhthmNet, we use a spatial-temporal representation encoding the HR signals from multiple ROI volumes as its input. Then the spatial-temporal representations are fed into a convolutional network for HR estimation. We also take into account the relationship of adjacent HR measurements from a video sequence via Gated Recurrent Unit (GRU) and achieves efficient HR measurement. In addition, we build a large-scale multi-modal HR database (named as VIPL-HR (1) ), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database contains various variations such as head movements, illumination variations, and acquisition device changes, replicating a less-constrained scenario for HR estimation. The proposed approach outperforms the state-of-the-art methods on both the public-domain and our VIPL-HR databases. (1) VIPL-HR is available at: http://vipl.ict.ac.cn/view_database.php?id=15																	1057-7149	1941-0042					2020	29						2409	2423		10.1109/TIP.2019.2947204													
J								Class-Specific Reconstruction Transfer Learning for Visual Recognition Across Domains	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reconstruction; Adaptation models; Machine learning; Data models; Correlation; Semantics; Learning systems; Transfer learning; cross-domain learning; semi-supervised learning; image classification	ADAPTATION; CLASSIFICATION; ALGORITHM; KERNEL; IMAGE	Subspace learning and reconstruction have been widely explored in recent transfer learning work. Generally, a specially designed projection and reconstruction transfer functions bridging multiple domains for heterogeneous knowledge sharing are wanted. However, we argue that the existing subspace reconstruction based domain adaptation algorithms neglect the class prior, such that the learned transfer function is biased, especially when data scarcity of some class is encountered. Different from those previous methods, in this article, we propose a novel class-wise reconstruction-based adaptation method called Class-specific Reconstruction Transfer Learning (CRTL), which optimizes a well modeled transfer loss function by fully exploiting intra-class dependency and inter-class independency. The merits of the CRTL are three-fold. 1) Using a class-specific reconstruction matrix to align the source domain with the target domain fully exploits the class prior in modeling the domain distribution consistency, which benefits the cross-domain classification. 2) Furthermore, to keep the intrinsic relationship between data and labels after feature augmentation, a projected Hilbert-Schmidt Independence Criterion (pHSIC), that measures the dependency between data and label, is first proposed in transfer learning community by mapping the data from raw space to RKHS. 3) In addition, by imposing low-rank and sparse constraints on the class-specific reconstruction coefficient matrix, the global and local data structure that contributes to domain correlation can be effectively preserved. Extensive experiments on challenging benchmark datasets demonstrate the superiority of the proposed method over state-of-the-art representation-based domain adaptation methods. The demo code is available in https://github.com/wangshanshanCQU/CRTL.																	1057-7149	1941-0042					2020	29						2424	2438		10.1109/TIP.2019.2948480													
J								Graph-Based Compensated Wavelet Lifting for Scalable Lossless Coding of Dynamic Medical Data	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Biomedical imaging; Computed tomography; Transforms; Encoding; Motion compensation; Scalability; Scalability; discrete wavelet transform; motion compensation; graph-based signal processing; computed tomography; magnetic resonance imaging	COMPRESSION	Lossless compression of dynamic 2D & x002B;t and 3D & x002B;t medical data is challenging regarding the huge amount of data, the characteristics of the inherent noise, and the high bit depth. Beyond that, a scalable representation is often required in telemedicine applications. Motion Compensated Temporal Filtering works well for lossless compression of medical volume data and additionally provides temporal, spatial, and quality scalability features. To achieve a high quality lowpass subband, which shall be used as a downscaled representative of the original data, graph-based motion compensation was recently introduced to this framework. However, encoding the motion information, which is stored in adjacency matrices, is not well investigated so far. This work focuses on coding these adjacency matrices to make the graph-based motion compensation feasible for data compression. We propose a novel coding scheme based on constructing so-called motion maps. This allows for the first time to compare the performance of graph-based motion compensation to traditional block- and mesh-based approaches. For high quality lowpass subbands our method is able to outperform the block- and mesh-based approaches by increasing the visual quality in terms of PSNR by 0.53dB and 0.28dB for CT data, as well as 1.04dB and 1.90dB for MR data, respectively, while the bit rate is reduced at the same time.																	1057-7149	1941-0042					2020	29						2439	2451		10.1109/TIP.2019.2947138													
J								Reconstruction of Binary Shapes From Blurred Images via Hankel-Structured Low-Rank Matrix Recovery	IEEE TRANSACTIONS ON IMAGE PROCESSING										Binary shape; Hankel structure; low-rank matrix recovery	PIECEWISE-CONSTANT IMAGES; FINITE RATE; ALGORITHM; SUPERRESOLUTION; SIGNALS	With the dominance of digital imaging systems, we are often dealing with discrete-domain samples of an analog image. Due to physical limitations, all imaging devices apply a blurring kernel on the input image before taking samples to form the output pixels. In this paper, we focus on the reconstruction of binary shape images from few blurred samples. This problem has applications in medical imaging, shape processing, and image segmentation. Our method relies on representing the analog shape image in a discrete grid much finer than the sampling grid. We formulate the problem as the recovery of a rank r matrix that is formed by a Hankel structure on the pixels. We further propose efficient ADMM-based algorithms to recover the low-rank matrix in both noiseless and noisy settings. We also analytically investigate the number of required samples for successful recovery in the noiseless case. For this purpose, we study the problem in the random sampling framework, and show that with mathcal random samples (where the size of the image is assumed to be timesn n(1) x n(2)) we can guarantee the perfect reconstruction with high probability under mild conditions. We further prove the robustness of the proposed recovery in the noisy setting by showing that the reconstruction error in the noisy case is bounded when the input noise is bounded. Simulation results confirm that our proposed method outperform the conventional total variation minimization in the noiseless settings.																	1057-7149	1941-0042					2020	29						2452	2462		10.1109/TIP.2019.2950512													
J								Tensor Multi-Task Learning for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Cameras; Task analysis; Measurement; Visualization; Training; Computational modeling; Person re-identification; multi-task learning; tensor optimization		This article presents a tensor multi-task model for person re-identification (Re-ID). Due to discrepancy among cameras, our approach regards Re-ID from multiple cameras as different but related classification tasks, each task corresponding to a specific camera. In each task, we distinguish the person identity as a one-vs-all linear classification problem, where one classifier is associated with a specific person. By constructing all classifiers into a task-specific projection matrix, the proposed method could utilize all the matrices to form a tensor structure, and jointly train all the tasks in a uniform tensor space. In this space, by assuming the features of the same person under different cameras are generated from a latent subspace, and different identities under the same perspective share similar patterns, the high-order correlations, not only across different tasks but also within a certain task, can be captured by utilizing a new type of low-rank tensor constraint. Therefore, the learned classifiers transform the original feature vector into the latent space, where feature distributions across cameras can be well-aligned. Moreover, this model can be incorporated into multiple visual features to boost the performance, and easily extended to the unsupervised setting. Extensive experiments and comparisons with recent Re-ID methods manifest the competitive performance of our method.																	1057-7149	1941-0042					2020	29						2463	2477		10.1109/TIP.2019.2949929													
J								Tackling the Multi-Objective Vehicle Routing Problem With Uncertain Demands	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Local Search; Multi-Objective; Optimization; Uncertainty; Vehicle Routing	EVOLUTIONARY ALGORITHM; OPTIMIZATION	In this article, the single capacitated vehicle routing problem with time windows and uncertain demands is studied. Having a set of customers whose actual demand is not known in advance, needs to be serviced. The goal of the problem is to find a set of routes with the lowest total travel distance and tardiness time, subject to vehicle capacity and time window constraints. Two uncertainty types can be distinguished in the literature: random and epistemic uncertainties. Because several studies focalized upon the random aspect of uncertainty, the article proposes to tackle the problem by considering dominance relations to handle epistemic uncertainty in the objective functions. Further, an epistemic multi-objective local search-based approach is proposed for studying the behavior of such a representation of demands on benchmark instances generated following a standard generator available in the literature. Finally, the results achieved by the proposed method using epistemic representation are compared to those reached by a deterministic version. Encouraging results have been obtained.																	1947-8283	1947-8291				JAN-MAR	2020	11	1			SI		1	22		10.4018/IJAMC.2020010101													
J								An Efficient VNS Algorithm to Solve the Multi-Attribute Technician Routing and Scheduling Problem	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Adaptive Memory; Advanced Diversity Management; Biased Fitness; Multi-Attribute; Technician Routing and Scheduling Problem; VNS	SEARCH; MAINTENANCE	This article addresses a technician routing and scheduling problem inspired from an application for the repair of electronic transactions equipment. It consists of designing routes for staff to perform requests while considering certain constraints and resources. The objective is to minimize a linear combination of total weighted distance, overtime, and maximize the served requests. An efficient meta-heuristic algorithm based on variable neighborhood search with an adaptive memory and advanced diversity management method is proposed. Numerical results show that the meta-heuristic outperforms the best existing algorithm from the literature which is a Tabu Search.																	1947-8283	1947-8291				JAN-MAR	2020	11	1			SI		23	35		10.4018/IJAMC.2020010102													
J								Automatic Circuit Design of CMOS Miller OTA Using Cuckoo Search Algorithm	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Circuit Simulator; CMOS Miller OTA; CS Algorithm; Fitness Function; Optimization; Simulation	OPTIMIZATION; COLONY	The circuit design of the CMOS based analog part of a mixed-signal integrated circuit (IC) needs a large fraction of the overall design cycle time. The automatic design of an analog circuit is inevitable, seeing recently development of System-on-Chip (SOC) design. This brings about the need to develop computer aided design (CAD) tools for automatic design of CMOS based analog circuits. In this article, a Cuckoo Search (CS) algorithm is presented for automatic design of a CMOS Miller Operational Transconductance Amplifier (OTA). The source code of the CS algorithm is developed using the C language. The Ngspice circuit simulator has been used as a fitness function creator and evaluator. A script file is written to provide an interface between the CS algorithm and the Ngspice simulator. BSIM3v3 MOSFET models with 0.18 mu m and 0.35 mu m CMOS technology have been used to simulate this circuit. The simulation results of this work are presented and compared with previous works reported in the literature. The experimental simulation results obtained by the CS algorithm satisfy all desired specifications for this circuit.																	1947-8283	1947-8291				JAN-MAR	2020	11	1			SI		36	44		10.4018/IJAMC.2020010103													
J								An Ultra-Fast Method for Clustering of Big Genomic Data	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Bioinformatics; Cancer; Clustering Process; DNA Methylation; Microarray Gene Expression; Running Time	GENE-EXPRESSION	The clustering process is used to identify cancer subtypes based on gene expression and DNA methylation datasets, since cancer subtype information is critically important for understanding tumor heterogeneity, detecting previously unknown clusters of biological samples, which are usually associated with unknown types of cancer will, in turn, gives way to prescribe more effective treatments for patients. This is because cancer has varying subtypes which often respond disparately to the same treatment. While the DNA methylation database is extremely large-scale datasets, running time still remains a major challenge. Actually, traditional clustering algorithms are too slow to handle biological high-dimensional datasets, they usually require large amounts of computational time. The proposed clustering algorithm extraordinarily overcomes all others in terms of running time, it is able to rapidly identify a set of biologically relevant clusters in large-scale DNA methylation datasets, its superiority over the others has been demonstrated regarding its relative speed.																	1947-8283	1947-8291				JAN-MAR	2020	11	1			SI		45	60		10.4018/IJAMC.2020010104													
J								Application of Hybrid Petri Nets for the Energy Dispatching of an Isolated Micro-Grid	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Energy Dispatching; House Energy State; Hybrid Petri Net; Micro-Grid; Provider		Renewable energy-based autonomous installations sometimes have an energy deficiency or excess caused by the intermittence of renewable sources and the dynamics of the load. Storage systems are unable to cover load supply during long weather instability. In case of neighboring autonomous installations, some have a lack of energy while neighbors have overproduction. Hence, interconnecting installations via a micro-grid (MG) should allow supply installations experiencing an energy lack by sending them energy surplus from others. This article presents a Hybrid Petri Net (HPN) strategy for a micro-grid energy provider in order to make hourly decisions on dispatching energy between the connected installations. An HPN model combines discrete events (house energy state) and continuous events (energy flow) to cover the need of some installations by energy surplus offered by its neighbours. The algorithm has been validated for three connected houses with different load profiles. Results show that installations cover each other in energy without the need to refer to their batteries in case of a lack of energy.																	1947-8283	1947-8291				JAN-MAR	2020	11	1			SI		61	72		10.4018/IJAMC.2020010105													
J								Deep Convolutional Neural Networks for Customer Churn Prediction Analysis	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Churn Prediction; ConvNets; DCNN; Deep Learning; Machine Learning; Telecommunications	INDUSTRY; MACHINE; SET	Several machine learning models have been proposed to address customer churn problems. In this work, the authors used a novel method by applying deep convolutional neural networks on a labeled dataset of 18,000 prepaid subscribers to classify/identify customer churn. The learning technique was based on call detail records (CDR) describing customers activity during two-month traffic from a real telecommunication provider. The authors use this method to identify new business use case by considering each subscriber as a single input image describing the churning state. Different experiments were performed to evaluate the performance of the method. The authors found that deep convolutional neural networks (DCNN) outperformed other traditional machine learning algorithms (support vector machines, random forest, and gradient boosting classifier) with F1 score of 91%. Thus, the use of this approach can reduce the cost related to customer loss and fits better the churn prediction business use case.																	1557-3958	1557-3966				JAN-MAR	2020	14	1					1	16		10.4018/IJCINI.2020010101													
J								Detecting DDoS Attacks Using Polyscale Analysis and Deep Learning	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Anomaly Detection Method; Convolutional Neural Network; Cyber and Physical Network Security; Discrete Wavelet Transform; Distributed Denial of Service Attack; Variance Fractal Dimension		Distributed denial-of-service (DDoS) attacks are serious threats to the availability of a smart grid infrastructure services because they can cause massive blackouts. This study describes an anomaly detection method for improving the detection rate of a DDoS attack in a smart grid. This improvement was achieved by increasing the classification of the training and testing phases in a convolutional neural network (CNN). A full version of the variance fractal dimension trajectory (VFDTv2) was used to extract inherent features from the stochastic fractal input data. A discrete wavelet transform (DWT) was applied to the input data and the VFDTv2 to extract significant distinguishing features during data pre-processing. A support vector machine (SVM) was used for data post-processing. The implementation detected the DDoS attack with 87.35% accuracy.																	1557-3958	1557-3966				JAN-MAR	2020	14	1					17	34		10.4018/IJCINI.2020010102													
J								Distributional Semantic Model Based on Convolutional Neural Network for Arabic Textual Similarity	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Arabic Language; Context Based Approach; Global Vectors Representation; Natural Language Processing; Paraphrase Detection; Semantic Similarity; Word Embedding; Word2vec		The problem addressed is to develop a model that can reliably identify whether a previously unseen document pair is paraphrased or not. Its detection in Arabic documents is a challenge because of its variability in features and the lack of publicly available corpora. Faced with these problems, the authors propose a semantic approach. At the feature extraction level, the authors use global vectors representation combining global co-occurrence counting and a contextual skip gram model. At the paraphrase identification level, the authors apply a convolutional neural network model to learn more contextual and semantic information between documents. For experiments, the authors use Open Source Arabic Corpora as a source corpus. Then the authors collect different datasets to create a vocabulary model. For the paraphrased corpus construction, the authors replace each word from the source corpus by its most similar one which has the same grammatical class applying the word2vec algorithm and the part-of-speech annotation. Experiments show that the model achieves promising results in terms of precision and recall compared to existing approaches in the literature.																	1557-3958	1557-3966				JAN-MAR	2020	14	1					35	50		10.4018/IJCINI.2020010103													
J								Generalized Ordered Weighted Simplified Neutrosophic Cosine Similarity Measure for Multiple Attribute Group Decision Making	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Cosine Similarity Measure; Generalized Ordered Weighted Simplified Neutrosophic Cosine Similarity (GOWSNCS) Measure; Group Decision Making; Simplified Neutrosophic Set	AGGREGATION OPERATORS; SETS	The paper proposes a generalized ordered weighted simplified neutrosophic cosine similarity (GOWSNCS) measure by combining the cosine similarity measure of simplified neutrosophic sets (SNSs) with the generalized ordered weighted averaging (GOWA) operator and investigates its properties and special cases. Then, the author develops a simplified neutrosophic group decision-making method based on the GOWSNCS measure to handle multiple attribute group decision-making problems with simplified neutrosophic information. The prominent characteristics of the GOWSNCS measure are that it not only is a generalization of the cosine similarity measure but also considers the associated weights for attributes and decision makers in the aggregation of the cosine similarity measures of SNSs to alleviate the influence of unduly large or small similarities in the process of information aggregation. Finally, an illustrative example of investment alternatives is provided to demonstrate the application and effectiveness of the developed approach.																	1557-3958	1557-3966				JAN-MAR	2020	14	1					51	62		10.4018/IJCINI.2020010104													
J								Moving Target Detection and Tracking Based on Improved FCM Algorithm	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Genetic Algorithm; Improved FCM; Kalman Filter; Moving Target Detection and Tracking		With the rapid development of computer intelligence technology, the majority of scholars have a great interest in the detection and tracking of moving targets in the field of video surveillance and have been involved in its research. Moving target detection and tracking has also been widely used in military, industrial control, and intelligent transportation. With the rapid progress of the social economy, the supervision of traffic has become more and more complicated. How to detect the vehicles on the road in real time, monitor the illegal vehicles, and control the illegal vehicles effectively has become a hot issue. In view of the complex situation of moving vehicles in various traffic videos, the authors propose an improved algorithm for effective detection and tracking of moving vehicles, namely improved FCM algorithm. It combines traditional FCM algorithm with genetic algorithm and Kalman filter algorithm to track and detect moving targets. Experiments show that this improved clustering algorithm has certain advantages over other clustering algorithms.																	1557-3958	1557-3966				JAN-MAR	2020	14	1					63	74		10.4018/IJCINI.2020010105													
J								A System Dynamics Model for Sales and Operations Planning: An Integrated Analysis for the Lime Industry	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Dynamic Modeling and Simulation; Financial Management; Integrated Business; Management Process; Scenario Analysis		The use of system dynamics techniques to model the sales and operations planning (S&OP), associated with the economic and financial processes, is an innovative proposal. The objectives of this article are to model and simulate the S&OP process integrated with the financial management in a Brazilian lime processing industry, based on the system dynamics approach. Initially, the model was validated. Then, over twenty scenarios were simulated to assess the behavior of the system with its key factors variation. In the microenvironment scenarios, the company's internal perspective was the only element taken into account. In turn, regarding the macro environment scenarios, the basis was the projection of lime consumption related to the country's GDP. The results have genuinely contributed to the industry researched, since the lime processing industry is struggling with obtaining enough supply due to lime acquisition price fluctuations and, consequently, the oscillation of its production costs.																	2160-9772	2160-9799				JAN-MAR	2020	9	1					1	17		10.4018/IJSDA.2020010101													
J								An Analysis of a Wind Turbine-Generator System in the Presence of Stochasticity and Fokker-Planck Equations	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Differential Equations (SDEs); Fokker-Planck Technique; Small Scale Turbulence; Stochastic; Wind Turbine-Generator System	NONLINEAR-SYSTEMS; SPEED; POWER; SIMULATION; STABILITY; MODELS; FLUID	In power systems dynamics and control literature, theoretical and practical aspects of the wind turbine-generator system have received considerable attentions. The evolution equation of the induction machine encompasses a system of three first-order differential equations coupled with two algebraic equations. After accounting for stochasticity in the wind speed, the wind turbine-generator system becomes a stochastic system. That is described by the standard and formal Ito stochastic differential equation. Note that the Ito process is a strong Markov process. The Ito stochasticity of the wind speed is attributed to the Markov modeling of atmospheric turbulence. The article utilizes the Fokker-Planck method, a mathematical stochastic method, to analyse the noise-influenced wind turbine-generator system by doing the following: (i) the authors develop the Fokker-Planck model for the stochastic power system problem considered here; (ii) the Fokker-Planck operator coupled with the Kolmogorov backward operator are exploited to accomplish the noise analysis from the estimation-theoretic viewpoint.																	2160-9772	2160-9799				JAN-MAR	2020	9	1					18	43		10.4018/IJSDA.2020010102													
J								Measurement System Analysis and System Thinking in Six Sigma: How They Relate and How to Use Them	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Improvement Initiatives; Measurement System Analysis; Six Sigma; System Thinking	PROJECT-MANAGEMENT; RISK-MANAGEMENT; DYNAMICS MODEL; CONTINUOUS IMPROVEMENT; PRODUCT DEVELOPMENT; FRAMEWORK; CULTURE; IMPLEMENTATION; INITIATIVES; PERFORMANCE	This article investigated measurement system analysis and system thinking in Six Sigma, as well as the factors that influence these actions. If the measurement system being used to accumulate data from the process delivers dependable and accurate results, the measurement system analysis regulates it. Process improvement initiatives can be derailed by faulty measurement systems. Also, managers who have read faulty data can be misled into making wrong decisions. To collect trustworthy data, a reliable measurement system is established with this process. A method to assess an organization as a system and interpret its practices as a whole with Six Sigma is system thinking. Also, fixing a system as a whole helps to identify the real causes of issues and to know where to address them. This article addressed the contribution of these two methods to an overall success of an organization operating Six Sigma. The most current variables, concepts, and models were studied within operations and project management. By using a design-science-investigate strategy, this study approved of a valuable growth reveal for reasonable and hypothetical application. This study allowed us to generate a fitting assessment model that will fill the research void. Also, this study contributed to the engineering field with improved project success rates and team communication.																	2160-9772	2160-9799				JAN-MAR	2020	9	1					44	62		10.4018/IJSDA.2020010103													
J								Cloud-Based Access Control Framework for Effective Role Provisioning in Business Application	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Aggregate Zero-Knowledge Proof Knowledge; Information Sharing; Oblivious Commitment-Based Envelope; Policy Enforcement Point; Role Based Access Control; Security		In the evolution of social networks and big data, secure information sharing is a crucial task. When information is shared between the user and the organization admin, security plays a key role in any business organization in terms of privacy. Though many fruitful solutions prevail to protect the data integrity and privacy, there is a huge space for novel data protection schemes where a large set of data are involved. In this article, the Cloud-Based Access Control (C-BAC) framework is proposed which can fit in any business organization application. In this C-BAC, Policy Enforcement Point (PEP) is used to avoid unwanted information sharing with the neighboring employee. C-BAC framework with RSA provides security, based on the number of employees with the data handled by the particular employee, better than the existing access control framework with asymmetric encryption standard (AES) and Rivest-Shamir-Adleman (RSA) in terms of individual information handling.																	2160-9772	2160-9799				JAN-MAR	2020	9	1					63	80		10.4018/IJSDA.2020010104													
J								Metric Dimension of Generalized Mobius Ladder and its Application to WSN Localization	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										wireless sensor network localization; metric dimension; resolving set; generalized Mobius ladder	WIRELESS SENSOR NETWORKS; GRAPHS; DEPLOYMENT; LOCATION	Localization is one of the key techniques in wireless sensor network. While the global positioning system (GPS) is one of the most popular positioning technologies, the weakness of high cost and energy consuming makes it difficult to install in every node. In order to reduce the cost and energy consumption only a few nodes, called beacon nodes, are equipped with GPS modules. The remaining nodes obtain their locations through localization. In order to find the minimum positions of beacons, a resolving set with minimal cardinality has been obtained in the network which is called metric basis. Simultaneous local metric basis of the network is also given in which each pair of adjacent vertices of the network is distinguished by some element of simultaneous local metric basis which makes the network design more reasonable. In this paper a new network, the generalized Mobius ladder M-m,M-n, has been introduced and its metric dimension and simultaneous local metric dimension of its two subfamilies have been calculated.																	1343-0130	1883-8014				JAN	2020	24	1					3	11															
J								Observer-Based Piecewise Multi-Linear Controller Designs for Nonlinear Systems Using Feedback and Observer Linearizations	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										observer-based controller; piecewise multilinear model; feedback linearization; observer linearization problem	STABILITY	This paper proposes observer-based piecewise multilinear controllers for nonlinear systems using feedback and observer linearizations. The piecewise model is a nonlinear approximation and fully parametric. Feedback linearizations are applied to stabilize the piecewise multi-linear control system. Furthermore, observer linearizations are more conservative in modeling errors compared with feedback linearizations. In this paper, we propose robust observer designs for piecewise multi-linear systems. Moreover, we design piecewise multi-linear controllers that combine the robust observer with various performance such as a regulator and tracking controller. These design methods realize a separation principle that allows an observer and a regulator to be designed separately. Examples are demonstrated through computer simulation to confirm the feasibility of our proposals.																	1343-0130	1883-8014				JAN	2020	24	1					12	25															
J								A Hybrid Fuzzy System Dynamics Approach for Risk Analysis of AUV Operations	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										autonomous underwater vehicle; hybrid system dynamics; fuzzy set theory; risk analysis	UNDERWATER VEHICLES	The maturing of autonomous technology has fostered a rapid expansion in the use of Autonomous Underwater Vehicles (AUVs). To prevent the loss of AUVs during deployments, existing risk analysis approaches tend to focus on technicalities, historical data and experts' opinion for probability quantification. However, data may not always be available and the complex interrelationships between risk factors are often neglected due to uncertainties. To overcome these shortfalls, a hybrid fuzzy system dynamics risk analysis (FuSDRA) is proposed. The approach utilises the strengths while overcoming limitations of both system dynamics and fuzzy set theory. Presented as a three-step iterative framework, the approach was applied on a case study to examine the impact of crew operating experience on the risk of AUV loss. Results showed not only that initial experience of the team affects the risk of loss, but any loss of experience in earlier stages of the AUV program have a lesser impact as compared to later stages. A series of risk control policies were recommended based on the results. The case study demonstrated how the FuSDRA approach can be applied to inform human resource and risk management strategies, or broader application within the AUV domain and other complex technological systems.																	1343-0130	1883-8014				JAN	2020	24	1					26	39															
J								A Heuristic-Based Model for MMMs-Induced Fuzzy Co-Clustering with Dual Exclusive Partition	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fuzzy co-clustering; dual exclusive partition; multinomial mixture models	ALGORITHM; DOCUMENTS	MMMs-induced fuzzy co-clustering achieves dual partition of objects and items by estimating two different types of fuzzy memberships. Because memberships of objects and items are usually estimated under different constraints, the conventional models mainly targeted object clusters only, but item memberships were designed for representing intra-cluster typicalities of items, which are independently estimated in each cluster. In order to improve the interpretability of co-clusters, meaningful items should not belong to multiple clusters such that each co-cluster is characterized by different representative items. In previous studies, the item sharing penalty approach has been applied to the MMMs-induced model but the dual exclusive constraints approach has not yet. In this paper, a heuristic-based approach in FCM-type coclustering is modified for adopting in MMMs-induced fuzzy co-clustering and its characteristics are demonstrated through several comparative experiments.																	1343-0130	1883-8014				JAN	2020	24	1					40	47															
J								On Sampling Techniques for Corporate Credit Scoring	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										corporate credit scoring; imbalanced dataset; balancing method; performance measurement; ensemble model	ART CLASSIFICATION ALGORITHMS; NEURAL-NETWORKS; RISK-ASSESSMENT	The imbalanced dataset is a crucial problem found in many real-world applications. Classifiers trained on these datasets tend to overfit toward the majority class, and this problem severely affects classifier accuracy. This ultimately triggers a large cost to cover the error in terms of misclassifying the minority class especially in credit-granting decision when the minority class is the bad loan applications. By comparing the industry standard with well-known machine learning and ensemble models under imbalance treatment approaches, this study shows the potential performance of these models towards the industry standard in credit scoring. More importantly, diverse performance measurements reveal different weaknesses in various aspects of a scoring model. Employing class balancing strategies can mitigate classifier errors, and both homogeneous and heterogeneous ensemble approaches yield the best significant improvement on credit scoring.																	1343-0130	1883-8014				JAN	2020	24	1					48	57															
J								Tsallis Entropy-Based Fuzzy Latent Semantics Analysis	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										latent semantic analysis; Tsallis entropy; fuzzy clustering		In this study, we present a fuzzy counterpart to the probabilistic latent semantic analysis (PLSA) approach. It is derived by solving the optimization problem of Tsallis entropy-penalizing free energy of a pseudo PLSA model by using a modified i.i.d. assumption. This derivation is similar to that of the conventional fuzzy counterpart of the PLSA, which involves solving the optimization problem of Shannon entropy-penalizing free energy. Furthermore, the proposed method is validated using numerical examples.																	1343-0130	1883-8014				JAN	2020	24	1					58	64															
J								Visualization of Potential Technical Solutions by SOM and Co-Clustering and its Extension to Multi-View Situation	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										patent documents; technical solution; self-organizing maps; co-clustering; multi-view clustering		In order to support inspiration of potential technical solutions, this paper considers visualization of solving means varied in patent documents through SOM. Non-structured patent document data can be quantified through two different scheme: word level co-occurrence probability vectors and correlation coefficients of the generated co-occurrence probability vectors. Comparing the two SOMs derived with the above schemes is useful for supporting innovation acceleration through extraction of important pairs of related factors in new technology development. In this paper, co-cluster structures are utilized for emphasizing field-related solutions by constructing multiple SOMs after co-clustering. Document x keyword co-occurrence analysis achieves extraction of co-clusters consisting of mutually related pairs in particular fields. Additionally, this paper also considers an extension to a multi-view situation, where each patent is characterized by additional patent classification system of F-term by Japan Patent Office. Through multi-view co-clustering among documents x keywords x F-terms, theme field-related knowledge is demonstrated to be extracted.																	1343-0130	1883-8014				JAN	2020	24	1					65	72															
J								A Kernel k-Means-Based Method and Attribute Selections for Diabetes Diagnosis	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										machine learning; clustering; PIMA dataset; MIMIC database; medical decision support system	CLASSIFICATION; PERFORMANCE; REDUCTION	Diabetes diagnosis is important clue to the high death rate and complication consequences caused by the disease. First, we propose a kernel k-means-based prediction method and explore attribute selections for effective and robust diabetes diagnosis. This method derives homogeneous sub-clusters in the high dimensional kernelized feature space to compute the distance of a new instance to those sub-clusters, and then apply the 1-nearest neighbor to classify it as positive or negative to the disease. Our experimental results could identify the best effective attribute group for each considered prediction method and show that the proposed method outperforms the existing ones for the task. Second, we introduce our developed diabetes visualization and decision support system, named DIAVIS, which is equipped with the proposed prediction method. This system can support doctors to diagnose diabetes and track patient health progress to prescribe proper medications in a treatment process.																	1343-0130	1883-8014				JAN	2020	24	1					73	82															
J								Aggregation of Epistemic Uncertainty in Forms of Possibility and Certainty Factors	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										uncertainty combination; information aggregation; possibility theory; certainty factors; information source model	COMBINATION; INFERENCE; OPERATORS; FUSION	Uncertainty aggregation is an important reasoning for making decisions in the real world, which is full of uncertainty. The paper proposes an information source model for aggregating epistemic uncertainties about truth and discusses uncertainty aggregation in the form of possibility distributions. A new combination rule of possibilities for truth is proposed. Then, this paper proceeds to discussion about a traditional but seemingly forgotten representation of uncertainty (i.e., certainty factors (CFs)) and proposes a new interpretation based on possibility theory. CFs have been criticized because of their lack of sound mathematical interpretation from the viewpoint of probability. Thus, this paper first establishes a theory for a sound interpretation using possibility theory. Then it examines aggregation of CFs based on the interpretation and some combination rules of possibility distributions. The paper proposes several combination rules for CFs having sound theoretical basis, one of which is exactly the same as the oft-criticized combination.																	1343-0130	1883-8014				JAN	2020	24	1					83	94															
