PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								De-risking long-term care insurance	SOFT COMPUTING										Long-term care insurance; De-risking; Disability risk; Conditional VaR minimization	STOCHASTIC MORTALITY; MANAGEMENT	In this paper, we propose a de-risking strategy model for LTC insurers facing with longevity and disability risks, by constructing hedge positions with vanilla disability swaps and options. We rely on long-term care insurance in a multiple state framework. The optimal hedge level for each de-risking strategies is computed, respectively, by minimizing the total cost of the de-risking strategy under the Conditional Value-at-Risk (CVaR) constraint on the total unfunded liabilities and minimizing the CVaR under a total cost constraint. A numerical application is performed, and the results suggest that a de-risking strategy based on disability derivatives can be a viable solution to reduce the portfolio riskiness of LTC insurers.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8627	8641		10.1007/s00500-019-04658-0													
J								Improving portfolios global performance using a cleaned and robust covariance matrix estimate	SOFT COMPUTING										Portfolio selection; Maximum variety portfolio; Minimum variance portfolio; Covariance matrix; Random matrix theory; Thresholding; Factor model; Elliptic distribution	NOISE; DISTRIBUTIONS; ALGORITHM; LOCATION	This paper presents how the use of a cleaned and robust covariance matrix estimate can improve significantly the overall performance of maximum variety and minimum variance portfolios. We assume that the asset returns are modelled through a multi-factor model where the error term is a multivariate and correlated elliptical symmetric noise extending the classical Gaussian assumptions. The factors are supposed to be unobservable and we focus on a recent method of model order selection, based on the random matrix theory to identify the most informative subspace and then to obtain a cleaned (or de-noised) covariance matrix estimate to be used in the maximum variety and minimum variance portfolio allocation processes. We apply our methodology on real market data and show the improvements it brings if compared with other techniques especially for non-homogeneous asset returns.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8643	8654		10.1007/s00500-020-04840-9													
J								On typical hesitant fuzzy automata	SOFT COMPUTING										Hesitant fuzzy set; Typical hesitant fuzzy automata; Nondeterminism	COMPUTING POWER; SETS; AGGREGATION; LANGUAGES; TYPE-2	In this paper, we study a new generalization for the notion of fuzzy automata, which we called typical hesitant fuzzy automata. First, we present the formulations of the mathematics framework for the theory of typical hesitant fuzzy automata. Second, we then show a method to transform nondeterministic typical hesitant fuzzy automata (in short nthfa) into deterministic typical hesitant fuzzy automata (in short dthfa), which is effective at removing nondeterminism but does not preserve faithfully the associated typical hesitant fuzzy language. Moreover, we show that the power of any nthfa is equivalent to a finite family of fuzzy automata.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8725	8736		10.1007/s00500-020-04896-7													
J								A characterization of pseudofinite MV-algebras	SOFT COMPUTING										MV-algebra; Pseudofiniteness; Definable wellfoundedness		We consider pseudofinite MV-algebras. As a main result, we show that an infinite MV-algebra is pseudofinite if and only if it is definably well founded, improving a result of a previous paper. Moreover, we show that the theory of pseudofinite MV-algebras has a partial form of elimination of quantifiers. Further, we show that the class of pseudofinite MV-chains and the class of pseudofinite MV-algebras are not finitely axiomatizable, we give some collapsing results for pseudofinite MV-algebras, we consider relative subalgebras of pseudofinite MV-algebras, and we study ideals of pseudofinite MV-algebras.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8751	8761		10.1007/s00500-020-04910-y													
J								Application of artificial neural networks to predict flow velocity in a 180 degrees sharp bend with and without a spur dike	SOFT COMPUTING										Flow pattern; Sharp bend channel; ADV; Spur dike; Artificial neural network; Outlier detection	EXTREME LEARNING-MACHINE; FIELD; SCOUR; REGRESSION; SIMULATION; MODELS	This work has compared the performance of three well-known artificial neural network (ANN) approaches in turbulent flow pattern modeling based on geometric characteristics of the channel (angle of horizon, distance from outer bank of the bend, and distance from the bed) in a 180 degrees sharp bend with and without a T-shaped spur dike. ANN methods discussed in this work are feed-forward neural network, cascade feed-forward neural network, and extreme learning machines. Acoustic Doppler velocimetry is used to measure the velocity components in x, y, and z directions. Conducting this research is innovative and significant from three aspects: First, the data of the flow pattern around T-shaped spur dikes in a 180 degrees bend, given its high importance in rivers in nature, are very rarely available; second, application of neural network models for understanding the flow nature is highly efficient at other bend points where no experimental or field measurements have been conducted; and third, with these models, the cost and time required for conducting numerical and experimental modeling for prediction of the flow velocity significantly decrease. Outliers (abnormalities or anomalies) may be generated by various factors in the measured velocities. Before modeling by ANNs, the self-organizing map clustering method is used to detect outliers to obtain more accurate models. Performance of ANN models is evaluated using correlation coefficient (R) and root mean squared error criteria. In order to compare the performance of ANNs in flow velocity modeling at different locations of the bend, seven different data groups are considered using different combinations of samples by random sampling. Results of comparison of ANN models with experimental data indicate that ANNs provide reasonable results in most cases and may be employed successfully in estimating the velocity in sharp bends with and without the presence of a spur dike. Moreover, it may be concluded that ANN models without spur dikes are more accurate than those with spur dikes due to creating less turbulence flow.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8805	8821		10.1007/s00500-019-04413-5													
J								Supplier selection using extended IT2 fuzzy TOPSIS and IT2 fuzzy MOORA considering subjective and objective factors	SOFT COMPUTING										Interval type-2 fuzzy sets; MCDM; Extended IT2F TOPSIS; Extended IT2F MOORA; Supplier selection; Subjective and objective factors	MODEL; SETS; CRITERIA	During recent years, determination of efficient supplier has become a major challenge for improving the organizational efficiency. However, determination of suitable suppliers is always a complex multiple criteria decision-making (MCDM) problem because it involves the consideration of a large number of objective and subjective factors and also the factors may be uncertain and conflict in nature. This paper presents two novel MCDM techniques in interval type-2 fuzzy (IT2F) environment capable of handling uncertain subjective and objective factors simultaneously for selection of efficient suppliers in real-life applications. Technique for order preference by similarity to the ideal solution (TOPSIS) and multi-objective optimization on the basis of ratio analysis (MOORA) methods are used in IT2F environment to evaluate subjective factors with regard to subjective factor measures (SFM), and traditional normalization technique is used to evaluate the objective factors in terms of objective factor measures (OFMs). Then, SFM and OFM are used to calculate supplier selection index (SSI) by using Brown and Gibson model. The proposed models are then demonstrated with a case study in an Indian manufacturing organization for selection of efficient suppliers. Sensitivity analysis and comparative study of the results are carried out. It is found that the model is useful and efficient for decision making and evaluation of suitable suppliers in an uncertain environment.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8899	8915		10.1007/s00500-019-04419-z													
J								Enhancing differential evolution algorithm with repulsive behavior	SOFT COMPUTING										Evolutionary algorithm; Differential evolution; Repulsive behavior; Artificial neural network training; Four-bar mechanism	PARTICLE SWARM OPTIMIZATION; MUTATION; PERFORMANCE; STRATEGIES	In the real world, differential evolution (DE) algorithm can effectively solve optimization problems in engineering; thus, DE has been applied in various fields. However, in complex multimodal problems, DE may encounter stagnation during iterations. Thus, we propose an improved DE algorithm with repulsive behavior, named RBDE. The core idea of RBDE is that offsprings no longer simply learn from the current optima but continue to explore the direction in which the current optimal individual is repelled by poorer individuals. This mechanism increases the diversity of the learning direction of a population. RBDE includes two types of repulsive behaviors: In the first, RBDE selects two parents as the source of repulsion and generates two different repulsive forces to promote the offspring to explore the optimal individual; the other considers that the gradient of the repulsion between the parents is the learning direction of the offspring. The repulsive behavior can effectively alleviate the stagnation of DE when dealing with multimodal problems. To evaluate the performance of RBDE, we use CEC2017 benchmarks to test RBDE and nine other algorithms. The results show that the performance of RBDE is better than that of the other nine algorithms. In addition, RBDE is used to train an artificial neural network and is applied to the optimization problem of four-bar linkages, whose results indicate that the model obtained by RBDE is more accurate than those by the other algorithms.																	1432-7643	1433-7479				JUN	2020	24	12			SI		9279	9305		10.1007/s00500-019-04454-w													
J								Supervised feature learning by adversarial autoencoder approach for object classification in dual X-ray image of luggage	JOURNAL OF INTELLIGENT MANUFACTURING										Transport security; Baggage control; Object detection; YOLOv3; Features learning; Autoencoder	FEATURE-EXTRACTION; EDS	X-ray inspection by control officers is not always consistent when inspecting baggage since this task are monotonous, tedious and tiring for human inspectors. Thus, a semi-automatic inspection makes sense as a solution in this case. In this perspective, the study presents a novel feature learning model for object classification in luggage dual X-ray images in order to detect explosives objects and firearms. We propose to use supervised feature learning by autoencoders approach. Object detection is performed by a modified YOLOv3 to detect all the presented objects without classification. The features learning is carried out by labeled adversarial autoencoders. The classification is performed by a support vector machine to classify a new object as explosive, firearms or non-threatening objects. To show the superiority of our proposed system, a comparative analysis was carried out to several methods of deep learning. The results indicate that the proposed system leads to efficient objects classification in complex environments, achieving an accuracy of 98.00% and 96.50% in detection of firearms and explosive objects respectively.																	0956-5515	1572-8145				JUN	2020	31	5					1101	1112		10.1007/s10845-019-01498-5													
J								Real-time machining data application and service based on IMT digital twin	JOURNAL OF INTELLIGENT MANUFACTURING										Digital twin; Intelligent machine tool; Machining data; Data fusion; Data service	BIG DATA; TOOLS; SYSTEM; OPTIMIZATION; PARADIGM; DESIGN	With the development of manufacturing, machining data applications are becoming a key technological component of enhancing the intelligence of manufacturing. The new generation of machine tools should be digitalized, highly efficient, network-accessible and intelligent. An intelligent machine tool (IMT) driven by the digital twin provides a superior solution for the development of intelligent manufacturing. In this paper, a real-time machining data application and service based on IMT digital twin is presented. Multisensor fusion technology is adopted for real-time data acquisition and processing. Data transmission and storage are completed using the MTConnect protocol and components. Multiple forms of HMIs and applications are developed for data visualization and analysis in digital twin, including the machining trajectory, machining status and energy consumption. An IMT digital twin model is established with the aim of further data analysis and optimization, such as the machine tool dynamics, contour error estimation and compensation. Examples of the IMT digital twin application are presented to prove that the development method of the IMT digital twin is effective and feasible. The perspective development of machining data analysis and service is also discussed.																	0956-5515	1572-8145				JUN	2020	31	5					1113	1132		10.1007/s10845-019-01500-0													
J								Identification and classification of materials using machine vision and machine learning in the context of industry 4.0	JOURNAL OF INTELLIGENT MANUFACTURING										Industry 4; 0; Image processing; Machine vision; Machine learning; Material classification; Support vector machine	NEURAL-NETWORK; SYSTEM	Manufacturing has experienced tremendous changes from industry 1.0 to industry 4.0 with the advancement of technology in fast-developing areas such as computing, image processing, automation, machine vision, machine learning along with big data and Internet of things. Machine tools in industry 4.0 shall have the ability to identify materials which they handle so that they can make and implement certain decisions on their own as needed. This paper aims to present a generalized methodology for automated material identification using machine vision and machine learning technologies to contribute to the cognitive abilities of machine tools as wells as material handling devices such as robots deployed in industry 4.0. A dataset of the surfaces of four materials (Aluminium, Copper, Medium density fibre board, and Mild steel) that need to be identified and classified is prepared and processed to extract red, green and blue color components of RGB color model. These color components are used as features while training the machine learning algorithm. Support vector machine is used as a classifier and other classification algorithms such as Decision trees, Random forests, Logistic regression, and k-Nearest Neighbor are also applied to the prepared data set. The capability of the proposed methodology to identify the different group of materials is verified with the images available in an open source database. The methodology presented has been validated by conducting four experiments for checking the classification accuracies of the classifier. Its robustness has also been checked for various camera orientations, illumination levels, and focal length of the lens. The results presented show that the proposed scheme can be implemented in an existing manufacturing setup without major modifications.																	0956-5515	1572-8145				JUN	2020	31	5					1229	1241		10.1007/s10845-019-01508-6													
J								Support vector machines based non-contact fault diagnosis system for bearings	JOURNAL OF INTELLIGENT MANUFACTURING										Support vector machines (SVM); Vibration signatures; Bearings; Discrete wavelet transform (DWT); Non-contact fault diagnosis	ARTIFICIAL NEURAL-NETWORKS; MAHALANOBIS DISTANCE; ROTATING MACHINERY; PREDICTION; WAVELET; GEAR	Bearing defects have been accepted as one of the major causes of failure in rotating machinery. It is important to identify and diagnose the failure behavior of bearings for the reliable operation of equipment. In this paper, a low-cost non-contact vibration sensor has been developed for detecting the faults in bearings. The supervised learning method, support vector machine (SVM), has been employed as a tool to validate the effectiveness of the developed sensor. Experimental vibration data collected for different bearing defects under various loading and running conditions have been analyzed to develop a system for diagnosing the faults for machine health monitoring. Fault diagnosis has been accomplished using discrete wavelet transform for denoising the signal. Mahalanobis distance criteria has been employed for selecting the strongest feature on the extracted relevant features. Finally, these selected features have been passed to the SVM classifier for identifying and classifying the various bearing defects. The results reveal that the vibration signatures obtained from developed non-contact sensor compare well with the accelerometer data obtained under the same conditions. A developed sensor is a promising tool for detecting the bearing damage and identifying its class. SVM results have established the effectiveness of the developed non-contact sensor as a vibration measuring instrument which makes the developed sensor a cost-effective tool for the condition monitoring of rotating machines.																	0956-5515	1572-8145				JUN	2020	31	5					1275	1289		10.1007/s10845-019-01511-x													
J								Towards rich motion skills with the lightweight quadruped robot Serval	ADAPTIVE BEHAVIOR										Quadruped robot; bio-inspiration; agility; locomotion; falling; slope	DESIGN; LOCOMOTION; GAIT; SPEED	Bio-inspired robotic designs introducing and benefiting from morphological aspects present in animals allowed the generation of fast, robust, and energy-efficient locomotion. We used engineering tools and interdisciplinary knowledge transferred from biology to build low-cost robots, able to achieve a certain level of versatility. Serval, a compliant quadruped robot with actuated spine and high range of motion in all joints, was developed to address the question of what mechatronic complexity is needed to achieve rich motion skills. In our experiments, the robot presented a high level of versatility (number of skills) at medium speed, with a minimal control effort and, in this article, no usage of its spine. Implementing a basic kinematics-duplication from dogs, we found strengths to emphasize, weaknesses to correct, and made Serval ready for future attempts to achieve more agile locomotion. In particular, we investigated the following skills: walk, trot, gallop, bound (crouched), sidestep, turn with a radius, ascend slopes including flat ground transition, perform single and double step-downs, fall, trot over bumpy terrain, lie/sit down, and stand up.																	1059-7123	1741-2633				JUN	2020	28	3			SI		129	150		10.1177/1059712319853227													
J								Adapting to environmental dynamics with an artificial circadian system	ADAPTIVE BEHAVIOR										Long-term autonomy; behavior-based robotics; circadian rhythms; action-selection	ACTION-SELECTION; MECHANISM; CLOCK	One of the core challenges of long-term autonomy is the environmental dynamics that agents must interact with. Some of these dynamics are driven by reliable cyclic processes, and thus are predictable. The most dominant of these is the daily solar cycle, which drives both natural phenomena like weather, as well as the activity of animals and humans. Circadian clocks are a widespread solution in nature to help organisms adapt to these dynamics, and demonstrate that many organisms benefit from maintaining simple models of their environments and how they change. Drawing inspiration from circadian systems, this work models relevant environmental states as times series, allowing for forecasts of the state to be generated without any knowledge of the underlying physics. These forecasts are treated as special percepts in a behavior-based architecture; providing estimates of the future state rather than measurements of the current state. They are incorporated into an ethologically based action-selection mechanism, where they influence the activation levels of behaviors. The approach was validated on a simulated agricultural task: a solar-powered agent monitoring pest populations. By using the artificial circadian system to leverage the forecasted state, the agent was able to improve performance and energy management.																	1059-7123	1741-2633				JUN	2020	28	3			SI		165	179		10.1177/1059712319846854													
J								Forward propagation closed loop learning	ADAPTIVE BEHAVIOR										Closed loop; neural network; error propagation; reinforcement learning	LONG-TERM POTENTIATION; AREA; REPRESENTATIONS; TRANSMISSION; ORGANIZATION	For an autonomous agent, the inputs are the sensory data that inform the agent of the state of the world, and the outputs are their actions, which act on the world and consequently produce new sensory inputs. The agent only knows of its own actions via their effect on future inputs; therefore desired states, and error signals, are most naturally defined in terms of the inputs. Most machine learning algorithms, however, operate in terms of desired outputs. For example, backpropagation takes target output values and propagates the corresponding error backwards through the network in order to change the weights. In closed loop settings, it is far more obvious how to define desired sensory inputs than desired actions, however. To train a deep network using errors defined in the input space would call for an algorithm that can propagate those errors forwards through the network, from input layer to output layer, in much the same way that activations are propagated. In this article, we present a novel learning algorithm which performs such 'forward-propagation' of errors. We demonstrate its performance, first in a simple line follower and then in a 1st person shooter game.																	1059-7123	1741-2633				JUN	2020	28	3			SI		181	194		10.1177/1059712319851070													
J								Holistic primary key and foreign key detection	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Data profiling application; Primary key; Foreign key; Database management	INCLUSION DEPENDENCIES	Primary keys (PKs) and foreign keys (FKs) are important elements of relational schemata in various applications, such as query optimization and data integration. However, in many cases, these constraints are unknown or not documented. Detecting them manually is time-consuming and even infeasible in large-scale datasets. We study the problem of discovering primary keys and foreign keys automatically and propose an algorithm to detect both, namely Holistic Primary Key and Foreign Key Detection (HoPF). PKs and FKs are subsets of the sets of unique column combinations (UCCs) and inclusion dependencies (INDs), respectively, for which efficient discovery algorithms are known. Using score functions, our approach is able to effectively extract the true PKs and FKs from the vast sets of valid UCCs and INDs. Several pruning rules are employed to speed up the procedure. We evaluate precision and recall on three benchmarks and two real-world datasets. The results show that our method is able to retrieve on average 88% of all primary keys, and 91% of all foreign keys. We compare the performance of HoPF with two baseline approaches that both assume the existence of primary keys.																	0925-9902	1573-7675				JUN	2020	54	3					439	461		10.1007/s10844-019-00562-z													
J								IncompFuse: a logical framework for historical information fusion with inaccurate data sources	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Inaccurate data sources; Incompatibility probability; Error detection	WEB	We propose a novel framework, called IncompFuse, that significantly improves the accuracy of existing methods for reconstructing aggregated historical data from inaccurate historical reports. IncompFuse supports efficient data reliability assessment using the incompatibility probability of historical reports. We provide a systematic approach to define this probability based on properties of the data and relationships between the reports. Our experimental study demonstrates high utility of the proposed framework. In particular, we were able to detect noisy historical reports with very high detection accuracy.																	0925-9902	1573-7675				JUN	2020	54	3					463	481		10.1007/s10844-019-00569-6													
J								Detection of road pavement quality using statistical clustering methods	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Clustering methods; Road noise; Pavement quality; K-means; Ward's method; Road detection	SOUND; NOISE; MODEL	Road owners are concerned with the state of the road surface and they try to reduce noise coming from the road as much as possible. Using sound level measuring equipment installed inside a car, we can indirectly measure the road pavement state. Noise inside a car is made up of rolling noise, engine noise and other confounding factors. Rolling noise is influenced by noise modifiers such as car speed, acceleration, temperature and road humidity. Engine noise is influenced by car speed, acceleration, and gear shifts. Techniques need to be developed which compensate for these modifying factors and filter out the confounding noise. This paper presents a hierarchical clustering method resulting in a mapping of the road pavement quality. We present the method using a dataset recorded in multiple cars under different circumstances. The data has been retrieved by placing a Raspberry Pi device within these cars and recording the sound and location during various trips at different times. The sound data of our dataset was then corrected for correlation with speed and acceleration. Furthermore, clustering techniques were used in order to estimate the type and condition of the pavement using this set of noise measurements. The algorithms were run on a small dataset and compared to a ground truth which was derived from visual observations. The results were best for a combination of Generalised Additive Model (GAM) correction on the data combined with hierarchical clustering. A connectivity matrix merging points close to each other further enhances the results for road pavement quality detection, and results in a road type detection rate around 90%.																	0925-9902	1573-7675				JUN	2020	54	3					483	499		10.1007/s10844-019-00570-z													
J								A scaled-MST-based clustering algorithm and application on image segmentation	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Minimum spanning tree; Clustering; Minimum spanning tree-based clustering; Image segmentation; Image integration		Minimum spanning tree (MST)-based clustering is one of the most important clustering techniques in the field of data mining. Although traditional MST-based clustering algorithm has been researched for decades, it still has some limitations for data sets with different density distribution. After analyzing the advantages and disadvantages of the traditional MST-based clustering algorithm, this paper presents two new methods to improve the traditional clustering algorithm. There are two steps of our first method: compute a scaled-MST with scaled distance to find the longest edges between different density clusters and clustering based on the MST. To improve the performance, our second scaled-MST-clustering works by merging the MST construction and inconsistent edges' detection into one step. To verify the effectiveness and practicability of the proposed method, we apply our algorithm on image segmentation and integration. The encouraging performance demonstrates the superiority of the proposed method on both small data sets and high dimensional data sets.																	0925-9902	1573-7675				JUN	2020	54	3					501	525		10.1007/s10844-019-00572-x													
J								A novel multi-strategy DE algorithm for parameter optimization in support vector machine	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Support vector machine; Differential evolution; Parameter optimization	DIFFERENTIAL EVOLUTION; FEATURE-SELECTION; SVM; MODEL	Support vector machine (SVM) is a powerful technique in pattern classification, but its performance is highly dependent on its parameters. In this paper, a new SVM optimized by a novel differential evolution (DE) with a hybrid parameter setting strategy and a population size adaptation method is proposed and simplified as FDE-PS-SVM. In the hybrid parameter setting strategy, the SVM parameter offspring are generated by DE operators with evolutionary parameters that are fixed or with the ones generated by fuzzy logic inference (FLI) according to a given probability. In the population size adaptation method, the population size is shrunk gradually during the search, which tries to balance the diversity and concentration ability of the algorithm to find better SVM parameters. Some benchmark data sets are used to evaluate the proposed algorithm. Experimental results show that the two proposed strategies are effective to search for better SVM parameters while the proposed FDE-PS-SVM algorithm outperforms other algorithms published in other literature.																	0925-9902	1573-7675				JUN	2020	54	3					527	543		10.1007/s10844-019-00573-w													
J								Unsupervised tag recommendation for popular and cold products	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Electronic commerce; Tagging review; Information search; Review summarization; Opinion mining; User feedback	PHRASE	The rapid expansion of the Internet and its connectivity has given tremendous growth to e-commerce sites. Product reviews form an indispensable part of e-commerce sites. However, it is challenging and laborious to go through hundreds of reviews. In this paper, we address the problem of summarizing reviews by means of informative and readable tags. We present a novel unsupervised method of generating tags and rank them based on relevance. We refine the generated tags using NLP syntactic rules to make them more informative. Our proposed Tagging Product Review (TPR) system takes into consideration the opinions expressed on the product or its aspects. We also address the problem of tag generation for cold products, which have only a limited number of reviews and that too, with very short content. We use transfer learning to build a tag cloud from popular product reviews and use it to identify good tags from cold product reviews. We evaluate our proposed system using online reviews of twelve products of varying popularity, collected from Amazon.com. Our result demonstrates the effectiveness of our approach at generating relevant tags compared to three popular baseline methods. Our proposed approach gives an average tag relevance score (NDCG) of around 79% for popular products and 85% for cold products. Our approach also gives an average precision of 89% for identifying correct tags. The results suggest that our TPR system successfully summarize reviews by means of tags.																	0925-9902	1573-7675				JUN	2020	54	3					545	566		10.1007/s10844-019-00574-9													
J								Study of parallel processing area extraction and data transfer number reduction for automatic GPU offloading of IoT applications	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Open IoT; GPGPU; Tacit computing; Data transfer optimization; Genetic algorithm; Automatic offloading	SERVICE	To overcome of the high cost of developing IoT (Internet of Things) services by vertically integrating devices and services, Open IoT has been developed to enable various IoT services to be developed by integrating horizontally separated devices and services. For Open IoT, we have proposed Tacit Computing technology to discover the devices that can provide the data users need on demand and use them dynamically. We have also proposed an automatic GPU (graphics processing unit) offloading method as an elementary technology of Tacit Computing. However, our GPU offloading method can improve only a limited number of applications because it only optimizes the extraction of parallelizable loop statements. Therefore, in this paper, to improve performances of more applications automatically, we propose an improved GPU offloading method with fewer data transfers between the CPU and GPU that can improve performance of many IoT applications. We evaluate our proposed GPU offloading method by applying it to Darknet and Fourier Transform, which are general large applications for CPU, and find that it can process them 3 times and 5 times as quickly as only using CPUs within 10-hour tuning time.																	0925-9902	1573-7675				JUN	2020	54	3					567	584		10.1007/s10844-019-00575-8													
J								WebKey: a graph-based method for event detection in web news	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Topic detection and tracking; Event detection; Data mining; Community detection	ALGORITHM	With rapid and vast publishing of news over the Internet, there is a surge of interest to detect underlying hot events from online news streams. There are two main challenges in event detection: accuracy and scalability. In this paper, we propose a fast and efficient method to detect events in news websites. First, we identify bursty terms which suddenly appear in a lot of news documents. Then, we construct a novel co-occurrence graph between terms in which nodes and edges are weighted based on important features such as click and document frequency within burst intervals. Finally, a weighted community detection algorithm is used to cluster terms and find events. We also propose a couple of techniques to reduce the size of the graph. The results of our evaluations show that the proposed method yields a much higher precision and recall than past methods, such that their harmonic mean is improved by at least 40%. Moreover, it reduces the running time and memory usage by a factor of at least 2.																	0925-9902	1573-7675				JUN	2020	54	3					585	604		10.1007/s10844-019-00576-7													
J								A utility based approach for data stream anonymization	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Data streams; Data anonymization; Data privacy; Clustering	K-ANONYMITY; PRIVACY; FRAMEWORK	Data streams are good models to characterize dynamic, on-line, fast and high-volume data requirements of today's businesses. However, sensitivity of data is usually an obstacle for deployment of many data streams applications. To address this challenging issue, many privacy preserving models, including k-anonymity, have been adapted to data streams. Data stream anonymization frameworks have already addressed how to preserve data quality as much as possible under bounded delays. In this work, our main motivation is to minimize average delay while keeping data quality high. It is our claim that data utility is a function of both data quality and data aging in data streams processing tasks. However, there is a tradeoff between data aging and data quality optimizations. To this end, we present a tunable data stream k-anonymization framework and an algorithm named UBDSA (Utility Based Approach for Data Stream Anonymization). To attain high quality anonymity groups, UBDSA also introduces a new distance metric, named CAIL (Cardinality Aware Information Loss). Our experimental evaluations compare performance of UBDSA with the literature, and the results show its merit in terms of better average delay and information loss.																	0925-9902	1573-7675				JUN	2020	54	3					605	631		10.1007/s10844-019-00577-6													
J								Social recommender systems: techniques, domains, metrics, datasets and future scope	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Recommender system; Social media; Collaborative filtering; Deep learning; Social networks; Social recommender system	TRUST; NETWORKS; POINT	With the evolution of social media, an enormous amount of information is shared every day. Recommender systems contribute significantly in handling big data and presenting relevant information, services and items to people. A substantial number of recommender system algorithms based on social media data have been proposed and applied to numerous domains in the literature. This paper presents a state-of-the-art survey of existing techniques of social recommender systems. We present different domains where the existing systems have been experimented. We also present a tabular representation of different metrics used by these papers. We discuss some frequently used datasets of these systems. Lastly, we discuss some of the future works in this area. The main aim of this paper is to provide a concise review of published papers to assist potential researchers in this field to devise new techniques.																	0925-9902	1573-7675				JUN	2020	54	3					633	667		10.1007/s10844-019-00578-5													
J								Parallel execution combinatorics with metaheuristics: Comparative study	SWARM AND EVOLUTIONARY COMPUTATION										Parallel; Sequential; Metaheuristics; Optimization; Energy; Genetic algorithm; Variable neighborhood search; Simulated annealing	GENETIC ALGORITHMS; OPTIMIZATION	Optimization arises everywhere in industrial and engineering fields, with complex and time-consuming problems to be solved. Exact search techniques cannot afford practical solutions for most of the real-life problems in reasonable time-bound. Metaheuristics proved to be numerically efficient solvers for such problems in terms of solution quality, however, they could require large time and energy to get the optimal solution. Parallelization (i.e., distributed) is a promising approach for overcoming the overwhelming energy and time consumption values of these methods. Despite recent approaches in running metaheuristics in parallel, the community still lacks for novel studies comparing and benchmarking the canonical optimization techniques while being running in parallel. In this work, we present two extensive studies to the solution quality, energy consumption, and execution time for three different metaheuristics (Genetic Algorithm, Variable Neighborhood Search, and Simulated Annealing) and their distributed counterparts. The main aim of our studies is exploring the efficiency of parallel execution of the metaheuristics while being running in new computing environments. Here, we want to identify the combinatorics between metaheuristics and solving optimization problems while being run in parallel. For our studies, we consider a multicore machine with 32 cores. This choice to a recent and commonly used system shall enrich the existing literature for multicore systems against the enormous existing studies over cluster systems. The analyses and discussions for the results of the different algorithms exhibit the combinatorics between the different metaheuristics and the parallel execution using a different number of cores. The outcome of these studies builds a guide for future designs of efficient and energy-aware optimization techniques.																	2210-6502	2210-6510				JUN	2020	55								100692	10.1016/j.swevo.2020.100692													
J								Study of population partitioning techniques on efficiency of swarm algorithms	SWARM AND EVOLUTIONARY COMPUTATION										Multi-population; Population partitioning; Swarm algorithms; Global optimisation	BAT ALGORITHM; GENETIC ALGORITHM; MULTI; OPTIMIZATION; SEARCH; SOLVE; MODEL	This paper presents a study of various population partitioning techniques and their effect on the efficiency of swarm algorithms. Population partitioning techniques based on different concepts have been studied. Prominent amongst them is self-adaptive multi-population (SAMP) technique where populations are added and deleted dynamically based on their diversity. This techniques start with a single randomly initialised population, called free population. After evolution, if the distance between solutions drops below a limit, it is considered to have converged. If all existing populations have converged, a new randomly generated population is added. SAMP keeps at least one free population at all times, hence ensuring the algorithm doesn't get trapped in local optima. Another promising population partitioning technique studied is random partitioning, where a single population is divided into many smaller sub-populations randomly. Few extensions to the studied techniques are proposed, like an adaptive hierarchical partitioning technique, seed based partitioning with fixed seeds, random partitioning with master population, SAMP with random partitioning etc. All the studied and proposed techniques are compared over a set of benchmark functions. The strongest amongst all techniques was found to be SAMPR. SAMPR is a hybrid of self-adaptive multi-population (SAMP) technique and random partitioning where after every few generations all populations are combined together and re-partitioned randomly. Efficiency of SAMPR is validated over seven well-known swarm algorithms. Extensive comparisons are conducted over multiple benchmark functions, CEC'14 function set and 800 GKLS generated functions. Results establish the efficiency of the proposed technique for improving performance of swarm algorithms.																	2210-6502	2210-6510				JUN	2020	55								100672	10.1016/j.swevo.2020.100672													
J								A multi-population memetic algorithm for the 3-D protein structure prediction problem	SWARM AND EVOLUTIONARY COMPUTATION										Optimization; Metaheuristics; Evolutionary and knowledge-based algorithms; Structural bioinformatics	BEE COLONY ALGORITHM; EFFICIENT ALGORITHM; SECONDARY STRUCTURE; OPTIMIZATION; CLASSIFICATION; RESOLUTION; DIVERSITY; SEQUENCE	In this paper, we present a knowledge-based memetic algorithm to tackle the three-dimensional protein structure prediction problem without the explicit use of experimentally determined protein structures' templates. Our algorithm proposal was divided into two main prediction steps: (i) solutions sampling and initialization; and (ii) structural models' optimization coming from the previous stage. The first step generates and classifies several structural models for a given target protein, through the Angle Probability List strategy, to identify distinct structural patterns and consider reasonable solutions in the memetic algorithm initialization. The Angle Probability List takes advantage of structural knowledge stored in the Protein Data Bank to reduce the size and, consequently, the conformational search space complexity. The second step of the method consists in the optimization of the structures generated in the first stage by the proposed memetic algorithm. It uses a tree-based population where each node can be seen as an independent subpopulation that interacts with each other over global search operations, aiming at knowledge sharing, population diversity, and better exploration of the multimodal search space. The method also encompasses ad hoc global search operators, whose objective is to increase the method exploration ability focusing on specific characteristics of the protein structure prediction problem, combined with the artificial bee colony algorithm used as an exploitation technique applied to each node of the tree. The proposed algorithm was tested on a set of 24 amino acid sequences, as well as compared to the reference method in the protein structure prediction area, the method of Rosetta. The obtained results show the ability of our method to predict three-dimensional protein structures with similar folding to the experimentally determined ones, regarding the structural metrics Root-Mean-Square Deviation and Global Distance Total Score Test. We also show that our method was able to reach comparable results to Rosetta, and in some cases, it outperformed Rosetta, corroborating the effectiveness of our proposal.																	2210-6502	2210-6510				JUN	2020	55								100677	10.1016/j.swevo.2020.100677													
J								A novel multi-objective Interactive Coral Reefs Optimization algorithm for the Unequal Area Facility Layout Problem	SWARM AND EVOLUTIONARY COMPUTATION										UA-FLP; Coral Reefs Optimization; Interactive algorithms; Bio-inspired algorithms	BAY STRUCTURE REPRESENTATION; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; FEATURE-SELECTION; SUBSTRATE LAYERS; DESIGN-PROBLEMS; BLOCK LAYOUT; SLICING TREE; TABU SEARCH; MODEL	The Unequal Area Facility Layout Problem (UA-FLP) has been widely analyzed in the literature using several heuristics and meta-heuristics to optimize some qualitative criteria, taking into account different restrictions and constraints. Nevertheless, the subjective opinion of the designer (Decision Maker, DM) has never been considered along with the quantitative criteria and restrictions. This work proposes a novel approach for the UA-FLP based on an Interactive Coral Reefs Optimization (ICRO) algorithm, which combines the simultaneous consideration of both quantitative and qualitative (DM opinion) features. The algorithm implementation is explained in detail, including the way of jointly considering quantitative and qualitative aspects in the fitness function of the problem. The experimental part of the paper illustrates the effect of including qualitative aspects in UA-FLP problems, considering three different hard UA-FLP instances. Empirical results show that the proposed approach is able to incorporate the DM preferences in the obtained layouts, without affecting much to the quantitative part of the solutions.																	2210-6502	2210-6510				JUN	2020	55								100688	10.1016/j.swevo.2020.100688													
J								Multi-agent coalition formation by an efficient genetic algorithm with heuristic initialization and repair strategy	SWARM AND EVOLUTIONARY COMPUTATION										Genetic algorithm; Coalition formation (CF); Ability requirement constraint; Heuristic initialization; Repair strategy	TASK ALLOCATION; OPTIMIZATION; SEARCH; ASSIGNMENT; NETWORK	In multi-agent systems (MAS), the coalition formation (CF) is an important problem focusing on allocating agents to different tasks. In this paper, three specific CF problems are considered, including the single-task single-coalition formation, the multi-task single-coalition formation, and the multi-task multi-coalition formation. The mathematical models of these three specific problems are formulated with the objective of minimizing the total cost while satisfying the ability requirement constraint. An efficient genetic algorithm with heuristic initialization and repair strategy (GAHIR) is proposed to solve the CF problem. Multiple initialization and repair methods, which utilize the prior knowledge of the specific problems, are proposed to improve the solution quality. Then, these methods are tested to prove their effectiveness. Finally, a comparison experiment about the proposed algorithm against several advanced algorithms is constructed. The results of statistical analysis by the Wilcoxon rank-sum test demonstrate that the proposed GAHIR can obtain better coalition schemes than its competitors in solving the CF problems. Furthermore, GAHIR has faster convergence speed in most instances.																	2210-6502	2210-6510				JUN	2020	55								100686	10.1016/j.swevo.2020.100686													
J								A multi-objective differential evolutionary algorithm with angle-based objective space division and parameter adaption for solving sodium gluconate production process and benchmark problems	SWARM AND EVOLUTIONARY COMPUTATION										Differential evolutionary algorithm; Angle-based; Adaption; Gaussian model; Multi-objective problem; Pareto front; Sodium gluconate production	PARTICLE SWARM OPTIMIZATION; FERMENTATION; DOMINANCE; MODEL	Convergence and diversity are two main performance indicators in multi-objective evolutionary algorithms. The fitness value in the objective space represents information which guides the evolution. To extract this useful information, a multi-objective differential evolutionary algorithm with angle-based objective space division and parameter adaption is proposed (MODE-ASP). In MODE-ASP, the objective space is split into several subspaces based on angle, and the optimal direction in each subspace is extracted to accelerate the convergence. A probability model is also built to achieve adaption of the parameters along with the evolution of the population. Compared with 5 state-of-the-art algorithms with 20 benchmark functions, MODE-ASP is shown to give a better performance. Moreover, the operating conditions of the sodium gluconate fermentation process are optimized with three proposed objective functions, to improve the utilization rates of equipment and conversion rates effectively. The MODE-ASP is shown to obtain a better Pareto front in this application.																	2210-6502	2210-6510				JUN	2020	55								100670	10.1016/j.swevo.2020.100670													
J								Multi-objective based scheduling algorithm for sudden drinking water contamination incident	SWARM AND EVOLUTIONARY COMPUTATION										Water distribution systems; Multi-objective optimization; Optimal scheduling; Sudden contamination incident	PARTICLE SWARM OPTIMIZATION; MANY-OBJECTIVE OPTIMIZATION; DISTRIBUTION-SYSTEMS; SENSOR PLACEMENT; SURROGATES; MANAGEMENT; IMPACTS	In recent years, drinking water contamination incidents have been occurring at an increasingly frequent rate, leading to significant economic losses and social issues. Establishing and improving emergency disposal mechanisms for water contamination incidents is an important issue that has become a foremost concern globally. In this study, we first perform a theoretical analysis on an optimal scheduling problem and then prove that the scheduling of the valve and hydrant is NP-complete. Subsequently, we propose a multi-objective optimization model for contaminant response and primarily investigate two conflicting objectives: first, to minimize the volume of contaminated water exposed to the public; and second, to minimize the operational costs of the hydrant and valves. Finally, a customized multi-objective non-dominated sorted genetic algorithm-II (NSGA-II) coupling with an EPANET simulation is proposed and two different sizes of water distribution networks are employed to demonstrate the validity of the proposed model and methodology. Furthermore, we investigate the impact of different parameters on the performance of our proposed algorithm.																	2210-6502	2210-6510				JUN	2020	55								100674	10.1016/j.swevo.2020.100674													
J								Solving mixed Pareto-Lexicographic multi-objective optimization problems: The case of priority chains	SWARM AND EVOLUTIONARY COMPUTATION										Pareto optimization; Lexicographic optimization; Evolutionary computation; Genetic algorithms; Numerical infinitesimals; Grossone infinity computing	EVOLUTIONARY ALGORITHM; PART I; METHODOLOGY; INFINITE; WORKING; MOEA/D	This paper introduces a new class of optimization problems, called Mixed Pareto-Lexicographic Multi-objective Optimization Problems (MPL-MOPs), to provide a suitable model for scenarios where some objectives have priority over some others. Specifically, this work focuses on a relevant subclass of MPL-MOPs, namely problems involving Pareto optimization of two or more priority chains. A priority chain (PC) is a sequence of objectives lexicographically ordered by importance. After examining the main features of those problems, named PC-MPL-MOPs, we propose an innovative approach to deal with them, built upon the Grossone Methodology, a recent theory which enables handling the priority in an elegant and powerful way. The most interesting aspect of this technique is the possibility to seamlessly embed it in any existing evolutionary algorithm, without altering its logical structure. In order to provide concrete examples, we implemented it on top of the well-known NSGA-II and MOEA/D algorithms, calling these new generalized versions PC-NSGA-II and PC-MOEA/D, respectively. In the second part of this article, we test the strength of our strategy in solving multi- and even many-objective problems with priority chains, comparing it against the results achieved by standard priority-based and non-priority-based approaches. Experiments show that our algorithms are generally able to produce more solutions and of higher quality.																	2210-6502	2210-6510				JUN	2020	55								100687	10.1016/j.swevo.2020.100687													
J								An enhanced-indicator based many-objective evolutionary algorithm with adaptive reference point	SWARM AND EVOLUTIONARY COMPUTATION										Performance indicator; Evolutionary algorithm; Reference points; Many-objective optimization	NONDOMINATED SORTING APPROACH; OPTIMIZATION; MOEA/D; PERFORMANCE	Indicator based many-objective evolutionary algorithms generally introduce the performance indicator as the selection criterion in environmental selection. In the calculation of some indicators, the reference points as sampled points on Pareto fronts are very important for their calculation. However, it is difficult to obtain good reference points on various types of Pareto fronts. To address this issue, this paper proposes an enhanced-indicator based many-objective evolutionary algorithm with adaptive reference point, termed EIEA. The algorithm proposes a reference point adaptation method to dynamically adapt the reference points for the calculation of indicators. Moreover, the calculation of IGD-NS is enhanced by employing the modified distance calculation to introduce the Pareto compliant which can further comprehensively measure the convergence and diversity. The proposed EIEA adopts Pareto dominance and the enhanced IGD-NS as the first selection criterion and the secondary selection criterion in environmental selection, respectively. The intensive experiments demonstrate that the proposed algorithm has good performance in solving problems with various types of Pareto fronts, surpassing several representative many-objective evolutionary algorithms for many-objective optimization.																	2210-6502	2210-6510				JUN	2020	55								100669	10.1016/j.swevo.2020.100669													
J								A random dynamic grouping based weight optimization framework for large-scale multi-objective optimization problems	SWARM AND EVOLUTIONARY COMPUTATION										Large-scale multi-objective problem; Weighted optimization framework; Random dynamic grouping; MMOPSO	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHMS; MOEA/D; SET	For large-scale multi-objective problems (LSMOPs), it is necessary to get a good grouping strategy or another way to reduce dimensions because of "the curse of dimensions". In this paper, a weighted optimization framework with random dynamic grouping is proposed for large-scale problems. A weight optimization framework utilizes a problem transformation scheme in which weights are chosen to be optimized instead of the decision variables in order to reduce the dimensionality of the search space. Random dynamic grouping is used to determine sizes of each group adaptively. And multi-objective particle swarm optimization with multiple search strategies (MMOPSO) is employed as an optimizer for both original variables and weight variables. The proposed algorithm is performed on 28 benchmark test problems with 1000 dimensions, and the experimental results show that it can get better performance than some the-state-of-art algorithms in fewer function evaluations. In addition, it can be extended to solve LSMOPs with 5000 dimensions.																	2210-6502	2210-6510				JUN	2020	55								100684	10.1016/j.swevo.2020.100684													
J								Comparative study of nature-inspired algorithms to design (1+alpha) and (2+alpha)-order filters using a frequency-domain approach	SWARM AND EVOLUTIONARY COMPUTATION										Fractional-step filter; Butterworth filter; Metaheuristic optimization; Differential evolution; Particle swarm optimization; Evolutionary strategy; Current feedback operational amplifier; Monte-Carlo simulation	DIFFERENTIAL EVOLUTION ALGORITHM; ORDER RATIONAL APPROXIMATION; PARTICLE SWARM OPTIMIZER; PASS BUTTERWORTH FILTER; PRACTICAL REALIZATION; GLOBAL OPTIMIZATION; INTEGRATORS; ENSEMBLE; ALPHA; IMPLEMENTATION	A precise control in the stopband attenuation characteristics can be achieved by using the fractional-step filters instead of the traditional integer-order fillers. In this paper, nine nature-inspired optimization algorithms, such as five advanced variants of differential evolution (DE), three advanced variants of particle swarm optimization (PSO), and an efficient evolutionary strategy method (CMA-ES-RIS) are employed to design the fractional-step low pass Butterworth filter (FLBF). The proposed (1 + alpha)and (2 + alpha) order models, where alpha is an element of (0, 1), are optimally approximated as an integer-order transfer function by using the magnitude-frequency information of the ideal FLBF. Comparisons regarding the solution quality and robustness reveal an improved accuracy for the DE variants and CMA-ES-RIS over all the PSO variants. Results from the pair-wise Wilcoxon rank-sum WA demonstrate the superiority of enhanced fitness-adaptive differential evolution (EFADE) algorithm as the most efficient optimization tool for solving this problem. Comparisons with the state-of-the-art approaches also confirm the superior modelling accuracy of the proposed FLBFs. The canonical structure circuit realization of the FLBFs using current feedback operational amplifiers is presented. Simulations carried out in OrCAD PSPICE platform suggest proximity in the magnitude responses between the proposed and the theoretical models. The optimal design of stable, minimum-phase (2 + alpha)order FLBFs is also presented for the first time without employing the cascading concept involving the integer-order Butterworth polynomials.																	2210-6502	2210-6510				JUN	2020	55								100685	10.1016/j.swevo.2020.100685													
J								Handling multi-objective optimization problems with unbalanced constraints and their effects on evolutionary algorithm performance	SWARM AND EVOLUTIONARY COMPUTATION										Multi-objective; Evolutionary algorithm; MOEA/D-M2M; Unbalanced constraints; Constraint-handling technique	MOEA/D	Despite the successful application of an extension of the Multi-Objective Evolution Algorithm based on Decomposition (MOEA/D-M2M) to solve unbalanced multi-objective optimization problems (UMOPs), its use in eonstrained unbalanced multi-objective optimization problems has not been fully explored. In an earlier paper, a definition of UMOPs was suggested that had two necessary conditions: 1) finding a favored subset of the Pareto set is easier than finding an unfavored subset, and 2) the favored subset of the Pareto set dominates a large part of the feasible space. The second condition strongly reduces the fraction of MOPs that are considered UMOPs. In this paper, we eliminate that second condition and consider a broader class of UMOPs. We design an unbalanced constrained multi-objective test suite with three different types of biased constraints, yielding three different types of constrained test problems in which the degree of imbalance is scalable via a set of parameters introduced for each problem. We analyse the characteristics of three types of constraints and the difficulties they present for potential solution algorithms-i.e., NSGA-II, MOEA/D and MOEA/D-M2M, with four constraint-handling techniques. MOEA/D-M2M is shown to significantly outperform the other algorithms on these problems due to its decomposition strategy.																	2210-6502	2210-6510				JUN	2020	55								100676	10.1016/j.swevo.2020.100676													
J								Heuristics and metaheuristics for the mixed no-idle flowshop with sequence-dependent setup times and total tardiness minimisation	SWARM AND EVOLUTIONARY COMPUTATION										Scheduling; No-idle flowshop; Heuristics; Total tardiness	ITERATED GREEDY ALGORITHM; SHOP SCHEDULING PROBLEM; BEAM SEARCH ALGORITHMS; PERMUTATION FLOWSHOP; DIFFERENTIAL EVOLUTION; MINIMIZING MAKESPAN; MACHINE; ART	This paper analyses the mixed no-idle flowshop scheduling problem by adding the sequence-dependent setup times constraint and focusing on minimising the total tardiness criterion. In the mixed no-idle flowshop environment, machines that allow idleness, and also those that do not, coexist. In this paper, an extension of this problem in which dependent setup times are included in machines that permit idleness is studied. In order to do so, a mixed integer linear programming (MILP) model is developed. Furthermore, a method to assess the total tardiness of a permutation sequence is provided. Moreover, a partial acceleration method is devised so as to compute the total tardiness in an insertion neighbourhood. Since this problem has not yet been studied in the literature, this research adapted the best known heuristics and metaheuristics available from related problems. Moreover, a new heuristic was developed in order to provide an efficient solving mechanism for the problem. The proposed heuristic was incorporated in the best metaheuristics found in the literature. The adapted methods, as well as our proposal, were tested and compared through statistical and computational experimentation in an extensive benchmark. The MILP was also evaluated and the optimal solutions were compared to those found by the new proposed heuristic. The results show that the proposed heuristic perform extremely well in terms of solution quality and computational efficiency, providing near optimal solutions in some cases.																	2210-6502	2210-6510				JUN	2020	55								100689	10.1016/j.swevo.2020.100689													
J								An algebraic framework for swarm and evolutionary algorithms in combinatorial optimization	SWARM AND EVOLUTIONARY COMPUTATION										Algebraic evolutionary algorithms; Combinatorial search spaces; Algebraic evolutionary computation	LINEAR ORDERING PROBLEM; DIFFERENTIAL EVOLUTION; MAKESPAN	A popular trend in evolutionary computation is to adapt numerical algorithms to combinatorial optimization problems. For instance, this is the case of a variety of Particle Swann Optimization and Differential Evolution implementations for both binary and permutation-based optimization problems. In this paper, after highlighting the main drawbacks of the approaches in literature, we provide an algebraic framework which allows to derive fully discrete variants of a large class of numerical evolutionary algorithms to tackle many combinatorial problems. The strong mathematical foundations upon which the framework is built allow to redefine numerical evolutionary operators in such a way that their original movements in the continuous space are simulated in the discrete space. Algebraic implementations of Differential Evolution and Particle Swarm Optimization are then proposed. Experiments have been held to compare the algebraic algorithms to the most popular schemes in literature and to the state-of-the-art results for the tackled problems. Experimental results clearly show that algebraic algorithms outperform the competitors and are competitive with the state-of-the-art results.																	2210-6502	2210-6510				JUN	2020	55								100673	10.1016/j.swevo.2020.100673													
J								A multi-clustering method based on evolutionary multiobjective optimization with grid decomposition	SWARM AND EVOLUTIONARY COMPUTATION										Clustering; Constrained decomposition with grids; Pareto front; Multiobjective evolutionary algorithm	NONDOMINATED SORTING APPROACH; GENETIC ALGORITHM; SELECTION	At present, it is a challenging task to determine the number of clusters (k), which has a great impact on the quality of major clustering methods. Multi-objective evolutionary algorithms (MOEAs), which determines k value adaptively, have been widely adopted for clustering. However, when the range of k becomes increasingly large, Pareto Front (PF) approximations obtained by an MOEA may not be uniformly distributed, leading to the difficulty of obtaining the optimal k value for clustering. For this reason, an MOEA base on constrained decomposition with grids (CCDG-K) is designed for clustering and better identify the optimal k value. CCDG-K adopts a grid system for decomposition. The grid system has an inherent property of reserving diversely populated solutions, which is very desirable for clustering. The experimental studies show that CCDG-K can deliver solutions of all k values which is of great help for obtaining the optimal k value. The experimental results also indicate that CCDG-K outperforms other algorithms in clustering.																	2210-6502	2210-6510				JUN	2020	55								100691	10.1016/j.swevo.2020.100691													
J								An Improved Ant Colony Optimization algorithm to the Periodic Vehicle Routing Problem with Time Window and Service Choice	SWARM AND EVOLUTIONARY COMPUTATION										Ant Colony Optimization; Multi objective optimization; Periodic Vehicle Routing Problem with Time; Window; Simulate Annealing; Service Choice	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; CONSTRAINTS; SEARCH	This article addresses a Periodic Vehicle Routing Problem with Time Window and Service Choice problem. This problem is basically a combination of existing Periodic Vehicle Routing Problem with Time Window and Periodic Vehicle Routing Problem with Service Choice. We model it as a multi objective problem. To solve this problem, we develop a heuristic algorithm based on Improved Ant Colony Optimization (IACO) and Simulate Annealing (SA) called Multi Objective Simulate Annealing - Ant Colony Optimization (MOSA-ACO). Improvements are made in following respects: a) a Euclidean distance based solution acceptance criterion is developed; b) a parameter control pattern is designed to generate different initial solutions; c) several local search strategies are added. Benchmark instances generated from Solomon's benchmark instances and Cordeau's benchmarks instances are applied. Comparison algorithms include four population based heuristics and IACO. Computation experiment results show that MOSA-ACO algorithm has a good performance on solving this problem.																	2210-6502	2210-6510				JUN	2020	55								100675	10.1016/j.swevo.2020.100675													
J								Configuring differential evolution adaptively via path search in a directed acyclic graph for data clustering	SWARM AND EVOLUTIONARY COMPUTATION										Data clustering; Ant colony optimization; Differential evolution; Hybrid algorithm	PARTICLE SWARM OPTIMIZATION; BIG DATA; ALGORITHM; ENSEMBLE	As an efficient data mining technique, data clustering has been widely-used for data analysis and extracting valuable hidden information. Leveraging the simplicity and effectiveness, the evolutionary optimization-driven clustering algorithms have exhibited promising performance and attracted tremendous attention. Up to the present, how to enable these algorithms to escape from local optima and accelerate convergence rates is an ongoing challenge. In this paper, we propose a novel adaptive Differential Evolution (DE) variant to deal with the above challenge when clustering data. In the improved DE algorithm, the four interdependent components, including mutation strategy, crossover strategy, scaling factor value, and crossover rate, are adaptively configured in an integrated manner via ant colony optimization (ACO) during the problem-solving process. To be specific, the relationships of four components in the DE algorithm are modeled as a directed acyclic graph, and a path in the graph exactly corresponds to a configuration for DE. During the optimization process, ant colony optimization is employed to search for a reasonable path for each individual of DE in terms of pheromones on arcs. In this manner, the configuration of the four interdependent components of DE will be generated dynamically, which is then used to guide the successive search behaviors of individuals in DE. Each individual has a path, representing a configuration for each component. After each iteration, individuals that generate promising solutions are allowed to deposit pheromone on the paths, resulting in more pheromones on the arcs appearing in better algorithm configurations (paths) more frequently. Through this manner, the search strategies and parameters of DE are comprehensively adapted by ACO. The proposed algorithm is named ACODE for short. To verify its effectiveness, the proposed ACODE is compared with four representative data clustering algorithms on eight widely-used benchmark datasets. The experimental results demonstrate the advantages of ACODE over half of the datasets.																	2210-6502	2210-6510				JUN	2020	55								100690	10.1016/j.swevo.2020.100690													
J								A Generalized Locally Linear Factorization Machine with Supervised Variational Encoding	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Frequency modulation; Encoding; Artificial neural networks; Euclidean distance; Optimization; Computational modeling; Factorization machines; locally linear coding; supervised variational encoding		Factorization Machines (FMs) learn weights for feature interactions, and achieve great success in many data mining tasks. Recently, Locally Linear Factorization Machines (LLFMs) have been proposed to capture the underlying structures of data for better performance. However, one obvious drawback of LLFM is that the local coding is only operated in the original feature space, which limits the model to be applied to high-dimensional and sparse data. In this work, we present a generalized LLFM (GLLFM) which overcomes this limitation by modeling the local coding procedure in a latent space. Moreover, a novel Supervised Variational Encoding (SVE) technique is proposed such that the distance can effectively describe the similarity between data points. Specifically, the proposed GLLFM-SVE trains several local FMs in the original space to model the higher order feature interactions effectively, where each FM associates to an anchor point in the latent space induced by SVE. The prediction for a data point is computed by a weighted sum of several local FMs, where the weights are determined by local coding coordinates with anchor points. Actually, GLLFM-SVE is quite flexible and other Neural Network (NN) based FMs can be easily embedded into this framework. Experimental results show that GLLFM-SVE significantly improves the performance of LLFM. By using NN-based FMs as local predictors, our model outperforms all the state-of-the-art methods on large-scale real-world benchmarks with similar number of parameters and comparable training time.																	1041-4347	1558-2191				JUN 1	2020	32	6					1036	1049		10.1109/TKDE.2019.2903403													
J								A Joint Two-Phase Time-Sensitive Regularized Collaborative Ranking Model for Point of Interest Recommendation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Collaboration; Data models; Motion pictures; Social networking (online); Urban areas; Sparse matrices; Heuristic algorithms; point-of-interest recommendation; time-aware recommendation; collaborative ranking; location-based social networks	INFORMATION; SYSTEMS	The popularity of location-based social networks (LBSNs) has led to a tremendous amount of user check-in data. Recommending points of interest (POIs) plays a key role in satisfying users needs in LBSNs. While recent work has explored the idea of adopting collaborative ranking (CR) for recommendation, there have been few attempts to incorporate temporal information for POI recommendation using CR. In this article, we propose a two-phase CR algorithm that incorporates the geographical influence of POIs and is regularized based on the variance of POIs popularity and users activities over time. The time-sensitive regularizer penalizes user and POIs that have been more time-sensitive in the past, helping the model to account for their long-term behavioral patterns while learning from user-POI interactions. Moreover, in the first phase, it attempts to rank visited POIs higher than the unvisited ones, and at the same time, apply the geographical influence. In the second phase, our algorithm tries to rank users favorite POIs higher on the recommendation list. Both phases employ a collaborative learning strategy that enables the model to capture complex latent associations from two different perspectives. Experiments on real-world datasets show that our proposed time-sensitive collaborative ranking model beats state-of-the-art POI recommendation methods.																	1041-4347	1558-2191				JUN 1	2020	32	6					1050	1063		10.1109/TKDE.2019.2903463													
J								Artifact-Based Workflows for Supporting Simulation Studies	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Data models; Analytical models; Computational modeling; Mathematical model; Adaptation models; Biological system modeling; Context modeling; Modeling and simulation life cycle; simulation study; artifact-based workflows; planning; user support	MODEL	Valid models are central for credible simulation studies. If those models do not exist, they need to be developed. In fact, entire simulation studies are often aimed at developing valid models. Thereby, successive model refinement and execution of diverse simulation experiments are closely intertwined. Whereas software-based support for individual simulation experiments exists, the intricate interdependencies and the diversity of tasks that govern simulation studies have prevented a more comprehensive support. To achieve the required flexibility while adhering to the constraints that apply between individual tasks, we adopt a declarative, artifact-based workflow approach. Therefore, central products of these simulation studies are identified and specified as artifacts: the conceptual model (with a focus on formally defined requirements), the simulation model, and the experiment. Each artifact is characterized by stages the artifact moves through to reach certain milestones and which are guarded by conditions. Thereby, the relations and constraints between artifacts become explicit. This is instrumental to check and ensure the consistency between conceptual model and simulation model, to automatically execute simulation experiments to probe the specified requirements, and to develop plans to provide goal-directed guidance to the user. We demonstrate the approach by using it to repeat an existing simulation study.																	1041-4347	1558-2191				JUN 1	2020	32	6					1064	1078		10.1109/TKDE.2019.2899840													
J								Citywide Bike Usage Prediction in a Bike-Sharing System	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Urban areas; Predictive models; Clustering algorithms; Meteorology; Time series analysis; Adaptation models; Prediction algorithms; Citywide bike usage prediction; Gaussian process regressor; Spatio-temporal dataset	BICYCLES; MODELS	To operate a bike-sharing system efficiently, system operators need to accurately predict how many bikes are to be rented and returned throughout the city. In this paper, we propose a Hierarchical Consistency Prediction (HCP) model to predict the citywide bike usage in the next period. First, an Adaptive Transition Constraint (AdaTC) clustering algorithm is proposed to cluster stations into groups, making the rent and transition at each cluster more regular than those at each single station. Second, a Similarity-based efficient Gaussian Process Regressor (SGPR) is proposed to respectively predict how many bikes are to be rented at different-scale locations, i.e., at each station, each cluster, and in the entire city. Besides largely improving the training and online prediction efficiency, our regressor considers external impacted factors, addresses the data unbalance issue, and better captures the non-linearity in spatio-temporal data. Third, we design a General Least Square (GLS) formulation to collectively improve those obtained predictions via a mutual reinforcement way. GLS makes the final predictions for rent more reasonable. Considering the causality between rent and return, a Transition based Inference (TINF) method is designed to infer the citywide bike return demand based on the predicted rent demands. Experiments on real-world data are conducted to confirm the effectiveness of our model.																	1041-4347	1558-2191				JUN 1	2020	32	6					1079	1091		10.1109/TKDE.2019.2898831													
J								Discerning Influence Patterns with Beta-Poisson Factorization in Microblogging Environments	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Twitter; Predictive models; Bayes methods; Computer science; Probabilistic logic; Sun; Social influence; Poisson factorization; microblogging		Social influence analysis in microblogging services has attracted much attention in recent years. However, most previous studies were focused on measuring users' (topical) influence. Little effort has been made to discern and quantify how a user is influenced. Specifically, the fact that user $i$i retweets a tweet from author $j$j could be either because $i$i is influenced by $j$j (i.e., $j$j is a topical authority), or simply because he is "influenced" by the content (interested in the content). To mine such influence patterns, we propose a novel Bayesian factorization model, dubbed Influence Beta-Poisson Factorization (IBPF). IBPF jointly factorizes the retweet data and tweet content to quantify latent topical factors of user preference, author influence and content influence. It generates every retweet record according to the sum of two causing terms: one representing author influence, and the other one derived from content influence. To control the impact of the two terms, for each user IBPF generates a probability for each latent topic by Beta distribution, indicating how strongly the user cares about the topical authority of the author. We develop an efficient variational inference algorithm for IBPF. We demonstrate the efficacy of IBPF on two public microblogging datasets.																	1041-4347	1558-2191				JUN 1	2020	32	6					1092	1103		10.1109/TKDE.2019.2897932													
J								Ensemble of Classifiers Based on Multiobjective Genetic Sampling for Imbalanced Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Prediction algorithms; Task analysis; Training; Genetics; Boosting; Machine learning algorithms; Imbalanced datasets; ensemble of classifiers; evolutionary algorithm	MULTIPLE CLASSES; NEURAL-NETWORKS; ROC CURVE; CLASSIFICATION; ALGORITHM; DIVERSITY; AREA	Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean.																	1041-4347	1558-2191				JUN 1	2020	32	6					1104	1115		10.1109/TKDE.2019.2898861													
J								GMC: Graph-Based Multi-View Clustering	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Matrix converters; Clustering algorithms; Clustering methods; Laplace equations; Computational modeling; Computational complexity; Kernel; Multi-view clustering; graph-based clustering; data fusion; Laplacian matrix; rank constraint	VIEW; SELECTION	Multi-view graph-based clustering aims to provide clustering solutions to multi-view data. However, most existing methods do not give sufficient consideration to weights of different views and require an additional clustering step to produce the final clusters. They also usually optimize their objectives based on fixed graph similarity matrices of all views. In this paper, we propose a general Graph-based Multi-view Clustering (GMC) to tackle these problems. GMC takes the data graph matrices of all views and fuses them to generate a unified graph matrix. The unified graph matrix in turn improves the data graph matrix of each view, and also gives the final clusters directly. The key novelty of GMC is its learning method, which can help the learning of each view graph matrix and the learning of the unified graph matrix in a mutual reinforcement manner. A novel multi-view fusion technique can automatically weight each data graph matrix to derive the unified graph matrix. A rank constraint without introducing a tuning parameter is also imposed on the graph Laplacian matrix of the unified matrix, which helps partition the data points naturally into the required number of clusters. An alternating iterative optimization algorithm is presented to optimize the objective function. Experimental results using both toy data and real-world data demonstrate that the proposed method outperforms state-of-the-art baselines markedly.																	1041-4347	1558-2191				JUN 1	2020	32	6					1116	1129		10.1109/TKDE.2019.2903810													
J								Mining Behavioral Sequence Constraints for Classification	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Databases; Data mining; Feature extraction; Distance measurement; Knowledge based systems; Task analysis; Sequence classification; sequential pattern mining; behavioral constraint templates; Declare	FREQUENT; PATTERNS; DISCOVERY	Sequence classification deals with the task of finding discriminative and concise sequential patterns. To this purpose, many techniques have been proposed, which mainly resort to the use of partial orders to capture the underlying sequences in a database according to the labels. Partial orders, however, pose many limitations, especially on expressiveness, i.e., the aptitude towards capturing certain behavior, and on conciseness, i.e., doing so in a compact and informative way. These limitations can be addressed by using a better representation. In this paper, we present the interesting Behavioral Constraint Miner (iBCM), a sequence classification technique that discovers patterns using behavioral constraint templates. The templates comprise a variety of constraints and can express patterns ranging from simple occurrence, to looping and position-based behavior over a sequence. Furthermore, iBCM also captures negative constraints, i.e., absence of particular behavior. The constraints can be discovered by using simple string operations in an efficient way. Finally, deriving the constraints with a window-based approach allows to pinpoint where the constraints hold in a string, and to detect whether patterns are subject to concept drift. Through empirical evaluation, it is shown that iBCM is better capable of classifying sequences more accurately and concisely in a scalable manner.																	1041-4347	1558-2191				JUN 1	2020	32	6					1130	1142		10.1109/TKDE.2019.2897311													
J								Optimal Margin Distribution Machine	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Support vector machines; Kernel; Training; Biological system modeling; Quadratic programming; History; Margin; margin distribution; minimum margin; classification		Support Vector Machine (SVM) has always been one of the most successful learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. However, recent theoretical results disclosed that maximizing the minimum margin does not necessarily lead to better generalization performance, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose the Optimal margin Distribution Machine (ODM), which can achieve a better generalization performance by optimizing the margin distribution explicitly. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be applied in any place where SVMs are used, and its superiority is verified both theoretically and empirically in this paper.																	1041-4347	1558-2191				JUN 1	2020	32	6					1143	1156		10.1109/TKDE.2019.2897662													
J								Semi-Supervised Learning with Auto-Weighting Feature and Adaptive Graph	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Semisupervised learning; Feature extraction; Data models; Sparse matrices; Adaptation models; Correlation; Kernel; Graph-based semi-supervised learning; label propagation; auto-weighting feature; adaptive neighborhood	LABEL PROPAGATION; FRAMEWORK	Traditional graph-based Semi-Supervised Learning (SSL) methods usually contain two separate steps. First, constructing an affinity matrix. Second, inferring the unknown labels. While such a two-step method has been successful, it cannot take full advantage of the correlation between affinity matrix and label information. In order to address the above problem, we propose a novel graph-based SSL method. It can learn the affinity matrix and infer the unknown labels simultaneously. Moreover, feature selection with auto-weighting is introduced to extract the effective and robust features. Further, the proposed method learns the data similarity matrix by assigning the adaptive neighbors for each data point based on the local distance. We solve the unified problem via an alternative minimization algorithm. Extensive experimental results on synthetic data and benchmark data show that the proposed method consistently outperforms the state-of-the-art approaches.																	1041-4347	1558-2191				JUN 1	2020	32	6					1167	1178		10.1109/TKDE.2019.2901853													
J								The Disruptions of 5G on Data-Driven Technologies and Applications	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										5G mobile communication; database systems; network slicing; Internet of Things; edge computing; federated learning; data privacy; security management	BIG DATA; MANAGEMENT; CHALLENGES; BLOCKCHAIN; CITY	With 5G on the verge of being adopted as the next mobile network, there is a need to analyze its impact on the landscape of computing and data management. In this paper, we analyze the impact of 5G on both traditional and emerging technologies and project our view on future research challenges and opportunities. With a predicted increase of 10-100x in bandwidth and 5-10x decrease in latency, 5G is expected to be the main enabler for smart cities, smart IoT and efficient healthcare, where machine learning is conducted at the edge. In this context, we investigate how 5G can help the development of federated learning. Network slicing, another key feature of 5G, allows running multiple isolated networks on the same physical infrastructure. However, security remains the main concern in the context of virtualization, multi-tenancy and high device density. Formal verification of 5G networks can be applied to detect security issues in massive virtualized environments. In summary, 5G will make the world even more densely and closely connected. What we have experienced in 4G connectivity will pale in comparison to the vast amounts of possibilities engendered by 5G.																	1041-4347	1558-2191				JUN 1	2020	32	6					1179	1198		10.1109/TKDE.2020.2967670													
J								Towards Automatic Construction of Diverse, High-Quality Image Datasets	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Noise measurement; Search engines; Manuals; Visualization; Scalability; Data models; Task analysis; Image dataset construction; multiple textual queries; dataset diversity		The availability of labeled image datasets has been shown critical for high-level image understanding, which continuously drives the progress of feature designing and models developing. However, constructing labeled image datasets is laborious and monotonous. To eliminate manual annotation, in this work, we propose a novel image dataset construction framework by employing multiple textual queries. We aim at collecting diverse and accurate images for given queries from the Web. Specifically, we formulate noisy textual queries removing and noisy images filtering as a multi-view and multi-instance learning problem separately. Our proposed approach not only improves the accuracy but also enhances the diversity of the selected images. To verify the effectiveness of our proposed approach, we construct an image dataset with 100 categories. The experiments show significant performance gains by using the generated data of our approach on several tasks, such as image classification, cross-dataset generalization, and object detection. The proposed method also consistently outperforms existing weakly supervised and web-supervised approaches.																	1041-4347	1558-2191				JUN 1	2020	32	6					1199	1211		10.1109/TKDE.2019.2903036													
J								Ultra-Scalable Spectral Clustering and Ensemble Clustering	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Clustering algorithms; Sparse matrices; Complexity theory; Robustness; Bipartite graph; Scalability; Approximation algorithms; Data clustering; large-scale clustering; spectral clustering; ensemble clustering; large-scale datasets; nonlinearly separable datasets	COMBINING MULTIPLE CLUSTERINGS	This paper focuses on scalability and robustness of spectral clustering for extremely large-scale datasets with limited resources. Two novel algorithms are proposed, namely, ultra-scalable spectral clustering (U-SPEC) and ultra-scalable ensemble clustering (U-SENC). In U-SPEC, a hybrid representative selection strategy and a fast approximation method for $K$K-nearest representatives are proposed for the construction of a sparse affinity sub-matrix. By interpreting the sparse sub-matrix as a bipartite graph, the transfer cut is then utilized to efficiently partition the graph and obtain the clustering result. In U-SENC, multiple U-SPEC clusterers are further integrated into an ensemble clustering framework to enhance the robustness of U-SPEC while maintaining high efficiency. Based on the ensemble generation via multiple U-SEPC's, a new bipartite graph is constructed between objects and base clusters and then efficiently partitioned to achieve the consensus clustering result. It is noteworthy that both U-SPEC and U-SENC have nearly linear time and space complexity, and are capable of robustly and efficiently partitioning 10-million-level nonlinearly-separable datasets on a PC with 64 GB memory. Experiments on various large-scale datasets have demonstrated the scalability and robustness of our algorithms. The MATLAB code and experimental data are available at https://www.researchgate.net/publication/330760669.																	1041-4347	1558-2191				JUN 1	2020	32	6					1212	1226		10.1109/TKDE.2019.2903410													
J								ESPM: Efficient Spatial Pattern Matching	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Pattern matching; Indexes; Planning; Information technology; Business; Australia; Marine vehicles; Spatial pattern matching; inverted linear quadtree	KEYWORD SEARCH	With recent advances in information technologies such as global position system and mobile internet, a huge volume of spatio-textual objects have been generated from location-based services, which enable a wide range of spatial keyword queries. Recently, researchers have proposed a novel query, called Spatial Pattern Matching (SPM), which uses a pattern to capture the user's intention. It has been demonstrated to be fundamental and useful for many real applications. Despite its usefulness, the SPM problem is computationally intractable. Existing algorithms suffer from the low efficiency issue, especially on large scale datasets. To enhance the performance of SPM, in this paper we propose a novel Efficient Spatial Pattern Matching (ESPM) algorithm, which exploits the inverted linear quadtree index and computes matched node pairs and object pairs level by level in a top-down manner. In particular, it focuses on pruning unpromising nodes and node pairs at the high levels, resulting in a large number of unpromising objects and object pairs to be pruned before accessing them from disk. We experimentally evaluate the performance of ESPM on real large datasets. Our results show that ESPM is over one order of magnitude faster than the state-of-the-art algorithm, and also uses much less I/O cost.																	1041-4347	1558-2191				JUN 1	2020	32	6					1227	1233		10.1109/TKDE.2019.2947505													
J								Genetic programming for natural language processing	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Grammatical evolution; Natural language processing; Applications; Challenges	NAMED ENTITY RECOGNITION; EVOLUTIONARY COMPUTATION; GRAMMATICAL EVOLUTION; CONVERSATIONAL AGENT; REGULAR EXPRESSIONS; GENERATION; RULES; ONTOLOGY; CORPUS	This work takes us through the literature on applications of genetic programming to problems of natural language processing. The purpose of natural language processing is to allow us to communicate with computers in natural language. Among the problems addressed in the area is, for example, the extraction of information, which draws relevant data from unstructured texts written in natural language. There are also domains of application of particular relevance because of the difficulty in dealing with the corresponding documents, such as opinion mining in social networks, or because of the need for high precision in the information extracted, such as the biomedical domain. There have been proposals to apply genetic programming techniques in several of these areas. This tour allows us to observe the potential-not yet fully exploited-of such applications. We also review some cases in which genetic programming can provide information that is absent from other approaches, revealing its ability to provide easy to interpret results, in form of programs or functions. Finally, we identify some important challenges in the area.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		11	32		10.1007/s10710-019-09361-5													
J								Applications of genetic programming to finance and economics: past, present, future	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Finance; Economics; Quantitative trading; Genetic programming	EXTENSIVE EVALUATION; RAINFALL PREDICTION; MARKET; SELECTION; ALGORITHMS; CONTEXT; TRADERS; MODELS	While the origins of genetic programming (GP) stretch back over 50 years, the field of GP was invigorated by John Koza's popularisation of the methodology in the 1990s. A particular feature of the GP literature since then has been a strong interest in the application of GP to real-world problem domains. One application domain which has attracted significant attention is that of finance and economics, with several hundred papers from this subfield being listed in the GP bibliography. In this article we outline why finance and economics has been a popular application area for GP and briefly indicate the wide span of this work. However, despite this research effort there is relatively scant evidence of the usage of GP by the mainstream finance community in academia or industry. We speculate why this may be the case, describe what is needed to make this research more relevant from a finance perspective, and suggest some future directions for the application of GP in finance and economics.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		33	53		10.1007/s10710-019-09359-z													
J								Evolutionary music: applying evolutionary computation to the art of creating music	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Music composition; Evolutionary computation; Computational creativity; Review	GENETIC ALGORITHM; GENERATIVE MUSIC; OPTIMIZATION; MELOMICS; SYSTEM; LAW	We present a review of the application of genetic programming (GP) and other variations of evolutionary computation (EC) to the creative art of music composition. Throughout the development of EC methods, since the early 1990s, a small number of researchers have considered aesthetic problems such as the act of composing music alongside other more traditional problem domains. Over the years, interest in these aesthetic or artistic domains has grown significantly. We review the implementation of GP and EC for music composition in terms of the compositional task undertaken, the algorithm used, the representation of the individuals and the fitness measure employed. In these aesthetic studies we note that there are more variations or generalisations in the algorithmic implementation in comparison to traditional GP experiments; even if GP is not explicitly stated, many studies use representations that are distinctly GP-like. We determine that there is no single compositional challenge and no single best evolutionary method with which to approach the act of music composition. We consider autonomous composition as a computationally creative act and investigate the suitability of EC methods to the search for creativity. We conclude that the exploratory nature of evolutionary methods are highly appropriate for a wide variety of compositional tasks and propose that the development and study of GP and EC methods on creative tasks such as music composition should be encouraged.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		55	85		10.1007/s10710-020-09380-7													
J								The impact of genetic programming in education	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Education; Intelligent tutoring systems; Pedagogical agents; Learning analytics	INTELLIGENT TUTORING SYSTEMS; PREDICTION; ALGORITHM; FEEDBACK	Since its inception genetic programming, and later variations such as grammar-based genetic programming and grammatical evolution, have contributed to various domains such as classification, image processing, search-based software engineering, amongst others. This paper examines the role that genetic programming has played in education. The paper firstly provides an overview of the impact that genetic programming has had in teaching and learning. The use of genetic programming in intelligent tutoring systems, predicting student performance and designing learning environments is examined. A critical analysis of genetic programming in education is provided. The paper then examines future directions of research and challenges in the application of genetic programming in education.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		87	97		10.1007/s10710-019-09362-4													
J								Genetic programming in the steelmaking industry	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Review; Steelmaking industry; Practical applications; Economic effects; Future applications	ARTIFICIAL NEURAL-NETWORK; ELECTRIC-ARC FURNACE; BLAST-FURNACE; MECHANICAL-PROPERTIES; HOT METAL; SINTER REDUCIBILITY; STEEL; PREDICTION; STRENGTH; SURFACE	Genetic programming is a powerful, robust and versatile tool that is suitable for predicting and forecasting, especially in the steelmaking industry, where the diversity of serial production processes and equipment strongly influence final product properties, quality and price. The article reviews a wide spectrum of implementation attempts of genetic programing in the steelmaking industry, including real practical applications where direct economic effects can be easily established. The article also presents remaining challenges.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		99	128		10.1007/s10710-020-09382-5													
J								Cartesian genetic programming: its status and future	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Cartesian genetic programming; Genetic programming; Evolutionary algorithms	NEURAL-NETWORKS; BOOLEAN FUNCTIONS; EVOLUTION; CLASSIFICATION; ACQUISITION; NEUTRALITY; DIAGNOSIS; MODULES; DESIGN	Cartesian genetic programming, a well-established method of genetic programming, is approximately 20 years old. It represents solutions to computational problems as graphs. Its genetic encoding includes explicitly redundant genes which are well-known to assist in effective evolutionary search. In this article, we review and compare many of the important aspects of the method and findings discussed since its inception. In the process, we make many suggestions for further work which could improve the efficiency of the CGP for solving computational problems.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		129	168		10.1007/s10710-019-09360-6													
J								Genetic programming theory and practice: a fifteen-year trajectory	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Evolutionary algorithms; GPTP		The GPTP workshop series, which began in 2003, has served over the years as a focal meeting for genetic programming (GP) researchers. As such, we think it provides an excellent source for studying the development of GP over the past fifteen years. We thus present herein a trajectory of the thematic developments in the field of GP.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		169	179		10.1007/s10710-019-09353-5													
J								Genetic programming in the twenty-first century: a bibliometric and content-based analysis from both sides of the fence	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Bibliometrics; Topic modeling; Literature review; Publication habits	SCIENCE; SCOPUS; WEB	In this work we present an extensive bibliometric and content-based analysis of the scientific literature about genetic programming in the twenty-first century. Our work has two key peculiarities. First, we revealed the topics emerging from the literature based on an unsupervised analysis of the textual content of titles and abstracts. Second, we executed all of our analyses twice, once on the papers published in the venues that are typical of the evolutionary computation research community and once on those published in all the other venues. This view from "both sides of the fence" allows us to gain broader and deeper insights into the actual contributions of our community.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		181	204		10.1007/s10710-019-09363-3													
J								Genetic programming and evolvable machines at 20	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Artificial intelligence; Genetic programming; Evolvable hardware; Robotics; Internet traffic analysis; GP bibliometric analysis		The journal and in particular the resource reviews have been running for 20 years. We summarise the GP literature, including top papers and authors, as seen by users of the genetic programming bibliography. Then revisit our original goals for GPEM book reviews and compare them with what has achieved.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		205	217		10.1007/s10710-019-09344-6													
J								Adversarial genetic programming for cyber security: a rising application domain where GP matters	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Coevolutionary algorithms; Cyber Security	COMPETITIVE COEVOLUTION; BOTNET DETECTION; EVOLUTIONARY; BEHAVIOR; GENERATION; SEARCH	Cyber security adversaries and engagements are ubiquitous and ceaseless. We delineate Adversarial Genetic Programming for Cyber Security, a research topic that, by means of genetic programming (GP), replicates and studies the behavior of cyber adversaries and the dynamics of their engagements. Adversarial Genetic Programming for Cyber Security encompasses extant and immediate research efforts in a vital problem domain, arguably occupying a position at the frontier where GP matters. Additionally, it prompts research questions around evolving complex behavior by expressing different abstractions with GP and opportunities to reconnect to the machine learning, artificial life, agent-based modeling and cyber security communities. We present a framework called RIVALS which supports the study of network security arms races. Its goal is to elucidate the dynamics of cyber networks under attack by computationally modeling and simulating them.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		219	250		10.1007/s10710-020-09389-y													
J								Automatic programming: The open issue?	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Automatic programming; Genetic programming; Open issue	SOFTWARE; GRAMMAR	Automatic programming, the automatic generation of a computer program given a high-level statement of the program's desired behaviour, is a stated objective of the field of genetic programming. As the general solution to a computational problem is to write a computer program, and given that genetic programming can automatically generate a computer program, researchers in the field of genetic programming refer to its ability to automatically solve problems. Genetic programming has also been described as an "invention machine" that is capable of generating human-competitive solutions. We argue that the majority of success and focus of our field has not actually been as a result of automatic programming. We set out to challenge the genetic programming community to refocus our research towards the objective of automatic programming, and to do so in a manner that embraces a wider perspective encompassing the related fields of, for example, artificial intelligence, machine learning, analytics, optimisation and software engineering.																	1389-2576	1573-7632				JUN	2020	21	1-2			SI		251	262		10.1007/s10710-019-09364-2													
J								Parallel Simulated Annealing with a Greedy Algorithm for Bayesian Network Structure Learning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Simulated annealing; Markov processes; Greedy algorithms; Bayes methods; Search problems; Convergence; Instruction sets; Bayesian networks; structure learning; heuristic search algorithm; parallel structure learning; memoization; simulated annealing with a greedy algorithm	INFORMATION	We present a hybrid algorithm called parallel simulated annealing with a greedy algorithm (PSAGA) to learn Bayesian network structures. This work focuses on simulated annealing and its parallelization with memoization to accelerate the search process. At each step of the local search, a hybrid search method combining simulated annealing with a greedy algorithm was adopted. The proposed PSAGA aims to achieve both the efficiency of parallel search and the effectiveness of a more exhaustive search. The Bayesian Dirichlet equivalence metric was used to determine an optimal structure for PSAGA. The proposed PSAGA was evaluated on seven well-known Bayesian network benchmarks generated at random. We first conducted experiments to evaluate the computational time performance of the proposed parallel search. We then compared PSAGA with existing variants of simulated annealing-based algorithms to evaluate the quality of the learned structure. Overall, the experimental results demonstrate that the proposed PSAGA shows better performance than the alternatives in terms of computational time and accuracy.																	1041-4347	1558-2191				JUN 1	2020	32	6					1157	1166		10.1109/TKDE.2019.2899096													
J								Data mining and machine learning approaches for prediction modelling of schistosomiasis disease vectors Epidemic disease prediction modelling	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Disease prediction modelling; Data imputation; Synthetic data simulation; Schistosomiasis; SMOTE; Incremental transductive approaches	SPATIAL-ANALYSIS; CLIMATE-CHANGE; CLASSIFICATION; PERFORMANCE; IMPUTATION	This research presents viable solutions for prediction modelling of schistosomiasis disease based on vector density. Novel training models proposed in this work aim to address various aspects of interest in the artificial intelligence applications domain. Topics discussed include data imputation, semi-supervised labelling and synthetic instance simulation when using sparse training data. Innovative semi-supervised ensemble learning paradigms are proposed focusing on labelling threshold selection and stringency of classification confidence levels. A regression-correlation combination (RCC) data imputation method is also introduced for handling of partially complete training data. Results presented in this work show data imputation precision improvement over benchmark value replacement using proposed RCC on 70% of test cases. Proposed novel incremental transductive models such as ITSVM have provided interesting findings based on threshold constraints outperforming standard SVM application on 21% of test cases and can be applied with alternative environment-based epidemic disease domains. The proposed incremental transductive ensemble approach model enables the combination of complimentary algorithms to provide labelling for unlabelled vector density instances. Liberal (LTA) and strict training approaches provided varied results with LTA outperforming Stacking ensemble on 29.1% of test cases. Proposed novel synthetic minority over-sampling technique (SMOTE) equilibrium approach has yielded subtle classification performance increases which can be further interrogated to assess classification performance and efficiency relationships with synthetic instance generation.																	1868-8071	1868-808X				JUN	2020	11	6					1159	1178		10.1007/s13042-019-01029-x													
J								Drug sensitivity prediction framework using ensemble and multi-task learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-task learning; Kernel function; Anti-cancer Drug; Drug response prediction; Ensemble learning	CLASSIFICATION; SELECTION	Radiation and hormone level targeted drug therapy are one of the most widely adopted treatment options for different types of cancer. But, due to genetic variations, cancer patients shows heterogeneity towards targeted drug therapies. In such a scenario precision medication necessitates the design of targeted drug therapy for each individual based on their genetic structure. Predictive modeling and drug response data of cancer cells are imperative in designing personalized cancer treatment. Recent advancement in cancer research has produced various pharmacogenomic databases, which further encourages ongoing research in precision medication. In this paper, we have proposed the drug sensitivity prediction framework using ensemble and multi-task learning. The proposed framework successfully maps non-linear relationships among anti-cancer drugs and have modeled their dependency. Further, the proposed framework is validated using publicly available real data sets-GDSC, CCLE, NCI-Dream. The proposed ensemble model shows quite promise in predicting anti-cancer drug response and has achieved lesser mean square error 3.28 (CGP), 0.49 (CCLE) and 0.54 (NCI-DREAM) in comparison to other existing counterparts.																	1868-8071	1868-808X				JUN	2020	11	6					1231	1240		10.1007/s13042-019-01034-0													
J								Sparse graphs using global and local smoothness constraints	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Graph estimation; Sparse graph; Laplacian smoothness; Graph-based semi-supervised learning; Graph-based label inference; Image recognition	ROBUST FACE RECOGNITION; DIMENSIONALITY REDUCTION	Graph-based learning methods are very useful for many machine learning approaches and classification tasks. Constructing an informative graph is one of the most important steps since a graph can significantly affect the final performance of the learning algorithms. Sparse representation is a useful tool in machine learning and pattern recognition area. Recently, it was shown that sparse graphs (sparse representation based graphs) provide a powerful approach to graph-based semi-supervised classification. In this paper, we introduce a new graph construction method that simultaneously provides a sparse graph and integrates manifold constraints on the sparse coefficients without any prior knowledge on the graph or on its similarity matrix. Furthermore, we propose an efficient solution to the optimization problem. The proposed method imposes that the sparse coding vectors of similar samples should be also similar. Different from existing graph construction methods that are based on the use of explicit constraints or a predefined graph matrix, the proposed smoothness constraints on the graph weights implicitly adapt data to the global structure of the estimated graph. A series of experiments conducted on several public image databases shows that the proposed method can outperform many state-of-the-art methods when applied to the problem of graph-based label propagation.																	1868-8071	1868-808X				JUN	2020	11	6					1241	1251		10.1007/s13042-019-01035-z													
J								A self-adaptive preference model based on dynamic feature analysis for interactive portfolio optimization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Feature selection; Random forest; Preference model; Portfolio optimization	EVOLUTIONARY MULTIOBJECTIVE OPTIMIZATION; DECISION-MAKING; GENETIC ALGORITHM; RANDOM FORESTS; DIVERSITY	In financial markets, there are various assets to invest in. Recognizing an investor's preferences is key to selecting a combination of assets that best serves his or her needs. Considering the mean-variance model for the portfolio optimization problem, this paper proposes an interactive multicriteria decision-making method and explores a self-adaptive preference model based on dynamic feature analysis (denoted RFFS-DT) to capture the decision maker (DM)'s complex preferences in the decision-making process. RFFS-DT recognizes the DM's preference impact factor and constructs a preference model. To recognize the impact factors of the DM's preferences, which could change during the decision-making process, three categories of possible features involved in three aspects of the mean-variance model are defined, and a feature selection method based on random forest is designed. Because the DM's preference structure could be unknown a priori, a decision-tree-based preference model is built and updated adaptively according to the DM's preference feedback and the selected features. The effectiveness of RFFS-DT for interactive multicriteria decision making is verified by a series of deliberately designed comparative experiments.																	1868-8071	1868-808X				JUN	2020	11	6					1253	1266		10.1007/s13042-019-01036-y													
J								Graph-based label propagation algorithm for community detection	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Complex network; Community detection; Label propagation algorithm	COMPLEX NETWORKS; MODEL	Community detection is one of the most important topics in complex network analysis. Among a variety of approaches for detecting communities, the label propagation algorithm (LPA) is the simplest and time-efficient approach. However, the original label propagation algorithm is not stable due to the randomness in its propagation process. In this paper, we propose a graph-based label propagation algorithm (GLPA) to detect communities incorporating the node similarity and connectivity information during the propagation of the labels. First, we define node similarity between adjacent nodes, and change each node's label to that of its most similar neighbor node. Based on the label propagation process, GLPA constructs a label propagation graph to get candidate communities. Then, GLPA calculates the connected components of the label propagation graph. Each connected component is treated as a candidate community in the next step. Second, GLPA constructs a weighted graph to obtain final communities, in which each connected component are treated as a super-node, and the number of edges lying between the corresponding components as the weight of edges. We compute the merging factor for each node in the weighted graph and merge super nodes with higher merging factor to its most similar node iteratively to reach the maximum complementary entropy. Compared with 8 other classical community detection algorithms on LFR artificial networks and 12 real world networks, the proposed algorithm GLPA shows preferable performance on stability, NMI, ARI, modularity.																	1868-8071	1868-808X				JUN	2020	11	6					1319	1329		10.1007/s13042-019-01042-0													
J								Online sales prediction via trend alignment-based multitask recurrent neural networks	KNOWLEDGE AND INFORMATION SYSTEMS										Online sales prediction; Recurrent neural networks; Attention mechanism; Time series analysis		While business trends are constantly evolving, the timely prediction of sales volume offers precious information for companies to achieve a healthy balance between supply and demand. In practice, sales prediction is formulated as a time series prediction problem which aims to predict the future sales volume for different products with the observation of various influential factors (e.g. brand, season, discount, etc.) and corresponding historical sales records. To perform accurate sales prediction under the offline setting, we gain insights from the encoder-decoder recurrent neural network (RNN) structure and have proposed a novel framework named TADA (Chen et al., in: ICDM, 2018) to carry out trend alignment with dual-attention, multitask RNNs for sales prediction. However, the sales data accumulates at a fast rate and is updated on a regular basis, rendering it difficult for the trained model to maintain the prediction accuracy with new data. In this light, we further extend the model into TADA(+), which is enhanced by an online learning module based on our innovative similarity-based reservoir. To construct the data reservoir for model retraining, different from most existing random sampling-based reservoir, our similarity-based reservoir selects data samples that are "hard" for the model to mine apparent dynamic patterns. The experimental results on two real-world datasets comprehensively show the superiority of TADA and TADA(+) in both online and offline sales prediction tasks against other state-of-the-art competitors.																	0219-1377	0219-3116				JUN	2020	62	6					2139	2167		10.1007/s10115-019-01404-8													
J								Scalable recovery of missing blocks in time series with high and low cross-correlations	KNOWLEDGE AND INFORMATION SYSTEMS										Recovery of missing blocks; Time series; Centroid decomposition; Correlation	DECOMPOSITION; IMPUTATION	Missing values are very common in real-world data including time-series data. Failures in power, communication or storage can leave occasional blocks of data missing in multiple series, affecting not only real-time monitoring but also compromising the quality of data analysis. Traditional recovery (imputation) techniques often leverage the correlation across time series to recover missing blocks in multiple series. These recovery techniques, however, assume high correlation and fall short in recovering missing blocks when the series exhibit variations in correlation. In this paper, we introduce a novel approach called CDRec to recover large missing blocks in time series with high and low correlations. CDRec relies on the centroid decomposition (CD) technique to recover multiple time series at a time. We also propose and analyze a new algorithm called Incremental Scalable Sign Vector to efficiently compute CD in long time series. We empirically evaluate the accuracy and the efficiency of our recovery technique on several real-world datasets that represent a broad range of applications. The results show that our recovery is orders of magnitude faster than the most accurate algorithm while producing superior results in terms of recovery.																	0219-1377	0219-3116				JUN	2020	62	6					2257	2280		10.1007/s10115-019-01421-7													
J								Incremental community discovery via latent network representation and probabilistic inference	KNOWLEDGE AND INFORMATION SYSTEMS										Community detection; Incremental community detection; Network embedding; Probabilistic inference	ALGORITHM	Most of the community detection algorithms assume that the complete network structure G=(V,E)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\mathcal {G}=(\mathcal {V},\mathcal {E})$$\end{document} is available in advance for analysis. However, in reality this may not be true due to several reasons, such as privacy constraints and restricted access, which result in a partial snapshot of the entire network. In addition, we may be interested in identifying the community information of only a selected subset of nodes (denoted by VT subset of V\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\mathcal {V}_{{\mathrm{T}}} \subseteq \mathcal {V}$$\end{document}), rather than obtaining the community structure of all the nodes in G\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\mathcal {G}$$\end{document}. To this end, we propose an incremental community detection method that repeats two stages-(i) network scan and (ii) community update. In the first stage, our method selects an appropriate node in such a way that the discovery of its local neighborhood structure leads to an accurate community detection in the second stage. We propose a novel criterion, called Information Gain, based on existing network embedding algorithms (Deepwalk and node2vec) to scan a node. The proposed community update stage consists of expectation-maximization and Markov Random Field-based denoising strategy. Experiments with 5 diverse networks with known ground-truth community structure show that our algorithm achieves 10.2% higher accuracy on average over state-of-the-art algorithms for both network scan and community update steps.																	0219-1377	0219-3116				JUN	2020	62	6					2281	2300		10.1007/s10115-019-01422-6													
J								Outsourcing analyses on privacy-protected multivariate categorical data stored in untrusted clouds	KNOWLEDGE AND INFORMATION SYSTEMS										Cloud computing; Data privacy; Data splitting; Nominal data	SEMANTIC SIMILARITY; INFORMATION-CONTENT; COMPUTATION; VARIANCE; SECURITY	Outsourcing data storage and computation to the cloud is appealing due to the cost savings it entails. However, when the data to be outsourced contain private information, appropriate protection mechanisms should be implemented by the data controller. Data splitting, which consists of fragmenting the data and storing them in separate clouds for the sake of privacy preservation, is an interesting alternative to encryption in terms of flexibility and efficiency. However, multivariate analyses on data split among various clouds are challenging, and they are even harder when data are nominal categorical (i.e., textual, non-ordinal), because the standard arithmetic operators cannot be used. In this article, we tackle the problem of outsourcing multivariate analyses on nominal data split over several honest-but-curious clouds. Specifically, we propose several secure protocols to outsource to multiple clouds the computation of a variety of multivariate analyses on nominal categorical data (frequency-based and semantic-based). Our protocols have been designed to outsource as much workload as possible to the clouds, in order to retain the cost-saving benefits of cloud computing while ensuring that the outsourced stay split and hence privacy-protected versus the clouds. The experiments we report on the Amazon cloud service show that by using our protocols the controller can save nearly all the runtime because it can integrate partial results received from the clouds with very little computation.																	0219-1377	0219-3116				JUN	2020	62	6					2301	2326		10.1007/s10115-019-01424-4													
J								Extreme pivots: a pivot selection strategy for faster metric search	KNOWLEDGE AND INFORMATION SYSTEMS										Nearest neighbors search; Pivot-based metric indexes; Extreme pivots	SIMILARITY SEARCH; ALGORITHM; SPACES	This manuscript presents the extreme pivots (EP) metric index, a data structure, to speed up exact proximity searching in the metric space model. For the EP, we designed an automatic rule to select the best pivots for a dataset working on limited memory resources. The net effect is that our approach solves queries efficiently with a small memory footprint, and without a prohibitive construction time. In contrast with other related structures, our performance is achieved automatically without dealing directly with the index's parameters, using optimization techniques over a model of the index. The EP's model is studied in-depth in this contribution. In practical terms, an interested user only needs to provide the available memory and a sample of the query distribution as parameters. The resulting index is quickly built, and has a good trade-off among memory usage, preprocessing, and search time. We provide an extensive experimental comparison with state-of-the-art searching methods. We also carefully compared the performance of metric indexes in several scenarios, firstly with synthetic data to characterize performance as a function of the intrinsic dimension and the size of the database, and also in different real-world datasets with excellent results.																	0219-1377	0219-3116				JUN	2020	62	6					2349	2382		10.1007/s10115-019-01423-5													
J								Missing data imputation using decision trees and fuzzy clustering with iterative learning	KNOWLEDGE AND INFORMATION SYSTEMS										Missing data imputation; Decision trees; Fuzzy clustering; Data mining	GENE-EXPRESSION DATA; VALUES; FRAMEWORK	Various imputation approaches have been proposed to address the issue of missing values in data mining and machine learning applications. To improve the accuracy of missing data imputation, this paper proposes a new method called DIFC by integrating the merits of decision tress and fuzzy clustering into an iterative learning approach. To compare the performance of the DIFC method against five effective imputation methods, extensive experiments are conducted on six widely used datasets with numerical and categorical missing data, and with various amounts and types of missing values. The experimental results show that the DIFC method outperforms other methods in terms of imputation accuracy. Further experiments on the effect of missing value types demonstrate the robustness of the DIFC method in dealing with different types of missing values. This paper contributes to missing data imputation research by providing an accurate and robust method.																	0219-1377	0219-3116				JUN	2020	62	6					2419	2437		10.1007/s10115-019-01427-1													
J								English-Arabic collocation extraction to enhance Arabic collocation identification	KNOWLEDGE AND INFORMATION SYSTEMS										Monolingual collocation; Bilingual collocation LSA; Word embeddings; Skip-gram; Association measure		Bilingual collocation extraction could improve the performance of monolingual extraction. This is especially true for the English-Arabic pair, as difficulties of Arabic collocation extraction can be overcome. We present in this paper two novel approaches for extracting both monolingual and bilingual collocations. The monolingual extraction approach is hybrid, based on linguistic patterns and statistical measures. We propose during statistical filtering to combine vector-based measures with different association measures via a voting procedure. The bilingual extraction capitalizes on different cues (position, frequency, cross-language correspondence between POS-patterns, distribution, translation). It allows enhancing the monolingual collocation extraction by considering not only collocation equivalents with direct translation. Indeed, it can validate unconfirmed collocations because they translate confirmed ones. The results showed, in particular, how the extraction of Arabic collocations can be improved by extracting English-Arabic ones. The precision of extracting Arabic collocations moved upward, respectively, from about 86 to 96%.																	0219-1377	0219-3116				JUN	2020	62	6					2439	2459		10.1007/s10115-019-01428-0													
J								Manufacturer driven strategic coordination as a response to "showrooming"	DECISION SUPPORT SYSTEMS										Multi-channel retail; Supply chain management; Showrooming; Game theory; Contract	CHANNEL SUPPLY CHAIN; PRICE; COMPETITION; CONTRACTS; ONLINE; COST	"Showrooming", a practice where consumers visit a brick-and-mortar store to examine and research a product before buying it online, is being increasingly observed in recent times. This not only adversely affects the physical store but also the manufacturer in the long run. Showrooming leads to reduced sales efforts on the part of the brick-and-mortar retailer which leads to a decline in showcasing of the product to consumers. This affects the overall demand for the product in the market. In this paper, we analyze the effect of wholesale prices set by the manufacturer on the retail prices of the products in a multi-channel environment affected by showrooming. To combat the adverse impact of showrooming, we further investigate the feasibility of a manufacturer-driven alliance with the brick-and-mortar retailer so that it expends adequate sales effort that leads to higher demand and a dedicated consumer base. We derive a three-parameter contract that can coordinate the channel and arrive at a win-win situation for the manufacturer and the brick-and-mortar retailer. Our analysis shows that the contract is more beneficial for a brick-and-mortar retailer with lower relative market potential. Additionally, we find that the contract brings down the retail price of the product which benefits the end consumers.																	0167-9236	1873-5797				JUN	2020	133								113305	10.1016/j.dss.2020.113305													
J								An examination of the effect of recent phishing encounters on phishing susceptibility	DECISION SUPPORT SYSTEMS										Phishing; Susceptibility; Decision making; Detection process; Outcome failure	DECISION-MAKING; DIFFICULTY; AVOIDANCE; FRAMEWORK; MODEL; VULNERABILITY; FAMILIARITY; INTUITION; ATTACKS; FAILURE	This paper examines online users' perceived susceptibility to phishing attacks. We posit that an individual's phishing susceptibility may be shaped by recent phishing encounters and, more importantly, that the effect of new experience on susceptibility will be heterogeneous among users. To facilitate our investigation, we focus on both the process and outcome of phishing detection. Survey data from college students confirms that one's susceptibility is affected by detection process difficulty and detection outcome failures in the recent phishing encounter. Results also reveal the importance of personal attributes, such as past success in phishing detection and phishing desensitization, in regulating the effects of a recent phishing encounter. Finally, results show the relationship between detection process difficulty and outcome failures, in addition to confirming antecedents to the two detection components. Our research generates new knowledge that contributes to the phishing literature and it also sheds new insights that inform practitioners, although the use of college students limits the generalizability of the current findings.																	0167-9236	1873-5797				JUN	2020	133								113287	10.1016/j.dss.2020.113287													
J								A new emergency decision support system: the automatic interpretation and contextualisation of events to model a crisis situation in real-time	DECISION SUPPORT SYSTEMS										Information system; Emergency decision support system; Complex event processing; Big Data; Situation awareness; Crisis management	INFORMATION FUSION; ONTOLOGY; INTEROPERABILITY; MANAGEMENT; PLATFORM; KNOWLEDGE; AWARENESS; AGILITY	This paper studies, designs and implements a new type of emergency decision support system that aims to improve the decision-making of emergency managers in crisis situations by connecting them to new, multiple data sources. The system combines event-driven and model-driven architectures and is dedicated to crisis cells. After its implementation, the system is evaluated using a realistic crisis scenario, in terms of its user interfaces, its ability to interpret data in real time and its ability to manage the 4Vs of Big Data. The input events correspond to traffic measurements, water levels, water flows, water predictions and flow predictions made available by French official services. The main contributions of this study are: (i) the connection between a complex event processing engine and a graph database containing the model of the crisis situation and (ii) the continuous updating of a common operational picture for the benefit of emergency managers. This study could be used as a framework for future research works on decision support systems facing complex, evolving situations.																	0167-9236	1873-5797				JUN	2020	133								113260	10.1016/j.dss.2020.113260													
J								Customer-centric prioritization of process improvement projects	DECISION SUPPORT SYSTEMS										Business process management; Business process improvement; Process decision-making; Customer centricity; Project portfolio selection; Kano model	DESIGN SCIENCE RESEARCH; BUSINESS PROCESS; PROCESS MANAGEMENT; SATISFACTION; PERFORMANCE; IMPACT; MODEL	Today, customers can conveniently compare products and decide how to interact with companies. With customer centricity becoming an important success factor, companies must drive customer satisfaction not only through excellent products but also through customer-centric processes. As many companies face an abundance of action possibilities, fast-changing customer needs, and scarce resources, guidance regarding the customercentric prioritization of process improvement projects is in high need. As existing approaches predominantly focus on process efficiency, we propose a decision model that accounts for the effects of process improvement on customer centricity in line with justificatory knowledge on value-based process decision-making, project portfolio selection, and the measurement of customer satisfaction. When building the decision model, we adopted the design science paradigm and used multi-criteria decision analysis as well as normative analytical modeling as research methods. We evaluated the model by discussing it with practitioners, by building a software prototype, and by applying it at a German insurance company. Overall, our research extends the prescriptive knowledge on process prioritization and customer process management.																	0167-9236	1873-5797				JUN	2020	133								113286	10.1016/j.dss.2020.113286													
J								How IT wisdom affects firm performance: An empirical investigation of 15-year US panel data	DECISION SUPPORT SYSTEMS										IT wisdom; Meta-capability; IT-enabled organizational capabilities perspective; Firm performance; IT business value	INFORMATION-TECHNOLOGY CAPABILITY; COMPETITIVE ADVANTAGE; ORGANIZATIONAL CAPABILITY; INNOVATION PERFORMANCE; STRATEGY; SYSTEMS; RESOURCES; BUSINESS; FLEXIBILITY; ANTECEDENTS	This paper introduces the concept of IT wisdom from a perspective of meta-capability and establishes the relationship between IT wisdom and firm performance. We propose that IT wisdom as a holistic meta-IT capability of a firm can strategically enable and exercise suitable IT capabilities and organizational capabilities to adapt to changing environments for improving firm performance. We test our hypotheses with panel data collected from 26 US companies over 15 years. The results of data analysis show that IT wisdom is significantly related to firm performance indicated by both accounting-based measures and market-based measures. Theoretical and practical implications of the findings are presented.																	0167-9236	1873-5797				JUN	2020	133								113300	10.1016/j.dss.2020.113300													
J								A text analytics framework for understanding the relationships among host self-description, trust perception and purchase behavior on Airbnb	DECISION SUPPORT SYSTEMS										Self-description; Trust perception; Purchase behavior; Airbnb; Text mining	SHARING ECONOMY; ONLINE; REPUTATION; TRUSTWORTHINESS; PERSPECTIVE; ANTECEDENTS; REVIEWS; SYSTEMS; SERVICE; IMPACT	Trust plays an important role in sharing transactions on short-term rental platforms. However, the impact of host self-description on trust perception and whether trust perception can influence purchase behavior remain understudied. Therefore, a text analytics framework was proposed to research the relationships among host self-description, trust perception and purchase behavior on Airbnb. Specifically, a deep-learning-based method was designed to automatically code trust perception of host self-descriptions. And the linguistic and semantic features of description texts were extracted with text mining methods. The estimated order quantity was used to quantify purchase behavior. Then, the influence of linguistic and semantic features on trust perception was identified, and the relationship between trust perception and purchase behavior was also verified. The empirical analysis derives the following findings: i. The readability of self-description is positively associated with trust perception; ii. Perspective taking expressed in self-description is also helpful; iii. Excessive positive sentiment expression can raise barriers to trust building; iv. Paying more attention to family relationship, openness, service and travel experience in self-description would be helpful; v. Trust perception can promote purchases. These findings can help hosts write better self-description, which contributes to trust building and purchases on short-term rental platforms.																	0167-9236	1873-5797				JUN	2020	133								113288	10.1016/j.dss.2020.113288													
J								Design of building construction safety prediction model based on optimized BP neural network algorithm	SOFT COMPUTING										Risk factors; Genetic algorithm; BP network; Safety prediction	ROUGH SET-THEORY	In order to solve the safety problem of the construction industry, the construction safety prediction model based on the optimized BP neural network algorithm is designed in this study. First, the characteristics of the construction industry were analyzed. As a labor-intensive industry, the construction industry is characterized by numerous factors such as large investment, long construction period and complicated construction environment. Due to the increasingly serious security problem, widespread concern over such problem has been aroused in society. Second, the problem of building construction safety management was summarized, six influencing factors were explored and a building construction safety prediction model based on rough set-genetic-BP neural network was established. Finally, the model was validated by a combination of multiparty consultation, empirical analysis and model comparison. The results showed that the model accurately predicted the risk factors during the construction process and effectively reduced casualties. Therefore, the model is feasible, effective and accurate.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7839	7850		10.1007/s00500-019-03917-4													
J								Exploration of stock index change prediction model based on the combination of principal component analysis and artificial neural network	SOFT COMPUTING										Stock price forecasting; Investment guidance; Principal component analysis; Bayesian regularization algorithm; BP neural network	REGULARIZATION	In order to establish an accurate effective stock forecasting model, the principal component analysis (PCA) was first used to analyze the main financial index data of some listed companies and the comprehensive score of evaluation index was obtained in this study. Then, the financial indicator data and the transaction indicator data were simultaneously used as the input variables of the stock price prediction research, three back propagation (BP) neural network algorithms were used for experiment, and its prediction situation was compared. Results show that the BP neural network based on Bayesian regularization algorithm has the highest prediction accuracy and can avoid over-fitting phenomenon in the training process of the model; the error between the predicted value and the actual value is small. Finally, this study constructed a stock price prediction study based on PCA and BP neural network algorithm as well as an investment stock selection strategy based on traditional stock selection analysis method. As a result, the proposed model is proved to be effective.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7851	7860		10.1007/s00500-019-03918-3													
J								Design of visual communication based on deep learning approaches	SOFT COMPUTING										Deep learning; Hierarchical semantics; Image recognition; Image retrieval; Scene analysis; Convolutional neural network; Spatial pyramid pooling	ANALYTICS; NETWORK; VISION	Aiming at the problem of object recognition caused by small object scale, multi-interaction (occlusion), and strong hiding characteristics in the scene analysis task, an object-region-enhanced network based on deep learning was proposed. The network integrated two core modules designed for the task: object area enhancement strategy and black-hole-filling strategy. The former directly corresponded the object region with high semantic confidence to the local region of the specific category channel of the convolutional feature image. Weighted features were used to improve contextual relationships, and difficult object regions were identified. The latter avoided the mistake of identifying some difficult areas as additional background classes by masking additional background classes. The results showed that the modular design scheme improved the overall parsing performance of the model by replacing the modules, and the two strategies were applied to other existing scenario parsing networks. A unified framework is proposed for handling scene resolution tasks. Benefiting from the modular design approach, the proposed algorithm improves overall performance by replacing convolution or detection modules. Object enhancement and black hole filling are applied to other systems to improve the system's ability to parse objects. Object area enhancement methods are used to recall objects that are not recognized in a standard split network. Black hole fill techniques can be used to resolve pixels that are incorrectly categorized into additional background classes that do not exist. Therefore, a variety of contextual semantic fusion strategies have certain reference value in the theoretical level of computer vision. More critically, this method has certain reference significance for the design and development of robust and practical application systems.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7861	7872		10.1007/s00500-019-03954-z													
J								Exploration of artistic creation of Chinese ink style painting based on deep learning framework and convolutional neural network model	SOFT COMPUTING										Deep learning; Convolutional neural network; Ink style rendering	CLASSIFICATION	For the purpose of applying information technology to the creation of ink style painting, the algorithm of ink painting rendering based on the deep learning framework and convolutional neural network model is designed and improved. Firstly, the ink style rendering program is written in Python. Secondly, VGG under Caffe architecture and Illustration 2Vec models are transplanted to TensorFlow architecture, and the image is rendered in ink style based on deep learning framework and convolutional neural network model. Finally, based on Node.js, the server-side program for image ink style rendering is built. Among them, Express is adopted as the Web-side framework, and the front-end page effect is completed. The results show that the ink rendering logic program is applicable, and the expected purpose is achieved.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7873	7884		10.1007/s00500-019-03985-6													
J								Robust optimization and mixed-integer linear programming model for LNG supply chain planning problem	SOFT COMPUTING										LNG supply; Liquefied gas sales planning; Robust optimization; Cuckoo optimization algorithm (COA); Optimal robustness level	VENDOR SELECTION; PROCUREMENT; SYSTEM; PRICE	A constant development of gas utilization in domestic households, industry, and power plants has slowly transformed gaseous petrol into a noteworthy wellspring of energy. Supply and transportation planning of liquefied natural gas (LNG) need a great attention from the management of the supply chain to provide a significant development of gas trading. Therefore, this paper addresses a robust mixed-integer linear programming model for LNG sales planning over a given time horizon aiming to minimize the costs of the vendor. Since the parameter of the manufacturer supply has an uncertain nature in the real world, and this parameter is regarded to be interval-based uncertain. To validate the model, various illustrative examples are solved using CPLEX solver of GAMS software under different uncertainty levels. Furthermore, a novel metaheuristic algorithm, namely cuckoo optimization algorithm (COA), is designed to solve the problem efficiently. The obtained comparison results demonstrate that the proposed COA can generate high-quality solutions. Furthermore, the comparison results of the deterministic and robust models are evaluated, and sensitivity analyses are performed on the main parameters to provide the concluding remarks and managerial insights of the research. Finally, a comparison evaluation is done between the total vendor profit and the robustness cost to find the optimal robustness level.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7885	7905		10.1007/s00500-019-04010-6													
J								Recognizing important factors of influencing trust in O2O models: an example of OpenTable	SOFT COMPUTING										O2O; Trust; Sentiment classification; Feature selection; SVM-RFE; LASSO	TEXT MINING APPROACH; FEATURE-SELECTION; SENTIMENT ANALYSIS; SVM-RFE; ONLINE; CLASSIFICATION; REVIEWS; SEGMENTATION; SATISFACTION; ORIENTATION	Online-to-offline/offline-to-online (O2O) business models have attracted lots of enterprisers to enter this market. In such a fast-growing competition, some studies indicated that lack of trust will bring a great damage to O2O business. Related works already confirm trust is the key factor to the success of O2O. Besides, social media has been changing the way providers communicate with consumers. Negative comments in social media will decrease the consumers' trust to O2O companies and platforms. Available O2O studies are almost always conducted by means of questionnaires or interviews, which cannot provide immediate customer response and require a lot of manpower and time. Since online reviews are the main information source for consumers. Therefore, this study presented a text mining-based scheme which uses text mining technique to find important factors from online electronic word-of-mouth, to replace the traditional questionnaire survey method of collecting data. Two feature selection methods, Support Vector Machines Recursive Feature Elimination and Least Absolute Shrinkage and Selection Operator have employed to select important factors that affect O2O trust. We also evaluate the performance of extracted feature subsets by Support Vector Machines. The findings can be referenced for O2O market enterprises to carefully response customers' comments to avoid hurting customers' trust and improve service quality.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7907	7923		10.1007/s00500-019-04019-x													
J								Building the electronic evidence analysis model based on association rule mining and FP-growth algorithm	SOFT COMPUTING										Criminal procedure law; Association rules; Electronic evidence analysis; FP-growth algorithm	INFORMATION	In China's criminal procedure law, electronic data is a kind of independent evidence. With the development of big data technology, more and more attention has been paid to the examination and application of electronic evidence in criminal trials. In order to obtain hidden knowledge from confused electronic evidence, an electronic evidence analysis model based on data mining is proposed. The main research is to apply the association rule technology of data mining to the analysis of electronic evidence, analyze the shortcomings of the existing association rule mining algorithm, and put forward the improved algorithm of the existing algorithm and a new idea of the algorithm. Based on FP-growth algorithm, an improved algorithm (ISPO-tree algorithm) is put forward and the theoretical proof is given. This algorithm only needs to browse the database once and adds the function of supporting a small amount of modified evidence. This algorithm improves the time efficiency of data pre-processing by making similar rules to make unequal attribute values equal and can mine more association rules under the optimum conditions of support and redundancy, and it improves the effectiveness of electronic evidence and the accuracy of criminal trial.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7925	7936		10.1007/s00500-019-04032-0													
J								Constructing rural e-commerce logistics model based on ant colony algorithm and artificial intelligence method	SOFT COMPUTING										Ant colony algorithm; Rural; e-commerce; Logistics model; Artificial intelligence	OPTIMIZATION	In order to study the application of ant colony algorithm in rural e-commerce logistics mode, the third-party distribution model was adopted as the logistics method with Xuzhou City's fruit and vegetable agricultural products as the key research object. Through the model construction, the problem of the third-party distribution model is analyzed. Based on the ant colony algorithm, the shortest path and cost are calculated using MATLAB software. The cost and efficiency problems under different variables are analyzed and the model evaluation is carried out. Finally, the difficulties at the urban and rural end of the third-party distribution model are solved. The results show that the method improves the distribution efficiency and reduces the logistics cost. Therefore, it is conducive to the advancement of rural e-commerce.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7937	7946		10.1007/s00500-019-04046-8													
J								Hybridized neural network and decision tree based classifier for prognostic decision making in breast cancers	SOFT COMPUTING										Radial basis function neural network; Decision trees; Breast cancer; Soft computing	GENETIC ALGORITHM; SEGMENTATION; ENHANCEMENT; MORTALITY; FEATURES	Artificial intelligence techniques and algorithms are applied at various fields such as face recognition, self-driving cars, industrial robots and health care. These real-world conundrums are solved employing artificial intelligence since it focuses on narrow tasks, and AI-driven tasks are very reliable and efficient because of its automated problem-solving techniques. Breast cancer is considered as the most common type of cancer among women. The well-known technique for detection of breast cancer is mammography which can diagnosis anomalies and determine cancerous cells. However, in the present breast cancer screenings, the retrospective studies reveal that approximately 20-40% of breast cancer cases are missed by radiologists. The main objective of the proposed algorithm is to exactly forecast the misclassified malignant cancers employing radial basis function network and decision tree. In order to obtain the effective classification algorithm, this work is compared with three widely employed algorithms, namely K-nearest neighbors, support vector machine and Naive Bayes algorithm, and the proposed algorithm achieves a high accuracy.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7947	7953		10.1007/s00500-019-04066-4													
J								On construction of the air pollution monitoring service with a hybrid database converter	SOFT COMPUTING										Cloud computing; Air pollution; Big data; NoSQL; Hybrid database system	CHALLENGES; INTERNET	Air pollution has a severe impact on human health, pollution harms human health, and people start to pay attention to how to use the monitoring system in real-time recording for analysis. To maintain smooth monitoring and analysis, we have to manage the historical data separated from the incoming data. Historical data is used when we need to analyze the data, while real-time incoming data is used to visualize the real condition. To achieve this objective, we need to collect real-time data from environmental protection open data resource. However, the data might grow faster and become huge; in this case, the relational database was not designed to process a large amount of data. Therefore, we require a database technology that can handle massive volume data, that is not only Structured Query Language (NoSQL). This method raises an important point regarding how to dump the data to NoSQL without change relational database (RDB) system. Accordingly, this paper proposed an air pollution monitoring system combines Hadoop cluster to dump data from RDB to NoSQL and data backup. By this way, it will not only reduce the performance of RDB loading but also keep the service status. Dump data to NoSQL need to process without affecting the real-time monitoring of air pollution monitoring system. In this part, we focus on without interruption web service, and it can be up to 60%, through optimizing it with dump method and backup data service, MapReduce can restart the service and distribute the database when RDB is impairing. Besides that, through three different types of conversion mode, we can get the best data conversion in our system. Finally, air pollution monitoring service provides a variant of air pollution factor as an essential basis of environment detection and analysis to serve people living in a more comfortable environment.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7955	7975		10.1007/s00500-019-04079-z													
J								A high-performance CNN method for offline handwritten Chinese character recognition and visualization	SOFT COMPUTING										Handwritten Chinese character recognition; Convolutional neural network; Global average pooling; Class activation maps	NEURAL-NETWORKS; ONLINE	Recent researches introduced fast, compact and efficient convolutional neural networks (CNNs) for offline handwritten Chinese character recognition (HCCR). However, many of them did not address the problem of network interpretability. We propose a new architecture of a deep CNN with high recognition performance which is capable of learning deep features for visualization. A special characteristic of our model is the bottleneck layers which enable us to retain its expressiveness while reducing the number of multiply-accumulate operations and the required storage. We introduce a modification of global weighted average pooling (GWAP)-global weighted output average pooling (GWOAP). This paper demonstrates how they allow us to calculate class activation maps (CAMs) in order to indicate the most relevant input character image regions used by our CNN to identify a certain class. Evaluating on the ICDAR-2013 offline HCCR competition dataset, we show that our model enables a relative 0.83% error reduction while having 49% fewer parameters and the same computational cost compared to the current state-of-the-art single-network method trained only on handwritten data. Our solution outperforms even recent residual learning approaches.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7977	7987		10.1007/s00500-019-04083-3													
J								Constructing the regional intelligent economic decision support system based on fuzzy C-mean clustering algorithm	SOFT COMPUTING										Decision support system (DSS); Fuzzy C-mean clustering; Economic forecasting	MANAGEMENT	In order to help decision-making departments to know the information of regional economic development in time, the economic prediction and early warning model and economic policy simulation model of the decision-making system are constructed by analyzing the influencing factors of regional economy and taking the fuzzy C-means clustering as the main algorithm. Based on the data in the statistical yearbook, the corresponding module functions of the system are completed. Then, the system is applied to predict the recent economic development indicators. The results show that the forecasting data and the real data of the indicators are on the same level as a whole. When the growth rate of foreign trade export, fixed assets investment, fiscal expenditure and total retail sales of social consumer goods are increased by 1 percentage point, respectively, the GDP growth rate will increase by 0.105, 0.113, 0.134, 0.087 and 0.075 percentage points. The research suggests that the support system basically meets the requirements, can achieve the purpose of prediction, and has certain practicability.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7989	7997		10.1007/s00500-019-04091-3													
J								Auto-encoder-based generative models for data augmentation on regression problems	SOFT COMPUTING										Generative models; Multi-task learning; Variational auto-encoders; Multivariable linear regression; Materials informatics	NOISE	Recently, auto-encoder-based generative models have been widely used successfully for image processing. However, there are few studies on the realization of continuous input-output mappings for regression problems. Lack of a sufficient amount of training data plagues regression problems, which is also a notable problem in machine learning, which affects its application in the field of materials science. Using variational auto-encoders (VAEs) as generative models for data augmentation, we address the issue of small data size for regression problems. VAEs are popular and powerful auto-encoder-based generative models. Generative auto-encoder models such as VAEs use multilayer neural networks to generate sample data. In this study, we demonstrate the effectiveness of multi-task learning (auto-encoding and regression tasks) relating to regression problems. We conducted experiments on seven benchmark datasets and on one ionic conductivity dataset as an application in materials science. The experimental results show that the multi-task learning for VAEs improved the generalization performance of multivariable linear regression model trained with augmented data.																	1432-7643	1433-7479				JUN	2020	24	11			SI		7999	8009		10.1007/s00500-019-04094-0													
J								Analysis of multi-level capital market linkage driven by artificial intelligence and deep learning methods	SOFT COMPUTING										Deep learning algorithm; Artificial intelligence; Capital market		Artificial intelligence has a very wide application in modern society. Deep learning algorithms promote the development of artificial intelligence industry. The development of artificial intelligence industry needs the support of capital market. As far as the relationship between the three is concerned, the artificial intelligence industry is in the middle position. In short, the computer deep learning algorithm is the technical basis for the development of artificial intelligence, and the capital market is an important guarantee for the development of the artificial intelligence industry. However, under the current development background of China, the capital market is far from meeting the improvement and development of high-tech industries. Therefore, in order to further clarify the important role of deep learning algorithms in the development of artificial intelligence and capital markets, this paper systematically analyzes these three relationships and proposes measures from the perspective of supporting theoretical research and development and improving capital markets.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8011	8019		10.1007/s00500-019-04095-z													
J								Online sequential pattern mining and association discovery by advanced artificial intelligence and machine learning techniques	SOFT COMPUTING										Multi-scale representation; Manifold clustering; Local modeling; Kernel machine; Time-frequency analysis	TECHNICAL ANALYSIS; NEURAL-NETWORKS; HEDGE RATIO; STOCK; MODEL; INDEX; FUTURES; OIL; INTEGRATION; INVESTMENT	With the advances in information science, vast amounts of financial time series data can been collected and analyzed. In modern time series analysis, sequential pattern mining (SPM) and association discovery (AD) are the most important techniques to predict the future trends. This study aims at developing advanced SPM and AD for financial data by cutting edge techniques from artificial intelligence and machine learning. The nonlinearity and non-stationarity of financial time series dynamics pose a major challenge for SPM and AD. This study employs time-frequency analysis to extract features for SPM. Then, a sparse multi-manifold clustering (SMMC) is used to partition the feature space into several disjointed regions for better AD. Finally, local relevance vector machines (RVMs) are employed for AD and perform the forecasting. Different from traditional methods, the novel forecasting system operates on multiple resolutions and multiple dynamic regimes. SMMC finds both the neighbors and the weights automatically by a sparse solution, which approximately spans a low-dimensional affine subspace at that point. RVM, the Bayesian kernel machines, can produce parsimonious models with excellent generalization properties. Taking multiple time series data from financial markets as an example, the empirical results demonstrate that the proposed model outperforms traditional models and significantly reduces the forecasting errors. The framework is effective and suitable for other time series forecasting.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8021	8039		10.1007/s00500-019-04100-5													
J								Application of cloud-based visual communication design in Internet of Things image	SOFT COMPUTING										Cloud computing; Visual communication design; Internet of Things image; Hadoop	BIG DATA; GRAPHIC-DESIGN	In order to apply cloud computing to study visual communication design, high-resolution remote sensing image features and applications were analyzed. A high-resolution remote sensing image storage model in the cloud computing environment was designed. Afterward, deep analysis and comparison were made toward the current mainstream cloud platform. Based on the characteristics of high-resolution remote sensing images, the key technologies in the design of high-resolution remote sensing image storage model in cloud computing environment were discussed. The results showed that the integration of cloud platform Hadoop and virtualization management cloud platform provided corresponding transparent use of distributed computing resources for different remote sensing image applications. Therefore, this method provides a new way for other industries to apply cloud computing environments.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8041	8050		10.1007/s00500-019-04111-2													
J								Developing GAHP concepts for measurement of travel agency organizational performance	SOFT COMPUTING										Organizational performance; Travel agency; Measurement model; Analytic hierarchy process; Grey relational analysis	GREY; DECISION	A model for organizational performance evaluation is proposed. Organizational performance support can reduce barriers to applications of multi-criteria decision-making procedures. The relevant criteria of the proposed model were derived from an expert group interview to include the analytic hierarchy process (AHP), which is performed to determine the weights of evaluation, and grey relational analysis (GRA), which is then performed to rank the organizational performance of the travel agency. The AHP is a measurement theory that prioritizes the hierarchy and consistency of judgmental data provided by a group of decision-makers. It incorporates the evaluations of all decision-makers into a final decision through pair-wise comparisons of the alternatives, without eliciting their utility functions on subjective and objective criteria. GRA examines the extent of connections between two digits through departing and scattering measurement methods. The combined AHP and GRA decision-making method can provide decision-makers with a valuable data analysis when evaluating organizational performance. Because the proposed model can inform organizational performance assessments, it is highly applicable to both academic and commercial organizational evaluation and development.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8051	8059		10.1007/s00500-019-04115-y													
J								ArWordVec: efficient word embedding models for Arabic tweets	SOFT COMPUTING										ArWordVec; Natural language processing; Word embeddings; Deep convolution neural networks; Arabic tweets		One of the major advances in artificial intelligence nowadays is to understand, process and utilize the humans' natural language. This has been achieved by employing the different natural language processing (NLP) techniques along with the aid of the various deep learning approaches and architectures. Using the distributed word representations to substitute the traditional bag-of-words approach has been utilized very efficiently in the last years for many NLP tasks. In this paper, we present the detailed steps of building a set of efficient word embedding models called ArWordVec that are generated from a huge repository of Arabic tweets. In addition, a new method for measuring Arabic word similarity is introduced that has been used in evaluating the performance of the generated ArWordVec models. The experimental results show that the performance of the ArWordVec models overcomes the recently available models on Arabic Twitter data for the word similarity task. In addition, two of the large Arabic tweets datasets are used to examine the performance of the proposed models in the multi-class sentiment analysis task. The results show that the proposed models are very efficient and help in achieving a classification accuracy ratio exceeding 73.86% with a high average F1 value of 74.15.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8061	8068		10.1007/s00500-019-04153-6													
J								Applying data-mining techniques for discovering association rules	SOFT COMPUTING										Data mining; Association rule mining (ARM); Chronic diseases	BIG DATA	Data mining has become a hot research topic, and how to mine valuable knowledge from such huge volumes of data remains an open problem. Processing huge volumes of data presents a challenge to existing computation software and hardware. This study proposes a model using association rule mining (ARM) which is a kind of data-mining technique for discovering association rules of chronic diseases from the enormous data that are collected continuously through health examination and medical treatment. This study makes three critical contributions: (1) It suggests a systematical model of exploring huge volumes of data using ARM, (2) it shows that helpful implicit rules are discovered through data-mining techniques, and (3) the results proved that the proposed model can act as an expert system for discovering useful knowledge from huge volumes of data for the references of doctors and patients to the specific chronic diseases prognosis and treatments.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8069	8075		10.1007/s00500-019-04163-4													
J								Visual communication design elements of Internet of Things based on cloud computing applied in graffiti art schema	SOFT COMPUTING										Cloud computing; Internet of Things; Graffiti art; Visual communication	BIG DATA; VISUALIZATION	In order to create more attractive graffiti works, the Internet of Things technology is used to collect people's preference information data, and then cloud computing and big data analysis are applied to calculate and analyze the data, so as to get more design elements in line with people's preferences, providing more material for the creators of graffiti art and facilitating their further creation of graffiti. In recent years, graffiti art, as a kind of postmodern marginal cultural art, has great expressiveness and creativity, which makes graffiti art widely used. Particularly in visual communication design, graffiti art has more characteristics and is deeply loved by the people. With the continuous development of science and technology, Internet of Things technology and cloud computing have acquired more information for many fields, thus assisting the field to complete deeper analysis and research. However, there are few applications in the field of graffiti art. Therefore, this paper combines cloud computing and Internet of Things to obtain more visual communication design elements to provide material for the creator, which is conductive for their continuing creation.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8077	8086		10.1007/s00500-019-04171-4													
J								Financial information prediction and information sharing supervision based on trend assessment and neural network	SOFT COMPUTING										Multi-level model; Predictive analysis; Financial information	TIME-SERIES; MODEL	In order to help financial users to invest, and to provide users with comprehensive and accurate information about financial securities, information about financial securities from multi-heterogeneous information is obtained. The characteristics of financial information are analyzed to provide valuable investment advice to users. According to the financial characteristics of the user's interest, the characteristics of the investor's interest are extracted from the heterogeneous information. Then, the multi-level model is proposed to analyze the characteristics. Through the multi-level model, the conversion of convertible bonds and the net value of closed funds are predicted. In the first level, based on the characteristics of convertible bonds and closed funds, three models of trend evaluation model, SVR (Support Vector Regression) model and neural network backpropagation network (BPN) model are used to predict financial characteristics. In the second level, the results produced by the three models in the first level are fused by the neural network. The third level optimizes the neural network based on the second level. The optimal initial weights and thresholds are selected by genetic algorithm to obtain better prediction results. The results show that the model can predict the characteristics of convertible bonds and closed funds more accurately. Therefore, the model provides a certain reference for financial users' investment.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8087	8096		10.1007/s00500-019-04176-z													
J								Using grey relational analysis and grey integrated multi-objective strategy to evaluate the risk factors of falling of aboriginal elders in Taiwan	SOFT COMPUTING										Grey relational analysis; Grey multi-objective decision making; Mixed-method study; Fall risk factors; Aboriginal elders	KRILL HERD ALGORITHM; OLDER-PEOPLE; INSTRUMENTAL ACTIVITIES; NONFATAL FALLS; UNITED-STATES; ADULTS; PREDICTION; COMMUNITY; INJURIES; COSTS	Aboriginal elders falls have been a widely discussed topic; however, factors such as emotional and physical functions, and social and cultural backgrounds are lacking. Most studies focusing on fall correlations concentrate on medical institutions or non-tribal community areas. Few studies focus on falling of elders in mountainous areas. Applying grey relational and multi-objective decision making to predict the prevalence of falls and associated risk factors for aboriginal elders is the purpose of this study. We targeted the Atayal and Tsuo aboriginal elders in central Taiwan, involving 160 members settled in mountainous townships in Taiwan. For this research study, the components are as follows: The grey theory predicts and evaluates the commonly used fall risk assessment tools, that is, quantative questionaires include: (1) focused history: medication history and Brief Symptom Rating Scale questionnaire, BSRS-5; (2) functional Assessment: ADL (Barthel Index) and IADL; (3) prevention of falls health beliefs; and (4) prevention of falls self-efficacy. With qualitative and in-depth interviews, 18 tribal elders, 55 years and above, were interviewed: 9 from the Atayal and 9 from the Tsou. Based on the Nvivo 12.0 data analysis software, this study concluded that the most predictable fall risk assessment tool is Downton Fall Risk Index (weighting: 0.8657), followed by BSR-5 (weighting: 0.5701), prevention of falls health beliefs (weighting: 0.448), Barthel scale (weighting: 0.4296), IADL (weighting: 0.1661), prevention of falls self-efficacy (weighting: 0.1391) and medical history (weighting: 0.0615). Further, qualitative results indicated that the issue of health inequalities under the influence of ethnic culture, including physical and disease characteristics, tend to be more than the proportion of heart and hypertension diseases, healthy behaviors such as unbalanced diet, cultural beliefs in gender division of labor and ethnic communication barriers and lack of cultural adaptability care resources. The risky tribal environment of mountainous areas is a major factor that leads to falls. The results provide an accurate assessment tool suitable for Taiwanese aboriginal elders' falls for future medical policy making and fall prevention intervention in cultural safety.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8097	8112		10.1007/s00500-019-04178-x													
J								Using big data computing framework and parallelized PSO algorithm to construct the reservoir dispatching rule optimization	SOFT COMPUTING										Spark; Reservoir dispatching rule optimization; Particle swarm optimization; Big data computing framework	PARTICLE SWARM OPTIMIZATION; OPERATION; SYNCHRONIZATION; SYSTEMS	The paper aims to study how to realize the rational allocation and efficient utilization of water resources among reservoirs and coordinate the balanced optimization of benefit among dispatching objectives under the premise of ensuring flood control safety. A multi-objective optimal dispatching system for reservoirs in Jinsha River basin based on the Spark big data computing framework and parallelized particle swarm optimization (PSO) is proposed. The characteristics of multiple objectives of water resources optimal dispatching system in Jinsha River basin are analyzed. The multiple objectives have been transformed into single objectives, and the solving model of the problem is obtained. Secondly, the parallel algorithm programming model, the PSO algorithm and its parallel strategy for solving optimization problems, and the parallel method of PSO based on Spark big data computing framework are studied. The results show that the research work provides a scientific theoretical basis and a feasible optimization method for the management and dispatch of cascade hydropower stations. Therefore, this study plays a decisive role in promoting the efficient operation of water resources optimal dispatching system and has good reference value for the development and application of big data parallel programming based on Spark platform.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8113	8124		10.1007/s00500-019-04188-9													
J								A novel quality-of-service-aware web services composition using biogeography-based optimization algorithm	SOFT COMPUTING										Web services composition; Web service; Quality of service; Biogeography-based optimization; Cloud computing	DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; PARTICLE SWARM; SELECTION; PARADIGM; ABC	With the development of technology and computer systems, web services are used to develop business processes. Since a web service only performs a simple operation, web services composition has become important to respond to these business processes. In recent times, the number of existing web services has grown increasingly; therefore, similar services are presented increasingly. These similar web services are discriminated based on the various quality of service (QoS) parameters. These quality parameters include cost, execution time, availability, and reliability. In order to have the best QoS, each user should select a subset of services that presents best quality parameters. On the other hand, due to huge number of services, selecting web services for composition is an NP-hard optimization problem. This paper presents an efficient method for solving this problem using biogeography-based optimization (BBO). BBO is a very simple algorithm with few control parameters and effective exploit. The proposed method offers promising solutions to this problem. Evaluation and simulation results indicate efficiency and feasibility of the proposed algorithm.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8125	8137		10.1007/s00500-019-04266-y													
J								Differential privacy data publishing in the big data platform of precise poverty alleviation	SOFT COMPUTING										Differential privacy; Precise poverty alleviation; Privacy budget; Error rate; Big data	KRILL HERD ALGORITHM	In order to study the application of differential privacy data release for the data platform of precise poverty alleviation (PPA), in this study, the data was protected by using differential privacy protection algorithm, and combined with artificial neural network to construct the algorithm model. And then based on MATLAB simulation experiment, the operation effect of the simulation model was verified through multiple angles. From the relationship between budget and coefficient, it can be concluded that compared with extraction procedure and noprivacy by statistical test, the algorithm proposed in this study was found to be more practical, and the result was close to the original data, and the effect was better; the error rate was also the lowest, not higher than 0.075. Comparing the accuracy of the algorithm with other algorithms, the result showed that other methods made the precision lower, but the function mechanism designed in this study did not; from the perspective of time, it is found that the time consumption of algorithm designed in this study was greatly reduced compared with other methods. Through the research in this paper, the model designed by combining artificial neural network and differential privacy achieved the expected effect. Although there are some shortcomings in the experimental process, in general, it can provide direction and guidance for the subsequent PPA work, and its social development has important guiding significance.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8139	8147		10.1007/s00500-019-04352-1													
J								Feature extraction method of 3D art creation based on deep learning	SOFT COMPUTING										3D model; Artification; Deep learning; Convolution neural network	KRILL HERD ALGORITHM; PEM FUEL-CELL; SEGMENTATION; DESIGN; FUSION; IMAGES; MODEL	In order to study the method of feature extraction of 3D art design model based on deep learning, in this study, a network community media communication research on 3D art creation based on deep learning and evolution strategy was proposed. The research results showed that the two feature extraction methods were reliable and robust. Based on the evolutionary strategy, the evolution matrix function was used to extract the characteristics of people's preferences. The experimental results showed that the process is feasible. It can be concluded that the method based on the method of deep learning and interactive evolution strategy, the feasibility of social media communication research of 3D art creation network based on deep learning and evolution strategy was verified by the combination of scientific creation and artistic creation by sacrificing time expenditure.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8149	8161		10.1007/s00500-019-04353-0													
J								A fused CNN model for WBC detection with MRMR feature selection and extreme learning machine	SOFT COMPUTING										White blood cell detection; Deep learning; Convolutional neural networks; Extreme learning machine; MRMR algorithm	CLASSIFICATION	White blood cell (WBC) test is used to diagnose many diseases, particularly infections, ranging from allergies to leukemia. A physician needs clinical experience to detect and classify the amount of WBCs in human blood. WBCs are divided into four subclasses: eosinophils, lymphocytes, monocytes, and neutrophils. In the present study, pre-trained architectures, namely AlexNet, VGG-16, GoogleNet, and ResNet, were used as feature extractors. The features obtained from the last fully connected layers of these architectures were combined. Efficient features were selected using the minimum redundancy maximum relevance method. Finally, unlike classical convolutional neural network (CNN) architectures, the extreme learning Machine (ELM) classifier was used in the classification stage thanks to the efficient features obtained from CNN architectures. Experimental results indicated that efficient CNN features yielded satisfactory results in a shorter execution time via ELM classification with an accuracy rate of 96.03%.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8163	8172		10.1007/s00500-019-04383-8													
J								Visualization analysis of big data research based on Citespace	SOFT COMPUTING										Big data; Citespace; Visualization analysis; Knowledge maps	BIBLIOMETRIC ANALYSIS	In recent years, with the massive growth of data, the world today has entered the era of big data. Big data has brought tremendous value to all fields of today's society, and it has also brought enormous challenges, which has attracted great attention from all walks of life. Analyze and forecast the research hotspots and future development trends in the field of big data, and understand the development changes and priorities in the field of big data research, which will play a significant role in promoting the development of social development and scientific research. In the era of big data, how to extract information from huge amounts of complex data and present complex information more clearly and clearly, the most effective way is to use visualization technology. The article uses the information visualization software Citespace to study the data related to big data in the Web of Science and CNKI database from 2008 to 2017 for 10 years, from macro to micro to the representative countries of the literature, keywords and co-cited documents. Through visualization analysis, the article clarifies the key research directions, key documents and hot spot frontiers in the field of big data research, forecasts the future development trends in this field, and compares the research situation at home and abroad, in order to provide readers and other researchers with certain reference and help.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8173	8186		10.1007/s00500-019-04384-7													
J								Exploration of social media for sentiment analysis using deep learning	SOFT COMPUTING										Sentiment analysis; Social media; Deep learning; LSTM; Bi-LSTM		With the rapid growth of web content from social media, such studies as online opinion mining or sentiment analysis of text have started receiving attention from government, industry, and academic sectors. In recent years, sentiment analysis has not only emerged under knowledge fusion in the big data era, but has also become a popular research topic in the area of artificial intelligence and machine learning. This study used the Militarylife PTT board of Taiwan's largest online forum as the source of its experimental data. The purpose of this study was to construct a sentiment analysis framework and processes for social media in order to propose a self-developed military sentiment dictionary for improving sentiment classification and analyze the performance of different deep learning models with various parameter calibration combinations. The experimental results show that the accuracy and F1-measure of the model that combines existing sentiment dictionaries and the self-developed military sentiment dictionary are better than the results from using existing sentiment dictionaries only. Furthermore, the prediction model trained using the activation function, Tanh, and when the number of Bi-LSTM network layers is two, the accuracy and F1-measure have an even better performance for sentiment classification.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8187	8197		10.1007/s00500-019-04402-8													
J								A network representation method based on edge information extraction	SOFT COMPUTING										Network representation learning; Edge network; Node representation vectors; Edge representation vectors	SIMILARITY	In recent years, network representation learning has attracted extensive attention in the academic field due to its significant application potential. However, most of the methods cannot explore edge information in the network deeply, resulting in poor performance at downstream tasks such as classification, clustering and link prediction. In order to solve this problem, we propose a novel way to extract network information. First, the original network is transformed into an edge network with structure and edge information. Then, edge representation vectors can be obtained directly by using an existing network representation model with edge network as its input. Node representation vectors can also be obtained by utilizing the relationships between edges and nodes. Compared with the structure of original network, the edge network is denser, which can help solving the problems caused by sparseness. Extensive experiments on several real-world networks demonstrate that edge network outperforms original network in various graph mining tasks, i.e., node classification and node clustering.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8223	8231		10.1007/s00500-019-04451-z													
J								Analysis on the construction of sports match prediction model using neural network	SOFT COMPUTING										BPNN; Prediction model; Sports match; Rolling prediction	WAVELET PACKET DECOMPOSITION; FOOTBALL PLAYERS; STRATEGY; INJURY	To grasp the development of sports events in time and adjust the strategy in the process of match in time, the traditional back-propagation neural network (BPNN) algorithm was improved, and the match prediction model was constructed by using the adaptive BPNN. Taking the football match data of the Union of European Football Associations Champions League 2016-2017 as the prediction sample, the match was predicted. Moreover, taking some match data of Barcelona in 2016-2017 as an example, the fitting accuracy of the improved adaptive BPNN prediction model, multiple linear regression (MLR) model and grey degree prediction model were compared and analyzed. The research results showed that the prediction model built by the improved adaptive BPNN algorithm had smaller prediction error after rolling prediction. By comparing with the fitting accuracy of MLR model and grey prediction model, it was also found that the prediction error of the prediction model proposed was almost zero, and the error of the other two models was large. It showed that the prediction model proposed had high accuracy and reliability. Therefore, applying neural network to build a sports match prediction model and then predict the match results can provide a certain theoretical basis for sports match practice and the prediction and analysis of the match results.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8343	8353		10.1007/s00500-020-04823-w													
J								Scheduling two-stage assembly flow shop with random machines breakdowns: integrated new self-adapted differential evolutionary and simulation approach	SOFT COMPUTING										Two-stage assembly flow shop problem (TAFSP); Random machines breakdowns; Meta-heuristic algorithm; Simulation technique; Artificial neural network; Taguchi method	IMPERIALIST COMPETITIVE ALGORITHM; DEPENDENT SETUP TIMES; BI-OBJECTIVE OPTIMIZATION; TOTAL COMPLETION-TIME; SINGLE-MACHINE; PREVENTIVE MAINTENANCE; MINIMIZING MAKESPAN; JOINT PRODUCTION; BOUND ALGORITHM; HEURISTICS	This paper takes random machines breakdowns and the two-stage assembly flow shop problem into consideration as a realistic assumption in industrial environments. In practical manufacturing environment, disruptions and unforeseen incidents occur, so a schedule being built based on deterministic information is not practical and may lead to poor performance. In this paper, machines in manufacturing and assembly stages are not always available due to random machines breakdowns which occur during processing of each operation. The goal is to minimize the expected the weighted sum of makespan and mean of completion time. Owning to its problem complexity and since the problem belongs to NP-hard class, use of meta-heuristic algorithms is justified to tackle the potential complexity of the problem considered, and hence, we proposed four meta-heuristics algorithms entitled: genetic algorithm, imperialist competitive algorithm, cloud theory-based simulated annealing and new self-adapted differential evolutionary (NSDE) to solve it. Machine breakdown and dynamic nature of the problem, the structural complexity increases markedly. In this regard, to overcome this form of complexity, simulation techniques are typically employed. Eventually, since the proposed problem has both types of complexities (algorithm complexity and structural complexity), simulation is integrated into the proposed meta-heuristic approaches to handle the complexities. We apply artificial neural network as a tuning tool for predicting the input parameters of each proposed meta-heuristics algorithms in uncertain condition. Also, we suggest Taguchi method as one the most important adjusting approaches for analyzing the effect of input parameters in each algorithm. The computational results show which proposed NSDE statistically is better than other proposed meta-heuristics algorithms according two important indicators: quality of solution and computational time.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8377	8401		10.1007/s00500-019-04407-3													
J								Modeling and simulation of novel dynamic control strategy for PV-wind hybrid power system using FGS-PID and RBFNSM methods	SOFT COMPUTING										Photovoltaic; Wind turbine; PID regulator; Fuzzy logic system; Neural network	PHOTOVOLTAIC SYSTEM; MPPT ALGORITHM; ENHANCEMENT; RESPONSES; GA	During the past years, hybrid solar-wind power systems containing photovoltaic (PV) and wind generators are used to minimize the intermittency problem of renewable power generation units. The improved modeling and control schemes for a grid-tied hybrid PV-wind system is presented in the current research work. The maximum power point tracking namely "MPPT" algorithm together with controlling the pitch angle are used, respectively, for the PV system and wind power generation to attain the maximum power for any given external weather conditions. A radial basis function network sliding mode known as the RBFNSM method is used to control the pitch angle in the wind energy system, while the PV system uses a proportional-integral-derivative controller equipped with the fuzzy gain scheduling in order to enhance the transient state and mitigate the settling time to ensure the stability of the mentioned system. To test the suggested control scheme's effectiveness, MATLAB simulations are carried out under various scenarios of the wind speed as well as solar irradiation. The obtained results show the efficiency of the adaptive MPPT method to harness the highest power under very challenging scenarios. The merits of the developed schemes are quickly and precisely tracking the maximum power output of the hybrid PV-wind system. Besides, the power flowing between the utility grid and the hybrid source with a fast transient response and improved stability performance is effectively controlled using the offered schemes.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8403	8425		10.1007/s00500-019-04408-2													
J								Electron radar search algorithm: a novel developed meta-heuristic algorithm	SOFT COMPUTING										Electron radar search algorithm; Meta-heuristics; Electron discharge; Optimization	ENGINEERING OPTIMIZATION; OPTIMAL-DESIGN; METAHEURISTICS; EXPLOITATION; EXPLORATION	This paper introduces a new optimization algorithm called electron radar search algorithm (ERSA) inspired by the electron discharge mechanism. It is based on the natural phenomenon of electric flow as the form of electron discharge through a gas, liquid, or solid environment. When the voltage between separated electrodes (anode and cathode) increases, electrons tendency to emission from a low potential state to the higher potential condition is grown up. However, electrons are trying to find the best path with the least resistance in the medium. At each point, electrons evaluate the surrounding environment with a radar mechanism and least resistance path is selected for the next move. Hence, in this paper, a novel developed meta-heuristic algorithm based on the electrons' search approach is presented and the algorithm is benchmarked on 20 mathematical functions with four well-known methods for validation and verification tests. Moreover, the algorithm is implemented in two engineering design problems (tension/expression spring and welded beam design optimization) and the results demonstrate that the ERSA performs more efficiently for solving unknown search spaces and the algorithm found best solution in approximately 95% of the reviewed benchmark functions.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8443	8465		10.1007/s00500-019-04410-8													
J								An intelligent and generic approach for detecting human emotions: a case study with facial expressions	SOFT COMPUTING										Human-computer interaction; Emotion classification; Ensemble of classifiers; Genetic algorithm; Generic approach	MODEL; TIME	Several studies in the field of human-computer interaction have focused on the importance of emotional factors related to the interaction of humans with computer systems. According to the knowledge of the users' emotions, intelligent software can be developed for interacting and even influencing users. However, such a scenario is still a challenge in the field of human-computer interaction. This article endeavors to enhance intelligence in such types of systems by adopting an ensemble-based model that is able to identify and classify emotions. We developed a system (music player) that can be used as a mechanism to interact and/or persuade someone to "change" his/her current emotional state. In order to do this, we also designed a generic model that accepts any kind of interaction or persuasion mechanism (e.g., preferred YouTube channel videos, games, etc.) to be deployed at runtime based on the needs of each user. We showed that the approach based on a genetic algorithm for the weight assignment of the ensemble achieved an accuracy average of 80%. Moreover, the results showed a 60% increase in the level of user's satisfaction regarding the interaction with users' emotions.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8467	8479		10.1007/s00500-019-04411-7													
J								Energy management of the energy storage-based micro-grid-connected system: an SOGSNN strategy	SOFT COMPUTING										Battery; Cost function; DG; FC; Load demand; MG; MT; PV; SOGSNN; WT	HIERARCHICAL CONTROL; OPTIMIZATION; GENERATION; ALGORITHM; OPERATION	In this paper, a novel hybrid algorithm is implemented for the system modelling and the optimal management of the micro-grid (MG)-connected systems with low cost. The increasing number of renewable energy sources and distributed generators requires new strategies for their operations in order to maintain the energy balance between the renewable sources and MG. Therefore, an efficient hybrid technique is proposed in the paper. The main objective of the process was the optimum operation of micro-sources for decreasing the electricity production cost by hourly day-ahead and real-time scheduling. The proposed hybrid technique is to manage the power flows between the energy sources and the grid. To achieve this point, demand response and minimum cost of energy are determined. The proposed hybrid technique is the combined performance of both the gravitational search algorithm (GSA)-based artificial neural network (ANN) and squirrel search algorithm (SSA), and it is named as SOGSNN. This technique is involved with the mathematical optimization problems that necessitate more than one fitness function to be optimized simultaneously. By using the inputs of MG-like wind turbine, photovoltaic array, fuel cell, micro-turbine, diesel generator and battery storage with corresponding cost functions, the GSA-based ANN learning phase is employed to predict the load demand. SSA clarifies the squirrel in optimizing the configuration of MG based on the load demand. The proposed hybrid technique is implemented in MATLAB/Simulink working platform and compared with other solution techniques like ANFASO method. The comparison result reveals that the superiority of the proposed technique confirms its ability to solve the problem.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8481	8494		10.1007/s00500-019-04412-6													
J								Video anomaly detection and localization via Gaussian Mixture Fully Convolutional Variational Autoencoder	COMPUTER VISION AND IMAGE UNDERSTANDING										Anomaly detection; Video surveillance; Variational autoencoder; Gaussian mixture model; Dynamic flow; Two-stream network		We present a novel end-to-end partially supervised deep learning approach for video anomaly detection and localization using only normal samples. The insight that motivates this study is that the normal samples can be associated with at least one Gaussian component of a Gaussian Mixture Model (GMM), while anomalies either do not belong to any Gaussian component. The method is based on Gaussian Mixture Variational Autoencoder, which can learn feature representations of the normal samples as a Gaussian Mixture Model trained using deep learning. A Fully Convolutional Network (FCN) that does not contain a fully-connected layer is employed for the encoder-decoder structure to preserve relative spatial coordinates between the input image and the output feature map. Based on the joint probabilities of each of the Gaussian mixture components, we introduce a sample energy based method to score the anomaly of image test patches. A two-stream network framework is employed to combine the appearance and motion anomalies, using RGB frames for the former and dynamic flow images, for the latter. We test our approach on two popular benchmarks (UCSD Dataset and Avenue Dataset). The experimental results verify the superiority of our method compared to the state of the art.																	1077-3142	1090-235X				JUN	2020	195								102920	10.1016/j.cviu.2020.102920													
J								LSTM guided ensemble correlation filter tracking with appearance model pool	COMPUTER VISION AND IMAGE UNDERSTANDING										LSTM; Correlation filter; Object tracking; Convolutional neural network; Appearance model pool; CNN feature aggregation	VISUAL OBJECT TRACKING; REGRESSION; NETWORKS	Deep learning based visual trackers have the potential to provide good performance for object tracking. Most of them use hierarchical features learned from multiple layers of a deep network. However, issues related to deterministic aggregation of these features from various layers, difficulties in estimating variations in scale or rotation of the object being tracked, as well as challenges in effectively modeling the object's appearance over long time periods leaves substantial scope to improve performance. In this paper, we propose a tracker that learns correlation filters over features from multiple layers of a VGG network. A correlation filter for an individual layer is used to predict the target location. We adaptively learn the contribution of an ensemble of correlation filters for the final location estimation using an LSTM. An adaptive approach is advantageous as different layers encode diverse feature representations and a uniform contribution would not fully exploit this contrastive information. To this end, we use an LSTM as it encodes the interactions for past appearances which is useful for tracking. Further, the scale and rotation parameters are estimated using respective correlation fillers. Additionally, an appearance model pool is used that prevents the correlation filter from drifting. Experimental results achieved on five public datasets - Object Tracking Benchmark (OTB100), Visual Object Tracking (VOT) Benchmark 2016, VOT Benchmark 2017, Tracking Dataset and UAV123 Dataset, reveal that our approach outperforms state of the art approaches for object tracking.																	1077-3142	1090-235X				JUN	2020	195								102935	10.1016/j.cviu.2020.102935													
J								Probabilistic object detection and shape extraction in remote sensing data	COMPUTER VISION AND IMAGE UNDERSTANDING											SHIP DETECTION; CAR DETECTION; IMAGES; AERIAL	Remote sensing mainly focuses on information extraction from data acquired by sensors on satellite and aerial platforms. Here, one such area of interest is ground object detection and shape extraction. Recently launched satellites and conventional aerial platforms (such as commercial UAV and professional drones) have sensors leading to more detailed and rich data source for this purpose. From these, data most of the times come in the form of optical images and LiDAR measurements. Resolution of this acquired data has increased significantly such that most ground objects (as buildings, trees, ships, cars, airplanes) can be detected and analyzed in detail. Therefore, computer vision methods have become extremely useful in remote sensing applications such as building detection and shape extraction for urban planning; tree crown measurement for crop yield forecasting; ship detection for monitoring unlawful fishery; car detection for traffic flow monitoring and intelligent transportation; and airplane detection for military and commercial operations. Researchers proposed several methods to automate the mentioned applications since manually handling them is extremely hard and prohibitively time consuming. Unfortunately, the proposed methods focus on one object type most of the times. Therefore, there is no general method to handle all the mentioned applications using computer vision tools. To overcome this problem, we propose a general framework for object detection and shape extraction in remote sensing data. Our method is based on probabilistic representation inspired by our previous work and perceptual organization principles. Due to space limitations, we only focus on buildings, trees, ships, airplanes, and cars as objects of interest in this study. We test the proposed method on several optical images acquired by different satellites and LiDAR data obtained from an aerial platform. For all objects of interest, we provide test results on both object detection and shape extraction steps. We analyze the proposed method based on these tests and discuss its strengths and weaknesses. We also comment on possible future extensions of the proposed method.																	1077-3142	1090-235X				JUN	2020	195								102953	10.1016/j.cviu.2020.102953													
J								Two motion models for improving video object tracking performance	COMPUTER VISION AND IMAGE UNDERSTANDING										Video object tracking; Prior information; Bayesian estimation framework; Vector auto-regression	FILTER	Two motion models are proposed to enhance the performance of video object tracking (VOT) algorithms. The first one is a random walk model that captures the randomness of motion patterns. The second one is a data-adaptive vector auto-regressive (VAR) model that exploits more regular motion patterns. The performance of these models is evaluated empirically using real-world datasets. Three real-time publicly available visual object trackers: the normalized cross-correlation (NCC) tracker, the New Scale Adaptive with Multiple Features (NSAMF) tracker, and the correlation filter neural network (CFNet) are modified using each of these two models. The tracking performances are then compared against the original formulation. It is observed that both models of the prior information lead to performance enhancement of all three trackers. This validates the hypothesis that when training videos are available, prior information embodied in the motion models can improve the tracking performance.																	1077-3142	1090-235X				JUN	2020	195								102951	10.1016/j.cviu.2020.102951													
J								Visual complexity analysis using deep intermediate-layer features	COMPUTER VISION AND IMAGE UNDERSTANDING										Visual complexity; Convolutional layers; Deep neural network; Feature extraction; Convolutional neural network; Activation energy; Memorability; Object classification; Scene classification	IMAGE QUALITY ASSESSMENT; SIMILARITY; REPRESENTATIONS; MEMORABILITY	In this paper, we focus on visual complexity, an image attribute that humans can subjectively evaluate based on the level of details in the image. We explore unsupervised information extraction from intermediate convolutional layers of deep neural networks to measure visual complexity. We derive an activation energy metric that combines convolutional layer activations to quantify visual complexity. To show the effectiveness of our proposed metric for various applications, we introduce SAVOIAS, a visual complexity dataset that compromises of more than 1,400 images from seven diverse image categories (e.g., advertisement and interior design). We demonstrate high correlations of our deep neural network-based measure of visual complexity with human-curated ground-truth (GT) scores on various widely used network architectures, e.g., VGG16, ResNetv-2-152, and EfficientNet, and in networks trained on two classification tasks (object and scene classification). This result reveals that intermediate convolutional layers of deep neural networks carry information about the complexity of images that is meaningful to people. Furthermore, we show that our method of measuring visual complexity outperforms traditional methods on SAVOIAS and two other state-of-the-art benchmark datasets. Moreover, we perform extensive analysis on the performance difference between our unsupervised method and a supervised method trained on the feature map, and show that by supervision, we can improve the prediction. Finally, we demonstrate that, within the context of a category, visually more complex images are also more memorable to human observers.																	1077-3142	1090-235X				JUN	2020	195								102949	10.1016/j.cviu.2020.102949													
J								AN ADAPTIVE DIRECT DATA DRIVEN CONTROL SCHEME FOR UNKNOWN PLANT	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Direct data driven control; Adaptive mechanism; Lyapunov stability; Parameter adjustment law	DESIGN; SYSTEMS	This paper develops an adaptive direct data driven control scheme for unknown plant in one closed loop system, whose goal is to achieve plant-model perfect matching condition. To apply direct data driven control to designing forward controller without the model of plant, virtual input is constructed to derive one optimization problem, whose decision variables are the unknown controller parameters. Through using the idea of adaptation, one parameter adjustment loop is added as the outer loop in such a way the unknown controller parameters are changed with environment, according to our constructed parameter adjustment law. Furthermore Lyapunov's stability theory is also used to derive parameter adjustment law such that stability can be guaranteed for the whole adaptive system. Such an adaptive direct data driven control can not only design controller without the model of plant, but also adjust unknown controller parameters adaptively through one constructed parameter adjustment mechanism. Finally two simulation examples confirm our theoretical results.																	1349-4198	1349-418X				JUN	2020	16	3					783	798		10.24507/ijicic.16.03.783													
J								THREE-DIMENSIONAL PANORAMA IMAGE OF TUBULAR STRUCTURE USING STEREO ENDOSCOPY	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										3D reconstruction; Stereo endoscope; Panorama image; Digestive tract; Scene flow	BARRETTS; ESOPHAGUS; AREA; EYE	The narrow viewing angle of endoscopes impedes obtaining complete images of the digestive tract. In this article, we propose a method to acquire 3D panorama images of the digestive tract using stereo endoscopy. The method consists of acquiring sequential images of the digestive tract by moving the endoscope, reconstructing 3D surfaces for each image by stereo techniques, estimating the endoscope position using scene flow, and reconstructing a 3D image of the digestive tract by merging the surfaces. We verified the method using cylindrical pipes and a pig esophagus covered with printed squares with sides of 10 mm. The estimation error of the endoscope position was within 3% after moving 80 mm. In addition, we mapped the panorama image into a planar image to evaluate the method outcomes. The panorama procedure reduced the number of missing points owing to stereo matching failure or deficiency of resolution and improved the quality of the image. The estimation error of the sides of the reconstructed square was within 2%. The proposed method might be suitable to obtain an expanded view of the digestive tract from stereo endoscopy. This method has the potential to map and quantify the pathologic lesions of the digestive tract.																	1349-4198	1349-418X				JUN	2020	16	3					799	812		10.24507/ijicic.16.03.799													
J								A HYBRID APPROACH OF THE PRODUCT IMAGE DESIGN OF TRAIN SEATS BASED ON KANSEI ENGINEERING THEORY	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Hybrid model; Kansei engineering; Product image; Form design; Color design; Train seats	MODEL	With the rapid development of industrial technology, product design has changed from "production-oriented" to "user-oriented". Aiming at the user-oriented product image design system, this paper proposes a hybrid research method of product image design system based on the theoretical framework of Kansei engineering by the experimental study of train seat form and color design. Firstly, GRA-Fuzzy logic submodel for product form and color design was constructed objectively. Next, based on GRA-Fuzzy logic sub-model of train seat image design, a hybrid model of product image design is obtained by combining Delphi method and utility optimization model. Finally, a decision-making optimization system of product image design is established. The experimental result shows that the hybrid model performs well in predicting the product image, which is conducive to promoting and optimizing the process of product image design based on user perception.																	1349-4198	1349-418X				JUN	2020	16	3					813	829		10.24507/ijicic.16.03.813													
J								HYBRID GRAY WOLF AND PARTICLE SWARM OPTIMIZATION FOR FEATURE SELECTION	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Feature selection; Hybrid optimization; Grey wolf optimizer; Metaheuristics; Particle swarm optimizer		The big data comprises a relatively developing area of study which is due to numerous facts gathered daily and the wishes to be helpful information for use in our day by day life. One of the most crucial pre-processing of data is the feature selection. This paper proposes a hybrid technique that combines two algorithms namely Gray Wolf Optimization (GWO) and Particle Swarm Optimization (PSO), the manner that lets in the critical functions to be recognized and lets the insignificant ones and the complexity to be erased. This enables the obligations of the gadget learning classification while making use of training to the classifier with the data set. A hybrid approach is primarily based on metaheuristics swarm intelligence algorithms, which simulate the gray wolf's management and hunting manner in nature and PSO which people are moving impacted by their local best positions and by the global best position. This hybridization is to acquire the balance between exploitation and exploration. We used seventeen datasets from UCI machine gaining knowledge of repository within the experiments and comparisons results to assess the effectiveness and quality of the GWOPSO.																	1349-4198	1349-418X				JUN	2020	16	3					831	844		10.24507/ijicic.16.03.831													
J								VELOCITY-FREE FAULT ESTIMATION AND COMPENSATION FOR DYNAMIC POSITIONING OF SHIPS WITH DISTURBANCE VIA FUZZY APPROACH	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Dynamic positioning; Fault-tolerant control; Takagi-Sugeno fuzzy models; Linear matrix inequality; Fault estimation	CONTROLLER-DESIGN; TOLERANT CONTROL; SYSTEM	In this paper, we focus on the output-feedback fault-tolerant controller design for dynamic positioning ships with consideration of actuator faults and external disturbances. Firstly, a Takagi-Sugeno fuzzy model is used to represent the nonlinear dynamic positioning system of ships. Secondly, to cope with potential actuator faults, fuzzy approach and high gain method are employed to generate the fault-tolerant control law. A rigorous closed-loop stability analysis is carried out and a stability condition is posed in a linear matrix inequality framework through which the controller and observer gains are computed directly. Finally, in order to illustrate feasibility and effectiveness of the proposed fuzzy fault-tolerant control approach, some simulations are carried on a dynamic positioning ship with actuator faults and disturbances.																	1349-4198	1349-418X				JUN	2020	16	3					845	865		10.24507/ijicic.16.03.845													
J								RESEARCH ON NODE AUTHORIZATION TRUSTED UPDATE MECHANISM BASED ON AGENT RE-ENCRYPTION IN IOT CLOUD	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Node authorization trusted update mechanism; Internet of Things (IoT); Cloud computing; Sharing protocol; Communication overhead; Cloud service provider (CSP)		This paper describes a trusted update mechanism of node authorization based on proxy re-encrypted in IoT cloud. It defines that the system consists of IoT data server, node authorization management server, re-encrypted proxy server and working node. The data is generated by trusted IoT data server through wireless sensor network. The network publishes to the node, which requires the node to register in the node authorization management server. The registered node will obtain the ciphertext data in the re-encrypted proxy server in the cloud, and obtain the decryption parameters in the node authorization management server for data decryption. This paper designs a secure and efficient cloud data sharing protocol, which satisfies the security of adaptive selection ciphertext, ensures the security of data storage and sharing, and also has the characteristics of unidirectionality, universality, non-interaction, non-transferability and anti-collusion attack. This paper also analyzes the computing and communication costs of data owners, cloud service provider (CSP) and data receivers under the data sharing protocol. The results show that in the cloud data sharing protocol based on this scheme, the data owner's one-time communication cost is 2368 bits, which is better than several comparison schemes and can meet the practical application requirements.																	1349-4198	1349-418X				JUN	2020	16	3					867	877		10.24507/ijicic.16.03.867													
J								PERSONAL IDENTIFICATION USING A DELAUNAY TRIANGLE AND OPTIC DISC RETINAL VASCULAR PATTERN	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Vessel extraction; Optic disc localization; Retinal identification; Delaunay triangle; Neighbour area ratio	VESSEL SEGMENTATION; ALGORITHM; IMAGES; LOCALIZATION	Retinal vascular patterns are unique and individual. They provide highly secure and correct identity authentication. In this study, we exploit an image alignment approach based on a geometric invariant, which is the area spanned by feature-point triplets for personal identification. First, we located the optic disc by using a projection of the vascular structure in vascular extraction and extracted feature points that are bifurcations of a retinal blood vessel in the vicinity of the optic disc as the landmarks. Delaunay triangulation is then applied to the extracted feature points. The absolute invariant is then derived by taking the ratio of successive triangular area patches. The alignment is achieved by establishing correspondences between feature points after a conformal sorting step based on a derived set of absolute affine invariants. The affine transformation parameters can then be calculated by the corresponding vertices of the most robust neighbouring triangle of both inquiry and reference images. The optic disc localization results successfully located 95.95% in six widely used retinal image databases. The algorithm of vascular extraction, applied on the DRIVE database, provided an average accuracy of approximately 94.1%. The best accuracy and sensitivity for neighbouring triangle matching obtained were 99.90% and 87.66%, respectively.																	1349-4198	1349-418X				JUN	2020	16	3					879	897		10.24507/ijicic.16.03.879													
J								SEMANTIC-BASED PROCESS MINING TECHNIQUE FOR ANNOTATION AND MODELLING OF DOMAIN PROCESSES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Semantic annotation; Ontologies; Reasoner; Process mining; Process modelling; Learning process; Event logs	ONTOLOGY; INTEGRATION	Semantic technologies aim to represent information or models in formats that are not just machine-readable but also machine-understandable. To this effect, this paper shows how the semantic concepts can be layered on top of the derived models to provide a more contextual analysis of the models through the conceptualization method. Technically, the method involves augmentation of informative value of the resulting models by semantically annotating the process elements with concepts that they represent in real-time settings, and then linking them to an ontology in order to allow for a more abstract analysis of the extracted logs or models. The work illustrates the method using the case study of a learning process domain. Consequently, the results show that a system which is formally encoded with semantic labelling (annotation), semantic representation (ontology) and semantic reasoning (reasoner) has the capacity to lift the process mining and analysis from the syntactic to a more conceptual level.																	1349-4198	1349-418X				JUN	2020	16	3					899	921		10.24507/ijicic.16.03.899													
J								RESOURCE ALLOCATION OPTIMIZATION STRATEGY USING IMPROVED DIFFERENTIAL EVOLUTION ALGORITHM IN MULTI-RADIO MULTI-CHANNEL WSNS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Resource allocation optimization; Wireless sensor networks; Multi-radio multi-channel; Improved differential evolution algorithms; Multi-objective optimization model; Slot allocation; Link capacity		In order to improve the performance of multi-radio and multi-channel wireless sensor networks (WSNs), a resource allocation optimization strategy using improved differential evolution algorithm in multi-radio and multi-channel WSNs is proposed. Firstly, considering the interaction among WSNs node energy consumption, channel allocation, power control and slot allocation, the channel model of multi-node communication and the interference model of the system are constructed. Then, taking the interference and conflict of links as constraints, and taking the objects that reduce energy consumption, increase network capacity and improve the balance of resource allocation as objective functions, a multi-objective optimization model of resource allocation is constructed. The establishment of multi-objective optimization model highlights the trade-off among different objectives. Two-population differential evolution algorithm is employed to solve the model constructed in this paper, and the results are compared with those of standard differential evolution algorithms. The experimental results show that the performance of the network in all aspects has been greatly improved by the use of the proposed algorithm. The simulation results show that the link collisions can be effectively avoided and the network interference is reduced with the utilization of the proposed algorithm. Compared with the existing algorithms, the capacity of key links can be raised by two to three times, and the network capacity and resource allocation balance are improved as a result of application of the proposed algorithm.																	1349-4198	1349-418X				JUN	2020	16	3					923	937		10.24507/ijicic.16.03.923													
J								MANUFACTURE ALLOCATION OF THE BATCH PROCESS AND THE CYCLIC FLOW PROCESS VIA STOCHASTIC ANALYSIS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Manufacture allocation; Potential energy; Fluctuations; Stochastic differential equation; Lagrange function	DELAY; MODEL	We report the manufacturing allocation which types are the batch processes (hereafter BP) and the cyclic flow processes (hereafter CFP), in a steady-state production system. It is the most important for the efficient production of small-and medium-sized manufacturing companies to secure profits. We analyze how to determine these two allocations through a stochastic theory and determine the optimal allocation. First, we construct the mathematical models in terms of potential energy for BP and CFP. Next, we discuss the potential in the production field based on the cost potentials. We define for each cost variable and the marginal utility function is derived. However, we discuss the allocation in a steady-state production system by consideration of actual operation. Finally, we report the numerical results for the allocation method.																	1349-4198	1349-418X				JUN	2020	16	3					939	954		10.24507/ijicic.16.03.939													
J								A NETWORK FLOW MODEL OF REGIONAL TRANSPORTATION OF E-COMMERCE AND ANALYSIS ON MATURITY CHANGE OF FRESH FRUIT	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Fresh fruit e-commerce; Regional transportation; Network flow model; Maturity change	COLD-CHAIN; DELIVERY	Fruit e-commerce with its huge development potential is attracting a large number of e-commerce giants. However, with the rapid development of fresh e-commerce, new challenges are emerging. In order to cope with the problem of vulnerability and difficult transportation of fresh fruit in the transportation process, this paper takes kiwi fruit as an example to analyze maturity change laws of fresh fruit after harvesting, and further analyze the change range of maturity of fresh fruit during transportation combined with regional transportation network. Finally, we regard the maturity change rate of fresh fruit as a variable of sensitivity analysis to observe the maturity change of fruits with different ripening rates when fruit is delivered, and then put forward suggestions for fruit farmers on the delivery decision-making, so as to provide theoretical basis of decision-making for the delivery, and improve the circulation efficiency of agricultural products.																	1349-4198	1349-418X				JUN	2020	16	3					955	972		10.24507/ijicic.16.03.955													
J								QUANTUM EXPRESSION OF NEURAL NETWORK AND CIRCUITS (ITS LOGIC OF CALCULATION)	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Classical neural network; Quantum interference and Feynman path integral; Bayes' theory; Entropy of information theory; Quantum neuron; Schrodinger equation; Complex fuzzy system	NEURONS	We have mentioned the theory of the neural network using quantum theory (wave function and path integrals) of quantized polarization wave, what we call, polaritons, and made some mathematical forms and an expression of calculations for arbitrary neural networks' circuits. The significantly important difference between the ordinary (classical) neural networks and our quantum networks was in whether our focused systems had quantum theoretical interferences or not. The quantum mechanical system had essentially quantum fluctuation and its interference in those systems. And quantum probability has relation to the probability amplitude, propagators and wave functions, which were ordinary complex numbers and those functions. So, common classical probability never contained any interference because of the real numbers' field. Then concretely we have shown how our quantum system contained much interference, which was applied to the Bayes' theory, the entropy of information theory, and the three-step's neural network of multi channels. And we knew that the quantum neural networks and polariton's model were related to the ordinary quantum information, classical neural networks and some information theories. We would like to propose a kind of complex fuzzy system (or complex neural networks). Both systems (i.e., fuzzy system and quantum system) have something in common on these shapes. We expect that some parts of brain functions and its networks are able to describe with methods of complex fuzzy neural networks or quantum fuzzy processes.																	1349-4198	1349-418X				JUN	2020	16	3					973	990		10.24507/ijicic.16.03.973													
J								THREE-DIMENSIONAL UAV COOPERATIVE PATH PLANNING BASED ON THE MP-CGWO ALGORITHM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Multi-UAV cooperation; Path planning; Gray wolf optimization; Multi-population; Chaotic local search	GREY WOLF OPTIMIZER; MULTIPLE UAVS	A path planning method based on multi-population chaotic gray wolf optimization (GWO) is proposed to address the three-dimensional path planning problems that occur in multi-UAV cooperative task execution. First, on the basis of three-dimensional planning space, the cost function model of multi-UAV cooperation is established in accordance with the requirement of path planning. Moreover, an initial track set is constructed by combining with the multi-population concept. Second, given that the GWO algorithm is easy to fall into a local optimum, a chaotic search strategy is adopted to improve this algorithm. Finally, the proposed algorithm is used to solve the path planning problem of obtaining multiple cooperative paths. Simulation result shows that this algorithm can satisfy the constraints related to path planning and realize multi-UAV cooperative path planning. In comparison with GWO, EA, and PSO algorithms, stability and search accuracy are improved through the proposed algorithm.																	1349-4198	1349-418X				JUN	2020	16	3					991	1006		10.24507/ijicic.16.03.991													
J								ADAPTIVE FINITE-TIME SYNERGETIC CONTROL DESIGN FOR POWER SYSTEMS WITH STATIC VAR COMPENSATOR	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Transient stability; Generator excitation; SVC; Synergetic control theory; Adaptive control; Finite-time stabilization	VOLTAGE REGULATION ENHANCEMENT; COORDINATED CONTROL; GENERATOR EXCITATION; TRANSIENT STABILITY; SVC	In this paper, an adaptive finite-time synergetic control for an electric power system including Static Var Compensator (SVC) is proposed. Despite unknown parameters, the developed controller is able to enhance transient stability and guarantee that all trajectories of the whole closed-loop system are driven to the desired manifold in a finite time. In this work, there are two unknown parameters of particular interest, that is, a damping coefficient and a mechanical input power. The dynamic characteristics of the proposed control are evaluated using a Single-Machine Infinite Bus (SMIB) power system implemented in MATLAB environment. The simulation results are presented to verify the effectiveness and feasibility of the developed control scheme once compared with that of an adaptive synergetic control technique. Further, they illustrate that in spite of unknown parameters, the proposed scheme offers better transient performances in terms of faster damping oscillations, a shorter settling time, and so on.																	1349-4198	1349-418X				JUN	2020	16	3					1007	1020		10.24507/ijicic.16.03.1007													
J								DYNAMIC PROGRAMMING ALGORITHM AND BAT ALGORITHM BASED STORM NODES SCHEDULING IN EDGE COMPUTING	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Edge computing; Storm nodes; Scheduling; Dynamic programming; Bat algorithm	OPTIMIZATION	The high communication delay and uneven load among heterogeneous edge nodes are affecting the performance of edge computing, and they are almost impossible to be solved by the traditional cloud computing platforms. In this paper, we address these problems by studying the scheduling optimization method for the storm nodes in edge computing environments. At first, a storm scheduling model is established, where server cluster structure and schedule workflow are formulated. Then, a heuristic dynamic programming algorithm is proposed to address the general scheduling issue and a bat-based scheduling strategy is proposed to address a special case faced by the heuristic dynamic programming algorithm. Finally, the experimental results show that the proposed algorithm can minimize the communication cost and guarantee the minimum scheduling requirements.																	1349-4198	1349-418X				JUN	2020	16	3					1021	1033		10.24507/ijicic.16.03.1021													
J								PACKET LENGTH AWARE TIMESTAMPING MECHANISM FOR DPDK BURST-ORIENTED PACKET PROCESSING	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Timestamp accuracy; DPDK burst-oriented packet processing; Timestamping mechanism; Cumulative time gaps		DPDK burst-oriented packet processing boosts up the performance of packet capture at the cost of packet timestamp accuracy. Being blind to packet timestamp deviates from the goal of monitoring since it has a significant effort on network traffic monitoring and analysis. In this paper, we analyze packet timestamp of DPDK burstoriented packet processing and model the cumulative time gaps of packet timestamp in the same burst. Then, we demonstrate that timestamping packet in a 1-by-1 fashion after acquiring system time is not suitable for DPDK as the cumulative time gaps depending on packets number in a burst and subsequent operations on each packet make it uncontrollable. At last, a packet length aware timestamping mechanism is proposed and demonstrated that it can conform to the theoretical model we give. Experiments show the rationality and effectiveness of our proposed timestamping mechanism.																	1349-4198	1349-418X				JUN	2020	16	3					1035	1046		10.24507/ijicic.16.03.1035													
J								A NEW SMART ALGORITHM TO PROVIDE AN INTEGRITY PATH IN NETWORKS FOR IMPROVING THE ABROAD HEALTHCARE SURGERY SETTING	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Component; Quality of service (QoS); Mobile ad hoc network (MANET); Internet of Things (IoT); Medical surgery		The Internet of Things (IoT) is a paradigm based on the networks that comprise many interconnected technologies. The current demand for better control, monitoring and management in many areas, as well as ongoing research in this field, have led to the appearance and creation of multiple systems such as smart homes, smart healthcare, smart cities and smart grids. The IoT is rapidly becoming a global phenomenon, with many issues such as standardization. However, there is not a robust and secure network routing system. Various MANET or IGP routing protocols have been proposed by researchers which could be utilized in the development of enhanced routing protocols for the IoT. Thus, studying these routing protocols in the MANET or IGP network will provide direction to the development and incorporation of reliable paths between the source and the destination in the network through the application of IoT features. In this paper, we propose a new algorithm for managing lead emergency data traffic in healthcare surgery settings from a far distance. In other words, we show how a trusted and smart algorithm can manage traffic in less time and without losing any information. The reported experimental results show an acceptable level of feasibility and effectiveness. In addition, we present an initial evaluation of the lightweight enhanced routing protocol policies and their integrity on the IoT.																	1349-4198	1349-418X				JUN	2020	16	3					1047	1058		10.24507/ijicic.16.03.1047													
J								A NEW COOPERATIVE CACHE OPTIMIZATION ALGORITHM FOR INTERNET OF VEHICLES BASED ON EDGE CLOUD NETWORK	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Cache optimization algorithm; Internet of Vehicles (IoV); Edge cloud; Cournot game; Long Short-Term Memory network (LSTM); Vehicle trajectory prediction	SCHEME	The cache capacity in the edge cloud is limited and it cannot provide a complete cache service for all vehicles. How to use the limited cache resources to minimize the delay of retrieving data has become the key of research. This research focuses on the cache resource allocation strategy based on edge cloud in urban crossroads scenario. Firstly, the scenario is described and modeled. Then, the above model is solved by using the link minimum cost cooperative cache algorithm based on Cournot game and the detailed flow of learning algorithm is given. In the simulation, the study builds a Long Short-Term Memory network (LSTM) neural network to predict the behavior of vehicles according to their driving data and then obtain the distribution of vehicle driving directions. Firstly, the real vehicle data was learned by using LSTM network to acquire the vehicle trajectory prediction model. Then, the performance of the proposed algorithm is verified by simulation and compared with several benchmark strategies. Simulation results show that the proposed algorithm can achieve higher returns than the benchmark strategies under different prediction accuracy rates and cache numbers. Meanwhile, it is also compared with several cache optimization algorithms. Simulation results indicate that the algorithm proposed in the paper is better in terms of average cache hit rate and average cache delay.																	1349-4198	1349-418X				JUN	2020	16	3					1059	1075		10.24507/ijicic.16.03.1059													
J								MISSING WELL LOG DATA HANDLING IN COMPLEX LITHOLOGY PREDICTION: AN NIS APRIORI ALGORITHM APPROACH	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Lithology classification; Well log; NIS Apriori algorithm; Rule extraction; Completeness; Missing data	RULE GENERATION; NEURAL-NETWORK; MODEL; CLUSTER; FISHER; MAPS	Lithology prediction is considered an essential requirement in the field of petroleum exploration. Since reservoirs consist of complex lithologies, predicting the lithology classes is gradually playing a pivotal role in the geosciences. During drilling operations the advancements of real time data recording have been so common in the petroleum industries in the past and majority of the logging data are recorded in real time process. However, sometimes the system encounters data loss or missing values while going through the logging procedures. Hence, the application of missing data estimation in automated lithology prediction is so essential. In this research a unique module is developed for classifying lithology from borehole log data consisting of incomplete log values by employing non-deterministic information systems apriori (NIS Apriori) algorithm. The unique characteristics of the proposed module are also presented in the paper. The research proposes certain and possible rules based on real data science semantics following the framework of NISs. By using the NIS Apriori algorithm it is proved that each rule t is determined by analyzing only a pair of t-dependent possible tables although each particular rule t is a dependant on so many possible tables. However, one of the applications of the NIS Apriori algorithm is its prospect of the handling missing values. This research proposes a white-box novel architecture to deal with the well log missing values by using the NIS Apriori algorithm which provides the results in terms of rules to classify complex lithology efficiently.																	1349-4198	1349-418X				JUN	2020	16	3					1077	1091		10.24507/ijicic.16.03.1077													
J								RECURRENT NEURAL NETWORK BASED STOCK PRICE PREDICTION USING MULTIPLE STOCK BRANDS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Stock price prediction; Stock brand; Recurrent neural networks; Dynamic time warping		Stock price is difficult to predict because its fluctuation factor is complicated. We herein propose a stock price predicting method using the stock price of the predicted stock brand as well as those of other stock brands. A recurrent neural network is applied as a learning module. Furthermore, we use dynamic time warping as a similarity measure of stock price as time-series data. Subsequently, we conduct an experiment using the stock price data of companies listed in the first section of the Tokyo Stock Exchange and demonstrate the effectiveness of the proposed method.																	1349-4198	1349-418X				JUN	2020	16	3					1093	1099		10.24507/ijicic.16.03.1093													
J								MAIN TECHNIQUES OF POLICY EVALUATION AND DECISION-MAKING ANALYSIS PLATFORM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Meaningful string extraction; Network embedding; Automatic generation of briefing; Policy evaluation		The policy text refers to documents produced in policy activities, and has always been important tools or carriers for policy evaluation. The traditional way to analyze policy texts mainly relies on people to read and compile valuable information. This method is of low informationization, efficiency and timeliness, and it is impossible to track valuable information in real time. In this article, network embedding, PMI-Entropy and other technologies are used to modify the performance of a Policy Evaluation and Decision-Making Analysis Platform; thus decisions can be evaluated and made more conveniently and precisely in the big data environment.																	1349-4198	1349-418X				JUN	2020	16	3					1101	1108		10.24507/ijicic.16.03.1101													
J								SWITCHED CAPACITOR-BASED HIGH VOLTAGE MULTIPLIER WITH 220V@50HZ INPUT FOR GENERATING UNDERWATER SHOCKWAVES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										AC-DC rectifier; High voltage multiplier; Non-thermal food processing; Switched capacitor; Underwater shockwave	DESIGN	Non-thermal food processing using underwater shockwaves is a cost-effective technology for preserving food items with minimal impacts on their nutritious property. In order to generate underwater shockwaves, a high voltage multiplier is the major element for system hardware implementation. To support the 220V and 50Hz AC input for generating 3.7kV DC output, this paper presents the high voltage multiplier based on switched capacitor technique. The proposed circuit consists of three main components, which are AC-DC rectifier, level shift driver, and parallel-connected voltage multiplier circuit. The theoretical relationship analysis utilizing a 4-terminal equivalent circuit is described for investigation of not only the output voltage but also the energy loss due to internal resistance and the power efficiency of the proposed high voltage multiplier. In addition, the operations of the proposed circuit are also simulated by using PSPICE program to confirm its workability.																	1349-4198	1349-418X				JUN	2020	16	3					1109	1116		10.24507/ijicic.16.03.1109													
J								DESIGN AND IMPLEMENTATION OF DISTRIBUTED LEDGER BASED HEALTH DATA MANAGEMENT SYSTEM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Personal health record; Blockchain; R3 Corda; Distributed ledger		In this paper, we propose a new health data management system based on distributed ledger. The Personal Health Record (PHR) systems are the most developed technology for a person to manage his or her own health data. The PHR systems have technical and legal problems despite rapid technological advances. It is technically very difficult to integrate and standardize dispersed data. In addition, there are many legal limitations because health data is very sensitive. Therefore, the PHR systems have attempted to solve these problems using blockchain. However, these attempts do not solve all problems. Personal health data is still dispersed, and in some countries it is not legally permitted to record health data in a blockchain. In most countries, individuals have a right to store their own health data. The proposed system stores health data in the device of each information subject participating in health data generation processes. The proposed system also makes the individual user's device a member of the network. Users can utilize the data as if they were using the data on a single system.																	1349-4198	1349-418X				JUN	2020	16	3					1117	1124		10.24507/ijicic.16.03.1117													
J								APPROXIMATE CONSISTENT WEIGHTED SAMPLING FOR EFFICIENT TOP-K SEARCH	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Generalized Jaccard similarity; Minwise hashing; Weighted sampling		Top-k search on weighted sets can be very slow since the computation cost of generalized Jaccard similarity is proportional to the dimensionality of sets. ICWS generates samples of high quality, but its hashing cost is too high to generate samples from high-dimensional weighted sets. We propose simple hashing methods, ICWS P and its variants, that approximate ICWS very efficiently in O(D center dot K/B). Extensive experiments show that hashing cost is reduced significantly while top-k precision and classification accuracy with estimated set similarity are almost as high as those of ICWS. Query time can also be improved since less than K samples are compared for sets of low similarity.																	1349-4198	1349-418X				JUN	2020	16	3					1125	1132		10.24507/ijicic.16.03.1125													
J								New multicriteria group decision support systems for small hydropower plant locations selection based on intuitionistic cubic fuzzy aggregation information	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										ICFHHG operator; ICFHOWG operator; ICFHWG operator; intuitionistic cubic fuzzy set; multicriteria group decision making	OPERATORS; NUMBERS; MADM	There has been a quick development in construction activities during the last couple of decades attributable to a general improvement in all features of humankind. Because of innovative progressions and regularly expanding human progress, there is a diligent requirement of power. Close by the ordinary energy sources, renewable energy sources have likewise lead significantly to the rising power requirement. All over the world in the past, a number of small hydropower plants (SHPPs) have been developed, as a renewable energy source. Generally, these SHPPs are being manufactured and worked by the private designers consenting to the administration rules. So as to help a designer in choosing the most productive and doable SHPP for development and consequent activity, the concept of the intuitionistic cubic fuzzy set (ICFS) theory is established and a few important operations for ICFSs are characterized, and also a strategy dependent on intuitionistic cubic fuzzy Hamacher hybrid averaging (ICFHHA) operator, intuitionistic cubic fuzzy Hamacher order weighted averaging (ICFHOWA) operator, and intuitionistic cubic fuzzy Hamacher weighted averaging (ICFHWA) operators is utilized in the present paper. The financial criteria and technobusiness, as assumed for examining the practicality of the candidate SHPPs, are presented qualitatively utilizing intuitionistic cubic fuzzy numbers (ICFNs). Further study their fundamental properties and the relationship among these aggregation operators. Developed group decision-making (DM) algorithm under intuitionistic cubic fuzzy (ICF) environment. An interpretative case for the analysis of SHPP for construction is given to demonstrate the feasibility and practicality of the mentioned new techniques. Further validate its effectiveness and benefits via a comparative analysis with pre-existing aggregation operators, and the outcomes demonstrate that the proposed SHPP determination model has some special favorable circumstances, which is progressively practical and adaptable for SHPP choice under a complex and uncertain environment.																	0884-8173	1098-111X				JUN	2020	35	6					983	1020		10.1002/int.22233													
J								An extended COPRAS method for multiattribute group decision making based on dual hesitant fuzzy Maclaurin symmetric mean	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										dual hesitant fuzzy sets; maclaurin symmetric mean; mobile payment; multiattribute group decision making	COMPLEX PROPORTIONAL ASSESSMENT; POWER AGGREGATION OPERATORS; SETS; INFORMATION; NUMBERS	The well-known Maclaurin symmetric mean (MSM) and the dual MSM (DMSM) are introduced as important operators to handle multiattribute group decision making (MAGDM) information. The MSM and the DMSM operators have the prominent characteristic of accurately describing the interdependence of multi-input arguments. Due to their advantage, we extend the MSM and the DMSM into the dual hesitant fuzzy environment to aggregate uncertain information. Particularly, we propose some novel aggregation operators, namely dual hesitant fuzzy MSM, weighted dual hesitant fuzzy MSM, dual hesitant fuzzy dual MSM, and weighted dual hesitant fuzzy dual MSM operators. Moreover, we study some properties and special remarks regarding different values of the parameter. With an extension of the complex proportional assessment method, we formulate a new approach for the dual hesitant fuzzy MAGDM. Finally, we test the applicability and feasibility of our proposed method by solving a mobile payment platform selection problem in Ghana.																	0884-8173	1098-111X				JUN	2020	35	6					1021	1068		10.1002/int.22234													
J								Interactive sonification strategies for the motion and emotion of dance performances	JOURNAL ON MULTIMODAL USER INTERFACES										Interactive sonification; Affective computing; Gesture; Musical interface	MUSIC PERFORMANCE; BODY MOVEMENT; COMMUNICATION; EXPRESSION; FEATURES; SPEECH; MODEL	Sonification has the potential to communicate a variety of data types to listeners including not just cognitive information, but also emotions and aesthetics. The goal of our dancer sonification project is to "sonify emotions as well as motions" of a dance performance via musical sonification. To this end, we developed and evaluated sonification strategies for adding a layer of emotional mappings to data sonification. Experiment 1 developed and evaluated four musical sonifications (i.e., sin-ification, MIDI-fication, melody module, and melody and arrangement module) to see their emotional effects. Videos were recorded of a professional dancer interacting with each of the four musical sonification strategies. Forty-eight participants provided ratings of musicality, emotional expressivity, and sound-motion/emotion compatibility via an online survey. Results suggest that increasing musical mappings led to higher ratings for each dimension for dance-type gestures. Experiment 2 used the musical sonification framework to develop four sonification scenarios that aimed to communicate a target emotion (happy, sad, angry, and tender). Thirty participants compared four interactive sonification scenarios with four pre-composed dance choreographies featuring the same musical and gestural palettes. Both forced choice and multi-dimensional emotional evaluations were collected, as well as motion/emotion compatibility ratings. Results show that having both music and dance led to higher accuracy scores for most target emotions, compared to music or dance conditions alone. These findings can contribute to the fields of movement sonification, algorithmic music composition, as well as affective computing in general, by describing strategies for conveying emotion through sound.																	1783-7677	1783-8738				JUN	2020	14	2			SI		167	186		10.1007/s12193-020-00321-3													
J								Focused Audification and the optimization of its parameters	JOURNAL ON MULTIMODAL USER INTERFACES										Sonification; Auditory display; Audification; Auditory graph; ECG data		We present a sonification method which we call Focused Audification (FA; previously: Augmented Audification) that allows to expand pure audification in a flexible way. It is based on a combination of single-side-band modulation and a pitch modulation of the original data stream. Based on two free parameters, the sonification's frequency range is adjustable to the human hearing range and allows to interactively zoom into the data set at any scale. The parameters have been adjusted in a multimodal experiment on cardiac data by laypeople. Following from these results we suggest a procedure for parameter optimization to achieve an optimal listening range for any data set, adjusted to human speech.																	1783-7677	1783-8738				JUN	2020	14	2			SI		187	198		10.1007/s12193-019-00317-8													
J								Bent line quantile regression via a smoothing technique	STATISTICAL ANALYSIS AND DATA MINING										change point; quantile regression; smoothing technique	ESTIMATORS; MODELS	Abent line quantile regression model can describe the conditional quantile function of the response variable with two different straight lines, which intersect at an unknown change point. This paper proposes a new approach via a smoothing technique to simultaneously estimate the location of the change point and other regression coefficients for the bent line quantile regression model. Furthermore, the asymptotic properties of the proposed estimator are derived, and a formal test procedure for the existence of a change point is also provided. Simulation studies are carried out to demonstrate the finite sample performance of the proposedmethod. We also illustrate the proposedmethod by applying it to the gross domestic product (GDP) per capita and the life expectancy at birth data.																	1932-1864	1932-1872				JUN	2020	13	3					216	228		10.1002/sam.11453													
J								Vertex nomination via seeded graph matching	STATISTICAL ANALYSIS AND DATA MINING										graph inference; graph matching; graph mining; seeded graph matching; stochastic block model; vertex nomination	NETWORK ALIGNMENT; RECOGNITION	Consider two networks on overlapping, nonidentical vertex sets. Given vertices of interest (VOIs) in the first network, we seek to identify the corresponding vertices, if any exist, in the second network. While in moderately sized networks graph matching methods can be applied directly to recover the missing correspondences, herein we present a principled methodology appropriate for situations in which the networks are too large/noisy for brute-force graph matching. Our methodology identifies vertices in a local neighborhood of the VOIs in the first network that have verifiable corresponding vertices in the second network. Leveraging these known correspondences, referred to as seeds, we match the induced subgraphs in each network generated by the neighborhoods of these verified seeds, and rank the vertices of the second network in terms of the most likely matches to the original VOIs. We demonstrate the applicability of our methodology through simulations and real data examples.																	1932-1864	1932-1872				JUN	2020	13	3					229	244		10.1002/sam.11454													
J								Lagged encoding for image-based time series classification using convolutional neural networks	STATISTICAL ANALYSIS AND DATA MINING										classification; convolutional neural network; image recognition; time series	DERIVATIVES	Time series classification is a thriving area of research in machine learning. Among many applications, it is frequently applied to human activity analysis. Time series describing a human in motion are ubiquitously collected via omnipresent mobile devices and can be subjected to further processing. In this paper, we propose a novel, deep learning approach to time series classification. It is based on a lagged time series representation stored as images and Convolutional Neural Network used to image classification. We present a comparative study on different variants of lagged time series representation and we evaluate their effectiveness in a series of empirical experiments. We show that the developed method provides satisfying classification accuracy. The proposed image-based time series encoding is less resource-consuming than encodings used in other image-based approaches to time series classification. It is worth to emphasize that the proposed time series encoding conceals original time series values. Images are saved without scales and the order of observations cannot be reconstructed. Thus, the method is particularly suitable for systems that need to store sensitive information.																	1932-1864	1932-1872				JUN	2020	13	3					245	260		10.1002/sam.11455													
J								Coupling humanoid walking pattern generation and visual constraint feedback for pose-regulation and visual path-following	ROBOTICS AND AUTONOMOUS SYSTEMS										Humanoid robots locomotion; Visual servoing; Visual path following; Visual geometric constraint	NAVIGATION; VISION	In this article, we show how visual constraints such as homographies and fundamental matrices can be integrated tightly into the locomotion controller of a humanoid robot to drive it from one configuration to another (pose-regulation), only by means of images. The visual errors generated by these constraints are stacked as terms of the objective function of a Quadratic Program so as to specify the final pose of the robot with a reference image. By using homographies or fundamental matrices instead of specific points, we avoid the features occlusion problem. This image-based strategy is also extended to solve the problem of following a visual path by a humanoid robot, which allows the robot to execute much longer paths and plans than when using just one reference image. The effectiveness of our approach is validated with a humanoid dynamic simulator. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103497	10.1016/j.robot.2020.103497													
J								A minimal biologically-inspired algorithm for robots foraging energy in uncertain environments	ROBOTICS AND AUTONOMOUS SYSTEMS											CAENORHABDITIS-ELEGANS; SOURCE LOCALIZATION; CHEMOTAXIS; AGGREGATION	This work details the design and simulation results of a bioinspired minimalist algorithm based on C. elegans, using autonomous agents to forage for attractant energy sources. The robotic agents are energy-constrained and depend on the energy they forage to recharge their batteries, which is significant as the foraging task is one of the canonical testbeds for cooperative robotics. The algorithm consists of 6 input parameters which were simulated and optimised in 9 unbounded environments of varying difficulty levels, containing attractant sources which robots would then have to forage from to maintain energy levels and survive the entirety of the simulation. The robots running the algorithm were then optimised using Evolutionary Algorithms and the best solutions in all 9 environments were categorised with the use of clustering techniques. The clustering results highlighted the different strategies which emerged. Ultimately across the 9 environments, 6 different strategies have been identified. The results demonstrate the applicability of the proposed algorithm to localise attractant sources and harvest energy in different scenarios using the same core algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103499	10.1016/j.robot.2020.103499													
J								A robust phase oscillator design for wearable robotic systems	ROBOTICS AND AUTONOMOUS SYSTEMS										Phase oscillator; Wearable robot; Hip exoskeleton; Nonlinear controller	STABILITY; EXOSKELETONS	The design of a phase-based robust oscillator for wearable robots, that could assist humans performing periodic or repetitive tasks, is presented in this paper. The bounds on perturbations, that guaranteed the stability of the output for the phase oscillator controller, were identified and the Lyapunov redesign method was applied to construct a robust controller using a bounding function. The robust controller produced a bounded control signal to modify the amplitude and frequency of the resulting second-order oscillator to modulate the stiffness and damping properties. In this paper, the focus is on the mathematical modeling of the controller, its dynamic stability and robustness for human-robot application. The proposed approach was verified through a simple pendulum experiment. The results provided evidence that a better limit cycle, with a controlled radial spread of the steady state, was obtained with Lyapunov redesigned phase oscillator. Finally, the potential of the proposed approach for hip assistance in a healthy subject wearing HeSa (Hip Exoskeleton for Superior Assistance) during periodic activities are discussed with preliminary results. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103514	10.1016/j.robot.2020.103514													
J								Wheeled motion kinematics and control of a hybrid mobility CENTAURO robot	ROBOTICS AND AUTONOMOUS SYSTEMS										Hybrid locomotion; Wheeled-legged robots; Wheeled robots; Legged robots; Kinematic modelling; Motion control	LOCOMOTION	Legged-wheeled robots combine the advantages of efficient wheeled mobility with the capability of adapting to real-world terrains through the legged locomotion. Thanks to their hybrid mobility skill, they can excel in many application scenarios where other mobile platforms are not suitable for. However, the improved versatility of their mobility increases the number of constraints in their motion control, where both the properties of legged and wheeled functionalities need to be considered. Relevant schemes for legged-wheeled motion control so far have attempted to address the problem by exploiting separate motion control of the wheeled and legged functionalities. The contribution of this paper is the introduction of derivation of the legged-wheeled motion kinematics without constraining the camber angles of the wheels. To this end, the wheel geometry is approximated by torus that more precisely represents a real wheel geometry than a standard sphere/cylinder. On the basis of the derived legged-wheeled motion kinematics, a first-order inverse kinematics (IK) scheme that resolves the legged-wheeled robot whole-body motion respecting the wheel rolling constraint is described. Furthermore, a higher-level method to resolve wheel steering to comply with a non-holonomic constraint is designed. A damping scheme is proposed to handle a structural singularity when a system non-holonomy deteriorates. Finally, the work adopts a floating base model that allows to easily incorporate the legged motion into the proposed scheme. The developed control scheme is tested in experiments on a legged-wheeled centaur-like robot - CENTAURO. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103482	10.1016/j.robot.2020.103482													
J								Skill transfer learning for autonomous robots and human-robot cooperation: A survey	ROBOTICS AND AUTONOMOUS SYSTEMS										Skill transfer; Robot learning; Human demonstrations; Neurophysiological skill acquisition	REAL-TIME; MOVEMENT PRIMITIVES; MYOELECTRIC CONTROL; REINFORCEMENT; INTERFACES; IMITATION; GRASP; ARM; NAVIGATION; PRECISION	Designing a robot system with reasoning and learning ability has gradually become a research focus in robotics research field. Recently, Skill Transfer Learning (STL), i.e., the ability of transferring human skills to robots, has become a research thrust for autonomous robots and human-robot cooperation. It provides the following benefits: (i) the skill transfer learning system with independent decision-making and learning ability enables the robot to learn and acquire manipulation skills in a complex and dynamic environment, which can overcome the shortages of conventional methods such as traditional programming, and greatly improve the adaptability of the robot to complex environments and (ii) human physiological signals allow us to extract motion control characteristics from physiological levels which create a rich sensory signal. In this survey, we provide an overview of the most important applications of STL by analyzing and categorizing existing works in autonomous robots and human-robot cooperation area. We close this survey by discussing remaining open challenges and promising research topics in future. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103515	10.1016/j.robot.2020.103515													
J								Grappling claws for a robot to climb rough wall surfaces: Mechanical design, grasping algorithm, and experiments	ROBOTICS AND AUTONOMOUS SYSTEMS										Flexible grasping claw; Grasping discrimination algorithm; 3D wall; Asperity of rough wall surfaces	PERFORMANCE	Wall-climbing robots have been widely applied in the inspection of smooth walls. However, only a few adhesion methods have been developed for robots that will allow them to climb cliffs and dusty, high-altitude, rough walls (constructed using coarse concrete, bricks, and stones, etc.) that may be subjected to vibrations. This paper proposes a suitable adhesion method that employs grappling-hook-like claws arranged in a cross shape. First, we address the implementation mechanism required. Then, a method of extracting the characteristic parameter is revealed rough wall was devised, 3D profiles of rough walls were simulated, and the discriminant conditions necessary for the claws to stably grasp the wall were provided. A method of triangulation is proposed to judge which regions of a 3D wall can be gripped, and we subsequently present a grasping discrimination algorithm for the interaction between the miniature claws and 3D wall profile. Finally, a prototype of the grappling-hook-like claw system was fabricated. A test platform was built to test the robot which incorporates an electromagnetic vibration shaker to simulate a vibrating wall. Experiments were then carried out on the robot using the vibrating wall and a random outdoor wall. The results verified the feasibility of the proposed claws and the validity of the discriminant algorithm for gripping 3D walls. Compared with traditional adhesion approaches, the proposed method (based on hook-like claws) is more adaptable to suit various types of wall. It also has higher resistance to disturbances and so provides a more reliable method of adhesion for robots on rough walls. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103501	10.1016/j.robot.2020.103501													
J								Multi-camera visual SLAM for off-road navigation	ROBOTICS AND AUTONOMOUS SYSTEMS										Multi-camera; Panorama; Off-road; Simultaneous localization and mapping	SIMULTANEOUS LOCALIZATION	With the rapid development of computer vision, vision-based simultaneous localization and mapping (vSLAM) plays an increasingly important role in the field of unmanned driving. However, traditional SLAM methods based on a monocular camera only perform well in simple indoor environments or urban environments with obvious structural features. In off-road environments, the situation that SLAM encounters could be complicated by problems such as direct sunlight, leaf occlusion, rough roads, sensor failure, sparsity of stably trackable texture. Traditional methods are highly susceptible to these factors, which lead to compromised stability and reliability. To counter such problems, we propose a panoramic vision SLAM method based on multi-camera collaboration, aiming at utilizing the characters of panoramic vision and stereo perception to improve the localization precision in off-road environments. At the same time, the independence and information sharing of each camera in multi-camera system can improve its ability to resist bumps, illumination, occlusion and sparse texture in an off-road environment, and enable our method to recover the metric scale. These characters ensure unmanned ground vehicles (UGVs) to locate and navigate safely and reliably in complex off-road environments. (C) 2020 The Authors. Published by Elsevier B.V.																	0921-8890	1872-793X				JUN	2020	128								103505	10.1016/j.robot.2020.103505													
J								Design of transfer learning structure for slot wedge tightness inspection robot	ROBOTICS AND AUTONOMOUS SYSTEMS										Transfer learning; Slot wedge inspection robot; CNN; RNN; Frequency domain feature		The tightness inspection for the slot wedges is significant for the safe operation of large generators. One of the traditional methods is analysis of the acoustic signals of knocking on the surface of the slot wedge by inspection experts. Nowadays the slot wedge inspecting robot is an effective way to measure the tightness of the slot wedges and classify the level of the slot wedges into different groups. However, there are many types of generators and the precision cannot be guaranteed if the model of one type of generators is applied to another. Although the machine learning methods such as CNN (Convolutional Neural Networks) and RNN (Recurrent Neural Networks) are widely used for classification, they are not suitable for model transfer between different generators. In this paper, a transfer learning based structure is introduced to solve the problem and also the mixture of RNN and CNN is designed to fulfill the system. The structure is tested to transfer models with the acoustic signal sampled by the inspecting robot between the 500 MW and 600 MW generators. Experiment results show that the transfer learning structure can transfer models from one type of generators to another. Compared with the state-of-the-art methods, the proposed structure can improve the inspection precision by at least 36.7% and obtain the average precision over 79.0%. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUN	2020	128								103507	10.1016/j.robot.2020.103507													
J								A scene comprehensive safety evaluation method based on binocular camera	ROBOTICS AND AUTONOMOUS SYSTEMS												Intelligent vehicle is an inevitable trend in urban transportation development. Whether for driver assistance systems or advanced driverless systems, safety issues are one of the cores of these systems. This paper proposes a scene safety evaluation method based on binocular camera. By proposing a comprehensive safety evaluation model, the uncertainty of the scene can be represented with a single value, which can be expressed as a quantitative evaluation of the safety of intended functionality (SOTIF). It provides real-time safety monitoring and protection for the driving process of drivers and occupants by the human-computer interaction. Based on the calculation results, different monitoring methods can be adopted for different levels of autonomous driving to further improve the safety of autonomous driving. (C) 2020 Published by Elsevier B.V.																	0921-8890	1872-793X				JUN	2020	128								103503	10.1016/j.robot.2020.103503													
J								The selective traveling salesman problem with draft limits	JOURNAL OF HEURISTICS										Selective traveling salesman problem with draft limits; Variable neighborhood search; Location neighborhood structures; Traveling salesman neighborhood structures; Mixed integer linear programming		This paper introduces the selective traveling salesman problem with draft limits, an extension of the traveling salesman problem with draft limits, wherein the goal is to design a maximum profit tour respecting draft limit constraints at the visited nodes. We propose a mixed integer linear programming (MILP) formulation for this problem. This MILP model is used to solve-to optimality-small size instances and to assess the quality of solutions obtained using a general variable neighborhood search heuristic that explores several neighborhood structures. Our extensive computational experiments confirm the efficiency of the method and the quality of the reported solutions.																	1381-1231	1572-9397				JUN	2020	26	3			SI		339	352		10.1007/s10732-019-09406-z													
J								A variable neighborhood search simheuristic for project portfolio selection under uncertainty	JOURNAL OF HEURISTICS										Project portfolio selection; Stochastic optimization; Net present value; Variable neighborhood search; Simheuristics	ANT COLONY OPTIMIZATION; METAHEURISTIC APPROACH; MODEL	With limited financial resources, decision-makers in firms and governments face the task of selecting the best portfolio of projects to invest in. As the pool of project proposals increases and more realistic constraints are considered, the problem becomes NP-hard. Thus, metaheuristics have been employed for solving large instances of the project portfolio selection problem (PPSP). However, most of the existing works do not account for uncertainty. This paper contributes to close this gap by analyzing a stochastic version of the PPSP: the goal is to maximize the expected net present value of the inversion, while considering random cash flows and discount rates in future periods, as well as a rich set of constraints including the maximum risk allowed. To solve this stochastic PPSP, a simulation-optimization algorithm is introduced. Our approach integrates a variable neighborhood search metaheuristic with Monte Carlo simulation. A series of computational experiments contribute to validate our approach and illustrate how the solutions vary as the level of uncertainty increases.																	1381-1231	1572-9397				JUN	2020	26	3			SI		353	375		10.1007/s10732-018-9367-z													
J								The parking allocation problem for connected vehicles	JOURNAL OF HEURISTICS										Parking allocation; 0-1 Programming; Variable neighborhood search		In this paper, we propose a parking allocation model that takes into account the basic constraints and objectives of a problem where parking lots are assigned to vehicles. We assume vehicles are connected and can exchange information with a central intelligence. Vehicle arrival times can be provided by a GPS device, and the estimated number of available parking slots, at each future time moment and for each parking lot is used as an input. Our initial model is static and may be viewed as a variant of the generalized assignment problem. However, the model can be rerun, and the algorithm can handle dynamic changes by frequently solving the static model, each time producing an updated solution. In practice this approach is feasible only if reliable quality solutions of the static model are obtained within a few seconds since the GPS can continuously provide new input regarding the vehicle's positioning and its destinations. We propose a 0-1 programming model to compute exact solutions, together with a variable neighborhood search-based heuristic to obtain approximate solutions for larger instances. Computational results on randomly generated instances are provided to evaluate the performance of the proposed approaches.																	1381-1231	1572-9397				JUN	2020	26	3			SI		377	399		10.1007/s10732-017-9364-7													
J								A biased-randomized variable neighborhood search for sustainable multi-depot vehicle routing problems	JOURNAL OF HEURISTICS										Sustainability; City logistics; Multi-depot vehicle routing problem; Variable neighborhood search; Biased randomization	HEURISTICS; LOGISTICS; ALGORITHM; METAHEURISTICS; FRAMEWORK	Urban freight transport is becoming increasingly complex due to a boost in the volume of products distributed and the associated number of delivery services. In addition, stakeholders' preferences and city logistics dynamics affect the freight flow and the efficiency of the delivery process in downtown areas. In general, transport activities have a significant and negative impact on the environment and citizens' welfare, which motivates the need for sustainable transport planning. This work proposes a metaheuristic-based approach for tackling an enriched multi-depot vehicle routing problem in which economic, environmental, and social dimensions are considered. Our approach integrates biased-randomization strategies within a variable neighborhood search framework in order to better guide the searching process. A series of computational experiments illustrates how the aforementioned dimensions can be integrated in realistic transport operations. Also, the paper discusses how the cost values change as different dimensions are prioritized.																	1381-1231	1572-9397				JUN	2020	26	3			SI		401	422		10.1007/s10732-018-9366-0													
J								A general variable neighborhood search for solving the multi-objective open vehicle routing problem	JOURNAL OF HEURISTICS										General variable neighborhood search; NSGA-II; Open vehicle routing problem; Sweep algorithm; Local search; Multi-objective optimization	ALGORITHM	The multi-objective open vehicle routing problem (MO-OVRP) is a variant of the classic vehicle routing problem in which routes are not required to return to the depot after completing their service and where more than one objective is optimized. This work is intended to solve a more realistic and general version of the problem by considering three different objective functions. MO-OVRP seeks solutions that minimize the total number of routes, the total travel cost, and the longest route. For this purpose, we present a general variable neighborhood search algorithm to approximate the efficient set. The performance of the proposal is supported by an extensive computational experimentation which includes the comparison with the well-known multi-objective genetic algorithm NSGA-II.																	1381-1231	1572-9397				JUN	2020	26	3			SI		423	452		10.1007/s10732-017-9363-8													
J								Prejudice in uncertain information merging: Pushing the fusion paradigm of evidence theory further	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Evidence theory; Information fusion; Separable belief functions; Belief change; Possibility theory; Information ordering	BELIEF FUNCTIONS; MULTIVARIATE BERNOULLI; PROBABILITIES; COMBINATION; REPRESENTATIONS; RULE	In his 1976 book, G. Shafer reinterprets Dempster lower probabilities as degrees of belief. He studies the fusion of independent elementary partially reliable pieces of evidence coming from different sources, showing that not all belief functions can be seen as the combination of simple support functions, representing such pieces of evidence, using Dempster rule. It only yields a special kind of belief functions called separable. In 1995, Ph. Smets has indicated that any non-dogmatic belief function can be seen as the combination of so-called generalized simple support functions, whose masses may lie outside the unit interval. It comes down to viewing a belief function as the result of combining two separable belief functions, one of which models reports from sources, and the other one expresses doubt, via a retraction operation. We propose a new interpretation of the latter belief function in terms of prejudice of the receiver, and consider retraction as a special kind of belief change. Its role is to weaken the support of some focal sets of a belief function, possibly stemming from the fusion of the incoming information. It provides an alternative extensive account of non-dogmatic belief functions as a theory of merging pieces of evidence and prejudices, which partially differs from Shafer approach's based on support functions and coarsenings. Retraction differs from discounting, revision, and from the symmetric combination of conflicting evidence. The approach relies on a so-called diffidence function on the positive reals ranging from full confidence to full diffidence. We also discuss information orderings and combination rules that rely on diffidence functions. Finally, we study the diffidence-based ordering and combination in the consonant case, and show that the diffidence view suggests a new branch of possibility theory, in agreement with likelihood functions. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						1	22		10.1016/j.ijar.2020.02.012													
J								An Ant Colony Optimization approach for symbolic regression using Straight Line Programs. Application to energy consumption modelling	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Energy efficiency; Symbolic regression; Straight Line Programs; Ant Colony Optimization	ELECTRICITY CONSUMPTION; PREDICTION; BUILDINGS; EFFICIENCY; FRAMEWORK	The increase of energy consumption and their direct effects on pollution and global warming have motivated governments to develop new strategies to promote a better usage of energy. One of the most important aspects related to energy efficiency is the need for a suitable model of energy consumption that can be used to make predictions or to aid experts in high level decision making processes. Symbolic regression techniques can be used to discover an energy consumption model that fits these purposes. Traditionally, the problem of symbolic regression has been solved by using genetic programming approaches to find the algebraic expression that best fits the regression problem data, where each expression is encoded as a tree structure. In previous works, we found that a different approach using Straight Line Programs as a representation technique could provide promising results for symbolic regression, although the size of the resulting algebraic expression might be increased when compared to the traditional approach. This work proposes an Ant Colony Optimization algorithm for Straight Line Programs to solve the problem, and makes a study to compare the approach with traditional genetic programming in a real energy consumption modelling problem. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						23	38		10.1016/j.ijar.2020.03.005													
J								On consistent functions for neighborhood systems	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Consistent function; Neighborhood system; Granular computing; Closure and interior operators; Neighborhood semantics; Modal logic	ROUGH SETS; INFORMATION-SYSTEMS; COMMUNICATION; SEMANTICS	In this paper, we propose a definition of system-consistent (sys-consistent) functions for neighborhood systems and compare it with a previous definition of granule-based consistent (gra-consistent) functions in the literature. We show that sys-consistency achieves the same level of generality as gra-consistency in the sense that the former subsumes all existing definitions of consistent functions that are known to be special cases of the latter. Then, we prove that sys-consistent functions are structure-preserving mappings with respect to interior and closure operators on neighborhood systems, whereas gra-consistent functions are not. In addition, we connect consistent functions with well-known model-theoretic notions of bisimulations and bounded morphisms in modal logic. As a consequence, this implies that properties described by modal formulas remain invariant under consistent mappings. Finally, we show that most (albeit not all) of the above-mentioned results still hold for some variants and extensions of the basic definition. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						39	58		10.1016/j.ijar.2020.03.002													
J								Label distribution learning: A local collaborative mechanism	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Label distribution learning; Multi-label learning; Label ambiguity; Sparse dictionary learning	SPARSE-REPRESENTATION; FACE RECOGNITION; CLASSIFICATION; OPTIMIZATION; ALGORITHM	Label distribution learning (LDL) is a generalized machine learning framework for dealing with label ambiguity, as it can explore the relative importance levels of different labels in the description of each sample. Although several algorithms have been proposed to solve LDL problems, they usually destroy the consistency of geometric structures between feature space and label space to a certain extent, which frequently plays a significant role in learning tasks. Meanwhile, most existing LDL algorithms only take predictive performances into consideration, while ignoring the computational cost and robustness to noises. To remedy above deficiencies, we propose a novel algorithm, i.e., Local Collaborative Representation based Label Distribution Learning, shortly LCR-LDL. In LCR-LDL, an unlabeled sample is treated as the collaborative representation of the local dictionary constructed by the neighborhood of the unlabeled sample, and the discriminatory information of representation coefficients is used to reconstruct the label distribution of the unlabeled sample. Experimental results on 20 real-world LDL data sets compared with results produced by 11 state-of-the-art algorithms show that, the proposed LCR-LDL algorithm can not only effectively improve the predictive performances for LDL tasks, but also exhibit higher robustness and a lightweight computational overhead. This study suggests new trends for considering the computational cost and robustness issues in the LDL community. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						59	84		10.1016/j.ijar.2020.02.003													
J								On the fictitious default algorithm in fuzzy financial networks	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Financial networks; Fuzzy financial data; Fictitious default; Fixed point	CONTAGION; RISK	In the literature on financial contagion, the possibility to deal only with imprecise information about the overall interbank exposures and the implications in the analysis of the stability of the financial system seem to be a relevant problem. In particular, previous literature has shown that fuzzy data arise naturally in this framework and turn to be tractable from the computational point of view. The present paper generalizes the well known fictitious default algorithmto the fuzzy setting, providing an existence result for the corresponding fuzzy fixed points, the convergence of the algorithm to fixed points, an implementation of the algorithm in MATLAB and numerical simulations. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						85	102		10.1016/j.ijar.2020.02.008													
J								Simple contrapositive assumption-based argumentation frameworks	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Structured argumentation; Dung's semantics; Assumption-based argumentation; Deductive argumentation; Defeasible reasoning; Inconsistency management	LOGIC	Assumption-based argumentation is one of the most prominent formalisms for logical (or structured) argumentation, with tight links to different forms of defeasible reasoning. In this paper we study the Dung semantics for extended forms of assumption-based argumentation frameworks (ABFs), based on any contrapositive propositional logic, and whose defeasible assumptions are expressed by arbitrary formulas in that logic. We show that unless the falsity propositional constant is part of the defeasible assumptions, the grounded and the well-founded semantics for ABFs lack most of the desirable properties they have in abstract argumentation frameworks (AAFs), and that for simple definitions of the contrariness operator and the attacks relations, preferred and stable semantics are reduced to naive semantics. We also show the redundancy of the closure condition in the standard definition of Dung's semantics for ABFs, and investigate the use of disjunctive attacks in this setting. Finally, we show some close relations of reasoning with ABFs to reasoning with maximally consistent sets of premises, and consider some properties of the induced entailments, such as being cumulative, preferential, or rational relations that satisfy non-interference. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						103	124		10.1016/j.ijar.2020.02.011													
J								L-partial metrics and their topologies	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Pointwise pseudo-quasi-metric; Weighted L-metric; L-partial metric; Remote-neighborhood mapping; L-topology	SPACES	The aim of this paper is to study the theory of weighted (pseudo-quasi-)metrics based on L-fuzzy sets. We first prove that the category of weighted pointwise quasi-metric spaces is isomorphic to that of L-partial metric spaces. Then we introduce the remote-neighborhood mapping and use it to characterize the L-partial pseudo-quasi-metric. Finally, we investigate properties of the L-topology induced by the L-partial pseudo-quasi-metrics. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						125	134		10.1016/j.ijar.2020.03.006													
J								A min-max regret approach to maximum likelihood inference under incomplete data	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Maximum likelihood; Incomplete data; Min-max-regret		Various methods have been proposed to express and solve maximum likelihood problems with incomplete data. In some of these approaches, the idea is that incompleteness makes the likelihood function imprecise. Two proposals can be found to cope with this situation: the maximax approach that maximizes the greatest likelihood value induced by precise data sets compatible with the incomplete observations, and the maximin approach that maximizes the least such likelihood value. These approaches prove to display extreme behaviors in some contexts, the maximax approach having a tendency to disambiguate the data, while the maximin approach favors uniform distributions. In this paper, we propose an alternative approach consisting in minimizing a relative regret criterion with respect to maximal likelihood solutions obtained for all precise data sets compatible with the coarse data. In contrast with the maximax and the maximin methods, the min-max-regret method relies on comparing relative likelihoods and obtains results that achieve a tradeoff between results of the two other methods. The methods are compared on toy examples and also on simulated random data as well as a supervised classification problem. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						135	149		10.1016/j.ijar.2020.03.003													
J								Probabilities of conditionals and previsions of iterated conditionals	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Coherence; Conditional random quantities; Conditional probabilities and previsions; Conjoined and iterated conditionals; Affirmation of the Consequent; Independence and uncorrelation	PARADIGM PSYCHOLOGY; QUASI CONJUNCTION; COHERENCE; DISJUNCTION; INFERENCE; FINETTI	We analyze selected iterated conditionals in the framework of conditional random quantities. We point out that it is instructive to examine Lewis's triviality result, which shows the conditions a conditional must satisfy for its probability to be the conditional probability. In our approach, however, we avoid triviality because the import-export principle is invalid. We then analyze an example of reasoning under partial knowledge where, given a conditional if A then Cas information, the probability of A should intuitively increase. We explain this intuition by making some implicit background information explicit. We consider several (generalized) iterated conditionals, which allow us to formalize different kinds of latent information. We verify that for these iterated conditionals the prevision is greater than or equal to the probability of A. We also investigate the lower and upper bounds of the Affirmation of the Consequent inference. We conclude our study with some remarks on the supposed "independence" of two conditionals, and we interpret this property as uncorrelation between two random quantities. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						150	173		10.1016/j.ijar.2020.03.001													
J								Constructing three-way concept lattice based on the composite of classical lattices	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way concept analysis; Attribute-induced three-way concept lattice; Object-induced three-way concept lattice; Composite operator	ATTRIBUTE REDUCTION; ROUGH SET; RULES; APPROXIMATIONS; ACQUISITION; ALGORITHMS	Three-way concept analysis provides a novel model for three-way decisions, and also extends the classical formal concept analysis, which has attracted many attentions in recent years. In order to construct three-way concept lattices, original formal context and its complement context need to be considered simultaneously. In this paper, a new method of constructing attribute-induced three-way (AE) and object-induced three-way (OE) concept lattices is proposed. Firstly, we introduce an AE-oriented composite operator and an OE-oriented composite operator that combine a pair of formal concepts coming from the concept lattices of original formal context and its complement context. Secondly, we define the candidate AE concepts and redundant AE concepts based on the AE-oriented composite operator, and define the candidate OE concepts and redundant OE concepts based on the OE-oriented composite operator, respectively. Finally, we propose an AE lattice construction algorithm and an OE lattice construction algorithm. Theoretical analysis and experimental results demonstrate that the proposed method is simple and effective, and it is a useful supplement to current three-way concept lattice construction methods. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						174	186		10.1016/j.ijar.2020.03.007													
J								A variable and a fixed ordering of intervals and their application in optimization with interval-valued functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Partial order relation of intervals; Cone of intervals; Variable ordering structure on intervals; Interval-valued functions; Interval optimization problems; Nondominated solutions	NONDOMINATED SOLUTIONS; VECTOR OPTIMIZATION	In this study, we introduce and analyze the concepts of a fixed ordering structure and a variable ordering structure on intervals. The fixed ordering structures on intervals are defined with the help of a pointed convex cone of intervals. A variable ordering is defined by a set-valued map whose values are convex cones of intervals. In the sequel, a few properties of a cone of intervals are derived. It is shown that a binary relation, defined by a convex cone of intervals, is a partial order relation on intervals; further, the relation is antisymmetric if the convex cone of intervals is pointed. Several results under which a variable ordering map of intervals satisfies the conditions of a partial ordering relation of intervals are provided. The introduced fixed and variable ordering of intervals are applied to define and characterize optimal elements of an optimization problem with interval-valued functions. Finally, we propose a numerical technique and present its algorithmic implementation to obtain the set of optimal elements of an interval optimization problem. We also provide illustrative examples to support the study. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUN	2020	121						187	205		10.1016/j.ijar.2020.03.004													
J								A simplified multi-objective particle swarm optimization algorithm	SWARM INTELLIGENCE										Particle swarm optimization; Genetic algorithm; Multi-objective optimization; Residue fluid catalytic cracking unit	DIFFERENTIAL EVOLUTION; PSO; RANKING; SEARCH	Particle swarm optimization is a popular nature-inspired metaheuristic algorithm and has been used extensively to solve single- and multi-objective optimization problems over the last two decades. Several local and global search strategies, and learning and parameter adaptation strategies have been included in particle swarm optimization to improve its performance over the years. Most of these approaches are observed to increase the number of user-defined parameters and algorithmic steps resulting in an increased complexity of the algorithm. This paper presents a simplified multi-objective particle swarm optimization algorithm in which the exploitation (guided) and exploration (random) moves are simplified using a detailed qualitative analysis of similar existing operators present in the real-coded elitist non-dominated sorting genetic algorithm and the particle swarm optimization algorithm. The developed algorithm is then tested quantitatively on 30 well-known benchmark problems and compared with a real-coded elitist non-dominated sorting genetic algorithm, and its variant with a simulated binary jumping gene operator and multi-objective non-dominated sorting particle swarm optimization algorithm. In the comparison, the developed algorithm is found to be superior in terms of convergence speed. It is also found to be better with respect to four recent multi-objective particle swarm optimization algorithms and four differential evolution variants in an extended comparative analysis. Finally, it is applied to a newly formulated industrial multi-objective optimization problem of a residue (bottom product from the crude distillation unit) fluid catalytic cracking unit where it shows a better performance than the other compared algorithms.																	1935-3812	1935-3820				JUN	2020	14	2					83	116		10.1007/s11721-019-00170-1													
J								On pruning search trees of impartial games	ARTIFICIAL INTELLIGENCE										Combinatorial game theory; Game tree; Sprague-Grundy value; Nimber; Mex function; Impartial game; Nim; Chomp; Cram	SPRAGUE-GRUNDY FUNCTION; NIM	In this paper we study computing Sprague-Grundy values for short impartial games under the normal play convention. We put forward new game-agnostic methods for effective pruning search trees of impartial games. These algorithms are inspired by the alpha-beta, a well-known pruning method for minimax trees. However, our algorithms prune trees whose node values are assigned by the mex function instead of min/max. We have verified the effectiveness of our algorithms experimentally on instances of some standard impartial games (that is Nim, Chomp, and Cram). Based on the results of our experiments we have concluded that: (1) our methods generally perform well, especially when transposition table can store only a small fraction of all game positions (which is typical when larger games are concerned); (2) one of our algorithms constitutes a more universal alternative to the state-of-the-art algorithm proposed by Lemoine and Viennot. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103262	10.1016/j.artint.2020.103262													
J								Qualitative case-based reasoning and learning	ARTIFICIAL INTELLIGENCE										Case-based reasoning; Qualitative spatial reasoning; Reinforcement learning; Robot soccer	KNOWLEDGE	The development of autonomous agents that perform tasks with the same dexterity as performed by humans is one of the challenges of artificial intelligence and robotics. This motivates the research on intelligent agents, since the agent must choose the best action in a dynamic environment in order to maximise the final score. In this context, the present paper introduces a novel algorithm for Qualitative Case-Based Reasoning and Learning (QCBRL), which is a case-based reasoning system that uses qualitative spatial representations to retrieve and reuse cases by means of relations between objects in the environment. Combined with reinforcement learning, QCBRL allows the agent to learn new qualitative cases at runtime, without assuming a pre-processing step. In order to avoid cases that do not lead to the maximum performance, QCBRL executes case-base maintenance, excluding these cases and obtaining new (more suitable) ones. Experimental evaluation of QCBRL was conducted in a simulated robot-soccer environment, in a real humanoid-robot environment and on simple tasks in two distinct gridworld domains. Results show that QCBRL outperforms traditional RL methods. As a result of running QCBRL in autonomous soccer matches, the robots performed a higher average number of goals than those obtained when using pure numerical models. In the gridworlds considered, the agent was able to learn optimal and safety policies. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103258	10.1016/j.artint.2020.103258													
J								Adapting a kidney exchange algorithm to align with human values	ARTIFICIAL INTELLIGENCE										Moral AI; Human-compatible AI; Computational social choice; Preference aggregation; Kidney exchanges	LONG-CHAINS	The efficient and fair allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who gets what-and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103261	10.1016/j.artint.2020.103261													
J								Limited lookahead in imperfect-information games	ARTIFICIAL INTELLIGENCE										Game theory; Equilibrium finding; Limited lookahead; Imperfect-information game; Nash equilibrium; Stackelberg equilibrium	PATHOLOGY	Limited lookahead has been studied for decades in perfect-information games. We initiate a new direction via two simultaneous deviation points: generalization to imperfect-information games and a game-theoretic approach. We study how one should act when facing an opponent whose lookahead is limited. We study this for opponents that differ based on their lookahead depth, based on whether they, too, have imperfect information, and based on how they break ties. We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player, showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard, NP-hard, or inapproximable. We proceed to design algorithms for computing optimal commitment strategies-for when the opponent breaks ties favorably, according to a fixed rule, or adversarially. We then experimentally investigate the impact of limited lookahead. The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium-but we prove this is not sufficient in general. Finally, we study the impact of noise in those estimates and different lookahead depths. (C) 2019 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103218	10.1016/j.artint.2019.103218													
J								Compatibility, desirability, and the running intersection property	ARTIFICIAL INTELLIGENCE										Compatibility; Coherence; Marginal problem; Conditional models; Probabilistic satisfiability; Running intersection property; Junction trees; Coherence graphs; Imprecise probability; Coherent sets of desirable gambles	PROBABILITY-MEASURES; MARGINAL PROBLEM; COHERENCE; DISTRIBUTIONS; EXISTENCE; MODELS; SATISFIABILITY; INDEPENDENCE; INEQUALITIES; EXTENSION	Compatibility is the problem of checking whether some given probabilistic assessments have a common joint probabilistic model. When the assessments are unconditional, the problem is well established in the literature and finds a solution through the running intersection property (RIP). This is not the case of conditional assessments. In this paper, we study the compatibility problem in a very general setting: any possibility space, unrestricted domains, imprecise (and possibly degenerate) probabilities. We extend the unconditional case to our setting, thus generalising most of previous results in the literature. The conditional case turns out to be fundamentally different from the unconditional one. For such a case, we prove that the problem can still be solved in general by RIP but in a more involved way: by constructing a junction tree and propagating information over it. Still, RIP does not allow us to optimally take advantage of sparsity: in fact, conditional compatibility can be simplified further by joining junction trees with coherence graphs. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103274	10.1016/j.artint.2020.103274													
J								An epistemic logic of blameworthiness	ARTIFICIAL INTELLIGENCE										Logic; Blameworthiness; Responsibility; Knowledge; Strategies; Know-how; Axiomatization; Completeness		Blameworthiness of an agent or a coalition of agents can be defined in terms of the principle of alternative possibilities: for the coalition to be responsible for an outcome, the outcome must take place and the coalition should be a minimal one that had a strategy to prevent the outcome. In this article we argue that in the settings with imperfect information, not only should the coalition have had a strategy, but it also should be the minimal one that knew that it had a strategy and what the strategy was. The main technical result of the article is a sound and complete bimodal logic that describes the interplay between knowledge and blameworthiness in strategic games with imperfect information. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103269	10.1016/j.artint.2020.103269													
J								How do fairness definitions fare? Testing public attitudes towards three algorithmic definitions of fairness in loan allocations	ARTIFICIAL INTELLIGENCE										Fairness; Public attitudes; Human experiments; Algorithmic definition	PERCEPTIONS	What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across three online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race or gender of the loan applicants). Overall, one definition (calibrated fairness) tends to be more preferred than the others, and the results also provide support for the principle of affirmative action. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103238	10.1016/j.artint.2020.103238													
J								Batch repair actions for automated troubleshooting	ARTIFICIAL INTELLIGENCE										Artificial Intelligence; Model-based diagnosis; Troubleshooting	MODEL-BASED DIAGNOSIS	Repairing a set of components as a batch is often cheaper than repairing each of them separately. A primary reason for this is that initiating a repair action and testing the system after performing a repair action often incurs non-negligible overhead. However, most troubleshooting algorithms proposed to date do not consider the option of performing batch repair actions. In this work we close this gap, and address the combinatorial problem of choosing which batch repair action to perform so as to minimize the overall repair costs. We call this problem the Batch Repair Problem (BRP) and formalize it. Then, we propose several approaches for solving it. The first seeks to choose to repair the set of components that are most likely to be faulty. The second estimates the cost wasted by repairing a given set of components, and tried to find the set of components that minimizes these costs. The third approach models BRP as a Stochastic Shortest Path Problem (SSP-MDP) [1], and solves the resulting problem with a dedicated solver. Experimentally, we compare the pros and cons of the proposed BRP algorithms on a standard Boolean circuit benchmark and a novel benchmark from the Physiotherapy domain. Results show the clear benefit of performing batch repair actions with our BRP algorithms compared to repairing components one at a time. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103260	10.1016/j.artint.2020.103260													
J								Intention as commitment toward time	ARTIFICIAL INTELLIGENCE										Intention; BDI logic; Belief revision	LOGIC; REVISION; BELIEF	In this paper we address the interplay among intention, time, and belief in dynamic environments. The first contribution is a logic for reasoning about intention, time and belief, in which assumptions of intentions are represented by preconditions of intended actions. Intentions and beliefs are coherent as long as these assumptions are not violated, i.e. as long as intended actions can be performed such that their preconditions hold as well. The second contribution is the formalization of what-if scenarios: what happens with intentions and beliefs if a new (possibly conflicting) intention is adopted, or a new fact is learned? An agent is committed to its intended actions as long as its belief-intention database is coherent. We conceptualize intention as commitment toward time and we develop AGM-based postulates for the iterated revision of belief-intention databases, and we prove a Katsuno-Mendelzon-style representation theorem. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUN	2020	283								103270	10.1016/j.artint.2020.103270													
J								Threshold optimization for F measure of macro-averaged precision and recall	PATTERN RECOGNITION										Macro-averaged F measure; Multi-label classification; Optimal threshold selection; Fixed point method		There are two different approaches to macro-averaging F measure for multi-label classification. The first encloses averaging F measure over all classes, which makes it easy to optimize. The second, extensively investigated in this paper, comprises the F measure of macro precision and recall calculation. We examine and compare these two measures when applied to different multi-label datasets. To optimize the performance measure, we adopt a widely known and proven modular approach. Classifiers sort the instances in descending order, according to a real-valued score of belonging to a corresponding class. After that, thresholds are selected so as to optimize the performance measure. If the number of classes is sufficiently large and the second alternative of macro-averaging F measure is employed, then it becomes non-trivial to define the optimal number of instances to assign to each class. Cyclic optimization procedure is widely used for threshold optimization although it results in a maximum in a special coordinate-wise sense. For a micro averaged F measure, such a coordinate-wise optimum is a maximum in the conventional sense of this term but it is not true for the F measure of macro precision and recall, which is shown by a counterexample. We reduce the problem of selecting the optimal threshold for each class to the problem of obtaining a fixed point of a specifically introduced transformation of a unit square. The suggested algorithm lets us localize all possible coordinate-wise maximums and detect the optimal among them. The approach is applied to datasets from diverse application domains. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107250	10.1016/j.patcog.2020.107250													
J								Abnormality detection in retinal image by individualized background learning	PATTERN RECOGNITION										Retinal abnormality detection; Retinal lesion detection; Computer-aided detection; Dictionary learning; Background learning; Retinal image reading	DIABETIC-RETINOPATHY; AUTOMATIC DETECTION; MATRIX FACTORIZATION; FUNDUS IMAGES; OPTIC DISC; DIAGNOSIS; IDENTIFICATION; CLASSIFICATION; SEGMENTATION; LESIONS	Computer-aided lesion detection (CAD) techniques, which provide potential for automatic early screening of retinal pathologies, are widely studied in retinal image analysis. While many CAD approaches based on lesion samples or lesion features can well detect pre-defined lesion types, it remains challenging to detect various abnormal regions (namely abnormalities) from retinal images. In this paper, we try to identify diverse abnormalities from a retinal test image by finely learning its individualized retinal background (IRB) on which retinal lesions superimpose. 3150 normal retinal images are collected as the priors for IRB learning. A preprocessing step is applied to all retinal images for spatial, scale and color normalization. Retinal blood vessels, which have individual variations in different images, are particularly suppressed from all images. A multi-scale sparse coding based learning (MSSCL) algorithm and a repeated learning strategy are proposed for finely learning the IRB. By the MSSCL algorithm, a background space is constructed by sparsely encoding the test image in a multi-scale manner using the dictionary learned from normal retinal images, which will contain more complete IRB information than any single-scale coding result. From the background space, the IRB can be well learned by low-rank approximation and thus different salient lesions can be separated and detected. The MSSCL algorithm will be iteratively repeated on the modified test image in which the detected salient lesions are suppressed, so as to further improve the accuracy of the IRB and suppress lesions in the IRB. Consequently, a high-accuracy IRB can be learned and thus both salient lesions and weak lesions that have low contrasts with the background can be clearly separated. The effectiveness and contributions of the proposed method are validated by experiments over different clinical data-sets and comparisons with the state-of-the-art CAD methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107209	10.1016/j.patcog.2020.107209													
J								MIMN-DPP: Maximum-information and minimum-noise determinantal point processes for unsupervised hyperspectral band selection	PATTERN RECOGNITION										Hyperspectral images (HSI); Unsupervised band selection; Maximum information and minimum noise (MIMN) criterion; Determinantal point processes (DPP)	MUTUAL-INFORMATION; CLASSIFICATION; ACCURACY; SUBSET	Band selection plays an important role in hyperspectral imaging for reducing the data and improving the efficiency of data acquisition and analysis whilst significantly lowering the cost of the imaging system. Without the category labels, it is challenging to select an effective and low-redundancy band subset. In this paper, a new unsupervised band selection algorithm is proposed based on a new band search criterion and an improved Determinantal Point Processes (DPP). First, to preserve the original information of hyperspectral image, a novel band search criterion is designed for searching the bands with high information entropy and low noise. Unfortunately, finding the optimal solution based on the search criteria to select a low-redundancy band subset is a NP-hard problem. To solve this problem, we consider the correlation of bands from both original hyperspectral image and its spatial information to construct a double-graph model to describe the relationship between spectral bands. Besides, an improved DPP algorithm is proposed for the approximate search of a low-redundancy band subset from the double-graph model. Experiment results on several well-known datasets show that the proposed optical band selection algorithm achieves better performance than many other state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107213	10.1016/j.patcog.2020.107213													
J								Object recognition based on convex hull alignment	PATTERN RECOGNITION										Object recognition; Shape instance detection; Depth image analysis; Convex hull; Shape alignment	REGISTRATION; FRAMEWORK; IMAGES	A common approach to recognition of objects in cluttered scenes is to generate hypotheses about objects present in the scene by matching local descriptors of point features. These hypotheses are then evaluated by measuring how well they explain a particular part of the scene. In this paper, we investigate an alternative approach, which is based on alignment of convex hulls of segments detected in a depth image with convex hulls of target 3D object models or their parts. This alignment is performed using the Convex Template Instance descriptor. This descriptor was originally proposed for fruit recognition and classification of segmented objects. We have adapted this approach to recognize objects in complex scenes. Furthermore, we propose a novel three-level hypothesis evaluation strategy which can be used to achieve highly efficient object recognition. The proposed approach is evaluated by comparison with nine state-of-the-art approaches using three challenging benchmark datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107199	10.1016/j.patcog.2020.107199													
J								Invariant subspace learning for time series data based on dynamic time warping distance	PATTERN RECOGNITION										Invariant subspace learning; Dynamic time warping (DTW); Time series; Dictionary learning	REPRESENTATION; CLASSIFICATION; SPARSE	Low-dimensional and compact representation of time series data is of importance for mining and storage. In practice, time series data are vulnerable to various temporal transformations, such as shift and temporal scaling, however, which are unavoidable in the process of data collection. If a learning algorithm directly calculates the difference between such transformed data based on Euclidean distance, the measurement cannot faithfully reflect the similarity and hence could not learn the underlying discriminative features. In order to solve this problem, we develop a novel subspace learning algorithm based on dynamic time warping (DTW) distance which is an elastic distance defined in a DTW space. The algorithm aims to minimize the reconstruction error in the DTW space. However, since DTW space is a semi-pseudo metric space, it is difficult to generalize common subspace learning algorithms for such semi-pseudo metric spaces. In this work, we introduce warp operators with which DTW reconstruction error can be approximated by reconstruction error between transformed series and their reconstructions in a subspace. The warp operators align time series data with their linear representations in the DTW space, which is in particular important for misaligned time series, so that the subspace can be learned to obtain an intrinsic basis (dictionary) for the representation of the data. The warp operators and the subspace are optimized alternatively until reaching equilibrium. Experiments results show that the proposed algorithm outperforms traditional subspace learning algorithms and temporal transform-invariance based methods (including SIDL, Kernel PCA, and SPMC et. al), and obtains competitive results with the state-of-the-art algorithms, such as BOSS algorithm. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107210	10.1016/j.patcog.2020.107210													
J								Identity-aware CycleGAN for face photo-sketch synthesis and recognition	PATTERN RECOGNITION										Convolutional neural network; Generative adversarial network; Photo-sketch synthesis; Photo-sketch recognition; Identity-aware training		Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-to-sketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107249	10.1016/j.patcog.2020.107249													
J								Deep morphological networks	PATTERN RECOGNITION										Mathematical Morphology; Deep learning; Edges detection; Denoising	IMAGES; CLASSIFICATION; SEGMENTATION; FUSION	Mathematical morphology provides powerful nonlinear operators for a variety of image processing tasks such as filtering, segmentation, and edge detection. In this paper, we propose a way to use these nonlinear operators in an end-to-end deep learning framework and illustrate them on different applications. We demonstrate on various examples that new layers making use of the morphological non-linearities are complementary to convolution layers. These new layers can be used to integrate the non-linear operations and pooling into a joint operation. We finally enhance results obtained in boundary detection using this new family of layers with just 0.01% of the parameters of competing state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107246	10.1016/j.patcog.2020.107246													
J								Guided CNN for generalized zero-shot and open-set recognition using visual and semantic prototypes	PATTERN RECOGNITION										Convolutional prototype learning; Generalized zero-shot Learning; Open set recognition		In the process of exploring the world, the curiosity constantly drives humans to cognize new things. Supposing you are a zoologist, for a presented animal image, you can recognize it immediately if you know its class. Otherwise, you would more likely attempt to cognize it by exploiting the side-information (e.g., semantic information, etc.) you have accumulated. Inspired by this, this paper decomposes the generalized zero-shot learning (G-ZSL) task into an open set recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR recognizes seen classes (if we have seen (or known) them) and rejects unseen classes (if we have never seen (or known) them before), while ZSL identifies the unseen classes rejected by the former. Simultaneously, without violating OSR's assumptions (only known class knowledge is available in training), we also first attempt to explore a new generalized open set recognition (G-OSR) by introducing the accumulated side-information from known classes to OSR. For G-ZSL, such a decomposition effectively solves the class overfitting problem with easily misclassifying unseen classes as seen classes. The problem is ubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR, introducing such semantic information of known classes not only improves the recognition performance but also endows OSR with the cognitive ability of unknown classes. Specifically, a visual and semantic prototypes-jointly guided convolutional neural network (VSG-CNN) is proposed to fulfill these two tasks (G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive experiments on benchmark datasets demonstrate the advantages of our learning framework. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107263	10.1016/j.patcog.2020.107263													
J								DeepCT: A novel deep complex-valued network with learnable transform for video saliency prediction	PATTERN RECOGNITION										Saliency prediction; Complex-valued network; Learnable transform; Convolutional LSTM		The past decade has witnessed the success of transformed domain methods for image saliency prediction. However, it is intractable to develop a transformed domain method for video saliency prediction, due to the limited choices on spatio-temporal transforms. In this paper, we propose learning the transform from training data, rather than the predefined transform in the existing methods. Specifically, we develop a novel deep Complex-valued network with learnable Transform (DeepCT) for video saliency prediction. The architecture of DeepCT includes the Complex-valued Transform Module (CTM), inverse CTM (iCTM) and Complex-valued Stacked Convolutional Long Short-Term Memory network (CS-ConvLSTM). In the CTM and iCTM, multi-scale pyramid structures are introduced, as we find that transforms at multiple receptive scales can improve the accuracy of saliency prediction. To make the CTM and iCTM "invertible", we further propose the cycle consistency loss in training DeepCT, which is composed of frame reconstruction loss and complex feature reconstruction loss. Additionally, the CS-ConvLSTM is developed to learn the temporal saliency transition across video frames. Finally, the experimental results show that our DeepCT method outperforms other 13 state-of-the-art methods for video saliency prediction. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107234	10.1016/j.patcog.2020.107234													
J								Radial-Based Undersampling for imbalanced data classification	PATTERN RECOGNITION										Machine learning; Classification; Imbalanced data; Undersampling; Radial basis functions	DATA-SETS; SMOTE	Data imbalance remains one of the most widespread problems affecting contemporary machine learning. The negative effect data imbalance can have on the traditional learning algorithms is most severe in combination with other dataset difficulty factors, such as small disjuncts, presence of outliers and insufficient number of training observations. Aforementioned difficulty factors can also limit the applicability of some of the methods of dealing with data imbalance, in particular the neighborhood-based oversampling algorithms based on SMOTE. Radial-Based Oversampling (RBO) was previously proposed to mitigate some of the limitations of the neighborhood-based methods. In this paper we examine the possibility of utilizing the concept of mutual class potential, used to guide the oversampling process in RBO, in the undersampling procedure. Conducted computational complexity analysis indicates a significantly reduced time complexity of the proposed Radial-Based Undersampling algorithm, and the results of the performed experimental study indicate its usefulness, especially on difficult datasets. (C) 2020 The Author. Published by Elsevier Ltd.																	0031-3203	1873-5142				JUN	2020	102								107262	10.1016/j.patcog.2020.107262													
J								Deep and joint learning of longitudinal data for Alzheimer's disease prediction	PATTERN RECOGNITION										Alzheimer's disease; Longitudinal scores prediction; Joint learning; Correntropy; Deep polynomial network	MILD COGNITIVE IMPAIRMENT; FEATURE REPRESENTATION; BASE-LINE; SPARSITY; CLASSIFICATION; DEMENTIA; ATROPHY; IMAGES; MODEL	Alzheimer's disease (AD) is an irreversible and progressive neurodegenerative disease. The close AD monitoring of this disease is essential for the patient treatment plan adjustment. For AD monitoring, clinical score prediction via neuroimaging data is highly desirable since it is able to reveal the disease status, adequately. For this task, most previous studies are focused on a single time point without considering relationship between neuroimaging data (e.g., Magnetic Resonance Imaging (MRI)) and clinical scores at multiple time points. Differing from these studies, we propose to build a framework based on longitudinal multiple time points data to predict clinical scores. Specifically, the proposed framework consists of three parts, feature selection based on correntropy regularized joint learning, feature encoding based on deep polynomial network, and ensemble learning for regression via the support vector regression method. Two scenarios are designed for scores prediction. Namely, scenario 1 uses the baseline data to achieve the longitudinal scores prediction, while scenario 2 utilizes all the previous time points data to obtain the predicted scores at the next time point, which can improve the score prediction's accuracy. Meanwhile, the missing clinical scores at longitudinal multiple time points are imputated to solve the incompleteness of the data. Extensive experiments on the public database of Alzheimer's Disease Neuroimaging Initiative (ADNI) demonstrate that our proposed framework can effectively reveal the relationship between clinical score and MRI data and outperforms the state-of-the-art methods in scores prediction. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107247	10.1016/j.patcog.2020.107247													
J								A novel density-based clustering algorithm using nearest neighbor graph	PATTERN RECOGNITION										Density-based clustering; Nearest neighbor graph; DBSCAN	DBSCAN; SEARCH	Density-based clustering has several desirable properties, such as the abilities to handle and identify noise samples, discover clusters of arbitrary shapes, and automatically discover of the number of clusters. Identifying the core samples within the dense regions of a dataset is a significant step of the density-based clustering algorithm. Unlike many other algorithms that estimate the density of each samples using different kinds of density estimators and then choose core samples based on a threshold, in this paper, we present a novel approach for identifying local high-density samples utilizing the inherent properties of the nearest neighbor graph (NNG). After using the density estimator to filter noise samples, the proposed algorithm ADBSCAN in which "A" stands for "Adaptive" performs a DBSCAN-like clustering process. The experimental results on artificial and real-world datasets have demonstrated the significant performance improvement over existing density-based clustering algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107206	10.1016/j.patcog.2020.107206													
J								Fingerprint pore matching using deep features	PATTERN RECOGNITION										Fingerprint recognition; Pore representation; Direct pore matching; Convolutional neural networks		As a popular living fingerprint feature, sweat pore has been adopted to build robust high resolution automated fingerprint recognition systems (AFRSs). Pore matching is an important step in high resolution fingerprint recognition. This paper proposes a novel pore matching method with high recognition accuracy. The method mainly solves the pore representation problem in the state-of-the-art direct pore matching method. By making full use of the diversity and large quantities of sweat pores on fingerprints, deep convolutional networks are carefully designed to learn a deep feature (denoted as DeepPoreID) for each pore. The inter-class difference and intra-class similarity of pore patch pairs can be well solved using deep learning. The DeepPoreID is then used to describe the local feature for each pore and finally integrated into the classical direct pore matching method. More specifically, pore patches, which are cropped from both Query and Template fingerprint images, are imported into the well-trained networks to generate DeepPoreID for pore representation. The similarity between those DeepPoreIDs are then obtained by calculating the Euclidian Distance between them. Subsequently, one-to-many coarse pore correspondences are established via comparing their similarity. Finally, classical Weighted RANdom SAmple Consensus (WRANSAC) is employed to pick true pore correspondences from coarse ones. The experiments carried on the two public high resolution fingerprint database have shown the effectiveness of the proposed DeepPoreID, especially for fingerprint matching with small image size. Meanwhile, better recognition accuracy is achieved by the proposed method when compared with the existing state-of-the-art methods. About 35% rise in equal error rate (EER) and about 30% rise in FMR1000 when compared with the best result evaluated on the database with image size of 320 x 240 pixels. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107208	10.1016/j.patcog.2020.107208													
J								Synthesizing Talking Faces from Text and Audio: An Autoencoder and Sequence-to-Sequence Convolutional Neural Network	PATTERN RECOGNITION										Convolutional neural network; Autoencoder; Regression; Face landmark; Face tracking; Lip sync; Video; Audio	SPEECH SYNTHESIS	Synthesizing talking face from text and audio is increasingly becoming a direction in human-machine and face-to-face interactions. Although progress has been made, several existing methods either have unsatisfactory co-articulation modeling effects or ignore relations between adjacent inputs. Moreover, some of these methods often train models on shaky head videos or utilize linear-based face parameterization strategies, which further decrease synthesized quality. To address the above issues, this study proposes a sequence-to-sequence convolutional neural network to automatically synthesize talking face video with accurate lip sync. First, an advanced landmark location pipeline is used to accurately locate the facial landmarks, which can effectively reduce landmark shake. Then, a part-based autoencoder is presented to encode face images into a low-dimensional space and obtain compact representations. A sequence-to-sequence network is also presented to encode the relation of neighboring frames with multiple loss functions, and talking faces are synthesized through a reconstruction strategy with a decoder. Experiments on two public audio-visual datasets and a new dataset called CCTV news demonstrate the effectiveness of the proposed method against other state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107231	10.1016/j.patcog.2020.107231													
J								Smooth robust tensor principal component analysis for compressed sensing of dynamic MRI	PATTERN RECOGNITION										Robust tensor principal component analysis; Compressed sensing; Low rank tensor approximation; Tensor total variation; Dynamic magnetic resonance imaging	LOW-RANK; THRESHOLDING ALGORITHM; IMAGE-RECONSTRUCTION; UNDERSAMPLED (K; DECOMPOSITION; SEPARATION; SPARSITY; T)-SPACE	Dynamic magnetic resonance imaging (DMRI) often requires a long time for measurement acquisition, and it is a crucial problem about the enhancement of reconstruction quality from a limited set of undersamples. The low-rank plus sparse decomposition model, which is also called robust principal component analysis (RPCA), is widely used for reconstruction of DMRI data in the model-based way. In this paper, considering that DMRI data are naturally in tensor form with block-wise smoothness, we propose a smooth robust tensor principal component analysis (SRTPCA) method for DMRI reconstruction. Compared with classical RPCA approaches, the low rank and sparsity terms are extended to tensor versions to fully exploit the spatial and temporal data structures. Moreover, a tensor total variation regularization term is used to encourage the multi-dimensional block-wise smoothness for the reconstructed DMRI data. The relaxed convex optimization model can be divided into several sub-problems by the alternating direction method of multipliers. Numerical experiments on cardiac perfusion and cine datasets demonstrate that the proposed SRTPCA method outperforms the state-of-the-art ones in terms of recovery accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107252	10.1016/j.patcog.2020.107252													
J								Semantic-aware scene recognition	PATTERN RECOGNITION										Scene recognition; Deep learning; Convolutional neural networks; Semantic segmentation		Scene recognition is currently one of the top-challenging research fields in computer vision. This may be due to the ambiguity between classes: images of several scene classes may share similar objects, which causes confusion among them. The problem is aggravated when images of a particular scene class are notably different. Convolutional Neural Networks (CNNs) have significantly boosted performance in scene recognition, albeit it is still far below from other recognition tasks (e.g., object or image recognition). In this paper, we describe a novel approach for scene recognition based on an end-to-end multi-modal CNN that combines image and context information by means of an attention module. Context information, in the shape of a semantic segmentation, is used to gate features extracted from the RGB image by leveraging on information encoded in the semantic representation: the set of scene objects and stuff, and their relative locations. This gating process reinforces the learning of indicative scene content and enhances scene disambiguation by refocusing the receptive fields of the CNN towards them. Experimental results on three publicly available datasets show that the proposed approach outperforms every other state-of-the-art method while significantly reducing the number of network parameters. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107256	10.1016/j.patcog.2020.107256													
J								Unsupervised representation learning by discovering reliable image relations	PATTERN RECOGNITION										Unsupervised learning; Visual representation learning; Unsupervised image classification; Mining reliable relations; Divide-and-conquer	FEATURES	Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107107	10.1016/j.patcog.2019.107107													
J								Sparse filtered SIRT for electron tomography	PATTERN RECOGNITION										Tomographic reconstruction; Filtered backprojection; Filter optimization; Filtered backprojection within SIRT	CT IMAGE-RECONSTRUCTION; ALGORITHM; SHAPE; ART	Electron tomographic reconstruction is a method for obtaining a three-dimensional image of a specimen with a series of two dimensional microscope images taken from different viewing angles. Filtered backprojection, one of the most popular tomographic reconstruction methods, does not work well under the existence of image noises and missing wedges. This paper presents a new approach to largely mitigate the effect of noises and missing wedges. We propose a novel filtered backprojection that optimizes the filter of the backprojection operator in terms of a reconstruction error. This data-dependent filter adaptively chooses the spectral domains of signals and noises, suppressing the noise frequency bands, so it is very effective in denoising. We also propose the new filtered backprojection embedded within the simultaneous iterative reconstruction iteration for mitigating the effect of missing wedges. Our numerical study is presented to show the performance gain of the proposed approach over the state-of-the-art. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107253	10.1016/j.patcog.2020.107253													
J								Appropriateness of performance indices for imbalanced data classification: An analysis	PATTERN RECOGNITION										Imbalanced classification; Performance evaluation indices; Precision; Recall; GMean; Area under the curve	ROC; RECALL; IMPACT	Indices quantifying the performance of classifiers under class-imbalance, often suffer from distortions depending on the constitution of the test set or the class-specific classification accuracy, creating difficulties in assessing the merit of the classifier. We identify two fundamental conditions that a performance index must satisfy to be respectively resilient to altering number of testing instances from each class and the number of classes in the test set. In light of these conditions, under the effect of class imbalance, we theoretically analyze four indices commonly used for evaluating binary classifiers and five popular indices for multi-class classifiers. For indices violating any of the conditions, we also suggest remedial modification and normalization. We further investigate the capability of the indices to retain information about the classification performance over all the classes, even when the classifier exhibits extreme performance on some classes. Simulation studies are performed on high dimensional deep representations of subset of the ImageNet dataset using four state-of-the-art classifiers tailored for handling class imbalance. Finally, based on our theoretical findings and empirical evidence, we recommend the appropriate indices that should be used to evaluate the performance of classifiers in presence of class-imbalance. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107197	10.1016/j.patcog.2020.107197													
J								Auto-weighted multi-view co-clustering via fast matrix factorization	PATTERN RECOGNITION										Co-clustering; Multi-view data; Matrix factorization; Auto-weighted	SCALE	Multi-view clustering is a hot research topic in machine learning and pattern recognition, however, it remains high computational complexity when clustering multi-view data sets. Although a number of approaches have been proposed to accelerate the computational efficiency, most of them do not consider the data duality between features and samples. In this paper, we propose a novel co-clustering approach termed as Fast Multi-view Bilateral K-means (FMVBKM), which can implement clustering task on row and column of the input data matrix, simultaneously. Specifically, FMVBKM applies the relaxed K-means clustering technique to multi-view data clustering. In addition, to decrease information loss in matrix factorization, we further introduce a new co-clustering method named as Fast Multi-view Matrix Tri-Factorization (FMVMTF). Extensive experimental results on six benchmark data sets show that the proposed two approaches not only have comparable clustering performance but also present the high computational efficiency, in comparison with state-of-the-art multi-view clustering methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107207	10.1016/j.patcog.2020.107207													
J								Online signature verification using single-template matching with time-series averaging and gradient boosting	PATTERN RECOGNITION										Biometrics; Forensics; Signature verification; Template matching; Variable importance; Gradient boosting; Dynamic time warping (DTW); Euclidean barycenter-based DTW barycenter averaging (EB-DBA)	DYNAMIC SIGNATURES; FEATURE-EXTRACTION; INFORMATION; FEATURES; VECTOR; ALGORITHM; DTW	In keeping with recent developments in artificial intelligence in the era of big data, there is a demand for online signature verification systems that operate at high speeds, provide a high level of security, and allow high tolerances while achieving sufficient performance. In response to these needs, the present study proposes a novel, single-template strategy using a mean template set and weighted multiple dynamic time warping (DTW) distances for a function-based approach to online signature verification. Specifically, to obtain an effective mean template for each feature while reflecting intra-user variability between all the reference samples, we adopt a novel time-series averaging method based on Euclidean barycenterbased DTW barycenter averaging. Then, by using the mean template set, we calculate multiple DTW distances from multivariate time series based on dependent and independent warping. Finally, to boost the discriminative power, we apply a weighting scheme using a gradient boosting model to efficiently combine the multiple DTW distances. Experimental results using the common SVC2004 Task1/Task2 and MCYT-100 signature datasets confirm that the proposed method is effective for online signature verification. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107227	10.1016/j.patcog.2020.107227													
J								Image decomposition based matrix regression with applications to robust face recognition	PATTERN RECOGNITION										Matrix regression; Low rank; Image decomposition; Pattern classification	SPARSE; REPRESENTATION; CLASSIFICATION; VERIFICATION	The previous matrix regression based methods mainly focus on designing a robust error term to characterize the occlusion and illumination changes. In actually, it is very challenging to give a strong model for solving the original images directly since the images contains rich and complex structure information. To address this problem, we aim to simplify the complex images and propose a simple and robust matrix regression based classification model. In our method, we firstly employ the local gradient distribution to decompose the image into a series of gradient images (LID for short). Each gradient image reveals the local structure information in different gradient orientations. Subsequently, we consider each gradient image as the diagonal block element and construct the diagonal block matrix for image representation. Nuclear norm based matrix regression model (NMR) is then applied to complete the classification tasks. The proposed model can be called ID-NMR for short. We further design a fast ADMM optimization algorithm to solve the proposed ID-NMR due to the fact that the big diagonal block matrix will increase the computational load. Experimental results show that the proposed method performs favorably compared with state-of-the-art regression based classification methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107204	10.1016/j.patcog.2020.107204													
J								A reduced universum twin support vector machine for class imbalance learning	PATTERN RECOGNITION										Universum; Rectangular kernel; Class imbalance; Imbalance ratio; Twin support vector machine	PINBALL LOSS; SVMS	In most of the real world datasets, there is an imbalance in the number of samples belonging to different classes. Various pattern classification problems such as fault or disease detection involve class imbalanced data. The support vector machine (SVM) classifier becomes biased towards the majority class due to class imbalance. Moreover, in the existing SVM based techniques for class imbalance, there is no information about the distribution of data. Motivated by the idea of prior information about data distribution, a reduced universum twin support vector machine for class imbalance learning (RUTSVM-CIL) is proposed in this paper. For the first time, universum learning is incorporated with SVM to solve the problem of class imbalance. Oversampling and undersampling of data is performed to remove the imbalance in the classes. The universum data points are used to give prior information about the data. To reduce the computation time of our universum based algorithm, we use a small sized rectangular kernel matrix. The reduced kernel matrix needs less storage space, and thus applicable for large scale imbalanced datasets. Comprehensive experimentation is performed on various synthetic, real world and large scale imbalanced datasets. In comparison to the existing approaches for class imbalance, the proposed RUTSVM-CIL gives better generalization performance for most of the benchmark datasets. Also, the computation cost of RUTSVM-CIL is very less, making it suitable for real world applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107150	10.1016/j.patcog.2019.107150													
J								HscoreNet: A Deep network for estrogen and progesterone scoring using breast IHC images	PATTERN RECOGNITION										Breast; Estrogen; Progesterone; Encoder; Decoder	U-NET; RECEPTOR EXPRESSION; SPARSE AUTOENCODER; CANCER; SEGMENTATION; NUCLEI; CLASSIFICATION; ER; CARCINOMA; FRAMEWORK	Estrogen and progesterone receptors serve as an important predictive and prognostic biomarkers for breast cancer immunohistological analysis. For breast cancer prognosis, pathologists manually compute the score based on the visual expression and the number of immunopositive and immunonegative nuclei. This manual scoring technique is time-consuming, cumbersome, expensive, error-prone, and susceptible to intra- and interobserver ambiguities. To solve these issues, we proposed a deep neural network (i.e., HscoreNet), which consists of three parts, i.e., encoder, decoder, and scoring layer. A total of 600 (300 ER and 300 PR) regions of interest at 40 xmagnification from 100 histologically confirmed slides were used in this study. The size of each region of interest was 2048 x 1536 pixels (width xheight). The encoder layer has been used to transform input pixels into a lower-dimensional representation, whereas the decoder reconstructs the output of the encoder through minimization of a cost function. The decoder generates an image that only contains immunopositive and immunonegative nuclei. The output of the decoder is fed to the input of the scoring layer. This layer computes the Histo-score or H-score based on the staining intensity, the color expression, and the number of immunopositive and immunonegative nuclei. Pathologists compute this score to subcategorize the cancer grades and to decide proper treatment procedures. Our proposed approach is affordable, accurate, and fast. We achieved excellent performance, with 95.87% precision and 94.53% classification accuracy. Our proposed approach streamlines the human error-prone and time-consuming process. This methodology can also be used for other types of histology and immunohistology image segmentation and scoring. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107200	10.1016/j.patcog.2020.107200													
J								Domain adaptive representation learning for facial action unit recognition	PATTERN RECOGNITION										Feature fusion; Feature fine-tuning; Facial action unit recognition; Deep fusion; Multi-Modal representation learning	3D	Learning robust representations for applications with multiple modalities of input can have a significant impact on improving performance. Traditional representation learning methods rely on projecting the input modalities to a common subspace to maximize agreement amongst the modalities for a particular task. We propose a novel approach to representation learning that uses a latent representation decoder to reconstruct the target modality and thereby employ the target modality purely as a supervision signal for discovering correlations between the modalities. Through cross modality supervision, we demonstrate that the learnt representation is able to improve upon the performance of the task of facial action unit (AU) recognition over modality specific representations and even their fused counterparts. As an extension, we explore a new transfer learning technique to adapt the learnt representation to the target domain. We also present a shared representation based feature fusion methodology to improve the performance of any multi-modal system. Our experiments on three AU recognition datasets - MMSE, BP4D and DISFA, show strong performance gains producing state-of-the-art results in spite of the absence of data from a modality. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				JUN	2020	102								107127	10.1016/j.patcog.2019.107127													
J								Unsupervised domain adaptive re-identification: Theory and practice	PATTERN RECOGNITION										Person re-identification; Unsupervised domain adaptation	PERSON REIDENTIFICATION; ADAPTATION; DEEP; DISSIMILARITY; ATTRIBUTE	We study the problem of unsupervised domain adaptive re-identification (re-ID) which is an active topic in computer vision but lacks a theoretical foundation. We first extend existing unsupervised domain adaptive classification theories to re-ID tasks. Concretely, we introduce some assumptions on the extracted feature space and then derive several loss functions guided by these assumptions. To optimize them, a novel self-training scheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively makes guesses for unlabeled target data based on an encoder and trains the encoder based on the guessed labels. Extensive experiments on unsupervised domain adaptive person re-ID and vehicle re-ID tasks with comparisons to the state-of-the-arts confirm the effectiveness of the proposed theories and self-training framework. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107173	10.1016/j.patcog.2019.107173													
J								Multi-scale differential feature for ECG biometrics with collective matrix factorization	PATTERN RECOGNITION										ECG biometrics; Multi-scale differential feature; Collective matrix factorization; Feature learning	RECOGNITION; AUTHENTICATION; ELECTROCARDIOGRAM	Electrocardiogram (ECG) biometrics has recently received considerable attention and is considered to be a promising biometric trait. Although some promising results on ECG biometrics have been reported, it is still challenging to perform this technique robustly and precisely. To address these issues, this paper presents a novel ECG biometrics framework: Multi-Scale Differential Feature for ECG biometrics with Collective Matrix Factorization (CMF). First, we extract the Multi-Scale Differential Feature (MSDF) from the one-dimensional ECG signal and then fuse MSDF with 1DMRLBP to generate the MSDF-1DMRLBP, which acts as the base feature of the ECG signal. Second, to extract discriminative information from the intermediate base features, we leverage the CMF technique to generate the final robust ECG representations by simultaneously embedding MSDF-1DMRLBP and label information. Consequently, the final robust features could preserve the intra-subject and inter-subject similarities. Extensive experiments are conducted on four ECG databases, and the results demonstrate that the proposed method can outperform the state-of-the-art in terms of both accuracy and efficiency. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107211	10.1016/j.patcog.2020.107211													
J								A quadrilateral scene text detector with two-stage network architecture	PATTERN RECOGNITION										Scene text detection; Deep learning; Quadrilateral regression		Many of the state-of-the-art methods can only localize scene texts with rotated rectangle boundaries, which may result in incorrect rectification of the detected scene texts and erroneous elimination of proposals or detections during non-maximum suppression (NMS). A few existing methods that can detect scene texts with quadrilateral boundaries, are just based on one-stage architectures or sliding windows scanning and thus have sub-optimal performance. To address these problems, we propose an end-to-end two-stage network architecture for scene text detection, which can accurately localize scene texts with quadrilateral boundaries. At the first stage, we propose a quadrilateral region proposal network (QRPN) for generating quadrilateral proposals, based on a newly proposed quadrilateral regression algorithm. At the second stage, we introduce a novel weighted RoI pooling module with learned weight masks to pool the features, and then classify the proposals and refine their shapes with the proposed quadrilateral regression algorithm again. Specially, during training, we adopt a dual-branch structure of detection heads, that is, jointly train the quadrilateral detection head and an additional rotated rectangle detection head. Furthermore, we develop an accelerated NMS algorithm with O(nlogn) complexity, for redundant quadrilateral text proposals and detections eliminating during the first and the second stage, respectively. Experiments on several challenging benchmarks demonstrate the superior performance of the proposed method, which achieves state-of-the-art results on widely used benchmarks ICDAR 2017 MLT, RCTW, and ICDAR 2015 Incidental Scene Text benchmark. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107230	10.1016/j.patcog.2020.107230													
J								Long video question answering: A Matching-guided Attention Model	PATTERN RECOGNITION										Long video QA; Matching-guided attention	NETWORK; IMAGE	Existing video question answering methods answer given questions based on short video snippets. The underlying assumption is that the visual content indicating the ground truth answer ubiquitously exists in the snippet. It might be problematic for long video applications, since involving large numbers of answer-irrelevant snippets will dramatically degenerate the performance. To deal with this issue, we focus on a rarely investigated but practically important problem, namely long video QA, by predicting answers directly from long videos rather than manually pre-extracted short video snippets. We accordingly propose a Matching-guided Attention Model (MAM) which jointly extracts question-related video snippets and predicts answers in a unified framework. To localize questions accurately and efficiently, we calculate corresponding matching scores and boundary regression results for candidate video snippet proposals generated by sliding windows of limited granularity. Guided by the matching scores, the model pays different attention to the extracted video snippet proposals for each question. Finally, we use the attended visual features along with the question to predict the answer in a classification manner. A key obstacle to training our model is that publicly available video QA datasets only contain short videos especially designed for short video QA. Thus, we generate two new datasets for this task on the top of TACoS Multi-level dataset and MSR-VTT dataset by generating QA pairs from the video captions, called TACoS-QA and MSR-VTT-QA. Experimental results show the effectiveness of our proposed method on both datasets by comparing with two short video QA methods and a baseline method. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107248	10.1016/j.patcog.2020.107248													
J								Blind single image super-resolution with a mixture of deep networks	PATTERN RECOGNITION										Blind super-resolution; Mixture of networks; Blur kernels; Lower bound; Latent variables		Existing deep neural network based image super-resolution (SR) methods are mostly designed for nonblind cases, where the blur kernel used to generate the low-resolution (LR) images is assumed to be known and fixed. However, this assumption does not hold in many real scenarios. Motivated by the observation that SR of LR images generated by different blur kernels are essentially different but also correlated, we propose a mixture model of deep networks, which is capable of clustering SR tasks of different blur kernels into a set of groups. Each group is composed of correlated SR tasks with similar blur kernels and can be effectively handled by a combination of specific networks in the mixture model. To achieve automatic SR tasks clustering and network selection, we model the blur kernel with a latent variable, which is inferred from the input image by an encoder network. Since the ground-truth of the latent variable is unknown in the training stage, we initialize the encoder network by pre-training it on the blur kernel classification task to avoid trivial solutions. To jointly train the mixture model and the encoder network, we further derive a lower bound of the likelihood function, which circumvents the intractability in direct maximum likelihood estimation. Extensive evaluations are performed on benchmark data sets and validate the effectiveness of the proposed method. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				JUN	2020	102								107169	10.1016/j.patcog.2019.107169													
J								Scene recognition: A comprehensive survey	PATTERN RECOGNITION										Scene recognition; Patch feature encoding; Spatial layout pattern learning; Discriminative region detection; Convolutional neural networks; Deep learning	IMAGE CLASSIFICATION; FEATURES; MODEL; CATEGORIZATION; REPRESENTATION; CODEBOOKS; OBJECT; KERNEL	With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107205	10.1016/j.patcog.2020.107205													
J								An accelerated correlation filter tracker	PATTERN RECOGNITION										Visual object tracking; Discriminative correlation filters; Accelerated optimisation; Alternating direction method of multipliers	OBJECT TRACKING	Recent visual object tracking methods have witnessed a continuous improvement in the state-of-the-art with the development of efficient discriminative correlation filters (DCF) and robust deep neural network features. Despite the outstanding performance achieved by the above combination, existing advanced trackers suffer from the burden of high computational complexity of the deep feature extraction and online model learning. We propose an accelerated ADMM optimisation method obtained by adding a momentum to the optimisation sequence iterates, and by relaxing the impact of the error between DCF parameters and their norm. The proposed optimisation method is applied to an innovative formulation of the DCF design, which seeks the most discriminative spatially regularised feature channels. A further speed up is achieved by an adaptive initialisation of the filter optimisation process. The significantly increased convergence of the DCF filter is demonstrated by establishing the optimisation process equivalence with a continuous dynamical system for which the convergence properties can readily be derived. The experimental results obtained on several well-known benchmarking datasets demonstrate the efficiency and robustness of the proposed ACFT method, with a tracking accuracy comparable to the start-of-the-art trackers. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107172	10.1016/j.patcog.2019.107172													
J								A novel image-dehazing network with a parallel attention block	PATTERN RECOGNITION										Image dehazing; Spatial attention mechanism; Channel-wise attention mechanism; Encoder-decoder; CNN	ALGORITHM	Image dehazing is a very important pre-processing step to many computer vision tasks such as object recognition and tracking. However, it is a challenging problem because the physical parameters of imaging, e.g. the depth information of scene pixels and the attenuation model, are usually unknown. Based on a physical model, different methods have been proposed to recover these parameters. Existing convolutional neural networks (CNNs) based methods try to solve the image dehazing problem using an end-to-end network to learn a direct mapping between a hazy image and its corresponding clear image. But the representational ability, spatial variant ability and dehazing capability of these network models are hindered by treating all the spatial and channel-wise features indiscriminately. Hence, we propose an end-to-end dehazing network with a parallel spatial/channel-wise attention block for capturing more informative spatial and channel-wise features respectively. Specifically, based on the encoder-decoder framework with a pyramid pooling operation, a novel parallel spatial/channel-wise attention block is proposed and applied to the end of the encoder for guiding the decoder to reconstruct better clear images. In the spatial/channel-wise attention block, the spatial attention module and the channel-wise attention module are connected in parallel, where the spatial attention module highlights important spatial positions of features. Meanwhile, the channel-wise module exploits inter-dependencies among the channel-wise features. Extensive experiments demonstrate that our network with a parallel spatial/channel-wise attention block can achieve better accuracy and visual results over state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107255	10.1016/j.patcog.2020.107255													
J								PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by Learning	PATTERN RECOGNITION										Ego-motion; Pose estimation; Deep learning; Recurrent Convolutional Neural Networks; Data augmentation	ODOMETRY	Visual ego-motion estimation is one of the longstanding problems which estimates the movement of cameras from images. Learning based ego-motion estimation methods have seen an increasing attention since its desirable properties of robustness to image noise and camera calibration independence. In this work, we propose a data-driven approach of learning based visual ego-motion estimation for a monocular camera. We use an end-to-end learning approach in allowing the model to learn a map from input image pairs to the corresponding ego-motion, which is parameterized as 6-DoF transformation matrix. We introduce a two-module Long-term Recurrent Convolutional Neural Networks called PoseConvGRU. The feature-encoding module encodes the short-term motion feature in an image pair, while the memorypropagating module captures the long-term motion feature in the consecutive image pairs. The visual memory is implemented with convolutional gated recurrent units, which allows propagating information over time. At each time step, two consecutive RGB images are stacked together to form a 6-channel tensor for feature-encoding module to learn how to extract motion information and estimate poses. The sequence of output maps is then passed through the memory-propagating module to generate the relative transformation pose of each image pair. In addition, we have designed a series of data augmentation methods to avoid the overfitting problem and improve the performance of the model when facing challengeable scenarios such as high-speed or reverse driving. We evaluate the performance of our proposed approach on the KITTI Visual Odometry benchmark and Malaga 2013 Dataset. The experiments show a competitive performance of the proposed method to the state-of-the-art monocular geometric and learning methods and encourage further exploration of learning-based methods for the purpose of estimating camera ego-motion even though geometrical methods demonstrate promising results. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107187	10.1016/j.patcog.2019.107187													
J								Uncertain motion tracking based on convolutional net with semantics estimation and region proposals	PATTERN RECOGNITION										Correlation filter; Semantics estimation; Visual tracking; Region proposals; Contextual information	VISUAL TRACKING; OBJECT; NETWORK; FILTER	In real world, most of the tracking methods suffer from uncertain motion, which may make the tracker failure because of a local search window with the motion smooth assumption. To address this problem, a novel tracking framework based on convolutional net with semantics estimation and region proposals is proposed. Firstly, we present a semantics object proposals generation strategy, including category-level semantics proposals, one-object-level semantics estimation and semantics-contextual proposals generation, to obtain a few of high-quality object-oriented proposals covering uncertain motion. Secondly, combining the globally sparse semantics region proposals prediction and correlation filter prediction, a hybrid semantics tracking algorithm is proposed, which obtains a coarse object location by the decision of multiple response maps. Finally, we learn and train independent correlation filter to estimate the scale of target for a higher tracking accuracy. Extensive experiments on two visual tracking benchmarks and results demonstrate our method achieves state-of-the-art performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107232	10.1016/j.patcog.2020.107232													
J								Multi-camera multi-player tracking with deep player identification in sports video	PATTERN RECOGNITION										Identity switch; Multi-target multi-camera tracking; Object detection; Player identification; CNN	PEOPLE	Identity switches caused by inter-object interactions remain a critical problem for multi-player tracking in real-world sports video analysis. Existing approaches utilizing the appearance model is difficult to associate detections and preserve identities due to the similar appearance of players in the same team. Instead of the appearance model, we propose a distinguishable deep representation for player identity in this paper. A robust multi-player tracker incorporating with deep player identification is further developed to produce identity-coherent trajectories. The framework consists of three parts: (1) the core component, a Deep Player Identification (DeepPlayer) model that provides an adequate discriminative feature through the coarse-to-fine jersey number recognition and the pose-guided partial feature embedding; (2) an Individual Probability Occupancy Map (IPOM) model for players 3D localization with ID; and (3) a K-Shortest Path with ID (KSP-ID) model that links nodes in the flow graph by a proposed player ID correlation coefficient. With the distinguishable identity, the performance of tracking is improved. Experiment results illustrate that our framework handles the identity switches effectively, and outperforms state-of-the-art trackers on the sports video benchmarks. (C) 2020 Published by Elsevier Ltd.																	0031-3203	1873-5142				JUN	2020	102								107260	10.1016/j.patcog.2020.107260													
J								Ensemble deep learning for automated visual classification using EEG signals	PATTERN RECOGNITION										Ensemble deep learning; Bagging algorithm; EEG; Automated visual classification	SINGLE-TRIAL CLASSIFICATION; POTENTIALS; NETWORKS; TASKS	This paper proposes an automated visual classification framework in which a novel analysis method (LSTMS-B) of EEG signals guides the selection of multiple networks that leads to the improvement of classification performance. The method, called LSTMS-B, combines deep learning and ensemble learning to extract the category-dependent representations of EEG signals. Specifically, it introduces Swish activation function into traditional LSTM which reduces the effect of vanishing gradient and optimize the training process. Besides, the Bagging theory is applied to increase the generalization. The LSTMS-B method reaches the average precision of 97.13% for learning EEG visual presentations, which greatly outperforms traditional LSTM network and other contrast models. Then, to verify its application value, a ResNet-based regression is trained using original images and relevant EEG representations learned before. We use the output of the regression as the features to classify the images, and finally obtain the average classification accuracy of 90.16%. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102									10.1016/j.patcog.2019.107147													
J								Scale robust deep oriented-text detection network	PATTERN RECOGNITION										Scene text detection; One-stage; Scale robust		Text detection is a prerequisite of text recognition, and multi-oriented text detection is a hot topic recently. The existing multi-oriented text detection methods fall short when facing two issues: 1) text scales change in a wide range, and 2) there exists the foreground-background class imbalance. In this paper, we propose a scale-robust deep multi-oriented text-detection model, which not only has the efficiency of the one-stage deep detection model, but also has the comparable accuracy of the two-stage deep text-detection model. We design the feature refining block to fuse multi-scale context features for the purpose of keeping text detection in a higher-resolution feature map. Moreover, in order to mitigate the foreground-background class imbalance, Focal Loss is adopted to up weight the hard-classified samples. Our method is implemented on four benchmark text datasets: ICDAR2013, ICDAR2015, COCOText and MSRA-TD500. The experimental results demonstrate that our method is superior to the existing one-stage deep text-detection models and comparable to the state-of-the-art text detection methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUN	2020	102								107180	10.1016/j.patcog.2019.107180													
J								Reducing distance computations for distance-based outliers	EXPERT SYSTEMS WITH APPLICATIONS										Distance-based outliers; Outlier detection; Parallel and distributed algorithms	MINING OUTLIERS; STRATEGIES	The mining task of outlier detection is essential in many expert and intelligent systems exploited in a wide range of applications, from intrusion detection to molecular biology. In some of such applications the ability to process large amounts of data in a very short time can be critical, for instance in intrusion and fraud detection. This paper explores a solution for the optimisation of an exact, unsupervised outlier detection method by avoiding unnecessary computations, and therefore reducing the running time and making the method usable also in settings where response times are crucial. In particular, we enhance the SolvingSet-based approach by using a mechanism that exploits the knowledge learned during the algorithm execution and avoids a large amount of distance computations. We demonstrate the strength of the proposed solution, named FastSolvingSet, through both theoretical and experimental analysis. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113215	10.1016/j.eswa.2020.113215													
J								Big Data with deep learning for benchmarking profitability performance in project tendering	EXPERT SYSTEMS WITH APPLICATIONS										Big Data; Project tendering; Text mining; Deep learning; Benchmarking KPIs	CONSTRUCTION; RISK; MANAGEMENT; MODEL; PRICE; PREDICTION	A reliable benchmarking system is crucial for the contractors to evaluate the profitability performance of project tenders. Existing benchmarks are ineffective in the tender evaluation task for three reasons. Firstly, these benchmarks are mostly based on the profit margins as the only key performance indicator (KPI) while there are other KPIs fit to drive the evaluation process. Secondly, these benchmarks don't take project context into account, thereby restricts their predictive accuracy. And finally, these benchmarks are obtained from small subsets of data, making it hard to generalise. As a result, estimators cannot probe into tenders to judge the strengths and weaknesses of their bids. This advancement is critical for not only choosing more lucrative opportunities but also driving negotiations during the tendering process. This study aims to develop a benchmarking system for tender evaluation using Big Data of 1.2 terabytes, comprising 5.7 million cells. A holistic list of seventeen (17) KPIs is identified from the email data using Text Mining approaches. Besides, eight (8) key project attributes are chosen for ensuring contextaware benchmarking using Focused Group Interviews (FG1s). At the crux of this work lies the proposition of a deep ensemble learner based on the decomposition-integration methodology. In the decomposition stage, the model predicts several attribute-specific benchmarks for each KPI using our proposed contextaware algorithm. In the integration stage, deep neural network-based learners are trained to generate final project-sensitive KPI benchmark. The learner is deployed in the Spring tool to support the tender evaluation of power infrastructure projects. A tender of 60km underground cabling project is evaluated using the proposed learner. The system spontaneously identified KPIs in the tender that require further attention to achieve greater profitability performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113194	10.1016/j.eswa.2020.113194													
J								Reliable shortest path finding in stochastic time-dependent road network with spatial-temporal link correlations: A case study from Beijing	EXPERT SYSTEMS WITH APPLICATIONS										Stochastic and time-dependent networks; Travel time reliability; Spatial-temporal correlations; Reliable shortest path; Personalized path navigation	TRAVEL-TIME; SOLUTION ALGORITHM; RELIABILITY; ROUTE; COMPUTATION; SUM	In view of the time-dependent characteristic of travel times in road networks and the travel time reliability (UR) requirements by different travelers, it is complicated and time-consuming to determine the reliable shortest path (RSP) in large-scale road networks. To search the RSP in stochastic and time-dependent (STD) network with spatial-temporal correlated link travel times, an efficient path finding algorithm is presented. First, the fitting test results based on floating car data show that it is more appropriate to characterize the travel time distributions (TTDs) of links using lognormal distributions. In order to quantify spatial-temporal correlations between links, correlation coefficients of link travel times are calculated. Also, influences of spatial distance (counted by the number of links), temporal distance (counted by the number of time intervals) and road type on link correlations is analyzed. Afterwards, the dynamic moment-matching method (DMM) is used to calculate the approximate path 1TD when correlated link travel times are considered. Accounting for different travelers' risk tolerance, a dynamic-moment-matching-based A* algorithm (STCRSP DMA*) is proposed to provide personalized path navigation for individual travelers. Last, numerical case studies based on abundant floating car data as well as a subsistent road network in Beijing are conducted to demonstrate the applicability and the computational advantage of the devised algorithm in solving RSP searching problems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113192	10.1016/j.eswa.2020.113192													
J								New Chebyshev distance measures for Pythagorean fuzzy sets with applications to multiple criteria decision analysis using an extended ELECTRE approach	EXPERT SYSTEMS WITH APPLICATIONS										Chebyshev distance; Elimination and choice translating reality method (ELECTRE); Multiple criteria decision making; Pythagorean fuzzy set; Chebyshev metric-based preference function	AGGREGATION OPERATORS; MEMBERSHIP GRADES; TOPSIS	This paper aims to propose novel Chebyshev distance measures for Pythagorean membership grades and establish their based elimination and choice translating reality method (ELECTRE) for addressing multiple criteria decision-making problems under uncertainty of Pythagorean fuzziness. Pythagorean fuzzy (PF) sets have a significant effect on fuzzy modeling for intelligent informatics and decision support because the degrees of membership, non-membership, and indeterminacy, strength of commitment, and direction of commitment featured by PF information are extended for a wider coverage of information span. The theory of PF sets is a powerful tool in dealing with imprecise and ambiguous evaluations for realistic problems and modeling intelligent decision making for complex systems. This paper focuses on both theory and applications of the Chebyshev metric for PF contexts, and special attention is devoted to the theoretical development of Chebyshev distances in connection with Pythagorean membership grades based on various types of representations. To surmount the difficulties confronted by the existing measures, such as low comprehensivity, incomparability in scaling, ignorance of square degrees in metric specification, double weighting, and inappropriate normalization, this paper makes a comprehensive comparison to validate the effectiveness and superiority of the proposed Chebyshev distance measures. To support decision making within complicated PF environments, this paper develops an extended ELECTRE approach based on the Chebyshev distance measure to conduct multiple criteria decision analysis involving PF information for determining partial and complete rankings of candidate alternatives. In particular, this paper constructs novel Chebyshev metric-based preference functions depending on the individual characteristics of the criteria. The developed PF ELECTRE approach leads to using all the information that characterizes a PF set via the concepts of the scalar function and various Chebyshev metric-based comparison indices, such as (net) concordance indices, (net) discordance indices, and the overall precedence index. Practical applications with a comparative analysis in the field of bridge-superstructure construction are conducted to examine the usefulness and advantages of the proposed methodology in the real world. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113164	10.1016/j.eswa.2019.113164													
J								A fusion method based on Deep Learning and Case-Based Reasoning which improves the resulting medical image segmentations	EXPERT SYSTEMS WITH APPLICATIONS										Fusion; Conflict management; Segmentation; Cancer tumour; Deep learning; Case-based reasoning	NETWORK; MODEL	The fusion of multiple segmentations of different biological structures is inevitable in the case where each structure has been segmented individually for performance reasons. However, when aggregating these structures for a final segmentation, conflicting pixels may appear. These conflicts can be solved by artificial intelligence techniques. Our system, integrated into the SAIAD project, carries out the fusion of deformed kidneys and nephroblastoma segmentations using the combination of Deep Learning and Case-Based Reasoning. The performances of our method were evaluated on 9 patients affected by nephroblastoma, and compared with other Al and non-Al methods adapted from the literature. The results demonstrate its effectiveness in resolving the conflicting pixels and its ability to improve the resulting segmentations. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113200	10.1016/j.eswa.2020.113200													
J								Combining a recursive approach via non-negative matrix factorization and Gini index sparsity to improve reliable detection of wheezing sounds	EXPERT SYSTEMS WITH APPLICATIONS										Wheezing; Detection; Non-negative matrix factorization; Gini index; Sparsity; Clustering	LUNG SOUNDS; EXTRACTION TECHNIQUE; RESPIRATORY SOUNDS; CLASSIFICATION; RECOGNITION; SPECTROGRAM; ALGORITHM; HEART; PARTS	Auscultation constitutes a fast, non-invasive and low-cost tool widely used to diagnose respiratory diseases in most of the health centres. However, the acoustic training and expertise acquired by the physician is still crucial to provide a reliable diagnosis of the status of the lung. Each wrong diagnosis increases the risk to the health of patients and the costs associated with the treatment of the disease detected. A wheezing detection system can be useful to the physician to minimize the subjectivity of the interpretation of the breathing sounds, misdiagnoses due to stress and elucidating complex acoustic scenes (such as louder background noises). Highlight that the presence of wheeze sounds is one of the main indicators of respiratory disorders from airway obstructions. This work presents an expert and intelligent system to detect wheeze sounds based on a recursive algorithm that combines orthogonal non-negative matrix factorization (ONMF) and the sparsity descriptor Gini index. The recursive algorithm is composed of four stages. The first stage is based on ONMF modelling to factorize the spectral bases as dissimilar as possible. The second stage clusters the ONMF bases into two categories: wheezing and normal breath. The third stage proposes a novel stopping criterion that controls the loss of wheezing spectral content at the expense of removing normal breath content in the recursive algorithm. Finally, the fourth stage determines the patient's condition to locate the temporal intervals in which wheeze sounds are active for unhealthy patients. Experimental results report that the proposed method: (i) provides the best detection performance compared to the recent state-of-the-art wheezing detection approaches, achieving the highest robustness in noisy environments; and (ii) reliably distinguishes the patient's condition (healthy/unhealthy). The strengths of the proposed method are the following: (i) its unsupervised nature since it does not depend on any training stage to learn in advanced the sounds of interest (wheezing). This fact could make this method attractive to be used in clinical settings because wheezing sound databases are often unavailable; and (ii) the modelling of the spectral behaviour by means of a common feature, the sparsity, that represents the typically energy distributions shown by most of the wheeze and normal breath sounds. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113212	10.1016/j.eswa.2020.113212													
J								Gender recognition using motion data from multiple smart devices	EXPERT SYSTEMS WITH APPLICATIONS										Gender recognition; Motion sensor; Multiple smart devices; Performance evaluation; Walking behavior	CLASSIFICATION; TRAITS	Using multiple smart devices, such as smartphone and smartwatch simultaneously, is becoming a popular life style with the popularity of wearables. This multiple-sensor setting provides new opportunities for enhanced user trait analysis via multiple data fusion. In this study, we explore the task of gender recognition by using motion data collected from multiple smart devices. Specifically, motion data are collected from smartphone and smart band simultaneously. Motion features are extracted from the collected motion data according to three aspects: time, frequency, and wavelet domains. We present a feature selection method considering the redundancies between motion features. Gender recognition is performed using four supervised learning methods. Experimental results demonstrate that using motion data collected from multiple smart devices can significantly improve the accuracy of gender recognition. Evaluation of our method on a dataset of 56 subjects shows that it can reach an accuracy of 98.7% compared with the accuracies of 93.7% and 88.2% when using smartphone and smart band individually. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113195	10.1016/j.eswa.2020.113195													
J								Evolutionary multiobjective optimization to target social network influentials in viral marketing	EXPERT SYSTEMS WITH APPLICATIONS										Viral marketing; Influence maximization; Influentials targeting; Social networks; Evolutionary multiobjective optimization; Agent-based modeling	INFLUENCE MAXIMIZATION; ALGORITHM; DYNAMICS; SYSTEMS	Marketers have an important asset if they effectively target social networks' influentials. They can advertise products or services with free items or discounts to spread positive opinions to other consumers (i.e., word-of-mouth). However, main research on choosing the best influentials to target is single-objective and mainly focused on maximizing sales revenue. In this paper we propose a multiobjective approach to the influence maximization problem with the aim of increasing the revenue of viral marketing campaigns while reducing the costs. By using local social network metrics to locate influentials, we apply two evolutionary multiobjective optimization algorithms, NSGA-II and MOEA/D, a multiobjective adaptation of a single-objective genetic algorithm, and a greedy algorithm. Our proposal uses a realistic agent-based market framework to evaluate the fitness of the chromosomes by simulating the viral campaigns. The framework also generates, in a single run, a set of non-dominated solutions that allows marketers to consider multiple targeting options . The algorithms are evaluated on five network topologies and a real data-generated social network, showing that both MOEA/D and NSGA-II outperform the single-objective and the greedy approaches. More interestingly, we show a clear correlation between the algorithms' performance and the diffusion features of the social networks. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113183	10.1016/j.eswa.2020.113183													
J								Topological data analysis in investment decisions	EXPERT SYSTEMS WITH APPLICATIONS										Topological data analysis; Persistence landscape; Takens' embedding; Portfolio selection; Enhanced indexing	INDEX TRACKING; PERSISTENT HOMOLOGY; ENHANCED INDEX; TIME-SERIES; FRECHET MEANS; PERFORMANCE; STRATEGIES; GEOMETRY	This article explores the applications of Topological Data Analysis (TDA) in the finance field, especially addressing the primordial problem of asset allocation. Firstly, we build a rationale on why TDA can be a better alternative to traditional risk indicators such as standard deviation using real data sets. We apply Takens embedding theorem to reconstruct the time series of returns in a high dimensional space. We adopt the sliding window approach to draw the time-dependent point cloud data sets and associate a topological space with them. We then apply the persistent homology to discover the topological patterns that appear in the multidimensional time series. The temporal changes in the persistence landscapes, which are the real-valued functions that encode the persistence of topological patterns, are captured via L-p norm. The time series of the L-p norms shows that it is better at measuring the dynamics of returns than the standard deviation. Inspired by our findings, we explore an application of TDA in Enhanced Indexing (EI) that aims to build a portfolio of fewer assets than that in the index to outperform the latter. We propose a two-step procedure to accomplish this task. In step one, we utilize the L-p norms of the assets to propose a filtration technique of selecting a few assets from a larger pool of assets. In step two, we propose an optimization model to construct an optimal portfolio from the class of filtered assets for EI. To test the efficiency of this enhanced algorithm, experiments are carried out on ten data sets from financial markets across the globe. Our extensive empirical analysis exhibits that the proposed strategy delivers superior performance on several measures, including excess mean returns from the benchmark index and tail reward-risk ratios than some of the existing models of EI in the literature. The proposed filtering strategy is also noted to be beneficial for both risk-seeking and risk-averse investors. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113222	10.1016/j.eswa.2020.113222													
J								Effect of dimensionality reduction on stock selection with cluster analysis in different market situations	EXPERT SYSTEMS WITH APPLICATIONS										Stock selection; Dimensionality reduction; Market situation; Rotation strategy; Deep learning	TIME-SERIES; PORTFOLIO; STRATEGIES; INDEX	Dimensionality reduction is inevitable in stock selection with cluster analysis. Considering relations among dimensionality reduction, noise trading, and market situations, we empirically investigate the effect of dimensionality-reduction methods-principal component analysis, stacked autoencoder, and stacked restricted Boltzmann machine-on stock selection with cluster analysis in different market situations. Based on the index fluctuation, the market is divided into sideways and trend situations. For the CSI 100 and Nikkei 225 constituent stocks, experimental results show that: (1) In sideways situations, dimensionality reduction hardly improves the performance of stock selection with cluster analysis; (2) the advantage of dimensionality reduction is mainly reflected in trend situations, but whether it is in an up or down trend depends on the market analyzed. More importantly, according to the above findings and assuming that the dimensionality-reduction effect will continue, we propose a rotation strategy with and without dimensionality reduction. The results of experiments show that the proposed rotation strategy outperforms the stock market indices as well as the stock-selection strategies based on dimensionality reduction and cluster analysis. These findings offer practical insights into how dimensionality reduction can be efficiently used for stock selection. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113226	10.1016/j.eswa.2020.113226													
J								Modeling and solving cloud service purchasing in multi-cloud environments	EXPERT SYSTEMS WITH APPLICATIONS										Decision support systems; Cloud computing; Multi-cloud; Cloud brokerage; Adaptive large neighborhood search; Microservices	LARGE NEIGHBORHOOD SEARCH; RESOURCE-MANAGEMENT PROBLEM; OPTIMIZATION; ALLOCATION; PLACEMENT	Nowadays, the range of cloud services offered by cloud providers varies sharply and poses a challenge for cloud consumers aiming for the most cost-effective and compliant solutions, especially when operating highly scalable microservice architectures. A remedy can be provided by a cloud brokerage intelligent mechanism selecting cloud services across multiple clouds on behalf of consumers by considering individual goals and requirements. In this paper, we present the Cloud Service Purchasing Problem (CSPP) which aims to minimize costs while incorporating specific consumer and application task requirements. To solve this problem, we propose a mixed-integer programming model and two large neighborhood search approaches. Using well-defined problem instances incorporating data from real cloud providers, we conduct several computational experiments to evaluate the performance of the proposed algorithms. They exhibit a competitive performance providing solutions for all scenarios within short computational times. Finally, the significance of this problem and decision support approaches is analyzed by comparing usage patterns with respect to different nowadays cloud providers and offered virtual machine types for various scenarios. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113165	10.1016/j.eswa.2019.113165													
J								Data integrity assessment for maritime anomaly detection	EXPERT SYSTEMS WITH APPLICATIONS										Data falsification; Integrity assessment; AIS	IDENTIFICATION SYSTEM AIS; KNOWLEDGE DISCOVERY; TRUST; SHIPS	In the last years, systems broadcasting mobility data underwent a rise in cyberthreats, jeopardising their normal use and putting both users and their environment at risk. In this respect, anomaly detection methods are needed to ensure an assessment of such systems. In this article, we propose a rule-based method for data integrity assessment, with rules built from the system technical specifications and by domain experts, and formalised by a logic-based framework, resulting in the triggering of situation-specific alerts. A use case is proposed on the Automatic Identification System, a worldwide localisation system for vessels, based on its poor level of security which allows errors, falsifications and spoofing scenarios. The discovery of abnormal reporting cases aims to assist marine traffic surveillance, preserve the human life at sea and mitigate hazardous behaviours against ports, off-shore structures and the environment. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113219	10.1016/j.eswa.2020.113219													
J								A novel decision-making method using R-Norm concept and VIKOR approach under picture fuzzy environment	EXPERT SYSTEMS WITH APPLICATIONS										Picture fuzzy set; R-norm picture fuzzy entropy; Hamming distance; TOPSIS	AGGREGATION OPERATORS; ENTROPY; INFORMATION; SETS; WEIGHTS	The Picture Fuzzy Sets (PFS) are well suitable to capture inconsistent, imprecise and uncertain information in multiple-criteria decision-making problems. This communication is intended to introduce one such information measure defined on PFSs called R-norm picture fuzzy information measure. Moreover, a new set of axioms is proposed as a criteria for picture fuzzy entropy. Besides establishing the validity of proposed R-norm picture fuzzy information measure, some of its major properties are also discussed. In application part, the proposed information measure is applied in predicting the outcome of elections in a poll bound country through opinion polls and to solve an investment problem. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113228	10.1016/j.eswa.2020.113228													
J								Using a segmenting description approach in multiple criteria decision aiding	EXPERT SYSTEMS WITH APPLICATIONS										Multiple criteria decision aiding; Inconsistency analysis; Robustness analysis; Argumentation; Linear programming; Preference disaggregation	ADDITIVE VALUE-FUNCTIONS; ORDINAL REGRESSION; PREFERENCE DISAGGREGATION; SET; OPTIMIZATION	We propose a new method for analyzing a set of parameters in a multiple criteria ranking method. Unlike the existing techniques, we do not use any optimization method, instead incorporating and extending a Segmenting Description (SD) approach. The algorithm rewrites a set of initial inequalities into an equivalent SD system by means of three fundamental operations in a way that makes it possible to either easily analyze all feasible solutions or identify all sources of inconsistency. While considering a value-based preference disaggregation method, we demonstrate the usefulness of the introduced method in a multipurpose decision analysis exploiting a system of inequalities that models the Decision Maker's (DM's) preferences. We focus on the contexts where informative results can be obtained through the analysis of (in)feasibility of such systems. Specifically, we discuss how SD can be applied for verifying the consistency between the revealed and estimated preferences as well as for identifying the reasons of potential incoherence. Moreover, we employ the method for conducting robustness analysis, i.e., discovering a set of all compatible parameter values and verifying the stability of suggested recommendation in view of multiplicity of feasible solutions. In addition, we make clear its suitability for generating arguments about the validity of outcomes and the role of particular criteria. Overall, the SD approach confirms its usefulness in analyzing the relationships between the DM's preference information, the compatible parameters of an assumed model, and the value-driven ranking procedure. We also discuss the favourable characteristics of the proposed method enhancing its suitability for use in Multiple Criteria Decision Aiding. These include possibility of studying any system of inequalities without an objective function, keeping in memory an entire process of transforming a system of inequalities that makes identification of potential incoherence straightforward, and avoiding the need for processing the inequalities contained in the basic system which is subsequently enriched with some hypothesis to be verified. The latter is substantial for saving time when a multitude of hypotheses need to be checked in the same conditions. The applicability and the advantages of using the proposed method in the decision aiding context are clearly exemplified on a numerical study. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113186	10.1016/j.eswa.2020.113186													
J								Symbiotic Organisms Search Algorithm for multilevel thresholding of images	EXPERT SYSTEMS WITH APPLICATIONS										Multi-level thresholding; Segmentation; Otsu's method; Kapur 's entropy; Meta-heuristic algorithms; Symbiotic Organisms Search Algorithm	EFFICIENT METHOD; PSO ALGORITHM; SEGMENTATION; OPTIMIZATION; ENTROPY	Thresholding is a frequently used method in image processing because of its consistency and low computational cost. Otsu's and Kapur's methods are two important techniques that were proved to be best thresholding methods. However, they have high computational complexity when extended to multilevel thresholding because of their exhaustively search. Recently, meta-heuristic algorithms have been successfully applied for thresholding problems. In this study, six different meta-heuristic algorithms based on Otsu's and Kapur's functions; Particle Swarm Optimization (PSO), Firefly Algorithm (FA), Symbiotic Organisms Search (SOS), Artifical Bee Colony (ABC), Genetic Algorithm (GA) and grey Wolf Optimizer (GWO) were used for multilevel thresholding problem and compared. Experimental results suggest that SOS, PSO and FA algorithms often have higher fitness values than other algorithms. Especially when more than two threshold values are determined, SOS algorithm mostly gives higher fitness values. PSNR and SSIM results of the algorithms are similar. In terms of computational complexity, the GWO algorithm has the fastest convergence. For standard deviations of objective functions; more stable results were obtained with SOS based on Kapur's function, SOS and PSO based on Otsu's function. Also, SOS based on Kapur's function was found to be the most successful algorithm in the Friedman test. As a result, although the GWO approached faster, the SOS algorithm produced more consistent results for both objective functions. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113210	10.1016/j.eswa.2020.113210													
J								Multiclass imbalanced learning with one-versus-one decomposition and spectral clustering	EXPERT SYSTEMS WITH APPLICATIONS										Imbalanced learning; Multiclass classification; One-versus-one decomposition; Spectral clustering	DATA-SETS; CLASSIFICATION; SMOTE	In many real-world applications, an algorithm needs to learn multiclass classification models from data with imbalanced class distributions. Multiclass imbalanced learning is currently receiving increased attention from researchers. In contrast to traditional imbalanced learning on binary datasets, multiclass imbalanced learning faces great challenges from the variety of changes in the class distributions as well as the inadequate performance of multiclass classification algorithms. In this paper, we propose a novel data preprocessing-based method to solve this problem. The proposed method combines a one-versus-one (OVO) decomposition of class pairs and a spectral clustering technique. This method first decomposes a multiclass dataset into several binary-class datasets. Then, it uses spectral clustering to divide the minority classes of binary-class subsets into subspaces and oversamples them according to the characteristics of the data. Sampling based on spectral clustering takes into account the distribution of the data and effectively avoids oversampling outliers. After the data approximately reaches the equilibrium point, multiclass classifiers can be trained from these rebalanced data. We compared the proposed method with five state-of-the-art multiclass imbalanced learning methods on seven multiclass datasets, using multiclass area under the ROC curve (MAUC), the precision of minor classes (P-min) and the average precision of all classes (P-avg) as the performance metrics. The experimental results show that our proposed method has the best overall performance. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113152	10.1016/j.eswa.2019.113152													
J								SLDeep: Statement-level software defect prediction using deep-learning model on static code features	EXPERT SYSTEMS WITH APPLICATIONS										Defect; Software fault proneness; Machine learning; Fault prediction model; Software metric	METRICS; CROSS	Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113156	10.1016/j.eswa.2019.113156													
J								Applying genetic algorithms with speciation for optimization of grid template pattern detection in financial markets	EXPERT SYSTEMS WITH APPLICATIONS										Genetic algorithm; Speciation; Pattern discovery; Grid of weights; Optimization; Financial markets		This paper presents a new computational finance approach. It combines a grid pattern recognition technique allied to an evolutionary computation optimization kernel based on Genetic Algorithms, creating a dynamic way to attribute a score to the signal that takes volatility into consideration and normalizing the pattern detection by fixing the grid size with the ultimate goal of reduce risk and increase profits. For pattern matching, a template based approach using a fixed size grid of weights is adopted to describe the desired trading patterns, taking not only the closing price into consideration, but also the variation of price in each considered time interval of the time series. The scores assigned to the grid of weights will be optimized by the Genetic Algorithm and, at the same time, the genetic diversity of possible solutions will be preserved using a speciation technique, giving time for individuals to be optimized within their own niche. The adoption of this approach has the goal of reducing the investment risk and check if it outperforms similar approaches. This system was tested against state-of-the-art solutions, namely the existing adaptable grid of weights and a non speciated approach, considering real data from the stock market. The developed approach using the grid of weights had 21.3% of average return over the testing period against 10.9% of the existing approach and the use of speciation improved some of the training results as genetic diversity was taken into consideration. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113191	10.1016/j.eswa.2020.113191													
J								A hybrid algorithm based on particle filter and genetic algorithm for target tracking	EXPERT SYSTEMS WITH APPLICATIONS										Video objects tracking; Particle filter; Genetic algorithm; Resampling; Sample impoverishment		The particle filter (PF) is an influential instrument for visual tracking: it relies on the Monte Carlo Chain Framework and Bayesian probability that is of tremendous importance for smart monitoring systems. The current study introduces a particle filter based upon genetic resampling. In the suggested method called Reduced Particle Filter based upon Genetic Algorithm (RPFGA), particles with the highest weights are chosen and go through evolution using a GA in the resampling phase of PF algorithm. Moreover, this study aims to introduce the ideas of marking (marking the target by user (observer) in the first frame of a video sequence) and decreasing image size. Applying both ideas leads to reduced number of particles, the processing time of each frame, and the total tracking time. Additionally, the performance of the offered RPFGA method to tackle the occlusion problem is enhanced by the marking idea. According to the results obtained in challenges, such as Occlusions (OCC), deformation (DEF), low resolution (LR), scale variations(SV), Fast Motions (FM), In-Plane Rotation (IPR), Out-Of-Plane Rotation (OPR), Motion Blur (MB), Illumination Variation (IV) and color similarity between the target and the background, and regarding precision and tracking time, the recommended hybrid approach only with a few particles overtakes the generic particle filter, Particle Swarm Optimization particle filter (PSO-PF) and the particle filter based upon improved cuckoo search (ICS-PF). The suggested method can be applied for real time video objects tracking. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113188	10.1016/j.eswa.2020.113188													
J								Evolution of communities in dynamic social networks: An efficient map-based approach	EXPERT SYSTEMS WITH APPLICATIONS										Complex network; Social network analysis; Dynamic network; Community evolution; Community matching	OVERLAPPING COMMUNITIES; COMPLEX NETWORKS; PREDICTION; DISCOVERY; TRACKING	The expanded domain of expert system applications has risen the impact of modeling and analysis of community evolution in social networks as an important part of the decision-making process. Social networks are time-variant systems, evolving through entities joining or leaving networks and establishing or terminating relationships. In this article, we study evolution of social networks at the level of community structure, by tracking different transformations of communities over time. Upon experimentation, we observed that a considerable portion of community evolution is partial events such as partial merge. Therefore, we define a broader set of community evolution to include partial events. Furthermore, we introduce ICEM, a novel method for Identification of Community Evolution by Mapping. ICEM determines community evolution by tracking community members, implemented with a hash-map. ICEM maps each member to a (t, c) pair, specifying it is last observed in time window t and community c. We evaluated our proposed approach with seventeen publicly available social network datasets and compared its performance against other well-known methods in the literature. Our experimental results indicated the performance superiority of our proposed solution. Additionally, we conducted separate comprehensive experiments using three community detection algorithms to highlight the effect of choosing different community discovery methods on community evolution results. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113221	10.1016/j.eswa.2020.113221													
J								Z-ERM DEA integrated approach for evaluation of banks & financial institutes in stock exchange	EXPERT SYSTEMS WITH APPLICATIONS										Enhanced Russell Measure (ERM); Fuzzy DEA; Ranking Indicator (Index); Stock exchange; Z-numbers	DATA ENVELOPMENT ANALYSIS; ENHANCED RUSSELL MEASURE; TECHNICAL EFFICIENCY; DECISION-MAKING; FUZZY; MODEL; SYSTEMS	The purpose of this study is to evaluate efficiency and effectiveness of banks, financial and credit institutes active in stock and super-stock exchange organizations using new Fuzzy DEA model which is the modified, improved and enhanced version of Russell's measurement model. In order to make it more efficient, we used the Z-numbers theory through which the calculations of uncertainty in the decision-making process can be showed more accurately. To do so, the literature of this industry and financial data of the research companies during 2012-2017 were initially overviewed and 16 indicators (indices/indexes) and the main criteria related to their performance evaluation were selected. Afterward, the productivity of banks and financial and credit institutions in different levels of alpha - cut were specified and compared using integrated approach of ERM and calculations of Z-numbers as well as considering opinions provided by the financial experts with possibility of concurrent evaluation of the efficiency and effectiveness in a model. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113218	10.1016/j.eswa.2020.113218													
J								IOTA: Interlinking of heterogeneous multilingual open fiscal DaTA	EXPERT SYSTEMS WITH APPLICATIONS										Data interlinking; Budget and spending data; String similarity measure; Open data; Translated string matching framework; Cluster computing		Open budget data are among the most frequently published datasets of the open data ecosystem, intended to improve public administrations and government transparency. Unfortunately, the prospects of analysis across different open budget data remain limited due to schematic and linguistic differences. Budget and spending datasets are published together with descriptive classifications. Various public administrations typically publish the classifications and concepts in their regional languages. These classifications can be exploited to perform a more in-depth analysis, such as comparing similar items across different, cross-lingual datasets. However, in order to enable such analysis, a mapping across the multilingual classifications of datasets is required. In this paper, we present the framework for Interlinking of Heterogeneous Multilingual Open Fiscal DaTA (IOTA). IOTA makes use of machine translation followed by string similarities to map concepts across different datasets. To the best of our knowledge, IOTA is the first framework to offer scalable implementation of string similarity using distributed computing. The results demonstrate the applicability of the proposed multilingual matching, the scalability of the proposed framework, and an in-depth comparison of string similarity measures. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113135	10.1016/j.eswa.2019.113135													
J								A fuzzy logic system for the home assessment of freezing of gait in subjects with Parkinsons disease	EXPERT SYSTEMS WITH APPLICATIONS										Freezing of gait; Smartphone; Fuzzy logic; Home monitoring	FALLS; ACCELEROMETER; TASKS; RISK	Gait dysfunctions are pathognomonic, progressive and, generally, continuous in Parkinson's Disease (PD). The Freezing of Gait (FoG) is an episodic gait disorder involving up to 70% of people with PD, within 10 years of clinical onset, and associated with an increased risk for falls and immobility, which in turn, contributes to greater disability. Automatic and objective monitoring of FoG may help clinicians to understand and treat this phenomenon. In this work, a smartphone app for real-time FoG detection is presented and tested both in a laboratory setting and at patients' home. The app implements a novel fuzzy logic algorithm that uses important spatio-temporal parameters of gait and is built according to clinical knowledge about FoG. The app includes a gait detection function and the evaluation of two important clinical statistics, i.e. FoG time and FoG number. The app FoG detection performance was assessed against clinicians evaluation and compared with the Moore-Bachlin FoG detection algorithm through ROC analysis, the calculation of confusion matrix, and FoG hit rate. The proposed algorithm achieved better results with respect to the Moore-Bachlin algorithm. Home reports were compared with respect to the FoG Questionnaire and laboratory reports; results indicated significant correlations for both FoG time and FoG number. The results confirm the reliability and accuracy of this app for FoG detection, supporting its wide use for diagnostic and therapeutic purposes. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113197	10.1016/j.eswa.2020.113197													
J								Swash: A collective personal name matching framework	EXPERT SYSTEMS WITH APPLICATIONS										Personal name matching; Entity matching; Collective matching; Entity resolution; Heterogeneous information network; Unsupervised learning		Having a unique personal identifier is a prerequisite to run person-centric analytical queries and data mining tasks, such as fraud detection, expert finding, and credit scoring. Personal names are the most commonly used identifier of individuals in datasets; however, the name of a person may not be unique across the dataset's records, especially where data are integrated from various sources. Intelligent systems utilize name matching methods to identify different name representations of persons. The performance of previous name matching methods is inadequate since they solely consider name similarities and ignore dissimilarities. Unavailability of Part of Name (PON, e.g., first name and last name) is an important limitation of dissimilarity consideration. To address this issue, this paper proposes an unsupervised personal name matching framework, namely Swash. This framework can model the information gatherable from a name dataset into a layered Heterogeneous Information Network, which facilitates control over the learning process. Swash predicts PON tags using a self-trainable algorithm and then collectively clusters the name vertices on the network. Evaluations on three public bibliographic datasets (i.e., CiteSeer, ArXiv, and DBLP) recognize the significance of the proposed framework. The results showed that Swash outperformed Fl of the state-of-the-art method up to 38%. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113115	10.1016/j.eswa.2019.113115													
J								Ensemble learning by means of a multi-objective optimization design approach for dealing with imbalanced data sets	EXPERT SYSTEMS WITH APPLICATIONS										Ensemble learning; Multi-objective optimization; Imbalanced data sets	MODEL; CLASSIFIERS; GENERATION; PROMETHEE; FRAMEWORK; ALGORITHM; SYSTEMS	Ensemble learning methods have already shown to be powerful techniques for creating classifiers. However, when dealing with real-world engineering problems, class imbalance is usually found. In such scenario, canonical machine learning algorithms may not present desirable solutions, and techniques for overcoming this problem must be used. In addition to using learning algorithms that alleviate the imbalance between classes, multi-objective optimization design (MOOD) approaches can be used to improve the prediction performance of ensembles of classifiers. This paper proposes a study of different MOOD approaches for ensemble learning. First, a taxonomy on multi-objective ensemble learning (MOEL) is proposed. In it, four types of existing approaches are defined: multi-objective ensemble member generation, multi-objective ensemble member selection, multi-objective ensemble member combination, and multiobjective ensemble member selection and combination. Additionally, new approaches can be derived by combining the previous ones, such as multi-objective ensemble member generation and selection, multiobjective ensemble member generation and combination and multi-objective ensemble member generation, selection and combination. With the given taxonomy, two experiments are conducted for comparing (1) the performance of the MOEL techniques for generating and aggregating base models on several imbalanced benchmark problems and (2) the performance of MOEL techniques against other machine learning techniques in a real-world imbalanced drinking-water quality anomaly detection problem. Finally, results indicate that MOOD is able to improve the predictive performance of existing ensemble learning techniques. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113232	10.1016/j.eswa.2020.113232													
J								An empirical evaluation of the inferential capacity of defeasible argumentation, non-monotonic fuzzy reasoning and expert systems	EXPERT SYSTEMS WITH APPLICATIONS										Defeasible argumentation; Argumentation theory; Explainable artificial intelligence; Non-monotonic reasoning; Fuzzy logic; Expert systems; Mental workload	KNOWLEDGE REPRESENTATION; MENTAL WORKLOAD; NASA-TLX; LOGIC; FRAMEWORK; INHERITANCE; ALGORITHMS; MODELS	Several non-monotonic formalisms exist in the field of Artificial Intelligence for reasoning under uncertainty. Many of these are deductive and knowledge-driven, and also employ procedural and semi-declarative techniques for inferential purposes. Nonetheless, limited work exist for the comparison across distinct techniques and in particular the examination of their inferential capacity. Thus, this paper focuses on a comparison of three knowledge-driven approaches employed for non-monotonic reasoning, namely expert systems, fuzzy reasoning and defeasible argumentation. A knowledge-representation and reasoning problem has been selected: modelling and assessing mental workload. This is an ill-defined construct, and its formalisation can be seen as a reasoning activity under uncertainty. An experimental work was performed by exploiting three deductive knowledge bases produced with the aid of experts in the field. These were coded into models by employing the selected techniques and were subsequently elicited with data gathered from humans. The inferences produced by these models were in turn analysed according to common metrics of evaluation in the field of mental workload, in specific validity and sensitivity. Findings suggest that the variance of the inferences of expert systems and fuzzy reasoning models was higher, highlighting poor stability. Contrarily, that of argument-based models was lower, showing a superior stability of its inferences across knowledge bases and under different system configurations. The originality of this research lies in the quantification of the impact of defeasible argumentation. It contributes to the field of logic and non-monotonic reasoning by situating defeasible argumentation among similar approaches of non-monotonic reasoning under uncertainty through a novel empirical comparison. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113220	10.1016/j.eswa.2020.113220													
J								Learn#: A Novel incremental learning method for text classification	EXPERT SYSTEMS WITH APPLICATIONS										Learn#; Incremental learning; Reinforcement leaming	NAIVE BAYES	Deep learning is an effective method for extracting the underlying information in text. However, it performs better on closed datasets and is less effective in real-world scenarios for text classification. As the data is updated and the amount of data increases, the models need to be retrained, in what is often a long training process. Therefore, we propose a novel incremental learning strategy to solve these problems. Our method, called Learn#, includes four components: a Student model, a reinforcement learning (RL) module, a Teacher model, and a discriminator model. The Student models first extract the features from the texts, then the RL module filters the results of multiple Student models. After that, the Teacher model reclassifies the filtered results to obtain the final texts category. To avoid increasing the Student models unlimitedly as the number of samples increases, the discriminator model is used to filter the Student models based on their similarity. The Learn# method has the advantage of a shorter training time than the One-Time model, because it only needs to train a new Student model each time, without changing the existing Student models. Furthermore, it can also obtain feedback during application and tune the models parameters over time. Experiments on different datasets show that our method for text classification outperforms many traditional One-Time methods, reducing training time by nearly 80%. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113198	10.1016/j.eswa.2020.113198													
J								Dynamic portfolio optimization based on grey relational analysis approach	EXPERT SYSTEMS WITH APPLICATIONS										Dynamic portfolio selection; Grey relational coefficient; Stock performance	FINANCIAL PERFORMANCE; STOCK-EXCHANGE; EXPERT-SYSTEMS; MODEL; INVESTMENT; ALGORITHM; COMPANY	This research focuses on the possibilities of Grey Relational Analysis (GRA) as a tool in portfolio selection. The main goal of the paper was to empirically evaluate possibilities of dynamic portfolio (re)structuring with respect to the results from ranking stocks when using the GRA approach. The contributions of this research include: utilizing the GRA in accordance with finance and investor's utility theory; performing a battery of comparisons of investment strategies which are, again, based on finance theory. Results from the analysis indicate that there exist possibilities of exploiting the advantages of GRA methodology in order to form stock portfolios. This is shown by comparing the portfolio performance, which has been calculated based on several important measures. This performance indicates that based on the investor's preferences, certain gains can be achieved (both in terms of risk and return). Thus, the importance of this research is found in combining the GRA approach as a tool for achieving widely known investment goals more efficiently, in quicker time and even with the inclusion of transaction costs. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113207	10.1016/j.eswa.2020.113207													
J								Spatially weighted order binary pattern for color texture classification	EXPERT SYSTEMS WITH APPLICATIONS										Color; Features; Local Binary Pattern (LBP); Texture classification	ROTATION-INVARIANT; IMAGE RETRIEVAL; SCENE; REPRESENTATION; DESCRIPTOR; SPACE; RECOGNITION; HISTOGRAMS; GRADIENT; REGIONS	In this paper, we propose a novel descriptor called spatially weighted order binary pattern (SWOBP) for color texture classification. The SWOBP descriptor not only encodes color order information in different channels but also encodes color order relationships in the spatial domain. To achieve these goals, we introduce a color gradient channel to complement the traditional color channels and explore a multi-channel color order pattern to jointly encode inter-channel features. Furthermore, we decompose local color differences into spatially weighted binary templates and use them to encode color order information in a local neighborhood. Finally, we aggregate all the encoded features into image histograms as texture descriptor. Experiments on five benchmark databases demonstrate that the proposed SWOBP descriptor achieves the state-of-the-art performance for color texture classification. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113167	10.1016/j.eswa.2019.113167													
J								Multiple premises entailment recognition based on attention and gate mechanism	EXPERT SYSTEMS WITH APPLICATIONS										Natural language inference; Multiple premise entailment; Attention mechanism; Gate mechanism; Fine-tune		Multi-premise natural language inference provides important technical support for automatic question answering, machine reading comprehension and other application fields. Existing approaches for Multiple Premises Entailment (MPE) task are to convert MPE data into Single Premise Entailment (SPE) data format, then MPE is handled in the same way as SPE. This process ignores the unique characteristics of multi-premise, which will result in loss of semantics. This paper proposes a mechanism based on Attention and Gate Fusion Network (AGNet). AGNet adopts a "Local Matching-Integration" strategy to consider the characteristics of multi-premise. In this process, an attention mechanism combined with a matching gate mechanism can fully describe the relationship between the premise and hypothesis. A self-attention mechanism and a fusion gate mechanism can deeply exploit the relationship from the multi-premise. In order to avoid over-fitting problem, we propose a pre-training method for our model. In terms of computational complexity, AGNet has good parallelism, reduces the time complexity to O(1) in the process of matching. The experiments show that our model has achieved new state-of-the-art results on MPE test set. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113214	10.1016/j.eswa.2020.113214													
J								Data-level information enhancement: Motion-patch-based Siamese Convolutional Neural Networks for human activity recognition in videos	EXPERT SYSTEMS WITH APPLICATIONS										Human activity recognition; Data augmentation; Deep learning; 3D Convolutional Neural Networks	VISUAL-ATTENTION; ACCURACY; MODEL	Data augmentation is critical for deep learning-based human activity recognition (HAR) systems. However, conventional data augmentation methods, such as random-cropping, may generate bad samples that are unrelated to a particular activity (e.g. the background patches without saliency motion information). As a result, the random-cropping based data augmentation may affect negatively the overall performance of HAR systems. Humans, in turn, tend to pay more attention to motion information when recognizing activities. In this work, we attempt to enhance the motion information in HAR systems and mitigate the influence of bad samples through a Siamese architecture, termed as Motion-patch-based Siamese Convolutional Neural Network (MSCNN). The term motion patch is defined as a specific square region that includes critical motion information in the video. We propose a simple yet effective method for selecting those regions. To evaluate the proposed MSCNN, we conduct a number of experiments on the popular datasets UCF-101 and HMDB-51. The mathematical model and experimental results show that the proposed architecture is capable of enhancing the motion information and achieves comparable performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113203	10.1016/j.eswa.2020.113203													
J								Driver fatigue transition prediction in highly automated driving using physiological features	EXPERT SYSTEMS WITH APPLICATIONS										Driver fatigue; PERCLOS; Fatigue transition prediction; Highly automated driving	MONITORING-SYSTEM; DROWSINESS; TIME; NETWORK; VEHICLES; TRACKING; STATE; EYE	One of the main causes of traffic accidents is driver fatigue due to monotonous driving, sleep deprivation, boredom, or a combination of these. Thus, fatigue detection systems have been proposed to alert drivers. However, how early driver fatigue can be detected often determines the effectiveness of the system. Traditional approaches aim to detect driver fatigue in real time, which can be too late in many critical situations, such as the takeover transition period in highly automated driving. Therefore, in this research, we aim to predict the driver's transition from non-fatigue to fatigue in highly automated driving using physiological features. First, we capitalized on PERCLOS (i.e., PERcent of time the eyelids CLOSure) as the ground truth of driver fatigue. Next, we selected the most important physiological features to predict driver fatigue proactively. Finally, using these critical physiological features, we built prediction models that were able to predict the fatigue transition at least 13.8 s ahead of time using a technique called nonlinear autoregressive exogenous network. The accuracy of fatigue transition prediction was promising for highly automated driving (F-1 measure = 97.4% and 99.1% for two types of models), which demonstrated the potential of the proposed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN	2020	147								113204	10.1016/j.eswa.2020.113204													
J								A two-fold transformation model for human action recognition using decisive pose	COGNITIVE SYSTEMS RESEARCH										Gabor wavelet transform; Human action and activity recognition; Decisive pose estimation; Ridgelet transform	UNIFIED FRAMEWORK; IMAGE; REPRESENTATION; SILHOUETTE; TRACKING; FEATURES; SYSTEM	Human action recognition in videos is a tough task due to the complex background, geometrical transformation and an enormous volume of data. Hence, to address these issues, an effective algorithm is developed, which can identify human action in videos using a single decisive pose. To achieve the task, a decisive pose is extracted using optical flow, and further, feature extraction is done via a twofold transformation of wavelet. The two-fold transformation is done via Gabor Wavelet Transform (GWT) and Ridgelet Transform (RT). The GWT produces a feature vector by calculating first-order statistics values of different scale and orientations of an input pose, which have robustness against translation, scaling and rotation. The orientation-dependent shape characteristics of human action are computed using RT. The fusion of these features gives a robust unified algorithm. The effectiveness of the algorithm is measured on four publicly datasets i.e. KTH, Weizmann, Ballet Movement, and UT Interaction and accuracy reported on these datasets are 96.66%, 96%, 92.75% and 100%, respectively. The comparison of accuracies with similar state-of-the-arts shows superior performance. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					JUN	2020	61						1	13		10.1016/j.cogsys.2019.12.004													
J								Modeling influence of change readiness on knowledge acquisition process: A case study	COGNITIVE SYSTEMS RESEARCH										Knowledge Acquisition (KA); Fuzzy Cognitive Map (FCM); Change Readiness (CR); Knowledge management (KM); Scenario analysis	FUZZY COGNITIVE MAPS; ORGANIZATIONAL-CHANGE; INNOVATION PERFORMANCE; ABSORPTIVE-CAPACITY; IMPLEMENTATION; DETERMINANTS; INTENTIONS; PERCEPTION; TRENDS; IMPACT	Change readiness (CR) has important impact on the success of knowledge acquisition (KA). So it is necessary for managers to know how KA is shaped by CR elements. Review of the extant literature shows a paucity in this regard specially regarding intra-organizational rather than inter-organizational level. Therefore, to bridge this gap, this study aims to present a fuzzy cognitive map (FCM) model in which interactions among CR elements for KA are identified. To do so, first, elements for measuring CR for KA were extracted from the relevant literature. Then, the identified elements were screened through distributing the first designed questionnaire among the select sample of survey organization. Then, the second questionnaire was used to measure the select elements and find the relation between them, using FCM and Mental Modeler Software. To improve CR for KA, some scenarios are suggested by the managers and the impact of each scenario on the whole CR elements is identified using sensitive analysis of the FCM model. After comparing the scenarios, it is concluded improving more elements in one scenario does not necessarily result in better impact on the whole CR for KA. This is due to the interaction among the elements which sometimes could be negative. The proposed model might help managers of the survey organization evaluate their CR improvement plans for KA before taking any actions. In addition, this study can give an idea to other organizations and industries to apply this approach according to their own relevant criteria. This study is among the first in its kind which presents a model using FCM method in which the interactions among the CR elements influencing KA are considered. (C) 2020 Published by Elsevier B.V.																	1389-0417					JUN	2020	61						14	31		10.1016/j.cogsys.2020.01.001													
J								FNDNet - A deep convolutional neural network for fake news detection	COGNITIVE SYSTEMS RESEARCH										Fake news; Social media; Machine learning; Deep learning; Neural network	SENTIMENT ANALYSIS; CLASSIFICATION; KNOWLEDGE	With the increasing popularity of social media and web-based forums, the distribution of fake news has become a major threat to various sectors and agencies. This has abated trust in the media, leaving readers in a state of perplexity. There exists an enormous assemblage of research on the theme of Artificial Intelligence (AI) strategies for fake news detection. In the past, much of the focus has been given on classifying online reviews and freely accessible online social networking-based posts. In this work, we propose a deep convolutional neural network (FNDNet) for fake news detection. Instead of relying on hand-crafted features, our model (FNDNet) is designed to automatically learn the discriminatory features for fake news classification through multiple hidden layers built in the deep neural network. We create a deep Convolutional Neural Network (CNN) to extract several features at each layer. We compare the performance of the proposed approach with several baseline models. Benchmarked datasets were used to train and test the model, and the proposed model achieved state-of-the-art results with an accuracy of 98.36% on the test data. Various performance evaluation parameters such as Wilcoxon, false positive, true negative, precision, recall, Fl, and accuracy, etc. were used to validate the results. These results demonstrate significant improvements in the area of fake news detection as compared to existing state-of-the-art results and affirm the potential of our approach for classifying fake news on social media. This research will assist researchers in broadening the understanding of the applicability of CNN-based deep models for fake news detection. (C) 2020 Elsevier B.V. All rights reserved.																	1389-0417					JUN	2020	61						32	44		10.1016/j.cogsys.2019.12.005													
J								Higher emotions and cognition	COGNITIVE SYSTEMS RESEARCH										Higher emotions; Cognition; Higher meanings; Beautiful; Experimental confirmations	EVOLUTION; LANGUAGE; CONSCIOUSNESS	The paper presents models describing higher emotions and cognition. It argues that contents of models at the top of the mental hierarchy are higher meanings, which in a simplified way could be called meanings of life. These models are vague-fuzzy and inaccessible to consciousness. Improving these models and making steps toward even tentatively conscious forms of them relates to the highest aesthetic emotions, emotions of the beautiful. Experimental data confirming these relations between the beautiful and higher meanings are presented. (C) 2019 Published by Elsevier B.V.																	1389-0417					JUN	2020	61						45	52		10.1016/j.cogsys.2017.04.008													
J								Real-time detection of the interaction between an upper-limb power-assist robot user and another person for perception-assist	COGNITIVE SYSTEMS RESEARCH										Wearable robots; Perception-assist; Motion intention; Interaction recognition; Fuzzy reasoning	EXOSKELETON; RECOGNITION	Assisting aged population or a population with disabilities is a critical problem in today's world. To compensate for their declined motor function, power-assist wearable robots have been proposed. In some cases, however, the cognitive function of these populations may have declined as well, and it may be insufficient to compensate only for their motor function deficiency. Perception-assist wearable robots, which can perceive environmental information using visual sensors attached to them, have been proposed to address this problem. This study addresses the problem of identifying motion intentions of the user of an upper-limb power-assist wearable robot, while the user engages in desired interactions with others. It is important to consider both the interacting parties in order to accurately predict proper interaction. Therefore, this paper presents an interaction recognition methodology by combining both the user's motion intention and the other party's motion intention with environmental information. A fuzzy reasoning model is proposed to semantically combine the motion intentions of both parties and environmental information. In this method, the motion intentions of both the user and the other party are simultaneously estimated using kinematic information and visual information, respectively, and they are employed for predicting the interactions between both parties. The effectiveness of the proposed approach is experimentally evaluated. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				JUN	2020	61						53	63		10.1016/j.cogsys.2020.01.002													
J								IVFuseNet: Fusion of infrared and visible light images for depth prediction	INFORMATION FUSION										Depth prediction; Partially coupled filter; Adaptive weighted fusion; Visible image; Infrared image		Depth prediction is an essential component in the research of unmanned driving. Most existing research works predict depth only based on visible light images or infrared images. However, both visible light images and infrared images have their own advantages and disadvantages, and these two kinds of images contain complementary information when the images are filmed from the same scence. In order to fuse the complementary information and predict depth under various conditions, this paper proposes a convolutional-neural-network-based architecture, called infrared and visible light images fusion network (IVFuseNet), for depth prediction. Specifically, we construct common-feature-fusion subnetwork, full-feature-fusion subnetwork, and high-resolution reconstruction subnetwork, aiming to leverage the complementarity of these two kinds of images. The common-feature-fusion subnetwork adopts a two-stream multilayer convolutional structure whose filters for each layer are partially coupled to fuse the common features extracted from infrared images and visible light images respectively. The full-feature-fusion subnetwork fuses the two-stream features generated from the common-feature-fusion subnetwork by adaptive fusion weights instead of prefixed fusion weights. Additional, residual dense convolution that can accurately map the fused low-resolution features to the corresponding high-resolution features is adopted in the high-resolution reconstruction subnetwork to enhance the reconstruction of the details for depth prediction. All three subnetworks collaborate together to conduct the depth prediction task. Our NUST-SR dataset is composed of the actual road scenes captured while unmanned vehicle driving. The proposed IVFuseNet obtains the best performances on this dataset. IVFuseNet decreases the root mean squared error to 3.4513 and the mean relative error to 0.1651 respectively and outperforms other methods.																	1566-2535	1872-6305				JUN	2020	58						1	12		10.1016/j.inffus.2019.12.014													
J								Decentralised multi-platform search for a hazardous source in a turbulent flow	INFORMATION FUSION										Autonomous search; Decentralised multi-sensor fusion; Sequential Monte Carlo estimation; Sensor control; Infotaxi	AD-HOC WSNS; DISTRIBUTED ESTIMATION; AUTONOMOUS VEHICLES; SOURCE SEEKING; NOISY LINKS; CONSENSUS; INFOTAXIS; STRATEGY	The paper presents a cognitive strategy that enables an interconnected group of autonomous vehicles (moving robots) to search and localise a source of hazardous emissions (gas, biochemical particles) in a coordinated manner. Dispersion of the emitted substance is assumed to be affected by turbulence, resulting in the absence of concentration gradients. The key feature of the proposed search strategy is that it can be applied in a completely decentralised manner as long as the communication network of autonomous vehicles forms a connected graph. By decentralised operation we mean that each moving robot performs computations (i.e. source estimation and robot motion control) locally. Coordination is achieved by exchanging the data with the neighbours only, in a manner which does not require global knowledge of the communication network topology.																	1566-2535	1872-6305				JUN	2020	58						13	23		10.1016/j.inffus.2019.12.011													
J								Multi-level information fusion for learning a blood pressure predictive model using sensor data	INFORMATION FUSION										Sensor fusion; Data fusion; Information fusion; Performance metric fusion; Blood pressure prediction	PULSE TRANSIT-TIME; MULTITARGET REGRESSION; BIOHARNESS(TM); PROMETHEE; ECG; HYPERTENSION; RELIABILITY; ENSEMBLES; STATEMENT; VALIDITY	The availability of commercial wearable bio-sensors provides an opportunity for developing smart phone applications for real-time diagnosis that can be used to improve the health of the user. We propose a multi-level information fusion approach for learning a predictive model for blood pressure (BP) using electrocardiogram (ECG) sensor data. The approach fuses the information on five different levels: i) data collection, where data from multiple ECG sensors is collected; ii) feature extraction, where features are extracted from the collected data by different preprocessing methods; iii) information fusion, fusing the evaluation information from different classifiers; iv) information fusion using the information from multi-target regression models for each BP class; and v) information fusion using the information from multi-target regression models from all configurations as a single model. This is used for predicting the blood pressure values (systolic BP (SBP), diastolic BP (DBP), and mean arterial pressure (MAP)). Evaluating the methodology by using a separate test set indicates that the multilevel information fusion provides promising results, which are acceptable and comparable to the state-of-the-art results obtained for blood pressure prediction.																	1566-2535	1872-6305				JUN	2020	58						24	39		10.1016/j.inffus.2019.12.008													
J								Online detection of anomaly behaviors based on multidimensional trajectories	INFORMATION FUSION										Anomaly behavior; Multidimensional; Online detection; Trajectory		In the surveillance domain, timely detection of anomaly behaviors is very important and is a great challenge to human operators due to information overload, fatigue and inattention. Many anomaly detection algorithms based on trajectories have been proposed for this problem. However, these algorithms generally have problems such as complex parameter setting, unfaithful statistical model, not well-calibrated false alarm rate, poor ability of online learning and sequential anomaly detection, etc. The theory of conformal prediction was introduced to solve these problems by constructing the sequential Hausdorff nearest neighbor conformal anomaly detector. Yet, it only considers position information of the targets and is not sensitive to velocity and course anomaly behaviors. And the run times are increasing as the increase of the data size, which is not appropriate for early warning surveillance application. In order to solve these problems, sequential multi-factor Hausdorff nearest neighbor conformal anomaly detector (SMFHNN-CAD) and sequential multi-factor Hausdorff nearest neighbor inductive conformal anomaly detector (SMFHNN-ICAD) based on multidimensional trajectories are proposed in this paper. Experiments in both simulated military scenario and realistic civilian scenario show the presented algorithm has a good performance to online detect anomaly behaviors and would have a wide prospect in early warning surveillance systems.																	1566-2535	1872-6305				JUN	2020	58						40	51		10.1016/j.inffus.2019.12.009													
J								Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges	INFORMATION FUSION										Robotics; Reinforcement Learning; Deep Learning; Lifelong Learning; Continual Learning; Catastrophic Forgetting	OBJECT; SYSTEMS; MODELS	Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge. Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.																	1566-2535	1872-6305				JUN	2020	58						52	68		10.1016/j.inffus.2019.12.004													
J								Theoretical analysis of Tsallis entropy-based quality measure for weighted averaging image fusion	INFORMATION FUSION										Image fusion; Quality assessment; Mutual information; Tsallis entropy; Theoretical analysis	MUTUAL INFORMATION; PERFORMANCE	Entropy-based metrics are often employed for objective image fusion quality assessment due to single layer implementation of entropy and a small parameter set. In this paper, we present the theoretical analysis of image fusion quality measure based on Tsallis entropy. The purpose of this study is to theoretically assess if the considered quality measure is able to fulfill the desired behaviors that are expected from an ideal information-based image fusion quality metric. To assess the Tsallis quality measure, the paper employs an image formation model to obtain a closed-form expression for quality while weighted averaging is used as a fusion algorithm. The paper demonstrates that the Tsallis-based quality measure violates the desired behaviors regarding the response to variation of signal-to-noise ratio and effect of entropy order on the measured quality sign. Investigations on real images are also performed and the results agree with the theoretical analysis.																	1566-2535	1872-6305				JUN	2020	58						69	81		10.1016/j.inffus.2019.12.010													
J								Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI	INFORMATION FUSION										Explainable Artificial Intelligence; Machine Learning; Deep Learning; Data Fusion; Interpretability; Comprehensibility; Transparency; Privacy; Fairness; Accountability; Responsible Artificial Intelligence	GENERALIZED ADDITIVE-MODELS; TRAINED NEURAL-NETWORKS; SUPPORT VECTOR MACHINES; RULE EXTRACTION; LOGISTIC-REGRESSION; DATA FUSION; BLACK-BOX; DECISION TREES; BIG DATA; FEATURE-SELECTION	In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.																	1566-2535	1872-6305				JUN	2020	58						82	115		10.1016/j.inffus.2019.12.012													
J								A Dual-Branch Attention fusion deep network for multiresolution remote-Sensing image classification	INFORMATION FUSION										Deep learning; Feature fusion; Attention mechanism; Panchromatic (PAN) and multispectral (MS) images; Multiresolution image classification	PIXEL-LEVEL; REPRESENTATION; ENHANCEMENT	In recent years, with the diversification of acquisition methods of very high resolution panchromatic (PAN) and multispectral (MS) remote sensing images, multiresolution remote sensing classification has become a research hotspot. In this paper, from the perspective of data-driven deep learning, we design a dual-branch attention fusion deep network (DBAF-Net) for the multiresolution classification. It aims to integrate the feature-level fusion and classification into an end-to-end network model. In the process of establishing a training sample library, unlike the traditional pixel-centric sampling strategy with fixed patch size, we propose an adaptive center-offset sampling strategy (ACO-SS), which allows each patch to adaptively determine the neighborhood range by finding the texture structure of the pixel to be classified. And the neighborhood range is not symmetrical with this pixel, we expect to capture the neighborhood information that is more conducive to its classification. In network structure, based on the captured patches by ACO-SS, we design a spatial attention module (SA-module) for PAN data and a channel attention module (CA-module) for MS data, thus highlighting the spatial resolution advantages of PAN data and the multi-channel advantages of MS data, respectively. Then these two features are interfused to improve and strengthen the fusion features in both spatial and channel. The quantitative and qualitative experimental results verify the robustness and effectiveness of the proposed method.																	1566-2535	1872-6305				JUN	2020	58						116	131		10.1016/j.inffus.2019.12.013													
J								CCE: An ensemble architecture based on coupled ANN for solving multiclass problems	INFORMATION FUSION										Ensemble of classifiers; Multiclass-classification tasks; Artificial Neural Networks; Diversity	CLASSIFIER ENSEMBLES; DIVERSITY; ACCURACY; FUSION	The resolution of multiclass classification problems has been usually addressed by using a "divide and conquer" strategy that splits the original problem into several binary subproblems. This approach is mandatory when the learning algorithm has been designed to solve binary problems and a multiclass version cannot be devised. Artificial Neural Networks, ANN, are binary learning models whose extension to multiclass problems is rather straightforward by using the standard 1-out-of N codification of the classes. However, the use of a single ANN can be inefficient in terms of accuracy and computational complexity when the data set is large, or the number of classes is high. In this work, we exhaustively describe CCE, a new classifier ensemble based on ANN. Each member of this new ensemble is a couple of multiclass ANN's. Each ANN is trained using different subsets of the dataset ensuring these subsets to be disjoint. This new approach allows to combine the benefits of the divide and conquer methodology, with the use of multiclass ANNs and with the combination of individual classification modules that give a complete answer to the addressed problem. The combination of these elements results in a classifier ensemble in which the diversity of the base classifiers provides high accuracy values. Moreover, the use of couples of ANN proves to be tolerant to labeling noise and computationally efficient. The performance of CCE has been tested on various datasets and the results show the higher performance of this approach with respect to other used classification systems.																	1566-2535	1872-6305				JUN	2020	58						132	152		10.1016/j.inffus.2019.12.015													
J								Autosomal dominantly inherited alzheimer disease: Analysis of genetic subgroups by machine learning	INFORMATION FUSION										Dominantly-inherited Alzheimer's disease (DIAD); DIAN; Alzheimer's disease (AD); Neuroimaging; Machine learning	WHITE-MATTER HYPERINTENSITIES; SURFACE-BASED ANALYSIS; HUMAN CEREBRAL-CORTEX; HYPOTHETICAL MODEL; FDG-PET; MRI; SEGMENTATION; ONSET; IMPLEMENTATION; RELIABILITY	Despite subjects with Dominantly-Inherited Alzheimer's Disease (DIAD) represent less than 1% of all Alzheimer's Disease (AD) cases, the Dominantly Inherited Alzheimer Network (DIAN) initiative constitutes a strong impact in the understanding of AD disease course with special emphasis on the presyptomatic disease phase. Until now, the 3 genes involved in DIAD pathogenesis (PSEN1, PSEN2 and APP) have been commonly merged into one group (Mutation Carriers, MC) and studied using conventional statistical analysis. Comparisons between groups using null-hypothesis testing or longitudinal regression procedures, such as the linear-mixed-effects models, have been assessed in the extant literature. Within this context, the work presented here performs a comparison between different groups of subjects by considering the 3 genes, either jointly or separately, and using tools based on Machine Learning (ML). This involves a feature selection step which makes use of ANOVA followed by Principal Component Analysis (PCA) to determine which features would be realiable for further comparison purposes. Then, the selected predictors are classified using a Support-Vector-Machine (SVM) in a nested k-Fold cross-validation resulting in maximum classification rates of 72-74% using PiB PET features, specially when comparing asymptomatic Non-Carriers (NC) subjects with asymptomatic PSEN1 Mutation-Carriers (PSEN1-MC). Results obtained from these experiments led to the idea that PSEN1-MC might be considered as a mixture of two different subgroups including: a first group whose patterns were very close to NC subjects, and a second group much more different in terms of imaging patterns. Thus, using a k-Means clustering algorithm it was determined both subgroups and a new classification scenario was conducted to validate this process. The comparison between each subgroup vs. NC subjects resulted in classification rates around 80% underscoring the importance of considering DIAN as an heterogeneous entity.																	1566-2535	1872-6305				JUN	2020	58						153	167		10.1016/j.inffus.2020.01.001													
J								ATE-SPD: simultaneous extraction of aspect-term and aspect sentiment polarity using Bi-LSTM-CRF neural network	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Aspect-based sentiment analysis; recurrent neural network; long short-term memory; natural language processing		Aspect-based sentiment analysis is one of the challenging problems among the various type of tasks in sentiment analysis. Sequential models specifically deep neural networks (like Recurrent Neural Networks) have been found to handle this problem in an efficient way. This paper presents a deep neural network model named ATE-SPD for aspect-based sentiment analysis that simultaneously extracts aspect-terms and their corresponding polarities in review sentences. This problem can be solved as a sequence labelling problem. We are using Bi-LSTM hybridised with CRF as both of these approaches are state-of-the-art approaches for sequence labelling tasks. It was observed that CRF is able to improve the performance of the traditional Bi-LSTM model. Another important contribution of this paper is that it provides a novel set of sequential tags for extracting aspect-terms along with their sentiment polarities. Aspect-terms and their polarities are determined without explicitly labelling the sentiment terms. The ATE-SPD is evaluated using a benchmark dataset of SemEval'14-Task4 and obtains state-of-the-art performance.																	0952-813X	1362-3079															10.1080/0952813X.2020.1764632		MAY 2020											
J								A Novel Natural Language Processing (NLP)-Based Machine Translation Model for English to Pakistan Sign Language Translation	COGNITIVE COMPUTATION										Machine translation; Natural language processing; Deaf people communication; Pakistan Sign Language; Cognition; Rule-based translation	SPANISH; SPEECH; SYSTEM	Background/Introduction The deaf community in the world uses a gesture-based language, generally known as sign language. Every country has a different sign language; for instance, USA has American Sign Language (ASL) and UK has British Sign Language (BSL). The deaf community in Pakistan uses Pakistan Sign Language (PSL), which like other natural languages, has a vocabulary, sentence structure, and word order. Majority of the hearing community is not aware of PSL due to which there exists a huge communication gap between the two groups. Similarly, deaf persons are unable to read text written in English and Urdu. Hence, the provision of an effective translation model can support the cognitive capability of the deaf community to interpret natural language materials available on the Internet and in other useful resources. Methods This research involves exploiting natural language processing (NLP) techniques to support the deaf community by proposing a novel machine translation model that translates English sentences into equivalent Pakistan Sign Language (PSL). Though a large number of machine translation systems have been successfully implemented for natural to natural language translations, natural to sign language machine translation is a relatively new area of research. State-of-the-art works in natural to sign language translation are mostly domain specific and suffer from low accuracy scores. Major reasons are specialised language structures for sign languages, and lack of annotated corpora to facilitate development of more generalisable machine translation systems. To this end, a grammar-based machine translation model is proposed to translate sentences written in English language into equivalent PSL sentences. To the best of our knowledge, this is a first effort to translate any natural language to PSL using core NLP techniques. The proposed approach involves a structured process to investigate the linguistic structure of PSL and formulate the grammatical structure of PSL sentences. These rules are then formalised into a context-free grammar, which, in turn, can be efficiently implemented as a parsing module for translation and validation of target PSL sentences. The whole concept is implemented as a software system, comprising the NLP pipeline and an external service to render the avatar-based video of translated words, in order to compensate the cognitive hearing deficit of deaf people. Results and Conclusion The accuracy of the proposed translation model has been evaluated manually and automatically. Quantitative results reveal a very promising Bilingual Evaluation Understudy (BLEU) score of 0.78. Subjective evaluations demonstrate that the system can compensate for the cognitive hearing deficit of end users through the system output expressed as a readily interpretable avatar. Comparative analysis shows that our proposed system works well for simple sentences but struggles to translate compound and compound complex sentences correctly, which warrants future ongoing research.																	1866-9956	1866-9964				JUL	2020	12	4					748	765		10.1007/s12559-020-09731-7		MAY 2020											
J								Kuan noise filter with Hough transformation based reweighted linear program boost classification for plant leaf disease detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Plant leaf disease detection; Pre-processing; Kuan filtering technique; Feature extraction; Hough transform; Reweighted linear program boost classification	NEURAL-NETWORK; IDENTIFICATION; SEGMENTATION; RECOGNITION; SUPERPIXEL; SVM	In agriculture, plant leaf disease detection is significant concern to attain high crop yield and production. The product quality and productivity are affected, when proper care is not taken, it causes severe effects on plants. Several standard techniques have been developed for disease classification. The major issue facing these techniques is the automatic identification of plant diseases with minimal processing time. In order to improve the disease detection accuracy (DDA) with minimum time, Kuan filtered Hough transformation based reweighted linear program boost classification (KFHT-RLPBC) technique is introduced. KFHT-RLPBC technique includes three processes such as pre-processing, feature extraction and classification. A number of leaf images are gathered from plant dataset. In the pre-processing, the noises in the input leaf images are removed using Kuan filter to improve the image quality for achieving the higher disease detection accuracy. The Hough transform is utilized for extracting shape, texture and color features. KFHT-RLPBC technique reduces the time complexity (TC) in disease identification through the feature extraction process. Finally, the classification is done by applying the reweighted linear program to boost classification (RLPBC) to identify the disease at an earlier stage by constructing the number of weak learners. The boosting classifier combines the weak learner results and makes a strong one for achieving higher disease detection accuracy with minimum error. With plant village dataset, experimental evaluation is performed using certain parameters namely peak signal to noise ratio, disease detection accuracy and time complexity. Experimental results confirm that KFHT-RLPBC technique enhances disease detection accuracy and peak signal to noise ratio and reduces time complexity than the existing works.																	1868-5137	1868-5145															10.1007/s12652-020-02149-x		MAY 2020											
J								The multi-demeanor fusion based robust intrusion detection system for anomaly and misuse detection in computer networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IDS; Intrusion detection system; ST; Stock well transformation; Uncertain c capitals clustering with multi data fusion		Combating the cyber threats, particularly attack detection, is a challenging area of the intrusion detection system (IDS). Exceptional development and internet usage raise concerns about how digital data can be securely communicated and protected. This work has proposed a multi demeanor fusion-based intrusion detection system where stream data mining based on ST-SR (stochastic relaxation). Thus it includes uncertain c capitals clustering with multi data fusion is incorporated to classify the fused network traffic information effectively. Subsequently, classified data would be sent to the web usage mining based on a stochastic Latent Semantic and synthetic Analyzer which analyzes the traffic information. Even though being classified and analyzed the network traffic information itself can't get connected to the secured network due to its dynamic nature so to handle this situation, this work has incorporated IDS model (intrusion detection model based on parallel ensemble using bagging) which predicts the quality of service of each network during network traffic and enables the user to get connected with a secured network which holds high packet delivery ratio, less packet loss, and high throughput.																	1868-5137	1868-5145															10.1007/s12652-020-01974-4		MAY 2020											
J								Predicting autism spectrum disorder from associative genetic markers of phenotypic groups using machine learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Autism spectrum disorder; Biomarkers; Dimensionality reduction; Hilbert-schmidt independence criterion lasso; Machine learning; Regularized genetic algorithm	FEATURE-SELECTION; CLASSIFICATION; EXPRESSION; CHILDREN; MODEL	Machine learning is a discipline of artificial intelligence, geared towards the development of various critical applications. Due to its high precision, it is widely adopted in the process of extracting useful hidden patterns and valuable insights from complex data structures. Data extracted from the real-time environment might contain some irrelevant information. The presence of noise in the data degrades the model performance. Gene expression is an important source, carries the genetic information of species. Gene expression pattern reveals the significant relationship between genes associated with several diseases. But due to irregular molecular interactions and reactions occurs during the transcription process, the gene expressions are minimally affected. It causes a detrimental effect on the identification of biological markers of the diseases. To address this problem, a novel gene selection strategy is proposed to identify the candidate gene biomarkers from the genomic data. Signal to Noise ratio with logistic sigmoid function, Hilbert-Schmidt Independence Criterion Lasso, and regularized genetic algorithm amalgamation finds the optimal features. The proposed system is tested with the microarray gene expression dataset of autism spectrum disorder (ASD), accessed from gene expression omnibus repository. FAM104B, CCNDBP1, H1F0, ZER1 are identified as the candidate biomarkers of ASD. The methodical performance evaluation of the proposed model is examined with widely used machine learning algorithms. The proposed methodology enhanced the prediction rate of ASD and attained an accuracy of 97.62%, outperformed existing methods. Also, this system could act as a significant tool to assist the medical practitioners for accurate ASD diagnosis.																	1868-5137	1868-5145															10.1007/s12652-020-02155-z		MAY 2020											
J								Knowledge based fuzzy c-means method for rapid brain tissues segmentation of magnetic resonance imaging scans with CUDA enabled GPU machine	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Segmentation; FCM; Brain tissues; GPU CUDA; Parallel FCM	ALGORITHMS	Fuzzy C-Means (FCM) plays a major role in brain tissue segmentation. The proposed method aims to implements rapid brain tissue segmentation from MRI human head scans using FCM in CPU and GPU. This method is known as FCM-GENIUS. This paper presents three novel steps to enrich the performance of conventional FCM algorithm in CPU. There are region of interest (ROI) selection, knowledge based initialization and knowledge based optimization. The ROI selection is a preprocessing step contains brain extraction and bounding box processes. An automatic knowledge based initialization to FCM algorithm using histogram smoothing for centroids selection from middle slice of the given MRI brain volume. Optimization helps to improve the computation speed up of FCM algorithm using MRI slice adjacency property. The materials used for the proposed work are gathering from internet brain segmentation repository (IBSR). The accuracy of segmentation also compared with traditional and existing methods. The proposed method yield equal segmentation accuracy compared with existing methods but reduces the segmentation time considerably up to seven times and average number of iterations up to three times. In addition, parallel FCM implements in GPU machine and the performance was compared with the conventional FCM in CPU. The single instruction multiple data (SIMD) model was used with the hybrid CPU-GPU implementation in the GPU machine to accelerate the medical image segmentation.																	1868-5137	1868-5145															10.1007/s12652-020-02132-6		MAY 2020											
J								Anomaly detection with inexact labels	MACHINE LEARNING										Anomaly detection; Inexact labels; AUC maximization	MULTIPLE; FRAMEWORK; SUPPORT; AREA	We propose a supervised anomaly detection method for data with inexact anomaly labels, where each label, which is assigned to a set of instances, indicates that at least one instance in the set is anomalous. Although many anomaly detection methods have been proposed, they cannot handle inexact anomaly labels. To measure the performance with inexact anomaly labels, we define the inexact AUC, which is our extension of the area under the ROC curve (AUC) for inexact labels. The proposed method trains an anomaly score function so that the smooth approximation of the inexact AUC increases while anomaly scores for non-anomalous instances become low. We model the anomaly score function by a neural network-based unsupervised anomaly detection method, e.g., autoencoders. The proposed method performs well even when only a small number of inexact labels are available by incorporating an unsupervised anomaly detection mechanism with inexact AUC maximization. Using various datasets, we experimentally demonstrate that our proposed method improves the anomaly detection performance with inexact anomaly labels, and outperforms existing unsupervised and supervised anomaly detection and multiple instance learning methods.																	0885-6125	1573-0565				AUG	2020	109	8					1617	1633		10.1007/s10994-020-05880-w		MAY 2020											
J								Distributed-elite local search based on a genetic algorithm for bi-objective job-shop scheduling under time-of-use tariffs	EVOLUTIONARY INTELLIGENCE										Bi-objective job shop scheduling; Electricity cost; Genetic algorithm; Total weighted tardiness; Time-of-use tariffs	TOTAL WEIGHTED TARDINESS; TOTAL-ENERGY CONSUMPTION; SHIFTING BOTTLENECK; SINGLE-MACHINE; FLOW-SHOP; EVOLUTIONARY ALGORITHM; BOUND ALGORITHM; FORECAST ENGINE; DEMAND-SIDE; OPTIMIZATION	The rapid growth of electricity demand has led governments around the world to implement energy-conscious policies, such as time-of-use tariffs. The manufacturing sector can embrace these policies by implementing an innovative scheduling system to reduce its energy consumption. Therefore, this study addresses bi-objective job-shop scheduling with total weighted tardiness and electricity cost minimization under time-of-use tariffs. The problem can be decomposed into two sub-problems, operation sequencing and start time determination. To solve this problem, we propose a distributed-elite local search based on a genetic algorithm that uses local improvement strategies based on the distribution of elites. Specifically, chromosome encoding uses two lines of gene representation corresponding to the operation sequence and start time. We propose a decoding method to obtain a schedule that incorporates operation sequencing and start time. A perturbation scheme to reduce electricity costs was developed. Finally, a local search framework based on the distribution of elites is used to guide the selection of individuals and the determination of perturbation. Comprehensive numerical experiments using benchmark data from the literature demonstrate that the proposed method is more effective than NSGA-II, MOEA/D, and SPEA2. The results presented in this work may be useful for the manufacturing sector to adopt the time-of-use tariffs policy.																	1864-5909	1864-5917															10.1007/s12065-020-00426-4		MAY 2020											
J								Consensus of nonlinear multi-agent systems with fuzzy modelling uncertainties via state-constraint hybrid impulsive protocols	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-agent system; Fuzzy logic system; Impulsive protocol; Partial input saturation; Actuator saturation	DYNAMICAL NETWORKS; LEADER; AGENTS; SYNCHRONIZATION	In this paper, the nonlinear multi-agent system which contains uncertainty and is controlled by state-constraint impulsive protocol is taken into consideration. For the uncertainty of the multi-agent system, it is replaced by fuzzy logic system approximately and a judgement strategy which only contains the relative information with neighbors is proposed in this paper. In order to do research in state-constraint impulsive protocol, three kinds of impulsive control protocols which conclude partial input saturation, double actuator saturation and single actuator saturation are discussed. Then, some sufficient conditions of the system are obtained to reach consensus. Finally, some numerical simulation examples are provided to prove the effectiveness of the theoretical analysis.																	1868-8071	1868-808X				DEC	2020	11	12					2653	2664		10.1007/s13042-020-01140-4		MAY 2020											
J								Fractional Order Echo State Network for Time Series Prediction	NEURAL PROCESSING LETTERS										Fractional order; Echo state network; Time series prediction; Parameter optimization	OPTIMIZATION; SYSTEMS	In this brief, considering the infinite memory of fractional-order differential equation, a fractional-order echo state network (FESN) is given for time series prediction. For the FESN, the reservoir state differential equation is replaced with fractional-order differential equation. According to the advantages of FESN, the dynamic characteristics of a class of time series can be fully reflected. In order to improve the prediction performance of FESN, a fractional-order output weights learning method and a fractional-order parameter optimization method are given to train the output weights and optimize the reservoir parameters, respectively. Finally, two numerical examples are used to show the effectiveness of FESN.																	1370-4621	1573-773X				AUG	2020	52	1			SI		603	614		10.1007/s11063-020-10267-y		MAY 2020											
J								To improve user key security and cloud user region-based resource scheduler using rail fence region-based load balancing algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Rail fence region-based load balancing (RFRLB); VM; Cloud service provider (CSP)		Computer services, such as cloud computing and distributed computing, offer very low costs. These services can be calculated in the form of hardware, operating system or application software. However, the prospect of a cloud service provider (CSP), then they are not easy to find. Cloud-based companies are offering customer computing servers that have proven to be less expensive for energy. In this proposed work, a systematic method of balancing tasks with the consumption of loads and security is addressed. The energy consumption of the cloud data center is minimized through our proposed load balancing algorithm. And also introduced a security model to improve the user's trust model providing in the cloud. In public key with encryption, properties are based on cryptography. In this proposed, a trusted and scalable load balancing approach named Rail Fence Region-based Load balancing (RFRLB) it is adaptive, dynamic, scalable and efficient. When one server fails, the task transfer to another VM server. It takes rows, queue lengths and heavy workloads, as well as account server confidence values. The load balancing technique deployment model used in various computing environments. In this regard, the main focus is on improving bandwidth usage and improving cloud user security performance, rather than issues such as power efficiency. This proposed RFLP method performs well in comparison with parameters analysis and its protection method.																	1868-5137	1868-5145															10.1007/s12652-020-02152-2		MAY 2020											
J								The hard problem of AI rights	AI & SOCIETY										Artificial intelligence; Ethics; AI rights; Animals rights; The 'hard problem' of consciousness		In the past few years, the subject of AI rights-the thesis that AIs, robots, and other artefacts (hereafter, simply 'AIs') ought to be included in the sphere of moral concern-has started to receive serious attention from scholars. In this paper, I argue that the AI rights research program is beset by an epistemic problem that threatens to impede its progress-namely, a lack of a solution to the 'Hard Problem' of consciousness: the problem of explaining why certain brain states give rise to experience. To motivate this claim, I consider three ways in which to ground AI rights-namely: superintelligence, empathy, and a capacity for consciousness. I argue that appeals to superintelligence and empathy are problematic, and that consciousness should be our central focus, as in the case of animal rights. However, I also argue that AI rights is disanalogous from animal rights in an important respect: animal rights can proceed without a solution to the 'Hard Problem' of consciousness. Not so with AI rights, I argue. There we cannot make the same kinds of assumptions that we do about animal consciousness, since we still do not understand why brain states give rise to conscious mental states in humans.																	0951-5666	1435-5655															10.1007/s00146-020-00997-x		MAY 2020											
J								When is a phenomenologist being hermeneutical?	AI & SOCIETY										Phenomenology; Hermeneutics; Verstehen; Husserl; Dilthey; Heidegger; Ihde; Heelan; Philosophy of science; Philosophy of technology; Postphenomenology	SCIENCE	Many philosophers of science and technology who see themselves as coming "after" Husserl also claim that their phenomenology is hermeneutical. Yet they neither practice the same sort of phenomenology, nor do they all have the same understanding of hermeneutics. Moreover, their differences often seem to be more a function of different pre-selected substantive commitments-say, to take a "material" turn or to be resolutely "empirical"-than the product of any serious effort to clarify what it is be hermeneutical. In this essay, after some discussion of Dilthey's reception among post-Husserlians (especially Patrick Heelan and Don Ihde), I consider how aspiring hermeneuts might make their own pre-possession of substantive and methodological commitments a hermeneutical topic. This is, of course, is not just a scholarly question of how post-Husserlian phenomenologists might make themselves more phenomenological. Without thoughtful self-awareness of these commitments, one's assumptions about the use of technology, design, the place of science in the larger culture and in relation to conceptions of human flourishing-all of these assumptions are likely to pass through into technoscientific practice with insufficient critical consideration.																	0951-5666	1435-5655															10.1007/s00146-020-00990-4		MAY 2020											
J								Artificial intelligence and its natural limits	AI & SOCIETY										Artificial intelligence; Consciousness; Conceptual thought; Mind; Perceptual thought; Turing test		An argument with roots in ancient Greek philosophy claims that only humans are capable of a certain class of thought termed conceptual, as opposed to perceptual thought, which is common to humans, the higher animals, and some machines. We outline the most detailed modern version of this argument due to Mortimer Adler, who in the 1960s argued for the uniqueness of the human power of conceptual thought. He also admitted that if conceptual thought were ever manifested by machines, such an achievement would contradict his conclusion. We revisit Adler's criterion in the light of the past five decades of artificial-intelligence (AI) research, and refine it in view of the classical definitions of perceptual and conceptual thought. We then examine two well-publicized examples of creative works (prose and art) produced by AI systems and show that evidence for conceptual thought appears to be lacking in them. Although clearer evidence for conceptual thought on the part of AI systems may arise in the near future, especially if the global neuronal workspace theory of consciousness prevails over its rival, integrated information theory, the question of whether AI systems can engage in conceptual thought appears to be still open.																	0951-5666	1435-5655															10.1007/s00146-020-00995-z		MAY 2020											
J								A new framework for grayscale ear images recognition using generative adversarial networks under unconstrained conditions	EVOLVING SYSTEMS										Biometrics; Ear recognition; CNN; DCGAN; AWE dataset; AMI dataset		Getting to an ear recognition model that can overcome all challenges and difficulties was and still the main objective of researchers for years. One particular problem we highlight here, which is the loss of color information during the test phase, in other words, feeding grayscale, mono-color or dark test images to a model that is trained with colored images. In this paper, we propose a framework that involves conditional Deep Convolutional Generative Adversarial Networks (DCGAN), and Convolutional Neural Network (CNN) models. The proposed framework consists of a generative model that is responsible for colorizing grayscale and dark images, followed by a classification model. The performance of the proposed framework has been evaluated using the constrained AMI and the unconstrained AWE ear datasets. Performance metrics have been measured under three experimental scenarios, the obtained results highlighted the significant negative impact of the absence of color information and proved the vital role of our framework.																	1868-6478	1868-6486															10.1007/s12530-020-09346-1		MAY 2020											
J								Towards an automatic coding of observational studies: Coding neurofeedback therapies of children with autism	EXPERT SYSTEMS										application; automatic behaviour coding; coding behaviours; computer Science; computer vision; data-driven; human computer interaction; machine learning; observational studies	ALGORITHM; SOFTWARE	The coding of observational data is commonly used to analyse and evaluate human behaviours. The technique can help researchers inform the design and impact of, for example, an Ubicomp system by studying specific behaviours of interest. There are some tools that can alleviate the burden of observational coding, like those that help to collect and organise data, but can still be error-prone and time-consuming. Moreover, most of these tools lack automation, requiring intense human interaction. In order to mitigate these issues, computer vision (CV) and machine learning (ML) techniques could be used to automate observational coding, but little work has focused on analysing the feasibility of such an approach, with the goal of reducing the total coding time while maintaining accuracy. In this work, we address this question by proposing an automated approach for a real-world case study and compare it to manual coding. The study is composed of 10 videos with an average duration of 17 min each, where the goal is to determine the attention of children with autism that participate in a neurofeedback therapy session. Each video was hand-coded by three human observers to define the ground truth and to measure the manual coding time. Results show that it is feasible to automate the coding of observational behaviours and obtain a noticeable reduction in coding time, but with a slight loss in accuracy. Moreover, we illustrate that the best solution would be a hybrid approach, using a semi-automated system that combines human expertise and ML predictions Keywords Observational studies, Coding behaviours, Automatic coding, Computer-Vision, Machine Learning.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12572	10.1111/exsy.12572		MAY 2020											
J								Decision support system on credit operation using linear and logistic regression	EXPERT SYSTEMS										credit scoring; finance; linear regression; logistic regression; machine learning	BIG DATA; CLASSIFICATION ALGORITHMS; FINANCIAL RATIOS; SAMPLE SELECTION; ANALYTICS; MODELS; RISK	The act of lending is based on trust in the borrower to honour the obligation of paying back the lender. Greater spreads on credit operations may help predict the expected recovery of the credit, based on the sufficiency and liquidity of the guarantee. This study aims to understand how predictive models can provide different estimations of expected recovery based on the same data sets. It classifies credit by the formulation of a rule that describes the values of a categorical variable according to some specified definition. It finds that a simple logistic regression model can easily be extended to a multiple logistic regression model by integrating more than one prediction variable, which indicates increasing difficulty in obtaining multiple observations with an increasing number of independent variables. It compares the efficiency of the logistic regression with that of a linear regression in predicting whether recovery is due in a credit operation, and, thus, identifies the best model for this purpose.																	0266-4720	1468-0394														e12578	10.1111/exsy.12578		MAY 2020											
J								Robust Object Tracking via Information Theoretic Measures	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Object tracking; information theoretic measures; correntropy; template update; robust to complex noises	VISUAL TRACKING; MINIMIZATION; CORRENTROPY; SIGNAL	Object tracking is a very important topic in the field of computer vision. Many sophisticated appearance models have been proposed. Among them, the trackers based on holistic appearance information provide a compact notion of the tracked object and thus are robust to appearance variations under a small amount of noise. However, in practice, the tracked objects are often corrupted by complex noises (e.g., partial occlusions, illumination variations) so that the original appearance-based trackers become less effective. This paper presents a correntropy-based robust holistic tracking algorithm to deal with various noises. Then, a half-quadratic algorithm is carefully employed to minimize the correntropy-based objective function. Based on the proposed information theoretic algorithm, we design a simple and effective template update scheme for object tracking. Experimental results on publicly available videos demonstrate that the proposed tracker outperforms other popular tracking algorithms.																	1476-8186	1751-8520				OCT	2020	17	5					652	666		10.1007/s11633-020-1235-2		MAY 2020											
J								Ground-level Ozone Prediction Using Machine Learning Techniques: A Case Study in Amman, Jordan	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Ozone prediction; machine learning; neural networks; supervised learning; regression	ARTIFICIAL NEURAL-NETWORKS; AIR-QUALITY; MODELS; PARAMETER	Air pollution is one of the most serious hazards to humans' health nowadays, it is an invisible killer that takes many human lives every year. There are many pollutants existing in the atmosphere today, ozone being one of the most threatening pollutants. It can cause serious health damage such as wheezing, asthma, inflammation, and early mortality rates. Although air pollution could be forecasted using chemical and physical models, machine learning techniques showed promising results in this area, especially artificial neural networks. Despite its importance, there has not been any research on predicting ground-level ozone in Jordan. In this paper, we build a model for predicting ozone concentration for the next day in Amman, Jordan using a mixture of meteorological and seasonal variables of the previous day. We compare a multi-layer perceptron neural network (MLP), support vector regression (SVR), decision tree regression (DTR), and extreme gradient boosting (XGBoost) algorithms. We also explore the effect of applying various smoothing filters on the time-series data such as moving average, Holt-Winters smoothing and Savitzky-Golay filters. We find that MLP outperformed the other algorithms and that using Savitzky-Golay improved the results by 50% for coefficient of determination (R-2) and 80% for root mean square error (RMSE) and mean absolute error (MAE). Another point we focus on is the variables required to predict ozone concentration. In order to reduce the time required for prediction, we perform feature selection which greatly reduces the time by 91% as well as shrinking the number of features required for prediction to the previous day values of ozone, humidity, and temperature. The final model scored 98.653% for R-2, 1.016 ppb for RMSE and 0.800 ppb for MAE.																	1476-8186	1751-8520				OCT	2020	17	5					667	677		10.1007/s11633-020-1233-4		MAY 2020											
J								Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Conditional image synthesis; Multimodal image synthesis; Deep generative models; Implicit maximum likelihood estimation		Many tasks in computer vision and graphics fall within the framework of conditional image synthesis. In recent years, generative adversarial nets have delivered impressive advances in quality of synthesized images. However, it remains a challenge to generate both diverse and plausible images for the same input, due to the problem of mode collapse. In this paper, we develop a new generic multimodal conditional image synthesis method based on implicit maximum likelihood estimation and demonstrate improved multimodal image synthesis performance on two tasks, single image super-resolution and image synthesis from scene layouts. We make our implementation publicly available.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2607	2628		10.1007/s11263-020-01325-y		MAY 2020											
J								Building a computational model for mood classification of music by integrating an asymptotic approach with the machine learning techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computational data analysis; Mood classification; Statistical modeling of music mood; Theoretical analysis of music mood; Kokborok music	BIG DATA ANALYTICS; INFORMATION-RETRIEVAL; GENRE; USER	In this paper, we are working to understand the statistical behavior of acoustic features of audio, for one of the low resource languages, which is Kokborok from North East India. First, we have developed a classification system for Kokborok music by using the traditional machine learning technique. We used mainly Timbre, Rhythm, and Intensity feature to classify songs between four classes having three subclasses. This classification system gives poor performance compared to other Indian languages and western languages. So, we develop a computational method to minimize the errors for each class for the overall system. For such poor low resource language, the ground truth set creation is very tough. So, the behavior of the audio features of each song is analyses mathematically to understand whether the truth set is correct or not. Technically the feature values have to be different for each class and in a similar range for subclasses. We have defined a statistical parameter called "alpha" (alpha), for estimating the better value of the accuracy rate. This parameter alpha eventually estimates the final accuracy rate. This alpha is calculated, and the final value of the accuracy rate was calculated by extrapolating when the number of songs goes to infinity. The method enhances the actual accuracy rate from 49 to 63%, in the limit when the number of samples goes to infinity. Overall, our approach, when used in conjunction with the machine learning method, can predict a better accuracy rate for Kokborok music.																	1868-5137	1868-5145															10.1007/s12652-020-02145-1		MAY 2020											
J								Parallel deep convolutional neural network for content based medical image retrieval	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep convolutional neural network; Deep learning; Parallelization; Overlapping	GRAY-SCALE	DICOM images which helps in diagnosis and prognosis would be critical component in health care systems. Speedy recovery of past historic DICOM images based on the given query image is becoming a critical requirement for the Laboratories and Doctors for quick inference and accurate analogy of the patient conditions. In existing, It is also identified that there is a presence of imbalanced data set which degrade the retrieval accuracy of the model which may reduce by using extract the different kinds of features. The DCNN classifiers are trained by datasets whose data distributions of individual classes are not even or similar, they have always suffered from imbalanced classification performance against classes. Through DCNN can be used to minimize the gaps in terms of accuracy and retrieval but still efficiency parallelization would be essential for faster training and retrieval time. Time complexity is always been a major issue in DCNN, to overcome the above complexity the parallelization of model or data dimension need to be adapted. In this paper, parallel deep convolutional neural network (PDCNN) model is proposed by hyper parameter optimimzation for CBMIR system. The proposed model incorporating the low level content features, high level semantic features and compact features along with DCNN features to tackle the imbalanced dataset problem and reducing the DCNN training time for DICOM images. The high-level and compact features are extracted to resolve the imbalanced dataset problem by using the following algorithms: (a) local binary pattern (LBP), (b) histogram of oriented gradients (HOG) and (c) radon. The data parallelism was adopted in the proposed DCNN model to reduce the network training time by execution of DCNN layers across multiple CPU cores on a single PC. The implementation results for the proposed model in terms of Precision, Recall and F measure values are 87%, 87% and 92% respectively.																	1868-5137	1868-5145															10.1007/s12652-020-02077-w		MAY 2020											
J								Artificial intelligence neural network based on intelligent diagnosis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial intelligence; Diagnosis of diabetes; BP neural network; Probabilistic neural network; Intelligent diagnosis		The medical model that uses artificial intelligence technology to assist diagnosis and treatment is called smart medicine. It can learn the medical knowledge of experts, and can simulate the thinking and reasoning of doctors to give patients a reliable diagnosis and treatment plan. The purpose of this article is to help young doctors with uneven distribution of medical resources and insufficient experience by exploring the application of artificial intelligence neural networks in the intelligent diagnosis of diabetes. This paper proposes to use BP neural network and probabilistic neural network to model diabetes diagnosis. First, the number of hidden layer units of BP network is selected according to the input feature vector. The training effect is best when the number of hidden layer units is 12, and the diagnostic accuracy rate is 91.7% through experiments. Then built a PNN network model, using 75% of the data for training, 25% of the data for testing, and testing the effectiveness of the network model. The diagnostic accuracy was 97.9%. Finally, the accuracy of the 20 neural network models was tested 20 times. After a comparative analysis, it is concluded that the PNN network model is better than the BP neural network model in terms of performance and accuracy. Compared with the traditional diagnosis process, the neural network-based diagnosis model can effectively save doctors time and improve diagnosis efficiency.																	1868-5137	1868-5145															10.1007/s12652-020-02108-6		MAY 2020											
J								Divide and compress discrete cosine lossless compression coder to reduce dimensionality of test data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Divide and compress; Test data compression; Finite state machine; Discrete cosine transform; SoC	PATTERN RUN-LENGTH; TEST DATA VOLUME; POWER DISSIPATION; SCAN POWER; CORES	The increasing test data volume is considered as a biggest challenge in circuit under test. This challenge leads to higher computational complexity associated with the testing circuits. To reduce the testing time and addresses the problem related to high overhead, high fault coverage andincreased power dissipationmethod is used in system-on-chip (SoC). A new test compression method called divide and compress (D&C) lossless compression coder with Discrete Cosine Transformis used to enhance the compression capability. This framework of D&C lossless compression coder combines the lossless and lossy compression methods. The Discrete Cosine Transform (DCT) manipulates each data bit inside file and this reduces the file size without any data loss after decoding to lossless compression.Depending on the data characteristic, the compression of data is done without any loss in data. Further, the decompression is attained using advanced Finite State Machine (FSM) to reduce overhead. The experimental results on large benchmark circuits like ITC'99 and ISCAS'99 proves that the proposed D&C lossless compression attains reduced testing time and better compression ratio. Further, itlays only soft burden on the hardware.																	1868-5137	1868-5145															10.1007/s12652-020-02124-6		MAY 2020											
J								Semi-Supervised Monocular Depth Estimation Based on Semantic Supervision	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Monocular depth estimation; Semantic segmentation; Convolutional neural network		Monocular depth estimation by unsupervised learning is a potential strategy, which is mainly self-supervised by calculating view reconstruction loss from stereo pairs or monocular sequences. However, most existing works only consider the geometric information during training, without using semantics. We propose a semantic monocular depth estimation (SE-Net), a neural network framework that estimates depth using semantic information and video sequences. The whole framework is semi-supervised, because we take advantage of labelled semantic ground truth data. In view of the structural consistency between the semantically segmented image and the depth map, we first perform semantic segmentation on the image, and then use the semantic labels to guide the construction of the depth estimation network. Experiments on the KITTI dataset show that learning semantic information from images can effectively improve the effect of monocular depth estimation, and SE-Net is superior to the most advanced methods in depth estimation accuracy.																	0921-0296	1573-0409				NOV	2020	100	2					455	463		10.1007/s10846-020-01205-0		MAY 2020											
J								Assessment of deflection of pile implanted on slope by artificial neural network	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; 3D Plaxis; Horizontal displacement; Slope; Mechanical and geometrical parameters of the soil; Pile	EXPONENTIAL STABILITY; DELAYS	The influence of the proximity of slope on the laterally loaded piles has been the subject of several researches. The main purpose of this study is developing a neural model able to predict the deflection of laterally loaded piles placed near a slope. To achieve this goal, we divided this work into two parts. In the first part, we have carried out a numerical modeling of an experimental model (Bouafia and Bouguerra in Fr Geotech J 75(2):47-56, 1996) using the three-dimensional finite element method. The relative error between the results of experimental and numerical model varies from 0.055 to 0.422, which indicates a good agreement. In the second part, after having validated the numerical model, we have created a database relating the deflection of pile to their contributory parameters. Artificial neural networks used this database in order to predict the pile deflection. The results obtained are very satisfactory with very acceptable errors (R-2 = 0.9647, RMSE = 0.0133 and MAE = 0.0066).																	0941-0643	1433-3058															10.1007/s00521-020-04985-6		MAY 2020											
J								Uncertain four-dimensional multi-objective multi-item transportation models via GP technique	SOFT COMPUTING										Chance constraint; Expected value; Goal programming; Four-dimensional multi-objective multi-item transportation problem; Uncertain theory; Uncertain matrix		In this paper, a new type of four-dimensional multi-objective multi-item transportation problem is established using uncertain theory. We formulate and derive the expected value goal programming model and chance-constrained goal programming model based on the uncertain theory, where unit transportation cost, availabilities, capacities of conveyances, demands, unit transportation time, unit loading and unloading time are represented as uncertain matrices. Based on some properties of uncertain theory, the expected value goal programming model and chance-constrained goal programming model are transformed into the corresponding deterministic equivalents form via the soft computing technique, i.e., generalized reduced gradient technique named by LINGO-14.0. After that, a real-life numerical example is given to illustrate the performance of the models. Finally, the sensitivity analysis of the proposed model is presented through chance-constrained goal programming method with respect to different confidence levels.																	1432-7643	1433-7479				NOV	2020	24	22					17291	17307		10.1007/s00500-020-05019-y		MAY 2020											
J								Design of optimal low-pass filter by a new Levy swallow swarm algorithm	SOFT COMPUTING										FIR filter design; Evolutionary techniques; Levy swallow swarm algorithm	OPTIMIZATION; EVOLUTIONARY	The swallow swarm optimization (SS) is a challenging method of optimization, which has a quicker convergence speed, not getting caught in the local extreme points. However, the SS suffers from a few shortcomings-(1) the movement speed of particles is not controlled suitably during the search due to the requirement of an inertia weight and (2) the less flexibility of variables does not permit to maintain a balance between the local and the global searches. To solve these problems, a new Levy swallow swarm optimization (SSLY) algorithm with the exploitation capability is proposed. This article also provides an optimal design methodology for the low-pass filter using the suggested SSLY technique. A new objective function is introduced to achieve the maximally flat frequency response, which is another important contribution to the field. The firefly algorithm (FA), the sine cosine algorithm (SCA) and the standard global optimizers-real coded genetic algorithm (GA), conventional particle swarm optimization (PSO), cuckoo search (CS) and SS, are considered for a comparison. The proposed SSLY outperforms the FA, SCA, GA, PSO, CS and SS algorithms. Results authenticate suitability of the proposed algorithm for solving the filter design problems in the FIR domain.																	1432-7643	1433-7479															10.1007/s00500-020-05065-6		MAY 2020											
J								An ultra-fast time series distance measure to allow data mining in more complex real-world deployments	DATA MINING AND KNOWLEDGE DISCOVERY										Time series; Distance measure; Matrix profile	ANOMALY DETECTION; CLASSIFICATION; CLASSIFIERS	At their core, many time series data mining algorithms reduce to reasoning about the shapes of time series subsequences. This requires an effective distance measure, and for last two decades most algorithms use Euclidean distance or DTW as their core subroutine. We argue that these distance measures are not as robust as the community seems to believe. The undue faith in these measures perhaps derives from an overreliance on the benchmark datasets and self-selection bias. The community is simply reluctant to address more difficult domains, for which current distance measures are ill-suited. In this work, we introduce a novel distance measure MPdist. We show that our proposed distance measure is much more robust than current distance measures. For example, it can handle data with missing values or spurious regions. Furthermore, it allows us to successfully mine datasets that would defeat any Euclidean or DTW distance-based algorithm. Additionally, we show that our distance measure can be computed so efficiently as to allow analytics on very fast arriving streams.																	1384-5810	1573-756X				JUL	2020	34	4					1104	1135		10.1007/s10618-020-00695-8		MAY 2020											
J								Hybrid genetic algorithm based on bin packing strategy for the unrelated parallel workgroup scheduling problem	JOURNAL OF INTELLIGENT MANUFACTURING										Unrelated parallel machine problem; Workgroup scheduling; Two-dimensional bin packing problem; Heuristic strategy; Genetic algorithm	RESOURCE-ALLOCATION; JOBS; NUMBER; TIMES	In this paper we focus on an unrelated parallel workgroup scheduling problem where each workgroup is composed of a number of personnel with similar work skills which has eligibility and human resource constraints. The most difference from the general unrelated parallel machine scheduling with resource constraints is that one workgroup can process multiple jobs at a time as long as the resources are available, which means that a feasible scheduling scheme is impossible to get if we consider the processing sequence of jobs only in time dimension. We construct this problem as an integer programming model with the objective of minimizing makespan. As it is incapable to get the optimal solution in the acceptable time for the presented model by exact algorithm, meta-heuristic is considered to design. A pure genetic algorithm based on special coding design is proposed firstly. Then a hybrid genetic algorithm based on bin packing strategy is further developed by the consideration of transforming the single workgroup scheduling to a strip-packing problem. Finally, the proposed algorithms, together with exact approach, are tested at different size of instances. Results demonstrate that the proposed hybrid genetic algorithm shows the effective performance.																	0956-5515	1572-8145															10.1007/s10845-020-01597-8		MAY 2020											
J								Using CNN with Bayesian optimization to identify cerebral micro-bleeds	MACHINE VISION AND APPLICATIONS										Cerebral micro-bleeding; CNN; Convolution filters; ReLU; Softmax; Max pooling; Batch normalization; Bayesian optimization; Gaussian process regression; Acquisition function; Image augmentation	COMPUTER-AIDED DETECTION; NEURAL-NETWORK; MICROBLEEDS; MR	This article studies the problem of detecting cerebral micro-bleeds (CMBs) using a convolutional neural network (CNN). Cerebral micro-bleeds (CMBs) are increasingly recognized neuroimaging findings, occurring with cerebrovascular diseases, dementia, and normal aging. Naturally enough, it becomes necessary to detect CMBs in the early stages of life. The focus of this article is to infuse new techniques like Bayesian optimization to find the optimum set of hyper-parameters efficiently, making even the simplest of CNN architectures perform well on the problem. Experimentally, we observe our CNN (five layers, i.e., two convolution, two pooling, and one fully connected) achieves accuracy = 98.97%, sensitivity = 99.66%, specificity = 98.14%, and precision = 98.54% on the test set (hold-out validation) when calculated over an average of ten runs. The proposed model outperformed state-of-the-art methods.																	0932-8092	1432-1769				MAY 30	2020	31	5							36	10.1007/s00138-020-01087-0													
J								Iterated Admissibility Through Forcing in Strategic Belief Models	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Iterated admissibility; Possibility models; Forcing		Iterated admissibility embodies a minimal criterion of rationality in interactions. The epistemic characterization of this solution has been actively investigated in recent times: it has been shown that strategies surviving m+1 rounds of iterated admissibility may be identified as those that are obtained under a condition called rationality and m assumption of rationality in complete lexicographic type structures. On the other hand, it has been shown that its limit condition, with an infinity assumption of rationality (R infinity AR), might not be satisfied by any state in the epistemic structure, if the class of types is complete and the types are continuous. In this paper we analyze the problem in a different framework. We redefine the notion of type as well as the epistemic notion of assumption. These new definitions are sufficient for the characterization of iterated admissibility as the class of strategies that indeed satisfy R infinity AR. One of the key methodological innovations in our approach involves defining a new notion of generic types and employing these in conjunction with Cohen's technique of forcing.																	0925-8531	1572-9583				DEC	2020	29	4					491	509		10.1007/s10849-020-09317-4		MAY 2020											
J								Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative adversarial network; Video generation; Subsampling layer		Training of generative adversarial network (GAN) on a video dataset is a challenge because of the sheer size of the dataset and the complexity of each observation. In general, the computational cost of training GAN scales exponentially with the resolution. In this study, we present a novel memory efficient method of unsupervised learning of high-resolution video dataset whose computational cost scales only linearly with the resolution. We achieve this by designing the generator model as a stack of small sub-generators and training the model in a specific way. We train each sub-generator with its own specific discriminator. At the time of the training, we introduce between each pair of consecutive sub-generators an auxiliary subsampling layer that reduces the frame-rate by a certain ratio. This procedure can allow each sub-generator to learn the distribution of the video at different levels of resolution. We also need only a few GPUs to train a highly complex generator that far outperforms the predecessor in terms of inception scores.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2586	2606		10.1007/s11263-020-01333-y		MAY 2020											
J								An intelligent multiple vehicle detection and tracking using modified vibe algorithm and deep learning algorithm	SOFT COMPUTING										Multiple vehicle detection; EYOLO; IVIBE; CKFA; Occlusion	OBJECTS; SYSTEM	Multiple vehicle detection is a promising and challenging role in intelligent transportation systems and computer vision applications. Most existing methods detect vehicles with bounding box representation and fail to offer the location of vehicles. However, the location information is vigorous for several real-time applications such as the motion estimation and trajectory of vehicles moving on the road. In this paper, we propose an advanced deep learning method called enhanced you only look once v3 and improved visual background extractor algorithms are used to detect the multi-type and multiple vehicles in an input video. More precisely, tracking is to find the trace of the upcoming vehicles using a combined Kalman filtering algorithm and particle filter techniques. To improve the tracking results, further, we propose the technique, namely multiple vehicle tracking algorithms, and tested with different weather conditions such as sunny, rainy, night and fog in input videos of 30 frames per second. The major research issues were found in the recent kinds of literature in ITS sector which is closely related to the real-time traffic environmental problems such as occlusions, camera oscillations, background changes, sensors, cluttering, camouflage, varying illumination changes in a day- and sunny and at nighttime vision. The experimental results are tested with the ten different input videos and two benchmark datasets KITTI and DETRAC. The most eight high- level features have been considered for automatic feature extraction and annotation. The attributes are length, width, height, number of mirrors and wheels and windscreen shielding glass to detect the target region of interest (vehicles) on road. In addition, further experiments are carried out in multiple-input videos of high definition quality using a monocular camera, and the average accuracy is 98.6%, and the time complexity of the algorithm is O(n) and also tracking results attained 96.6%. The dataset and input videos are discussed in comparative results with the F-test measure done for multiple vehicles.																	1432-7643	1433-7479				NOV	2020	24	22					17417	17429		10.1007/s00500-020-05042-z		MAY 2020											
J								Unbounded-Time Safety Verification of Guarded LTI Models with Inputs by Abstract Acceleration	JOURNAL OF AUTOMATED REASONING										Safety analysis; Invariant generation; Reachability computation; LTI models; Dynamical models; Abstract acceleration; CEGAR	HYBRID SYSTEMS; ALGORITHM	Reachability analysis of dynamical models is a relevant problem that has seen much progress in the last decades, however with clear limitations pertaining to the nature of the dynamics and the soundness of the results. This article focuses on sound safety verification of unbounded-time (infinite-horizon) linear time-invariant (LTI) models with inputs using reachability analysis. We achieve this using counterexample-guided Acceleration: this approach over-approximates the reachability tube of the LTI model over an unbounded time horizon by using abstraction, possibly finding concrete counterexamples for refinement based on the given safety specification. The technique is applied to a number of LTI models and the results show robust performance when compared to state-of-the-art tools.																	0168-7433	1573-0670															10.1007/s10817-020-09562-z		MAY 2020											
J								Improved SOVA decoding for UEP wireless transmission of JPEG 2000 images over MIMO-OFDM systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										JPEG 2000; MIMO; UEP; Turbo decoding; SOVA; PSNR	DESIGN; CODES	In this paper, improved Joint Source Channel (JSC) decoding using turbo codes is employed that are unified into JPEG 2000 decoder architecture. Rather than using conventional decoding approach, the proposed system provides reduced BER and complexity. This work deliberates Soft Output Viterbi Algorithm (SOVA) for its real time implementation features. Nevertheless, the algorithm suffers from performance degradation due to optimistic and correlation effects. The objective of this work is to enhance the performance of SOVA by using appropriate reduction factors that integrates and eliminates both these distortions. Acquired Integrated Factor is incorporated in the turbo decoders of the image transmission system. Bit Error rate and Peak Signal to Noise Ratio (PSNR) analysis is made by considering Additive White Gaussian Noise and Rayleigh fading channel through MATLAB simulation. To illustrate the visual quality improvement of the proposed scheme, comparison with other FEC codes is done with significant PSNR gains.																	1868-5137	1868-5145															10.1007/s12652-020-02141-5		MAY 2020											
J								A beta-hill climbing optimizer for examination timetabling problem	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Timetabling; beta-Hill climbing; Operation Research; Optimization; Local Search	MULTIPLE HEURISTIC ORDERINGS; SEARCH APPROACH; ALGORITHM; SELECTION	Examination timetable is a non-trivial task for administrators of the academic institutions repeated every semester. In terms of optimization, examination timetabling is a combinatorial optimization problem concerned with assigning a set of exams to a predefined number of timeslots and rooms with accordance to a given constraints. In this paper, the extended version of hill climbing algorithm called beta-hill climbing is utilized to tackle the examination timetabling problem. beta-hill climbing is a new local search-based method that has two operators (beta-operator and N-operator) to iterate towards the optimal solution. The saturation degree heuristic method is utilized in the improvement loop of beta-hill climbing to ensure the solution feasibility. For experimental evaluation, Carter dataset is used comprising 12 instances selected from several real-world universities. Eight convergence scenarios are designed to sensitively analyze the behavior of the proposed algorithm. For comparative evaluations, the results produced by beta-hill climbing are comparatively comparable with previous methods that utilized the same Carter instances.																	1868-5137	1868-5145															10.1007/s12652-020-02047-2		MAY 2020											
J								Consensus and majority vote feature selection methods and a detection technique for web phishing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Malicious website; Cybercrimes; Phishing attack; Phishing detection; Machine learning; Feature selection; Cyber security		Phishing is one of the most frequently occurring forms of cybercrime that Internet users face and represents a violation of cybersecurity principles. Phishing is a fraudulent attack that is performed over the Internet with the purpose of obtaining and using without authorization the sensitive information of Internet users, such as usernames, passwords, credit card details, and bank account information. Some widely used phishing attempts involve using email spoofing or instant messaging, aiming to convince a victim to visit the spoofed websites, which will result in obtaining the victim's information. In this work, we identify and analyze the most important features needed to detect the spoofed websites in virtue of two new feature selection techniques. The first proposed feature selection technique uses underlying feature selection methods that vote on each feature, and if such methods agree on a specific feature, that feature is selected. The second feature selection technique also uses underlying feature selection methods that vote on each feature, and if the majority vote on a specific feature, the feature is selected. We also propose a phishing detection technique based on both AdaBoost and LightGBM ensemble methods to detect the spoofed websites. The proposed method achieves a very high accuracy compared to that of the existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02054-3		MAY 2020											
J								Essentiality for bridging the gap between low and semantic level features in image retrieval systems: an overview	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										CBIR; Image retrieval; Semantic gap; Relevance feedback	RELEVANCE FEEDBACK	The modern world uses digital data invariably and the advent of smart phones and cameras add to images and communicating through images in social media is a very common happening due to the exposure to internet and the economical availability of data. Moreover, people explore the web for much news and on different topics. They feel satisfied to look into image type info than textual information. So, image retrieval is paramount nowadays and it is very much useful in many societal and defence applications. The methods used for retrieval started through text based and query based is popularly used now. The content based retrieval uses many stages, approaches, technologies, algorithms etc. It is intended to provide a broad perspective about CBIR in this work. Moreover, the computational and time complexities involved during retrieval relies on the performance of the entire system. One of the aspects that can improve the precision is the reduction of semantic gap. Taking this as the base, we have explored a literature and presented the points related to various points including relevance feedback so as to enable other researchers to carry out experimental work on the suggested areas.																	1868-5137	1868-5145															10.1007/s12652-020-02139-z		MAY 2020											
J								Detection of distributed denial of service using deep learning neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Denial-of-service; Deep learning neural network; Cost minimization; Detection accuracy; False reduction		The need for developing a neural network classifier in an intrusion detection system for network security purpose is a necessary. Today, worldwide various types of sophisticated attacks damage the network in both wire and wireless. The medium of wireless network is air which is used to transmit the data where several categories of attacks damage the network system. Among the attacks, denial-of-service (DoS) attack easily access the network and also very difficult to prevent. Therefore, the protection of various resources in the network is a challenging one and the detection of DoS attack process is an important issue in the network. For this purpose, it requires high-performed machine learning classifier with less computational time, minimum false positive and high detection accuracy. This paper evaluates the network performance using deep learning neural network classifier with cost minimization strategy for a publicly available dataset. The proposed approach utilizes the KDD Cup, DARPA 1999, DARPA 2000, and CONFICKER datasets. The performance metrics such as detection accuracy, cost per sample, average delay, packet loss, overhead, packet delivery ratio and throughput are used for the performance analysis. From the simulation result observed that DNN Cost minimization algorithm provides better result in terms of high detection accuracy 99% with less false reduction, high average delay, less packet loss, less overhead, high in packet delivery ratio and throughput is high compared to existing algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02144-2		MAY 2020											
J								A hybrid encoding strategy for classification of medical imaging modalities	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bag of visual words; Fisher vector; Medical image modality classification; Scale invariant feature transform; Vector of locally aggregated descriptors	RETRIEVAL; BENEFITS; SYSTEMS	In this work, an automated system is designed to identify and classify the modality of medical images. We considered six modalities in this work: X-ray (XR), computed tomography (CT), magnetic resonance imaging (MR), positron emission tomography (PET), ultrasound (US) and photographs (PX). The methodology is based on encoding scale invariant feature transform (SIFT) features using Bag of Visual Words (BoVW), vector of locally aggregated descriptors (VLAD) and Fisher vector (FV). The encoded features are fed to support vector machine (SVM) classifier for training. The classification accuracy of all the classifiers based on three encoding strategies is compared and analyzed. The hybrid model is then implemented by selecting the best performance from each case. The major contribution of this research work is the application of VLAD for modality classification task which has not been tried so far. Combining the best performance of three encoding strategies, the overall classification accuracy obtained with the proposed system is 90.7%. For identification task, the scores from all the three encoding strategies are combined and the recognition rate obtained is 77.7%.																	1868-5137	1868-5145															10.1007/s12652-020-02129-1		MAY 2020											
J								Multilevel thresholding based image segmentation using new multistage hybrid optimization algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multilevel thresholding; Kapur's entropy; Image segmentation; Multistage hybrid algorithm; Nature inspired metaheuristics	ENTROPY	Thresholding is one of the highly accepted methods for image segmentation because of its simplicity in nature. The selection of optimal threshold values in threshold-based image segmentation is a tricky job. In this work, Kapur's entropy is used to solve the optimal threshold selection problem and a multistage hybrid nature-inspired optimization algorithm is used to get the best possible parameters for this objective function. The proposed method has three stages namely: primary stage, booster stage and final stage. Particle swarm optimization (PSO), artificial bee colony optimization (ABC) and ant colony optimization (ACO) used at these stages. In this proposed work various benchmarked images have been used for experimentation purpose. The proposed method has been assessed and performance is compared with well-known metaheuristic optimization like PSO, ABC, ACO, classical Otsu thresholding method and modified bacterial foraging optimization qualitatively and quantitatively. Peak signal to noise ratio and Structure Similarity Index are used for qualitative assessment. Wilcoxon p value test, ANOVA test and box plots are used for statistical analysis. The experimental results showed that the proposed method performed better in terms of quality and consistency.																	1868-5137	1868-5145															10.1007/s12652-020-02143-3		MAY 2020											
J								Perturbation-based classifier	SOFT COMPUTING										Pattern recognition; Perturbation; Normal distribution; Bayes theory	COVARIANCE-MATRIX ESTIMATION; ESTIMATOR; NETWORKS	The Bayes classifier depends on the conditional densities and the prior probabilities. Among many density functions, the Gaussian density has received more attention mainly motivated by its analytical tractability. The parameters of the Bayes classifier for the Gaussian distribution data are generally unknown, and approximations are calculated for the mean vector (mu) over cap and the covariance matrix (Sigma) over cap. When a pattern is inserted in the training set of the class omega(i), the values of the parameters (mu) over cap (i) and (Sigma) over cap (i) change by an amount given by Delta(mu) over cap (i) and Delta(Sigma) over cap (i), respectively. The insertion of one pattern can cause a perturbation, so we claim that this perturbation can be used for supervised classification purposes. Based on this assumption, we propose a supervised classifier called Perturbation-based Classifier PerC that assigns the class of the query pattern as the one that presents the smallest perturbation among all the classes after the insertion of this query pattern in the classes. The rationale is that the addition of a pattern that belongs to one specific class should not alter much the distribution of that class. PerC only uses the perturbations (Delta(mu) over cap (i) and Delta(Sigma) over cap (i)) to evaluate the class of a query pattern; so, it is a parameter-free classifier. The proposed method was assessed on 21 datasets from the UCI Machine Learning Repository, and its results were compared with classifiers from the literature. Results have shown that PerC obtains very competitive recognition rates.																	1432-7643	1433-7479				NOV	2020	24	21					16565	16576		10.1007/s00500-020-04960-2		MAY 2020											
J								Determination of Customer Satisfaction using ImprovedK-means algorithm	SOFT COMPUTING										Customer relationship management; K-means; Customer life cycle value; Data mining; Customer satisfaction	MEANS CLUSTERING-ALGORITHM; COLLABORATIVE K-MEANS; SEGMENTATION; INDUSTRY; INITIALIZATION; MANAGEMENT; MODEL	Effective management of customer's knowledge leads to efficient Customer Relationship Management (CRM). To accurately predict customer's behaviour, clustering, especiallyK-means, is one of the most important data mining techniques used in customer relationship management marketing, with which it is possible to identify customers' behavioural patterns and, subsequently, to align marketing strategies with customer preferences so as to maintain the customers. However, it has been observed in various studies onK-means clustering that customers with different behavioural indicators in clustering may seem to be the same, implying that customer behavioural indicators do not play any significant role in customer clustering. Therefore, if the level of customer participation depends on behavioural parameters such as their satisfaction, it can have a negative effect on theK-means clusters and has no acceptable result. In this paper, customer behavioural features-malicious feature-is considered in customer clustering, as well as a method for finding the optimal number of clusters and the initial values of cluster centres to obtain more accurate results. Finally, according to the organizations' need to extract knowledge from customers' views through ranking customers based on factors affecting customer value, a method is proposed for modelling their behaviour and extracting knowledge for customer relationship management. The results of the evaluation of the customers of Hamkaran System's Company show that the improvedK-means method proposed in this paper outperformsK-means in terms of speed and accuracy.																	1432-7643	1433-7479				NOV	2020	24	22					16947	16965		10.1007/s00500-020-04988-4		MAY 2020											
J								Neoteric ranked set sampling for robust (X)over-bar and R control charts	SOFT COMPUTING										Robust control chart; Neoteric ranked set sampling; Simulation; Relative efficiency		Neoteric ranked set sampling (NRSS) is defined as an efficient sampling design compared to counterparts in the literature. NRSS differs from ranked set sampling (RSS) by selecting ordered sample units, and this design provides more accurate results for estimation of population parameters compared to RSS. This sampling design is firstly used by Koyuncu and Karagoz (Qual Technol Quant Manag 15(5):602-621, 2018) to construct control charts under bivariate asymmetric distributions. Robust control charts are another important topic for monitoring of process when the contamination exists. The novelty of this paper is that we have used NRSS design firstly in statistical process control to monitor robust control charts. Moving this direction, we have proposed to use NRSS design in modified robust methods to construct (X) over bar and R charts under contaminated skewed distributions. The performances of the (X) over bar and R control charts for monitoring the process by using NRSS are evaluated according to Type I risk probabilities. Based on the simulation study, the NRSS design in modified robust methods gives the most efficient results compared to existing methods.																	1432-7643	1433-7479				NOV	2020	24	22					17195	17204		10.1007/s00500-020-05012-5		MAY 2020											
J								Solving differential equations with artificial bee colony programming	SOFT COMPUTING										Differential equations; Analytical solution; Automatic programming; Artificial bee colony programming; Swarm optimization	ALGORITHM; OPTIMIZATION	Relying on artificial bee colony programming (ABCP), we present in this paper, for the first time, a novel methodology for solving differential equations. The three-phase evolving process of ABCP is managed to apply on the issue of recovering the exact solution of differential equations through a well-posed problem. In fact, the original ABCP model which has been initially developed for symbolic regression cannot be used directly as differential problems might have multiple outputs. Moreover, the definition of fitness function is a critical problem-dependent issue for model design. In this sense, a problem-specific ABCP algorithm is worked out in the present contribution. With the proposed algorithm, solution with multiple outputs can evolve under a multiple-tree framework toward the exact solution. For fitness function evaluation, different forms are derived for ordinary and partial differential equations by performing experiments with multiple runs. Results on several differential equations are reported and compared to other advanced methods to assess the feasibility and the potential of the proposed method. A computational performance evaluation is provided for the considered examples and completed with an additional study on the impact of key control parameters.																	1432-7643	1433-7479															10.1007/s00500-020-05051-y		MAY 2020											
J								Solutions of linear uncertain fractional-order delay differential equations	SOFT COMPUTING										Uncertainty theory; Fractional-order differential equation; Delay; Analytic solution	FINITE-TIME STABILITY; SYSTEMS	Uncertain fractional-order delay differential equation is a class of fractional-order functional differential equations driven by Liu process. This paper devotes to studying linear uncertain fractional-order delay differential equation. The explicit representation and iterative formula of the solution to linear uncertain fractional-order delay differential equations are obtained. Meanwhile, the inverse uncertainty distribution of the solution to linear uncertain fractional-order delay differential equation by the alpha-path is presented.																	1432-7643	1433-7479															10.1007/s00500-020-05037-w		MAY 2020											
J								Effects of personality traits on user trust in human-machine collaborations	JOURNAL ON MULTIMODAL USER INTERFACES										Personality traits; Trust; Uncertainty; Cognitive load; Predictive decision making; Human-machine collaboration	AUTOMATION; METAANALYSIS; DESIGN; IMPACT; LOAD	Data analytics-driven solutions are widely used in various intelligent systems, where humans and machines make decisions collaboratively based on predictions. Human factors such as personality and trust have significant effects on such human-machine collaborations. This paper investigates effects of personality traits on user trust in human-machine collaborations under uncertainty and cognitive load conditions. A user study of 42 subjects in a repeated factorial design experiment found that uncertainty presentation led to increased trust but only under low cognitive load conditions when users had sufficient cognitive resources to process the information. Presentation of uncertainty under high load conditions led to a decrease in trust. When further drilling down into personality trait groups of users, overall, users with low Openness showed the highest trust. Furthermore, under the low cognitive load condition, it was found that the trust was enhanced under ambiguity uncertainty with low Agreeableness, low Neuroticism, high Extraversion, high Conscientiousness, and high Openness. Under the high cognitive load condition, high Neuroticism and low Extraversion benefitted the trust without the uncertainty presentation. The results demonstrated that different personality traits affected trust differently under uncertainty and cognitive load conditions. A framework of user trust feedback loop was set up to incorporate the study results into human-machine collaborations for the meaningful participatory design.																	1783-7677	1783-8738				DEC	2020	14	4			SI		387	400		10.1007/s12193-020-00329-9		MAY 2020											
J								Using Long Short-Term Memory for Building Outdoor Agricultural Machinery	FRONTIERS IN NEUROROBOTICS										artificial intelligence; robot; deep learning; intelligent agriculture; automation equipment; long short-term memory	PRECISION; IDENTIFICATION; NETWORKS; PEST	Today, climate change has caused a decrease in agricultural output or overall yields that are not as expected; however, with the ongoing population explosion, many undeveloped countries have transformed into emerging countries and have transformed farmland to be used in other types of applications. The resulting decline in agricultural output further increases the severity of the food crisis. In this context, this study proposes an outdoor agricultural robot that uses Long Short-Term Memory (LSTM). The key features of this innovation include: (1) the robot is portable, and it uses green power to reduce installation cost, (2) the system combines the current environment with weather forecasts through LSTM to predict the correct timing for watering, (3) detecting the environment and utilizing information from weather forecasts can help the system to ensure that growing conditions are suitable for the crops, and (4) the robot is mainly for outdoor applications because such farms lack sufficient electricity and water resources, which makes the robot critical for environmental control and resource allocation. The experimental results indicate that the robot developed in this study can detect the environment effectively to control electricity and water resources. Additionally, because the system is planned to increase agricultural output significantly, the study predicts the variables through multivariate LSTM, which controls the power supply from the solar power system.																	1662-5218					MAY 29	2020	14								27	10.3389/fnbot.2020.00027													
J								Dynamic image reconstruction and synthesis framework using deep learning algorithm	IET IMAGE PROCESSING										biomedical MRI; learning (artificial intelligence); neural nets; medical image processing; image classification; brain; image reconstruction; support vector machines; synthesis framework; image synthesis technique; high-level features; deep convolutional neural network system; image patches; medicinal imaging; deep learning algorithm image reconstruction; synthesis performance; synthesis system; reconstruction experiments; reconstruction rate; image reconstruction framework; cerebrum magnetic resonance image; LONI imaging dataset; ADNI imaging dataset	NETWORKS	This research study shows an effective deformable complex 3D image reconstruction and image synthesis technique by consolidating needed high-level features from a deep convolutional neural network (CNN) system. By recognising the inherent deep features in image patches lead to information discovery in medicinal imaging. Utilising the ADNI and LONI imaging datasets, the performance of the proposed deep learning algorithm image reconstruction and synthesis performance was verified. For validation, various performance indices obtained with the proposed deep learning algorithm were compared with two conventional algorithms namely support vector machine and CNN. Likewise, to reveal the adaptability of the proposed image reconstruction and synthesis system, synthesis and reconstruction experiments were directed on the 7 T cerebrum magnetic resonance image. As presented in the study outcomes, the proposed method can accomplish predominant performance compared with other cutting-edge techniques with either low- or high-level features in terms of the synthesis and reconstruction rate. The proposed algorithm has a training time of 5 s with a structural similarity index of 0.97. In all investigations, the outcome shows that the proposed image reconstruction framework reliably exhibited progressively precise outcomes when contrasted with best in class. Hence, it can be used for possible precise image reconstruction and synthesis related applications.																	1751-9659	1751-9667				MAY 29	2020	14	7					1219	1226		10.1049/iet-ipr.2019.0900													
J								Optimisation analysis of pulmonary nodule diagnostic test based on deep belief nets	IET IMAGE PROCESSING										medical image processing; belief networks; lung; learning (artificial intelligence); patient diagnosis; cancer; fault diagnosis; pulmonary nodule phenomenon; interference factors; actual detection process; detection accuracy; nodular shape; deep belief network-based diagnosis model; pulmonary nodules; detection effect; multilayer nonlinear structure; previous clinical data; model learning rate; training effect; diagnostic effect; optimisation analysis; pulmonary nodule diagnostic test; deep belief nets; missed diagnosis; lung cancer	LUNG-CANCER; IMAGE FEATURES; ACCURACY; BIOPSY	At present, the rate of missed diagnosis of lung cancer is high. The reason is that the pulmonary nodule phenomenon cannot be effectively monitored due to various interference factors in the actual detection process. In order to improve the detection accuracy, this study combined with the actual situation to analyse the diversity of nodular shape and constructed a deep belief network-based diagnosis model for pulmonary nodules. At the same time, in order to improve the detection effect, this study sets the model to have multi-layer non-linear structure and analyses the previous clinical data to improve the model learning rate and training effect. In addition, in order to verify the performance of the model, the diagnostic effect of the model is studied by comparative experiments. The research shows that the model proposed in this study is higher than the traditional algorithm in detection accuracy, which can provide theoretical reference for subsequent related research.																	1751-9659	1751-9667				MAY 29	2020	14	7					1227	1232		10.1049/iet-ipr.2019.1022													
J								Support vector machine combined with magnetic resonance imaging for accurate diagnosis of paediatric pancreatic cancer	IET IMAGE PROCESSING										medical image processing; learning (artificial intelligence); cancer; patient diagnosis; paediatrics; support vector machines; biomedical MRI; image processing technology; PC; computer-assisted method; MRI detection model; algorithm model; combines image processing; detection effect; MRI diagnosis; support vector machine; magnetic resonance imaging; performing paediatric pancreatic cancer diagnosis; poor diagnosis; diagnostic effect; machine learning; SVM	MRI; RISK; THERAPY; CT	When performing paediatric pancreatic cancer (PC) diagnosis, magnetic resonance imaging (MRI) will be interfered by many factors, resulting in poor diagnosis. In order to improve the diagnostic effect of MRI on PC, this study is based on machine learning and combines SVM (support vector machine) and MRI detection with the support of image processing technology and improves the diagnostic accuracy of MRI for PC by the computer-assisted method. At the same time, in the research, this study combines the characteristics of MRI detection to construct an MRI detection model based on SVM. In addition, this study obtains test samples through data collection, analyses the performance of the algorithm model through actual cases, and combines image processing to improve the detection effect. Studies have shown that the algorithm model proposed in this study can effectively improve the accuracy of MRI diagnosis of PC and provide a theoretical reference for subsequent related research.																	1751-9659	1751-9667				MAY 29	2020	14	7					1233	1239		10.1049/iet-ipr.2019.1041													
J								Deep3DSCan: Deep residual network and morphological descriptor based framework for lung cancer classification and 3D segmentation	IET IMAGE PROCESSING										cancer; computerised tomography; image classification; feature extraction; image segmentation; medical image processing; lung; learning (artificial intelligence); image matching; image fusion; deep residual network; morphological descriptor based framework; lung cancer classification; increasing incidence rate; lung cancer patients; early diagnosis; mortality rate; cancerous lesions; low contrast variation; visual similarity; benign nodules; malignant nodules; deep learning techniques; natural image segmentation; unseen situations; reasonable scale invariance; minute differences; domain-specific features; domain agnostic nature; computer aided design; ensemble framework Deep3DSCan; lung cancer segmentation; deep features; handcrafted descriptors; fine-tuned residual network; morphological techniques; fused features; publicly available LUNA16 dataset; template matching technique	COMPUTED-TOMOGRAPHY IMAGES; AUTOMATED DETECTION; HELICAL CT; NODULES; MORTALITY	With the increasing incidence rate of lung cancer patients, early diagnosis could help in reducing the mortality rate. However, accurate recognition of cancerous lesions is immensely challenging owing to factors such as low contrast variation, heterogeneity and visual similarity between benign and malignant nodules. Deep learning techniques have been very effective in performing natural image segmentation with robustness to previously unseen situations, reasonable scale invariance and the ability to detect even minute differences. However, they usually fail to learn domain-specific features due to the limited amount of available data and domain agnostic nature of these techniques. This work presents an ensemble framework Deep3DSCan for lung cancer segmentation and classification. The deep 3D segmentation network generates the 3D volume of interest from computed tomography scans of patients. The deep features and handcrafted descriptors are extracted using a fine-tuned residual network and morphological techniques, respectively. Finally, the fused features are used for cancer classification. The experiments were conducted on the publicly available LUNA16 dataset. For the segmentation, the authors achieved an accuracy of 0.927, significant improvement over the template matching technique, which had achieved an accuracy of 0.927. For the detection, previous state-of-the-art is 0.866, while ours is 0.883.																	1751-9659	1751-9667				MAY 29	2020	14	7					1240	1247		10.1049/iet-ipr.2019.1164													
J								Computational approach to body mass index estimation from dressed people in 3D space	IET IMAGE PROCESSING										anthropometry; image reconstruction; clothing; video signal processing; image colour analysis; medical computing; body mass index estimation; BMI estimation; three-dimensional visual data; estimated body volume; body weight; height estimation; normally dressed people; body volume estimation; reconstructed 3D data; estimated BMI; BMI computation method; RGB-D video dataset; KinectFusion	ANTHROPOMETRIC ESTIMATION; WEIGHT; DENSITY; SHAPE; AGE	Body mass index (BMI) defines as a person's weight divided by the square of height (BMI = (weight (lb)/height (in)(2)) x 703), which is an important indicator of the health condition. The authors study BMI estimation from the three-dimensional (3D) visual data by measuring the correlation between the estimated body volume and BMIs, and then develop an efficient BMI computation method. Their approach consists of body weight and height estimation from normally dressed people in 3D space. To address the influence of loose clothes on body volume estimation, two clothes models are developed to make the volume estimation more accurate. A new RGB-D video dataset is collected for this study, and the reconstructed 3D data are provided by the KinectFusion on depth data. Experimental results show the effectiveness of the approach to work on normal conditions of dressed people. The mean absolute error of the estimated BMI can achieve 2.54 in their experiments.																	1751-9659	1751-9667				MAY 29	2020	14	7					1248	1256		10.1049/iet-ipr.2019.1170													
J								Multi-dimensional data modelling of video image action recognition and motion capture in deep learning framework	IET IMAGE PROCESSING										feature extraction; computer vision; image classification; image motion analysis; learning (artificial intelligence); gesture recognition; video signal processing; image colour analysis; mixture models; image capture; convolutional neural nets; motion capture; deep-learning framework; Gauss mixture model; human body; dense trajectory feature; deep learning feature; deep video feature; video RGB tricolour feature; deep learning network model; small-scale gesture data sets; high recognition accuracy; large-scale data sets; small-scale gesture actions; Imperial Computer Vision &amp; Learning Lab human behaviour data set; multidimensional data modelling; video image action recognition; deep learning framework; small-range human motion recognition; gradient histogram; global encoding algorithm; convolutional neural network		In order to improve the accuracy of small-range human motion recognition in video and the computational efficiency of large-scale data sets, a multi-dimensional data model of motion recognition and motion capture in video image based on deep-learning framework was proposed. First, the moving foreground of the target is extracted by the Gauss mixture model, and the human body is recognised by the gradient histogram. At the second level, the dense trajectory feature and the deep learning feature are fused, according to the integration of global encoding algorithm and convolutional neural network. In the deep learning feature, the fusion of the deep video feature and the video RGB tricolour feature is taken as the feature of deep learning. Finally, the classification is based on the deep learning network model. The simulation experiments based on large-scale real data sets and small-scale gesture data sets show that the algorithm has high recognition accuracy for large-scale data sets and small-scale gesture actions. In addition, Imperial Computer Vision & Learning Lab human behaviour data set is used to classify the experimental data. The average classification accuracy is 85.79%. The algorithm can run at a speed of about 20 frames per second.																	1751-9659	1751-9667				MAY 29	2020	14	7					1257	1264		10.1049/iet-ipr.2019.0588													
J								Fabric defect detection using saliency of multi-scale local steering kernel	IET IMAGE PROCESSING										quality control; fabrics; image fusion; object detection; textile industry; image texture; feature extraction; singular value decomposition; image colour analysis; fabric defect detection; efficient FDD method; saliency analysis; multiscale local steering kernel; Commission International Eclairage L*a*b colour space; LSK features; desired defective maps; multiscale averaging fusion scheme; RGB fabric image; matrix cosine similarity	CLASSIFICATION	Fabric defect detection (FDD) plays an important role in the quality control in textile industry. In this study, the authors propose an efficient FDD method by using the saliency analysis of multi-scale local steering kernel (LSK). In the proposed method, a given RGB fabric image is first converted into the Commission International Eclairage (CIE) L*a*b colour space and then the LSK in each colour channel is computed by the singular value decomposition and the centre surrounding definition. Next, the matrix cosine similarity is employed to measure the similarity between different LSK features for generating the desired defective maps. Finally, a multi-scale averaging fusion scheme is applied to integrate the obtained defective maps at different scales for the final defective map. The experimental results indicate that the proposed method achieves the state-of-the-art performance on FDD compared to the other competitors.																	1751-9659	1751-9667				MAY 29	2020	14	7					1265	1272		10.1049/iet-ipr.2018.5857													
J								Automatic cloud segmentation from INSAT-3D satellite image via IKM and IFCM clustering	IET IMAGE PROCESSING										clouds; image classification; fuzzy set theory; geophysical image processing; image segmentation; remote sensing; atmospheric techniques; automatic cloud segmentation; IFCM clustering; satellite imagery; satellite images; image texture; FCM clustering algorithms; unsupervised pixel classification; low-level clouds; middle-level clouds; high-level clouds; noncloudy region; visible image; VIS image; cloudy; IKM segmentation algorithm; IFCM segmentation algorithm; low-level cloud region; middle-level cloud region; high-level cloudregion; satellite system-three-dimensional satellite image; k-means clustering algorithms; fuzzy c-means clustering algorithms; TIR image histogram	ALGORITHM; SPACE	Cloud extraction and classification from satellite imagery is important for many applications in remote sensing. Satellite images are segmented based on distance, intensity and texture of the images. The popular segmentation algorithms, k-means (KM) and fuzzy c-means (FCM) clustering algorithms, face some problems such as unknown number of groups, unknown initialization and dead centers. In this paper, an unsupervised pixel classification by the KM and FCM algorithms is improved and the selection of centroids is made automatic. The proposed improved k-means (IKM) and improved fuzzy c-means (IFCM) clustering algorithms segment the INSAT-3D satellite's thermal infrared image into low-level, middle-level, high-level clouds and non-cloudy region. As human beings can easily find the clouds in the satellite images, visible image is used to differentiate the clouds from the background. A threshold is found from the histogram of the visible image to separate the cloudy and non-cloudy pixels. The other three thresholds to divide the clouds into three types are found from the thermal infrared image's histogram. The segmentation results of IKM and IFCM algorithms are compared with the existing segmentation algorithms. The comparison shows that IFCM algorithm matches well with original image followed by IKM algorithm as compared with existing algorithms.																	1751-9659	1751-9667				MAY 29	2020	14	7					1273	1280		10.1049/iet-ipr.2018.5271													
J								Super-resolution mapping of hyperspectral satellite images using hybrid genetic algorithm	IET IMAGE PROCESSING										geophysical image processing; genetic algorithms; geophysical techniques; hyperspectral imaging; hydrological techniques; Hopfield neural nets; terrain mapping; image resolution; oceanographic techniques; reservoirs; remote sensing; hyperspectral satellite images; multipurpose reservoirs; hydrographic surveys; acoustic surveys; high-resolution images; water-spread area; EO-1 advanced land imager; Peechi Reservoir; hybrid genetic algorithm-based super-resolution mapping approach; hybrid GA-based super-resolution mapping approach; HNN-based super-resolution mapping approach; resolution hyperspectral image; Hopfield neural network	MARKOV-RANDOM-FIELD; NEURAL-NETWORK; PIXEL; RESERVOIR; SEDIMENTATION	To assess the rate of sedimentation and the consequent reduction in the storage capacity, periodical capacity surveys of multi-purpose reservoirs is essential. Hydrographic surveys and acoustic surveys are time-consuming and expensive. The limited availability and high cost of the high-resolution images require a different methodology to accurately estimate the water-spread area of the reservoir. In this study, 30 m resolution hyperspectral image (hyperion) and multi-spectral image (The Earth Observing One (EO-1) advanced land imager) are used to estimate the water-spread area of the Peechi Reservoir, South India. A hybrid genetic algorithm (GA)-based super-resolution mapping approach is developed and demonstrated, which incorporates the multi-objective GA and Hopfield neural network (HNN). The hybrid GA-based super-resolution mapping approach gives a global optimum solution in half of the original computation time. Furthermore, mapping approach gives an error of 6.38% for the multi-spectral image and a lesser error of 3.86% for the hyperspectral image, while the HNN-based super-resolution mapping approach gives an error of 8.23% for the multi-spectral image and 5.71% for the hyperspectral image. Thus, in this work, an efficient technique based on hybrid GA is presented, which is a useful tool for accurate mapping of water bodies at the sub-pixel scale using hyperspectral imagery.																	1751-9659	1751-9667				MAY 29	2020	14	7					1281	1290		10.1049/iet-ipr.2018.5108													
J								Adaptive frequency median filter for the salt and pepper denoising problem	IET IMAGE PROCESSING										adaptive filters; image denoising; median filters; adaptive frequency median filter; pepper denoising problem; AFMF; pepper noise; adaptive condition; adaptive median filter; AMF; original grey value	NOISE; REMOVAL; ALGORITHM; DENSITY	In this article, the authors propose an adaptive frequency median filter (AFMF) to remove the salt and pepper noise. AFMF uses the same adaptive condition of adaptive median filter (AMF). However, AFMF employs frequency median to restore grey values of the corrupted pixels instead of the median of AMF. The frequency median can exclude noisy pixels from evaluating a grey value of the centre pixel of the considered window, and it focuses on the uniqueness of grey values. Hence, the frequency median produces a grey value closer to the original grey value than the one by the median of AMF. Therefore, AFMF outperforms AMF. In experiments, the authors tested the proposed method on a variety of natural images of the MATLAB library, as well as the TESTIMAGES data set. Additionally, they also compared the denoising results of AFMF to the ones of other state-of-the-art denoising methods. The results showed that AFMF denoises more effectively than other methods.																	1751-9659	1751-9667				MAY 29	2020	14	7					1291	1302		10.1049/iet-ipr.2019.0398													
J								Texture and colour region separation based image retrieval using probability annular histogram and weighted similarity matching scheme	IET IMAGE PROCESSING										wavelet transforms; trees (mathematics); image texture; image colour analysis; feature extraction; content-based retrieval; probability; image retrieval; shape recognition; image matching; edge detection; iterative methods; colour region separation based image retrieval; content-based image retrieval system; CBIR; primitive image visual features; colour dominant part; texture feature; intensity dominant part; iterative algorithm; texture dominant part; probability-based semantic centred annular histogram; unique colour features; weighted distance-based feature comparison scheme; shape feature similarities; query image; database images; image retrieval experiments; texture image datasets; retrieval performances; unique shape feature extraction; weighted similarity matching scheme; scale-invariant feature transform; two-dimensional dual-tree complex wavelet transform; edge maps	HARMONIC FOURIER MOMENTS; INFORMATION; TRANSFORM	Content-based image retrieval (CBIR) uses primitive image features for retrieval of similar images from a dataset. Generally, researchers extract these visual features from the whole image. Therefore, the extracted features contain overlapped information of texture, colour, and shape features, and it is a critical challenge in the field of CBIR. This problem can be overcome by extracting the colour features from the colour as well as shape and texture features from the intensity dominant part only. In this study, the authors have proposed an iterative algorithm to separate colour and texture dominant part of the image into two different images. Here, a combination of edge maps and gradients has been used to achieve separate colour and texture images. Further, scale-invariant feature transform and 2D dual-tree complex wavelet transform has been realised to extract unique shape and texture features from the texture image. Simultaneously, a probability-based semantic centred annular histogram has been suggested to extract unique colour features from the colour image. Finally, a novel weighted distance-based feature comparison scheme has been proposed for similarity matching and retrieval. All the image retrieval experiments have been carried out on seven standard datasets and demonstrated significant improvements over other state-of-arts CBIR systems																	1751-9659	1751-9667				MAY 29	2020	14	7					1303	1315		10.1049/iet-ipr.2018.6619													
J								No-reference video quality assessment method based on spatio-temporal features using the ELM algorithm	IET IMAGE PROCESSING										data compression; feedforward neural nets; feature extraction; learning (artificial intelligence); video coding; Pearson correlation coefficient; Spearman correlation coefficient; full-reference metrics; quality monitoring; video transmission; reception system; reference video quality assessment method; spatio-temporal features; ELM algorithm; extreme learning machine algorithm; single-hidden layer feedforward neural network; no-reference video quality assessment; stop criteria; joint photographic experts group no-reference; LIVE video data base	EXTREME LEARNING-MACHINE	This work presents an application of the extreme learning machine (ELM) algorithm based on a single-hidden layer feedforward neural network for no-reference video quality assessment. The present research introduces an augmented version of ELM through simple stop criteria, which proved the effectiveness of the video quality assessment method. The authors present empirical studies using LIVE video data base show that the proposed method delivers accuracy (Pearson's correlation coefficient) and monotonicity (Spearman's correlation coefficient) with subjective scores against no-reference, Joint Photographic Experts Group No-Reference, metric and full-reference metrics, for instance, peak signal-to-noise ratio, structural similarity (SSIM) and multi-scale-SSIM indexes, and the proposed method is suitable for quality monitoring of video transmission and reception system.																	1751-9659	1751-9667				MAY 29	2020	14	7					1316	1326		10.1049/iet-ipr.2019.0941													
J								Construction of high dynamic range image based on gradient information transformation	IET IMAGE PROCESSING										image colour analysis; transforms; feature extraction; image fusion; gradient methods; local contrast; image luminance levels; gradient domain; visual information; spatial structure; source images; high dynamic range image; gradient information transformation; fusion method; exposure weights; multiscale Laplacian pyramid scheme; weight maps measurement; dense scale-invariant feature; just-noticeable-distortion technique	EXPOSURE FUSION; DECOMPOSITION; PHOTOGRAPHY; ENHANCEMENT; FEATURES; FLASH	This study proposes a fusion method for high dynamic range images based on gradient information transformation. In the proposed work, the authors first measure the three exposure weights of the source images, namely, local contrast, luminance and spatial structure. Then, the exposure weights are merged through a multi-scale Laplacian pyramid scheme. For the weight maps measurement, the dense scale-invariant feature transform method is used to calculate the local contrast around each pixel location, rather than a single pixel. The image luminance levels are computed in the gradient domain to get more visual information and the authors leverage the dictionary learning to effectively extract the luminance of images. Additionally, to better preserve the spatial structure of the source images, the just-noticeable-distortion technique is employed. By comparing the experimental results both subjectively and objectively, it is evident that the proposed method represents an improvement over some exciting methods.																	1751-9659	1751-9667				MAY 29	2020	14	7					1327	1338		10.1049/iet-ipr.2019.0118													
J								Multi-focus image fusion with Siamese self-attention network	IET IMAGE PROCESSING										image fusion; image sensors; visual perception; image representation; image enhancement; image resolution; learning (artificial intelligence); feature extraction; image classification; convolutional neural nets; fusion task; multifocus image fusion; Siamese self-attention network; convolutional neural networks; MFF; local receptive field limitations; convolutional operator; self-attention mechanism; Siamese SA network; focused defocused regions; convolution operators; input image; captured features; CNN-based methods	SEGMENTATION; PERFORMANCE	Recently, convolutional neural networks (CNNs) have achieved impressive progress in multi-focus image fusion (MFF). However, it always fails to capture sufficient discrimination features due to the local receptive field limitations of the convolutional operator, restricting most current CNN-based methods' performance. To address this issue, by leveraging self-attention (SA) mechanism, the authors propose Siamese SA network (SSAN) for MFF. Specifically, two kinds of SA modules, position SA (PSA) and channel SA (CSA) are utilised to model the long-range dependencies across focused and defocused regions in the multi-focus image, alleviating the local receptive field limitations of convolution operators in CNN. To search a better feature representation of the input image for MFF, the captured features obtained by PSA and CSA are further merged through a learnable 1 x 1 convolution operator. The whole pipeline is in a Siamese network fashion to reduce the complexity. After training, the authors SSAN can accomplish well the fusion task with no post-processing. Experiments demonstrate that their approach outperforms other current state-of-the-art methods, not only in subjective visual perception but also in the quantitative assessment.																	1751-9659	1751-9667				MAY 29	2020	14	7					1339	1346		10.1049/iet-ipr.2019.0883													
J								Automatic segmentation for pulmonary nodules in CT images based on multifractal analysis	IET IMAGE PROCESSING										image classification; computerised tomography; image segmentation; medical image processing; lung; multifractals analysis; two-stage false nodules; nodule candidates; 118 CT images; segment pulmonary nodules; multifractal analysis; solitary pulmonary nodules	CLASSIFICATION	To characterise pulmonary nodules, the volume analysis of solitary pulmonary nodules (SPNs) is important for diagnosis. To accurately estimate the volume of pulmonary nodules and reduce physician workloads, this study presents an intelligent method to automatically detect and segment pulmonary nodules from computed tomography (CT) scans. To detect nodule candidates, a modified 'Iso' capacity measure based on multifractals analysis is proposed to reflect the local singularity properties and capture the textures of pulmonary nodules. Subsequently, the proposed two-stage false nodules pruning procedure is applied to categorise a true SPN from nodule candidates. Preliminary results demonstrate that the proposed method can automatically and successfully detect and segment 118 SPNs in 118 CT images from a public lung database. The average and standard deviation of segmentation overlap measures are 0.71 and 0.08, respectively. The authors' method is competitive compared to the methods reported in the literature. Besides, the proposed method can avoid the problem of selecting seeds when applying region-growing techniques to segment pulmonary nodules.																	1751-9659	1751-9667				MAY 29	2020	14	7					1347	1353		10.1049/iet-ipr.2019.0884													
J								Chua's diode and strange attractor: a three-layer hardware-software co-design for medical image confidentiality	IET IMAGE PROCESSING										random number generation; cellular automata; chaos; security of data; cryptography; medical image processing; image processing; Chua's diode; strange attractor; three-layer hardware-software co-design; medical image confidentiality; medical image security; improved techniques; diffusion processes; pixel level manipulations; nonlinear circuits; hardware components; medical image encryption; Chua diode circuit; 256 x 256 random synthetic image; DICOM image; scrambling diffusion; average correlation coefficients; encrypted pixels	ENCRYPTION SCHEME; GENETIC ALGORITHM	Medical image security relies upon the arrival of improved techniques for enhancing the confusion as well as diffusion processes in the pixel level manipulations. Non-linear circuits constructed with hardware components can be employed for hardware-software co-design in medical image encryption. The proposed approach has adopted Chua diode circuit for the generation of 256 x 256 random synthetic image with the entropy of 7.9972. This synthetic image has been utilised to scramble the DICOM image for the second level of diffusion as the cellular automata provides the first level of scrambling and diffusion. The average correlation coefficients of encrypted pixels are 0.00204, -0.00298 and -0.00054 in three directions. The encrypted pixels pass the NIST SP 800-22 test suite and offer resistance to statistical, differential and chosen plain text attacks.																	1751-9659	1751-9667				MAY 29	2020	14	7					1354	1365		10.1049/iet-ipr.2019.0562													
J								Tablet identification using support vector machine based text recognition and error correction by enhanced n-grams algorithm	IET IMAGE PROCESSING										support vector machines; health care; drugs; text analysis; feature extraction; tablet identification; support vector machine based text recognition; error correction; grams algorithm; health care professionals; misidentified tablets; adverse drug reaction; ill health; unknown tablets; tablet images; unknown images; multiple feature sets; text region; support vector machine classifier; post processing algorithm; deletion errors; corrected text; template tablet database; similar tablets; common public		Unidentified and/or misidentified tablets always present challenges to both patients and health care professionals alike. Consumption of these misidentified tablets often results in adverse drug reaction and sometimes may even cause ill health leading to death. Thus, identification of unknown tablets is an important task in the medical industry. This study proposes an algorithm that uses the text imprinted on the tablet images to identify unknown images. Text imprinted on pills often contains important information that can be used to identify tablets. In this study, multiple feature sets were extracted from tablet images. The proposed work first identifies the text region, from which the text is recognised using support vector machine classifier. A post processing algorithm based on the confusion model combined with n-grams is used to correct the substitution, insertion and deletion errors in the text recognised. Finally, the corrected text is matched with a template tablet database to identify similar tablets. Experimental results showed that the proposed system is efficient with respect to accuracy and can be safely used by both common public and health care professionals to identify tablets.																	1751-9659	1751-9667				MAY 29	2020	14	7					1366	1372		10.1049/iet-ipr.2019.0993													
J								Novel deep learning model for facial expression recognition based on maximum boosted CNN and LSTM	IET IMAGE PROCESSING										image classification; image sequences; neural nets; learning (artificial intelligence); feature extraction; face recognition; LSTM layer; publically available FER databases; novel deep learning model; facial expression recognition; conventional FER methods; constrained datasets; real-time images; real-time image sequences; deep learning framework; convolutional neural network; short-term memory cell; real-time FER; different pre-processing techniques; subtle edge information; pre-processed images; individual CNN architecture; spatial features; spatial feature maps; individual CNN layers	FACE; NETWORKS; PATTERN	Deep learning has received increasing attention in all fields and has made considerable progress in facial expression recognition (FER). Mainly, the conventional FER methods are trained for constrained datasets, which may not operate well for real-time images. Such real-time image sequences limit the accuracy and efficacy of the traditional system. In this work, the authors present a novel deep learning framework which combines convolutional neural network (CNN) with long short-term memory (LSTM) cell for real-time FER. The novel framework has three main aspects: (i) two different pre-processing techniques are employed to handle illumination variances and to preserve subtle edge information of each image; (ii) correspondingly, the pre-processed images are inputted to the two individual CNN architecture which extracts the spatial features very effectively; (iii) spatial feature maps from two individual CNN layers are fused and integrated with an LSTM layer which extracts temporal relations between the successive frames. They experimented the authors' proposed method on three publically available FER databases and also with self-created database. With pre-processing, their proposed model achieves comparable and better results to the state-of-the-art.																	1751-9659	1751-9667				MAY 29	2020	14	7					1373	1381		10.1049/iet-ipr.2019.1188													
J								Improved water quality mapping based on cross-fusion of Sentinel-2 and Landsat-8 imageries	IET IMAGE PROCESSING										water quality; wavelet transforms; remote sensing; mean square error methods; image fusion; image colour analysis; geophysical techniques; geophysical image processing; regression analysis; geophysical signal processing; improved water quality mapping; Sentinel-2; Landsat-8 imageries; Landsat-8 cross-fusion; original images; traditional fusion methods including intensity-hue-saturation; Gram-Schmidt transform; cross-fusion methods; water quality parameter; higher CR values; WQP maps; root-mean-square error values; dissolved oxygen; chemical oxygen demand; biological oxygen demand; electrical conductivity maps; input images; final maps; mapping PH; WQM accuracy; output maps; traditional maps	CLASSIFICATION; PARAMETERS; RADAR	This study proposed methods based on Sentinel-2 and Landsat-8 cross-fusion for improving water quality mapping (WQM). Therefore, four traditional fusion methods including intensity-hue-saturation, Gram-Schmidt transform, wavelet transform and Brovey transform and different scenarios of cross-fusion have been implemented. The proposed cross-fusion methods highly improved the correlation coefficient (CR) between the images and the water quality parameter (WQP). Considering the higher CR values, the created WQP maps showed very good accuracy, in which the root-mean-square error values were 0.03, 0.59, 0.96, 0.26 and 279.76 for potential hydrogen (PH), dissolved oxygen (DO), chemical oxygen demand (COD), biological oxygen demand (BOD) and electrical conductivity (EC) maps, respectively. Also, the effect of considering 1 px value or the mean of a 3x3 window of the input images for calculating the regression models on the accuracy of the final maps was tested. Only the best outputs for mapping PH and DO parameters were based on applying the mean of a 3x3 window. The results also showed that increasing the window size could increase the computational complexity and decrease the WQM accuracy. Comparing the output maps with the traditional maps confirmed the higher accuracy of the proposed methods.																	1751-9659	1751-9667				MAY 29	2020	14	7					1382	1392		10.1049/iet-ipr.2019.1503													
J								Efficient image noise estimation based on skewness invariance and adaptive noise injection	IET IMAGE PROCESSING										estimation theory; optimisation; image processing; image denoising; image complexity; efficient image noise estimation; skewness invariance; precise estimation; noise level; image processing; noise standard deviation estimation; natural images; skewness-scale invariance; transform domain; adaptive noise injection strategy; natural clean image; preliminary noise estimation method; nonlinear optimisation problem; noise rectification; high-noise circumstance; preliminary estimation; noise STD	LEVEL ESTIMATION; STATISTICS	The precise estimation of the noise level is a crucial issue in image processing. In this study, the authors propose a new method for noise standard deviation (STD) estimation from natural images based on skewness-scale invariance in the transform domain and an adaptive noise injection strategy. The method is divided into two steps. The first step assumes that the natural clean image has the property of constancy of skewness in the transform domain. Then, a preliminary noise estimation method based on skewness invariance is designed by solving a constrained non-linear optimisation problem. The second step involves noise rectification via noise injection. According to the phenomenon that compared with the high-noise circumstance, the error of preliminary estimation is more serious under a low amount of noise, the noise STD is re-estimated by injecting another noise for which the STD is known. In addition, the threshold model with respect to image complexity is established to identify whether a second estimation is needed. The experimental results demonstrate the efficacy of the proposed method and performance is superior to other state-of-the-art methods.																	1751-9659	1751-9667				MAY 29	2020	14	7					1393	1401		10.1049/iet-ipr.2019.1548													
J								Non-linear calibration optimisation based on the Levenberg-Marquardt algorithm	IET IMAGE PROCESSING										cameras; calibration; nonlinear programming; optical radar; nonlinear calibration optimisation; Levenberg-Marquardt algorithm; attitude measurement; nonlinear optimisation algorithm; laser radar; point cloud; Bouget technique; Zhang technique; Hartley algorithm; calibration; camera; binocular vision		An outstanding calibration algorithm is the most important factor that affects the precision of attitude measurement. This study proposes a non-linear optimisation algorithm to refine the solutions of the initial guess obtained using the Zhang's technique, the Bouget's technique, or the Hartley's algorithm. Large sets of point correspondences were adopted to test the validity of the proposed method. Extensive practical experiments demonstrated that the proposed method can significantly improve the accuracy of calibration and ultimately obtains higher measurement precision. The error of the reprojection in the proposed method was <0.13 px. At a range of 1 m, the error rate was 0.5% for the length test and about 3% for the angle test. This study proposes a new method to calibrate the relationship between laser radar and the camera. Binocular vision was used to reconstruct the point cloud of the non-cooperative target. At the same time, data was also obtained using laser radar. Finally, the two groups of systems were fused. Accurate and dense three-dimensional information of the target was obtained. It could not only obtain the dense pose information of the target surface but also the texture and colour feature information of the target surface.																	1751-9659	1751-9667				MAY 29	2020	14	7					1402	1414		10.1049/iet-ipr.2019.1489													
J								Property-based shadow detection and removal method for licence plate image	IET IMAGE PROCESSING										object detection; edge detection; traffic engineering computing; median filters; filtering theory; licence plate image; S-V-channel-property-based shadow detection algorithm; shadow edge location algorithm; filter-based median filtering algorithm; generic shadow detection; property-based shadow removal method	SEGMENTATION; LOCALIZATION; MODEL	Shadow detection and removal is a classic research in the field of image processing. This work aims to address the problem of shadow detection and removal for licence plate. The shadow detection and removal method based on inherent properties of licence plate is presented in this study. First, an S-V-channel-property-based shadow detection algorithm is investigated to extract the background-shaded area of licence plate. Afterwards, a shadow edge location algorithm is proposed to detect the complete shadow edge. Then, the shadow is removed on H, S, and V channels with different strategies, respectively. Also, a filter-based median filtering algorithm is employed to remove the pseudo-edge. Finally, the proposed method is verified through comparing with other generic shadow detection and removal methods quantitatively and qualitatively.																	1751-9659	1751-9667				MAY 29	2020	14	7					1415	1425		10.1049/iet-ipr.2018.5660													
J								Robust Visual Saliency Optimization Based on Bidirectional Markov Chains	COGNITIVE COMPUTATION										Saliency detection; Bidirectional absorbing; Markov chain; Background and foreground possibility	BOTTOM-UP; TOP-DOWN; OBJECT DETECTION; INFORMATION; ATTENTION; MODEL	Saliency detection aims to automatically highlight the most important area in an image. Traditional saliency detection methods based on absorbing Markov chain only take into account boundary nodes and often lead to incorrect saliency detection when the boundaries have salient objects. In order to address this limitation and enhance saliency detection performance, this paper proposes a novel task-independent saliency detection method based on the bidirectional absorbing Markov chains that jointly exploits not only the boundary information but also the foreground prior and background prior cues. More specifically, the input image is first segmented into number of superpixels, and the four boundary nodes (duplicated as virtual nodes) are selected. Subsequently, the absorption time upon transition node's random walk to the absorbing state is calculated to obtain the foreground possibility. Simultaneously, foreground prior (as the virtual absorbing nodes) is used to calculate the absorption time and get the background possibility. In addition, the two aforementioned results are fused to form a combined saliency map which is further optimized by using a cost function. Finally, the superpixel-level saliency results are optimized by a regularized random walks ranking model at multi-scale. The comparative experimental results on four benchmark datasets reveal superior performance of our proposed method over state-of-the-art methods reported in the literature. The experiments show that the proposed method is efficient and can be applicable to the bottom-up image saliency detection and other visual processing tasks.																	1866-9956	1866-9964															10.1007/s12559-020-09724-6		MAY 2020											
J								Question -Led object attention for visual question answering	NEUROCOMPUTING																													0925-2312	1872-8286				MAY 28	2020	391						227	233		10.1016/j.neucom.2018.11.102													
J								A semi -supervised convolutional transfer neural network for 3D pulmonary nodules detection	NEUROCOMPUTING											FALSE-POSITIVE REDUCTION																		0925-2312	1872-8286				MAY 28	2020	391						199	209		10.1016/j.neucom.2018.12.081													
J								Energy-Efficiency Clustering and Data Collection for Wireless Sensor Networks in Industry 4.0	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Drone; Dynamic clustering; Energy consumption; Industry 4; 0; Optimization; Routing; WSNs	ALGORITHM; LIFETIME; STRATEGY	Wireless Sensor Networks (WSNs)-based networking systems introduces to transfer from traditional industry to digital industry, commonly known as the fourth stage of industrialization (Industry 4.0). The WSN is an encouraging technology for many industrial applications because of their several potential benefits. However, they impose several challenges when using them for various monitoring and control applications in the Industry 4.0. Improving lifetime and minimizing power consumption are the main challenges of wireless sensor networks. Solving this problem consist of optimizing node deployment, offering an energy-efficient routing protocol, and providing a clustering approach for sensor nodes in order to optimize battery utilization. Despite the fact that Cluster Head (CH) is overwhelmed with nodes traffic and dies quickly, in most studies, the choice of CH and the creation of clusters take into consideration only the value of residual energy in the sensor nodes, which causes an unequal load balance cluster. The purpose of this study is to propose an advanced clustering in wireless sensors networks, that takes into account not only the value of the residual energy but also the degree of connection, the distance between the CH and other network nodes and the antenna orientation. Besides, a drone routing approach based on artificial intelligence is adapted for data collection to overcome the problem of hot spot. The proposed approach is compared with existing clustering methods such as LEACH, LEACH-C, and LEACH-B that are designed for fixed WSNs. The results of simulations obtained depict that clustering algorithm with optimized routing significantly improves the network lifetime.																	1868-5137	1868-5145															10.1007/s12652-020-02146-0		MAY 2020											
J								Energy efficient momento based dynamic scheduling for lifetime maximization in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Lifetime maximization; Scheduling; Momento management; Routing; TS; LMS		The problem of routing with scheduling and lifetime maximization in WSN has been well studied. There are number of algorithms discussed earlier for the support of lifetime maximization and scheduling of nodes in WSN. However, they suffer to achieve higher performance in maximization of lifetime of sensor nodes. To improve the performance, an energy efficient momento based dynamic scheduling algorithm (EEMDS) is presented in this article. The proposed method considers energy and previous transmission history of different nodes to perform scheduling. The method first collects the list of sensor which has packets to be transmitted and allocates momento according to the priority of nodes. The node selected has been assigned with the momento which is required for the data transmission. Once the source and destination node has been identified, then according to the topology of nodes, a set of routes has been identified. For each route and the list of intermediate nodes, the method estimates the transmission support and lifetime maximization support values. The transmission support has been measured based on the number of sensors and energy where the lifetime support is measured according to the energy parameter and the previous transmission history. Finally, a small set of nodes are selected and scheduled for working mode. The route available in the selected route has been used to perform data transmission. According to the result of route selection, the list of nodes present in the route is identified. Such nodes present in the selected route are scheduled to be wakeup in the current transmission where the remaining nodes present in the network are scheduled to be in sleep mode. This increases the throughput performance and lifetime of the entire network.																	1868-5137	1868-5145															10.1007/s12652-020-02131-7		MAY 2020											
J								Energy aware reliable route selection scheme with clustered RP model for wireless sensor networks to promote interaction between human and sensors	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Networks; Interaction between sensor and human; Energy efficiency; Network lifetime	ALGORITHM; CUCKOO	Wireless sensor network (WSN) is one of the significant flourishing areas of research, due to its controllability, accessibility and adaptability. WSN is deployed for better control and track of the industrial equipments, which paves way for the proposals of numerous real-time applications out of it. The primary goal of a WSN is to promote communication between the industrial sensor nodes, such that the information is shared and decisions can be made. However, communication works out at the cost of energy, which is considerably scarce in WSN. Thus, energy efficient routing is necessary for a network to work for a reasonable time. This work presents an energy efficient route selection scheme, which selects the reliable route out of all possible routes. The reliable route is selected by considering the trust level of the sensor nodes, which results in faster and reliable data transmission. The performance of the proposed work is tested and compared with the existing approaches in terms of packet delivery rate, latency, energy efficiency and network lifetime.																	1868-5137	1868-5145															10.1007/s12652-020-02147-z		MAY 2020											
J								An investigation of unicyclic graphs in which the isolate bondage number is equal to three in graph network theory	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Tree network; Isolate bondage number in networks; Bondage number; Perfect edge domination	DOMINATION	A set S (of vertices) of a graph G is termed a dominating set of G if each vertex in V - S is adjacent to a node in S. A dominating set S such as the subgraph induced by S has an isolated vertex is termed an isolate dominating set and also the minimum count of an isolate dominating set is termed the isolate domination number of G and it is represented by gamma(is)(G). A subset X subset of E is said to be an edge dominating set if each edge in X - E is adjacent to some edge in S. The edge domination number is that the count of the smallest edge dominating set of G and is employed by gamma'. A collection of edges X of E is claimed to be a perfect edge dominating set if each edge not in X is adjacent to precisely one edge in X. The ideal edge domination number is that the minimum cardinality has taken perfect edge dominating sets of G and is denoted by gamma(p)'. In this paper, we initiate the survey of bondage related to isolate domination. The isolate bondage number b is(G) is outlined to be the minimum cardinality of a collection of edges whose relieved from G ends up in a graph G' fulfilling > gamma(is)(G') > gamma (i)s (G). We obtain several results for isolate dominating set and identical values of isolate bondage number. Moreover, we investigate some bounds for the isolate bondage number, and this bound is keen and analyze under which conditions the domination parameter and isolate domination parameter are equal. Also, we found some more results for perfect edge domination, and we characterize trees for which gamma' = gamma(p)' and further exciting results.																	1868-5137	1868-5145															10.1007/s12652-020-02105-9		MAY 2020											
J								Comparative study of Pareto optimal multi objective cuckoo search algorithm and multi objective particle swarm optimization for power loss minimization incorporating UPFC	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimal location; Pareto-optimal technique; Multiobjective cuckoo search algorithm; Multiobjective particle swarm optimization; Unified power flow controller; FACTS	CONTROLLER	The Flexible AC Transmission System (FACTS) devices are being commissioned in electrical power systems across the globe owing to the vast array of benefits they offer. The optimal performance of the FACTS devices can be harnessed only if they are installed at a strategic location. In this paper, the authors suggest the merit of multiobjective cuckoo search (MOCS) algorithm in mitigation of transmission losses by strategically installing unified power flower controller (UPFC) at an optimal location. Active power loss and reactive power loss reduction is the multiobjective optimization considered for the study. The Pareto-optimal technique is employed to extract the Pareto-optimal solution for the multiobjective problem considered. The Fuzzy logic method is utilized to yield the best-compromise solution from the pool of Pareto-optimal solution. The proposed approach is tested on a standard IEEE 30 bus test system. Furthermore, the efficacy of the MOCS algorithm is demonstrated by comparing the results with that of multiobjective particle swarm optimization (MOPSO).																	1868-5137	1868-5145															10.1007/s12652-020-02142-4		MAY 2020											
J								Dynamic cross propagation algorithm based detection of micro calcification in digital mammogram	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Dynamic cross propagation algorithm (DCPA) based automatic classification; Micro calcification clusters in mammograms a superior region of interest (SROI)	BREAST; CANCER	Micro calcification in mammograms may be considered early signs of breast cancer. However, their detection by a variety of factors is a very challenging task to find cancer in an instant starting stage. A breast compression, more difficult breast anatomy, and in some cases, inaccessible size calculations, as well as the significant variation of low contrast, inherent to mammograms. Therefore a computerized image processing scheme is implemented for detecting early-stage Microcalcification in mammograms. Necessarily the masses and microcalcification may be a prominent early symptom of breast cancer. This proposed Dynamic Cross Propagation Algorithm (DCPA) tested several images from digital database mammography for cancer research and diagnosis. In the first stage of preprocessing will proceed for reducing noise in the original image. In the second stage to Finding of interest, which can be done by using a superior region of interest (SROI) detection and region segmentation, is in the area of a suspicious mass on a mammogram. After detecting suspicious mass on the mammogram, the features like mean, standard deviation, variance, skewness, and entropy values are obtained for further classification. In the Fourth stage the feature result, based classification using the proposed Dynamic Cross Propagation Algorithm (DCPA) compiles to find the loss function in the micro image and compare it with the train the model. In this method, SROI fractal dimensions have computed the value that corresponds with training data to find the suspicious microcalcification regions. The proposed system shows a high rate of true positives and a low rate of false-positive, a superior performance compared to the conventional method. The performance of the proposed DCPA technique is analyzed in terms of precision, recall and F-measure. The experimental results produced a classification that gives better accuracy 98.38% of which indicates that the proposed system is more promising in classification digital mammograms.																	1868-5137	1868-5145															10.1007/s12652-020-02133-5		MAY 2020											
J								An Analysis of Trapezoidal Intuitionistic Fuzzy Preference Relations Based on (alpha,beta)-cuts	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Preference relations; Pairwise comparison; Trapezoidal intuitionistic fuzzy numbers; (alpha,beta)-cuts	GROUP DECISION-MAKING; SET; PRESERVATION; PRIORITIES; NUMBERS; MODELS; MADM	Pairwise relations are accepted ways to compare a finite set of elements. Some researchers believe that human mind acts the same when choosing among different elements. Another feature of human judgments that must be considered in these comparisons is their uncertainty in expressing the preference of elements. Combining these two features of human mind, a method is developed for pairwise comparisons, using trapezoidal intuitionistic fuzzy numbers. In this paper, decision-makers are allowed to express their preferences and then determine their beliefs and hesitancy regarding these judgments. The application of the proposed method is demonstrated by a real world case about choosing strategic priorities of chain stores. Decision-makers can increase their ability to specify the preferences of elements under their internal uncertainty by this method.																	1562-2479	2199-3211															10.1007/s40815-020-00875-0		MAY 2020											
J								Stochastic recurrent wavelet neural network with EEMD method on energy price prediction	SOFT COMPUTING										Prediction; Stochastic recurrent wavelet neural network; Ensemble empirical mode decomposition; Energy indexes; Error evaluation; Multiscale complexity-invariant distance	SHORT-TERM LOAD; EMPIRICAL MODE DECOMPOSITION; TIME-SERIES; WIND; ALGORITHM; MEMORY; MULTISTEP; SEARCH; SYSTEM	Novel hybrid neural network prediction model (denoted by E-SRWNN) is formed by combining ensemble empirical mode decomposition (EEMD) and stochastic recurrent wavelet neural network (SRWNN), in order to improve the precision of energy indexes price forecasting. Energy index price series are non-stationary, nonlinear and random. EEMD method is utilized to decompose the closing prices of four energy indexes into subsequences with different frequencies, and the SRWNN model is composed by adding stochastic time effective function and recurrent layer to the wavelet neural network (WNN). Stochastic time effective function makes the model assign different weights to the historical data at different times, and the introduction of recurrent layer structure will enhance the data learning. In this paper, E-SRWNN model is compared with other WNN-based models and the deep learning network GRU. In the error evaluation, the general standards, such as linear regression analysis, mean absolute error and theil inequality coefficient, are utilized to compare the predicted effects of different models, and then multiscale complexity-invariant distance is applied for further analysis. Empirical research illustrates that the proposed E-SRWNN model displays strong forecasting ability and accurate forecasting results in energy price series forecasting.																	1432-7643	1433-7479				NOV	2020	24	22					17133	17151		10.1007/s00500-020-05007-2		MAY 2020											
J								On the weighted Gini-Simpson index: estimating feasible weights using the optimal point and discussing a link with possibility theory	SOFT COMPUTING										Weighted Gini-Simpson index; Maximizer's solution; Inversion procedure; Feasible weights; Probability-possibility transformation; Epistemic threshold	UNCERTAIN-INFORMATION; TRANSFORMATIONS; DIVERSITY	Following a brief historic note on the foundations and some applications of the Gini-Simpson index and its weighted versions, it focuses on the inversion problem relative to the maximum point of the weighted index, which has the nature of a probability distribution given a set of positive weights. Then, after revisiting the general background of the topic under study is formulated the process of estimating feasible weights that would generate the maximizer's solution. The methodology herein explained uses a fixed-pole method, what provides a unique solution restricted to the support of the maximizer under a normalization constraint. The procedure is exemplified numerically, and an appellative link with possibility theory is discussed. The novel probability-possibility transformation here outlined, anchored in the optimal point of the weighted Gini-Simpson index and an associated consistency sufficient condition, seems to be plausible and even useful in given contexts, either as a basis for decision-making procedures or for testing real weights operating as driving forces of the composition. Also, the method provides a distinction between virtual and epistemic possibilities, relative to a defined quantitative threshold.																	1432-7643	1433-7479				NOV	2020	24	22					17187	17194		10.1007/s00500-020-05011-6		MAY 2020											
J								Path Capsule Networks	NEURAL PROCESSING LETTERS										Neural networks; Capsule networks; Modularity		Capsule network (CapsNet) was introduced as an enhancement over convolutional neural networks, supplementing the latter's invariance properties with equivariance through pose estimation. CapsNet achieved a very decent performance with a shallow architecture and a significant reduction in parameters count. However, the width of the first layer in CapsNet is still contributing to a significant number of its parameters and the shallowness may be limiting the representational power of the capsules. To address these limitations, we introduce Path Capsule Network (PathCapsNet), a deep parallel multi-path version of CapsNet. We show that a judicious coordination of depth, max-pooling, regularization by DropCircuit and a new fan-in routing by agreement technique can achieve better or comparable results to CapsNet, while further reducing the parameter count significantly.																	1370-4621	1573-773X				AUG	2020	52	1			SI		545	559		10.1007/s11063-020-10273-0		MAY 2020											
J								Local Structure Preservation for Nonlinear Clustering	NEURAL PROCESSING LETTERS										Clustering; Similarity; Local structure; Sparse learning	KERNEL FUNCTION	In this paper, we propose a new nonlinear clustering method to preserve local structure of the features. Specifically, our method applies the gaussian kernel function to achieve high dimensional projection so as to make the original data linearly separable. Our method establishes the similarity matrix of data features in low-dimensional space to conduct local structure learning, as a result, it can avoid the divergence of sample sets and retain the original nearest neighbor structural relations. Furthermore, our method uses the sparse learning to remove the redundant features to make the model more robust in the process of learning. Experimental results on eight benchmark datasets show that our proposed method was superior to the state-of-the-art clustering methods in terms of clustering performance.																	1370-4621	1573-773X															10.1007/s11063-020-10251-6		MAY 2020											
J								High-Quality Video Generation from Static Structural Annotations	INTERNATIONAL JOURNAL OF COMPUTER VISION										Unsupervised learning; Conditioned generative model; Image and video synthesis; Motion prediction and estimatiovn		This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released:.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2552	2569		10.1007/s11263-020-01334-x		MAY 2020											
J								Compositional GAN: Learning Image-Conditional Binary Composition	INTERNATIONAL JOURNAL OF COMPUTER VISION										Conditional Generative Adversarial Network; Composition; Decomposition		Generative Adversarial Networks can produce images of remarkable complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we propose a novel self-consistent Composition-by-Decomposition network to compose a pair of objects. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. We evaluate our approach through qualitative experiments and user evaluations. Our results indicate that the learned model captures potential interactions between the two object domains, and generates realistic composed scenes at test time.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2570	2585		10.1007/s11263-020-01336-9		MAY 2020											
J								Cost-sensitive KNN classification	NEUROCOMPUTING											ALGORITHM																		0925-2312	1872-8286				MAY 28	2020	391						234	242		10.1016/j.neucom.2018.11.101													
J								Local differential privacy for social network publishing	NEUROCOMPUTING																													0925-2312	1872-8286				MAY 28	2020	391						273	279		10.1016/j.neucom.2018.11.104													
J								Multi -class multimodal semantic segmentation with an improved 3D fully convolutional networks	NEUROCOMPUTING											BRAIN-TUMOR SEGMENTATION; HIPPOCAMPUS																		0925-2312	1872-8286				MAY 28	2020	391						220	226															
J								A genetic algorithm based framework for local search algorithms for distributed constraint optimization problems	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multi-agent system; Distributed constraint optimization problem; Incomplete algorithm; Genetic algorithm; Local search algorithm	DECENTRALIZED COORDINATION; SATISFACTION; BREAKOUT; ADOPT	Local search algorithms are widely applied in solving large-scale Distributed constraint optimization problems (DCOPs) where each agent holds a value assignment to its variable and iteratively makes a decision on whether to replace its assignment according to its neighbor states. However, the value assignments of their neighbors confine their search to a small space so that agents in local search algorithms easily fall into local optima. Fortunately, Genetic Algorithms (GAs) can direct a search process to a more promising space and help the search process to break up the confine of local states. Accordingly, we propose a GA-based framework (LSGA) to enhance local search algorithms, where a series of genetic operators are redesigned for agents in distributed scenario to accommodate DCOPs. First, a fitness function is designed to evaluate the assignments for each agent, considering the balance of local benefits and global benefits. Then, a new method is provided to decide crossover positions in terms of agent-communication and topological structure of DCOPs. Besides, a self-adaptive crossover probability and a self-adaptive mutation probability are proposed to control the uses of crossover operator and mutation operator, respectively. And more importantly, the LSGA framework can be easily applied in any local search algorithm. The experimental results demonstrate the superiority of the use of LSGA in the typical search algorithms over state-of-the-art incomplete algorithms.																	1387-2532	1573-7454				MAY 28	2020	34	2							41	10.1007/s10458-020-09464-9													
J								Data Association and Localization of Classified Objects in Visual SLAM	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Localization and mapping; Data association; Semantic mapping; Unsupervised learning	RECOGNITION	Maps generated by many visual Simultaneous Localization and Mapping algorithms consist of geometric primitives such as points, lines or planes. These maps offer a topographic representation of the environment, but they contain no semantic information about the environments. Object classifiers leveraging advances in machine learning are highly accurate and reliable, capable of detecting and classifying thousands of objects. Classifiers can be incorporated into a SLAM pipeline to add semantic information to a scene. Frequently, this semantic information is conducted for each frame of the image, but semantic labeling is not persistent over time. In this work, we present a nonparametric statistical approach to perform matching/association of objects detected over consecutive image frames. These associated classified objects are then localized in the accrued map using an unsupervised clustering method. We test our approach on multiple data sets, and it shows strong performance in terms of objects correctly associated from frame to frame. We also have tested our algorithm on three data sets in our lab environment using tag markers to demonstrate the accuracy of classified object localization process.																	0921-0296	1573-0409				OCT	2020	100	1					113	130		10.1007/s10846-020-01189-x		MAY 2020											
J								Safety evaluation method of hoisting machinery based on neural network	NEURAL COMPUTING & APPLICATIONS										Neural network; Hoisting machinery; The information entropy; Fuzzy mathematics; Safety evaluation	INFORMATION ENTROPY; CLASSIFICATION	Hoisting machinery as a material handling equipment, widely used in the national economy departments, in the national "safe, efficient, green and harmonious" under the application requirements, to improve the intrinsically safe hoisting machinery, a complex system, in this paper, the affecting the safe operation of the hoisting machinery hazards, summary and analysis based on the intrinsic safety theory and correlation analysis method, on the nature of the hoisting machinery safety assessment model is established. The theory of information entropy and fuzzy mathematics, the safety evaluation method of hoisting machinery based on neural network is studied. Through summarizing the hazard factor of hoisting machinery, lifting machinery design, manufacture, installation, alteration, use, and management and so on, this paper analyzes advantages and disadvantages of commonly used safety assessment or prediction method, based on the "human-environment" of safety evaluation of ideas, will influence of lifting machinery into the ontology equipment hazards, organizational security hazards, essence of safety culture and emergency fault handling of hazards. In the paper, two neural networks are used to predict the failure rate, and the accuracy of the two methods is compared. Firstly, BP neural network is optimized by genetic algorithm for prediction. BP neural network optimized by genetic algorithm is the most widely used neural network for prediction. Secondly, Elman neural network is used for prediction. Two neural networks are used to predict the failure rate, study the structural weight of neural network, obtain the prediction result graph and prediction error graph of neural network, and analyze the results, so as to judge the availability of using neural network method to predict the failure rate.																	0941-0643	1433-3058															10.1007/s00521-020-04963-y		MAY 2020											
J								Autoencoder-based image processing framework for object appearance modifications	NEURAL COMPUTING & APPLICATIONS										Visual object editing; Convolutional autoencoders; Supervised principal component analysis; Machine learning		The presented paper introduces a novel method for enabling appearance modifications for complex image objects. Qualitative visual object properties, quantified using appropriately derived visual attribute descriptors, are subject to alterations. We adopt a basic convolutional autoencoder as a framework for the proposed attribute modification algorithm, which is composed of the following three steps. The algorithm begins with the extraction of attribute-related information from autoencoder's latent representation of an input image, by means of supervised principal component analysis. Next, appearance alteration is performed in the derived feature space (referred to as 'attribute-space'), based on appropriately identified mappings between quantitative descriptors of image attributes and attribute-space features. Finally, modified attribute vectors are transformed back to latent representation, and output image is reconstructed in the decoding part of an autoencoder. The method has been evaluated using two datasets: images of simple objects-digits from MNIST handwritten-digit dataset and images of complex objects-faces from CelebA dataset. In the former case, two qualitative visual attributes of digit images have been selected for modifications: slant and aspect ratio, whereas in the latter case, aspect ratio of face oval was subject to alterations. Evaluation results prove, both in qualitative and quantitative terms, that the proposed framework offers a promising tool for visual object editing.																	0941-0643	1433-3058															10.1007/s00521-020-04976-7		MAY 2020											
J								A robust image steganography using teaching learning based optimization based edge detection model for smart cities	COMPUTATIONAL INTELLIGENCE										edge detection; image hiding; metaheuristic algorithm; steganography; TLBO; transferred data		Recently, Internet becomes a most common medium for transferring critical data and the security of the transmitted data gains maximum priority. Image steganography has been developed as a well-known model of data hiding which verifies the security level of the transferred data. The images offer high capacity, and the occurrence of accessibility over the Internet is more. An effective steganography model is required for achieving better embedding capacity and also maintaining the other variables in an acceptable value. This article introduces a new robust image steganography using Teaching Learning Based Optimization (TLBO) edge detection model. The TBLO is basically a metaheuristic algorithm which is inspired from the teaching and learning procedure in classrooms. The former stage indicates the learning from the teacher and the latter phase represents the interaction among the learners. The experimental validation takes place in a comprehensive way under several views and the outcome pointed out the superior results of the presented model.																	0824-7935	1467-8640				AUG	2020	36	3					1275	1289		10.1111/coin.12348		MAY 2020											
J								An overview of stability analysis and state estimation for memristive neural networks	NEUROCOMPUTING										Neural networks; Memristive neural networks; Time-delay; Network-induced phenomena; State estimation	GLOBAL EXPONENTIAL STABILITY; TIME-VARYING SYSTEMS; RANDOMLY OCCURRING NONLINEARITIES; MARKOVIAN JUMP SYSTEMS; H-INFINITY; ROUND-ROBIN; ASYMPTOTIC STABILITY; DISTRIBUTED DELAYS; COMPLEX NETWORKS; SENSOR NETWORKS	This paper gives a review of recent advances on memristive neural networks with emphasis on the issues of stability analysis and state estimation. First, the concept of memristive neural network is recalled with a brief introduction of its background. Then, certain types of frequently seen neural networks are reviewed comprehensively with latest progress. Some engineering-oriented phenomena that appear extensively in the context of networked systems are introduced and summarized, including random dynamics, time-delays and network-induced incomplete information, etc. From different perspectives, several techniques explored for designing the required state estimators of memristive neural networks are discussed in detail. Some latest progress regarding the stability analysis and state estimation problems for discrete time memristive neural networks are presented. Finally, we provide the conclusions and point out certain future research directions. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 28	2020	391						1	12		10.1016/j.neucom.2020.01.066													
J								Directional attention weaving for text -grounded conversational question answering	NEUROCOMPUTING																													0925-2312	1872-8286				MAY 28	2020	391						13	24															
J								Weakly supervised semantic segmentation by iterative superpixel-CRF refinement with initial clues guiding	NEUROCOMPUTING											LOCALIZATION																		0925-2312	1872-8286				MAY 28	2020	391						25	41															
J								Visual -audio emotion recognition based on multi -task and ensemble learning with multiple features ?	NEUROCOMPUTING											FEATURE-SELECTION; FRAMEWORK																		0925-2312	1872-8286				MAY 28	2020	391						42	51															
J								FRIEND: Feature selection on inconsistent data	NEUROCOMPUTING											EMBEDDED FEATURE-SELECTION; MUTUAL INFORMATION; OPTIMIZATION; SEARCH; MACHINE																		0925-2312	1872-8286				MAY 28	2020	391						52	64															
J								Tag recommendation by text classification with attention -based capsule network	NEUROCOMPUTING																													0925-2312	1872-8286				MAY 28	2020	391						65	73															
J								A novel dataset-specific feature extractor for zero -shot learning	NEUROCOMPUTING											NEURAL-NETWORKS																		0925-2312	1872-8286				MAY 28	2020	391						74	82															
J								Toward improving ECG biometric identification using cascaded convolutional neural networks	NEUROCOMPUTING											CLASSIFICATION; AUTHENTICATION; MOBILE																		0925-2312	1872-8286				MAY 28	2020	391						83	95															
J								Physics Informed Extreme Learning Machine (PIELM)?A rapid method for the numerical solution of partial differential equations	NEUROCOMPUTING											BOUNDARY-VALUE-PROBLEMS; NEURAL-NETWORK METHODS; SOLVING ORDINARY																		0925-2312	1872-8286				MAY 28	2020	391						96	118															
J								Double robust principal component analysis	NEUROCOMPUTING											FACTORIZATION; EIGENFACES																		0925-2312	1872-8286				MAY 28	2020	391						119	128															
J								Session -based recommendation via flow -based deep generative networks and Bayesian inference	NEUROCOMPUTING																													0925-2312	1872-8286				MAY 28	2020	391						129	141															
J								Sparse multiple instance learning with non -convex penalty	NEUROCOMPUTING											VARIABLE SELECTION; SUPPORT; CLASSIFICATION; FRAMEWORK																		0925-2312	1872-8286				MAY 28	2020	391						142	156															
J								Adaptive neural finite -time containment control for nonlower triangular nonlinear multi -agent systems with dynamics uncertainties	NEUROCOMPUTING											TRACKING CONTROL; SURFACE CONTROL; CONSENSUS																		0925-2312	1872-8286				MAY 28	2020	391						157	166															
J								LSTM-Cubic A * -based auxiliary decision support system in air traffic management	NEUROCOMPUTING											TRAJECTORY PREDICTION; OPTIMIZATION; ATTENTION; AWARENESS; DISTANCE; NETWORK; SEARCH; RUNWAY																		0925-2312	1872-8286				MAY 28	2020	391						167	176															
J								Cost -sensitive joint feature and dictionary learning for face recognition	NEUROCOMPUTING											CLASSIFICATION; FUSION																		0925-2312	1872-8286				MAY 28	2020	391						177	188															
J								Learning longitudinal classification -regression model for infant hippocampus segmentation	NEUROCOMPUTING											MULTI-ATLAS SEGMENTATION; BRAIN-DEVELOPMENT; FEATURE-SELECTION; RANDOM FORESTS; LABEL FUSION; ROBUST; IMAGES; MRI																		0925-2312	1872-8286				MAY 28	2020	391						191	198															
J								Interactive resource recommendation with optimization by tag association and significance analysis	NEUROCOMPUTING											ALGORITHM; SEARCH																		0925-2312	1872-8286				MAY 28	2020	391						210	219															
J								Robust self -tuning spectral clustering	NEUROCOMPUTING											SEGMENTATION; ALGORITHM																		0925-2312	1872-8286				MAY 28	2020	391						243	248															
J								Underwater salient object detection by combining 2D and 3D visual features	NEUROCOMPUTING											VISION; CONTRAST																		0925-2312	1872-8286				MAY 28	2020	391						249	259															
J								An adaptive fractional -order BP neural network based on extremal optimization for handwritten digits recognition	NEUROCOMPUTING											PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; ALGORITHM; PARAMETERS; CLASSIFIER; DESIGN; MODEL																		0925-2312	1872-8286				MAY 28	2020	391						260	272															
J								A freight inspection volume forecasting approach using an aggregation/disaggregation procedure, machine learning and ensemble models	NEUROCOMPUTING											SUPPORT VECTOR MACHINE; CONTAINER THROUGHPUT; NEURAL-NETWORKS; CLASSIFIERS; CLASSIFICATION; PORT; FLOW																		0925-2312	1872-8286				MAY 28	2020	391						282	291															
J								A data -driven approach to spoken dialog segmentation	NEUROCOMPUTING											SYSTEMS																		0925-2312	1872-8286				MAY 28	2020	391						292	304															
J								Hybrid short term prediction to address limited timeliness of public transport data streams	NEUROCOMPUTING											INTERNET; THINGS																		0925-2312	1872-8286				MAY 28	2020	391						305	317															
J								Optimization with the evolution strategy by example of electrical -discharge drilling	NEUROCOMPUTING											PROCESS PARAMETERS																		0925-2312	1872-8286				MAY 28	2020	391						318	324															
J								Robust weighted regression via PAELLA sample weights	NEUROCOMPUTING											PREDICTION; MODELS; MEXICO																		0925-2312	1872-8286				MAY 28	2020	391						325	333															
J								Experimental assessment of quality in injection parts using a fuzzy system with adaptive membership functions	NEUROCOMPUTING											MODEL; OPTIMIZATION; LOGIC																		0925-2312	1872-8286				MAY 28	2020	391						334	344															
J								Health assessment of LFP automotive batteries using a fractional -order neural network	NEUROCOMPUTING											MODEL; SYSTEMS; STATE																		0925-2312	1872-8286				MAY 28	2020	391						345	354															
J								Bayesian networks plus reinforcement learning: Controlling group emotion from sensory stimuli	NEUROCOMPUTING											RECOGNITION; DESIGN																		0925-2312	1872-8286				MAY 28	2020	391						355	364															
J								Deep learning-based soft computing model for image classification application	SOFT COMPUTING										Biomedical application; Lung cancer; Swarm intelligence; Deep learning; Sustainable model; Soft computing model	CONVOLUTIONAL NEURAL-NETWORK; PARTICLE SWARM OPTIMIZATION; LUNG-CANCER DETECTION	The growth of swarm intelligence approaches and machine learning models in the field of medical image processing is extravagant, and the applicability of these approaches for various types of cancer classification has as well grown in the recent years. Considering the growth of these machine learning models, in this work attempt is taken to develop an optimized deep learning neural network classifier for classifying the nodule tissues in the lung cancer images which is an important application in biomedical area. The optimized model developed is the hybrid version of adaptive multi-swarm particle swarm optimizer with the new improved firefly algorithm resulting in better exploration and exploitation mechanism to determine near-optimal solutions. Multi-swarm particle swarm optimizer (MSPSO) possesses strong exploration capability due to its regrouping schedule nature, and the improved firefly algorithm (ImFFA) possesses better exploitation mechanism due to its inherit attractiveness and intensity feature. At this juncture, the new adaptive MSPSO-ImFFA is applied to the deep learning neural classifier to overcome the local and global minima occurrences and premature convergence by tuning its weight values. As a result, in this work the new adaptive MSPSO-ImFFA-based deep learning neural network classifier is employed to classify the lung cancer tissues of the considered lung computed tomography images. Results obtained prove the effectiveness of the deep learning classifier for the considered lung image sample datasets in comparison with the other methods compared from the previous literature works.																	1432-7643	1433-7479															10.1007/s00500-020-05048-7		MAY 2020											
J								Fuzzy cognitive maps for decision-making in dynamic environments	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Autonomous systems; Decision-making; Dynamic environments; Fuzzy cognitive maps; Multi-agent models	FCM; SYSTEM	This paper describes a new modification of fuzzy cognitive maps (FCMs) for the modeling of autonomous entities that make decisions in a dynamic environment. The paper offers a general design for an FCM adjusted for the decision-making of autonomous agents through the categorization of its concepts into three different classes according to their purpose in the map:Needs,Activities, andStates(FCM-NAS). The classification enables features supporting decision-making, such as the easy processing of input from sensors, faster system reactions, the modeling of inner needs, the adjustable frequency of computations in a simulation, and self-evaluation of the FCM-NAS that supports unsupervised evolutionary learning. This paper presents two use cases of the proposed extension to demonstrate its abilities. It was implemented into an agent-based artificial life model, where it took advantage of all the above features in the competition for resources, natural selection, and evolution. Then, it was used as decision-making for human activity simulation in an ambient intelligence model, where it is combined with scenario-oriented mechanism proving its modularity.																	1389-2576	1573-7632															10.1007/s10710-020-09393-2		MAY 2020											
J								Ensemble learning based on fitness Euclidean-distance ratio differential evolution for classification	NATURAL COMPUTING										Machine learning; Ensemble learning; Multimodal evolutionary algorithm; Neural network	SELECTIVE ENSEMBLE; ALGORITHMS; FOREST	Ensemble learning is a system that combines a set of base learners to improve the performance in machine learning, where accuracy and diversity of base learners are two important factors. However, these two factors are usually contradictory. To address this problem, in this paper, we propose a novel ensemble learning algorithm based on fitness Euclidean-distance ratio differential evolution, to train the neural network ensemble. FEFERR_ELA employs a multimodal evolutionary algorithm that is capable of producing diverse solutions to search for optimal solutions corresponding to parameters of base learners, where each optimal solution leads to one trained model. A dynamic ensemble selection scheme is applied to select appropriate individuals for the ensemble. The proposed algorithm is evaluated on several benchmark problems and compared with some related ensemble learning models. The experimental results demonstrate that the proposed algorithm outperforms the related works and can produce the neural network ensembles with better generalization.																	1567-7818	1572-9796															10.1007/s11047-020-09791-6		MAY 2020											
J								A novel distributed training on fog node in IoT backbone networks for security	SOFT COMPUTING										Distributed training on fog node (DT-FN); Intrusion detection systems (IDS); Machine learning (ML)		Security is a significant issue with ubiquitous connectivity, more so with the widespread adoption of Internet of Things (IoT). The novelty of attacks with each passing day poses a conundrum to the organizations and sectors deploying the IoT. The fact remains that conventional cybersecurity frameworks face the trouble of distinguishing unknown attacks in most scenarios. Recent studies explore the endless capabilities of machine learning (ML) in reinstating the security of the IoT infrastructure. ML includes the capability of self-study and training for discovering the path for security breach and attack detection. The training methodology and progression in ML is superior to centralized detection systems. A novel methodology of distributed training on fog node in the IoT architecture with exchange of parameters (DT-FN) enhancement of intelligence through machine learning is proposed in this paper. The analyses have demonstrated that appropriated assault recognition framework has outsmarted the incorporated discovery frameworks utilizing ML model. Intrusion detection system of IoT has been incorporated on DL for more effectiveness and identification of higher level security threats. Target prejudgement-based interruption identification framework for IoT has also been discussed. The key metrics that are considered are detection rate, detection accuracy, false alarm rate, F1 measurement, precision and recall.																	1432-7643	1433-7479															10.1007/s00500-020-05047-8		MAY 2020											
J								The carousel of ethical machinery	AI & SOCIETY										Machine ethics; Ethical AI; Evolutionary morality; Social impacts of AI		Human beings have been aware of the risks associated with knowledge or its associated technologies since the dawn of time. Not just in Greek mythology, but in the founding myths of Judeo-Christian religions, there are signs and warnings against these dangers. Yet, such warnings and forebodings have never made as much sense as they do today. This stems from the emergence of machines capable of cognitive functions performed exclusively by humans until recently. Besides those technical problems associated with its design and conceptualization, the cognitive revolution, brought about by the development of AI also gives rise to social and economic problems that directly impact humanity. Therefore, it is vital and urgent to examine AI from a moral point of view. The moral problems are two-fold: on the one hand, those associated with the type of society we wish to promote through automation, complexification and power of data processing available today; on the other, how to program decision-making machines according to moral principles acceptable to those humans who will share knowledge and action with them.																	0951-5666	1435-5655															10.1007/s00146-020-00994-0		MAY 2020											
J								Optimized virtual network function provisioning technique for mobile edge cloud computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile edge cloud computing; Network function virtualization; Service function chain; Optimization	PLACEMENT; SIMULATION	In the cloud environment, network functions virtualization (NFV) means to diminish cost and disentangle tasks of such system benefits through the virtualization advancements. To authorize organize approaches in NFV-based cloud conditions, arrange administrations are made out of virtualized network functions (VNFs) that are anchored together as service function chains. This fills different needs-cloud farms could be chosen to streamline the rate, the picked presentation constraints could be kept inside as far as possible, and the speed of arrangement can be expanded. Diverse VNFs can be affixed together to shape distinctive administration chains for various system administrations, to meet different client information directing requests. From the service provider perspective, such administrations are normally actualized by VNF examples in a cloudlet system comprising of a lot of server farms and switches. In this paper, a method is proposed for dynamic VNFs provisioning in cloud situations. The technique considers the inertness necessity of various requests for service chain functions. It permits the inactivity delicate requests to diminish the start to finish system interval by using edge assets in the cloud. We assess our technique with enormous scale reenactments that think about practical system topologies and our prototype in a substrate composed of four private data centers and three public clouds. The framework scales well for systems of thousands of switches utilizing differing topologies and enhances the virtual network acceptance ratio and inserting a delay.																	1868-5137	1868-5145															10.1007/s12652-020-02122-8		MAY 2020											
J								Trbaggboost: an ensemble-based transfer learning method applied to Indian Sign Language recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ensemble learning; Indian Sign Language; Sign language recognition; Transfer learning		An efficient sign language recognition (SLR) system would help speech and hearing-impaired people to communicate with normal people. This work aims to develop a SLR system for Indian sign language using data acquired from multichannel surface electromyogram, tri-axis accelerometers and tri-axis gyroscopes placed on both the forearms of signers. A novel ensemble-based transfer learning algorithm called Trbaggboost is proposed, which uses small amount of labeled data from a new subject along with labelled data from other subjects to train an ensemble of learners for predicting unlabeled data from the new subject. Conventional machine learning algorithms such as decision tree, support vector machine and random forest (RF) are used as base learners. The results for classification of signs using Trbaggboost are compared with commonly used transfer learning algorithms such as TrAdaboost, TrResampling, TrBagg, and simple bagging approach such as RF. Average accuracy for classification of signs performed by a new subject is achieved as 69.56% when RF is used without transfer learning. When just two observations of labeled data from a new subject are integrated with training data of an existing SLR system, average classification accuracy for TrAdaboost, TrResampling, TrBagg and RF are 71.07%, 72.92%, 76.10% and 76.79%, respectively. However, for the same number of labelled data from the new subject, Trbaggboost yields an average classification accuracy of 80.44%, indicating the effectiveness of the algorithm. Moreover, the classification accuracy for Trbaggboost improves up to 97.04% as the number of labelled data from the new user increase.																	1868-5137	1868-5145															10.1007/s12652-020-01979-z		MAY 2020											
J								Investigating the effect of value stream mapping on procurement effectiveness: a case study	JOURNAL OF INTELLIGENT MANUFACTURING										Supply chain management; Value chain; Case study; Lean management; Survey studies	SUPPLY MANAGEMENT; PURCHASING PROCESS; GREEN PROCUREMENT; DESIGN; PERFORMANCE; IMPROVEMENT; SIMULATION; LOGISTICS; SERVICES; SYSTEMS	In order to find a new lean methodology to improve the process of procurement management, this paper takes the warehouse in the manufacturing enterprise as a case to study. During lean consulting service provision, the firsthand material which the research work needs through the spot inspection was collected. To analysis the whole procurement process, the procurement value stream map based on the data collected was plot. Then the whole procurement process can be seen in the procurement value stream map, including making procurement plans, audit, procurement, transportation, inspection, storage, classifying and receiving materials. According to the procurement value stream map, wastes and non-value activities in the procurement process can be easily found and the improvement methods can be prompted. The contribution of this paper main reflects in two aspects: (1) the procurement value stream mapping (P-VSM) enriches improvement methodology of procurement management; (2) the P-VSM methodology enlarges the application scope of lean tools on one side.																	0956-5515	1572-8145															10.1007/s10845-020-01594-x		MAY 2020											
J								Sequential graph-based routing algorithm for electrical harnesses, tubes, and hoses in a commercial vehicle	JOURNAL OF INTELLIGENT MANUFACTURING										Commercial vehicle; Dijkstra's algorithm; Minimal spanning tree; Pipe routing algorithm; Routing design methodology	ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; PIPE; SYSTEM; MODEL; NETWORKS; LAYOUT	The routing design of the various electrical wires, tubes, and hoses of a commercial vehicle requires a significant number of man-hours because of the variety of the commercial vehicles, frequent design changes of other vehicular components and the manual trial-and-error approaches. This study proposes a new graph-based routing algorithm to find the collision-free routing path in the constrained space of a commercial vehicle. Minimal spanning tree is adopted to connect multi-terminal points in a graph and Dijkstra's algorithm is used to find the shortest route among the candidate paths; the design domain is divided into several sub-domains to simplify the graph and the proposed algorithm solves the routing problems in a sequential manner to deal intermediate points. Then, the proposed method was applied to the design of the routes for four different routing components of a commercial truck. The results indicate that the developed methodology can provide a satisfactory routing design satisfying all the requirements of the design experts in the automotive industry.																	0956-5515	1572-8145															10.1007/s10845-020-01596-9		MAY 2020											
J								Reinforcement learning-based collision-free path planner for redundant robot in narrow duct	JOURNAL OF INTELLIGENT MANUFACTURING										Path planning; Obstacle avoidance; Self-motion; Reinforcement learning	CONFIGURATION; MANIPULATORS; AVOIDANCE; ALGORITHM	Compared with obstacle avoidance in open environment, collision-free path planning for duct-enter task is often challenged by narrow and complex space inside ducts. For obstacle avoidance, redundant robot is usually applied for this task. The motion of redundant robot can be decoupled to end-effector motion and self-motion. Current methods for duct-enter task are not robust due to the difficulty to properly define the self-motion. This difficulty mainly comes from two aspects: the definition of distances from robot to obstacles and the fusion of multiple data. In this work, we adapt the ideas underlying the success of human to handling this kind tasks, variable optimization strategies and learning, for one robust path planner. Proposed planner applies reinforcement learning skills to learn proper self-motion and achieves robust planning. For achieving robust behavior, state-action planner is creatively designed with three especially designed strategies. Firstly, optimization function, the kernel part of self-motion, is considered as part of action. Instead of taking every joint motion, this strategy embeds reinforcement learning skills on self-motion, reducing the search domain to null space of redundant robot. Secondly, robot end orientation is taken into action. For duct-enter task, robot end link is the motion starter for exploring movement just like the snake head. The orientation of robot end link when passing through some position can be referred by following links. Hence the second strategy can accelerate exploring by reduce the null space to possible redundant robot manifold. Thirdly, path guide point is also added into action part. This strategy can divide one long distance task into several short distance tasks, reducing the task difficulty. After these creative designs, the planner has been trained with reinforcement learning skills. With the feedback of robot and environment state, proposed planner can choose proper optimization strategies, just like the human brain, for avoiding collision between robot body and target duct. Compared with two general methods, Virtual Axis method with orientation Guidance and Virtual Axis, experiment results show that the success rate is separately improved by 5.9% and 49.7%. And two different situation experiments are carried out on proposed planner. Proposed planner achieves 100% success rate in the situation with constant start point and achieves 98.7% success rate in the situation with random start point meaning that the proposed planner can handle the perturbation of start point and goal point. The experiments proves the robustness of proposed planner.																	0956-5515	1572-8145															10.1007/s10845-020-01582-1		MAY 2020											
J								Online Bayesian shrinkage regression	NEURAL COMPUTING & APPLICATIONS										Regression; Regularisation; Online learning; Competitive analysis		The present work introduces an original and new online regression method that extends the shrinkage via limit of Gibbs sampler (SLOG) in the context of online learning. In particular, we theoretically show how the proposed online SLOG (OSLOG) is obtained using the Bayesian framework without resorting to the Gibbs sampler or considering a hierarchical representation. Moreover, in order to define the performance guarantee of OSLOG, we derive an upper bound on the cumulative squared loss. It is the only online regression algorithm with sparsity that gives logarithmic regret. Furthermore, we do an empirical comparison with two state-of-the-art algorithms to illustrate the performance of OSLOG relying on three aspects: normality, sparsity and multicollinearity showing an excellent achievement of trade-off between these properties.																	0941-0643	1433-3058															10.1007/s00521-020-04947-y		MAY 2020											
J								Fuzzy-aided solution for out-of-view challenge in visual tracking under IoT-assisted complex environment	NEURAL COMPUTING & APPLICATIONS										Visual tracking; Fuzzy aided; Out of view; Complex environment; Template update; Internet of things		With the rapid development in computer vision domain, research on object tracking has directed more attention by scholars. Out of view (OV) is an important challenge often encountered in the tracking process of objects, especially in Internet of Things surveillance. Therefore, this paper proposes a fuzzy-aided solution for OV challenge. This solution uses a fuzzy-aided system to detect whether the target is poorly tracked by using the response matrix of samples. When poor tracking occurs, the target is relocated according to the stored template. The proposed solution is tested on OTB100 dataset, where the experimental results show that the auxiliary solution is effective for the OV challenge. The proposed solution also ensures the tracking speed and overall success rate of visual tracking as well as improves the robustness to a certain extent for IoT-assisted complex environment.																	0941-0643	1433-3058															10.1007/s00521-020-05021-3		MAY 2020											
J								Optimization analysis of sport pattern driven by machine learning and multi-agent	NEURAL COMPUTING & APPLICATIONS										Machine learning; Intelligent Sports; Simulation; Control; Robot		The intelligent simulation of Sports can match the actual game and is of great significance to the development of Sports. Sports is a system in which multiple agents work together. Compared with a single agent, the learning space of multiple agents increases sharply as the number of agents increases, so the learning difficulty increases. Therefore, based on machine learning technology, this study combines with the actual situation to build a Sports simulation system. Moreover, after establishing a more reasonable team defensive formation and strategy, the overall movement of the agent is optimized, and the corresponding structural model has been established in combination with various actions. In addition, this study designs a controlled trial to analyze the performance of the model. The research shows that the proposed method has certain effects and can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-05022-2		MAY 2020											
J								Design of area efficient VLSI architecture for carry select adder using logic optimization technique	COMPUTATIONAL INTELLIGENCE										area efficient; CSLA adder; VLSI architectures	HIGH-SPEED	Square Root Carry Select Adder (SQRT-CSLA) is accomplishing the noteworthy attention in the arena of VLSI (Very-Large-Scale Integration) systems as it can process the computations with high speed. Though the current trending SQRT-CSLA adder designing techniques are effective in various performance metrics, there is a possibility to improve the design addressing various performance metrics. This article proposes CSLA architecture by employing Zero Finding Logic using the Logic optimization technique (ZFCLOT). CSLA using ZFCLOT is designed, simulated, and synthesized using 90 nm cadence tools. CSLA using ZFCLOT achieves an area efficiency of 46.127% and a power efficiency of 48.4% as against to SQRT-CSLA.																	0824-7935	1467-8640															10.1111/coin.12347		MAY 2020											
J								Bio-PUF-MAC authenticated encryption for iris biometrics	COMPUTATIONAL INTELLIGENCE										Message Authentication Codes; PUF; Secure lightweight Encryption; True random number generation	FINGERPRINT	Biometrics is one of the ways for human authentication. Fabrication of biometrics by intruders, limits the accuracy of authentication. The user-specific keys (ie,) pseudo-random numbers give more security for biometric template protection and increase the accuracy of authentication also. The user-specific token or keys can also be fabricated by intruders by any of the prediction methods. To avoid the creation of fake biometric and fake user-specific keys, a device-specific Physical Unclonable Function (PUF) is proposed. In this article, iris authentication is provided by unclonable PUF-based true random numbers to enhance the unique authentication. Nonreversible Message Authentication Codes (MAC) are developed using PUF and Discrete Wavelet Transform features of iris biometrics. Systematically, MAC codes also created with, encryption algorithm. Encryption is additionally providing confidentiality in the individual iris. Experiments are done with CUHK Iris Image Dataset. Proposed Bio-PUF system has significant functional advantages in point of view of the unclonable pseudo-random number from PUF. Experimentally, Avalanche effect, entropy, NCPR, and UACI parameters are analyzed with PUF-based crypt functions. For 75% of matching with the Bio-PUF-MAC codes with enrolment, the accuracy for correct identification is 77.73%.																	0824-7935	1467-8640				AUG	2020	36	3					1221	1241		10.1111/coin.12332		MAY 2020											
J								Traffic wave model based on vehicle-infrastructure cooperative and vehicle communication data	COMPUTATIONAL INTELLIGENCE										kinematics equation; on-board internal communication system; traffic flow; traffic wave; vehicle-infrastructure cooperative	GO WAVES; FLOW; TECHNOLOGIES; SYSTEMS	Urban trunk road system undertakes the main traffic trip, and congestion occurs frequently in rush hours. In order to clearly describe the propagation process of traffic waves in signalized intersections, and then optimize phase difference. This article proposes a kinematic model for the traffic wave based on the physical mechanism of car-following and the kinematic characteristics of the traffic wave propagation. The actual road traffic monitoring data was extracted from the vehicle-infrastructure cooperative system and vehicle internal communication system. Then we obtained the values of the stop-and-start wave velocity. Compared with the measured data, the results showed that the calculation of the wave velocity of the traffic wave model had a relative error of up to 5% vs the measured data, confirming the validity of the model. Through the analysis of the model, we obtained the difference in the effects on traffic wave velocity of the vehicle speed and the space headway. Our findings provide a theoretical basis for coordinated control and management of urban trunk road traffic and phase difference optimization of signalized intersections. Meanwhile, the research results also provide a theoretical basis for alleviating traffic congestion during the rush hour.																	0824-7935	1467-8640															10.1111/coin.12346		MAY 2020											
J								Topology-based generation of sport training sessions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimization; Topology; Sport training sessions; Metaheuristics		Recently, sports training sessions have been generated automatically according to the TRIMP load quantifier that can be calculated easily using data obtained from mobile devices worn by an athlete during the session. This paper focuses on generating a sport training session in cycling, and bases on data obtained from power-meters that, nowadays, present unavoidable tools for cyclists. In line with this, the TSS load quantifier, based on power-meter data, was applied, while the training plan was constructed from a topology of already realized training sessions represented as a topological graph, where the edges in the graph are equipped with the real length, absolute ascent and average power needed for overcoming the path between incident nodes. The problem is defined as an optimization, where the optimal path between two user selected nodes is searched for, and solved with an Evolutionary Algorithm using variable length representation of individuals, an evaluation function inspired by the TSS quantifier, while the variation operators must be adjusted to work with the representation. The results, performed on an archive of sports training sessions by an amateur cyclist showed the suitability of the method also in practice.																	1868-5137	1868-5145															10.1007/s12652-020-02048-1		MAY 2020											
J								Analysis of microtomographic images in automatic defect localization and detection	MACHINE VISION AND APPLICATIONS										Image analysis; Image processing; Neural networks; Segmentation; mu-CT microtomography; X-CT computed tomography; X-ray	RAY COMPUTED-TOMOGRAPHY; FATIGUE	The paper presents a fast method of fully automatic localization and classification of defects in aluminium castings based on computed microtomography images. In the light of current research and based on available publications, where such analysis is made on the basis of images obtained from standard radiography (x-ray), this is a new approach which uses microtomographic images (mu-CT). In addition, the above-mentioned solutions most often analyze a pre-separated portion of an image, which requires the initial operator interference. The authors' own pre-processing methods, which allow to separate the element area and potential defect areas from mu-CT images, and methods of extraction of selected features describing these areas have been proposed in the solution discussed here. A neural network trained using the Levenberg-Marquardt method with error backpropagation has been used as a classifier. The optimal network structure 20-4-1 and a set of 20 features describing the analysed areas have been determined as a result of performed tests. The applied solutions have provided 89% correct detection for any defect size and 96.73% for large defects, which is comparable to the results obtained from methods using x-ray images. This has confirmed that it is possible to use mu-CT images in automatic defect localization in 3D. Thanks to this method, quantitative analysis of aluminium castings can be carried out without user interaction and fully automated.																	0932-8092	1432-1769				MAY 27	2020	31	5							35	10.1007/s00138-020-01084-3													
J								Fast and Memory-Efficient Import Vector Domain Description	NEURAL PROCESSING LETTERS										One-class learning; Import vector domain description; Nystrom method; Kernel methods		One-class learning is a classical and hard computational intelligence task. In the literature, there are some effective and powerful solutions to address the problem. There are examples in the kernel machines realm, Support Vector Domain Description, and the recently proposed Import Vector Domain Description (IVDD), which directly delivers the sample probability of belonging to the class. Here, we propose and discuss two optimization techniques for IVDD to significantly improve the memory footprint and consequently to scale to datasets that are larger than the original formulation. We propose two strategies. First, we propose using random features to approximate the gaussian kernel together with a primal optimization algorithm. Second, we propose a Nystrom-like approximation of the functional together with a fast converging and accurate self-consistent algorithm. In particular, we replace the a posteriori sparsity of the original optimization method of IVDD by randomly selecting a priori landmark samples in the dataset. We find this second approximation to be superior. Compared to the original IVDD with the RBF kernel, it achieves high accuracy, is much faster, and grants huge memory savings.																	1370-4621	1573-773X															10.1007/s11063-020-10243-6		MAY 2020											
J								Global Exponential Stability of Hybrid Non-autonomous Neural Networks with Markovian Switching	NEURAL PROCESSING LETTERS										Global exponential stability; Hybrid non-autonomous neural networks (HNNNs); Pulse delay; Markovian switching; Halanay inequality	TIME-VARYING DELAYS; SYNCHRONIZATION; PARAMETERS	This paper discusses the global exponential stability for a class of hybrid non-autonomous neural networks (HNNNs) with Markovian switching, which includes the factors of time delays and impulse disturbance. A novel Halanay inequality with cross terms is established by using stochastic analysis technique. Some sufficiency criteria for the global exponential stability of the HNNNs with Markovian switching are derived by the Halanay inequality and some mathematical analysis methods. The results obtained have better fault tolerance and redundancy under certain accuracy than the existing results in the literature. Finally, numerical experiments are provided to illustrate our theoretical results.																	1370-4621	1573-773X															10.1007/s11063-020-10262-3		MAY 2020											
J								Distance-guided local search	JOURNAL OF HEURISTICS										Meta-heuristic methodologies; Local search; Distance between solutions; Intensification	ALGORITHM; SCATTER	We present several techniques that use distances between candidate solutions to achieve intensification in Local Search (LS) algorithms. An important drawback of classical LS is that after visiting a very high-quality solution the search process can "forget about it" and continue towards very different areas. We propose a method that works on top of a given LS to equip it with a form of memory so as to record the highest-quality visited areas (spheres). More exactly, this new method uses distances between candidate solutions to perform a coarse-grained recording of the LS trajectory,i.e., it records a number of discovered spheres. The (centers of the) spheres are kept sorted in a priority queue in which new centers are continually inserted as in insertion-sort algorithms. After thoroughly investigating a sphere, the proposed method resumes the search from the first best sphere center in the priority queue. The resulting LS trajectory is no longer a continuous path, but a tree-like structure, with closed branches (already investigated spheres) and open branches (as-yet-unexplored spheres). We also explore several other techniques relying on distances,e.g., in Section 2.3, we show how to use distance information to prevent the search from looping indefinitely on large (quasi-)plateaus. Experiments on three problems based on different encodings (partitions, vectors and permutations) confirm the intensification potential of the proposed ideas.																	1381-1231	1572-9397				OCT	2020	26	5					711	741		10.1007/s10732-020-09446-w		MAY 2020											
J								Bifurcation analysis for energy transport system and its optimal control using parameter self-tuning law	SOFT COMPUTING										Nonlinear dynamical system; Bifurcation theory; Normal form theory; Optimal control; Stability	TAKENS-BOGDANOV BIFURCATIONS; HOPF-BIFURCATION; ADAPTIVE-CONTROL; PERIODIC-ORBITS; MODEL; ASSIGNMENT	A dynamical system for optimal path of energy and resources between two cities in China is considered in this paper. We have discussed dynamics of variables and parameters involved in mentioned system. Bifurcation analysis around non-hyperbolic equilibria is also explained for codimension 1 and 2 bifurcations. Furthermore, double-zero eigenvalue condition is calculated for the proposed model. We have adopted methodology of the generalized vectors for existence of Bogdanov-Takens bifurcation critical point and used analytical computations instead of center manifold theorem for Bogdanov-Takens bifurcation around zero equilibria. Further, with the aid of bifurcation diagram, phase portraits and time history, we discussed occurrence of period doubling, Hopf bifurcation and chaotic region of our proposed model. Based on Lyapunov function and robust control, optimal controllers are designed using Hamilton-Jacobi theorem for the stability of disturbance and aperiodic solution in optimal transportation system (3) due to energy imports from cityAto cityB.																	1432-7643	1433-7479				NOV	2020	24	22					17221	17231		10.1007/s00500-020-05014-3		MAY 2020											
J								Modelling drugs interaction in treatment-experienced patients on antiretroviral therapy	SOFT COMPUTING										Antiretroviral therapy; Drugs interaction; Least squares auto-tuning; Patient response modelling; Personalised medicine; Soft computing	FUZZY-LOGIC SYSTEMS; PREDICTION; CYP3A4; ALGORITHMS; COUNTS	Understanding pharmacology and drug resistance patterns plus appropriate use of laboratory testing is vital for managing treatment-experienced patients with new agents. While we acknowledge that patients with extensive drug resistance now have multiple options for suppressive therapy, and expert care is essential to avoid the rapid emergence of resistance to these new agents, clinicians are unaware of the inherent (hidden) patterns created by combined drug regimens that could trigger adverse drug reactions. This paper proposes a novel hybrid system framework that combines soft computing techniques, for drugs interaction modelling and precise patient response optimisation. A Fuzzy Logic system was developed to address the uncertainty in treatment change episodes (TCEs). A weighted least-squares cost function was then employed to auto-tune hyperparameters for training the neural network. After acceptable tuning, the final hyperparameters served the neural network-to efficiently learn the ensuing patterns for precice drug interaction classification. The proposed framework was experimented with clinical data of TCEs from two disparate sources: a publicly available HIV database (the Stanford HIV database:), and clinical data collected from 13 health centers managing HIV cases in Akwa Ibom State of Nigeria (the Akwa-Ibom HIV database). In both databases, a correlation of prognostic markers suggests strong association between first line CD4 and follow-up CD4 counts; while a moderately weak association was observed for first line and follow-up viral loads. Correlation of physiological feature gave very strong association between first line and follow-up body mass index in Akwa-Ibom database. Analysis of the patients progress explains the decreased potency of CD4 count and body mass index as HIV predictors. The root mean square error (RMSE) and classification accuracy were used as performance metrics for measuring the precision of our hybrid framework. Results obtained showed improved RMSE and classification accuracy for both databases, when compared with existing works.																	1432-7643	1433-7479				NOV	2020	24	22					17349	17364		10.1007/s00500-020-05024-1		MAY 2020											
J								A new hybrid optimization method combining moth-flame optimization and teaching-learning-based optimization algorithms for visual tracking	SOFT COMPUTING										Optimization; Moth-flame optimization; MFO-TLBO; Visual tracking; Population-based algorithm; Teaching-learning-based optimization	DIFFERENTIAL EVOLUTION	In this paper, a new hybrid algorithm based on moth-flame optimization (MFO) and teaching-learning-based optimization (TLBO) algorithm named as MFO-TLBO is proposed to overwhelm their shortcomings and inherit their advantages using the low-level coevolutionary mixed hybrid. In the best interests of this, we progress the competence of exploitation in TLBO with the ability of exploration in the MFO algorithm to demonstrate the metiers of both methods. The sole inspiration behind integrating modifications in MFO is to benefit the procedure to avoid immature convergence and to steer the search in the direction of the potential search region in a quicker way. The proposed algorithm was tested on the set of best known unimodal and multimodal benchmark functions in various dimensions. The obtained results from basic and nonparametric statistical tests confirmed that this hybrid method dominates in terms of convergence and success rate. Furthermore, MFO-TLBO is applied to visual tracking as a real-life application. All experimental outcomes, illustrations and comparative investigation found that the MFO-TLBO algorithm can vigorously track a random target object in many stimulating circumstances than the other trackers successfully.																	1432-7643	1433-7479															10.1007/s00500-020-05032-1		MAY 2020											
J								A two-stage density clustering algorithm	SOFT COMPUTING										Clustering; Density peak; Efficiency		Clustering by fast search and find of density peaks (CFDP) is a popular density-based algorithm. However, it is criticized because it is inefficient and applicable only to some types of data, and requires the manual setting of the key parameter. In this paper, we propose the two-stage density clustering algorithm, which takes advantage of granular computing to address the aforementioned issues. The new algorithm is highly efficient, adaptive to various types of data, and requires minimal parameter setting. The first stage uses the two-round-means algorithm to obtain root n small blocks, where n is the number of instances. This stage decreases the data size directly from n to root n. The second stage constructs the master tree and obtains the final blocks. This stage borrows the structure of CFDP, while the cutoff distance parameter is not required. The time complexity of the algorithm is O(mn(3/2)), which is lower than O(mn(2)) for CFDP. We report the results of some experiments performed on 21 datasets from various domains to compare a new clustering algorithm with some state-of-the-art clustering algorithms. The results demonstrated that the new algorithm is adaptive to different types of datasets. It is two or more orders of magnitude faster than CFDP.																	1432-7643	1433-7479															10.1007/s00500-020-05028-x		MAY 2020											
J								Multiple-strategy learning particle swarm optimization for large-scale optimization problems	COMPLEX & INTELLIGENT SYSTEMS										Large-scale optimization; Multiple-strategy learning particle swarm optimization; Two-stage searching mechanism	GENETIC ALGORITHM; COOPERATIVE COEVOLUTION; DIFFERENTIAL EVOLUTION; SCHEME; TIME	The balance between the exploration and the exploitation plays a significant role in the meta-heuristic algorithms, especially when they are used to solve large-scale optimization problems. In this paper, we propose a multiple-strategy learning particle swarm optimization algorithm, called MSL-PSO, to solve problems with large-scale variables, in which different learning strategies are utilized in different stages. At the first stage, each individual tries to probe some positions by learning from the demonstrators who have better performance on the fitness value and the mean position of the population. All the best probed positions, each of which has the best fitness among all positions probed by its corresponding individual, will compose a new temporary population. The temporary population will be sorted on the fitness values in a descending order, and will be used for each individual to find its demonstrators, which is based on the rank of the best probed solution in the temporary population and the rank of the individual in the current population, to learn using a new strategy in the second stage. The first stage is used to improve the exploration capability, and the second one is expected to balance the convergence and diversity of the population. To verify the effectiveness of MSL-PSO for solving large-scale optimization problems, some empirical experiments are conducted, which include CEC2008 problems with 100, 500, and 1000 dimensions, and CEC2010 problems with 1000 dimensions. Experimental results show that our proposed MSL-PSO is competitive or has a better performance compared with ten state-of-the-art algorithms.																	2199-4536	2198-6053															10.1007/s40747-020-00148-1		MAY 2020											
J								Image real-time augmented reality technology based on spatial color and depth consistency	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image color; Depth calculation; Stereo vision matching; Augmented reality	ENVIRONMENT; IMPROVE; STUDENT; DISPLAY	Augmented reality can enhance people's perception of the environment by embedding virtual objects or other information in real-time images. In this paper, the color image is used as a reference to calculate the confidence of the original depth map, and stereo matching is performed according to the feature points. The depth map is mainly enhanced by the color, edge, and segmentation results of the color image. A deep computing system based on augmented reality is designed. The system can use a binocular camera to collect object images in real time and obtain better parallax images by correcting the calibrated image. The semi-global block matching algorithm and depth calculation are used to realize the tracking registration of virtual objects. Experiments in different environments show that the system has good real-time performance, light invariance, and depth consistency.																	1861-8200	1861-8219															10.1007/s11554-020-00988-7		MAY 2020											
J								Using machine learning techniques for DSP software performance prediction at source code level	CONNECTION SCIENCE										Performance prediction; source code level; machine learning	REGRESSION; ALGORITHM	Efficient performance prediction at the source code level is essential in reducing the turnaround time of software development. In this paper, we introduce a new prediction model, which combines several machine learning algorithms, such as KNN, clustering, similarity, sample and attribute weighting with multiple linear regression techniques, to predict the execution time of Digital Signal Processing (DSP) software at the source code level. Prediction at source code level tends to both under-predict the performance for certain testing samples and over-predict for some other samples. Therefore, we propose a new algorithm called MAX/MIN algorithm to select the best-predicted execution time. To validate the new model, we measure experimentally the execution time of a set of functions selected from PHY DSP Benchmark and run them on TIC64 DSP processor. It is observed that the average absolute relative prediction error is less than 10% between the computed performance from the new model and the actual measured execution time.																	0954-0091	1360-0494															10.1080/09540091.2020.1762542		MAY 2020											
J								A secure image watermarking for tamper detection and localization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image watermarking; Tamper detection; Tamper localization; Copy-paste attack; Copy-move forgery; Constant average attack	FRAGILE WATERMARKING; MEDICAL IMAGES; REVERSIBLE WATERMARKING; ROBUST; AUTHENTICATION; SCHEME	Data security has become a major concern of present time. Digital image watermarking is seen as a viable solution to ensure the integrity of image data. Watermarking algorithms can detect the presence of various attacks on images. In this work, a secure and efficient fragile image watermarking technique is proposed. The proposed watermarking algorithm aims to find image tampering and its location. The proposed approch is a spatial domain, block-based embedding technique. It introduces a unique key based embedding using SHA-1 (Secure Hashing Algorithm) hashing. The watermarked images have high PSNR and SSIM. The algorithm can handle various size of tampering, from very small to large. The proposed technique has been tested for a wide variety of tampering attacks like copy-paste, copy-move, constant average, and general tampering. It can efficiently detect tampering in the presence of these attacks.																	1868-5137	1868-5145															10.1007/s12652-020-02135-3		MAY 2020											
J								Using model's temporal features and hierarchical structure for similar activity recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activity recognition; Similar activity; Hierarchical structure; Markov logic network	SEGMENTATION; AMBIENT; SERIES	Using Sensor-based approach in activity recognition usually requires the deployment of many ambient sensors to objects and environments. Each sensor can be triggered by more than one activity, e.g., a touch sensor of a cooker can be triggered by cooking, doing dish and so on. An activity consists of some sensor events. When the number of same sensors are in the majority of two activities, the two activities are defined as similar activities which are difficult to distinguish. To address the challenge of recognizing similar activities, this paper conceives a new activity recognition approach incorporating high-dimensional features of duration and time block characteristics to improve the inference performance. In a further step, we take advantage of these similar activities to build a hierarchical structure model which can improve capacities of expandability and standardization. We design experiments of similar activity in our daily life to evaluate this solution. The results show that high-dimensional temporal features improved similar activity inference accuracy on an average of 1.88 times, and the use of hierarchical structure can generalize specific rules to standard ones which decreases similar activity recognition computation time on an average of 0.36 times.																	1868-5137	1868-5145															10.1007/s12652-020-02035-6		MAY 2020											
J								Estimation of loadability limit with N-1 and N-2 outages using evolutionary computation techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Loadability limit; Contingency; FACTS devices; Severity indices; Differential evolution; Modified differential evolution	DIFFERENTIAL EVOLUTION; MAXIMUM LOADABILITY; OPTIMIZATION; SYSTEM	Voltage stability primarily depends on the voltage magnitude, phase angle, real and reactive power constraint of the electric power system. Even during emergencies like contingency (outages), the stability of the electric power structure will be enhanced by improving the Loadability Limit (LL) of the transmission sector. Flexibility in the real and reactive power flow in the transmission system is achieved by the Flexible AC Transmission System (FACTS) devices. These devices can be placed anywhere in the transmission sector. To get effective control over the power flow through the transmission lines and to achieve the maximum loadability with the minimal installation cost, optimal choice and placement of FACTS devices are essential. In this manuscript, efforts had been taken to analyze the LL with outages for hybrid electric power structures. The proposed method is simulated and tested with the hybrid version of standard IEEE 30 bus system. Three types of FACTS devices like Thyristor Controlled Series Capacitor (TCSC), Static VAr Compensator (SVC) and Unified Power Flow Controller (UPFC) are efficiently selected and placed in the transmission lines. For the optimal positioning and placement of these devices, the Contingency Severity Index (CSI) and Fast Voltage Stability Index (FVSI) have been used. Differential Evolution (DE) and Modified Differential Evolution (MDE) algorithms are applied to optimize the obtained results. DE is an evolutionary search based soft computing algorithm popularly handed to resolve multifarious problems. MDE is the enhanced version of DE that embraces a prior knowledge about the solution space at every stage of the search. The main focus of this work is (i) to identify the weak branches and buses in the system using CSI and FVSI. (ii) to optimize the number, location and settings of FACTS devices using various soft computing techniques like DE and MDE. (iii) to evaluate ML of a transmission system with FACTS devices under normal and contingency conditions using DE and MDE. (iv) to calculate the cost required for the installation of FACTS devices. (v) to enhance ML of pool and hybrid model of deregulated electric power market with contingency using the optimal number, rating and positioning of FACTS devices.																	1868-5137	1868-5145															10.1007/s12652-020-02111-x		MAY 2020											
J								Improved diode assisted voltage fed three phase quasi z-source inverter for photovoltaic application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Quasi z-source inverter; PWM control; Switch boost inverter; Fuzzy logic controller	NEURAL-NETWORK APPROACH; BOOST CONTROL; FUZZY	In this paper, we propose a design and simulation of improved diode assisted voltage fed three phase quasi z-source inverter for photovoltaic application. Variable shoot through time interval is generated by adding low frequency voltage with constant voltage and the control signal for variable shoot is generated using boost PWM control. The output quality of three phase QZSI is improved by replacing the diode with active power switch in Switch Boost Inverter (SBI). Additionally, we use fuzzy logic controller for optimizing the shoot through duty ratio and modulation index from the source side rectifier, which increases the gain of three phase QZSI. The proposed topology in photovoltaic application increases the input voltage gain and voltage boost property by reducing the number of components, voltage ratings and stress across the switches in three phase QZSI. The proposed method enables improved voltage gain in single stage conversion, which is used for boost function. The simulation of proposed photovoltaic model is built on Simulink. Performance of proposed is evaluated against various metrics including power consumption, delay and power delay product and it is compared against existing VSI.																	1868-5137	1868-5145															10.1007/s12652-020-02061-4		MAY 2020											
J								Classification of lung cancer stages with machine learning over big data healthcare framework	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Benign; Health care; Machine learning; Malignant; Map-reduce; Sputum	PREDICTION	With the fast pace in collating big data healthcare framework and accurate prediction in detection of lung cancer at early stages, machine learning gives the best of both worlds. In this paper, a streamlining of machine learning algorithms together with apache spark designs an architecture for effective classification of images and stages of lung cancer to the greatest extent. We experiment on a combination of binary classification (SVM-non linear SVM with Radial Basis Function RBF) and Multi-class classification (WTA-SVM winner-takes-all with support vector machine) with threshold technique (T-BMSVM) to classify nodules into malignant or benign nodules and also their malignancy levels respectively. The dataset used for processing is sputum cell images that have been collected from microscope lab images. We have argued for handling and processing large sizes of data sets as sputum cell images in the field of classification using the map-reduce framework in MATLAB and Pyspark, which works better with Apache spark. Our approach outperforms the other methods by achieving stability even in increasing dataset size in leaps and bounds and with a minimum error rate. It achieves 86% accuracy and other metrics are AUC-0.88, misclassification rate through which it was proved that Support Vector Machine (SVM) outperforms other classifiers. These outsourced outcomes reveal that extracting properties of features extracted from the lung cancer images successfully and SVM combined with binary classification, even classification works better with Multi-class rather than SVM, therefore, may be considered as a promising tool to diagnose the stages of nodules and classify the severity of cancer. Also, Scalability and convergence analysis embed to prove the improving results of multi-class classification than SVM.																	1868-5137	1868-5145															10.1007/s12652-020-02071-2		MAY 2020											
J								Multi-period mean-semi-entropy portfolio management with transaction costs and bankruptcy control	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Semi-entropy; Multi-period portfolio selection; Transaction costs; Bankruptcy control	OPTIMIZATION MODELS; EXPECTED VALUE; SELECTION; CONSUMPTION; ALGORITHM; DECISIONS	This study investigates a multi-period portfolio management problem under fuzzy settings. For the first time, the newly proposed semi-entropy in the literature is employed as an efficient downside risk measure for risk control in multi-period portfolio optimization. Fuzzy techniques for financial modeling show advantageous performance when future financial market conditions cannot be effectively detected with only historical data. We describe the assert returns by fuzzy variables. Two realistic constraints of transaction costs and bankruptcy events are taken into consideration in our model formulation of a multi-period mean-semi-entropy optimization program. The formulated program is rewritten as a crisp single-objective nonlinear programming by introducing a risk-aversion factor, and final solution to the program is obtained by using genetic algorithm. For the demonstration of computational results, we provide a numerical example with real-life stock data, which illustrates the main modelling concept and the efficiency of genetic algorithm solving method. Comparative analyses over several baseline models show the advantages of adopting fuzzy semi-entropy as an efficient downside risk measure for multi-period portfolio investment optimization.																	1868-5137	1868-5145															10.1007/s12652-020-02053-4		MAY 2020											
J								A new standardisation and selection framework for real-time image dehazing algorithms from multi-foggy scenes based on fuzzy Delphi and hybrid multi-criteria decision analysis methods	NEURAL COMPUTING & APPLICATIONS										Real-time image dehazing algorithms; Standardisation; Selection; Fuzzy Delphi method; Entropy; VIKOR	FAST SINGLE-IMAGE; PATIENT MONITORING SYSTEMS; EXTENDED VIKOR METHOD; HEALTH-CARE SERVICES; VISIBILITY ENHANCEMENT; TRACKING CHANNELS; SHANNONS ENTROPY; OPEN CHALLENGES; ACUTE-LEUKEMIA; BENCHMARKING	Given the rapid development of dehazing image algorithms, selecting the optimal algorithm based on multiple criteria is crucial in determining the efficiency of an algorithm. However, a sufficient number of criteria must be considered when selecting an algorithm in multiple foggy scenes, including inhomogeneous, homogenous and dark foggy scenes. However, the selection of an optimal real-time image dehazing algorithm based on standardised criteria presents a challenge. According to previous studies, a standardisation and selection framework for real-time image dehazing algorithms based on multi-foggy scenes is not yet available. To address this gap, this study proposes a new standardisation and selection framework based on fuzzy Delphi (FDM) and hybrid multi-criteria analysis methods. Experiments are also conducted in three phases. Firstly, the image dehazing criteria are standardised based on FDM. Secondly, an evaluation experiment is conducted based on standardised criteria and nine real-time image dehazing algorithms to obtain a multi-perspective matrix. Third, entropy and VIKOR methods are hybridised to determine the weight of the standardised criteria and to rank the algorithms. Three rules are applied in the standardisation process to determine the criteria. To objectively validate the selection results, mean is applied for this purpose. The results of this work can be taken into account in designing efficient methods and metrics for image dehazing.																	0941-0643	1433-3058															10.1007/s00521-020-05020-4		MAY 2020											
J								A generative adversarial network with structural enhancement and spectral supplement for pan-sharpening	NEURAL COMPUTING & APPLICATIONS										Pan-sharpening; Generative adversarial network; Enhanced structural information; Spectral supplement	WAVELET TRANSFORM; FUSION; RESOLUTION; IMAGES	Pan-sharpening aims to obtain high-resolution multi-spectral images by fusing panchromatic images and low-resolution multi-spectral images though reasonable rules. This paper proposed a novel generative adversarial network for pan-sharpening, which utilizes the supplemented spectral information from low-resolution multi-spectral images and the enhanced structural information from panchromatic images to generate high-resolution multi-spectral images. Firstly, the forward differential operator is used to extract the spatial structural information of the panchromatic image both in the horizontal and vertical directions. Secondly, an architecture of generative adversarial network is designed. The enhanced structural information generated by the accumulation of the structural information of the two directions is added to the image fusion process in generator and the discriminating process in discriminator, and a new optimization objective is designed accordingly. What is more, the low-resolution multi-spectral image is added to the convolution process in the generator as a supplement to the spectral information. Finally, in order to obtain better image generation effect, a special objective function of the generator is designed, which adds a unique relationship to reduce the loss of spatial structural information and spectral information of fused images. Experiments on QuickBird and WorldView-3 satellites datasets show that the proposed method can generate high quality fused images and is better than most advanced methods in both objective indicators and intuitive observations.																	0941-0643	1433-3058															10.1007/s00521-020-04973-w		MAY 2020											
J								Improving graph-based label propagation algorithm with group partition for fraud detection	APPLIED INTELLIGENCE										Label propagation; Group partition; Semi-supervised; Knowledge graph; Fraud detection; Risk management		Fraudulent user detection is a crucial issue in financial risk management. Due to the lack of labeled data and the reliability of labeling, label propagation algorithms (LPA) are effective solutions in this scenario. Most existing models only propagate the risk probabilities for individual users through feature level, while ignoring the real-world graph structure and the characteristics of gang crime. This paper improves the graph-based LPA through group partition, which can be directly implemented on the graph at hand with full consideration of the group information. The exhaustive experimental results testify the performance of our proposed model KGLPA over other off-the-shelf models and amend the insufficiency of feature-based LPA with higher reliability and stability to improve the detection of fraudulent users and secure the marketing budgets.																	0924-669X	1573-7497				OCT	2020	50	10					3291	3300		10.1007/s10489-020-01724-1		MAY 2020											
J								A novel classification algorithm based on kernelized fuzzy rough sets	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Fuzzy kernel; Fuzzy rough set; Positive region; Classification hyperplane	ATTRIBUTE REDUCTION	Fuzzy kernels are a special kind of kernels which are usually employed to calculate the upper and lower approximations, as well as the positive region in kernelized fuzzy rough sets, and the positive region characterizes the degree of consistency between conditional attributes and decision attributes. When the classification hyperplane exists between two classes of samples, the positive region is transformed into the sum of the distances from the samples to classification hyperplane. The larger the positive region, the higher the degree of consistency. In this paper, we construct a novel model to solve the classification hyperplane from the geometric meaning of the positive region in kernelized fuzzy rough sets. Then, a classification model is developed through maximizing the sum of the distances from the samples to classification hyperplane, and this optimization problem that addresses this objective function is transformed to its dual problem. Experimental results show that the proposed classification algorithm is effective.																	1868-8071	1868-808X				NOV	2020	11	11					2565	2572		10.1007/s13042-020-01142-2		MAY 2020											
J								Developing a novel inverse data envelopment analysis (DEA) model for evaluating after-sales units	EXPERT SYSTEMS										data envelopment analysis; DEA; after-sales service; DEA; inverse DEA; SBM; slacks-based measure	SLACKS-BASED MEASURE; EFFICIENCY	This paper proposes a novel model of inverse data envelopment analysis (IDEA) based on the slack-based measure (SBM) approach. The developed inverse SBM model can maintain relative efficiency of decision making units (DMUs) with new input and output. This model can also measure the input and output volumes when a decision maker (DM) increases efficiency score. The inverse SBM model is a kind of multi-objective non-linear programming (MONLP) problem, which is not easy to solve. Therefore, we suggest a linear programming model for solving inverse SBM model. In this model efficiency score of DMU under evaluation remains unchanged. Furthermore, we suggest an optimal combination of inputs and outputs in the production possibility set (PPS). A case study is presented to demonstrate the efficacy of our proposed model.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12579	10.1111/exsy.12579		MAY 2020											
J								GRATIS: GeneRAting TIme Series with diverse and controllable characteristics	STATISTICAL ANALYSIS AND DATA MINING										mixture autoregressive models; simulation; time series features; time series forecasting; time series generation	SMOOTH MIXTURES; SELECTION; APPROXIMATION; MODELS	The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We simulate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application.																	1932-1864	1932-1872				AUG	2020	13	4					354	376		10.1002/sam.11461		MAY 2020											
J								Applications of probabilistic hesitant fuzzy rough set in decision support system	SOFT COMPUTING										Probabilistic set; Hesitant fuzzy set; Rough set; Probabilistic hesitant fuzzy rough set; Decision-making technique	AGGREGATION OPERATORS; EXTENSION	The objective of this manuscript is to present the notion of probabilistic hesitant fuzzy rough (PHFR) set and their basic operations. As a generalization of the sets, PHFR set is a more profitable way to express the uncertainties in the data. For it, firstly, we define the basic operational laws like, the union, intersection and the composition of probabilistic hesitant fuzzy approximation spaces with some basic properties are discussed in details. Secondly, presented the novel decision-making technique based on the PHFR sets over two nonempty fixed sets to deal with uncertainty in decision-making problems. Finally, two numerical examples are provided with some comparative study to validate the proposed approach.																	1432-7643	1433-7479				NOV	2020	24	22					16759	16774		10.1007/s00500-020-04971-z		MAY 2020											
J								Big healthcare data for Trivial client having Novel Smart Attire (NSA)	SOFT COMPUTING										Big Data; Trivial client; Full table scan; HDFS; Lucene; Map reduce; Novel Smart Attire (NSA)	CHRONIC HEART-FAILURE	Big Data and Big Data technologies are changing the world. Healthcare is no exemption. Hospitals need to face and solve Big Data problems including collecting, processing, storing, analysing and retrieving the real-time and accumulated historical healthcare data. Big Data technologies will benefit medicine by precise diagnosis, correct treatment decisions and individualized medicine prescriptions, effective prevention planning for avoiding preventable deaths, feasible clinical trial testing outcomes or conclusions drawn on a specific medical drug for a disease, faster discovery of the root-causes and cures of many diseases such as the variety of cancers and age-related diseases; and timely prediction of disease epidemics. Big Data in healthcare emerges from the large electronic health datasets. These datasets are very difficult to manage with conventional hardware and software. In this research proposal, the emotions of the patients are monitored continuously by using Smart Attire, which collects the data and transmits for further processing and actions. The ordering system, named as Trivial client, is proposed and expanded to improve the execution of the records, that are built to hits ratio, by empowering the bigger list quality space that in class ordering arrangements. This facilitates efficient and high-throughput image processing with parallel programs typically executed on a cluster. It provides a solution for how to store a large collection of images on the Hadoop Distributed File System and make them available for efficient distributed processing.																	1432-7643	1433-7479															10.1007/s00500-020-05044-x		MAY 2020											
J								Picky losers and carefree winners prevail in collective risk dilemmas with partner selection	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Cooperation; Collective risk dilemma; Public goods game; Game theory; Partner selection; Human-robot interaction; Complex systems	EVOLUTIONARY GAME-THEORY; SOCIAL DILEMMAS; COOPERATION; DYNAMICS; COORDINATION; FAIRNESS; HUNT	Understanding how to design agents that sustain cooperation in multi-agent systems has been a long-lasting goal in distributed artificial intelligence. Proposed solutions rely on identifying free-riders and avoiding cooperating or interacting with them. These mechanisms of social control are traditionally studied in games with linear and deterministic payoffs, such as the prisoner's dilemma or the public goods game. In reality, however, agents often face dilemmas in which payoffs are uncertain and non-linear, as collective success requires a minimum number of cooperators. The collective risk dilemma (CRD) is one of these games, and it is unclear whether the known mechanisms of cooperation remain effective in this case. Here we study the emergence of cooperation in CRD through partner-based selection. First, we discuss an experiment in which groups of humans and robots play a CRD. This experiment suggests that people only prefer cooperative partners when they lose a previous game (i.e., when collective success was not previously achieved). Secondly, we develop an evolutionary game theoretical model pointing out the evolutionary advantages of preferring cooperative partners only when a previous game was lost. We show that this strategy constitutes a favorable balance between strictness (only interact with cooperators) and softness (cooperate and interact with everyone), thus suggesting a new way of designing agents that promote cooperation in CRD. We confirm these theoretical results through computer simulations considering a more complex strategy space. Third, resorting to online human-agent experiments, we observe that participants are more likely to accept playing in a group with one defector when they won in a previous CRD, when compared to participants that lost the game. These empirical results provide additional support to the human predisposition to use outcome-based partner selection strategies in human-agent interactions.																	1387-2532	1573-7454				MAY 25	2020	34	2							40	10.1007/s10458-020-09463-w													
J								Self-adaptive learning for hybrid genetic algorithms	EVOLUTIONARY INTELLIGENCE										Hybrid genetic algorithms; Evolution strategies; Learning strategies; Self-adaptive learning; Reinforcement learning; Memetic algorithms; Metaheuristics; Baldwinism; Lamarckism	MEMETIC ALGORITHMS; PERFORMANCE	Local search can be introduced into genetic algorithms to create a hybrid, but any improvement in performance is dependent on the learning mechanism. In the Lamarckian model, a candidate solution is replaced by a fitter neighbour if one is found by local search. In the Baldwinian model, the original solution is retained but with an upgraded fitness if a fitter solution is found in the local search space. The effectiveness of using either model or a variable proportion of the two within a hybrid genetic algorithm is affected by the topology of the fitness function and the details of the hybrid algorithm. This paper investigates an intelligent adaptive approach to decide on the learning mechanism to be used by an individual over the course of the search. Evolution is used to self-adapt both the frequency of a steepest-descent local search and the relative proportions of Lamarckian and Baldwinian inheritance. Experiments have shown that this form of adaptive learning can improve the ability to find high-quality solutions and can accelerate the hybrid search without the need to find optimal control parameters for the learning process.																	1864-5909	1864-5917															10.1007/s12065-020-00425-5		MAY 2020											
J								Automatic tuning of hyperparameters using Bayesian optimization	EVOLVING SYSTEMS										Hyperparameters; Optimization; CIFAR-10; Black box function		Deep learning is a field in artificial intelligence that works well in computer vision, natural language processing and audio recognition. Deep neural network architectures has number of layers to conceive the features well, by itself. The hyperparameter tuning plays a major role in every dataset which has major effect in the performance of the training model. Due to the large dimensionality of data it is impossible to tune the parameters by human expertise. In this paper, we have used the CIFAR-10 Dataset and applied the Bayesian hyperparameter optimization algorithm to enhance the performance of the model. Bayesian optimization can be used for any noisy black box function for hyperparameter tuning. In this work Bayesian optimization clearly obtains optimized values for all hyperparameters which saves time and improves performance. The results also show that the error has been reduced in graphical processing unit than in CPU by 6.2% in the validation. Achieving global optimization in the trained model helps transfer learning across domains as well.																	1868-6478	1868-6486															10.1007/s12530-020-09345-2		MAY 2020											
J								The Medical Treatment Service Matching Based on the Probabilistic Linguistic Term Sets with Unknown Attribute Weights	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multiple-attribute two-sided matching; Probabilistic linguistic term sets; Decision-making trial and evaluation laboratory; Prospect theory; Multi-attributive border approximation area comparison	ANALYTIC HIERARCHY PROCESS; GROUP DECISION-MAKING; CONSENSUS PROCESS; PROSPECT-THEORY; DEMATEL METHOD; SELECTION; MODEL; ALGORITHM; SYSTEMS; REGRET	In multi-attribute two-sided matching (MATSM) problems, the attribute weights play an important role. The existing methods usually neglect the interaction and the effect among multiple attributes, resulting in irrational matching results. This paper takes this interaction into consideration. With the complexity of the matching environment, the uncertainties of agents should be considered. The probabilistic linguistic term set (PLTS) is a useful tool to describe the uncertainty and the limited cognition of agents. Thus, this paper aims to provide a novel MATSM method under the probabilistic linguistic environment with unknown attribute weights. Firstly, the attribute weights are determined by providing the probabilistic linguistic decision-making trial and evaluation laboratory (PL-DEMATEL) method. Besides, this paper constructs the gain and loss (GL) matrices and calculates the agents' perceived values (PVs) by introducing prospect theory (PT). Then, the PVs are aggregated into the comprehensive PVs (CPVs) based on the obtained attribute weights. Next, this paper also proposes a ranking method, called probabilistic linguistic multi-attribute border approximation area comparison (PL-MABAC) method, to rank the multiple agents, which lay a solid foundation for stable matching constraint of the programming model. The matching results are obtained by solving the programming model. Finally, a case study of matching medical treatment service providers and demanders is presented to validate the proposed method. The comparative analyses and discussions are also provided to demonstrate its effectiveness.																	1562-2479	2199-3211				JUL	2020	22	5					1487	1505		10.1007/s40815-020-00844-7		MAY 2020											
J								An Intuitionistic Fuzzy Set Approach for Multi-attribute Information Classification and Decision-Making	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Intuitionistic fuzzy set (IFS); Multi-attribute; Information classification; Decision-making	ROUGH SETS; UNCERTAINTY	This article introduced a new multi-attribute information classification method by employing intuitionistic fuzzy set (IFS) approach. The proposed method was referred as four-way intuitionistic decision space (4WIDS). In the 4WIDS, IFS theory was used to model the inherent uncertainty of multi-attribute information. For generating more precise level of decision-rules, granular computing (GrC) approach was employed. The proposed 4WIDS method was appropriate for the classification of the multi-attribute information into four different regions as positive IFS, negative IFS, uncertain IFS and gray IFS regions. Detail methodology of the 4WIDS was explained by presenting its representation in a precise way. This study also presented various definitions, properties and theorems in the support of the 4WIDS method. The 4WIDS was applied in benchmark datasets that included Pima Indians diabetes, Thyroid disease, Fisher's Iris and Spambase datasets. Experimental results including statistical analysis indicated that the proposed 4WIDS outperformed existing classification methods, such as Naive Bayes, Decision tree, PART, J48, logistic model trees (LMT), rough set (RS), gray multi-granulation rough set (GMGRS) and multi-granulation fuzzy rough set (MGFRS).																	1562-2479	2199-3211				JUL	2020	22	5					1506	1520		10.1007/s40815-020-00879-w		MAY 2020											
J								Decentralised fractional order pi decontroller tuned using grey wolf optimization for three interacting cylindrical tanks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Three interacting cylindrical tank; Decentralized controller; RGA analysis; Condition number; Grey wolf optimizer; Fractional order PI controller	CONTROLLER-DESIGN; SYSTEM	Interacting processes are available in Industries. Control of such interacting processes is a challenging problem. Various control schemes like multiloop control, decentralised control with decoupler and centralised control are available for interacting processes. Multiloop decentralised control is the simplest among all the control because of its simplicity and easy adaptation. Decentralised control involves splitting of MIMO systems into n number of SISO systems and design of controller for the SISO system. But the interaction effects cannot be eliminated and involves decoupler to nullify the interaction effects in traditional approach. Online tuning of the optimal parameters is done by putting the process under servo regulatory condition and by varying the set point of tanks simultaneously leading to a controller which avoids decoupler. In this article, the servo and regulatory conditions for the system is analysed by using fractional order PI controller. Performance shows that for MIMO process the proposed FOPI controller is best suited for control applications.																	1868-5137	1868-5145															10.1007/s12652-020-02068-x		MAY 2020											
J								Traffic parameter estimation and control system based on machine vision	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent transportation system; Traffic parameter; Image processing; Traffic simulation		With the rapid development of urbanization in the world, it has brought enormous pressure on urban traffic management and control such as traffic congestion. An excellent urban traffic management and control system consists of three critical aspects: obtaining traffic parameters, developing traffic control scheme, and evaluating traffic control scheme. Intersection signal timing is one of the most important parts in urban traffic control. This paper proposed an intersection signal timing system based on traffic video which consists of three parts: acquisition of video-based traffic parameters, calculation of traffic flow-based signal timing scheme, and evaluation of intersection signal timing scheme. In the first part, we used advanced techniques such as deep learning and image processing to obtain traffic parameters such as traffic flow, vehicle type, composition of different vehicle types, and speed of vehicles passing through a scene in a traffic video. In the second part, we calculated the signal timing scheme of the video at the traffic scene through the obtained traffic flow information with Webster method. In the third part, the detailed traffic parameters and signal timing scheme were input into the VISSIM software for traffic microscopic simulation, which was used to evaluate the signal timing scheme. The experimental results show that the accuracy of the detailed traffic flow information obtained by the proposed system can reach more than 90%, the accuracy of composition of different vehicle types can be achieved more than 98%, and the vehicle speed accuracy can reach more than 95%. Therefore, the system improves the reliability and adaptability of the whole signal timing network. At the same time, the simulation results show that the proposed system integrates the acquisition of traffic parameters and the calculation and evaluation of signal timing schemes, and provides a good solution for solving research problems and actual needs such as signal timing optimization.																	1868-5137	1868-5145															10.1007/s12652-020-02052-5		MAY 2020											
J								CHPT: an improved coverage-hole patching technique based on tree-center in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Coverage-hole; Hole-area estimation; Hole-boundary detection; Hole-healing; Patch-positions; Wireless sensor networks	DEPLOYMENT	The occurrence of coverage-hole (CH) in a wireless sensor network degrades the performance of the network in terms of data transmission. Therefore, after the detection of CH, hole-patching is the next important task in order to reduce the energy consumption and to improve the network lifetime. In this paper, we propose an improved coverage-hole patching technique (CHPT) based on the concept of center of tree. The prerequisite to hole-patching is the hole-detection, which is done based on Delaunay Triangle and empty circle property. Subsequently, using the Inner empty circle property, the hole-area is estimated. And finally, CHPT is used to determine the hole-boundary and formulate the sub-trees for identifying the patch-positions to be covered by deploying additional sensor nodes. Experimental results show that the average percentage of coverage in CHPT increases up to 98.6% in terms of additional sensors and with a given sensing radius.																	1868-5137	1868-5145															10.1007/s12652-020-02038-3		MAY 2020											
J								Hungarian optimization technique based efficient resource allocation using clustering unbalanced estimated cost matrix	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Makespan; Hungarian; Virtual machine; Cloud service provider	CLOUD; TASKS; TIME	The prosperity of cloud technology generates expanding numerous real-time applications runs online. Meantime, real time tasks scheduling is an important criteria for cloud service provider to manage its Quality of Service (QoS) because all customer's wishes gratifying resource allotment in cloud. Cloud Service Provider (CSP) constitute service level agreement with the customers before the provision of resources. Allocating the resource according to the customer need and utilizing the systems efficiently are the major concern of CSP to increase the profit in their business. For this purpose Hungarian optimization technique is used and it is a standard technique which provides load balanced allocation of resources to the task. Still it cannot be used directly for cloud Virtual Machine (VM) allocation, because load balancing will not give better makespan and the standard Hungarian method is not suitable for unbalanced cost matrix. In this paper efficient resource allocation method called Cluster Cost Matrix - Hungarian (CCM-H) algorithm is proposed to optimize the performance. Algorithm consists of two phases. In first phase algorithm calculates the weighted values of tasks and based on the value tasks are clustered to convert the unbalanced cost matrix to balanced cost matrix. In second phase, according to the balanced cost matrix VM allocation is performed using Hungarian optimization technique. The metrics used for the performance analysis are makespan and utilization factor. The proposed CCM-H algorithm is compared with various existing and standard algorithms called First Come First Serve (FCFS), Min-Min based iterative Hungarian, FCFS based iterative algorithm, Max-min based iterative algorithm Laha and Gupta (Comput Ind Eng, 2016), Group based algorithms Lu et al. (IEEE Trans Wirel Commun, 2017) and normal Hungarianalgorithm, with bench mark dataset Braun (2015) and synthetic dataset which is created with random number generation function. Output shows that how the proposed method outperforms all the existing models.																	1868-5137	1868-5145															10.1007/s12652-020-02063-2		MAY 2020											
J								A novel synthetic aperture radar image change detection system using radial basis function-based deep convolutional neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Non local mean spatio temporal adaptive filtering (NLMSTAF); Log-ratio; Gauss-log-ratio operator; Local tetra pattern (LTrP); Wavelet statistical transform (WST); Maximally stable external region (MSER); Hybrid Gray Wolf optimization-genetic algorithm (Hybrid GWO-GA); Radial basis function-deep convolutional neural network (RBF-DCNN)	UNSUPERVISED CHANGE DETECTION; SAR IMAGES	Today, the automatic change detection and also classification as of the Synthetic Aperture Radar (SAR) images remain a hard process. In the existing research, the availability of Speckle Noise (SN), high time-consumption, and low accuracy are the chief issues. To resolve such issues, this paper proposed a novel SAR image change detection system utilizing a Radial Basis Function-based Deep Convolutional Neural Network (RBF-DCNN). The proposed methodology comprises six phases, namely, pre-processing, obtaining difference image, pixel-level image fusion, Feature Extraction (FE), Feature Selection (FS), and also change detection (CD) utilizing the classifier. Initially, the noise is eliminated as of the input, SAR image 1 and SAR image 2, utilizing the NLMSTAF approach. Subsequently, the difference image is attained by utilizing a Log-ratio operator (LRO) and Gauss-LRO, and the attained difference image is then fused. Next, the LTrP, WST, edge, and MSER features are extracted from the fused image. As of those features that were extracted, the necessary features are selected utilizing the Hybrid GWO-GA algorithm. The features (selected) are finally inputted to the RBF-DCNN classifier for detecting the changes in an image. Experimental outcomes established that the proposed work renders better performance on considering the existing system.																	1868-5137	1868-5145															10.1007/s12652-020-02091-y		MAY 2020											
J								Research and analysis of factors affecting bending performance of multi-cavity flexible actuator	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-cavity flexible actuator; Mathematical modeling; Finite element analysis	TENTACLES; TONGUES; TRUNKS; RODS; SOFT	Due to the inherent flexibility of the material, the flexible manipulator is highly adaptable to the environment. Compared with rigid robots, it is characterized by better adaptability, better flexibility, and better human-computer interaction performance. Therefore, it has gradually become a field popularly studied in the world. In this paper, a multi-cavity flexible actuator is fabricated, which consists of two parts, a main chamber and a base. The main body chamber is made of super-elastic material, and the base is mainly made of super-elastic material. There is a strain-limiting layer in the middle of the base, and the differential effect between these two parts is used to realize the bending action of the actuator. The pneumatic driving method is adopted, and the grasping action of the flexible gripper is realized by changing the air pressure inside the chamber. The mathematical model of flexible actuator was established and simulated by Abaqus finite element software to explore the effect of the length, the width and the height of the single chamber of the actuator on its bending performance. The experimental results show that: when the air pressure is constant, (1) the height and the width of the single chamber have a great influence on the bending performance, and all show a positive correlation; (2) compared with the height and the width, the length of the single chamber has a relatively smaller effect on the bending performance, and there is a negative correlation.																	1868-5137	1868-5145															10.1007/s12652-020-02074-z		MAY 2020											
J								Mutual Information Based Feature Selection for Stereo Visual Odometry	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Visual odometry; Feature selection; Mutual information; Motion estimation	CLASSIFICATION	Visual odometry (VO) is one of the promising techniques that estimates pose using the camera and does not necessarily require other sensor aiding. With increasing automation and the use of miniaturized systems such as mobile devices, wearable gadgets, & gaming consoles, demand for efficient algorithms have risen. In this paper, an attempt is made to remove the redundant features from the VO pipeline that do not have a significant effect on the estimation process. A probabilistic approach based on fast mutual information (MI) computation is suggested here as the basis for removing features. The MI value acts as a beacon for selecting distinct features while eliminating the redundant ones, thus improving the overall system speed and reducing storage requirements. The proposed MI-based feature selection framework for VO has been experimented on the KITTI vision benchmark suite and EuRoC MAV datasets available publicly. The estimated trajectory results have shown that the proposed technique is better in terms of computational efficiency and has similar accuracy as compared to the normal VO pipeline. Further investigations have also been carried out over the VSLAM framework to test its applicability in a real-time system.																	0921-0296	1573-0409															10.1007/s10846-020-01206-z		MAY 2020											
J								Forecast model of perceived demand of museum tourists based on neural network integration	NEURAL COMPUTING & APPLICATIONS										Tourist perception; Neural network integration; BP neural network (BPNN); Particle swarm optimization (PSO); Quantum particle swarm optimization (QPSO)		With the development of experiential tourism and the improvement of people's living standards, people have begun to transform tourist destinations into museum tourism. However, no effective method for predicting the demand for museum tourism has yet emerged. In order to be able to build a prediction model that can perceive the needs of museum tourists, this article uses advanced algorithms based on neural network integration and calls different algorithms: QPSO-BPNN, QPSO, PSO, PSO-BPNN, and BPNN. When the training ratio increases to 90%, the prediction accuracy of the three algorithms, BPNN, PSO, and PSO-BPNN, is less than 80%, and the prediction accuracy of the QPSO-BPNN algorithm has reached 92.5%. Under the condition of equal training set ratio, the prediction accuracy of QPSO-BPNN algorithm is always significantly higher than that of PSO-BPNN algorithm. When the training set proportions are 50%, 70%, and 90%, the changes in population size parameters have little effect on the prediction accuracy of the algorithm. Based on the above experiments, it is known that the QPSO-BPNN algorithm is less sensitive to the size of the population, and the algorithm has good robustness. With the increase in the number of initial classifiers, the prediction accuracy of the QPSO-BPNN algorithm has improved significantly. The experimental results are consistent with the previous theoretical derivation analysis, and the accuracy of the algorithm has a positive correlation with the number of classifiers.																	0941-0643	1433-3058															10.1007/s00521-020-05012-4		MAY 2020											
J								Design of backtracking search heuristics for parameter estimation of power signals	NEURAL COMPUTING & APPLICATIONS										Parameter estimation; Power signals; Evolutionary algorithm; BSA	ECONOMIC-DISPATCH PROBLEMS; NEURAL-NETWORKS; ALGORITHM; FREQUENCY; STATE	This study presents a novel implementation of evolutionary heuristics through backtracking search optimization algorithm (BSA) for accurate, efficient and robust parameter estimation of power signal models. The mathematical formulation of fitness function is accomplished by exploiting the approximation theory in mean squared errors between actual and estimated responses, as well as, true and approximated decision variables. Variants of BSA-based meta-heuristics are applied for parameter estimation problem of power signals for identification of amplitude, frequency and phase parameters for different scenarios of noise variation. Analysis of performance evaluation for BSAs is conducted through exhaustive statistical observations in terms of mean weight deviation, root mean square error and Thiel inequality coefficient-based assessment metrics, as well as, ANOVA tests for statistical significance.																	0941-0643	1433-3058															10.1007/s00521-020-05029-9		MAY 2020											
J								Cooperative coding and caching scheduling via binary particle swarm optimization in software-defined vehicular networks	NEURAL COMPUTING & APPLICATIONS										SDVN; I2V communication; Network coding; Binary particle swarm optimization	PREDICTION	With recent development in vehicular communication technologies, much attention has been paid to data dissemination in vehicular networks. In particular, the infrastructure-to-vehicle (I2V) communication is one of the primary technologies to provide a variety of information services. To enhance the bandwidth efficiency of I2V communication, this work considers in a software-defined vehicular networks (SDVN), aiming at exploiting synergistic effects of network coding and vehicular caching. First, we consider a data service scenario in which roadside unites (RSUs) are connected with the controller, which exercises scheduling decisions based on service requests received from vehicles. On this basis, we formulate a cooperative coding and caching scheduling problem with the objective of maximizing the bandwidth efficiency of I2V communication. Then, we propose a binary particle swarm optimization (BPSO)-based coding scheduling (BPSO_CS) algorithm. Finally, we build the simulation model and give a comprehensive performance evaluation. The results conclusively demonstrate the superiority of the proposed solution.																	0941-0643	1433-3058															10.1007/s00521-020-04978-5		MAY 2020											
J								A new point-of-interest group recommendation method in location-based social networks	NEURAL COMPUTING & APPLICATIONS										Location-based social networks; Point-of-interest; POI group recommendation; Extreme learning machine	EXTREME LEARNING-MACHINE; MODEL	POI group recommendation is one of the hottest research topics in location-based social networks, which recommends the most agreeable places for a group of users. However, traditional POI group recommendation methods only generate a consensus function to aggregate individual preference into group preference and they do not consider all the factors that can determine the results of POI group recommendation, which leads to a low recommendation accuracy. What's more, these methods have a long running time. Therefore, in this paper, we propose a new POI group recommendation method with an extreme learning machine (ELM) called PGR-ELM. The PGR-ELM method regards POI group recommendation as a binary classification problem. First, three features are extracted from three factors: POI popularity, group members' distance to POI, members' interest preferences combined affinity between group members. These features simultaneously consider all the factors that can determine the results of recommendation and guarantee the effectiveness of POI group recommendation. Then, the extracted features are input to train an ELM classifier because of its fast learning speed, which guarantees the efficiency of POI group recommendation. Finally, extensive experiments verify the accuracy and efficiency of PGR-ELM method.																	0941-0643	1433-3058															10.1007/s00521-020-04979-4		MAY 2020											
J								Sentiment word co-occurrence and knowledge pair feature extraction based LDA short text clustering algorithm	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										LDA; Sentiment analysis; Word co-occurrence; Knowledge pairs; Feature extraction	MODEL	The Latent Dirichlet Allocation (LDA) topic model is a popular research topic in the field of text mining. In this paper, Sentiment Word Co-occurrence and Knowledge Pair Feature Extraction based LDA Short Text Clustering Algorithm (SKP-LDA) is proposed. A definition of a word bag based on sentiment word co-occurrence is proposed. The co-occurrence of emotional words takes full account of different short texts. Then, the short texts of a microblog are endowed with emotional polarity. Furthermore, the knowledge pairs of topic special words and topic relation words are extracted and inserted into the LDA model for clustering. Thus, semantic information can be found more accurately. Then, the hidden n topics and Top30 special words set of each topic are extracted from the knowledge pair set. Finally, via LDA topic model primary clustering, a Top30 topic special words set is obtained that is clustered by K-means secondary clustering. The clustering center is optimized iteratively. Comparing with JST, LSM, LTM and ELDA, SKP-LDA performs better in terms of Accuracy, Precision, Recall and F-measure. The experimental results show that SKP-LDA reveals better semantic analysis ability and emotional topic clustering effect. It can be applied to the field of micro-blog to improve the accuracy of network public opinion analysis effectively.																	0925-9902	1573-7675															10.1007/s10844-020-00597-7		MAY 2020											
J								Living models or life modelled? On the use of models in the free energy principle	ADAPTIVE BEHAVIOR										Free energy; models; realism; instrumentalism; covariation; anti-representationalism		The free energy principle (FEP) is an information-theoretic approach to living systems. FEP characterizes life by living systems' resistance to the second law of thermodynamics: living systems do not randomly visit the possible states, but actively work to remain within a set of viable states. In FEP, this is modelled mathematically. Yet, the status of these models is typically unclear: are these models employed by organisms or strictly scientific tools of understanding? In this article, I argue for an instrumentalist take on models in FEP. I shall argue that models used as instruments for knowledge by scientists and models as implemented by organisms to navigate the world are being conflated, which leads to erroneous conclusions. I further argue that a realist position is unwarranted. First, it overgenerates models and thus trivializes the notion of modelling. Second, even when the mathematical mechanisms described by FEP are implemented in an organism, they do not constitute a model. They are covariational, not representational in nature, and precede the social practices that have shaped our scientific modelling practice. I finally argue that the above arguments do not affect the instrumentalist position. An instrumentalist approach can further add to conceptual clarity in the FEP literature.																	1059-7123	1741-2633														1059712320918678	10.1177/1059712320918678		MAY 2020											
J								Hybrid firefly with differential evolution algorithm for multi agent system using clustering based personalization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-agent system (MAS); Genetic algorithms (GAs); Differential evolution (DE) and firefly algorithm (FA); Hybrid firefly algorithm differential evolution (HFADE)		Multi-Agent System (MAS) appears to be an efficient, low cost, flexible, and reliable form of system, these features turns the MAS as a perfect solution for resolving complicated jobs. Personalisation is defined as the process of addressing the learner-specific techniques and their intentions or ideologies for assisting and promoting the process of an individual's learning success. The process of both modelling and estimating the above mentioned tasks in the internet is now turning out to be a tedious task due to the continuous growth in their sizes. Here, a decentralized technique based on a multi agent optimized clustering process has been found to work well for large data sets. Genetic Algorithms (GAs) are observed as the stochastic global optimization techniques that are meant for solving the optimization problems. The Firefly algorithm (FA) is the most efficient algorithms adopted for performing the global optimization tasks in complicated search spaces. Another type of population-oriented algorithm is the Differential Evolution (DE) algorithm. In this research article a novel combination of DE and the Firefly global optimization algorithms considered as the Hybrid Firefly Algorithm Differential Evolution (HFADE) for performing the clustering tasks in an efficient manner. The effectiveness of the HFADE was experimented with benchmark functions, the achieved results shows the Hybrid algorithm well suitable for the Learning Optimisation Problems.																	1868-5137	1868-5145															10.1007/s12652-020-02120-w		MAY 2020											
J								Load balancing based hyper heuristic algorithm for cloud task scheduling	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloudlet; Heuristic; Honey bee; Resource allocation; Virtual machine (VM); Hyper-heuristic		The cloud computing environment provides computing assets in a pay-per-use way for IT service providers. Guaranteeing QoS amid job scheduling is a most noticeable need. This paper proposed an algorithm that expects to accomplish all-around adjusted load crosswise over virtual machines for minimizing makespan time. The proposed algorithm provides balanced scheduling solutions by employing the honey bee load balancing and improvement detection operator to conclude which low-level heuristic is to be utilized to search improved candidate solutions. The consequences of the proposed task scheduling algorithm are matched with existing heuristic-based scheduling procedures. The experimental consequences demonstrate that our approach is efficient when it is compared with the existing algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02127-3		MAY 2020											
J								An enhanced design and random optimization for oversampling increment n-ary sumation modulator	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Delta-sigma modulator; Oversampling; Optimization; Random search; Signal to noise ratio (SNR); And signals	DELTA-SIGMA MODULATOR	Delta Sigma Modulator (DSM- increment n-ary sumation ) is a high-precision information converter that examines the Signal to Noise Ratio (SNR) in Radio Frequency Transmitter (RFT). This paper proposes an advancement model alongside with increment n-ary sumation model for the designing process. The predictable result is low Over Sampling Rate (OSR) DSM, which would benefit fast, high-multifaceted nature computations, primarily required for wireless applications. The enhanced DSM is a non-ideal second-order feed-forward signal processing. The enhancement of the DSM in Multipoint Random pursuit (MPRS) significantly improves coefficients of DSM to investigate the SNR and Nyquist rate. The advantage of multi-point in DSM is relatively easy for implementation on complex problems, with black-box function evaluations. This optimal DSM will deliver low OSR for wireless applications. This low OSR assumes a prevalent job in the sign preparation, and it impacts the general multifaceted nature and cost of the productive increment n-ary sumation converter. From the results of the SNR 68.28 dB, the sampling rate is 64-256, and finally, frequency is 1.92. This enhanced model executed using MATLAB reenactments and the outcomes guarantee a decrease in OSR by SNR rate. This model contrasted with other ordinary and versatile modulators. To examine the adequacy of the work, the yield signal data transmission seen to build multiple times with no expansion in the inspecting recurrence.																	1868-5137	1868-5145															10.1007/s12652-020-02106-8		MAY 2020											
J								Stylistic data-driven possibilistic fuzzy clustering and real-life application on epilepsy biomedical electronic signals detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Manifold data; Stylistic data; Possibilistic fuzzy clustering; Stylistic standardization matrix	C-MEANS; STYLE	Biomedical electronic signals play an important role in clinical diagnosis. EEG as one kind of biomedical electronic signals has been widely used for an epilepsy diagnosis. EEG data are strongly characterized by the inner cluster style. The classic clustering-based detection techniques such as FCM, K-means, and AP cannot effectively partition the manifold data without considering the inner cluster style. Therefore, in this paper, we propose a novel stylistic data-driven possibilistic fuzzy clustering technique (SD-PFC). SD-PFC has its merits in two aspects: (1) a stylistic standardization matrix is used to represent the stylistic information of samples contained in the inner clusters. (2) The distance matrix is re-constructed by samples which are transformed by style normalization matrices. Extensive experiments on artificial datasets and real-life datasets show the effectiveness of the novel clustering technique.																	1868-5137	1868-5145															10.1007/s12652-020-02112-w		MAY 2020											
J								Cloud management architecture to improve the resource allocation in cloud IAAS platform	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud vertical elasticity manager; Memory reporter; Memory over subscription granter; Dynamic memory allocation	QUALITY	Cloud computing (CC) is an attractive emerging technology due to offering services based on-demand by the process of virtualization. Since CC platform offers services based on-demand it has been widely used in the field of various emerging IT infrastructure. In cloud platform each application is run in individual virtual machine (VM) for execution of services within the host. Since cloud platform operates on on-demand service it need to cope with multiple application in single time hence it is necessary to adopt an effective approach for balancing memory utilization in cloud network. For effective utilization of available memory existing approaches uses probability distribution method for allocating resources in cloud platform but still there exists a lack of utilization of available memory in cloud platform. This paper aims to develop an effective approach for dynamic memory allocation in VM in cloud platform. For memory allocation among VM in cloud platform proposed approach uses cloud vertical elasticity manager (CVEM), memory reporter (MR), memory over subscription granter (MOG). The MOG uses a scheduler to allocate the memory in a dynamic way inside a host. Finally, we adopt host elasticity rule to balance the available memory to allocate dynamically the memory inside an available host in cloud.																	1868-5137	1868-5145															10.1007/s12652-020-02026-7		MAY 2020											
J								Deep network in network	NEURAL COMPUTING & APPLICATIONS										Exponential linear unit (ELU); Convolutional neural networks (CNNs); Deep MLPconv; Image recognition; Network in Network (NiN)	NEURAL-NETWORKS; LEARNING DEEP; REPRESENTATION	The different CNN models use many layers that typically include a stack of linear convolution layers combined with pooling and normalization layers to extract the characteristics of the images. Unlike these models, and instead of using a linear filter for convolution, the network in network (NiN) model uses a multilayer perception (MLP), a nonlinear function, to replace the linear filter. This article presents a new deep network in network (DNIN) model based on the NiN structure, NiN drag a universal approximator, (MLP) with rectified linear unit (ReLU) to improve classification performance. The use of MLP leads to an increase in the density of the connection. This makes learning more difficult and time learning slower. In this article, instead of ReLU, we use the linear exponential unit (eLU) to solve the vanishing gradient problem that can occur when using ReLU and to speed up the learning process. In addition, a reduction in the convolution filters size by increasing the depth is used in order to reduce the number of parameters. Finally, a batch normalization layer is applied to reduce the saturation of the eLUs and the dropout layer is applied to avoid overfitting. The experimental results on the CIFAR-10 database show that the DNIN can reduce the complexity of implementation due to the reduction in the adjustable parameters. Also the reduction in the filters size shows an improvement in the recognition accuracy of the model.																	0941-0643	1433-3058															10.1007/s00521-020-05008-0		MAY 2020											
J								Lipschitz constrained GANs via boundedness and continuity	NEURAL COMPUTING & APPLICATIONS										Generative adversarial networks; Lipschitz constraint; Boundedness; Continuity		One of the challenges in the study of generative adversarial networks (GANs) is the difficulty of its performance control. Lipschitz constraint is essential in guaranteeing training stability for GANs. Although heuristic methods such as weight clipping, gradient penalty and spectral normalization have been proposed to enforce Lipschitz constraint, it is still difficult to achieve a solution that is both practically effective and theoretically provably satisfying a Lipschitz constraint. In this paper, we introduce the boundedness and continuity (BC) conditions to enforce the Lipschitz constraint on the discriminator functions of GANs. We prove theoretically that GANs with discriminators meeting the BC conditions satisfy the Lipschitz constraint. We present a practically very effective implementation of a GAN based on a convolutional neural network (CNN) by forcing the CNN to satisfy the BC conditions (BC-GAN). We show that as compared to recent techniques including gradient penalty and spectral normalization, BC-GANs have not only better performances but also lower computational complexity.																	0941-0643	1433-3058															10.1007/s00521-020-04954-z		MAY 2020											
J								The Distance Induced OWA Operator with Application to Multi-criteria Group Decision Making	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										DIOWA operator; Distance measure; Group decision making; Hamming distance; Interval number; Order-inducing variable	INDUCED AGGREGATION OPERATORS; SIMILARITY MEASURES; EUCLIDEAN DISTANCE; FUZZY-SETS; ALGORITHM	Some aggregation operators with distance measures have been proposed, but distance measure values play the role of argument variables. In this paper, the induced ordered weighted averaging (IOWA) operator with the Hamming distance is proposed, termed as the distance induced ordered weighted averaging (DIOWA) operator. The most distinctive characteristic of the DIOWA operator is that the distance measure values play the role of order-inducing variables. Some properties and special cases of the DIOWA operator are analyzed. This new operator is further extended to the uncertain situations represented by interval numbers. A new multi-criteria group decision-making (MCGDM) method with the proposed operators in this paper is also studied. Finally, a numerical example about how to select the best candidate is given to show how to use the DIOWA operator with interval numbers in group decision making.																	1562-2479	2199-3211				JUL	2020	22	5					1624	1634		10.1007/s40815-020-00863-4		MAY 2020											
J								Sparse Low-Rank and Graph Structure Learning for Supervised Feature Selection	NEURAL PROCESSING LETTERS										Graph learning; Low-rank constraint; Orthogonal constraint; Spectral feature selection		Spectral feature selection (SFS) is superior to conventional feature selection methods in many aspects, by extra importing a graph matrix to preserve the subspace structure of data. However, the graph matrix of classical SFS that is generally constructed by original data easily outputs a suboptimal performance of feature selection because of the redundancy. To address this, this paper proposes a novel feature selection method via coupling the graph matrix learning and feature data learning into a unified framework, where both steps can be iteratively update until achieving the stable solution. We also apply a low-rank constraint to obtain the intrinsic structure of data to improve the robustness of learning model. Besides, an optimization algorithm is proposed to solve the proposed problem and to have fast convergence. Compared to classical and state-of-the-art feature selection methods, the proposed method achieved the competitive results on twelve real data sets.																	1370-4621	1573-773X															10.1007/s11063-020-10250-7		MAY 2020											
J								Implicit mood computing via LSTM and semantic mapping	SOFT COMPUTING										Affective computing; Mood; LSTM; Semantic mapping		This article proposes an implicit mood computing system. The implicit mood computing task is a part of affective computing. Previous works in affective computing mostly focus on twitters, blogs, movie interviews, and news corpus. These works detect sentiment polarity (positive/negative), emotion types (joy, sadness, anger, etc.), or mood types (boring, tired, happy, etc.) of the text. Different from previous studies, our work focuses on the literature texts and detects the implicit mood of them. The implicit mood is sometimes discussed as the tone or the atmosphere of the text. The implicit mood is an important affective feature in the literature such as poetry, prose, and drama. Our work regards the implicit mood as a semantic phenomenon. We capture the feature of implicit mood via a semantic mapping approach and the long short-term memory neural network. The proposed system is capable of identifying 12 kinds of implicit moods with a promising result.																	1432-7643	1433-7479				OCT	2020	24	20					15795	15809		10.1007/s00500-020-04909-5		MAY 2020											
J								Healthcare predictive analytics for disease progression: a longitudinal data fusion approach	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Healthcare predictive analytics; Longitudinal data fusion; Machine learning; Regression; Group lasso	REGRESSION; SHRINKAGE; SELECTION; MODEL	Healthcare predictive analytics using electronic health records (EHR) offers a promising direction to address the challenging tasks of health assessment. It is highly important to precisely predict the potential disease progression based on the knowledge in the EHR data for chronic disease care. In this paper, we utilize a novel longitudinal data fusion approach to model the disease progression for chronic disease care. Different from the conventional method using only initial or static clinical data to model the disease progression for current time prediction, we design a temporal regularization term to maintain the temporal successivity of data from different time points and simultaneously analyze data from data source level and feature level based on a sparse regularization regression approach. We examine our approach through extensive experiments on the medical data provided by the Alzheimer's Disease Neuroimaging Initiative (ADNI). The results show that the proposed approach is more useful to simulate and predict the disease progression compared with the existing methods.																	0925-9902	1573-7675				OCT	2020	55	2					351	369		10.1007/s10844-020-00606-9		MAY 2020											
J								Grasp Pose Detection with Affordance-based Task Constraint Learning in Single-view Point Clouds	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot grasp; Grasp pose detection; Object affordance; Convolutional neural networks; Constraints learning	MANIPULATION	Learning to grasp novel objects is a challenging issue for service robots, especially when the robot is performing goal-oriented manipulation or interaction tasks whilst only single-view RGB-D sensor data is available. While some visual approaches focus on grasping that satisfy force-closure standards only, we further link affordances-based task constraints to the grasp pose on object parts, so that both force-closure standard and task constraints can be ensured. In this paper, a new single-view approach is proposed for task-constrained grasp pose detection. We propose to learn a pixel-level affordance detector based on a convolutional neural network. The affordance detector provides a fine grained understanding of the task constraints on objects, which are formulated as a pre-segmentation stage in the grasp pose detection framework. The accuracy and robustness of grasp pose detection are improved by a novel method for calculating local reference frame as well as a position-sensitive fully convolutional neural network for grasp stability classification. Experiments on benchmark datasets have shown that our method outperforms the state-of-the-art methods. We have also validated our method in real-world and task-specific grasping scenes, in which higher success rate for task-oriented grasping is achieved.																	0921-0296	1573-0409				OCT	2020	100	1					145	163		10.1007/s10846-020-01202-3		MAY 2020											
J								Optimization of supply chain efficiency management based on machine learning and neural network	NEURAL COMPUTING & APPLICATIONS										Machine learning; Neural network; Supply chain; Efficiency; Performance evaluation	LOGISTICS	Supply chain management is of great significance to business operations and socioeconomic development. However, the current supply chain efficiency management cannot effectively control the risk caused by the inefficient supply chain management. In order to study the improvement in supply chain efficiency management, supported by machine learning and neural network technology, this study builds a supply chain risk management model based on learning and neural network. Moreover, this study evaluates the risk indicator system based on the current status of supply chain management. In addition, the model simulation research is carried out in the MATLAB platform, and the validity analysis of the model is performed with examples. Finally, after training the data through the training model, the risk assessment value is output, and strategies for coping with the risk are given. The research shows that the model proposed in this paper has a certain practical effect and can be considered for application.																	0941-0643	1433-3058															10.1007/s00521-020-05023-1		MAY 2020											
J								A deep learning method for motor fault diagnosis based on a capsule network with gate-structure dilated convolutions	NEURAL COMPUTING & APPLICATIONS										Deep learning; Capsule network; Dilated convolutions; Input gate structure of LSTM; Motor fault diagnosis	NEURAL-NETWORK	Motor fault diagnosis is critical to predictive maintenance of electrical motor condition monitoring, whereas the conventional motor fault diagnosis method cannot effectively diagnose conditions caused by motors' complex structure, non-stationary signals and mechanical big data. To solve the mentioned problems and enhance the fault diagnostic accuracy and generalization performance under different actual motor conditions, this study proposes an efficient, noise-resistant, end-to-end deep learning algorithm based on a novel capsule network with gate-structure dilated convolutions (GDCCN) for motor fault diagnosis; such algorithm is subtly incorporated with the input gate structure of long short-term memory network (LSTM), the dilated convolutions, as well as the capsule network. In the GDCCN model, the raw vibration signals are directly fed into the input gate structure of LSTM, which are employed to effectively remove noise and harvest more valuable information from the input sample. The dilated convolution is exploited in the output denoising feature maps to exponentially expand the receptive field of convolution kernel, so more redundant information can be acquired to reduce the effect of randomness. The capsule network is introduced to generate a set of vector neurons to represent an entity existing in the feature maps; as a result, more specific feature representations can be extracted, and the feature can be comprehended, thereby enhancing the diagnostic accuracy. As revealed from the experimental results, the GDCCN-based intelligent motor fault diagnosis method outperforms the other classical DL algorithms in diagnosis accuracy, noise resistance, generalization and transfer-learning performance of different workloads.																	0941-0643	1433-3058															10.1007/s00521-020-04999-0		MAY 2020											
J								Adaptive fuzzy controlled hybrid shunt active power filter for power quality enhancement	NEURAL COMPUTING & APPLICATIONS										Adaptive fuzzy hysteresis current controller; Harmonic compensation hybrid shunt active power filter; Power quality	IMPROVEMENT; EXTRACTION; HARMONICS; SYSTEM; UPQC	A novel switching pulse generation methodology based on adaptive fuzzy hysteresis current controlled hybrid shunt active power filter (A-F-HCC-HSAPF) is presented in this paper for compensating reactive power and harmonics in distribution network. The harmonic problems are mainly evolved because of extensive use of nonlinear loads in industry and domestic sectors. There are some adverse effects of harmonics such as: malfunctioning of sensitive equipment, resonance issues, conductors heating, power losses and reduced efficiency in distribution system. To mitigate harmonics issues passive, filters are used but when the harmonic component increases the design of passive filters is complex and becomes bulky. With the advancement in power electronic, the active power filter has been designed. Generally, the rating of the active filter is very high for some applications; hence hybrid shunt active power filter (HSAPF) is proposed by low-rated shunt active power filter (SAPF) and low-cost shunt passive filter. Proportional-integral or fuzzy logic controller is used to estimate the reference current and to regulate the dc capacitor voltage. To generate the switching pulse for the voltage source inverter of the SAPF, a novel adaptive fuzzy hysteresis current controller (A-F-HCC) is adopted. The performance of the proposed A-F-HCC-HSAPF is investigated during steady-state and transient conditions using MATLAB/Simulink and real-time environments.																	0941-0643	1433-3058															10.1007/s00521-020-05027-x		MAY 2020											
J								MNSSp3: Medical big data privacy protection platform based on Internet of things	NEURAL COMPUTING & APPLICATIONS										Privacy protection; Platform; Gene data; EMR; EEG data	INFORMATION; FRAMEWORK	How to transform the growing medical big data into medical knowledge is a global topic. However, medical data contains a large amount of personal privacy information, especially electronic medical records, gene data and electroencephalography data; the current methods and tools for data sharing are not efficient or cannot be applied in real-life applications. Privacy disclosure has become the bottleneck of medical big data sharing. In this context, we conducted research of medical data from the data collection, data transport and data sharing to solve the key problems of privacy protection and put forward a privacy protection sharing platform called MNSSp3 (medical big data privacy protection platform based on Internet of things), which attempts to provide an effective medical data sharing solution with the privacy protection algorithms for different data types and support for data analytics. The platform focuses on the transmission and sharing security of medical big data to provide users with mining methods and realizes the separation of data and users to ensure the security of medical data. Moreover, the platform also provides users with the capacity to upload privacy algorithms independently. We discussed the requirements and the design components of the platform, then three case studies were presented to verify the functionality of the platform, and the results of the experiments show clearly the benefit and practicality of the proposed platform.																	0941-0643	1433-3058															10.1007/s00521-020-04873-z		MAY 2020											
J								Evaluation of a dynamic classification method for multimodal ambiguities based on Hidden Markov Models	EVOLVING SYSTEMS										Hidden markov models; Human-computer interaction; Multimodal interaction; Natural language processing	BAYESIAN NETWORKS; RECOGNITION	The wide interest in ambiguities is because it represents uncertainty but also a fundamental item of discussion for who is interested in the interpretation of languages also considering that it is functional for communicative purposes. This paper addresses ambiguity issues in terms of identification of the meaningful features of multimodal ambiguities and it evaluates a dynamic HMM-based classification method that is able to classify ambiguities by learning, and progressively adapting the model to the evolution of the interaction, refining the existing classes, or identifying new ones. The comparative evaluation of the considered method of the considered method with other surveyed methods demonstrates an improvement considering the performance evaluation measures.																	1868-6478	1868-6486															10.1007/s12530-020-09344-3		MAY 2020											
J								Intuitionistic Fuzzy Sliding Controller for Uncertain Hyperchaotic Synchronization	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Intuitionistic fuzzy controller; Chaotic systems and synchronization; Sliding mode control; Lyapunov stability	MODE CONTROL; CHAOS SYNCHRONIZATION; SYSTEMS; DESIGN	In this study, the design of an intuitionistic fuzzy controller for synchronization of two non-identical hyperchaotic systems is proposed. Since hyperchaotic systems has high sensitivity to the initial condition, disturbance and parameter variability. Synchronization of hyperchaotic systems is used to test the controller performance. On the other hand, using of fuzzy logic-based controller has an increasing tendency. As known, fuzzy logic control (FLC), only membership functions are used to obtain a realistic model of the systems. But the intuitionistic fuzzy logic control (IFLC) allows us to obtain a more realistic model than FLC because it also takes into account the degree of non-membership and the degree of uncertainty beside of degree of membership to model examined system. Fuzzy logic-based controllers are hybridized with robust control methods such as sliding mode controller to improve the performance of controller. To take advantages of SMC with fuzzy logic-based IFLC, the IFSMC controller obtained by hybridizing these two methods was designed for hyperchaotic systems. To demonstrate the performance of IFSMC, the results obtained from the synchronization of hyperchaotic systems with FSMC (fuzzy sliding mode controller) and IFSMC were compared. The stability of IFSMC is proved by Lyapunov stability condition. The numerical results and analysis show the efficiency of the IFSMC with regards to synchronization control of uncertain chaotic systems having challenging external disturbances in terms of robustness, minimum tracking error.																	1562-2479	2199-3211				JUL	2020	22	5					1430	1443		10.1007/s40815-020-00878-x		MAY 2020											
J								Third-Party Cold Chain Medicine Logistics Provider Selection by a Rough Set-Based Gained and Lost Dominance Score Method	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multiple criteria group decision making; Rough number; Third-party logistics provider selection; Cold chain medicine logistics; Gained and lost dominance score method; Linguistic term set	LINGUISTIC TERM SETS; DECISION-MAKING; FUZZY APPROACH; EFFICIENCY	Choosing third-party cold chain logistics suppliers can be regarded as a multiple criteria group decision-making problem since multiple aspects of suppliers are required to be evaluated by multiple experts. This paper aims to address the problem of selecting the optimal third-party cold chain medicine logistics provider considering the uncertainties caused by the qualitative criteria that are difficult to accurately evaluate and the limitation of experts' knowledge and cognition. We propose a rough set-based gained and lost dominance score method in which experts are supposed to use linguistic terms to express their information. First, we combine rough numbers with linguistic scale functions to depict the semantics of linguistic terms and convert the formal expression information of linguistic terms into numerical information, which has an advantage of expressing imprecision and subjective judgments of experts. In addition, given that different experts often have different emphases on different attributes, we investigate a rough set-based gained and lost dominance score method in which experts have different weights for different criteria. Finally, an illustrative example of selecting the optimal third-party cold chain medicine logistics is given with comparative analysis, showing the efficiency of the proposed method. This method provides a new way to solve the selection problem of third-party logistics suppliers.																	1562-2479	2199-3211				SEP	2020	22	6			SI		2055	2069		10.1007/s40815-020-00867-0		MAY 2020											
J								A deep convolution generative adversarial networks based fuzzing framework for industry control protocols	JOURNAL OF INTELLIGENT MANUFACTURING										Fuzz testing; Industrial control protocol; Quality control; Deep adversarial learning; Convolution neural networks; Long short-term memory; Industry 4; 0		A growing awareness is brought that the safety and security of industrial control systems cannot be dealt with in isolation, and the safety and security of industrial control protocols (ICPs) should be considered jointly. Fuzz testing (fuzzing) for the ICP is a common way to discover whether the ICP itself is designed and implemented with flaws and network security vulnerability. Traditional fuzzing methods promote the safety and security testing of ICPs, and many of them have practical applications. However, most traditional fuzzing methods rely heavily on the specification of ICPs, which makes the test process a costly, time-consuming, troublesome and boring task. And the task is hard to repeat if the specification does not exist. In this study, we propose a smart and automated protocol fuzzing methodology based on improved deep convolution generative adversarial network and give a series of performance metrics. An automated and intelligent fuzzing framework BLSTM-DCNNFuzz for application is designed. Several typical ICPs, including Modbus and EtherCAT, are applied to test the effectiveness and efficiency of our framework. Experiment results show that our methodology outperforms the existing ones like General Purpose Fuzzer and other deep learning based fuzzing methods in convenience, effectiveness, and efficiency.																	0956-5515	1572-8145															10.1007/s10845-020-01584-z		MAY 2020											
J								Shadowed sets with higher approximation regions	SOFT COMPUTING										Fuzzy set; Shadowed set; Three-way approximation	3-WAY APPROXIMATIONS; FUZZY-SETS; UNCERTAINTY	This paper mainly discusses three points involving shadowed set approximation of a given fuzzy set. Firstly, a principle of uncertainty balance, which guarantees that preservation of uncertainty in the induced shadowed set is studied. Secondly, an alternative formulation for determining the optimum partition thresholds of shadowed sets is suggested. This formulation helps us study principle of uncertainty balance in shadowed sets with higher approximation regions. Thirdly, five-region shadowed set, which effectively deals with the issue of uncertainty balance, is introduced. We provide a closed-form formula for determining its optimum partition thresholds and generalize it to n(>= 5)-region shadowed sets. Finally, some examples from synthetic and real dataset are provided to demonstrate the feasibility of the suggested methods.																	1432-7643	1433-7479				NOV	2020	24	22					17009	17033		10.1007/s00500-020-04992-8		MAY 2020											
J								Multi-label charge predictions leveraging label co-occurrence in imbalanced data scenario	SOFT COMPUTING										Charge prediction; Imbalanced data; Few-shot charges; Multi-label classification; Label co-occurrence	CONVOLUTIONAL NEURAL-NETWORK; SENTIMENT ANALYSIS; CLASSIFICATION; FRAMEWORK; FUSION	Charge prediction is to predict associated charges based on fact descriptions and plays a significant role in legal aid systems. It is a fundamental and challenging task to automatically predict charges in the multi-label classification paradigm, which is fit to real applications. Existing works either focus on balanced data scenario and multiple charges or few-shot charges with a single label. Moreover, previous models utilize special initialization with label patterns to improve the performance of the multi-label classification task, which is only applicable when there is less training data, resulting in poor robustness. To this end, a multi-task convolutional neural network combined with bidirectional long short-time memory leveraging label co-occurrence framework, called CBLLC, is introduced to predict multiple charges with article information on imbalanced data occasion. We develop a new learning mechanism to train the framework of charge and article patterns when there is a lot of training data, increasing its robustness. In CBLLC, the data preprocessing process serves to aid the training in a more generalized manner and reduce overfitting. A salient word annotation is introduced to deal with few-shot charges. A better classification result is obtained with processed data and improves the generality of the model. Experimental results of Chinese AI and Law Challenge test set show the superiority of our proposed method compared with the state-of-the-art methods. In particular, a macro-F1 score of 92.9% for charges and 86.6% for articles is achieved with co-occurrence of charges and patterns of articles.																	1432-7643	1433-7479															10.1007/s00500-020-05029-w		MAY 2020											
J								A greyness reduction framework for prediction of grey heterogeneous data	SOFT COMPUTING										Operational rule; Greyness reduction; Grey interval number; Prediction model; Stock replenishment scheduling	DECISION-MAKING MODEL; FUZZY; NUMBER; MANAGEMENT	Existing operational rules of interval grey numbers do not make full use of possible background information when determining the interval boundaries, and this may result in inconsistent results if applying different logical operations. This paper finds that multiplication and division rules of interval grey numbers do not meet the calculation rule of inverse operators. Direct solution and inverse solution of the same interval grey number object may differ not only in numerical ranges but also in greyness degrees. To improve the accuracy of grey number calculation, new operational rules for multiplication and division of interval grey numbers are proposed. Then the traditional prediction modelling method of grey heterogeneous data is refined and expanded by integrating a greyness reduction preprocessing, which is based on the proposed calculation rules. Application of the expanded heterogeneous interval grey number prediction model to a stock replenishment scheduling problem in emergency rescue scenarios is included to illustrate the new operational rules of grey numbers and their application in prediction algorithm, and the proposed approach is compared with other existing methods to demonstrate its effectiveness.																	1432-7643	1433-7479															10.1007/s00500-020-05040-1		MAY 2020											
J								Utilizing external corpora through kernel function: application in biomedical named entity recognition	PROGRESS IN ARTIFICIAL INTELLIGENCE										Support vector machines; Kernel function; Named entity recognition; Biomedical informatics; Feature extraction	CLASSIFIERS	Performance of word sequential labelling tasks like named entity recognition and parts-of-speech tagging largely depends on the features chosen in the task. But, in general representing a word as well as capturing its characteristics properly through a set of features is quite difficult. Moreover, external resources often become essential in order to build a high-performance system. But, acquiring required knowledge demands domain-specific processing and feature engineering. Kernel functions along with support vector machine may offer an alternative way to more efficiently capture similarity between words using both the local context and the external corpora. In this paper, we aim to compute similarity between the words using their context information, syntactic information and occurrence statistics in external corpora. This similarity value is gathered through a kernel function. The proposed kernel function combines two sub-kernels. One of these captures global information through words co-occurrence statistics accumulated from a large corpora. The second kernel captures local semantic information of the words through word specific parse tree fragmentation. We test this proposed kernel using JNLPBA 2004 Biomedical Named Entity Recognition and BioCreative II 2006 Gene Mention Recognition task data-sets. In our experiments, we observe that the proposed method is effective on both the data-sets.																	2192-6352	2192-6360				SEP	2020	9	3					209	219		10.1007/s13748-020-00208-0		MAY 2020											
J								Administrative due process when using automated decision-making in public administration: some notes from a Finnish perspective	ARTIFICIAL INTELLIGENCE AND LAW										Administrative due process; Legal safeguards; Administration; Decision-making; Constitution; Software design	SECTOR	Various due process provisions designed for use by civil servants in administrative decision-making may become redundant when automated decision-making is taken into use in public administration. Problems with mechanisms of good government, responsibility and liability for automated decisions and the rule of law require attention of the law-maker in adapting legal provisions to this new form of decision-making. Although the general data protection regulation of the European Union is important in acknowledging automated decision-making, most of the legal safeguards within administrative due process have to be provided for by the national law-maker. It is suggested that all countries have a need to review their rules of administrative due process with a view to bringing them up to date regarding the requirements of automated decision-making. In whichever way the legislation is framed, the key issues are that persons who develop the algorithm and the code as well as persons who run or deal with the software within public authorities are aware of the preventive safeguards of legality in the context of automated decision-making, not only of the reactive safeguards constituted by the complaint procedures, and that legal mechanisms exist under which these persons can be held accountable and liable for decisions produced by automated decision-making. It is also argued that only rule-based systems of automatized decision-making are compatible with the rule of law and that there is a general interest in preventing a development into a rule of algorithm.																	0924-8463	1572-8382															10.1007/s10506-020-09269-x		MAY 2020											
J								Sentiment analysis with deep neural networks: comparative study and performance assessment	ARTIFICIAL INTELLIGENCE REVIEW										Deep neural networks; Sentiment analysis; Performance evaluation; Aspect-based sentiment analysis; Opinion mining	MEMORY NETWORKS; WORD EMBEDDINGS; TEXT; CLASSIFICATION; MACHINE; MODEL	The current decade has witnessed the remarkable developments in the field of artificial intelligence, and the revolution of deep learning has transformed the whole artificial intelligence industry. Eventually, deep learning techniques have become essential components of any model in today's computational world. Nevertheless, deep learning techniques promise a high degree of automation with generalized rule extraction for both text and sentiment classification tasks. This article aims to provide an empirical study on various deep neural networks (DNN) used for sentiment classification and its applications. In the preliminary step, the research carries out a study on several contemporary DNN models and their underlying theories. Furthermore, the performances of different DNN models discussed in the literature are estimated through the experiments conducted over sentiment datasets. Following this study, the effect of fine-tuning various hyperparameters on each model's performance is also examined. Towards a better comprehension of the empirical results, few simple techniques from data visualization have been employed. This empirical study ensures deep learning practitioners with insights into ways to adapt stable DNN techniques for many sentiment analysis tasks.																	0269-2821	1573-7462				DEC	2020	53	8					6155	6195		10.1007/s10462-020-09845-2		MAY 2020											
J								Solar energy plant project selection with AHP decision-making method based on hesitant fuzzy linguistic evaluation	COMPLEX & INTELLIGENT SYSTEMS										Hesitant fuzzy linguistic terms; AHP; Solar energy; Decision making	ANALYTIC HIERARCHY PROCESS; EXTENT ANALYSIS METHOD; TECHNOLOGIES; MODEL; VIKOR; STATE	Increased energy demand is expected to be met by reliable and continuous energy sources. Renewable energy which is obtained from nature and can continuously reload itself from natural sources is a new generation energy type. The sun, which is the main source of renewable energies and produces heat and electricity by direct and indirect methods, is an important renewable energy source. The installation of solar energy systems takes place under the basic technical, economic and political factors. Alternative solar energy plant projects are evaluated linguistically under the main criteria based on the knowledge and experience of the experts. Hesitant fuzzy linguistic terms are used to incorporate the uncertain and hesitant expressions into the decision-making process. The decision-making process that takes place with hesitant linguistic expressions in multiple sub-criteria is based on the AHP model. The inclusion of hesitant statements in the decision-making process with the AHP model enables more realistic choices among the alternatives. System technology (0.18), energy policy (0.15) and energy price change (0.13) appear as the most important factors in the pairwise comparison of the factors based on hesitant fuzzy linguistic evaluations. The results coincide with the need for high efficiency in solar energy systems, the importance of governmental supportive policies and the effects of price competition in the energy sector. Also, the closeness of the overall priority values of all projects (0.189, 0.23, 0.287, 0.135, 0.158) indicates that the decision makers take into account the effective factors.																	2199-4536	2198-6053				OCT	2020	6	3					507	529		10.1007/s40747-020-00152-5		MAY 2020											
J								Adaptive Equivalent-input-disturbance Approach to Improving Disturbance-rejection Performance	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Adaptive control; disturbance rejection; equivalent input disturbance (EID); linear matrix inequality (LMI); Lyapunov stability; root locus	STATE DELAY; SYSTEMS	This paper presents an adaptive equivalent-input-disturbance (AEID) approach that contains a new adjustable gain to improve disturbance-rejection performance. A linear matrix inequality is derived to design the parameters of a control system. An adaptive law for the adjustable gain is presented based on the combination of the root locus method and Lyapunov stability theory to guarantee the stability of the AEID-based system. The adjustable gain is limited in an allowable range and the information for adjusting is obtained from the state of the system. Simulation results show that the method is effective and robust. A comparison with the conventional EID approach demonstrates the validity and superiority of the method.																	1476-8186	1751-8520				OCT	2020	17	5					701	712		10.1007/s11633-020-1230-7		MAY 2020											
J								Markov Weighted Fuzzy Time-Series Model Based on an Optimum Partition Method for Forecasting Air Pollution	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Air pollution; Fuzzy logical relationship; Fuzzy time-series; Partitioning methods; Markov transition matrix	URBAN AIR; HYBRID MODEL; ENROLLMENTS; INTERVALS; OPTIMIZATION; PREDICTION; LENGTHS; INDEX	Air pollution is one of the main environmental issues faced by most countries around the world. Forecasting air pollution occurrences is an essential topic in air quality research due to the increase in awareness of its association with public health effects, and its development is vital to managing air quality. However, most previous studies have focused on enhancing accuracy, while very few have addressed uncertainty analysis, which may lead to insufficient results. The fuzzy time-series model is a better option in air pollution forecasting. Nevertheless, it has a limitation caused by utilizing a random partitioning of the universe of discourse. This study proposes a novel Markov weighted fuzzy time-series model based on the optimum partition method. Fitting the optimum partition method has been done based on five different partition methods via two stages. The proposed model is first applied for forecasting air pollution using air pollution index (API) data collected from an air monitoring station located in Klang city, Malaysia. The performance of the proposed model is evaluated based on three statistical criteria, which are the mean absolute percentage error, mean squared error and Theil's U statistic, using the daily API data. For further validation of the model, it is also implemented for benchmark enrolment data from the University of Alabama. According to the analysis results, the proposed model greatly improved the performance of air pollution index and enrolment prediction accuracy, for which it outperformed several state-of-the-art fuzzy time-series models and classic time-series models. Thus, the proposed model could be a better option for air quality forecasting for managing air pollution.																	1562-2479	2199-3211				JUL	2020	22	5					1468	1486		10.1007/s40815-020-00841-w		MAY 2020											
J								From QBFs to MALL and Back via Focussing	JOURNAL OF AUTOMATED REASONING										Focussing; Linear logic; QBFs; Alternation; Polynomial hierarchy		In this work we investigate how to extract alternating time bounds from `focussed' proof systems. Our main result is the obtention of fragments of MALLw (MALL with weakening) complete for each level of the polynomial hierarchy. In one direction we encode QBF satisfiability and in the other we encode focussed proof search, and we show that the composition of the two encodings preserves quantifier alternation, yielding the required result. By carefully composing with well-known embeddings of MALLw into MALL, we obtain a similar delineation of MALL formulas, again carving out fragments complete for each level of the polynomial hierarchy. This refines the well-known results that both MALLw and MALL are PSPACE-complete. A key insight is that we have to refine the usual presentation of focussing to account for deterministic computations in proof search, which correspond to invertible rules that do not branch. This is so that we may more faithfully associate phases of focussed proof search to their alternating time complexity. This presentation seems to uncover further dualities, at the level of proof search, than usual presentations, so could be of proof theoretic interest in its own right.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1221	1245		10.1007/s10817-020-09564-x		MAY 2020											
J								Building Strategies into QBF Proofs	JOURNAL OF AUTOMATED REASONING										QBF; DQBF; Resolution; Proof complexity	DEPENDENCY SCHEMES; CERTIFICATION; RESOLUTION; FORMULAS	Strategy extraction is of great importance for quantified Boolean formulas (QBF), both in solving and proof complexity. So far in the QBF literature, strategy extraction has been algorithmically performed from proofs. Here we devise the first QBF system where (partial) strategies are built into the proof and are piecewise constructed by simple operations along with the derivation. This has several advantages: (1) lines of our calculus have a clear semantic meaning as they are accompanied by semantic objects; (2) partial strategies are represented succinctly (in contrast to some previous approaches); (3) our calculus has strategy extraction by design; and (4) the partial strategies allow new sound inference steps which are disallowed in previous central QBF calculi such as Q-Resolution and long-distance Q-Resolution. The last item (4) allows us to show an exponential separation between our new system and the previously studied reductionless long-distance resolution calculus. Our approach also naturally lifts to dependency QBFs (DQBF), where it yields the first sound and complete CDCL-style calculus for DQBF, thus opening future avenues into CDCL-based DQBF solving.																	0168-7433	1573-0670															10.1007/s10817-020-09560-1		MAY 2020											
J								CATTELL'S parallel proportional profiles The triumph of a prodigal rotation	JOURNAL OF CHEMOMETRICS										factor rotation; invariant factors model; PARAFAC; CANDECOMP; parallel proportional profiles; structural equation models	MULTIVARIATE LONGITUDINAL DATA; PRINCIPAL COMPONENT ANALYSIS; 3-MODE FACTOR-ANALYSIS; INDIVIDUAL-DIFFERENCES; MODELS; DECOMPOSITION; UNIQUENESS; PARAFAC; COMPLEXITY; INVARIANCE	In a primarily informal and conceptual way we trace the history of Cattell's parallel proportional profiles principle for factor rotation, also known as confactor rotation. Its original idea in connection with standard and confirmatory factor analysis of sets of covariance matrices from random samples is discussed as is its use in the non-stochastic framework of three-mode analysis. It will be shown that unfortunately the principle as an alternative for simple structure rotation has not led to a wide-spread use in the social and behavioral sciences, but that it has celebrated triumphs in sciences like chemistry, signal processing, and other physical sciences mostly under the flag of the PARAFAC/CANDECOMP model, tensor decomposition, or canonical polyadic decomposition.																	0886-9383	1099-128X														e3235	10.1002/cem.3235		MAY 2020											
J								Accurate and robust tracking of rigid objects in real time	JOURNAL OF REAL-TIME IMAGE PROCESSING										Visual object tracking; Real-time tracking; Shape model tracking	RECOGNITION; LANGUAGE; COMPILER	We present the shape model object tracker, which is accurate, robust, and real-time capable on a standard CPU. The tracker has a failure mode detection, is robust to nonlinear illumination changes, and can cope with occlusions. It uses subpixel-precise image edges to track roughly rigid objects with high accuracy and is virtually drift-free even for long sequences. Furthermore, it is inherently capable of object re-detection when tracking fails. To evaluate the accuracy, robustness, and efficiency of the tracker precisely, we present a challenging new tracking dataset with pixel-precise ground truth. The precise ground-truth labels are created automatically from the photo-realistic synthetic VIPER dataset. The tracker is thoroughly evaluated against the state of the art through a number of qualitative and quantitative experiments. It is able to perform on par with the current state-of-the-art deep-learning trackers, but is at least 45 times faster, even without using a GPU. The efficiency and low memory consumption of the tracker are validated in further experiments that are conducted on an embedded device.																	1861-8200	1861-8219															10.1007/s11554-020-00978-9		MAY 2020											
J								A proposed decentralized formation control algorithm for robot swarm based on an optimized potential field method	NEURAL COMPUTING & APPLICATIONS										Potential field method; Neural network optimization; Swarm robotics; Formation control	COOPERATIVE CONTROL; AVOIDANCE; SYSTEMS	Lately, robot swarm has widely employed in many applications like search and rescue missions, fire forest detection and navigation in hazard environments. Each robot in a swarm is supposed to move without collision and avoid obstacles while performing the assigned job. Therefore, a formation control is required to achieve the robot swarm three tasks. In this article, we introduce a decentralized formation control algorithm based on the potential field method for robot swarm. Our formation control algorithm is proposed to achieve the three tasks: avoid obstacles in the environment, keep a fixed distance among robots to maintain a formation and perform an assigned task. An artificial neural network is engaged in the online optimization of the parameters of the potential force. Then, real-time experiments are conducted to confirm the reliability and applicability of our proposed decentralized formation control algorithm. The real-time experiment results prove that the proposed decentralized formation control algorithm enables the swarm to avoid obstacles and maintain formation while performing a certain task. The swarm manages to reach a certain goal and tracks a given trajectory. Moreover, the proposed decentralized formation control algorithm enables the swarm to escape from local minima, to pass through two narrow placed obstacles without oscillation near them. From a comparison between the proposed decentralized formation control algorithm and the traditional PFM, we obtained that NN-swarm successes to reach its goal with average accuracy 0.14 m compared to 0.22 m for the T-swarm. The NN-swarm also keeps a fixed distance between robots with a higher swarming error reaches 34.83%, while the T-swarm reaches 23.59%. Also, the NN-swarm is more accurate in tracking a trajectory with a higher tracking error reaches 0.0086 m compared to min. error of T-swarm equals to 0.01 m. Besides, the NN-swarm maintains formation much longer than T-swarm while tracking trajectory reaches 94.31% while the T-swarm reaches 81.07% from the execution time, in environments with different numbers of obstacles.																	0941-0643	1433-3058															10.1007/s00521-020-05032-0		MAY 2020											
J								Brain-computer interface for amyotrophic lateral sclerosis patients using deep learning network	NEURAL COMPUTING & APPLICATIONS										Electrooculography; Amyotrophic lateral sclerosis; Convolution neural network; Cross power spectral density; Human-computer interface; Brain-computer interface	EYE-MOVEMENTS; SYSTEM; ELECTROOCULOGRAPHY	Individuals with Motor Neuron Disease were unable to move from one place to another because it gradually reduced all the voluntarily movement due to the degeneration of upper and lower motors neurons. The solution to this problem was to develop rehabilitating devices using biosignals. In this study, we have designed and developed electrooculogram-based wheelchair control using Cross Power Spectral Density. The convolution neural network to verify the performance and recognition accuracy of the wheelchair navigation in the indoor environment by using four trained users and four untrained users between the different age-groups and obtained the accuracy of 91.18% and 86.88% by using four fundamental tasks. From the indoor performance, the subject S4 from trained users outperforms all the trained subjects with an average classification accuracy of 93.51%. To verify the recognition accuracy, we conducted the online performance from the online performances subject S4 from trained subjects outperforms remaining trained subjects at the same time the subject S6 from untrained subjects outperforms all the untrained subjects. From the entire study, we analyzed that classification accuracy of subjects S4 was appreciated compared to other subjects. Through the research, we confirmed that the entire trained subject's performance was maximum compared to the untrained subjects in all the circumstances.																	0941-0643	1433-3058															10.1007/s00521-020-05026-y		MAY 2020											
J								Cross-Domain Polarity Models to Evaluate User eXperience in E-learning	NEURAL PROCESSING LETTERS										Machine learning; Artificial neural networks; Sentiment analysis; User experience; Virtual learning environments; Learning management systems	RELIABILITY; UX	Virtual learning environments are growing in importance as fast as e-learning is becoming highly demanded by universities and students all over the world. This paper investigates how to automatically evaluate User eXperience in this domain using sentiment analysis techniques. For this purpose, a corpus with the opinions given by a total of 583 users (107 English speakers and 476 Spanish speakers) about three learning management systems in different courses has been built. All the collected opinions were manually labeled with polarity information (positive, negative or neutral) by three human annotators, both at the whole opinion and sentence levels. We have applied our state-of-the-art sentiment analysis models, trained with a corpus of a different semantic domain (a Twitter corpus), to study the use of cross-domain models for this task. Cross-domain models based on deep neural networks (convolutional neural networks, transformer encoders and attentional BLSTM models) have been tested. In order to contrast our results, three commercial systems for the same task (MeaningCloud, Microsoft Text Analytics and Google Cloud) were also tested. The obtained results are very promising and they give an insight to keep going the research of applying sentiment analysis tools on User eXperience evaluation. This is a pioneering idea to provide a better and accurate understanding on human needs in the interaction with virtual learning environments and a step towards the development of automatic tools that capture the feed-back of user perception for designing virtual learning environments centered in user's emotions, beliefs, preferences, perceptions, responses, behaviors and accomplishments that occur before, during and after the interaction.																	1370-4621	1573-773X															10.1007/s11063-020-10260-5		MAY 2020											
J								APSO-MVS: an adaptive particle swarm optimization incorporating multiple velocity strategies for optimal leader selection in hybrid MANETs	SOFT COMPUTING										Mobile ad hoc network; Auto-configuration; Particle swarm optimization (PSO); Duplicate address detection (DAD); Velocity updation	KRILL HERD ALGORITHM; AUTOCONFIGURATION; CONFIGURATION; PROTOCOL; EFFICIENT; SCHEME	In this paper, we propose a hierarchical topological-based auto-configuration scheme for MANETs providing global internet connectivity among leader and member nodes to reduce the control overhead. The proposed scheme has performed the duplication address detection (DAD) operation through selecting a pre-configured node called coordinator node by a new joining cluster node. Hence, the overhead is reduced by the elimination of DAD messages broadcasting in the whole network. Also, the clustering problem in MANETs is solved by introducing a new adaptive particle swarm optimization with multiple velocity strategy (APSO-MVS) algorithm for a new leader selection with the frequent departure and failure of a leader node. However, to enhance the robustness and global searching ability of classical PSO, the three new velocity updating strategies are used in a newly developed APSO-MVS algorithm. This proposed APSO-MVS algorithm has considered multiple node metrics (node distance from the cluster group centre, node speed and node density) for the selection of an optimal leader node. Simulation results have proved the efficacy of proposed protocol in overhead reduction compared to other existing auto-configuration protocols and in terms of 15 benchmark test functions.																	1432-7643	1433-7479															10.1007/s00500-020-05034-z		MAY 2020											
J								P-MOIA-RS: a multi-objective optimization and decision-making algorithm for recommendation systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommendation accuracy; Recommendation diversity; Multi-objective immune optimization; PARETO Refinement; PROMETHEE; Recommendation system	PERSONALIZED RECOMMENDATION; EVOLUTIONARY ALGORITHM	Besides accuracy, diversity of recommendation list is also important for users. Hence, the optimization of the recommendation system can be abstracted as a multi-objective problem because accuracy and diversity are contradictory goals. Available multi-objective optimization based recommendation schemes return the Pareto set for the target users. However, the scale of Pareto solutions is uncontrollable. If a Pareto set contains too many solutions, it will not be quite useful for users to make the final decision. In this paper, multi-objective immune algorithm is used to improve recommendation accuracy and diversity, then we can get the pareto set. Further, we introduce PROMETHEE into the recommendation system to get a more precise evaluation of Pareto solutions. By combining PROMETHEE with Pareto, we redefine recommendation as Top-n PROMETHEE Pareto optimization problem and a multi-objective immune optimization and decision-making algorithm is presented. The experimental results show that the proposed algorithm, compared with other existing algorithms, can generate more diverse and accurate recommendation list and provide more precise decision-making for the user.																	1868-5137	1868-5145															10.1007/s12652-020-01997-x		MAY 2020											
J								Analysis of the interaction between elderly people and a simulated virtual coach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human behavior analysis; Human-machine interaction; Spanish; Emotional analysis from speech; Language and face	SCIENCE	The EMPATHIC project develops and validates new interaction paradigms for personalized virtual coaches (VC) to promote healthy and independent aging. To this end, the work presented in this paper is aimed to analyze the interaction between the EMPATHIC-VC and the users. One of the goals of the project is to ensure an end-user driven design, involving senior users from the beginning and during each phase of the project. Thus, the paper focuses on some sessions where the seniors carried out interactions with a Wizard of Oz driven, simulated system. A coaching strategy based on the GROW model was used throughout these sessions so as to guide interactions and engage the elderly with the goals of the project. In this interaction framework, both the human and the system behavior were analyzed. The way the wizard implements the GROW coaching strategy is a key aspect of the system behavior during the interaction. The language used by the virtual agent as well as his or her physical aspect are also important cues that were analyzed. Regarding the user behavior, the vocal communication provides information about the speaker's emotional status, that is closely related to human behavior and which can be extracted from the speech and language analysis. In the same way, the analysis of the facial expression, gazes and gestures can provide information on the non verbal human communication even when the user is not talking. In addition, in order to engage senior users, their preferences and likes had to be considered. To this end, the effect of the VC on the users was gathered by means of direct questionnaires. These analyses have shown a positive and calm behavior of users when interacting with the simulated virtual coach as well as some difficulties of the system to develop the proposed coaching strategy.																	1868-5137	1868-5145															10.1007/s12652-020-01983-3		MAY 2020											
J								Bio matter in creative practises for fashion and design	AI & SOCIETY										Bio textile; Acetobacterxylinum; Alternative methods; Bacterial cellulose; Fashion; Sustainability		Through an examination of the bacteria that produce the cellulose, an investigation of the growing process and properties, and a discussion of an artistic exploration, one can fully grasp bio cellulose's potential in becoming a synergist for sustainable fashion. This new creative and radical approach re-imagines the future materials for fashion and other fields requiring textile applications that are grown and renewable. Questions how textile can be created to be sustainable, biodegradable and infinitely reusable and mainly what the paradigms for the new aesthetics are. The aim of this article is to convey the need for new biomaterials inspired by nature.																	0951-5666	1435-5655															10.1007/s00146-020-00957-5		MAY 2020											
