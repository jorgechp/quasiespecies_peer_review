PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								High-Dimensional Poisson Structural Equation Model Learning via l(1)-Regularized Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian Networks; Directed Acyclic Graph; Identifiability; Structure Learning; l(1)-Regularization; Multivariate Count Distribution	BAYESIAN NETWORKS	In this paper, we develop a new approach to learning high-dimensional Poisson structural equation models from only observational data without strong assumptions such as faithfulness and a sparse moralized graph. A key component of our method is to decouple the ordering estimation or parent search where the problems can be efficiently addressed using l(1)-regularized regression and the moments relation. We show that sample size n = Omega(d(2) log(9) p) is sufficient for our polynomial time Moments Ratio Scoring (MRS) algorithm to recover the true directed graph, where p is the number of nodes and d is the maximum indegree. We verify through simulations that our algorithm is statistically consistent in the high-dimensional p > n setting, and performs well compared to state-of-the-art ODS, GES, and MMHC algorithms. We also demonstrate through multivariate real count data that our MRS algorithm is well-suited to estimating DAG models for multivariate count data in comparison to other methods used for discrete data.																	1532-4435						2019	20								95														
J								Exact Clustering of Weighted Graphs via Semidefinite Programming	JOURNAL OF MACHINE LEARNING RESEARCH										clustering; densest subgraph; stochastic block models; semidefinite programming; sparse graphs	RECOVERY; CLIQUE; NORM	As a model problem for clustering, we consider the densest k-disjoint-clique problem of partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of k large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of nodes not belonging to any cluster. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size n of the graph tends to infinity, we can recover clusters of size polylogarithmic in n under certain conditions on the distribution of edge weights. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure.																	1532-4435						2019	20								30														
J								Complete Search for Feature Selection in Decision Trees	JOURNAL OF MACHINE LEARNING RESEARCH										Feature Selection; Decision Trees; Wrapper models; Complete Search	CLASSIFICATION	The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features. We design an exact enumeration procedure of the subsets of features that lead to all and only the distinct decision trees built by a greedy top-down decision tree induction algorithm. The procedure stores, in the worst case, a number of trees linear in the number of features. By exploiting a further pruning of the search space, we design a complete procedure for finding delta-acceptable feature subsets, which depart by at most delta from the best estimated error over any feature subset. Feature subsets with the best estimated error are called best feature subsets. Our results apply to any error estimator function, but experiments are mainly conducted under the wrapper model, in which the misclassification error over a search set is used as an estimator. The approach is also adapted to the design of a computational optimization of the sequential backward elimination heuristic, extending its applicability to large dimensional datasets. The procedures of this paper are implemented in a multi-core data parallel C++ system. We investigate experimentally the properties and limitations of the procedures on a collection of 20 benchmark datasets, showing that oversearching increases both overfitting and instability.																	1532-4435						2019	20								104														
J								Regularization via Mass Transportation	JOURNAL OF MACHINE LEARNING RESEARCH										Distributionally robust optimization; optimal transport; Wasserstein distance; robust optimization; regularization	DISTRIBUTIONALLY ROBUST OPTIMIZATION; SUPPORT; REGRESSION; ALGORITHM; DISTANCE	The goal of regression and classification methods in supervised learning is to minimize the empirical risk, that is, the expectation of some loss function quantifying the prediction error under the empirical distribution. When facing scarce training data, over fitting is typically mitigated by adding regularization terms to the objective that penalize hypothesis complexity. In this paper we introduce new regularization techniques using ideas from distributionally robust optimization, and we give new probabilistic interpretations to existing techniques. Specifically, we propose to minimize the worst-case expected loss, where the worst case is taken over the ball of all (continuous or discrete) distributions that have a bounded transportation distance from the (discrete) empirical distribution. By choosing the radius of this ball judiciously, we can guarantee that the worst-case expected loss provides an upper confidence bound on the loss on test data, thus offering new generalization bounds. We prove that the resulting regularized learning problems are tractable and can be tractably kernelized for many popular loss functions. The proposed approach to regluarization is also extended to neural networks. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.																	1532-4435						2019	20								103														
J								Low Permutation-rank Matrices: Structural Properties and Noisy Completion	JOURNAL OF MACHINE LEARNING RESEARCH										Non-negative matrix completion; recommender systems; permutation-based model; minimax theory; oracle inequalities	DECOMPOSITIONS; FACTORIZATION; RECOVERY	We consider the problem of noisy matrix completion, in which the goal is to reconstruct a structured matrix whose entries are partially observed in noise. Standard approaches to this underdetermined inverse problem are based on assuming that the underlying matrix has low rank, or is well-approximated by a low rank matrix. In this paper, we propose a richer model based on what we term the "permutation-rank" of a matrix. We first describe how the classical non-negative rank model enforces restrictions that may be undesirable in practice, and how and these restrictions can be avoided by using the richer permutation-rank model. Second, we establish the minimax rates of estimation under the new permutation-based model, and prove that surprisingly, the minimax rates are equivalent up to logarithmic factors to those for estimation under the typical low rank model. Third, we analyze a computationally efficient singular-value-thresholding algorithm, known to be optimal for the low-rank setting, and show that it also simultaneously yields a consistent estimator for the low-permutation rank setting. Finally, we present various structural results characterizing the uniqueness of the permutation-rank decomposition, and characterizing convex approximations of the permutation-rank polytope.																	1532-4435						2019	20								101														
J								Measuring the Effects of Data Parallelism on Neural Network Training	JOURNAL OF MACHINE LEARNING RESEARCH										neural networks; stochastic gradient descent; data parallelism; batch size; deep learning		Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.																	1532-4435						2019	20								112														
J								Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics	JOURNAL OF MACHINE LEARNING RESEARCH										Online learning; reproducing kernel Hilbert space; multi-kernel learning; random features; dynamic and adversarial environments		Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. Especially when the latter is not available, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops a scalable multi-kernel learning scheme (termed Raker) to obtain the sought nonlinear learning function 'on the fly,' first for static environments. To further boost performance in dynamic environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed. AdaRaker accounts not only for data-driven learning of kernel combination, but also for the unknown dynamics. Performance is analyzed in terms of both static and dynamic regrets. AdaRaker is uniquely capable of tracking nonlinear learning functions in environments with unknown dynamics, and with with analytic performance guarantees. Tests with synthetic and real datasets are carried out to showcase the effectiveness of the novel algorithms.(1)																	1532-4435						2019	20								22														
J								Determining the Number of Latent Factors in Statistical Multi-Relational Learning	JOURNAL OF MACHINE LEARNING RESEARCH										Information criteria; Knowledge graph; Model selection consistency; RESCAL model; Statistical relational learning; Tensor factorization		Statistical relational learning is primarily concerned with learning and inferring relationships between entities in large-scale knowledge graphs. Nickel et al. (2011) proposed a RESCAL tensor factorization model for statistical relational learning, which achieves better or at least comparable results on common benchmark data sets when compared to other state-of-the-art methods. Given a positive integer s, RESCAL computes an s-dimensional latent vector for each entity. The latent factors can be further used for solving relational learning tasks, such as collective classification, collective entity resolution and link-based clustering. The focus of this paper is to determine the number of latent factors in the RESCAL model. Due to the structure of the RESCAL model, its log-likelihood function is not concave. As a result, the corresponding maximum likelihood estimators (MLEs) may not be consistent. Nonetheless, we design a specific pseudometric, prove the consistency of the MLEs under this pseudometric and establish its rate of convergence. Based on these results, we propose a general class of information criteria and prove their model selection consistencies when the number of relations is either bounded or diverges at a proper rate of the number of entities. Simulations and real data examples show that our proposed information criteria have good finite sample properties.																	1532-4435						2019	20								23														
J								Learning Unfaithful K-separable Gaussian Graphical Models	JOURNAL OF MACHINE LEARNING RESEARCH										Gaussian graphical model selection; separable graphs; high-dimensional statistical learning; faithful conditional independence relations; structural consistency	SELECTION	The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set S graph separates nodes u and v then X-u is conditionally independent of X-v given X-S. The opposite direction need not be true, that is, X-u perpendicular to X-v vertical bar X-S need not imply S is a node separator of u and v. When it does, the relation X-u perpendicular to X-v vertical bar X-S is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form X-i perpendicular to X-j vertical bar X-S. We study two classes of separable Gaussian graphical models, namely, weakly K-separable and strongly K- separable Gaussian graphical models. Using the above test for faithfulness, we introduce algorithms to learn the topologies of weakly K-separable and strongly K-separable Gaussian graphical models with Omega (K log p) sample complexity. For strongly K-separable Gaussian graphical models, we additionally provide a method with error bounds for learning the off-diagonal precision matrix entries.																	1532-4435						2019	20								109														
J								Optimal Transport: Fast Probabilistic Approximation with Exact Solvers	JOURNAL OF MACHINE LEARNING RESEARCH										computational vs statistical accuracy; covering numbers; empirical optimal transport; resampling; risk bounds; spanning tree; Wasserstein distance	WASSERSTEIN; CONVERGENCE; BARYCENTERS; DISTANCE	We propose a simple subsampling scheme for fast randomized approximate computation of optimal transport distances on finite spaces. This scheme operates on a random subset of the full data and can use any exact algorithm as a black-box back-end, including state-of-the-art solvers and entropically penalized versions. It is based on averaging the exact distances between empirical measures generated from independent samples from the original measures and can easily be tuned towards higher accuracy or shorter computation times. To this end, we give non-asymptotic deviation bounds for its accuracy in the case of discrete optimal transport problems. In particular, we show that in many important instances, including images (2D-histograms), the approximation error is independent of the size of the full problem. We present numerical experiments that demonstrate that a very good approximation in typical applications can be obtained in a computation time that is several orders of magnitude smaller than what is required for exact computation of the full problem.																	1532-4435						2019	20								105														
J								Transport Analysis of Infinitely Deep Neural Network	JOURNAL OF MACHINE LEARNING RESEARCH										representation learning; denoising autoencoder; flow representation; continuum limit; backward heat equation; Wasserstein geometry; ridgelet analysis	REPRESENTATION; BOUNDS	We investigated the feature map inside deep neural networks (DNNs) by tracking the transport map. We are interested in the role of depth - why do DNNs perform better than shallow models? - and the interpretation of DNNs- what do intermediate layers do? Despite the rapid development in their application, DNNs remain analytically unexplained because the hidden layers are nested and the parameters are not faithful. Inspired by the integral representation of shallow NNs, which is the continuum limit of the width, or the hidden unit number, we developed the flow representation and transport analysis of DNNs. The flow representation is the continuum limit of the depth, or the hidden layer number, and it is specified by an ordinary differential equation ( ODE) with a vector field. We interpret an ordinary DNN as a transport map or an Euler broken line approximation of the flow. Technically speaking, a dynamical system is a natural model for the nested feature maps. In addition, it opens a new way to the coordinate-free treatment of DNNs by avoiding the redundant parametrization of DNNs. Following Wasserstein geometry, we analyze a flow in three aspects: dynamical system, continuity equation, and Wasserstein gradient flow. A key finding is that we specified a series of transport maps of the denoising autoencoder (DAE), which is a cornerstone for the development of deep learning. Starting from the shallow DAE, this paper develops three topics: the transport map of the deep DAE, the equivalence between the stacked DAE and the composition of DAEs, and the development of the double continuum limit or the integral representation of the flow representation. As partial answers to the research questions, we found that deeper DAEs converge faster and the extracted features are better; in addition, a deep Gaussian DAE transports mass to decrease the Shannon entropy of the data distribution. We expect that further investigations on these questions lead to the development of an interpretable and principled alternatives to DNNs.																	1532-4435						2019	20								2														
J								An asymptotic analysis of distributed nonparametric methods	JOURNAL OF MACHINE LEARNING RESEARCH										distributed learning; nonparametric models; high-dimensional models; Gaussian processes; convergence rates	CONFIDENCE BANDS; EQUIVALENCE; COVERAGE; BAYES	We investigate and compare the fundamental performance of several distributed learning methods that have been proposed recently. We do this in the context of a distributed version of the classical signal-in-Gaussian-white-noise model, which serves as a benchmark model for studying performance in this setting. The results show how the design and tuning of a distributed method can have great impact on convergence rates and validity of uncertainty quantification. Moreover, we highlight the difficulty of designing nonparametric distributed procedures that automatically adapt to smoothness.																	1532-4435						2019	20								87														
J								scikit-multilearn: A scikit-based Python environment for performing multi-label classification	JOURNAL OF MACHINE LEARNING RESEARCH										Python; multi-label classification; label-space clustering; multi-label embedding; multi-label stratification		The scikit-multilearn is a Python library for performing multi-label classification. It is compatible with the scikit-learn and scipy ecosystems and uses sparse matrices for all internal operations; provides native Python implementations of popular multi-label classification methods alongside a novel framework for label space partitioning and division and includes modern algorithm adaptation methods, network-based label space division approaches, which extracts label dependency information and multi-label embedding classifiers. The library provides Python wrapped access to the extensive multi-label method stack from Java libraries and makes it possible to extend deep learning single-label methods for multi-label tasks. The library allows multi-label stratification and data set management. The implementation is more efficient in problem transformation than other established libraries, has good test coverage and follows PEP8. Source code and documentation can be downloaded from http://scikit.m1 and also via pip. The project is BSD-licensed.																	1532-4435						2019	20								6														
J								Bayesian Space-Time Partitioning by Sampling and Pruning Spanning Trees	JOURNAL OF MACHINE LEARNING RESEARCH										Spatial Clustering; Product Partition Models; Random Spanning Trees; Bayesian Clustering	MARKOV-CHAIN; MODEL; ALGORITHMS; CLUSTERS	A typical problem in spatial data analysis is regionalization or spatially constrained clustering, which consists of aggregating small geographical areas into larger regions. A major challenge when partitioning a map is the huge number of possible partitions that compose the search space. This is compounded if we are partitioning spatio-temporal data rather than purely spatial data. We introduce a spatio-temporal product partition model that deals with the regionalization problem in a probabilistic way. Random spanning trees are used as a tool to tackle the problem of searching the space of possible partitions making feasible this exploration. Based on this framework, we propose an efficient Gibbs sampler algorithm to sample from the posterior distribution of the parameters, specially the random partition. The proposed Gibbs sampler scheme carries out a random walk on the space of the spanning trees and the partitions induced by deleting tree edges. In the purely spatial situation, we compare our proposed model with other state-of-art regionalization techniques to partition maps using simulated and real social and health data. To illustrate how the temporal component is handled by the algorithm and to show how the spatial clusters vary along the time we presented an application using human development index data. The analysis shows that our proposed model is better than state-of-art alternatives. Another appealing feature of the method is that the prior distribution for the partition is interpretable with a trivial coin flipping mechanism allowing its easy elicitation.																	1532-4435						2019	20								85														
J								A Representer Theorem for Deep Neural Networks	JOURNAL OF MACHINE LEARNING RESEARCH										splines; regularization; sparsity; learning; deep neural networks; activation functions	LINEAR INVERSE PROBLEMS; SPLINES; KERNELS	We propose to optimize the activation functions of a deep neural network by adding a corresponding functional regularization to the cost function. We justify the use of a second-order total-variation criterion. This allows us to derive a general representer theorem for deep neural networks that makes a direct connection with splines and sparsity. Specifically, we show that the optimal network configuration can be achieved with activation functions that are nonuniform linear splines with adaptive knots. The bottom line is that the action of each neuron is encoded by a spline whose parameters (including the number of knots) are optimized during the training procedure. The scheme results in a computational structure that is compatible with existing deep-ReLU, parametric ReLU, APL (adaptive piecewise-linear) and MaxOut architectures. It also suggests novel optimization challenges and makes an explicit link with l(1) minimization and sparsity-promoting techniques.																	1532-4435						2019	20								110														
J								The Common-directions Method for Regularized Empirical Risk Minimization	JOURNAL OF MACHINE LEARNING RESEARCH											FINITE NEWTON METHOD	State-of-the-art first- and second-order optimization methods are able to achieve either fast global linear convergence rates or quadratic convergence, but not both of them. In this work, we propose an interpolation between first- and second-order methods for regularized empirical risk minimization that exploits the problem structure to efficiently combine multiple update directions. Our method attains both optimal global linear convergence rate for first-order methods, and local quadratic convergence. Experimental results show that our method outperforms state-of-the-art first- and second-order optimization methods in terms of the number of data accesses, while is competitive in training time.																	1532-4435						2019	20								58														
J								Scalable Kernel K-Means Clustering with Nystrom Approximation: Relative-Error Bounds	JOURNAL OF MACHINE LEARNING RESEARCH										kernel k-means clustering; the Nystrom method; randomized linear algebra	MATRIX; ALGORITHMS; SEGMENTATION	Kernel k-means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear k-means clustering algorithm. However, kernel k-means clustering is computationally expensive when the non-linear feature map is high-dimensional and there are many input points. Kernel approximation, e.g., the Nystrom method, has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present. This work analyzes the application of this paradigm to kernel k-means clustering, and shows that applying the linear k-means clustering algorithm to k/c(1 + o(1)) features constructed using a so-called rank-restricted Nystrom approximation results in cluster assignments that satisfy a 1+ epsilon approximation ratio in terms of the kernel k-means cost function, relative to the guarantee provided by the same algorithm without the use of the Nystrom method. As part of the analysis, this work establishes a novel 1+ epsilon relative-error trace norm guarantee for low-rank approximation using the rank-restricted Nystrom approximation. Empirical evaluations on the 8 : 1 million instance MNIST8M dataset demonstrate the scalability and usefulness of kernel k-means clustering with Nystrom approximation. This work argues that spectral clustering using Nystrom approximation a popular and computationally efficient, but theoretically unsound approach to non-linear clustering-should be replaced with the efficient and theoretically sound combination of kernel k-means clustering with Nystrom approximation. The superior performance of the latter approach is empirically verified.																	1532-4435						2019	20								12														
J								Robust Estimation of Derivatives Using Locally Weighted Least Absolute Deviation Regression	JOURNAL OF MACHINE LEARNING RESEARCH										composite quantile regression; differenced method; LowLAD; LowLSR; outlier and heavy-tailed error; robust nonparametric derivative estimation	COMPOSITE QUANTILE REGRESSION; DIFFERENCE SEQUENCE; EFFICIENT; VARIANCE; SIZER	In nonparametric regression, the derivative estimation has attracted much attention in recent years due to its wide applications. In this paper, we propose a new method for the derivative estimation using the locally weighted least absolute deviation regression. Different from the local polynomial regression, the proposed method does not require a finite variance for the error term and so is robust to the presence of heavy-tailed errors. Meanwhile, it does not require a zero median or a positive density at zero for the error term in comparison with the local median regression. We further show that the proposed estimator with random difference is asymptotically equivalent to the (infinitely) composite quantile regression estimator. In other words, running one regression is equivalent to combining infinitely many quantile regressions. In addition, the proposed method is also extended to estimate the derivatives at the boundaries and to estimate higher-order derivatives. For the equidistant design, we derive theoretical results for the proposed estimators, including the asymptotic bias and variance, consistency, and asymptotic normality. Finally, we conduct simulation studies to demonstrate that the proposed method has better performance than the existing methods in the presence of outliers and heavy-tailed errors, and analyze the Chinese house price data for the past ten years to illustrate the usefulness of the proposed method.																	1532-4435						2019	20								60														
J								Distributed Inference for Linear Support Vector Machine	JOURNAL OF MACHINE LEARNING RESEARCH										Linear support vector machine; distributed inference; Bahadur representation; asymptotic theory	REGRESSION; CLASSIFICATION; QUANTILES; BEHAVIOR	The growing size of modern data brings many new challenges to existing statistical inference methodologies and theories, and calls for the development of distributed inferential approaches. This paper studies distributed inference for linear support vector machine (SVM) for the binary classification task. Despite a vast literature on SVM, much less is known about the inferential properties of SVM, especially in a distributed setting. In this paper, we propose a multi-round distributed linear-type (MDL) estimator for conducting inference for linear SVM. The proposed estimator is computationally efficient. In particular, it only requires an initial SVM estimator and then successively refines the estimator by solving simple weighted least squares problem. Theoretically, we establish the Bahadur representation of the estimator. Based on the representation, the asymptotic normality is further derived, which shows that the MDL estimator achieves the optimal statistical efficiency, i.e., the same efficiency as the classical linear SVM applying to the entire data set in a single machine setup. Moreover, our asymptotic result avoids the condition on the number of machines or data batches, which is commonly assumed in distributed estimation literature, and allows the case of diverging dimension. We provide simulation studies to demonstrate the performance of the proposed MDL estimator.																	1532-4435						2019	20								113														
J								Maximum Likelihood for Gaussian Process Classification and Generalized Linear Mixed Models under Case-Control Sampling	JOURNAL OF MACHINE LEARNING RESEARCH										Gaussian Processes; Expectation Propagation; Composite Likelihood; Selection Bias; Linear Mixed Models	REGRESSION-MODELS; APPROXIMATE INFERENCE; BAYESIAN-INFERENCE; LONGITUDINAL DATA; BINARY TRAITS; GENETIC-RISK; ASSOCIATION; DISEASE; HERITABILITY; PREDICTION	Modern data sets in various domains often include units that were sampled non-randomly from the population and have a latent correlation structure. Here we investigate a common form of this setting, where every unit is associated with a latent variable, all latent variables are correlated, and the probability of sampling a unit depends on its response. Such settings often arise in case-control studies, where the sampled units are correlated due to spatial proximity, family relations, or other sources of relatedness. Maximum likelihood estimation in such settings is challenging from both a computational and statistical perspective, necessitating approximations that take the sampling scheme into account. We propose a family of approximate likelihood approaches which combine composite likelihood and expectation propagation. We demonstrate the efficacy of our solutions via extensive simulations. We utilize them to investigate the genetic architecture of several complex disorders collected in case-control genetic association studies, where hundreds of thousands of genetic variants are measured for every individual, and the underlying disease liabilities of individuals are correlated due to genetic similarity. Our work is the first to provide a tractable likelihood-based solution for case-control data with complex dependency structures.																	1532-4435						2019	20								108														
J								Dependent relevance determination for smooth and structured sparse regression	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian nonparametric; Sparsity; Structure learning; Gaussian Process; fMRI	VARIABLE SELECTION; ALZHEIMERS-DISEASE; SHRINKAGE; THALAMUS; AREA; FMRI	In many problem settings, parameter vectors are not merely sparse but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as "region sparsity." Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging.																	1532-4435						2019	20								89														
J								The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising	JOURNAL OF MACHINE LEARNING RESEARCH										HOSVD; Entry-wise perturbation; Gaussian noise; High dimensional clustering	DECOMPOSITIONS; EIGENVECTORS; BOUNDS	The higher order singular value decomposition (HOSVD) of tensors is a generalization of matrix SVD. The perturbation analysis of HOSVD under random noise is more delicate than its matrix counterpart. Recently, polynomial time algorithms have been proposed where statistically optimal estimates of the singular subspaces and the low rank tensors are attainable in the Euclidean norm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and introduce estimators of the singular subspaces with sharp deviation bounds in the sup-norm. We also investigate a low rank tensor denoising estimator and demonstrate its fast convergence rate with respect to the entry-wise errors. The sup-norm perturbation bounds reveal unconventional phase transitions for statistical learning applications such as the exact clustering in high dimensional Gaussian mixture model and the exact support recovery in sub-tensor localizations. In addition, the bounds established for HOSVD also elaborate the one-sided sup-norm perturbation bounds for the singular subspaces of unbalanced (or fat) matrices.																	1532-4435						2019	20								61														
J								Relative Error Bound Analysis for Nuclear Norm Regularized Matrix Completion	JOURNAL OF MACHINE LEARNING RESEARCH										matrix completion; nuclear norm regularization; least squares; low-rank; full-rank; relative error bound		In this paper, we develop a relative error bound for nuclear norm regularized matrix completion, with the focus on the completion of full-rank matrices. Under the assumption that the top eigenspaces of the target matrix are incoherent, we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix. Although multiple works have been devoted to analyzing the recovery error of full-rank matrix completion, their error bounds are usually additive, making it impossible to obtain the perfect recovery case and more generally difficult to leverage the skewed distribution of eigenvalues. Our analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion. To the best of our knowledge, this is the first relative bound that has been proved for the regularized formulation of matrix completion.																	1532-4435						2019	20								97														
J								Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local Minima in Nonconvex Matrix Recovery	JOURNAL OF MACHINE LEARNING RESEARCH										matrix factorization; nonconvex optimization; Restricted Isometry Property; matrix sensing; spurious local minima	RANK SOLUTIONS; COMPLETION; SIGNAL	Nonconvex matrix recovery is known to contain no spurious local minima under a restricted isometry property (RIP) with a sufficiently small RIP constant delta. If delta is too large, however, then counterexamples containing spurious local minima are known to exist. In this paper, we introduce a proof technique that is capable of establishing sharp thresholds on delta to guarantee the inexistence of spurious local minima. Using the technique, we prove that in the case of a rank-1 ground truth, an RIP constant of delta < 1/2 is both necessary and sufficient for exact recovery from any arbitrary initial point (such as a random point). We also prove a local recovery result: given an initial point x(0) satisfying f(x(0)) <= (1 - delta)(2) f(0), any descent algorithm that converges to second-order optimality guarantees exact recovery. Keywords: matrix factorization, nonconvex optimization, Restricted Isometry Property, matrix sensing, spurious local minima																	1532-4435						2019	20								114														
J								PyOD: A Python Toolbox for Scalable Outlier Detection	JOURNAL OF MACHINE LEARNING RESEARCH										anomaly detection; outlier detection; outlier ensembles; neural networks; machine learning; data mining; Python	SUPPORT	PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https : //github . com/yzhao062/pyod.																	1532-4435						2019	20								96														
J								Scalable Interpretable Multi-Response Regression via SEED	JOURNAL OF MACHINE LEARNING RESEARCH										reduced-rank Regression; scalability; high dimensionality; greedy algorithm; sparse eigenvector estimation	PRINCIPAL COMPONENT ANALYSIS; OPTIMAL RATES; SELECTION; CONSISTENCY; EIGENVALUE; FRAMEWORK; SHRINKAGE; ALGORITHM	Sparse reduced-rank regression is an important tool for uncovering meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-r sparse singular value decomposition from a generalized eigenvalue problem to find the optimal low-rank and sparse matrix estimate. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Moreover, SEED employs only basic matrix operations that can be efficiently parallelized in high performance computing devices. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.																	1532-4435						2019	20								107														
J								Multi-class Heterogeneous Domain Adaptation	JOURNAL OF MACHINE LEARNING RESEARCH										Heterogeneous domain adaptation; multi-class classification; compressed sensing	UNIQUE NONNEGATIVE SOLUTION; SPARSE RECOVERY; LASSO; KERNEL; MATRIX; BINARY	A crucial issue in heterogeneous domain adaptation (HDA) is the ability to learn a feature mapping between different types of features across domains. Inspired by language translation, a word translated from one language corresponds to only a few words in another language, we present an efficient method named Sparse Heterogeneous Feature Representation (SHFR) in this paper for multi-class HDA to learn a sparse feature transformation between domains with multiple classes. Specifically, we formulate the problem of learning the feature transformation as a compressed sensing problem by building multiple binary classifiers in the target domain as various measurement sensors, which are decomposed from the target multi-class classification problem. We show that the estimation error of the learned transformation decreases with the increasing number of binary classifiers. In other words, for adaptation across heterogeneous domains to be successful, it is necessary to construct a sufficient number of incoherent binary classifiers from the original multi-class classification problem. To achieve this, we propose to apply the error correcting output correcting (ECOC) scheme to generate incoherent classifiers. To speed up the learning of the feature transformation across domains, we apply an efficient batch-mode algorithm to solve the resultant nonnegative sparse recovery problem. Theoretically, we present a generalization error bound of our proposed HDA method under a multi-class setting. Lastly, we conduct extensive experiments on both synthetic and real-world datasets to demonstrate the superiority of our proposed method over existing state-of-the-art HDA methods in terms of prediction accuracy and training efficiency.																	1532-4435						2019	20								57														
J								Revisiting Counting Solutions for the Global Cardinality Constraint	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SEARCH	Counting solutions for a combinatorial problem has been identified as an important concern within the Artificial Intelligence field. It is indeed very helpful when exploring the structure of the solution space. In this context, this paper revisits the computation process to count solutions for the global cardinality constraint in the context of counting-based search. It first highlights an error and then presents a way to correct the upper bound on the number of solutions for this constraint.																	1076-9757	1943-5037					2019	66						411	441		10.1613/jair.1.11325													
J								Community Structure in Industrial SAT Instances	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											10 CHALLENGES	Modern SAT solvers have experienced a remarkable progress on solving industrial instances. It is believed that most of these successful techniques exploit the underlying structure of industrial instances. Recently, there have been some attempts to analyze the structure of industrial SAT instances in terms of complex networks, with the aim of explaining the success of SAT solving techniques, and possibly improving them. In this paper, we study the community structure, or modularity, of industrial SAT instances. In a graph with clear community structure, or high modularity, we can find a partition of its nodes into communities such that most edges connect variables of the same community. Representing SAT instances as graphs, we show that most application benchmarks are characterized by a high modularity. On the contrary, random SAT instances are closer to the classical Erdos-Renyi random graph model, where no structure can be observed. We also analyze how this structure evolves by the effects of the execution of a CDCL SAT solver, and observe that new clauses learned by the solver during the search contribute to destroy the original structure of the formula. Motivated by this observation, we finally present an application that exploits the community structure to detect relevant learned clauses, and we show that detecting these clauses results in an improvement on the performance of the SAT solver. Empirically, we observe that this improves the performance of several SAT solvers on industrial SAT formulas, especially on satisfiable instances.																	1076-9757	1943-5037					2019	66						443	472		10.1613/jair.1.11741													
J								Multi-agent Inverse Reinforcement Learning for Certain General-Sum Stochastic Games	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											ALGORITHM	This paper addresses a subset of problems of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Specifically, five variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurately estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local uCS, but also maximizes the scale of the solution. We apply a similar treatment to the problem of uNE-MIRL. The remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.																	1076-9757	1943-5037					2019	66						473	502		10.1613/jair.1.11541													
J								Synthesizing Argumentation Frameworks from Examples	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SET PROGRAMMING ENCODINGS; ABSTRACT ARGUMENTATION; COMPLEXITY; AGGREGATION; ALGORITHMS; SYSTEMS; ATTACK; MODEL	Argumentation is today a topical area of artificial intelligence (AI) research. Abstract argumentation, with argumentation frameworks (AFs) as the underlying knowledge representation formalism, is a central viewpoint to argumentation in AL Indeed, from the perspective of AI and computer science, understanding computational and representational aspects of AFs is key in the study of argumentation. Realizability of AFs has been recently proposed as a central notion for analyzing the expressive power of AFs under different semantics. In this work, we propose and study the AF synthesis problem as a natural extension of realizability, addressing some of the shortcomings arising from the relatively stringent definition of realizability. In particular, realizability gives means of establishing exact conditions on when a given collection of subsets of arguments has an AF with exactly the given collection as its set of extensions under a specific argumentation semantics. However, in various settings within the study of dynamics of argumentation-including revision and aggregation of AFs-non-realizability can naturally occur. To accommodate such settings, our notion of AF synthesis seeks to construct, or synthesize, AFs that are semantically closest to the knowledge at hand even when no AFs exactly representing the knowledge exist. Going beyond defining the AF synthesis problem, we study both theoretical and practical aspects of the problem. In particular, we (i) prove NP-completeness of AF synthesis under several semantics, (ii) study basic properties of the problem in relation to realizability, (iii) develop algorithmic solutions to NP-hard AF synthesis using the constraint optimization paradigms of maximum satisfiability and answer set programming, (iv) empirically evaluate our algorithms on different forms of AF synthesis instances, as well as (v) discuss variants and generalizations of AF synthesis.																	1076-9757	1943-5037					2019	66						503	554															
J								Acceptable Planning: Influencing Individual Behavior to Reduce Transportation Energy Expenditure of a City	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											TIME	Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce COPTER - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in COPTER that produces acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with a high fidelity multi-modal transportation simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.																	1076-9757	1943-5037					2019	66						555	587															
J								Variable Elimination in Binary CSPs	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											CONSTRAINT SATISFACTION; BROKEN TRIANGLES; OPTIMIZATION; CONSISTENCY; SEARCH	We investigate rules which allow variable elimination in binary CSP (constraint satisfaction problem) instances while conserving satisfiability. We study variable-elimination rules based on the language of forbidden patterns enriched with counting and quantification over variables and values. We propose new rules and compare them, both theoretically and experimentally. We give optimised algorithms to apply these rules and show that each defines a novel tractable class. Using our variable-elimination rules in preprocessing allowed us to solve more benchmark problems than without.																	1076-9757	1943-5037					2019	66						589	624															
J								On Non-Cooperativeness in Social Distance Games	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											STABILITY; OPTIMALITY; PRICE	We consider Social Distance Games (SDGs), that is cluster formation games in which the utility of each agent only depends on the composition of the cluster she belongs to, proportionally to her harmonic centrality, i.e., to the average inverse distance from the other agents in the cluster. Under a non-cooperative perspective, we adopt Nash stable outcomes, in which no agent can improve her utility by unilaterally changing her coalition, as the target solution concept. Although a Nash equilibrium for a SDG can always be computed in polynomial time, we obtain a negative result concerning the game convergence and we prove that computing a Nash equilibrium that maximizes the social welfare is NP-hard by a polynomial time reduction from the NP-complete Restricted Exact Cover by 3-Sets problem. We then focus on the performance of Nash equilibria and provide matching upper bound and lower bounds on the price of anarchy of Theta(n), where n is the number of nodes of the underlying graph. Moreover, we show that there exists a class of SDGs having a lower bound on the price of stability of 6/5 - epsilon, for any epsilon > 0. Finally, we characterize the price of stability of SDGs for graphs with girth 4 and girth at least 5, the girth being the length of the shortest cycle in the graph.																	1076-9757	1943-5037					2019	66						625	653		10.1613/jair.1.11808													
J								On the Time and Space Complexity of Genetic Programming for Evolving Boolean Conjunctions	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											DRIFT ANALYSIS; ALGORITHMS; OPTIMIZATION; MUTATION	Genetic programming (GP) is a general purpose bio-inspired meta-heuristic for the evolution of computer programs. In contrast to the several successful applications, there is little understanding of the working principles behind GP. In this paper we present a performance analysis that sheds light on the behaviour of simple GP systems for evolving conjunctions of n variables (AND(n)). The analysis of a random local search GP system with minimal terminal and function sets reveals the relationship between the number of iterations and the progress the GP makes toward finding the target function. Afterwards we consider a more realistic GP system equipped with a global mutation operator and prove that it can efficiently solve AND(n) by producing programs of linear size that fit a training set to optimality and with high probability generalise well. Additionally, we consider more general problems which extend the terminal set with undesired variables or negated variables. In the presence of undesired variables, we prove that, if non-strict selection is used, then the algorithm fits the complete training set efficiently while the strict selection algorithm may fail with high probability unless the substitution operator is switched off. If negations are allowed, we show that while the algorithms fail to fit the complete training set, the constructed solutions generalise well. Finally, from a problem hardness perspective, we reveal the existence of small training sets that allow the evolution of the exact conjunctions even with access to negations or undesired variables.																	1076-9757	1943-5037					2019	66						655	689		10.1613/jair.1.11821													
J								Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											TRANSLATION; OPINIONS	Sentiment analysis benefits from large, hand-annotated resources in order to train and test machine learning models, which are often data hungry. While some languages, e. g., English, have a vast array of these resources, most under-resourced languages do not, especially for fine-grained sentiment tasks, such as aspect-level or targeted sentiment analysis. To improve this situation, we propose a cross-lingual approach to sentiment analysis that is applicable to under-resourced languages and takes into account target-level information. This model incorporates sentiment information into bilingual distributional representations, by jointly optimizing them for semantics and sentiment, showing state-of-the-art performance at sentence-level when combined with machine translation. The adaptation to targeted sentiment analysis on multiple domains shows that our model outperforms other projection-based bilingual embedding methods on binary targeted sentiment tasks. Our analysis on ten languages demonstrates that the amount of unlabeled monolingual data has surprisingly little effect on the sentiment results. As expected, the choice of a annotated source language for projection to a target leads to better results for source-target language pairs which are similar. Therefore, our results suggest that more efforts should be spent on the creation of resources for less similar languages to those which are resource-rich already. Finally, a domain mismatch leads to a decreased performance. This suggests resources in any language should ideally cover varieties of domains.																	1076-9757	1943-5037					2019	66						691	742															
J								Interpretable Charge Prediction for Criminal Cases with Dynamic Rationale Attention	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												Charge prediction which aims to determine appropriate charges for criminal cases based on textual fact descriptions, is an important technology in the field of AI&Law. Previous works focus on improving prediction accuracy, ignoring the interpretability, which limits the methods' applicability. In this work, we propose a deep neural framework to extract short but charge-decisive text snippets - rationales - from input fact description, as the interpretation of charge prediction. To solve the scarcity problem of rationale annotated corpus, rationales are extracted in a reinforcement style with the only supervision in the form of charge labels. We further propose a dynamic rationale attention mechanism to better utilize the information in extracted rationales and predict the charges. Experimental results show that besides providing charge prediction interpretation, our approach can also capture subtle details to help charge prediction.																	1076-9757	1943-5037					2019	66						743	764		10.1613/jair.1.11377													
J								Full Characterization of Parikh's Relevance-Sensitive Axiom for Belief Revision	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											INTERPOLATION; LOGIC	In this article, the epistemic-entrenchment and partial-meet characterizations of Parikh's relevance-sensitive axiom for belief revision, known as axiom (P), are provided. In short, axiom (P) states that, if a belief set K can be divided into two disjoint compartments, and the new information y relates only to the first compartment, then the revision of K by should not affect the second compartment. Accordingly, we identify the subclass of epistemic-entrenchment and that of selection-function preorders, inducing AGM revision functions that satisfy axiom (P). Hence, together with the faithful-preorders characterization of (P) that has already been provided, Parikh's axiom is fully characterized in terms of all popular constructive models of Belief Revision. Since the notions of relevance and local change are inherent in almost all intellectual activity, the completion of the constructive view of (P) has a significant impact on many theoretical, as well as applied, domains of Artificial Intelligence.																	1076-9757	1943-5037					2019	66						765	792		10.1613/jair.1.11838													
J								Enhancing Statement Evaluation in Argumentation via Multi-labelling Systems	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											LOGIC; FRAMEWORK; SEMANTICS	In computational models of argumentation, the justification of statements has drawn less attention than the construction and justification of arguments. As a consequence, significant losses of sensitivity and expressiveness in the treatment of statement statuses can be incurred by otherwise appealing formalisms. In order to reappraise statement statuses and, more generally, to support a uniform modelling of different phases of the argumentation process we introduce multi-labelling systems, a generic formalism devoted to represent reasoning processes consisting of a sequence of labelling stages. In this context, two families of multi-labelling systems, called argument-focused and statement-focused approach, are identified and compared. Then they are shown to be able to encompass several prominent literature proposals as special cases, thereby enabling a systematic comparison evidencing their merits and limits Further, we show that the proposed model supports tunability of statement justification by specifying a few alternative statement justification labellings, and we illustrate how they can be seamlessly integrated into different formalisms.																	1076-9757	1943-5037					2019	66						793	860															
J								Deep Dialog Act Recognition using Multiple Token, Segment, and Context Information Representations	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											CLASSIFICATION	Automatic dialog act recognition is a task that has been widely explored over the years. In recent works, most approaches to the task explored different deep neural network architectures to combine the representations of the words in a segment and generate a segment representation that provides cues for intention. In this study, we explore means to generate more informative segment representations, not only by exploring different network architectures, but also by considering different token representations, not only at the word level, but also at the character and functional levels. At the word level, in addition to the commonly used uncontextualized embeddings, we explore the use of contextualized representations, which are able to provide information concerning word sense and segment structure. Character-level tokenization is important to capture intention-related morphological aspects that cannot be captured at the word level. Finally, the functional level provides an abstraction from words, which shifts the focus to the structure of the segment. Additionally, we explore approaches to enrich the segment representation with context information from the history of the dialog, both in terms of the classifications of the surrounding segments and the turn-taking history. This kind of information has already been proved important for the disambiguation of dialog acts in previous studies. Nevertheless, we are able to capture additional information by considering a summary of the dialog history and a wider turn-taking context. By combining the best approaches at each step, we achieve performance results that surpass the previous state-of-the-art on generic dialog act recognition on both the Switchboard Dialog Act Corpus (SwDA) and the ICSI Meeting Recorder Dialog Act Corpus (MRDA), which are two of the most widely explored corpora for the task. Furthermore, by considering both past and future context, similarly to what happens in an annotation scenario, our approach achieves a performance similar to that of a human annotator on SwDA and surpasses it on MRDA.																	1076-9757	1943-5037					2019	66						861	899															
J								General Game Playing with Imperfect Information	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SEARCH	General Game Playing is a field which allows the researcher to investigate techniques that might eventually be used in an agent capable of Artificial General Intelligence. Game playing presents a controlled environment in which to evaluate AI techniques, and so we have seen an increase in interest in this field of research. Games of imperfect information offer the researcher an additional challenge in terms of complexity over games with perfect information. In this article, we look at imperfect-information games: their expression, their complexity, and the additional demands of their players. We consider the problems of working with imperfect information and introduce a technique called HyperPlay, for efficiently sampling very large information sets, and present a formalism together with pseudo code so that others may implement it. We examine the design choices for the technique, show its soundness and completeness then provide some experimental results and demonstrate the use of the technique in a variety of imperfect-information games, revealing its strengths, weaknesses, and its efficiency against randomly generating samples. Improving the technique, we present HyperPlay-II, capable of correctly valuing information-gathering moves. Again, we provide some experimental results and demonstrate the use of the new technique revealing its strengths, weaknesses and its limitations.																	1076-9757	1943-5037					2019	66						901	935															
J								Classifying Inconsistency Measures Using Graphs	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											LOGIC	The aim of measuring inconsistency is to obtain an evaluation of the imperfections in a set of formulas, and this evaluation may then be used to help decide on some course of action (such as rejecting some of the formulas, resolving the inconsistency, seeking better sources of information, etc). A number of proposals have been made to define measures of inconsistency. Each has its rationale. But to date, it is not clear how to delineate the space of options for measures, nor is it clear how we can classify measures systematically. To address these problems, we introduce a general framework for comparing syntactic measures of inconsistency. It is based on the notion of an inconsistency graph for each knowledgebase (a bipartite graph with a set of vertices representing formulas in the knowledgebase, a set of vertices representing minimal inconsistent subsets of the knowledgebase, and edges representing that a formula belongs to a minimal inconsistent subset). We then show that various measures can be computed using the inconsistency graph. Then we introduce abstractions of the inconsistency graph and use them to construct a hierarchy of syntactic inconsistency measures. Furthermore, we extend the inconsistency graph concept with a labeling that extends the hierarchy to include some other types of inconsistency measures.																	1076-9757	1943-5037					2019	66						937	987															
J								A Semantic Characterization for ASP Base Revision	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											LOGIC PROGRAMS; ANSWER	The paper deals with base revision for Answer Set Programming (ASP). Base revision in classical logic is done by the removal of formulas. Exploiting the non-monotonicity of ASP allows one to propose other revision strategies, namely addition strategy or removal and/or addition strategy. These strategies allow one to define families of rule-based revision operators. The paper presents a semantic characterization of these families of revision operators in terms of answer sets. This semantic characterization allows for equivalently considering the evolution of syntactic logic programs and the evolution of their semantic content. It then studies the logical properties of the proposed operators and gives complexity results.																	1076-9757	1943-5037					2019	66						989	1029															
J								Strategic Abstention based on Preference Extensions: Positive Results and Computer-Generated Impossibilities	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SOCIAL CHOICE FUNCTIONS; SET; MANIPULATION; AGGREGATION; PARADOXES; IMPLIES	Voting rules allow multiple agents to aggregate their preferences in order to reach joint decisions. A common flaw of some voting rules, known as the no-show paradox, is that agents may obtain a more preferred outcome by abstaining from an election. We study strategic abstention for set-valued voting rules based on Kelly's and Fishburn's preference extensions. Our contribution is twofold. First, we show that, whenever there are at least five alternatives and seven agents, every Pareto-optimal majoritarian voting rule suffers from the no-show paradox with respect to Fishburn's extension. This is achieved by reducing the statement to a finite-yet very large-problem, which is encoded as a formula in propositional logic and then shown to be unsatisfiable by a SAT solver. We also provide a human-readable proof which we extracted from a minimal unsatisfiable core of the formula. Secondly, we prove that every voting rule that satisfies two natural conditions cannot be manipulated by strategic abstention with respect to Kelly's extension and give examples of well-known Pareto-optimal majoritarian voting rules that meet these requirements.																	1076-9757	1943-5037					2019	66						1031	1056															
J								Approximating Weighted and Priced Bribery in Scoring Rules	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											COALITIONAL MANIPULATION; ALGORITHMS; COMPLEXITY; DICHOTOMY	The classic BRIBERY problem is to find a minimal subset of voters who need to change their vote to make some preferred candidate win. Its important generalizations consider voters who are weighted and also have different prices. We provide an approximate solution for these problems for a broad family of scoring rules (which includes Borda, t-approval, and Dowdall), in the following sense: for constant weights and prices, if there exists a strategy which costs Psi, we efficiently find a strategy which costs at most Psi + (O) over tilde (root Psi). An extension for non-constant weights and prices is also given. Our algorithm is based on a randomized reduction from these BRIBERY generalizations to weighted coalitional manipulation (WCM). To solve this WCM instance, we apply the Birkhoff-von Neumann (BvN) decomposition to a fractional manipulation matrix. This allows us to limit the size of the possible ballot search space reducing it from exponential to polynomial, while still obtaining good approximation guarantees. Finding a solution in the truncated search space yields a new algorithm for WCM, which is of independent interest.																	1076-9757	1943-5037					2019	66						1057	1098															
J								If Nothing Is Accepted - Repairing Argumentation Frameworks	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											MINIMAL UNSATISFIABLE SUBSETS; ABDUCTIVE FRAMEWORK; INCONSISTENCY; ALGORITHMS; DIAGNOSIS	Conflicting information in an agent's knowledge base may lead to a semantical defect, that is, a situation where it is impossible to draw any plausible conclusion. Finding out the reasons for the observed inconsistency (so-called diagnoses) and/or restoring consistency in a certain minimal way (so-called repairs) are frequently occurring issues in knowledge representation and reasoning. In this article we provide a series of first results for these problems in the context of abstract argumentation theory regarding the two most important reasoning modes, namely credulous as well as sceptical acceptance. Our analysis includes the following problems regarding minimal repairs/diagnoses: existence, verification, computation of one and enumeration of all solutions. The latter problem is tackled with a version of the so-called hitting set duality first introduced by Raymond Reiter in 1987. It turns out that grounded semantics plays an outstanding role not only in terms of complexity, but also as a useful tool to reduce the search space for diagnoses regarding other semantics.																	1076-9757	1943-5037					2019	66						1099	1145															
J								Preference Orders on Families of Sets-When Can Impossibility Results Be Avoided?	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											POWER SET; RANKING SETS; EXTENSION; CHOICE; MANIPULATION	Lifting a preference order on elements of some universe to a preference order on subsets of this universe is often guided by postulated properties the lifted order should have. Wellknown impossibility results pose severe limits on when such liftings exist if all non-empty subsets of the universe are to be ordered. The extent to which these negative results carry over to other families of sets is not known. in this paper, we consider families of sets that induce connected subgraphs in graphs. For such families, common in applications, we study whether lifted orders satisfying the well-studied axioms of dominance and (strict) independence exist for every or, in another setting, for some underlying order on elements (strong and weak orderability). We characterize families that are strongly and weakly orderable under dominance and strict independence, and obtain a tight bound on the class of families that are strongly orderable under dominance and independence.																	1076-9757	1943-5037					2019	66						1147	1197		10.1613/jair.1.11879													
J								Unsupervised Evaluation and Weighted Aggregation of Ranked Classification Predictions	JOURNAL OF MACHINE LEARNING RESEARCH										Ensemble learning; Ensemble classifier; Unsupervised Learning; AUC; Spectral Decomposition	MATRIX; RANKING; AREA	Ensemble methods that aggregate predictions from a set of diverse base learners consistently outperform individual classifiers. Many such popular strategies have been developed in a supervised setting, where the sample labels have been provided to the ensemble algorithm. However, with the rising interest in unsupervised algorithms for machine learning and growing amounts of uncurated data, the reliance on labeled data precludes the application of ensemble algorithms to many real world problems. To this end we develop a new theoretical framework for ensemble learning, the Strategy for Unsupervised Multiple Method Aggregation (SUMMA), that estimates the performances of base classifiers and uses these estimates to form an ensemble classifier. SUMMA also generates an ensemble ranking of samples based on the confidence score it assigns to each sample. We illustrate the performance of SUMMA using a synthetic example as well as two real world problems.																	1532-4435						2019	20								166														
J								A Kernel Multiple Change-point Algorithm via Model Selection	JOURNAL OF MACHINE LEARNING RESEARCH										model selection; kernel methods; change-point detection; concentration inequality	TIME-SERIES; DIMENSIONALITY REDUCTION; JOINT SEGMENTATION; INEQUALITIES; STATISTICS; PENALTIES; NUMBER; BREAKS	We consider a general formulation of the multiple change-point problem, in which the data is assumed to belong to a set equipped with a positive semidefinite kernel. We propose a model-selection penalty allowing to select the number of change points in Harchaoui and Cappe's kernel-based change-point detection method. The model-selection penalty generalizes non-asymptotic model-selection penalties for the change-in-mean problem with univariate data. We prove a non-asymptotic oracle inequality for the resulting kernel-based change-point detection method, whatever the unknown number of change points, thanks to a concentration result for Hilbert-space valued random variables which may be of independent interest. Experiments on synthetic and real data illustrate the proposed method, demonstrating its ability to detect subtle changes in the distribution of data.																	1532-4435						2019	20								162														
J								Why do deep convolutional networks generalize so poorly to small image transformations?	JOURNAL OF MACHINE LEARNING RESEARCH										Machine Learning; Deep Convolutional Neural Networks; Generalization		Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.																	1532-4435						2019	20								184														
J								DataWig: Missing Value Imputation for Tables	JOURNAL OF MACHINE LEARNING RESEARCH										missing value imputation; deep learning; heterogeneous data		With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: github.com/awslabs/datawig																	1532-4435						2019	20								175														
J								Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning	JOURNAL OF MACHINE LEARNING RESEARCH										representation learning; generative models; empirical evaluation; disentanglement; morphometrics		Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: "to what extent has my model learned to represent specific factors of variation in the data?" We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at https : //github. com/dccastro/Morpho-MNIST.																	1532-4435						2019	20								178														
J								Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals	JOURNAL OF MACHINE LEARNING RESEARCH										constrained optimization; non-convex; fairness; churn; swap regret; non-zero-sum game		We show that many machine learning goals can be expressed as "rate constraints" on a model's predictions. We study the problem of training non-convex models subject to these rate constraints (or other non-convex or non-differentiable constraints). In the non-convex setting, the standard approach of Lagrange multipliers may fail. Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian with gradient-based methods. To solve these issues, we introduce a new "proxy-Lagrangian" formulation. This leads to an algorithm that, assuming access to an optimization oracle, produces a stochastic classifier by playing a two-player non-zero-sum game solving for what we call a semi-coarse correlated equilibrium, which in turn corresponds to an approximately optimal and feasible solution to the constrained optimization problem. We then give a procedure that shrinks the randomized solution down to a mixture of at most m + 1 deterministic solutions, given m constraints. This culminates in a procedure that can solve non-convex constrained optimization problems with possibly non-differentiable and non-convex constraints, and enjoys theoretical guarantees. We provide extensive experimental results covering a broad range of policy goals, including various fairness metrics, accuracy, coverage, recall, and churn.																	1532-4435						2019	20								172														
J								Log-concave sampling: Metropolis-Hastings algorithms are fast	JOURNAL OF MACHINE LEARNING RESEARCH										Log-concave sampling; Langevin algorithms; MCMC algorithms; conductance methods	HIT-AND-RUN; GEOMETRIC-CONVERGENCE; MONTE-CARLO; LANGEVIN; RATES; BOUNDS; ERROR	We study the problem of sampling from a strongly log-concave density supported on Rd, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by simulating a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most delta delta for a density with condition number K, we show that MALA requires O (kappa d log(1/delta)) steps from a warm start, as compared to the O(kappa(2)d/delta(2)) steps established in past work on ULA. We also demonstrate the gains of a modified version of MALA over ULA for weakly log-concave densities. Furthermore, we derive mixing time bounds for the Metropolized random walk (MRW) and obtain O(kappa) mixing time slower than MALA. We provide numerical examples that support our theoretical findings, and demonstrate the benefits of Metropolis-Hastings adjustment for Langevin-type sampling algorithms.																	1532-4435						2019	20								183														
J								Fast Automatic Smoothing for Generalized Additive Models	JOURNAL OF MACHINE LEARNING RESEARCH										Automatic L-2 Regularization; Empirical Bayes; Expectation-maximization Algorithm; Generalized Additive Model; Laplace Approximation; Marginal Maximum Likelihood	FREQUENCY-DISTRIBUTION; MAXIMUM-LIKELIHOOD; REGRESSION; SELECTION; PARAMETER; SCALE	Generalized additive models (GAMs) are regression models wherein parameters of probability distributions depend on input variables through a sum of smooth functions, whose degrees of smoothness are selected by L-2 regularization. Such models have become the de-facto standard nonlinear regression models when interpretability and flexibility are required, but reliable and fast methods for automatic smoothing in large data sets are still lacking. We develop a general methodology for automatically learning the optimal degree of L-2 regularization for GAMs using an empirical Bayes approach. The smooth functions are penalized by hyper-parameters that are learned simultaneously by maximization of a marginal likelihood using an approximate expectation-maximization algorithm. The latter involves a double Laplace approximation at the E-step, and leads to an efficient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than the best existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data.																	1532-4435						2019	20								173														
J								All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously	JOURNAL OF MACHINE LEARNING RESEARCH										Rashomon; permutation importance; conditional variable importance; Ustatistics; transparency; interpretable models	RANDOM FORESTS; DISPARITIES; FEATURES; RISK; RACE	Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model f(x) = x(T)beta with a fixed coefficient vector beta) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.																	1532-4435						2019	20								177														
J								Shared Subspace Models for Multi-Group Covariance Estimation	JOURNAL OF MACHINE LEARNING RESEARCH										covariance estimation; spiked covariance model; Stiefel manifold; large p, small n; high-dimensional data; empirical Bayes; gene expression data	CLASS-II GENE; EXPRESSION; LEUKEMIA; DIMENSIONALITY; EIGENVALUES; DISCOVERY	We develop a model-based method for evaluating heterogeneity among several p x p covariance matrices in the large p, small n setting. This is done by assuming a spiked covariance model for each group and sharing information about the space spanned by the group-level eigenvectors. We use an empirical Bayes method to identify a low-dimensional subspace which explains variation across all groups and use an MCMC algorithm to estimate the posterior uncertainty of eigenvectors and eigenvalues on this subspace. The implementation and utility of our model is illustrated with analyses of high-dimensional multivariate gene expression.																	1532-4435						2019	20								171														
J								Stochastic Canonical Correlation Analysis	JOURNAL OF MACHINE LEARNING RESEARCH										Canonical correlation analysis; sample complexity; shift-and-invert preconditioning; streaming CCA		We study the sample complexity of canonical correlation analysis (CCA), i.e., the number of samples needed to estimate the population canonical correlation and directions up to arbitrarily small error. With mild assumptions on the data distribution, we show that in order to achieve epsilon-suboptimality in a properly defined measure of alignment between the estimated canonical directions and the population solution, we can solve the empirical objective exactly with N(epsilon, Delta, gamma) samples, where Delta is the singular value gap of the whitened cross-covariance matrix and 1/gamma is an upper bound of the condition number of auto-covariance matrices. Moreover, we can achieve the same learning accuracy by drawing the same level of samples and solving the empirical objective approximately with a stochastic optimization algorithm; this algorithm is based on the shift-and-invert power iterations and only needs to process the dataset for O (log 1/c) passes. Finally, we show that, given an estimate of the canonical correlation, the streaming version of the shift-and-invert power iterations achieves the same learning accuracy with the same level of sample complexity, by processing the data only once.																	1532-4435						2019	20								167														
J								DPPy: DPP Sampling with Python	JOURNAL OF MACHINE LEARNING RESEARCH										determinantal point processes; sampling; MCMC; random matrices; Python		Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is a challenge and therefore we present DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms for both finite and continuous DPPs. The project is hosted on GitHubc) and equipped with an extensive documentation.																	1532-4435						2019	20								180														
J								Model Selection in Bayesian Neural Networks via Horseshoe Priors	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian Neural Networks; Model Selection; Horseshoe Priors; Variational Inference; Structured approximations		The promise of augmenting accurate predictions provided by modern neural networks with well-calibrated predictive uncertainties has reinvigorated interest in Bayesian neural networks. However, model selection even choosing the number of nodes remains an open question. Poor choices can severely affect the quality of the produced uncertainties. In this paper, we explore continuous shrinkage priors, the horseshoe, and the regularized horseshoe distributions, for model selection in Bayesian neural networks. When placed over node pre-activations and coupled with appropriate variational approximations, we find that the strong shrinkage provided by the horseshoe is effective at turning off nodes that do not help explain the data. We demonstrate that our approach finds compact network structures even when the number of nodes required is grossly over-estimated. Moreover, the model selection over the number of nodes does not come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches. These effects are particularly apparent in sample-limited settings, such as small data sets and reinforcement learning.																	1532-4435						2019	20								182														
J								Differentiable reservoir computing	JOURNAL OF MACHINE LEARNING RESEARCH										reservoir computing; fading memory property; finite memory; echo state property; differentiable reservoir filter; Volterra series representation; state-space systems; system identification; machine learning	FADING-MEMORY; GENERALIZED SYNCHRONIZATION; VOLTERRA SERIES; SYSTEMS; PERSISTENCE; NETWORKS	Numerous results in learning and approximation theory have evidenced the importance of differentiability at the time of countering the curse of dimensionality. In the context of reservoir computing, much effort has been devoted in the last two decades to characterize the situations in which systems of this type exhibit the so-called echo state (ESP) and fading memory (FMP) properties. These important features amount, in mathematical terms, to the existence and continuity of global reservoir system solutions. That research is complemented in this paper with the characterization of the differentiability of reservoir filters for very general classes of discrete-time deterministic inputs. This constitutes a novel strong contribution to the long line of research on the ESP and the FMP and, in particular, links to existing research on the input-dependence of the ESP. Differentiability has been shown in the literature to be a key feature in the learning of attractors of chaotic dynamical systems. A Volterra-type series representation for reservoir filters with semi -infinite discrete-time inputs is constructed in the analytic case using Taylor's theorem and corresponding approximation bounds are provided. Finally, it is shown as a corollary of these results that any fading memory filter can be uniformly approximated by a finite Volterra series with finite memory.																	1532-4435						2019	20								179														
J								On the Convergence of Gaussian Belief Propagation with Nodes of Arbitrary Size	JOURNAL OF MACHINE LEARNING RESEARCH										belief propagation; Gaussian distributions; higher-dimensional marginals; preconditioning; inference	GRAPHICAL MODELS	This paper is concerned with a multivariate extension of Gaussian message passing applied to pairwise Markov graphs (MGs). Gaussian message passing applied to pairwise MGs is often labeled Gaussian belief propagation (GaBP) and can be used to approximate the marginal of each variable contained in the pairwise MG. We propose a multivariate extension of GaBP (we label this GaBP-m) that can be used to estimate higher-dimensional marginals. Beyond the ability to estimate higher-dimensional marginals, GaBP-m exhibits better convergence behavior than GaBP, and can also provide more accurate univariate marginals. The theoretical results of this paper are based on an extension of the computation tree analysis conducted on univariate nodes to the multivariate case. The main contribution of this paper is the development of a convergence condition for GaBP-m that moves beyond the walk-summability of the precision matrix. Based on this convergence condition, we derived an upper bound for the number of iterations required for convergence of the GaBP-m algorithm. An upper bound on the dissimilarity between the approximate and exact marginal covariance matrices was established. We argue that GaBP-m is robust towards a certain change in variables, a property not shared by iterative solvers of linear systems, such as the conjugate gradient (CG) and preconditioned conjugate gradient (PCG) methods. The advantages of using GaBP-m over GaBP are also illustrated empirically.																	1532-4435						2019	20								165														
J								Learning Overcomplete, Low Coherence Dictionaries with Linear Inference	JOURNAL OF MACHINE LEARNING RESEARCH										independent components analysis; dictionary learning; coherence	SPARSE; ALGORITHM; STATISTICS; NETWORK; NEURONS; MODELS; FIELDS; CODE	Finding overcomplete latent representations of data has applications in data analysis, signal processing, machine learning, theoretical neuroscience and many other fields. In an overcomplete representation, the number of latent features exceeds the data dimensionality, which is useful when the data is undersampled by the measurements (compressed sensing or information bottlenecks in neural systems) or composed from multiple complete sets of linear features, each spanning the data space. Independent Components Analysis (ICA) is a linear technique for learning sparse latent representations, which typically has a lower computational cost than sparse coding, a linear generative model which requires an iterative, nonlinear inference step. While well suited for finding complete representations, we show that overcompleteness poses a challenge to existing ICA algorithms. Specifically, the coherence control used in existing ICA and other dictionary learning algorithms, necessary to prevent the formation of duplicate dictionary features, is ill-suited in the overcomplete case. We show that in the overcomplete case, several existing ICA algorithms have undesirable global minima that maximize coherence. We provide a theoretical explanation of these failures and, based on the theory, propose improved coherence control costs for overcomplete ICA algorithms. Further, by comparing ICA algorithms to the computationally more expensive sparse coding on synthetic data, we show that the limited applicability of overcomplete, linear inference can be extended with the proposed cost functions. Finally, when trained on natural images, we show that the coherence control biases the exploration of the data manifold, sometimes yielding suboptimal, coherent solutions. All told, this study contributes new insights into and methods for coherence control for linear ICA, some of which are applicable to many other nonlinear models.																	1532-4435						2019	20								174														
J								New Convergence Aspects of Stochastic Gradient Algorithms	JOURNAL OF MACHINE LEARNING RESEARCH										Stochastic Gradient Algorithms; Asynchronous Stochastic Optimization; SGD; Hogwild; bounded gradient		The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is violated for cases where the objective function is strongly convex. In Bottou et al. (2018), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. We show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime. We then move on to the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime in the case of diminished learning rate. It is well-known that SGD converges if a sequence of learning rates {70 satisfies 7 17t 00 and Et o 7/t2 < 00 " We show the convergence of SGD for strongly convex objective function without using bounded gradient assumption when {70 is a diminishing sequence and oo. Inz_,V't 0 other words, we extend the current state-of-the-art class of learning rates satisfying the convergence of SGD.																	1532-4435						2019	20								176														
J								Neural Empirical Bayes	JOURNAL OF MACHINE LEARNING RESEARCH										empirical Bayes; unnormalized densities; concentration of measure; Langevin MCMC; associative memory	DENSITY-FUNCTION; MEAN SHIFT; GRADIENT	We unify kernel density estimation and empirical Bayes and address a set of problems in unsupervised machine learning with a geometric interpretation of those methods, rooted in the concentration of measure phenomenon. Kernel density is viewed symbolically as X (sic) Y where the random variable X is smoothed to Y = X + N(0, sigma(2)/(d)), and empirical Bayes is the machinery to denoise in a least-squares sense, which we express as X Y. A learning objective is derived by combining these two, symbolically captured by X (sic) Y. Crucially, instead of using the original nonparametric estimators, we parametrize the energy function with a neural network denoted by phi; at optimality, del phi approximate to -del log f where f is the density of Y. The optimization problem is abstracted as interactions of high-dimensional spheres which emerge due to the concentration of isotropic Gaussians. We introduce two algorithmic frameworks based on this machinery: (i) a "walk-jump" sampling scheme that combines Langevin MCMC (walks) and empirical Bayes (jumps), and (ii) a probabilistic framework for associative memory, called NEBULA, defined a la Hopfield by the gradient flow of the learned energy to a set of attractors. We finish the paper by reporting the emergence of very rich "creative memories" as attractors of NEBULA for highly-overlapping spheres.																	1532-4435						2019	20								181														
J								Kernel Regression with Coefficient-based l(q)-regularization	JOURNAL OF MACHINE LEARNING RESEARCH										Learning Theory; Kernel Regression; Coefficient-based; l(q)-regularization (0 < q <= 1); Sparsity; l(2)-empirical Covering Number	SUPPORT VECTOR MACHINES; LEARNING RATES; REGULARIZATION; BOUNDS; APPROXIMATION; SELECTION; SPARSITY	In this paper, we consider the l(q)-regularized kernel regression with 0 < q <= 1. In form, the algorithm minimizes a least-square loss functional adding a coefficient-based l(q) penalty term over a linear span of features generated by a kernel function. We study the asymptotic behavior of the algorithm under the framework of learning theory. The contribution of this paper is two-fold. First, we derive a tight bound on the l(2) - empirical covering numbers of the related function space involved in the error analysis. Based on this result, we obtain the convergence rates for the f regularized kernel regression which is the best so far. Second, for the case 0 < q < 1, we show that the regularization parameter plays a role as a trade-off between sparsity and convergence rates. Under some mild conditions, the fraction of non-zero coefficients in a local minimizer of the algorithm will tend to 0 at a polynomial decay rate when the sample size m becomes large. As the concerned algorithm is non-convex, we also discuss how to generate a minimizing sequence iteratively, which can help us to search a local minimizer around any initial point.																	1532-4435						2019	20								161														
J								The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks	JOURNAL OF MACHINE LEARNING RESEARCH										causal discovery; directed acyclic graphs; faithfulness; high dimensions; random graphs	BAYESIAN NETWORKS; SEPARATION; EXPRESSION; GRAPHS	We consider the task of estimating a high-dimensional directed acyclic graph, given observations from a linear structural equation model with arbitrary noise distribution. By exploiting properties of common random graphs, we develop a new algorithm that requires conditioning only on small sets of variables. The proposed algorithm, which is essentially a modified version of the PC-Algorithm, offers significant gains in both computational complexity and estimation accuracy. In particular, it results in more efficient and accurate estimation in large networks containing hub nodes, which are common in biological systems. We prove the consistency of the proposed algorithm, and show that it also requires a less stringent faithfulness assumption than the PC-Algorithm. Simulations in low and high-dimensional settings are used to illustrate these findings. An application to gene expression data suggests that the proposed algorithm can identify a greater number of clinically relevant genes than current methods.																	1532-4435						2019	20								164														
J								Determinantal Point Processes for Coresets	JOURNAL OF MACHINE LEARNING RESEARCH										Coresets; Determinantal Point Processes; Sensitivity	ALGORITHMS; MATRIX; APPROXIMATION	When faced with a data set too large to be processed all at once, an obvious solution is to retain only part of it. In practice this takes a wide variety of different forms, and among them "coresets" are especially appealing. A coreset is a (small) weighted sample of the original data that comes with the following guarantee: a cost function can be evaluated on the smaller set instead of the larger one, with low relative error. For some classes of problems, and via a careful choice of sampling distribution (based on the so-called "sensitivity" metric), iid random sampling has turned to be one of the most successful methods for building coresets efficiently. However, independent samples are sometimes overly redundant, and one could hope that enforcing diversity would lead to better performance. The difficulty lies in proving coreset properties in non-iid samples. We show that the coreset property holds for samples formed with determinantal point processes (DPP). DPPs are interesting because they are a rare example of repulsive point processes with tractable theoretical properties, enabling us to prove general coreset theorems. We apply our results to both the k-means and the linear regression problems, and give extensive empirical evidence that the small additional computational cost of DPP sampling comes with superior performance over its iid counterpart. Of independent interest, we also provide analytical formulas for the sensitivity in the linear regression and 1-means cases.																	1532-4435						2019	20								168														
J								DBSCAN: Optimal Rates For Density-Based Cluster Estimation	JOURNAL OF MACHINE LEARNING RESEARCH										DBSCAN; density-based clustering; cluster tree; minimax optimality; Holder smooth density	NONPARAMETRIC-ESTIMATION; SINGLE LINKAGE; CONSISTENCY; TREE	We study the problem of optimal estimation of the density cluster tree under various smoothness assumptions on the underlying density. Inspired by the seminal work of Chaudhuri et al. (2014), we formulate a new notion of clustering consistency which is better suited to smooth densities, and derive minimax rates for cluster tree estimation under Holder smooth densities of arbitrary degree. We present a computationally efficient, rate optimal cluster tree estimator based on simple extensions of the popular DBSCAN algorithm of Ester et al. (1996). Our procedure relies on kernel density estimators and returns a sequence of nested random geometric graphs whose connected components form a hierarchy of clusters. The resulting optimal rates for cluster tree estimation depend on the degree of smoothness of the underlying density and, interestingly, match the minimax rates for density estimation under the sup-norm loss. Our results complement and extend the analysis of the DBSCAN algorithm in Sriperumbudur and Steinwart (2012). Finally, we consider level set estimation and cluster consistency for densities with jump discontinuities. We demonstrate that the DBSCAN algorithm attains the minimax rate in terms of the jump size and sample size in this setting as well.																	1532-4435						2019	20								170														
J								Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets	JOURNAL OF MACHINE LEARNING RESEARCH										Sparse; Sparse Group Lasso; Screening; Fenchel's Dual; Decomposition; Convex Sets; Composite Function Optimization	SELECTION; REGRESSION; PREDICTORS	Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l(1) and l(2) norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable to sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method-called DPC (decomposition of convex set)-for non-negative Lasso. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.																	1532-4435						2019	20								163														
J								Embarrassingly Parallel Inference for Gaussian Processes	JOURNAL OF MACHINE LEARNING RESEARCH										Gaussian process; parallel inference; machine learning; Bayesian non-parametrics		Training Gaussian process-based models typically involves an O(N-3) computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of K Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarrassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost.																	1532-4435						2019	20								169														
J								A new model to determine the hierarchical structure of the wireless sensor networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Sensor network; clustering; fuzzy inference system; weighted averaging based on levels	FUZZY-LOGIC	Wireless sensor networks are one of the rising areas of scientific research. Common purpose of these investigations is usually constructing optimal structure of the network by prolonging its lifetime. In this study, a new model has been proposed to construct a hierarchical structure of wireless sensor networks. Methods used in the model to determine clusters and appropriate cluster heads are k-means clustering and fuzzy inference system (FIS), respectively. The weighted averaging based on levels (WABL) defuzzification method is used to calculate crisp outputs of the FIS. A new theorem for calculation of WABL values has been proved in order to simplify getting the crisp values from complex fuzzy outputs of the FIS. The proposed methodology is experimented via simulation example, and experiments confirm its validity.																	1300-0632	1303-6203					2019	27	6					4023	4037		10.3906/elk-1811-142													
J								On the performance of quick artificial bee colony algorithm for dynamic deployment of wireless sensor networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Quick artificial bee colony algorithm; wireless sensor networks; dynamic deployment problem; probabilistic detection model; binary detection model	PARTICLE SWARM OPTIMIZATION; COVERAGE	In recent years, the use of wireless sensor networks (WSNs) has increased and there have been significant improvements in this field. Especially with smarter, cheaper, and smaller sensor nodes, various kinds of information can be detected and collected in different environments and under different conditions. WSNs have thus been used in many applications such as military, surveillance, target tracking, home, medical, and environmental applications. As the popularity of WSNs increases, problems related to these networks are being realized. The dynamic deployment problem is one of the main challenges that have a direct effect on the performance of WSNs. In this study, a novel optimization technique named the quick artificial bee colony (qABC) algorithm was applied to the dynamic deployment problem of WSNs. qABC is a new version of the artificial bee colony algorithm (ABC) and it redefines the onlooker bee phase of ABC in a more detailed way. In order to see the performance of qABC on this problem, WSNs that include only mobile sensors or both stationary and mobile sensors were considered with binary and probabilistic detection models. Some experimental studies were conducted for tuning the colony size (CS) and neighborhood radius (r) parameters of the qABC algorithm, and the performance of the proposed method was compared with the standard ABC algorithm and some other recently introduced approaches including a parallel ABC, a cooperative parallel ABC, a version of ABC powered by a transition control mechanism (tlABC), and a parallel version of tlABC. Additionally, some CPU time analyses were provided for qABC and ABC considering different dimensions of the problem. Simulation results show that the qABC algorithm is an effective method that can be used for the dynamic deployment problem of WSNs, and it generally improves the convergence performance of the standard ABC on this problem when r >= 1.																	1300-0632	1303-6203					2019	27	6					4038	4054		10.3906/elk-1902-189													
J								A modified gravitational search algorithm and its application in lifetime maximization of wireless sensor networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Wireless sensor network; energy-efficiency protocol; clustering method; network life; gravitational search algorithm; tournament selection	PARTICLE SWARM OPTIMIZATION	Recently, academic communities and industrial sectors have been affected by significant advancements in wireless sensor networks (WSNs). Employing clustering methods is the dominant method to maximize the WSN's lifetime, which is considered to be a major issue. Metaheuristic algorithms have attracted wide attention in the research area of clustering. In this paper, first a novel nature-inspired optimization algorithm based on the gravitational search algorithm (GSA) is defined. To control the exploitation and exploration capabilities of this algorithm, along with calculating the masses value, the tournament selection method is employed. Tournament size, the parameter of this method, is computed automatically using a function during the computational process of the algorithm. The abilities of the algorithm are balanced using this problem-independent parameter. Therefore, the performance of the proposed algorithm is improved in this paper. Moreover, a modified GSA is applied to an energy-efficient clustering protocol for WSNs to minimize the objective function defining the compact clusters that have cluster heads with high energy. The proposed search algorithm is evaluated in terms of some standard test functions. The results suggest that this method has better performance than other state-of-the-art optimization algorithms. In addition, simulation results indicate that the proposed method for the clustering problem in WSNs has better performance on network lifetime and delivery data packets in BS than other popular clustering methods.																	1300-0632	1303-6203					2019	27	6					4055	4069		10.3906/elk-1904-14													
J								Application of multiscale fuzzy entropy features for multilevel subject-dependent emotion recognition	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Emotion recognition; multiscale fuzzy entropy; electroencephalogram; support vector machine	ELECTROENCEPHALOGRAM; SIGNALS; COMPLEXITY	Emotion recognition can be used in clinical and nonclinical situations. Despite previous works which mostly used time and frequency features of electroencephalogram (EEG) signals in subject-dependent emotion recognition issues, we used multiscale fuzzy entropy as a nonlinear dynamic feature. The EEG signals of the well-known Database for Emotion Analysis Using Physiological signals dataset was used for classification of two and three levels of emotions in arousal and valence space. The compound feature selection with a cost of average accuracy of support vector machine classifier was used to reduce feature dimensions. For subject-dependent systems, the proposed method is superior in comparison to previous works with 90.81% and 90.53% accuracies in two-level classification and 79.83% and 77.80% accuracies in three-level classification in arousal and valence dimensions, respectively.																	1300-0632	1303-6203					2019	27	6					4070	4081		10.3906/elk-1805-126													
J								A depth-based nearest neighbor algorithm for high-dimensional data classification	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Subspace-clustering; data-depth; information gain; nearest neighbor; classification	SUBSPACE; SELECTION	Nearest neighbor algorithms like k-nearest neighbors (kNN) are fundamental supervised learning techniques to classify a query instance based on class labels of its neighbors. However, quite often, huge volumes of datasets are not fully labeled and the unknown probability distribution of the instances may be uneven. Moreover, kNN suffers from challenges like curse of dimensionality, setting the optimal number of neighbors, and scalability for high-dimensional data. To overcome these challenges, we propose an improvised approach of classification via depth representation of subspace clusters formed from high-dimensional data. We offer a consistent and principled approach to dynamically choose the nearest neighbors for classification of a query point by i) identifying structures and distributions of data; ii) extracting relevant features, and iii) deriving an optimum value of k depending on the structure of data by representing data using data depth function. We propose an improvised classification algorithm using a depth-based representation of clusters, to improve performance in terms of execution time and accuracy. Experimentation on real-world datasets reveals that proposed approach is at least two orders of magnitude faster for high-dimensional dataset and is at least as accurate as traditional kNN.																	1300-0632	1303-6203					2019	27	6					4082	4101		10.3906/elk-1807-163													
J								A hybrid feature-selection approach for finding the digital evidence of web application attacks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Web application attacks; machine learning; feature selection; digital evidence		The most critical challenge of web attack forensic investigations is the sheer amount of data and level of complexity. Machine learning technology might be an efficient solution for web attack analysis and investigation. Consequently, machine learning applications have been applied in various areas of information security and digital forensics, and have improved over time. Moreover, feature selection is a crucial step in machine learning; in fact, selecting an optimal feature subset could enhance the accuracy and performance of the predictive model. To date, there has not been an adequate approach to select optimal features for the evidence of web attack. In this study, a hybrid approach that selects the relevant web attack features by combining the filter and wrapper methods is proposed. This approach has been validated by experimental measurements on 3 web attack datasets. The results show that our proposed approach can find the evidence with high recall, high accuracy, and low error rates. We believe that the results presented herein may help us to improve accuracy and recall of machine learning techniques; particularly, in the field of web attack investigation. The tools that use this approach may help digital forensic professionals and law enforcement in finding the evidence much more efficiently and faster.																	1300-0632	1303-6203					2019	27	6					4102	4117		10.3906/elk-1812-18													
J								Decision-making for small industrial Internet of Things using decision fusion	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Decision-making; decision fusion; Dempster-Shafer; dendrogram-based support vector machine; artificial neural network	COMPUTATIONAL MODELS	The industrial Internet of Things (IIoT) is a new field of Internet of Things (IoT) that has gained more popularity recently in industrial units and makes it possible to access information anywhere and anytime. In other words, geographic coordinates cannot prevent obtaining equipment and its data. Today, it is possible to manage and control equipment simply without spending time in an operational area and just by using the IIoT. This system collects data from manufacturing and production units by using wireless sensor networks or other networks for classification of fault detection. These data are then used after analysis to allow operational decisions to be made in shorter amounts of time. In fact, the IIoT increases the efficiency and accuracy of the "connection, collection, analysis, and operation" cycle. The information collected through different sensors in the IIoT is unreliable and uncertain due to the sensitivity of the sensors to noise, failure, and loss of information during transmission. One of the most important techniques offered to deal with this uncertainty in information is the decision fusion method. Among the decision fusion techniques, the Dempster-Shafer and improved Dempster-Shafer theory, which is also known as Yager theory, are efficient and effective ways to manage the uncertainty and have been used in many types of research. This paper offers an architecture for decision fusion in a small IIoT using Dempster-Shafer and Yager theories. In this architecture, data collected from the desired environment are fed to classifiers for classification. In this architecture, artificial neural networks and a dendrogram-based support vector machine are used as classifiers. To increase the accuracy of classifier results, the Dempster-Shafer and Yager theories are used to combine these results. To prove the performance, the proposed method was applied for detection of faults in an induction motor and human activity detection in an environment. This proposed method improved the accuracy of the system and decreased its uncertainty significantly according to obtained results from these two example use cases.																	1300-0632	1303-6203					2019	27	6					4134	4150		10.3906/elk-1809-60													
J								Enabling space time division multiple access in IETF 6TiSCH protocol	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										IETF 6TiSCH; smart antennas; Internet of Things; Cooja; Contiki OS; wireless sensor networks	WIRELESS SENSOR NETWORKS; AD HOC; INTERNET; THINGS; MAC	IETF 6TiSCH standard aims to create reliable, deterministic, and low-power networks by scheduling bandwidth resources in time and frequency domains. The main emphasis of 6TiSCH protocol is that it creates Internet of things (IoT) networks with a deterministic and controllable delay. However, many of its benefits are tied to the ability of the 6TiSCH scheduler to optimally distribute radio resources among wireless nodes which may not be possible when the number of frequency resources are limited and several other wireless technologies share the same frequency band (e.g., WiFi, Bluetooth and IEEE 802.15.4). Here the integration of a low-complexity directional antenna system with IETF 6TiSCH protocol is investigated with the aim of creating a 6TiSCH solution with higher spatial reuse. 6TiSCH nodes equipped with such smart directional antennas can schedule bandwidth resources not only in time and frequency domain but also in spatial (space) domain.																	1300-0632	1303-6203					2019	27	6					4151	4166		10.3906/elk-1903-127													
J								Energy saving scheduling in a fog-based IoT application by Bayesian task classification approach	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Fog computing; tasks scheduling; machine learning; Bayesian classification	ENABLING TECHNOLOGIES	The Internet of things increases information volume in computer networks and the concept of fog will help us to control this volume more efficiently. Scheduling resources in such an environment would be an NP-Hard problem. This article has studied the concept of scheduling in fog with Bayesian classification which could be applied to gain the task requirements like the processing ones. After classification, virtual machines will be created in accordance with the predicted requirements. The ifogsim simulator has been applied to study our fog-based Bayesian classification scheduling (FBCS) method performance in an EEG tractor application. Algorithms have been evaluated on a practical application of brain signal tracking system. According to the results, the FBCS method, compared with other methods, has reduced the energy consumption in the cloud and the executing task cost in cloud; and also the average of energy consuming in mobiles has been decreased by smart decision making.																	1300-0632	1303-6203					2019	27	6					4167	4187		10.3906/elk-1902-152													
J								Evaluating the attributes of remote sensing image pixels for fast k-means clustering	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Remote sensing images; clustering; k-means; color transformation; distance norms	DISTANCE	Clustering process is an important stage for many data mining applications. In this process, data elements are grouped according to their similarities. One of the most known clustering algorithms is the k-means algorithm. The algorithm initially requires the number of clusters as a parameter and runs iteratively. Many remote sensing image processing applications usually need the clustering stage like many image processing applications. Remote sensing images provide more information about the environments with the development of the multispectral sensor and laser technologies. In the dataset used in this paper, the infrared (IR) and the digital surface maps (DSM) are also supplied besides the red (R), the green (G), and the blue (B) color values of the pixels. However, remote sensing images come with very large sizes (6000 x 6000 pixels for each image in the dataset used). Clustering these large-size images using their multiattributes consumes too much time if it is used directly. In the literature, some studies are available to accelerate the k-means algorithm. One of them is the normalized distance value (NDV)-based fast k-means algorithm that benefits from the speed of the histogram-based approach and uses the multiattributes of the pixels. In this paper, we evaluated the effects of these attributes on the correctness of the clustering process with different color space transformations and distance measurements. We give the success results as peak signal-to-noise ratio and structural similarity index values using two different types of reference data (the source images and the ground-truth images) separately. Finally, we give the results based on accuracy measurement for evaluating both the success of the clustering outputs and the reliability of the NDV-based measurement methods presented in this paper.																	1300-0632	1303-6203					2019	27	6					4188	4202		10.3906/elk-1901-190													
J								GACNN SleepTuneNet: a genetic algorithm designing the convolutional neural network architecture for optimal classification of sleep stages from a single EEG channel	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Without hand-crafted feature extraction; electroencephalogram (EEG); genetic algorithm; convolutional neural network	STATISTICAL FEATURES; SYSTEM	This study presents a method for designing-by a genetic algorithm, without manual intervention-the feature learning architecture for classification of sleep stages from a single EEG channel, when using a convolutional neural network called GACNN SleepTuneNet. Two EEG electrode positions were selected, namely FP2-F4 and FPz-Cz, from two available datasets. Twenty-five generations were involved in diagnosis without hand-crafted features, to learn the architecture for classification of sleep stages based on AASM standard. Based on the results, our model not only achieved the highest classification accuracy, but it also distinguished the sleep stages based on either of the two EEG electrode signals, in both datasets. The results show that our model performed the best with highest overall accuracy rates and kappa statistic (CAP sleep: 95.61% and 0.94; Sleep EDF: 92.51% and 0.90) among other state-of-the-art methods that require no manual intervention. Our model could automatically learn the features for classification of sleep stages, for different raw EEG electrode positions in different datasets, without user-assisted feature extraction.																	1300-0632	1303-6203					2019	27	6					4203	4219		10.3906/elk-1903-186													
J								Defect detection of seals in multilayer aseptic packages using deep learning	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Multilayer aseptic packages; seal; Faster R-CNN	RECOGNITION	Sealing in aseptic packages, one of the healthiest and cheapest technologies to protect food from parasites in the liquid food industry, requires a detailed and careful control process. Since the controls are made manually and visually by expert machine operators, the human factor can lead to the failure to detect defects, resulting in high cost and food safety risks. Therefore, this study aims to perform a leak test in aseptic package seals by a system that makes decisions using independent deep learning methods. The proposed Faster R-CNN and the Updated Faster R-CNN deep learning models were subjected to training and testing with a total of 400 images taken from a real production environment, resulting in a correct classification rate of 99.25%. As a result, it can be said that the study is the second study that performs a computer-aided quality control process with promising results, having distinctive features such as being the first study that conducts analysis using the deep learning method.																	1300-0632	1303-6203					2019	27	6					4220	4230		10.3906/elk-1903-112													
J								ABC-based stacking method for multilabel classification	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Ensemble learning; stacking; artificial bee colony; cross-entropy; multilabel learning	PARTICLE SWARM OPTIMIZATION; BEE COLONY ALGORITHM; FEATURE-SELECTION; ENSEMBLE	Multilabel classification is a supervised learning problem wherein each individual instance is associated with multiple labels. Ensemble methods are effective in managing multilabel classification problems by creating a set of accurate, diverse classifiers and then combining their outputs to produce classifications. This paper presents a novel stacking-based ensemble algorithm, ABC-based stacking, for multilabel classification. The artificial bee colony algorithm, along with a single-layer artificial neural network, is used to find suitable meta-level classifier configurations. The optimization goal of the meta-level classifier is to maximize the average accuracy of classification of all the instances involved. We run an experiment on 10 benchmark datasets of varying domains and compare the proposed approach to five other ensemble algorithms to demonstrate the feasibility and effectiveness of ABC-based stacking.																	1300-0632	1303-6203					2019	27	6					4231	4245		10.3906/elk-1902-188													
J								A novel randomized recurrent artificial neural network approach: recurrent random vector functional link network	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Recurrent random vector functional link; random vector functional link; inner feedback; outer feedback	ALGORITHMS	The random vector functional link (RVFL) has successfully been employed in many applications since 1989. RVFL has a single hidden layer feedforward structure that also has direct links between the input layer and the output layer. Although nonlinearity, high generalization capacity, and fast training ability can be provided in RVFL, it can be found from the literature that higher nonlinearity can be obtained by adding recurrent feedback to an artificial neural network. In this paper, the recurrent type of RVFL (R-RVFL), which has both outer feedbacks and also inner feedbacks, is proposed. In order to evaluate and validate the proposed approach, a total of 109 public datasets were employed. Obtained results showed that R-RVFL can be employed successfully in terms of obtained success rates.																	1300-0632	1303-6203					2019	27	6					4246	+		10.3906/elk-1903-75													
J								Sparse Bayesian approach to fast learning network for multiclassification	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Extreme learning machines; fast learning network; Bayesian learning; sparse Bayesian; multiclassification	MACHINE; REGRESSION	This paper proposes a novel artificial neural network called sparse-Bayesian-based fast learning network (SBFLN). In SBFLN, sparse Bayesian regression is used to train the fast learning network (FLN), which is an improved extreme learning machine (ELM). The training process of SBFLN is to randomly generate the input weights and the hidden layer biases, and then find the probability distribution of other weights by the sparse Bayesian approach. SBFLN calculates the predicted output through Bayes estimator, so it can provide a natural marginal possibility for classification problems and can solve the overfitting problem caused by the least-squares estimation in FLN. In addition, the sparse Bayesian approach can automatically trim most redundant neurons in hidden layer, which makes the network more compact and accurate. To verify the effectiveness of the improvements in this paper, the results of SBFLN are evaluated in 15 benchmark classification problems. The experimental results show that SBFLN is not sensitive to the number of neurons in the hidden layer, and the performance of SBFLN is competitive or superior to some other state-of-the-art algorithms.																	1300-0632	1303-6203					2019	27	6					4256	4268		10.3906/elk-1808-75													
J								A Fine-grain and scalable set-based cache partitioning through thread classification	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Cache memory; partitioning algorithms; multicore processing		As contemporary processors utilize more and more cores, cache partitioning algorithms tend to preserve cache associativity with a finer-grain of control to achieve higher throughput and fairness goals. In this study, we propose a scalable set-based cache partitioning mechanism, which welds an allocation policy and an enforcement scheme together. We also propose a set-based classifier to better allocate partitions to more deserving threads, a fast set redirection logic to map accesses to dedicated cache sets, and a double access mechanism to overcome the performance penalty due to a repartitioning phase. We compare our work to the best line-grain cache partitioning scheme that is available in the literature. Our results show that set-based partitioning improves throughput and fairness by 5.6% and 4.8% on average, respectively. The maximum achievable gains are as high as 33% in terms of throughput and 23% in terms of fairness.																	1300-0632	1303-6203					2019	27	6					4269	4283		10.3906/elk-1903-183													
J								Parallel algorithms for computing sparse matrix permanents	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Sparse matrices; permanent; Ryser's algorithm; multicore CPUs; shared-memory parallelism; load balancing	COMPUTATION	The permanent is an important characteristic of a matrix and it has been used in many applications. Unfortunately, it is a hard to compute and hard to approximate the immanant. For dense/full matrices, the fastest exact algorithm, RYSER, has O(2(n-1)n) complexity. In this work, a parallel algorithm, SKIPPER, is proposed to exploit the sparsity within the input matrix as much as possible. SKIPPER restructures the matrix to reduce the overall work, skips the unnecessary steps, and employs a coarse-grain, shared-memory parallelization with dynamic scheduling. The experiments show that SKIPPER increases the performance of exact permanent computation up to 140x compared to the naive version for general matrices. Furthermore, thanks to the coarse-grain parallelization, 14-15x speedup on average is obtained with tau = 16 threads over sequential SKIPPER. Overall, by exploiting the sparsity and parallelism, the speedup is 2000x for some of the matrices used in the experimental setting. The proposed techniques in this paper can be used to significantly improve the performance of exact permanent computation by simply replacing RYSER with SKIPPER, especially when the matrix is highly sparse.																	1300-0632	1303-6203					2019	27	6					4284	4297		10.3906/elk-1904-135													
J								A crowdsensing-based framework for urban air quality decision support	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Air quality management; crowdsensing; mobile application; decisional system; pollution prediction; traffic regulation		Air pollution is considered a major health problem in urban areas. Small sensor technology integrated with smart phones can be widely used to collect air quality information in real time using mobile applications. By applying the concept of crowdsensing, citizens and authorities can be aware of exposure to pollution during their daily activities in urban areas. This paper describes an on-road air quality monitoring and control approach based on the crowdsensing paradigm. In addition to collecting air pollution data, we are exploring the possibility of using this technology to effectively detect critical situations and redistribute all information through a proactive decision support framework. This information can be combined with sensed air quality parameters for displaying, on an interactive map, the detected pollutants' concentrations using sensors attached to smart phones. The proposed framework provides users with real-time traffic and air quality information, traffic recommendations and notifications, and environmental conditions. Moreover, the authorities can use this system to improve urban mobility and traffic regulation. Such behavior and movements related to geographic information can provide a better understanding of the dynamics of a road network. In this work, we propose to combine the benefits of the crowdsensing paradigm with both machine learning and Big Data tools. An artificial neural networks model and the A* algorithm are used for air quality prediction and the least polluted path finding. All data processing tasks are performed over a Hadoop-based framework.																	1300-0632	1303-6203					2019	27	6					4298	4313		10.3906/elk-1809-22													
J								Performance evaluation of WebRTC-based online consultation platform	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										WebRTC; online consultation; video call; cross platform; telemedicine	HEALTH-CARE; TELEMEDICINE; COMMUNICATION; OUTCOMES; QUALITY; FUTURE; COST	Information technologies give patients the opportunity to communicate with medical professionals remotely. Telemedicine uses these technologies to provide advanced healthcare and medical services. We present a medical online consultation application based on Web Real-Time Communications (WebRTC) technology enabling chat, audio, and video calls. Communication architecture and protocols of the application are explained in detail. Additionally, the user interface of the application is shown via performed calls. The application is tested and evaluated on different network connections (3G, 4G, local, and DSL) and different browsers and mobile operating systems (Android, Chrome, Firefox, Internet Explorer, iOS, Opera, Safari). During calls, communication quality parameters such as round-trip time (RTT) and packet loss, obtained via the WebRTC application programming interface, are analyzed. 3G, 4G, and local connections show low packet losses (<1%). Packet losses are high (>1%) in Android, Chrome, iOS, Opera, and Safari for DSL connection, but RTT values are low (<100 ms) in all different conditions excluding iOS. In the presented application, RTT and packet loss remain lower than 100 ms and 1%, respectively, in various scenarios, indicating good communication quality. RTT and packet loss are related to total time and hang time parameters, which describe the necessary time to establish and to end a call. It is shown that communication quality of the application can simply be measured by analyzing the total time parameter. This enables predictable information for communication quality for WebRTC-based applications without continuously monitoring RTT and packet loss for the first time.																	1300-0632	1303-6203					2019	27	6					4314	4327		10.3906/elk-1903-44													
J								Rough fuzzy cuckoo search for triclustering microarray gene expression data	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Tricluster; Rough fuzzy cuckoo search; microarray gene expression data; rough fuzzy k-means; gene ontology; time series data analysis		Analyzing time series microarray gene expression data is a computational challenge due to its three-dimensional characteristics. Triclustering techniques are applied to three-dimensional data for mining similarly expressed genes under a subset of conditions and time points. In this work, a novel rough fuzzy cuckoo search algorithm is proposed for triclustering genes across samples and time points simultaneously. By applying the upper and lower approximation of rough set theory and the objective function of fuzzy k-means, rough fuzzy k-means was incorporated into a cuckoo search to handle the uncertainty of the data. The proposed method was applied to three real-life time series gene expression datasets. This work was evaluated using four validation indices and correlation analysis was performed to indicate the cluster quality. The proposed work was also compared with the existing triclustering algorithms and it outperformed the other methods.																	1300-0632	1303-6203					2019	27	6					4328	4339		10.3906/elk-1809-86													
J								A new technique for the measurement and assessment of carotid artery wall vibrations using ultrasound RF echoes	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Carotid wall; fractal dimension; Lyapunov exponent; entropy; phase information; radiofrequency signals	THICKNESS; ELASTICITY; PLAQUES; ATHEROSCLEROSIS; RADIOFREQUENCY; DOWNSTREAM; UPSTREAM; TRACKING; AGE	Atherosclerosis is known as the leading cause of heart attacks and brain strokes. One of the symptoms of this disease is the reduction of artery wall motion caused by age. This study presents a novel method to extract high frequency components of wall motion, wall vibrations, based on discrete wavelet transform. The fractal dimension, largest Lyapunov exponent, and spectral entropy are then analyzed to indicate the chaotic behavior in wall vibrations. Phase information from demodulated radiofrequency signals is extracted and the entropy of phase-difference is computed as a statistical measure for better characterization of the artery wall tissue. The results show that these features correlate with age (P < 0.001) and also increase with age. The phase-difference entropy also shows significant correlation with age (r = 0.34, P < 0.001). The measurement results indicate that while age increases, vibrations of the artery wall are irregular and represent chaotic behavior. Our results raise hopes that the proposed approach may be effective in diagnosing atherosclerosis.																	1300-0632	1303-6203					2019	27	6					4340	4353		10.3906/elk-1901-216													
J								Possible effects of dielectrophoretic fields in the brains of MRI operators and MS patients: a radiologically isolated syndrome evaluation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Multiple sclerosis; dielectrophoretic force; Dawson fingers; MRI; radiologically isolated syndrome; MRI operators; etiology	MULTIPLE-SCLEROSIS	Frequent use of magnetic resonance imaging (MRI) devices, which are major contributors in understanding health problems in the human body, is a subject that needs to be taken into consideration both for patients and for operators who are constantly in the vicinity of devices. In this context, electromagnetic impact assessment of an MRI device was performed at the point where the patient entered the device. Dielectrophoretic fields induced by radio frequency (RF) coils of an MRI scanner on male and female operator brain models were computed by using dispersive electrical medium parameters. The main cause of induced secondary dielectrophoretic fields by the RF coils of the MRI scanner is the veins modelled as monopole antennas on the lateral ventricle. The results explain that the dielectrophoretic fields near the veins on the ependymal surfaces are the main cause of Dawson fingers that may develop in the brains of multiple sclerosis (MS) patients and people at risk of the disease. Due to the use of the phantom results and the dispersive values of the electrical medium parameters, the results can be said to be close to the actual values and reliable. Therefore, the study will contribute to the confirmation of the hypotheses, developed by the author from a different perspective, related to the etiology of MS and will provide an accurate understanding of the concept of radiologically isolated syndrome. Everyone, including MRI designers, neurologists, radiologists, operators, and MS patients, can find any of the original information about MS that they need.																	1300-0632	1303-6203					2019	27	6					4354	4360		10.3906/elk-1811-149													
J								Automatic prostate segmentation using multiobjective active appearance model in MR images	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Active shape model; active appearance model; prostate segmentation; genetic algorithm; objective function; nonlinear filtering	MAGNETIC-RESONANCE; ULTRASOUND; ALGORITHM; ATLAS	Prostate cancer is the second largest cause of mortality among men. Prostate segmentation, i.e. the precise determination of the prostate region in magnetic resonance imaging (MRI), is generally used for prostate volume measurement, which can be used as a potential prostate cancer indicator. This paper presents a new fully automatic statistical model called the multiobjective active appearance model (MOAAM) for prostate segmentation in MR images. First, in the training stage, the appearance model, including the shape and texture model, is developed by applying principal component analysis to the training images, already outlined by a physician. Then noise and roughness are properly removed in the preprocessing step by Sticks filter and nonlinear filtering. This helps us provide the proper conditions for the prostate region detection. Finally, in order to detect the prostate region, a new multiobjective function is optimized using a suitable search algorithm. The proposed method has been applied to prostate images for segmenting the prostate boundaries. The evaluation results indicate that the presented method can yield a DSC value of 87.4 +/- 5.00%, is less sensitive to the edge information and initialization, and has a stronger capture range in comparison with existing methods.																	1300-0632	1303-6203					2019	27	6					4361	4377		10.3906/elk-1903-7													
J								Heart attack mortality prediction: an application of machine learning methods	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Machine learning; data mining; classification; incomplete data; imbalanced data; Bayesian networks; acute myocardial infarction		The heart is an important organ in the human body, and acute myocardial infarction (AMI) is the leading cause of death in most countries. Researchers are doing a lot of data analysis work to assist doctors in predicting the heart problem. An analysis of the data related to different health problems and its functions can help in predicting the wellness of this organ with a degree of certainty. Our research reported in this paper consists of two main parts. In the first part of the paper, we compare different predictive models of hospital mortality for patients with AMI. All results presented in this part are based on real data of about 603 patients from a hospital in the Czech Republic and about 184 patients from two hospitals in Syria. Although the learned models may be specific to the data, we also draw more general conclusions that we think are generally valid. In the second part of the paper, because the data is incomplete and imbalanced we develop the Chow-Liu and tree-augmented naive Bayesian to deal with that data in better conditions, and compare the quality of these algorithms with others.																	1300-0632	1303-6203					2019	27	6					4378	4389		10.3906/elk-1811-4													
J								Decoupling network for Tx/Rx body coil for 7T MRI	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Magnetic resonance imaging; 7-Tesla; mutual coupling; decoupling network; radiofrequency coils; resonant circuit; transmit/receive coil	HIGH-FIELD MRI; ARRAY; DESIGN; POWER; HEAD	The parallel imaging technique is widely used in 7T MRI scanners. It employs multichannel RF coil arrays to apply a concurrent excitation and acquisition method. Concurrent excitation faces significant challenges in terms of electromagnetic coupling between the RF coil elements. In order to prevent interference between the RF coil elements' exciters, several decoupling methods have been developed to compensate for coupling and to permit independent work for the exciters. This paper studies the coupling between meander coils arranged in two different geometrical setups and investigates the isolation performance between the coils by applying two different decoupling networks depending on the geometrical setup of the coils. These two decoupling networks in addition to a T-shaped decoupling network have been integrated into a Tx/Rx body coil for 7 T to compensate for mutual coupling between array coil elements. The results have been obtained by using CST Microwave Studio (CST AG, Darmstadt, Germany).																	1300-0632	1303-6203					2019	27	6					4390	4402		10.3906/elk-1904-171													
J								Compact metal-plate slotted WLAN-WIMAX antenna design with USB Wi-Fi adapter application	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Planar metal antenna; metal plate antenna; slotted antenna; dual band; triple band; USB WiFi adapter; WLAN application	MONOPOLE ANTENNA; ONE-PIECE; DIPOLE ANTENNA; FLAT-PLATE; CONCURRENT; SYSTEM; LOOP; PIFA	In this study, a compact antenna design, which operates in the 2.4, 5.2, and 5.8 GHz (WLAN) and 3.5 and 5.5 GHz (WiMAX) frequency bands, has been implemented to be compatible with the 802.11.ac/n standards. The proposed metal antenna is made of a copper plate of thickness 0 5 mm with a compact overall physical size of 20 mm x 30 mm Although it is low-profile, it can work with high efficiency because it has a cheap planar metal structure and it does not contain any expensive dielectric material. The antenna is investigated in terms of S parameters, input impedance, efficiency, surface current distributions, and radiation pattern. The implemented antenna has been used in a USB WiFi adapter design for a desktop computer as an indoor WLAN application. The protector outer jacket of the WiFi adapter has been designed using a 3D printer, and the adapter card and driver are acquired commercially. As a result, the produced WiFi adapter has been realized in a size of approximately 60% smaller than the other modules using commercially available monopole antennas on the market. The WiFi adapter provides IEEE 802.11.n/g/b standards and supports USB 2.0. It has been observed that the speed measurement tests have been performed successfully and that the download-link and upload-link can reach 600 Mbps data rates. In addition, BPSK, QPSK, and 16QAM with OFDM modulation techniques are used.																	1300-0632	1303-6203					2019	27	6					4403	4417		10.3906/elk-1904-122													
J								Improving word embeddings projection for Turkish hypernym extraction	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Projection learning; word embeddings; hypernym relation; deep learning		Corpus-driven approaches can automatically explore is-a relations between the word pairs from corpus. This problem is also called hypernym extraction. Formerly, lexico-syntactic patterns have been used to solve hypernym relations. The language-specific syntactic rules have been manually crafted to build the patterns. On the other hand, recent studies have applied distributional approaches to word semantics. They extracted the semantic relations relying on the idea that similar words share similar contexts. Former distributional approaches have applied one-hot bag-of-word (BOW) encoding. The dimensionality problem of BOW has been solved by various neural network approaches, which represent words in very short and dense vectors, or word embeddings. In this study, we used word embeddings representation and employed the optimized projection algorithm to solve the hypernym problem. The supervised architecture learns a mapping function so that the embeddings (or vectors) of word pairs that are in hypernym relations can be projected to each other. In the training phase, the architecture first learns the embeddings of words and the projection function from a list of word pairs. In the test phase, the projection function maps the embeddings of a given word to a point that is the closest to its hypernym. We utilized the deep learning optimization methods to optimize the model and improve the performances by tuning hyperparameters. We discussed our results by carrying out many experiments based on cross-validation. We also addressed problem-specific loss function, monitored hyperparameters, and evaluated the results with respect to different settings. Finally, we successfully showed that our approach outperformed baseline functions and other studies in the Turkish language.																	1300-0632	1303-6203					2019	27	6					4418	4428		10.3906/elk-1903-65													
J								Empirical model development for the estimation of clearness index using meteorological parameters	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Clearness index; meteorological variables; cloudiness; sunshine hours; statistical evaluation	GLOBAL SOLAR-RADIATION; DIFFUSE FRACTION; POWER-PLANTS; SAUDI-ARABIA; ENERGY; SYSTEM; FEASIBILITY	The clearness index is an indispensable parameter required for the design and analysis of solar energy systems. In the absence of measured values for a specific location, the clearness index can be estimated from other measured meteorological variables. In this study three meteorological parameters, sunshine hours, monthly mean values of the temperature difference (Delta T), and cloudiness, are used to develop empirical models for the estimation of clearness index. The empirical models are developed for five major cities in Pakistan (Karachi, Multan, Lahore, Islamabad, and Quetta). For empirical model development, long-term data (1991 to 2010) of monthly average clearness index, sunshine hours, average daily minimum and maximum temperatures, and cloudiness have been used. The accuracy of the models has been tested by statistical indicators that include mean percentage error (MPE), coefficient of determination (R-2), mean absolute relative error (MARE), mean bias error (MBE), and root mean square error (RMSE). The error analysis revealed that the proposed models are suitable for the estimation of the clearness index. It is also concluded that multiple regression models give better estimates of clearness index for all the stations (0.80 <= R-2 <= 0.86) compared to single parameter model and therefore are recommended. The study indicated that clear sky conditions prevail throughout the months at all the investigated sites (0.58 <= K-T( )<= 0.68), which is a good indicator for solar energy utilization. The statistical indicators also suggest that multilinear regression model M-3 gives a better representation of the climate system and using three parameters reduces the uncertainties in the developed model.																	1300-0632	1303-6203					2019	27	6					4429	4441		10.3906/elk-1903-27													
J								On the output regulation for linear fractional systems	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Regulation theory; fractional ordinary differential equations; linear systems; tracking of references		In this work, the regulation problem is extended to the field of fractional-order linear systems considering the Caputo fractional derivative. The regulation equations are obtained on the basis of the Francis equations. It is also shown that the linear fractional regulator exists at t = 0 only if the order of the plant is not greater than the order of the reference system.																	1300-0632	1303-6203					2019	27	6					4442	4455		10.3906/elk-1807-347													
J								An optimized harmonic elimination method based on synchronized microcontroller architecture	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Pulse width modulation; synchronized architecture; harmonic elimination; fast Fourier transform	CASCADED H-BRIDGE; GENERALIZED TECHNIQUES; THYRISTOR INVERTERS; VOLTAGE CONTROL; PWM; STRATEGY	This paper proposes an optimized synchronous PWM method for harmonic elimination in a quasi square wave inverter. The synchronized PWM method enables online harmonic computation and PWM pulse generation in a multitasking digital controller to eliminate lower order harmonics. The multitasking digital controller reduces the look-up table requirement and helps in realizing efficient implementation to eliminate dominant harmonics. This method offers a simple scalable solution for combined fifth and seventh harmonic elimination using two low-cost eight-bit PIC microcontrollers (PIC18F4550, PIC18F452). Experimental results are demonstrated for a single-phase three-level inverter. The proposed method achieves 90% reduction of fifth and seventh harmonic components from a quasi square wave inverter output.																	1300-0632	1303-6203					2019	27	6					4456	4471		10.3906/elk-1906-153													
J								Modified recycling folded cascode OTA with enhancement in transconductance and output impedance	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Amplifiers; CMOS; input referred noise; recycling folded cascode; slew rate; unity gain bandwidth; transconductance		A modified recycling folded cascode (MRFC) operational transconductance amplifier (OTA) for achieving high DC gain, slew rate, and unity gain bandwidth (UGB) is proposed in this paper. Positive feedback is adopted to enhance DC gain and unity gain bandwidth. The proposed MRFC OTA is compared with conventional folded cascode (FC), recycling folded cascode (RFC), and other OTAs existing in the literature. Three OTAs, FC, RFC, and MRFC, are realized and implemented using the UMC 180 nm CMOS process for the same bias current of 300 mu A. The designs are simulated in the Cadence Spectre Environment. From the simulation results, it may be noted that the proposed amplifier achieves a gain of 76.24 dB and unity gain bandwidth of 74.7 MHz with an input referred noise and slew rate of 139.2 mu V-rms and 64.05 V/mu s respectively. The proposed amplifier occupies an area of 2760 mu m(2).																	1300-0632	1303-6203					2019	27	6					4472	4485		10.3906/elk-1902-82													
J								NVRH-LUT: A nonvolatile radiation-hardened hybrid MTJ/CMOS-based look-up table for ultralow power and highly reliable FPGA designs	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Magnetic tunnel junction; nonvolatility; hybrid MTJ/CMOS logic circuits; radiation immunity; soft error; single-event upset; single-event double-node upset	SPIN; MRAM; TECHNOLOGY; MEMORY	Complementary metal oxide semiconductor (CMOS) downscaling leads to various challenges, such as high leakage current and increase in radiation sensitivity. To solve such challenges, hybrid MTJ/CMOS technology-based design has been considered as a very promising approach thanks to the high speed, low power, good scalability, and full compatibility of magnetic tunnel junction (MTJ) devices with CMOS technology. One important application of MTJs is the efficient utilization in building nonvolatile look-up tables (NV-LUTs) used in reconfigurable logic. However, NV-LUTs face severe reliability issues in nanotechnology due to the increasing process variations, reduced supply voltage, and high energetic particle strike at sensitive nodes of CMOS circuits. This paper proposes a nonvolatile radiation-hardened look-up table (NVRH-LUT) for advanced reconfigurable logic. Compared with previous works, the proposed NVRH-LUT is fully robust against single-event upsets and also single-event double-node upsets that are among the main reliability-challenging issues for NV-LUTs. Results have shown that NVRH-LUT not only provides increasing reliability and reduced bit error rate but also offers low delay and low energy consumption.																	1300-0632	1303-6203					2019	27	6					4486	4501		10.3906/elk-1812-179													
J								An improved space charge distribution analytical model to assess field-effect transistor's intrinsic capacitors	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Metal-semiconductor field-effect transistor; analytical capacitor model; Miller capacitors; Schottky barrier gate	I-V CHARACTERISTICS; MESFET	In this paper, an analytical model has been developed for improved assessment of Miller capacitors for high-frequency metal-semiconductor field-effect transistors. Depletion layer underneath the Schottky barrier gate has been divided into four distinct regions, and by evaluating the charges associated with each region, gate-to-source (C-GS) and gate-to-drain (C-GD) capacitors, commonly known as Miller capacitors, have been defined accordingly. Mathematical expressions have been developed both for the linear as well as for the saturation region. Miller capacitors and their variation as a function of applied bias have been assessed. It has been shown that the proposed technique offers better accuracy in determining the Miller capacitors, especially C-GD of the device relative to other reported analytical capacitor models. This improved accuracy has been achieved by involving the entire Schottky barrier depletion layer piecewise for the assessment of charges defining the Miller capacitors. Thus, the developed technique could be a useful tool in assessing the AC response of the device with more precision.																	1300-0632	1303-6203					2019	27	6					4502	4517		10.3906/elk-1810-43													
J								Hypothesis-based vertex shift method for embedding secret logos in the geometric features of 3D objects	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										3D objects; watermarking; peak signal-to-noise ratio; bit error rate; vertex signal-to-noise ratio; attack; translation attack; uniform scaling attack	WATERMARKING	A recent challenge in information technology is to protect secret data and preserve the ownership of a product. There are many duplicate products being released on a daily basis. Owners have a high risk in proving their products. Watermarking is a technique used to preserve ownership by hiding the owner's information in their products. The proposed hypothesis-based vertex shifting algorithm embeds 2D secret logos in 3D cover objects. The 3D objects are represented using vertices and facets. 3D watermarking faces various challenges and one among them is capacity. In this work, capacity is addressed by using a hypothesis-based vertex shift method that enables the embedding process for all the coordinates of the vertex. The method works by partitioning the vertex based on a shift factor called svalue. The svalue is chosen based on the visual quality of the watermarked object. The metrics used for testing are bit error rate for the recovered watermark, peak-to-signal noise ratio, and vertex signal-to-noise ratio (VSNR) of the watermarked 3D image. The proposed algorithm shows that a maximum of 3 bits can be embedded in a vertex when compared with the existing algorithms. The VSNR value of the proposed algorithm is high (125.87) compared to the existing algorithms. This shows that the algorithm withstands visual quality inspection. Hence, it is a robust watermarking algorithm for embedding secret logos into 3D objects with better visual quality and higher resilience against translation and uniform scaling attacks.																	1300-0632	1303-6203					2019	27	6					4518	4529		10.3906/elk-1810-62													
J								A novel method based on comparison using threshold scale for CFAR detectors under environments with conditions of electromagnetic interference	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										CA-CFAR; CMLD-CFAR; excision-CFAR; excision parameter; cell under test	ROBUST RADAR DETECTION; PERFORMANCE ANALYSIS; CLUTTER; GO; TARGETS; CA	Detection of a noisy signal is a complex process. Many radar systems are working in an environment where the signal processing parts cannot overcome the effects of interference sources due to their high power. These sources of conflict may completely erode the signal or may make a mistake in deciding. It may make the return of the echoes of the goals difficult. To solve this problem, the detector processor can use a new algorithm to estimate noise power and then can set the threshold in different positions of the cell under test. The proposed algorithm, by differentiating between homogeneous and interference environments in a multitarget structure, selects a set of reference cells that surround the cell under test to estimate the unknown noise/clutter and determine the effective threshold. Then, to evaluate the performance of cell averaging of constant false alarm rate (CA-CFAR), censored mean level detector CFAR (CMLDCFAR), and excision CFAR (EX-CFAR) detectors, we compared threshold, false alarm, and detection probability in terms of different correlation coefficients. The values were obtained using simulation by MATLAB software. The simulation results show that the excision parameter, by adding to the window of the reference cells that surround the cell under test, reduces the effects of background noise on the received signal. We conclude from the proposed method that the hybrid detector not only has higher quality detection interactions in heterogeneous environments but also has relatively less computational complexity than CA-CFAR, CMLD-CFAR, and EX-CFAR detectors.																	1300-0632	1303-6203					2019	27	6					4530	4540		10.3906/elk-1812-40													
J								A hybrid multiband printed loop antenna for WLAN/WiMAX bands for applications in MIMO systems	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Printed antenna; loop antenna; multiband; MIMO systems	WIMAX; WLAN	In this article a thin hybrid multiband printed loop antenna is presented for application in multiple-input multiple-output (MIMO) systems. The proposed antenna has several advantages; for example, the pseudofilter feature of the proposed antenna is one of this antenna's advantages. Because of the antenna's width (9 5 mm), it can be used on the edges of the board. The first resonant frequency is generated by the original loop of the antenna. Incorporation of different embedded components, i.e. a subsidiary loop, extended ground-traces, parasitic patch, and the slits, results in the proposed antenna resonating as a multiband antenna. This antenna is adjusted in such a way that it resonant in WLAN (2.4, 5.2, and 5.8 GHz) and WiMAX (3.5 GHz) bands. Four antennas are used as a MIMO system that are sequentially and perpendicularly placed along the edges of the board. In conclusion, the antenna design is confirmed according to simulation results obtained from the constructed prototype and associated measuring results.																	1300-0632	1303-6203					2019	27	6					4541	4550		10.3906/elk-1901-167													
J								Line independency-based network modelling for backward/forward load flow analysis of electrical power distribution systems	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Distribution system; backward/forward load flow; network modelling; independent lines	CURRENT INJECTION	In this paper a straightforward method for line independency-based modelling of electrical power distribution systems is proposed. The proposed method can determine the backward and forward sweeping routes of distribution systems for calculating line currents and bus voltages. To do that, the method identifies the independent lines in consecutive steps. An independent line is a line in the distribution system whose current does not depend on the current of other lines in the system. The proposed line independency-based network modelling is required to be performed only once and prior to the load flow analysis. The output of the proposed method, which is suitable for backward/forward load flow analysis, includes matrices which determine the steps, the order of lines, and the start and end points in the system for hierarchical calculation of currents and voltages. In this paper, the forward/backward approach is used as the load-flow algorithm since it is suitable for radial distribution systems with unbalanced loads. The proposed methodology is applied on two IEEE distribution systems and the results show its efficiency in load flow analysis.																	1300-0632	1303-6203					2019	27	6					4551	4566		10.3906/elk-1812-137													
J								Multiple distributed generations placement and sizing based on voltage stability index and power loss minimization	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Distributed generation; optimal distributed generation allocation; voltage stability; power loss reduction; voltage profile improvement	OPTIMUM DG PLACEMENT; ALLOCATION	This paper proposes a rapid analytical method to determine the locations and sizes of multiple distributed generations (DGs) inside a distribution network. DGs' locations are chosen with the aim of enhancing voltage stability and their sizes are picked so as to minimize system power losses. To evaluate the effect of the DG's nature on the system's performance, the proposed DG allocation method is tested for all DG types and the impact of the combination of different types of DGs is equally investigated, which offers a guide to designing an optimal hybrid network.																	1300-0632	1303-6203					2019	27	6					4567	4579		10.3906/elk-1812-45													
J								Optimal siting, sizing, and parameter tuning of STATCOM and SSSC using MPSO and remote coordination of the FACTS for oscillation damping of power systems	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										FACTS devices; interarea oscillations; phasor measurement units; stability; wide-area; coordination	STABILITY; SVC	In electromechanical oscillation damping within power system, power system stabilizers (PSSs) are often deployed. However, a PSS is less effective in damping interarea oscillation and is limited by changes in network configuration due to weak tie-lines and load variations. Consequently, this paper presents a wide-area coordination approach that damps interarea oscillations using FACTS devices and phasor measurement units. We selected a static synchronous compensator (STATCOM) and static series synchronous compensator (SSSC) for realistic power system interarea oscillation damping. The performance of the coordinated FACTS installed in a power system depends on their suitable locations, sizes, tuned parameters, and remote input signal selection. Hence, we formulated a multiobjective problem to provide the STATCOM and SSSC's optimal solutions using multiobjective particle swarm optimization. In addition, we employed a participation factor to select suitable generator speed deviations as the wide area stabilizing signal. The proposed approach was tested on different configurations of the Kundur 2-area 4-machine test system within MATLAB and Psat environments. The outcome of the nonlinear simulation proved that the multimachine power system stability was enhanced.																	1300-0632	1303-6203					2019	27	6					4580	4595		10.3906/elk-1811-14													
J								Computation of stability regions for load frequency control systems including incommensurate time delays	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Incommensurate time delays; stability region; PI controller design; load frequency control systems	DEPENDENT STABILITY; PI CONTROLLERS; CONSTANT; FEEDBACK; GAIN	This article studies the impact of incommensurate communication time delays on stability regions defined in proportional-integral (PI) controller parameter space for a two-area load frequency control (LFC) system. Distributed power generations and large power plants increase the complexity and control issues of interconnected power systems. In interconnected power systems, LFC systems need to have complex communication networks to exchange data between control center and geographically dispersed generations. The receiving/transmitting of remote measuring data through communication infrastructures causes inevitable time delays, which adversely affect controller performance and stability of the LFC system. Time delays introducing feedback control loops of a multiarea LFC system could exhibit incommensurate characteristics. In this study, a simple graphical method based on extracting a stability boundary locus is implemented to get PI controller parameters responsible for stabilizing the LFC system having incommensurate delay values. The boundaries of the stability regions in the PI controller parameter space are confirmed by time-domain simulations and a numerical algorithm known as the quasipolynomial mapping-based root finder algorithm. Results illustrate that incommensurate delays have remarkable effects on the stability region.																	1300-0632	1303-6203					2019	27	6					4596	4607		10.3906/elk-1904-6													
J								The effect of snowfall and icing on the sustainability of the power output of a grid-connected photovoltaic system in Konya, Turkey	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Hybrid inverter; icing; photovoltaic; snowfall	PERFORMANCE; IMPACT; PLANT	When the module surface is covered with various factors such as snow and icing which prevent the solar irradiance from reaching the photovoltaic cells, the power production of the system and its performance decrease. The purpose of this study is to determine the energy production losses of a grid-connected photovoltaic plant due to snowfall and icing. The effect of snowfall and icing has been examined on a photovoltaic system consisting of a hybrid inverter with two separate maximum power point tracking inputs and 36 monocrystalline modules, which are mounted on the supporting system horizontally in the south direction and at a constant tilt angle of 30 degrees. The plant area is located in a priority and snowy region (Konya,Turkey) where large-scale photovoltaic system installations are carried out. In order to evaluate the effect of snowfall, the minute resolution data of the hybrid inverter which provides connection to the grid is used. The change over time of the power generated by the two arrays of the plant was examined comparatively. For comparison, one of the arrays was continuously cleared. The recorded data was used to determine the expected energy output of the array covered with snow. Besides, the solar irradiance and ambient temperature data obtained from the meteorological station were used to accurately identify and evaluate the effects of snowfall with digital images recorded in the site area. The results showed that surface clearing of modules had a significant positive effect on the power output of the system. In the array entirely covered with snow, the daily energy loss exceeds 93%. In months of heavy snowfall, the monthly energy loss is 18% depending on time of being covered with snow of the modules. When the production data of 2017 and 2018 is evaluated, it is seen that the total energy loss of the plant varies between 1% and 2%.																	1300-0632	1303-6203					2019	27	6					4608	4623		10.3906/elk-1901-178													
J								The impact of demand response programs on UPFC placement	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										FACTS devices; demand response programs; harmony search algorithm; congestion management; loss reduction	MULTIOBJECTIVE OPTIMAL LOCATION; POWER-SYSTEMS; CONGESTION MANAGEMENT; FACTS DEVICES; TRANSMISSION; ALLOCATION; CONTROLLER; ALGORITHM; OPERATION	Demand response (DR) and flexible AC transmission system (FACTS) devices can be effectively used for congestion management in power transmission systems. However, demand response program (DRP) implementation can itself affect the optimum location of FACTS devices, which is one of the main issues in power system planning. This paper investigates the impact of DRPs on unified power flow controller (UPFC) placement. The harmony search algorithm is employed to determine the optimum locations and parameter setting of UPFC in a long-term framework. The optimization problem is solved with different objectives including generation and congestion cost reduction, as well as loss reduction. In this paper, the proposed approach is analyzed using 5 different cases, which are defined in such a way that they demonstrate the impact of DRPs on the UPFC placement problem. The IEEE reliability test system is used as an illustrative example to demonstrate the necessity of considering DRPs for UPFC placement.																	1300-0632	1303-6203					2019	27	6					4624	4639		10.3906/elk-1901-54													
J								Global maximum operating point tracking for PV system using fast convergence firefly algorithm	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Global maximum operating point; PV panel; fast convergence firefly algorithm; partial shading	POWER	Global maximum operating point (GMOP) tracking is an important requirement of solar photovoltaic (PV) systems under partial shading conditions (PSCs). Though the perturb and observe algorithm is simple and effective, it fails to recognize the GMOP. This paper explores the application of the firefly algorithm (FA) to the maximum power point tracking (MPPT) problem of PV systems. In order to determine the shortest path to reach the GMOP under various PSCs, a new fast convergence firefly algorithm (FA) is proposed. Additionally, the change in firefly position is limited to a maximum value identified based on the characteristics of the PSC. The fast convergence method is guaranteed to find the GMOP, avoiding the local operating point obstacle through a repeated space search technique. Using MATLAB, the algorithm is implemented on a model PV system. An experimental 300-W PV system is developed to validate the operating point of the PV system under various PSCs. The proposed method is tested on a 5-kW solar power plant. The results demonstrate that the proposed MPPT algorithm outperforms particle swarm optimization, FA-based MPPTs, and other methods available in the literature.																	1300-0632	1303-6203					2019	27	6					4640	4658		10.3906/elk-1805-108													
J								A new approach for wind turbine placement problem using modified differential evolution algorithm	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Wind turbine placement; binary-real coding; differential evolution algorithm; optimization	OPTIMIZATION; FARM	Energy use is increasing worldwide with industrialization and advancing technology. Following this increase, renewable energy resources are increasingly preferred to reduce the costs of energy production. Wind energy is preferred as a renewable energy resource because it is clean and safe. Wind turbines are used to meet the demand for wind energy. They are placed close to each other to generate higher amounts of energy. However, the wake effect problem arises in these types of layouts, and this hinders the turbines from producing the desired yield. A modified differential evolution (MDE) algorithm was proposed in this study to solve the placement problem for wind turbines, and employed a binary-real-coded method - obtained by combining binary coding and real coding. The proposed method contains three different modifications: generation of the initial population, regeneration, and mutation. The effective distribution of the wind turbines on land was achieved with a preliminary operation proposed to generate the initial population. In addition, with the MDE method, population regeneration and elitism were carried out to increase the diversity of population and to preserve the success of the method. Finally, a mutation operation was performed on the individuals to alternate the presence or absence of wind turbines. To investigate the performance of the MDE method in solving the wind turbine placement problem, the method was applied to a study area of 2 x 2 km. The results were compared with those obtained with other methods used in the published literature for the wind turbine placement problem. The most successful and productive placement was achieved using the proposed method, and experimental results showed that the MDE is an efficient and successful tool to solve the wind turbine placement problem.																	1300-0632	1303-6203					2019	27	6					4659	4672		10.3906/elk-1901-192													
J								A transmission optimization algorithm for smart load controllers	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Smart grid; smart load controller; transmission optimization algorithm		This paper introduces a transmission optimization algorithm for wireless transmissions between smart load controllers and a corresponding gateway in wireless personal area networks, where smart load controllers connect several electrical appliances through their corresponding load interfaces, measure the power consumption from each electrical appliance connected to the load controller, and control on/off switching through its load interface. The aim of this paper is to reduce the traffic load of power consumption data in electrical appliances used in the building area network and the smart grid network. The proposed algorithm allows the smart load controller to efficiently reduce traffic load even if watt-hours calculated in the smart load controllers are the same as the watt-hours in the gateway and the errors of the present values of power consumption interpreted in the gateway fall under the typical tolerance allowed in the specifications of manufacturers of watt-hour meters used in the electrical power industry.																	1300-0632	1303-6203					2019	27	6					4673	4688		10.3906/elk-1904-114													
J								Dynamic radar cross-section characteristic analysis of wind turbine based on scaled model experimental	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Reradiation interference; wind turbine scaled model; dynamic radar cross-section; experimental measurement	MITIGATION	Accurately acquiring and analyzing the dynamic radar cross-section (RCS) of wind turbine have a great significance to solve the reradiation interference between wind farms and radar stations. Since the results of high-frequency approximation algorithm are only applicable to the qualitative analysis of electromagnetic scattering, it is almost impossible to accurately acquire the dynamic RCS of wind turbine in actual engineering cases. To this end, we proposed to acquire the dynamic RCS of wind turbine based on the scaled model experimental measurement in a large anechoic chamber. The key techniques of setting up the scaled model as well as the experimental platform were described based on the principle of electromagnetic similarity. The accuracy of experimental result is verified by the comparison with numerical calculation and full-sized experiment reported in literature. By using the control variable method, we were able to measure and analyze the amplitude and phase variation of dynamic RCS with frequency, azimuth, and rotational speed, and achieved the transformation of RCS data into engineering practice. This not only lays a foundation for solving the reradiation interference between wind farms and radar stations, but also provides data support for subsequent theoretical research.																	1300-0632	1303-6203					2019	27	6					4689	4701		10.3906/elk-1901-214													
J								Design and control of an LCL-type single-phase grid-connected inverter with inverter current feedback using the phase-delay method	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Inverter; grid-connected system; proportional resonant controller; solar energy; AC module; LCL filter	ALGORITHM	In this study, a novel single-phase grid-connected microinverter system and its control applications are introduced for solar energy systems. The proposed system consists of two stages to transfer solar power to the grid. In the first stage, an isolated high-gain DC/DC converter is used to increase low solar panel output voltage. In the second stage, an inverter is used to supply a sinusoidal current to the grid. Moreover, a proportional resonant controller is adopted to reduce grid current total harmonic distortions (THDs) and an LCL filter is used to provide better harmonic attenuation. However, the ratio between the sampling frequency f(s) and the resonance frequency f(res) should be greater than 6 in the LCL filter with the inverter current feedback for a stable system. In order to obtain higher phase margins, the sampling frequency should be increased, which increases the inverter switching frequency. The present study shows that f(s)/f(res )can be lower than 6 by placing a phase delay on the inverter current feedback path to guarantee adequate stability margins. The effectiveness and feasibility of the proposed method are confirmed by the experimental test results based on a 300-W laboratory prototype.																	1300-0632	1303-6203					2019	27	6					4702	4714		10.3906/elk-1709-141													
J								Comparative evaluation of a-b-c and stationary frame of reference for permanent magnet brushless DC motor drive applied for generation of switching pattern	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Permanent magnet brushless DC motor; stationary reference frame; current control; model predictive control	COMMUTATION-TORQUE-RIPPLE; MINIMIZATION; REDUCTION; STRATEGY; SPEED; STATE; BLDCM	This paper focuses on the evaluation of the operation of permanent magnet brushless DC (PM BLDC) motor through the control implemented in the natural reference frame (a-b-c frame) and the stationary reference frame (alpha - beta frame). To a large extent, ripple-free torque and attainment of higher torque-speed characteristic depend on the gating pulses/switching patterns of the inverter employed. PM BLDC motor requires injection of the square wave current for the maximum torque per ampere, so the reference is required to be generated and tracked accordingly. The effectiveness of the strategies of pulse-width-modulation-based current control in the a-b-c plane and, the case of single-voltage-vector model predictive control and proposed two-voltage-vector model predictive control in the stationary plane have been evaluated. The parameter for comparison is based on improved tracking performance of the reference, ease in analysis of the control technique and less complexity in implementation of the control. The analysis has been verified through experimentation, with the control implemented through C2000 TMS320F28335 microcontroller commenting on the advantages of the stationary-plane-based approach.																	1300-0632	1303-6203					2019	27	6					4715	4730		10.3906/elk-1901-161													
J								A novel initial rotor position alignment method for permanent magnet synchronous motor using incremental encoder	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Permanent magnet synchronous motors; initial rotor position; incremental encoder	INJECTION; ALGORITHM	This paper presents a new method for the alignment of the rotor of permanent magnet synchronous motors with the phase axis of the stator during start-up. Once the rotor alignment is achieved, the real rotor position angle can be measured by using an incremental encoder and this value can be used in the field oriented control of the motor. Typically, a current is forced into the q-axis. In the proposed method a current is formed in the d-axis instead. Rotor alignment with the phase axis is achieved without any sudden motion by using a PI controller in the current loop. Preventive measures for exceptional situations that may occur during the application of this method are also discussed. Experimental results show that the performance of the proposed method is very satisfactory.																	1300-0632	1303-6203					2019	27	6					4731	4743		10.3906/elk-1901-28													
J								A two-stage power converter architecture with maximum power extraction for low-power energy sources	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Power converter; boost converter; charge pump; energy harvesting; maximum power	MICROBIAL FUEL-CELL; MANAGEMENT-SYSTEM	A two-stage power converter with maximum power extraction for energy harvesting is presented. The power converter consists of two stages; a maximum power extraction stage (i.e. first stage) and a regulation stage (i.e. second stage). The first stage consists of a number of charge pumps connected in parallel to extract power from the energy source while the second stage steps up low input voltage level to a usable level for a load. Proposed converter operates as low as 0.3 V and the output up-converts to 3.3 V. The proposed converter is aimed to extract maximum power from either low-power energy sources or high-power energy sources without increasing the complexity of the converter. Measured results indicate that the tracking efficiency is enhanced by 117% 123% over a single charge pump in the first stage converter. Proposed power converter provides 66.7% more power extracted from the energy source than the single charge pump one. The end-to-end efficiency is enhanced by 1.67X as compared to the single charge pump implementation.																	1300-0632	1303-6203					2019	27	6					4744	4755		10.3906/elk-1811-6													
J								Quantification of resistive wall instability for particle accelerator machines	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Wake potential; resistive wall impedance; medium-beta long bunches		The aim of this study is to quantify longitudinal resistive wall impedances, corresponding wake functions, and wake potentials for different accelerator machines of interest. Accurate calculations of wake potentials by particle-in-cell codes are extremely difficult for the investigated parameters; therefore, we use an analytical approach and consider large domains with fine discretization for the required numerical integrations. The semianalytical wake potential computations are benchmarked against numerical general purpose 2D/3D Maxwell solver software codes and a different analytical approach for a certain set of parameters. We report examples to illustrate limitations of wake potential estimations from coupling impedances, and computations for the machines using realistic beam parameters and machine conditions. A numerical example where the aim is to find the wake potential of the machine from the 5% noisy impedance data is given.																	1300-0632	1303-6203					2019	27	6					4756	4767		10.3906/elk-1903-194													
J								Survey of network embedding techniques for social networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Dimensionality reduction; network embedding; latent space		High dimensionality of data is a challenging scenario in the current era as the digital transformation of the society is in process. This problem is particularly complex in social networks as in such systems, it is coupled with other challenges such as interdependency of data points and heterogeneity of data sources. To overcome such disadvantages and aid in creation of downstream applications for social network analysis, network embedding techniques have been proposed. These techniques, in themselves, are not important but are the backbone of various network-based applications. Due to the scientific interest in this domain there has been a mushrooming of embedding techniques. It has therefore become crucial to learn the intuitions behind these techniques in order to compare and contrast them. The current analytical study is drawn with the following broad objectives: providing practitioners with understanding of network representative learning mathematical study of state-of-the-art techniques and highlighting the evolution of the literature in this field.																	1300-0632	1303-6203					2019	27	6					4768	4782		10.3906/elk-1807-333													
J								Predicting CO and NOx emissions from gas turbines: novel data and a benchmark PEMS	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Predictive emission monitoring systems; CO; NOx; exhaust emission prediction; gas turbines; extreme learning machine; database	EXTREME LEARNING-MACHINE; MODEL	Predictive emission monitoring systems (PEMS) are important tools for validation and backing up of costly continuous emission monitoring systems used in gas-turbine-based power plants. Their implementation relies on the availability of appropriate and ecologically valid data. In this paper, we introduce a novel PEMS dataset collected over five years from a gas turbine for the predictive modeling of the CO and NOx emissions. We analyze the data using a recent machine learning paradigm, and present useful insights about emission predictions. Furthermore, we present a benchmark experimental procedure for comparability of future works on the data.																	1300-0632	1303-6203					2019	27	6					4783	4796		10.3906/elk-1807-87													
J								Adaptive iir filter design using self-adaptive search equation based artificial bee colony algorithm	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Artificial bee colony; digital infinite impulse response filters; system identification; self-adaptive strategy	IDENTIFICATION; OPTIMIZATION; SYSTEMS; SIZE	Infinite impulse response (IIR) system identification problem is defined as an IIR filter modeling to represent an unknown system. During a modeling task, unknown system parameters are estimated by metaheuristic algorithms through the IIR filter. This work deals with the self-adaptive search-equation-based artificial bee colony (SSEABC) algorithm that is adapted to optimal IIR filter design. SSEABC algorithm is a recent and improved variant of artificial bee colony (ABC) algorithm in which appropriate search equation is determined with a self-adaptive strategy. Moreover, the success of the SSEABC algorithm enhanced with a competitive local search selection strategy was proved on benchmark functions in our previous studies. The SSEABC algorithm is utilized in filter modelings which have different cases. In order to demonstrate the performance of the SSEABC algorithm on IIR filter design, we have also used canonical ABC, modified ABC (MABC), best neighbor-guided ABC, and an ABC with an adaptive population size (APABC) algorithms as well as other algorithms in the literature for comparison. The obtained results and the analysis on performance evolution of compared algorithms on several filter design cases indicate that SSEABC outperforms all considered ABC variants and other algorithms in the literature.																	1300-0632	1303-6203					2019	27	6					4797	4817		10.3906/elk-1809-83													
J								Zebra crossing detection based on HSV colour model and projective invariant for driver assistance system	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										zebra crossing; crosswalks; intelligent transport systems; HSV; region of interest; Moore-neighbourhood; bounding quadrilateral; K-means clustering; Calinski-Harabasz; projective invariant		This paper implements a colour and shape-based Zebra crossings detection model. As Zebra crossings are white coloured, a robust binarisation based on the V channel of the input image is used to find the large rectangle sized white stripes of Zebra crossing. After the morphological open operation, boundary tracing is done using Moore-neighbourhood tracing algorithm along with Jacob's stopping criterion. For every region, the bounding quadrilateral with the minimum area is computed and the candidate regions are then filtered out using some criteria, e.g., aspect ratio. The y-coordinates of the lower vertices of the white stripes are considered as 'cluster' and the optimal number of clusters is determined using the Calinski-Harabasz criterion. These clusters are now checked to determine whether they represent crosswalks and then validate using projective invariant. Various Zebra crossing images are used to test the proposed framework and the results are presented to prove its effectiveness.																	2049-887X	2049-8888					2019	6	1					1	14		10.1504/IJAPR.2019.104278													
J								Using empirical mode decomposition and Kullback-Leibler distance for online handwritten signature verification	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										online signature; verification; biometrics; Kullback-Leibler divergence; empirical mode decomposition; EMD; Hilbert transform; HT	TRANSFORM	The signature is one of the most accepted biometric modalities in the world. In this paper, we present a new method for online handwritten signature based on the empirical mode decomposition (EMD). After extracting each of the signature coordinates, a phase of preprocessing and normalisation is carried out. Then, the features of the signatures are extracted by using the EMD. After that, three similarity measures are used to match the signatures between them. The database used in our work is the one used in SVC (2004). Experimental results confirm the effectiveness of our approach and show the level of its reliability. Finally, the proposed method gives an EER of 2.03% and allows high rates of recognition compared to other approaches.																	2049-887X	2049-8888					2019	6	1					15	29		10.1504/IJAPR.2019.104279													
J								Influence of noise, light and shadows on image segmentation algorithms	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										image segmentation; image clustering; algorithms		Image segmentation is a required step in object recognition tasks, dividing the image into multiple regions or clusters. Each image pixel is assigned to one of the clusters using different metrics such as pixel colour value, grey-scale intensity, edges, shapes, among others. The most diverse image segmentation algorithms have been proposed. It is important to analyse the robustness of those algorithms, taking into consideration diverse types of noise and difficult conditions. In this paper we choose a small set of frequently-used algorithms and analyse their behaviour in robustness-testing conditions. We inject difficulties (noise, shadows, various degrees of illumination) and compare the quality of the segmentation of algorithms against a ground truth. The objective is to analyse how differences in illumination, shadows or noise influence the output of the algorithms, and how they compare on those metrics. Based on those results we conclude about the quality of the approaches tested.																	2049-887X	2049-8888					2019	6	1					30	42		10.1504/IJAPR.2019.104282													
J								Deep neural network-based phoneme classification of standard Khasi dialect in continuous speech	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										acoustic model; deep neural network; DNN; Gaussian mixture model; GMM; hidden Markov model; HMM; language model; Mel frequency cepstral coefficient; MFCC; phone error rate; PER; voice activity detection; VAD; word error rate; WER		In this paper, deep neural network (DNN) is used to classify phonemes of the standard Khasi dialect which is one of the commonly used dialects in the state of Meghalaya. For this, clean speech data were recorded in the laboratory from native speakers. In the proposed system, a monophone and a triphone hidden Markov models (HMMs) were also built to compare the results obtained. Our finding showed that DNN outperformed the classification over the other two models with a classification accuracy of 89.70%.																	2049-887X	2049-8888					2019	6	1					43	51		10.1504/IJAPR.2019.104288													
J								Player skill estimation for soccer match prediction	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										Elo ranking; skill-based ranking; soccer ranking		In this paper we propose two algorithms for assessing skill of soccer players. We introduce an adaptation of the Elo rating algorithm, which is widely used in chess, to handle teams. A different approach is proposed to assess player skill as a function of the proportion of matches won. Since it is hard to measure skill directly, to estimate the performance of our approach we propose as a proxy the number of correctly predicted match outcomes of a classifier that takes as input our algorithms' output. Numerical experiments suggest that our approach is competitive with book-keeper's estimates, even though we rely only on historical game outcome data.																	2049-887X	2049-8888					2019	6	1					52	57		10.1504/IJAPR.2019.104292													
J								In-line grading system for mango fruits using GLCM feature extraction and soft-computing techniques	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										mature mango; unripe mango; food quality control; radial basis function; RBF; neural network	FACE RECOGNITION; ANTICIPATION	In the fruit production industries and supermarkets, mature (ripe) fruits are demanded for consumption by the consumers and also for production in fruit processing industries. Therefore, there is an urgent need for an in-line grading system in such industry to aid the grading of mango fruit; in order to enhance the use of ripe and mature mangoes for production. Also, such in-line grading systems will speed up the production in these industries since machines are faster which gives a better and standard result as compared with human operators. In this work, we have implemented an in-line grading system using GLCM feature extraction and soft computing techniques. Two models have been implemented to classify the mango fruits into mature (ripe) and immature (unripe) fruits. These models are the feed-forward network trained with back-propagation neural network and the radial basis function network. These models are compared with each other and also with the result of other proposed systems using the same database to ascertain the best result required in such industry.																	2049-887X	2049-8888					2019	6	1					58	75		10.1504/IJAPR.2019.104294													
J								A robust multi-level sparse classifier with multi-modal feature extraction for face recognition	INTERNATIONAL JOURNAL OF APPLIED PATTERN RECOGNITION										multi-level sparse; MLS classifier; face recognition; sparse representation; pattern recognition; nearest neighbours	PRINCIPAL COMPONENT ANALYSIS; DISCRIMINANT-ANALYSIS; REPRESENTATION; PCA	In the past few years, face recognition based on sparse representation is providing satisfactory classification accuracy. Face images involved in real life applications usually exhibit considerable pose, lighting, and expression variations, resulting in significant performance degradation of traditional sparse-based algorithms. In this paper, a novel face recognition method is developed as multi-level sparse (MLS) classifier with multi-modal feature extraction, which integrates benefits of sparse representation manifolds. In MLS classifier, sparse representation-based classification is performed at multiple levels to extract the hierarchical relationship information between training and testing images, which not only improves classification accuracy but also makes the system scalable. Also, the use of multi-modal feature in MLS classifier makes it discriminative to face changes while robust to intra-personal variations. To highlight the competency of proposed method, results are compared with sparse representation and other existing state-of-art methods in terms of mean classification error. An investigation on classification accuracy is performed to showcase the reliability of proposed method.																	2049-887X	2049-8888					2019	6	1					76	102		10.1504/IJAPR.2019.104300													
J								AN INTEGRATED THREE-FLOW APPROACH FOR FRONT-END SERVICE COMPOSITION	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										End-User Service Composition (EUSC); Front-End Service Composition (FESC); Application User Interface; End-user programmers		End-User Service Composition (EUSC) aims to enable end-user programmers who are not professional developers develop applications by composing or aggregating existing web services. Despite the effort, studies have shown that, end-user programmers are not able to deal with the technical complexities involved in EUSC. One way to deal with this issue is Front-End Service Composition (FESC), which allows end-user programmers to compose web services at the presentation layer of an application by configuring user interface (UI) widgets that represent the back-end web services. However, there are not many studies on FESC and the existing ones suffer two main problems, namely, lack of control flow and/or application flow visualization, and they still require end-user programmers to have certain technical knowledge in the service composition process that these end-user programmers generally do not possess. Following that, this study proposes an integrated three-flow approach (application flow, control flow and data flow) to deal with the current limitations of FESC. The approach generates the GUI of web services automatically, thus allowing the UI of the application to be developed at the same time the required web services are assembled. The approach allows end-user programmers to explicitly configure the three different types of flows involved in service composition. A proof-of-concept prototype, QuickWSC, that incorporates the three-flow approach was developed. It adopts a side-by-side multiple-view design to support visual configuration of the three flows in an uncluttered yet synchronized manner that adhered to established design guidelines. A user evaluation study was conducted on QuickWSC. The results show that it is easy to compose web services by explicitly specifying the three flows, that the configurations of the three flows integrated in the two views helps in composing application from web services, and that no technical knowledge is required to use QuickWSC.																	0127-9084						2019					3		1	24		10.22452/mjcs.sp2019no3.1													
J								A COMPARATIVE STUDY OF THREE CORNER FEATURE BASED MOVING OBJECT DETECTION USING AERIAL IMAGES	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Moving Object Detection; Corner; Computer Vision; Image Processing		Corner feature based moving objects detection is an essential and fundamental research problem in the broader aspects of computer vision and pattern recognition research domain. Performance of various corner features based aerial types image processing specially for moving objects detection is still an unsolved issue due to up and down of performance which makes it difficult to choose the appropriate corner features for detection purpose. The core part mentioned in this research is to categorize significant corner characteristics of the objects using various corner features based detection methods in image extracted from aerial video. This research demonstrated three kinds of corner features, i.e. Moravec, Susan and Harris corners due to capability of these corner features to interpret high and low intensity various for aerial types of images. Standard datasets were used to evaluate each of the corner feature based detection. Based on comprehensive experimental analysis, Harris corner was observed performing efficiently comparing with Moravec and Susan corner based detection for both datasets considered by this research. Experimental results reveals the capacity of each corner characteristics based detection methodology in terms with the effectiveness using various performance metrics for moving object detection using aerial images.																	0127-9084						2019					3		25	33		10.22452/mjcs.sp2019no3.2													
J								EVOLUTIONARY COMBINATORIAL OPTIMIZATION FOR WORD EMBEDDING (ECOWE) IN SENTIMENT CLASSIFICATION	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Evolutionary Computation; Genetic Algorithm; Combinatorial Optimization; Word Embedding; Sentiment Analysis; Sentiment Classification		Word embedding is widely used in various natural language processing (NLP) tasks, especially sentiment classification. Huge computational costs for training new embeddings from scratch have made pretrained word embeddings such as Word2Vec and Glove popular for reuse of word vectors. Inadequacy of a single embedding necessitated newer techniques to combine two embeddings. However, the combined embeddings proposed in existing works are static and only provide a one-size-fits-all solution regardless of the problem and dataset at hand. Optimization is a more promising technique to overcome the limitations of simplistic techniques in existing works related to combined word embeddings because optimization provides unique and optimal solutions according to the problem and dataset at hand. In this paper, a new genetic algorithm based combinatorial optimization algorithm called Evolutionary Combinatorial Optimization for Word Embedding (ECOWE) is proposed to produce combinations of word embeddings, which yield optimal accuracy for the specific sentiment classification dataset that is used. Results show that absolute percentages of improvement ranging from 1.7% to 12.9%, averaging around 5.5% and relative percentages of improvement ranging from 2.4% to 19.5%, averaging around 8.1% have been achieved over the benchmark model accuracy values for all datasets. The ECOWE accuracy values for all datasets have also been found to be statistically significant compared to benchmark accuracy values with a z-score of - 2.2014 using two-tailed Wilcoxon signed rank test with 5% significance level.																	0127-9084						2019					3		34	45		10.22452/mjcs.sp2019no3.3													
J								ROBUST FEATURE EXTRACTION BASED ON SPECTRAL AND PROSODIC FEATURES FOR CLASSICAL ARABIC ACCENTS RECOGNITION	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Quranic accents; spectral; prosodic; MFCC; GMM; Malay speakers; ASR	SPEAKER RECOGNITION; SPEECH; IDENTIFICATION	The variability of speech patterns produced by individuals is unique. The uniqueness is due to the accent influenced by the individual's native dialect. Modeling individual variation of spoken language is a challenge under the Automatic Speech Recognition (ASR) field. The individual differences concerning of accent revealed the critical issues in Classical Arabic (CA) recitation among Malay speakers. This problem is caused by the misarticulate phonemes, which affected by the Malay colloquial dialect and native language. Most of ASR researchers are unable to understand the behavior of phonemes and speech patterns in CA, thus degrading the ASR performance. This paper focuses on identifying the accent of Malay speakers on the recitation of Surah Al-Fati.ah with 7 Quranic accents, using the proposed feature extraction technique. In this work, the technique presented is a combination of spectral and prosodic features, which are mainly designed for accent in ASR. Differed with current conventional method, where the spectral feature alone has been applied for feature extraction in many ASR research. The prosodic elements in CA such as pitch, energy and spectral-tilt need to be taken into consideration, thus a significant variety of features for each phoneme able to help in distinguishing one accent from another. Meanwhile, the spectral representation of Mel-Frequency Cepstral Coefficients (MFCC) is utilized for the decorrelating property of the cepstrum. At present, Gaussian Mixture Models (GMM) has been applied for the classification stage. From experimental results, the system performance is the best when the prosodic is integrated with MFCC, alongside the GMM with 81.7%- 89.6% of accuracy. It was 5.5%-7.3% increment as compared to MFCC alone.																	0127-9084						2019					3		46	72		10.22452/mjcs.sp2019no3.4													
J								USABILITY TESTING ON MOTIONMOUSE: A PROTOTYPE TO CONTROL ANDROID TABLET USING EMOTIV EPOC	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Human-computer interaction; Android; Mobile interaction; Usability test; Brain-computer interface	BRAIN-COMPUTER-INTERFACE; GAME; BCI	Portable brain-computer interfaces (BCI) headsets that available today in the market provides users with unique ability and experience to interact with their mobile phones and tablets. Researchers try to use BCI outside the labs by implementing and developing applications for BCI mostly on Android platform, which are of different types and purposes, such as communication purposes, controlling external devices and for entertainment such as games. But typically, those BCI related applications are somewhat limited in terms of their scope of control, i.e. the users can only interact with one application at a time and cannot switch to other application without touching the device screen. This paper addresses the development of a MotionMouse, a prototype that provides users with an alternative to fully interact and control an Android tablet by controlling not just one, but many different application types. The evaluation of the prototype was based on ISO 9241-11:2018 standards with a result that demonstrate the ability of controlling and generating touch events on the device using Emotiv EPOC+ with effectiveness of 63.82% (accuracy of the correct target) and efficiency of 3.22 clicks per minute. In addition, the participants also found that it was easy to perform the tasks with few showed excitements during the experiment.																	0127-9084						2019					3		73	86		10.22452/mjcs.sp2019no3.5													
J								FACEBOOK USER REACTIONS AND EMOTION: AN ANALYSIS OF THEIR RELATIONSHIPS AMONG THE ONLINE DIABETES COMMUNITY	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Emotion analysis; Facebook; Reactions; Diabetes; Social media	SOCIAL MEDIA; COMMUNICATION; SENTIMENT; DISTRESS; SEEKING	With the advent of Web 2.0 technologies such as social media, online text sources provide large scale data repositories out of which valuable knowledge about human emotions can be derived. This paper aims to (i) detect and classify emotions of the Facebook diabetes community, (ii) examine the relationship of emotion and Facebook reactions, and (iii) identify user reaction predictors for each of the emotion. A total of 15K posts were randomly selected from several official Facebook diabetes support groups. Pre-processing was administered, resulting in 2475 Facebook posts for further analysis in this study. Emotion detection was first administered using Indico API, with results revealing anger, sadness and fear to be the top most emotions experienced, whilst love and wow emerged as the highest-ranking reactions. Precision and recall indicate the performance of the emotion detection mechanism ranged between 65 - 82% for all the emotions, compared to the human annotation. The average F- score recorded was 78%. Both love and wow were found to significantly predict joy and fear, whereas angry was found to predict anger. The findings indicate that human emotions can be effectively detected based on users' textual communication, and significant relationships exists between several reactions and emotions.																	0127-9084						2019					3		87	97		10.22452/mjcs.sp2019no3.6													
J								GESTURE INTERFACING FOR PEOPLE WITH DISABILITY OF THE ARM, SHOULDER AND HAND (DASH) FOR SMART DOOR CONTROL: GOMS ANALYSIS	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										DASH; GOMS; Human Computer Interaction; Gesture Interface; Smart Door		People with disabilities may interact with their environment differently from other human beings. This is also the case with people with the disability of the arm, shoulder and hand (DASH). However, most environments do not include supportive design for DASH. This study aimed to explore and analyze the body parts used by people with DASH to open doors in a real-world environment and to find an efficient interface design for people with DASH to open doors through a computer interface. This study was conducted in three parts: interviews of three people with DASH, observation of the videos of people with DASH opening doors, and GOMS analysis of five designed interfaces for people with DASH to open doors/windows through a computer interface. It was found that head gesture was suitable to be used as an interaction medium and through the GOMS analysis that the Type 5 (every task has a different head gesture movement) and Type 3 (positioning heads at different vertical or horizontal positions) are two most efficient designed interfaces for head gesture with regards to opening a door.																	0127-9084						2019					3		98	117		10.22452/mjcs.sp2019no3.7													
J								FUZZY ENCODER FRAMEWORK FOR FOUR LAYERS COLOR QR CODE	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Fuzzy; QR code; Framework; Color QR code; Encoder; Color reference		QR code is a popular type of two-dimension barcode due to high demand of QR code which makes it an active research area specifically to overcome QR code limitation. One of QR code limitation is encoding data size limitation. In this paper, we proposed four layers QR code encoder utilizing fuzzy technique to overcome size limitation. The framework extended the maximum capacity for three layers color QR code by 25%. The fuzzy encoder will select the best-fit color QR code in the aspect of the number of colors according to the file size and the space on paper. Then, the encoder will divide the file into a maximum of four layers of a black and white QR code and give each layer a specific color by color multiplexing for those QR codes. We produced a color QR code with a maximum capacity of four times larger than existing black and white QR code. The encoder also proposed color reference on the locator pattern in the QR code to easily identify the number of colors used in the generated color QR code.																	0127-9084						2019					3		118	130		10.22452/mjcs.sp2019no3.8													
J								IOT SECURITY RISK MANAGEMENT MODEL FOR HEALTHCARE INDUSTRY	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Security issues; IoT; Model; Risk Management; Healthcare	INTERNET; CHALLENGES; PRIVACY; THINGS	Internet of Things (IoT) is predicted as one of the biggest emerging environments in the future that creates simultaneous smart communication between machines or a variety of digital devices. Besides improving how data can be controlled, monitored and collected, IoT also allows us to generate revenue through the identifying of new business opportunities and deployment of advanced analytics processes for decision-making purposes. IoT enables us to accelerate changes in the healthcare environment in improving patient engagement and outcome, as well as transforming healthcare from reactive to proactive accessibility. Nevertheless, IoT's expansion brings new vulnerabilities, risk and security challenges for healthcare practitioners and their patients. However, there is still a lack of study focusing on IoT risk management in healthcare. Existing researches tend to focus only on the implementation of IoT peripherals in a healthcare environment and tend to embed the secured applications solution with it. Since healthcare information and data are highly confidential, it is important to ensure that a secured health IoT application is in place. Thus, the aim of this study is to investigate the IoT risk management aspects in a healthcare environment with particular attention to proposing a step-by-step process of an IoT risk management model. The proposed IoT risk management model was developed leveraging on DEMATEL IoT Risk Assessment Procedure. This study was conducted based on a case study of one hospital in Sudan which has recently invested in IoT technologies in their operation. Interviews were conducted with selected respondents from the hospital and findings suggested that the selected case study does not have an established IoT risk management mechanism due to the ad-hoc IoT implementation approach. The case study also lacks of protection for health data and information with several unclear work process. As a solution, this study proposes an enhanced IoT risk management model for healthcare with consideration of three risk categories; 1) Secured Technology, 2) Human Privacy and 3) Trustable Process and Data. The proposed model was then evaluated by three IoT experts and two IT healthcare practitioners based on System Usability Score (SUS) and received Good Usability score which means the model is usable as a healthcare IoT risk management model.																	0127-9084						2019					3		131	144		10.22452/mjcs.sp2019no3.9													
J								GROUP-BASED ASYNCHRONOUS DISTRIBUTED ALTERNATING DIRECTION METHOD OF MULTIPLIERS IN MULTICORE CLUSTER	COMPUTING AND INFORMATICS										ADMM; global consensus optimization; multicore cluster; logistic regression; GAD-ADMM	ADMM	The distributed alternating direction method of multipliers (ADMM) algorithm is one of the effective methods to solve the global consensus optimization problem. Considering the differences between the communication of intra-nodes and inter-nodes in multicore cluster, we propose a group-based asynchronous distributed ADMM (GAD-ADMM) algorithm: based on the traditional star topology network, the grouping layer is added. The workers are grouped according to the process allocation in nodes and model similarity of datasets, and the group local variables are used to replace the local variables to compute the global variable. The algorithm improves the communication efficiency of the system by reducing communication between nodes and accelerates the convergence speed by relaxing the global consistency constraint. Finally, the algorithm is used to solve the logistic regression problem in a multicore cluster. The experiments on the Ziqiang 4000 showed that the GAD-ADMM reduces the system time cost by 35% compared with the AD-ADMM.																	1335-9150						2019	38	4					765	789		10.31577/cai_2019_4_765													
J								AGENT-BASED SYSTEM FOR MOBILE SERVICE ADAPTATION USING ONLINE MACHINE LEARNING AND MOBILE CLOUD COMPUTING PARADIGM	COMPUTING AND INFORMATICS										Agent-based system; machine learning; adaptation; mobile service; mobile cloud computing	EDGE	An important aspect of modern computer systems is their ability to adapt. This is particularly important in the context of the use of mobile devices, which have limited resources and are able to work longer and more efficiently through adaptation. One possibility for the adaptation of mobile service execution is the use of the Mobile Cloud Computing (MCC) paradigm, which allows such services to run in computational clouds and only return the result to the mobile device. At the same time, the importance of machine learning used to optimize various computer systems is increasing. The novel concept proposed by the authors extends the MCC paradigm to add the ability to run services on a PC (e.g. at home). The solution proposed utilizes agent-based concepts in order to create a system that operates in a heterogeneous environment. Machine learning algorithms are used to optimize the performance of mobile services online on mobile devices. This guarantees scalability and privacy. As a result, the solution makes it possible to reduce service execution time and power consumption by mobile devices. In order to evaluate the proposed concept, an agent-based system for mobile service adaptation was implemented and experiments were performed. The solution developed demonstrates that extending the MCC paradigm with the simultaneous use of machine learning and agent-based concepts allows for the effective adaptation and optimization of mobile services.																	1335-9150						2019	38	4					790	816		10.31577/cai_2019_4_790													
J								MULTILEVEL ALGEBRAIC APPROACH FOR PERFORMANCE ANALYSIS OF PARALLEL ALGORITHMS	COMPUTING AND INFORMATICS										Complexity and performance of numerical algorithms; performance metrics; data decomposition; concurrency; parallel algorithms		In order to solve a problem in parallel we need to undertake the fundamental step of splitting the computational tasks into parts, i.e. decomposing the problem solving. A whatever decomposition does not necessarily lead to a parallel algorithm with the highest performance. This topic is even more important when complex parallel algorithms must be developed for hybrid or heterogeneous architectures. We present an innovative approach which starts from a problem decomposition into parts (sub-problems). These parts will be regarded as elements of an algebraic structure and will be related to each other according to a suitably defined dependency relationship. The main outcome of such framework is to define a set of block matrices (dependency, decomposition, memory accesses and execution) which simply highlight fundamental characteristics of the corresponding algorithm, such as inherent parallelism and sources of overheads. We provide a mathematical formulation of this approach, and we perform a feasibility analysis for the performance of a parallel algorithm in terms of its time complexity and scalability. We compare our results with standard expressions of speed up, efficiency, overhead, and so on. Finally, we show how the multilevel structure of this framework eases the choice of the abstraction level (both for the problem decomposition and for the algorithm description) in order to determine the granularity of the tasks within the performance analysis. This feature is helpful to better understand the mapping of parallel algorithms on novel hybrid and heterogeneous architectures.																	1335-9150						2019	38	4					817	850		10.31577/cai_2019_4_817													
J								PROCESS MATCHING: PERFORMANCE TRADE-OFF BETWEEN SUMMARY AND FULL-LENGTH DESCRIPTIONS	COMPUTING AND INFORMATICS										Information retrieval systems; process retrieval; text-matching; summary-full description for process matching	MODEL; SEARCH; IDENTIFICATION; SPACE	Business process models are used by modeling experts to concisely depict the workflow of an organization that plays a pivotal role in the development of ERP systems. A growing number of organizations also maintain the textual process descriptions of these process models as the descriptions are understandable across the board. A recent study has revealed that these textual descriptions can also be used for an accurate process model search. However, the use of textual descriptions is a resource-intensive task due to the sheer size of the descriptions. To that end, in this paper, we have proposed an approach that relies on the use of summary textual descriptions, instead of full-length descriptions, to enhance the performance of process matching. To evaluate the proposed approach, we have used four diverse text summarization techniques, including a state-of-the-art deep learning based technique, for generating summary descriptions, and seven text-matching techniques for finding relevant process specifications. Our empirical study has established that the Vector Space Model is the most effective technique for process matching. Furthermore, the use of Lingo generated summaries, at a compression rate of 50 %, can achieve a higher efficiency as well as effectiveness than the full-length textual process descriptions.																	1335-9150						2019	38	4					851	882		10.31577/cai_2019_4_851													
J								AN EFFECTIVE METAHEURISTIC FOR MULTIPLE TRAVELING REPAIRMAN PROBLEM WITH DISTANCE CONSTRAINTS	COMPUTING AND INFORMATICS										Traveling repairmen problem; distance constraints; insertion heuristic; tabu search; variable neighborhood search	ALGORITHM	Multiple Traveling Repairman Problem with Distance Constraints (MTRPD) is an extension of the NP-hard Multiple Traveling Repairman Problem. In MTRPD, a fleet of identical vehicles is dispatched to serve a set of customers with the following constraints. First, each vehicle's travel distance is limited by a threshold. Second, each customer must be visited exactly once. Our goal is to find the visiting order that minimizes the sum of waiting times. To solve MTRPD we propose to combine the Insertion Heuristic (IH), Variable Neighborhood Search (VNS), and Tabu Search (TS) algorithms into an effective two-phase metaheuristic that includes a construction phase and an improvement phase. In the former phase, IH is used to create an initial solution. In the latter phase, we use VNS to generate various neighborhoods, while TS is employed to mainly prohibit from getting trapped into cycles. By doing so, our algorithm can support the search to escape local optima. In addition, we introduce a novel neighborhoods' structure and a constant time operation which are efficient for calculating the cost of each neighboring solution. To show the efficiency of our proposed metaheuristic algorithm, we extensively experiment on benchmark instances. The results show that our algorithm can find the optimal solutions for all instances with up to 50 vertices in a fraction of seconds. Moreover, for instances from 60 to 80 vertices, almost all found solutions fall into the range of 0.9 %-1.1 % of the optimal solutions' lower bounds in a reasonable duration. For instances with a larger number of vertices, the algorithm reaches good-quality solutions fast. Moreover, in a comparison to the state-of-the-art metaheuristics, our proposed algorithm can find better solutions.																	1335-9150						2019	38	4					883	916		10.31577/cai_2019_4_883													
J								HSIC REGULARIZED LTSA	COMPUTING AND INFORMATICS										Dimensionality reduction; RKHS; Hilbert-Schmidt operators; LTSA; HSIC	NONLINEAR DIMENSIONALITY REDUCTION; FRAMEWORK	Hilbert-Schmidt Independence Criterion (HSIC) measures statistical independence between two random variables. However, instead of measuring the statistical independence between two random variables directly, HSIC first transforms two random variables into two Reproducing Kernel Hilbert Spaces (RKHS) respectively and then measures the kernelled random variables by using Hilbert-Schmidt (HS) operators between the two RKHS. Since HSIC was first proposed around 2005, HSIC has found wide applications in machine learning. In this paper, a HSIC regularized Local Tangent Space Alignment algorithm (HSIC-LTSA) is proposed. LTSA is a well-known dimensionality reduction algorithm for local homeomorphism preservation. In HSIC-LTSA, behind the objective function of LTSA, HSIC between high-dimensional and dimension-reduced data is added as a regularization term. The proposed HSIC-LTSA has two contributions. First, HSIC-LTSA implements local homeomorphism preservation and global statistical correlation during dimensionality reduction. Secondly, HSIC-LTSA proposes a new way to apply HSIC: HSIC is used as a regularization term to be added to other machine learning algorithms. The experimental results presented in this paper show that HSIC-LTSA can achieve better performance than the original LTSA.																	1335-9150						2019	38	4					917	936		10.31577/cai_2019_4_917													
J								REVISITING MULTIPLE PATTERN MATCHING	COMPUTING AND INFORMATICS										Text algorithms; pattern matching; multiple pattern matching; q-grams; bit-parallelism; compressed pattern matching	COMPLEXITY	We consider the classical exact multiple string matching problem. The proposed solution is based on a combination of a few ideas: using q-grams instead of single characters, pattern superimposition, bit-parallelism and alphabet size reduction. We discuss the pros and cons of various alternatives to achieve the possibly best combination of techniques. The main contribution of this paper are different alphabet mapping methods that allow to reduce memory requirements and use larger q-grams. The experimental results show that the presented algorithm is competitive in most practical cases. One of the tests shows also that tailoring our scheme to search over a byte-encoded text results in speedups in comparison to searching over a plain text.																	1335-9150						2019	38	4					937	962		10.31577/cai_2019_4_937													
J								OPTIMIZATION OF THE MORPHER MORPHOLOGY ENGINE USING KNOWLEDGE BASE REDUCTION TECHNIQUES	COMPUTING AND INFORMATICS										Machine learning; natural language processing; inflection; lemmatization; agglutination; morphology; optimization; rule base reduction		Morpher is a novel morphological rule induction engine designed and developed for agglutinative languages. The Morpher engine models inflection using general string-based transformation rules and it can learn multiple arbitrary affix types, too. In order to scale the engine to training sets containing millions of examples, we need an efficient management of the generated rule base. In this paper we investigate and present several optimization techniques using rule elimination based on context length, support and cardinality parameters. The performed evaluation tests show that using the proposed optimization techniques, we can reduce the average inflection time to 0.52%, the average lemmatization time to 2.59 % and the number of rules to 2.25% of the original values, while retaining a high correctness ratio of 98 %. The optimized model can execute inflection and lemmatization in acceptable time after training millions of items, unlike other existing methods like Morfessor, MORSEL or MorphoChain.																	1335-9150						2019	38	4					963	985		10.31577/cai_2019_4_963													
J								A NEW OPEN INFORMATION EXTRACTION SYSTEM USING SENTENCE DIFFICULTY ESTIMATION	COMPUTING AND INFORMATICS										Information extraction; open information extraction; relation extraction; knowledge discovery; fact extraction		The World Wide Web has a considerable amount of information expressed using natural language. While unstructured text is often difficult for machines to understand, Open Information Extraction (OIE) is a relation-independent extraction paradigm designed to extract assertions directly from massive and heterogeneous corpora. Allocation of low-cost computational resources is a main demand for Open Relation Extraction (ORE) systems. A large number of ORE methods have been proposed recently, covering a wide range of NLP tools, from "shallow" (e.g., part-of-speech tagging) to "deep" (e.g., semantic role labeling). There is a trade-off between NLP tools depth versus efficiency (computational cost) of ORE systems. This paper describes a novel approach called Sentence Difficulty Estimator for Open Information Extraction (SDE-OIE) for automatic estimation of relation extraction difficulty by developing some difficulty classifiers. These classifiers dedicate the input sentence to an appropriate OIE extractor in order to decrease the overall computational cost. Our evaluations show that an intelligent selection of a proper depth of ORE systems has a significant improvement on the effectiveness and scalability of SDE-OIE. It avoids wasting resources and achieves almost the same performance as its constituent deep extractor in a more reasonable time.																	1335-9150						2019	38	4					986	1008		10.31577/cai_2019_4_986													
J								Tri-band Impedance Matching Network Design Using Particle Swarm Optimization Algorithm	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										computer aided engineering; evolutionary computation; impedance matching; microwave circuits; particle swarm optimization	SEARCH	A solution strategy is presented using a five-section transmission line impedance transformer aiming for multiple band matching network circuits. In this paper, the analysis, which is based on the transmission line theory and application of the evolutionary algorithm for the solution of the stated problem are explained. Design of the matching networks was performed and optimized at three different frequencies 1.8 GHz, 2.4 GHz and 3 GHz at the same time. Tests were performed for two different load configurations. The optimized design values obtained from the particle swarm optimization algorithm were verified for correctness using microwave simulator. After the fabrication of the circuits, the measurements were taken for these circuits for the validation of the design. From the observations that were made, it can be concluded that particle swarm optimization can be a good choice for the design and optimization of multiple band matching network circuits.																	1582-7445	1844-7600					2019	19	4					37	46		10.4316/AECE.2019.04005													
J								Recommendation of expert group to question and answer sites based on user behaviors and diversity	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Expert group; group recommendation; diversity criteria; user behaviors		Question-and-answering (Q&A) sites are information systems that allow users to ask and answer questions. Users can learn by frequently discussing, answering questions, or exchanging opinions with other experts using Q&A systems. In addition, they can arrange the existing top answers using a number of upvotes and downvotes from experts and crowd wisdom. The number of knowledge-sharing sites has increased significantly in recent years. However, some Q&A sites began to shrink (Yahoo Answers) or were shut down (Google Answers). The main reason is low-quality answers because they do not connect visitors and experts with the right questions. In addition, a question may contain several subtopics with which the expert is unfamiliar. The recommendation of a list of experts closest to the question will lead to a long-tail problem. In this paper, we propose an expert group recommendation method for Q&A systems by taking into consideration users' behaviors and diversity criteria in the group. Users' behavior is analyzed to determine a group of experts or non-experts on specific topics. Diversity is an important factor in promoting the sustained comprehensible growth of Q&A sites and avoid following the crowd. Experiments on a Quora dataset show that our method achieves better results in terms of accuracy in comparison with other methods.																	1064-1246	1875-8967					2019	37	6					7117	7129		10.3233/JIFS-179325													
J								Mining class association rules on imbalanced class datasets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Class association rules; associative classification; imbalanced class dataset; clustering; data mining	INDUCTIVE LEARNING ALGORITHM; CLASSIFICATION	The task of discovering sets of good rules from imbalanced class datasets may not come easy for existing class association rule mining algorithms. The reason is that they often generate rules belonging to the dominant classes. For example, in medical applications, some symptoms of illness are not popular, and the doctors are very interested in the rules associated with these symptoms. This paper proposes a novel approach for mining class association rules (CARs) in imbalanced class datasets. Firstly, assuming there are n given classes, the training dataset is split into n corresponding groups. For each group, the data is clustered by the k-means algorithm into k groups where the value of k is equal to the number of records of the smallest group. Secondly, we combine all records from the groups after clustering and use the CAR-Miner-Diff algorithm to mine all CARs. We also propose an iterative method to get a highly accurate classifier. From experiments, we show that the proposed approach outperforms existing algorithms while maintaining a large number of useful rules in the classifier.																	1064-1246	1875-8967					2019	37	6					7131	7139		10.3233/JIFS-179326													
J								Building collective intelligence through experience: a survey on the use of the KREM model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Knowledge technologies; ontologies; reasoning; experience; meta-knowledge	KNOWLEDGE STRUCTURE; DECISIONAL DNA; SYSTEM; SET; DESIGN; ONTOLOGY	This article presents a survey on the use of KREM, a generic knowledge-based framework for building collective intelligence through experience. After a discussion on the disadvantages of the traditional architecture used to deploy intelligent systems, the KREM architecture (Knowledge, Rules, Experience, Meta-Knowledge) is presented. The novelty of the proposal comes from the inclusion of the capitalisation of experience and the use of meta-knowledge in the traditional architecture previously discussed. KREM improves the efficiency of traditional intelligent systems by allowing incomplete expert knowledge models to be used, gradually completing them, learning with experience. In addition, the use of metaknowledge can guide their execution more effectively. This framework has been successfully used in various projects in different application areas, which are presented and discussed.																	1064-1246	1875-8967					2019	37	6					7141	7153		10.3233/JIFS-179327													
J								Decisional DNA based intelligent knowledge model for flexible manufacturing system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Set of experience knowledge structure (SOEKS); Decisional DNA; Virtual engineering object (VEO); Virtual engineering process (VEP)	SET	Modeling an effective mechanism for design and control strategies for the implementation of a flexible manufacturing system (FMS) has been a challenge. Consequently, to overcome this issue various techniques have applied in the past but most of these models are effective only for some specific situation or an element of FMS. In this study, the knowledge representation technique of Decisional DNA (DDNA) is applied to FMS to develop a generic model to achieve effective scheduling and manufacturing flexibility. Decisional DNA based Virtual Engineering Objects (VEO) are used as communicating media between machines, equipment and works pieces. The concept of Virtual Engineering Process (VEP) is applied for modeling routing flexibility. VEOs combined with VEPs form FMS-DDNA model, which facilitates in enhancing the performance of FMS, by inducing intelligence based on its own previous experience thus making it practical and smart.																	1064-1246	1875-8967					2019	37	6					7155	7167		10.3233/JIFS-179328													
J								Establishing intelligent enterprise through community of practice for product innovation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Smart Innovation Engineering; product innovation; set of experience; community of practice; industry 4.0	DECISIONAL DNA; KNOWLEDGE MANAGEMENT; SYSTEMS	This paper presents the idea of implementing the virtual Community of Practice for Product Innovation processes towards the establishment of intelligent enterprise. Since the fourth industrial revolution is passing through the developing phase, implementation of Cyber-Physical Production Systems require more realistic approach. Knowledge Management and Engineering plays an important role in manufacturing industries facing global competition. One of the most promising areas where Knowledge Management is studied and applied is product innovation. This paper explains the efficient and systematic methodology for Knowledge Management through Community of Practice for product innovation. Manufacturing industries can connect with similar industries at global level, sharing and using technical and experiential knowledge in decision making thus converting them into intelligent enterprises.																	1064-1246	1875-8967					2019	37	6					7169	7178		10.3233/JIFS-179329													
J								Experience based decisional DNA to support smart product design	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Product development; product design; set of experience knowledge structure; decisional DNA; material selection process		This paper presents the idea of Smart Virtual Product Development (SVPD) system to support product design. The foundations of the system are based upon smart knowledge management techniques called Set of Experience Knowledge Structure (SOEKS) and Decisional DNA (DDNA). It enhances the industrial product development process by using the previous experiential knowledge gathered from the formal decisional activities. This experiential knowledge is collected from the group of similar products having some common functions and features. The developed system comprises of three modules: design knowledge management (DKM), manufacturing capability analysis and process planning (MCAPP), and product inspection planning (PIP). The working of design knowledge management module is presented in this study and is validated by using an industrial case study, which suggests that it is capable of capturing and reusing the required design knowledge for material selection process. The developed system has the capability to facilitate decision making and mistake proofing during early stages of product design. It can be beneficial for small and medium enterprises (SMEs) involved in product development.																	1064-1246	1875-8967					2019	37	6					7179	7187		10.3233/JIFS-179330													
J								A switching multi-level method for the long tail recommendation problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Recommender systems; collaborative filtering; switching; multi-level; long tail recommendations	SYSTEMS	Recommender systems are decision support systems that play an important part in generating a list of product or service recommendations for users based on the past experiences and interactions. The most popular recommendation method is Collaborative Filtering (CF) that is based on the users' rating history to generate the recommendation. Although, recommender systems have been applied successfully in different areas such as e-Commerce and Social Networks, the popularity bias is still one of the challenges that needs to be further researched. Therefore, we propose a multi-level method that is based on a switching approach which solves the long tail recommendation problem (LTRP) when CF fails to find the target case. We have evaluated our method using two public datasets and the results show that it outperforms a number of bases lines and state-of-the-art alternatives with a further reduce of the recommendation error rates for items found in the long tail.																	1064-1246	1875-8967					2019	37	6					7189	7198		10.3233/JIFS-179331													
J								From human mesenchymal stromal cells to osteosarcoma cells classification by deep learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Human mesenchymal stromal cells; Osteosarcoma cells; deep learning; convolutional neural networks; convolutional object detection systems; cell classification		Early diagnosis of cancer often allows for a more vast choice of therapy opportunities. After a cancer diagnosis, staging provides essential information about the extent of disease in the body and the expected response to a particular treatment. The leading importance of classifying cancer patients at the early stage into high or low-risk groups has led many research teams, both from the biomedical and bioinformatics field, to study the application of Deep Learning (DL) methods. The ability of DL to detect critical features from complex datasets is a significant achievement in early diagnosis and cell cancer progression. In this paper, we focus the attention on osteosarcoma. Osteosarcoma is one of the primary malignant bone tumors which usually afflicts people in adolescence. Our contribution to classification of osteosarcoma cells is made as follows: a DL approach is applied to discriminate human Mesenchymal Stromal Cells (MSCs) from osteosarcoma cells and to classify the different cell populations under investigation. Glass slides of different cell populations were cultured including MSCs, differentiated in healthy bone cells (osteoblasts) and osteosarcoma cells, both single cell populations or mixed. Images of such samples of isolated cells (single-type of mixed) are recorded with traditional optical microscopy. DL is then applied to identify and classify single cells. Proper data augmentation techniques and cross-fold validation are used to appreciate the capabilities of a convolutional neural network to address the cell detection and classification problem. Based on the results obtained on individual cells, and to the versatility and scalability of our DL approach, the next step will be its application to discriminate and classify healthy or cancer tissues to advance digital pathology.																	1064-1246	1875-8967					2019	37	6					7199	7206		10.3233/JIFS-179332													
J								Improving Youtube video retrieval by integrating crowdsourced timed metadata	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Video tag; video retrieval; crowdsourcing; multimedia content annotation; gamification; social media; YouTube		The development of efficient methods for searching and browsing large assets of video content has been considered by the academia and content owners for long. Different approaches that range from manual structured annotations, to unstructured metadata collected from several sources, as well as multimedia processing for automatic description of the content, can be identified. The growth on the number of hours of video content put online in video sharing platforms has however shown that video retrieval is still quite inefficient as rich contextual data that describes the content is most of the times still not available. Additionally, metadata is usually not linked to timed moments of content, making direct access to the most relevant moments not possible. In this paper, an approach for making web videos available in the YouTube platform more accessible is presented. The solution is based on a collaborative process presented as a game that enables collecting metadata from the crowd while implementing mechanisms that remove erroneous information usually encountered in this type of information. Metadata, exported to YouTube in the form of captions and descriptions, contributes to enhance video retrieval, guaranteeing a better user experience and exposure of the content.																	1064-1246	1875-8967					2019	37	6					7207	7221		10.3233/JIFS-179333													
J								Towards implementing defect prediction in the software development process	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										software metrics; software development process; defect prediction; re-open prediction; predicting feature defectiveness; defect prediction economy	PERFORMANCE; CROSS	Defect prediction is a method of identifying possible locations of software defects without testing. Software tests can be laborious and costly thus one may expect defect prediction to be a first class citizen in software engineering. Nonetheless, the industry apparently does not see it that way as the level of practical usages is limited. The study describes the possible reasons of the low adoption and suggests a number of improvements for defect prediction, including a confusion matrix-based model for assessing the costs and gains. The improvements are designed to increase the level of practitioners acceptance of defect prediction by removing the recognized by authors implementation obstacles. The obtained predictors showed acceptable performance. The results were processed through the suggested model for assessing the costs and gains and showed the potential of significant benefits, i.e. up to 90% of the overall cost of the considered test activities.																	1064-1246	1875-8967					2019	37	6					7223	7238		10.3233/JIFS-179334													
J								Data reduction and stacking for imbalanced data classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Instance selection; clustering; stacking; imbalanced data; team of agents	INTEGRATION	Class imbalance arises when the number of examples belonging to one class is much greater than the number of examples belonging to another. The discussed approach focuses on combining several techniques including data reduction and stacking with the aim of improving the performance of the machine classification in the case of imbalanced data. The paper proposes a cluster-based data reduction approach assuming that the instances are selected from a cluster, the data reduction is carried-out on instances belonging to the majority classes, and the aim of the instance selection is to reduce the imbalance ratio between the majority and minority classes. The process of instance selection is carried out with using an agent-based population learning algorithm. To increase performance and generalization ability of the prototype-based machine learning classification it was decided to use the stacking technique. The proposed approach is validated experimentally using several benchmark datasets from the KEEL repository. Advantages and main features of the approach are discussed considering the results of the computational experiment.																	1064-1246	1875-8967					2019	37	6					7239	7249		10.3233/JIFS-179335													
J								A sentiment analysis method of objects by integrating sentiments from tweets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Sentiment-analysis; sentiment-integration; object-determination		Sentiment analysis has been gaining importance in many applications such as recommendation systems, the decision making support and prediction models. Sentiment analysis helps to understand and evaluate public opinion regarding social events, product services, and political trends, especially the feelings expressed through comments by users in social networks such as Twitter, Facebook, and Instagram. There have been a lot of research attempts to address the tweets sentiment analysis problem with high accuracy, particularly in case of tweets that express a single sentiment towards a single object. However, the results of the classification are not highly accurate in cases such as the following: a user expresses multiple sentiments towards a single object in a tweet; a user presents multiple sentiments towards multiple objects; and a user indicates a single sentiment towards multiple objects. Furthermore, the previous studies only analyze the sentiment of each tweet without considering the objects and the sentiment towards each object from an entire set of tweets. This study attempts to deal with the limitations of the previous methods; an approach is proposed herein, based on integrating the sentiment towards a particular object from all tweets related to that object. The proposed method focuses on determining the objects and indicating the sentiment towards the specific objects by combining the sentiments related to each object from the entire set of tweets. On experimental evaluation, the proposed method is observed to have achieved a fairly good result in terms of the error ratio and achieved information.																	1064-1246	1875-8967					2019	37	6					7251	7263		10.3233/JIFS-179336													
J								A distance-based approach for merging probabilistic knowledge bases	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Probabilistic knowledge base; knowledge merging; merging operator; algorithm		In the stages of development of probabilistic expert systems, knowledge merging is a major concern. To deal with knowledge merging problems, several approaches have been put forward. However, in the proposed models, each original probabilistic knowledge base (PKB) is represented by a set of probabilistic functions fulfilling such knowledge base. The drawbacks of the solutions are that the output of model is also a set of probabilistic functions satisfying the resulting PKB and there is no algorithm for implementing the merging process of PKBs in which each of them consists of probabilistic constraints. In this paper, distance-based approach is utilized to propose a new method of merging PKBs to ensure that both the input and output of methods are represented by sets of probabilistic constraints. To this aim, the relationship between the probability rules and the probabilistic constraints, and the several transformation methods for the representation of the original PKB are presented, a set of merging operators (MOs) is proposed, and several desirable logical properties are investigated and discussed. Several algorithms for merging PKBs are presented and the computational complexities of these algorithms are also analyzed and evaluated.																	1064-1246	1875-8967					2019	37	6					7265	7278		10.3233/JIFS-179337													
J								Toward evaluating the level of crowd wisdom using interval estimates	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Collective intelligence; wisdom of crowds; interval estimates	DIVERSITY; OVERCONFIDENCE; JUDGMENTS	Recently, the use of the wisdom of crowds (WoC) for finding solutions to a wide range of real-life problems has dramatically expanded. Prior studies have revealed that diversity, independence, decentralization, and aggregation are the determinants of collective wisdom. However, these findings are often based on the so-called point estimates - single values are used as the representations of individual predictions on the task of estimating unknown quantities or predicting outcomes of future events. In some situations, interval values, which are often called interval estimates, can be used for such representations. Accordingly, one can provide an individual prediction in the form of an interval value including a lower and an upper bounds. Taking into account this kind of representation, in this paper, we present a case study in which collectives of randomly selected predictions can outperform those of most accurate predictions. Then, we evaluate the WoC level by taking into account diversity and cardinality. The computational experiments have indicated that diversity is positively related to collective wisdom. Finally, we discuss some related theoretical and practical implications for further research.																	1064-1246	1875-8967					2019	37	6					7279	7289		10.3233/JIFS-179338													
J								Updating consensus in case of adding or removing new elements to the profile	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										consensus theory; knowledge integration; knowledge management	ONTOLOGY EVOLUTION	Data integration is one of the trending topics in the modern computer science. It is not an uncommon task to deliver a unified perspective on a set of heterogenous data that would serve as a consensus of participating elements. Many computationally expensive solutions can be found in the literature. Moreover, one cannot determine how potential changes applied to inputs of these methods impact their results. In this paper we present a framework of managing evolving data and handling the entailments of the unforeseen alterations of inputs in terms of performing sound data integration in an acceptable time. We base our work on the consensus theory and provide theoretical foundations, an experimental evaluation and a statistical analysis of obtained results.																	1064-1246	1875-8967					2019	37	6					7291	7302		10.3233/JIFS-179339													
J								Decentralized infrastructure for knowledge discovery in the Semantic Web	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Semantic Web; query federation; ontologies; SPARQL; decentralized architecture		An enormous volume of well-structured data with explicit semantics, in accordance with W3C's standards, becomes a reality in the Web of Linked Data. However, the Semantic Web promise to turn it into a machine-process able global graph of knowledge still encounters numerous impediments. Efficient access and discovery along with the semantic heterogeneity have been identified as major stumbling blocks. Following the design principles for Semantic Web and Linked Data, we present ActiveDiscovery, a decentralized infrastructure for distributed SPARQL query evaluation based on its terminological entities, namely the ontologies used in a query. ActiveDiscovery's main goal is to facilitate distributed and transparent semantic search based on structural rather than keyword-based querying in the Semantic Web. Key architectural extensions regarding metadata, indexing and ontology alignment are proposed to achieve transparency for federated query execution in a decentralized manner. The rewriting procedure for extensional SPARQL query is considered regarding the proposed components and SERVICE clause as a standard recommendation for query federation. We investigate the feasibility of our approach and present preliminary results of initial evaluation. We conclude by indicating questions which need to be addressed in future work.																	1064-1246	1875-8967					2019	37	6					7303	7312		10.3233/JIFS-179340													
J								A vector-fuzzy model of a decentralized collective	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Unsupervised collective; collective knowledge; social influence; knowledge diffusion	CONSENSUS; POWER	In this paper we present an extended model of an unsupervised collective, that is a group where each member communicates with others to form opinions, instead of a single supervisor determining the overall collective knowledge. We describe the social influence theories that are the basis of the proposed model, and how they translate to a multi-agent model of the collective. We define two measures of social influence that are formalizations of concepts presented in sociological research. We perform a simulation experiment, where we observe the behavior of the collective in relation to those measures. Finally, we present a road-map of future improvements possible in the model, working towards a real world test of its feasibility.																	1064-1246	1875-8967					2019	37	6					7313	7323		10.3233/JIFS-179341													
J								Reduction of a Forrester effect in a supply chain management system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Supply chain management; Forrester effect; consensus methods	INTELLIGENCE	The supply chain is a key element of successful operation of businesses in a turbulent economic situation. Swift management of delivery of raw materials and finished products while keeping costs as low as possible and maintaining proper customer service is becoming as vital as the quality and price of a product when gaining competitive advantage. This leads companies to a wide search for the best strategies that allow efficient management of the supply chain. The basic problem, however, is the occurrence of the so-called Forrester effect (also known as the bullwhip effect). It involves intensified transposing of changes in demand onto the execution of product flow in supply chains. The aim of this article is to develop a manner to reduce the Forrester effect using the consensus method. The first part of the article analyzed the current state of knowledge on the discussed problem. Then it presented basic elements of the developed prototype of a SCM system and defined the meth-od for reducing the Forrester effect using a consensus algorithm. The final part of the article de-scribes the way to conduct an experiment that involves verifying the consensus algorithm and analyzes the results of the verification and their influence on the reduction of the Forrester effect.																	1064-1246	1875-8967					2019	37	6					7325	7335		10.3233/JIFS-179342													
J								A method for updating ontology-based user profile in Personalized Document Retrieval System using Bayesian networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Recommendation system; cold-start problem; user profile; social networks; collaborative filtering; Bayesian network		Traditional approaches to content-based recommendation and collaborative filtering do not suffer from cold-start problem, which is a challenge to recommend items for an unknown user. In this paper we present a Personalized Document Retrieval System which takes into account a social network information about the users. The overall idea of the system is to cluster users into groups of similar interests based on theirs usage data and to determine a representative profile for each of the groups. When a new user joins the system, he or she is classified into one of existing group based on his or her user data and the representative profile of the group becomes a starting profile for the new user. This paper focuses on a method for updating ontology-based user profile using Bayesian network approach. We analyze some properties of proposed updating method and describe an idea of experimental evaluations.																	1064-1246	1875-8967					2019	37	6					7337	7346		10.3233/JIFS-179343													
J								Distributed ant system for difficult transport problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										parallel and distributed computing; ant colony optimization; swarm intelligence; high performance computing	COLONY	Solving difficult, usually NP-hard problems, requires metaheuristic-based approach. Such algorithms are very often demanding from the point of view of computational power. Therefore various approaches to parallelize or distribute such systems were made. Many of such algorithms are structurally very easy to parallelize, e.g. evolutionary ones. However, swarm computing algorithms, in particular ACO (Ant Colony Optimization), in order to be implemented properly must use a significant amount of global knowledge (pheromones matrix). Therefore strict parallelization/distribution strategies for ACO are difficult to work-out. In the presented paper we propose a novel approach for parallelization and distribution of the most important element of ACO, namely the pheromone table. Our prototype implementation is tested on a real-world HPC (High Performance Computing) infrastructure, with good observed scalability. At the end of this paper we present actual experimental results focusing on two class of problems, namely TSP (Travelling Salesman Problem) and VRPTW (Vehicle Routing Problem with Time Windows), using popular benchmarks.																	1064-1246	1875-8967					2019	37	6					7347	7356		10.3233/JIFS-179344													
J								Collective profitability in semi-competitive intermediation networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Welfare economics; intermediation networks; collective intelligence; multi-agent system; graph theory; linear algebra	AGENTS	We analyze intermediation business processes that enable companies to use multiple distribution channels for expanding their market horizon of potential customers that are interested in purchasing their products and/or services. These distribution channels are represented by sequences of intermediation transactions supported by usually self-interested middle-agents that enable the connection of the providers with the end costumers. We propose a new formal model of network-structured intermediation business processes represented as Directed-Acyclic-Graphs. Using this model we obtained sound theoretical results of collectively profitable intermediation transactions. This paves the way for further proposal of optimal pricing strategies of the participating agents in semi-competitive environments.																	1064-1246	1875-8967					2019	37	6					7357	7368		10.3233/JIFS-179345													
J								A model of information diffusion in dynamic social networks based on evidence theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Information credibility; information diffusion; social networks; confidence degree		Social networks currently belong to a vast area of research as information spreads at a remarkable speed due to technology, and social connections have become easily accessible in the online environment. Social networks are dynamic entities, which new individuals can join, or other links can be lost because members no longer interact with one-another. Dynamic analysis of social networks is important in topology changes of the network and also in information diffusion. Some information that spreads through the social network may be untrue, hence in this paper we propose a protocol based on evidence theory with Dempster-Shafer and Yager's rules in which the network becomes more immune to false information. We also analyze the impact of topology change for an initial network by adding new connections in the information diffusion process. We show information diffusion by coloring the nodes of the network and also illustrate the time evolution of messages for a better accuracy in our comparisons. The experimental results confirm that the proposed model fits the behavior of inhibiting false information.																	1064-1246	1875-8967					2019	37	6					7369	7381		10.3233/JIFS-179346													
J								A heuristic method for the travelling salesman problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Symmetric travelling salesman problem (STSP); assignment problem; hamiltonian circuit; simple graph; simple circuit; complete graph K-n; list L-i; heuristics		In this paper we describe a heuristic procedure for solving the travelling salesman problem in the symmetric case without using the triangle inequality c(ij) <= c(ik) + c(kj). A complete proof of the correctness of the algorithm and example of the presentation how the method works are given. There is estimated computational complexity, which is at most O(m(2)), where m is a number of the edges of the complete graph with n vertices - K-n. There is shown also, it is possible obtain the following bound that HEURISTIC SOLUTION/OPTIMAL SOLUTION < 3, if some specific inequality considering weights (costs) of edges is satisfied.																	1064-1246	1875-8967					2019	37	6					7383	7388		10.3233/JIFS-179347													
J								A methodology to analyze heart data using fuzzy automata	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy automata; formal specification; ECG	NORMAL LIMITS; ELECTROCARDIOGRAM; VALUES; MODEL	Uncertainty and imprecision play an important role in the specification and analysis of complex systems. Therefore, it is important to provide methodologies and tools to support the correct development of these systems. In this paper we present a new formalism, based on fuzzy automata, to facilitate the different phases involved in the development of a system where information is fuzzy. The formal syntax and semantics of our formalism are based on previous work, which has been adapted to be easily implemented and automated. We introduce a methodology to analyze systems modelled with one of our fuzzy automata. Finally, we show how our framework can be used to define a model of the heart based on electrocardiograms (ECGs) and use this model to analyze data of real patients.																	1064-1246	1875-8967					2019	37	6					7389	7399		10.3233/JIFS-179348													
J								A step towards information extraction: Named entity recognition in Bangla using deep learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Named entity recognition; information extraction; word embedding; sequence labelling; Bi-LSTM; densely connected network; Bangla; annotation; dataset; NLP; neural network; character level feature extraction; CNN		Information Extraction allows machines to decipher natural language through using two tasks: Named Entity Recognition and Relation Extraction. In order to build such a system for Bangla Language, in this work a Named Entity Recognition (NER) System is proposed, which requires a minimum information to deliver a decent performance having less dependency on handcrafted features. The proposed model is based on Deep Learning, which is accomplished through the use of a Densely Connected Network (DCN) in collaboration with a Bidirectional-LSTM (BiLSTM) and word embedding, i.e., DCN-BiLSTM. Such a system, specific to the Bangla language, has never been done before. Furthermore, a unique dataset was made since no Named Entity Recognition dataset exists for Bangla language till date. In the dataset, over 71 thousand Bangla sentences have been collected, annotated, and classified into four different groups using IOB tagging scheme. Those groups are person, location, organization, and object. Due to Bangla's morphological structure, character level feature extraction is also applied so that we can access more features to determine relational structure between different words. This is initially done with the use of a Convolutional Neural Network but is later outperformed by our second approach which is through the use of a Densely Connected Network (DCN). As for the training portion, it has been done for two variations of word embedding which are word2vec and glove, the outcome being the largest vocabulary size known to both models. A detailed discussion in regard to the methodology of the NER system is explained in a comprehensive manner followed by an examination of the various evaluation scores achieved. The proposed model in this work resulted in having a Fl score of 63.37, which is evaluated at Named Entity Level.																	1064-1246	1875-8967					2019	37	6					7401	7413		10.3233/JIFS-179349													
J								Summarizing videos into a target language: Methodology, architectures and evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Automatic speech recognition; statistical machine translation; video summarization; text boundary segmentation; collecting data; text and audio summarization; objective and subjective evaluations		The aim of the work is to report the results of the Chist-Era project AMIS (Access Multilingual Information opinionS). The purpose of AMIS is to answer the following question: How to make the information in a foreign language accessible for everyone? This issue is not limited to translate a source video into a target language video since the objective is to provide only the main idea of an Arabic video in English. This objective necessitates developing research in several areas that are not, all arrived at a maturity state: Video summarization, Speech recognition, Machine translation, Audio summarization and Speech segmentation. In this article we present several possible architectures to achieve our objective, yet we focus on only one of them. The scientific locks are be presented, and we explain how to deal with them. One of the big challenges of this work is to conceive a way to evaluate objectively a system composed of several components knowing that each of them has its limits and can propagate errors through the first component. Also, a subjective evaluation procedure is proposed in which several annotators have been mobilized to test the quality of the achieved summaries.																	1064-1246	1875-8967					2019	37	6					7415	7426		10.3233/JIFS-179350													
J								Oboyob: A sequential-semantic Bengali image captioning engine	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image captioning; CNN; LSTM; natural language processing; computer vision; Bengali image captioning; merge architecture; par-inject architecture; machine translated caption subsampling		Understanding the context with generation of textual description from an input image is an active and challenging research topic in computer vision and natural language processing. However, in the case of Bengali language, the problem is still unexplored. In this paper, we address a standard approach for Bengali image caption generation though subsampling the machine translated dataset. Later, we use several pre-processing techniques with the state-of-the-art CNN-LSTM architecture-based models. The experiment is conducted on standard Flickr-8K dataset, along with several modifications applied to adapt with the Bengali language. The training caption subsampled dataset is computed for both Bengali and English languages for further experiments with 16 distinct models developed in the entire training process. The trained models for both languages are analyzed with respect to several caption evaluation metrics. Further, we establish a baseline performance in Bengali image captioning defining the limitation of current word embedding approaches compared to internal local embedding.																	1064-1246	1875-8967					2019	37	6					7427	7439		10.3233/JIFS-179351													
J								Defining semantic networks using association-oriented metamodel	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Semantic networks; Semantic Knowledge Base; partitioned semantic nets; association design patterns		Knowledge representation is one of the most explored areas in nowadays computer science research. In this paper authors pursue definition and semantics of semantic networks that are defined as part of Semantic Knowledge Base being a hybrid knowledge oriented system. The approach presented in here aims at introducing advanced properties of networks such as cardinality, partitioning or certainty at the same time using simple structure based on two operands and operators. Following paper is an extension of a conference publication that introduced advanced aspects of semantic networks modelling with the use of Association-Oriented Metamodel. The extension includes a discussion related to the formal description of the structure, as well as the description and use of association-oriented design patterns.																	1064-1246	1875-8967					2019	37	6					7453	7464		10.3233/JIFS-179353													
J								A fuzzy GGA-based approach to speed up the evolutionary process for diverse group stock portfolio optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Collective intelligence; diverse group stock portfolio; fuzzy grouping genetic algorithm; grouping problem; individual repair mechanism; portfolio optimization	ANT COLONY OPTIMIZATION; COLLECTIVE INTELLIGENCE; INFEASIBLE SOLUTIONS	Investment is always an interesting and important issue for people since the international financial crises are hard to predict and the government's policy may have an influence on economic activities. In the past, many researches have been proposed on portfolio issues. In some of these studies, the group stock portfolio (GSP) is utilized to provide various alternative stocks to an investor. The diverse group stock portfolio (DGSP) optimization approach has then been designed because the diversity of industries within a group can affect the performance of a final GSP. However, these approaches still have some problems to be solved. In this paper, we propose an algorithm to improve the efficiency and effectiveness of the previous work. In the proposed approach, a new chromosome representation and an enhanced fitness function are applied to find a better DGSP with lower risk than before. Moreover, we design a fuzzy grouping genetic algorithm (FGGA) based on the concept of collective intelligence which utilizes the fuzzy logic to dynamically tune the parameters in the evolution process for finding an appropriate DGSP. A mechanism is also designed to repair non-feasible chromosomes in the population. Through the above improvements, the proposed approach can not only focus on finding the best solution but also speed up the evolution process. Finally, experiments made on real datasets show the merits of the proposed approach.																	1064-1246	1875-8967					2019	37	6					7465	7479		10.3233/JIFS-179354													
J								Agent-based monitoring of the task scheduling in computational clouds	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										multiagent systems; monitoring; computational cloud; autonomous agent; batch scheduling	SYSTEM; DESIGN	The monitoring of the computational processes in highly distributed environments remains challenging in today's High Performance Computing. In this paper, we define the agent-based cloud monitoring system for supporting the computational tasks scheduling and resource allocation. The system consists of two types of agents, which may decide about the initialization of the schedule execution and monitor the work of the cloud computational nodes. The decision about running the new scheduling process is based on the expected number of available computational units in the specified time window. The efficiency of the proposed MAS-based model was justified through 40 empirical tests, where clouds without and within the MAS support were compared. The multiagent system (MAS) effectiveness has been expressed in the average number of floating point operations completed at the cloud resources in one second. The obtained results show the importance of setting the optimal initial time for execution of the new schedule. Our experiments show that for running the new schedule, at least 25% of the computing units in the clouds should be in the idle mode. Also the batches of tasks should not be too large, cause the waiting time for new schedule for execution should be short and not greater than 10% of expected batch execution time.																	1064-1246	1875-8967					2019	37	6					7481	7492		10.3233/JIFS-179355													
J								Training contextual neural networks with rectifier activation functions: Role and adoption of sorting methods	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Classifiers; self-consistency; aggregation functions; scan-paths	ALGORITHM	Contextual neural networks are effective and very usable machine learning models being generalization of multilayer perceptron. They allow to solve classification problems with high accuracy while strongly limiting activity of connections between hidden neurons. Within this article we present novel study of properties of contextual neuronal networks with Hard and Exponential Rectifier activation functions and of their influence on behavior of the Generalized Error Backpropagation method. It is used to show how to optimize efficiency of the sorting phase of this algorithm when applied to train evaluated models. This considerably extends our previous related paper which was limited to analysis of contextual neuronal networks with Leaky Rectifier and Sigmoidal activation functions. This article includes wide description of contextual neural networks and generalized error backpropagation algorithm as well as the discussion of their connection with self-consistency paradigm, which is frequently used in quantum physics. Also the relation of the latter with sorting methods and considered rectifier functions during training of contextual neural networks is studied in details. Conclusions are backed up by the results of performed experiments. Reported outcomes of simulations confirm the ability of contextual neural networks to limit activity of connections between their neurons and - what is more important - indicate the detailed rules of selection of the most efficient sorting algorithm for updating scan-paths of contextual neurons that are using Hard and Exponential Rectifier activation functions. Presented results have considerable value both for research and practical applications - especially where the efficiency of training of contextual neural networks is crucial.																	1064-1246	1875-8967					2019	37	6					7493	7502		10.3233/JIFS-179356													
J								Dashboard for exploring personalities based on mobile user log data	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Big5 traits; personality; indicators; data warehouse; mobile logs; Naive Bayes classification; dashboard	MODEL	The usefulness and ease of use of Big5 dashboard have been proposed to explore hierarchical structure of personality traits. First, Big5 system architecture and its components are described. Afterwards, we present how to calculate Big5 indicators from available big mobile data sets. Hereafter, Big5 traits can be predicted based on those just-specified indicators. To proof of our concepts, implementation results will be presented in the context of the Big5 dashboard which has been designed and developed to predict Big5 personalities in a representative and interactive manner.																	1064-1246	1875-8967					2019	37	6					7503	7509		10.3233/JIFS-179357													
J								Collaborative DCA: An intelligent collective optimization scheme, and its application for clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Collective optimization; DC programming; DCA; Collaborative DCA; Clustering	DIFFERENCE; ALGORITHM	This paper deals with a new and efficient collective optimization approach, based on DC (Difference of Convex functions) programming and DCA (DC Algorithm), powerful tools of nonconvex programming Exploiting the efficiency and the flexibility of DCA we develop the so-called collaborative DCA in which divers DCA based algorithms are cooperated in an effective way. Two versions of collaborative DCA are proposed and their applications on clustering, a fundamental problem in unsupervised learning, are studied. Numerical experiments are performed on several datasets. The comparative results with three DCA component algorithms show that the collaborative DCA outperforms them on quality and it realizes a good trade-off between the quality of solutions and the running time.																	1064-1246	1875-8967					2019	37	6					7511	7518		10.3233/JIFS-179358													
J								Sparse reconstruction of ISOMAP representations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Isometric feature mapping; inverse problem; sparse priors; sparse reconstruction; alternating directions method of multipliers	SUPERRESOLUTION	Isometric feature mapping (ISOMAP) is one of the classical methods of nonlinear dimensionality reduction (NLDR) and seeks for low dimensional (LD) structure of high dimensional (HD) data. However, the inverse problem of ISOMAP has never been investigated, which recovers the HD sample from the related LD sample, and its application prospect in data representation, generation, compression and visualization will be very brilliant. Because the inverse problem of ISOMAP is ill-posed and undetermined, the sparsity of HD data is employed to reconstruct the HD data from the corresponding LD data. The theoretical architecture of sparse reconstruction of ISOMAP representation comprises the original ISOMAP algorithm, learning algorithm of sparse dictionary, general ISOAMAP embedding algorithm and sparse ISOMAP reconstruction algorithm. The sparse ISOMAP reconstruction algorithm is an optimization problem with sparse priors of the HD data, which is resolved by the alternating directions method of multipliers (ADMM). It is uncovered from the experimental results that, in the case of very LD ISOMAP representation, the proposed method outperforms the state-of-theart methods, such as discrete cosine transformation (DCT) and sparse representation (SR), in the reconstruction performance of signal, image and video data.																	1064-1246	1875-8967					2019	37	6					7519	7536		10.3233/JIFS-179359													
J								Comparison of the effectiveness of automatic EEG signal class separation algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										EEG signal; discrete wavelet transform (DWT); autoencoder; EEG signal classification	CLASSIFICATION	In this paper, a methodology for automatic brain activity class identification of EEG (electroencephalographic) signals is presented. EEG signals are gathered from seventeen subjects performing one of the three tasks: resting, watching a music video and playing a simple logic game. The methodology applied consists of several steps, namely: signal acquisition, signal processing utilizing z-score normalization, parametrization and activity classification. The EEG signal is acquired from a headset containing 14 electrodes. For the parametrization two methods are used, namely, Discrete Wavelet Transform (DWT) employed as a reference parametrization technique and autoencoder neural network. Parameters obtained with those methods are fed to the input of classifiers which assigned them to one of three activity classes. Then, the effectiveness of the assignment of the frames of EEG data into appropriate classes is observed and compared. Results obtained using both methods show differences in accuracy with regard to the task detected depending on factors such as type of parametrization or complexity of the classifier employed for EEG activity classification.																	1064-1246	1875-8967					2019	37	6					7537	7543		10.3233/JIFS-179360													
J								Optimizing predictability of rating scales by differential evolution for the use by collective intelligent information and database systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Rating scale; DE	PAIRWISE COMPARISONS; GLOBAL OPTIMIZATION; CLASSIFICATION; PACKAGE	In this study, differential evolution (DE) optimization is proposed for rating scale predictability improvement. An arbitrary assignment of equal values for rating scale items is used as the classifier although domain experts are aware that the contribution of individual items may vary. Most academic examinations are conducted by the use of rating scales. Rating scales are also used in psychiatry (especially for screening). This study demonstrates that the differential evolution is effective for optimizing the predictability of rating scales.																	1064-1246	1875-8967					2019	37	6					7545	7553		10.3233/JIFS-179361													
J								SparkHINlog: Extension of SparkDatalog for heterogeneous information network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bibliographic network; datalog rules; heterogeneous information networks; meta-path; spark graphframes		Real world data is often interconnected, forming large and complex heterogeneous information networks (HINs) with multiple types of objects and links such as bibliographic network (DBLP) and knowledge bases (YaGo). Querying metapaths requires exploration of path instances which can be computational cost in large HINs. However, existing meta-path based studies mostly focus on analytical applications of meta-paths, rather than systems to query meta-paths efficiently in large HINs. To bridge this gap, in this work we present SparkHINlog, a system based on Apache Spark, to handle meta-paths queries efficiently on large scale HINs. In SparkHINlog we propose an algorithm to not only translate meta-paths to Datalog rules, but also to manage the working memory area of Datalog efficiently to increase the scalability of SparkHlNlog. To avoid the computing overhead of join operation to discover path instances when evaluating these rules, we leverage Motif Finding, a powerful tool of GraphFrames Library. With motif finding, SparkHlNLog can speed up the time to evaluate the rules by path finding on graph instead on joining two relations. We conduct experimental comparisons with SparkDatalog, the state-of-the-art large-scale Datalog system, and verify the efficacy and effectiveness of our system in supporting meta-path queries.																	1064-1246	1875-8967					2019	37	6					7555	7566		10.3233/JIFS-179362													
J								Flexible and efficient agent-based metaheuristic computing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										parallel and distributed computing; agent-based platform; metaheuristics		Agent-based metaheuristics computing paradigm (EMAS) has been proposed over 20 years ago by Cetnarowicz. Since then, many efforts were made in order to evaluate, formally analyze and further develop this paradigm towards creating new algorithms as EMAS hybrids, or EMAS-inspired techniques. However, at the same time a significant work has been done in order to build efficient software frameworks supporting this (and similar) computing paradigms. These frameworks were based not only on classic object-oriented programming, but also on functional approach and recently also utilizing heterogeneous infrastructure. This paper presents an overview of the most important findings in this area, including novel ways of processing the agents and component orientation, which allow for both high flexibility and high efficiency of provided solutions. The discussed concepts are illustrated with a case study of a system solving hard computational problem leveraging GPGPU.																	1064-1246	1875-8967					2019	37	6					7567	7578		10.3233/JIFS-179363													
J								Resource constrained portfolio scheduling problem (RCPoSP): A hybrid approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Resource constrained scheduling; constraint satisfaction problem; constraint logic programming; mathematical programming; decision support; group scheduling; fact-based representation; hybrid methods	OPTIMIZATION	The resource constrained portfolio scheduling problem (RCPoSP), in which orders are grouped in portfolios, is proposed in this study. In the RCPoSP the objective is to deliver all orders in the portfolio at the same time after processing. This problem finds many applications in industrial services, manufacturing companies and, where all items (products, services, items etc.) ordered by the customer have to be delivered at the same time in one lot. The goal is to reduce the delivery costs and/or that all elements of the delivery have the same priority, etc. The presented problem also concerns the scheduling of new orders in project portfolios and/or a new project portfolio etc. The minimizations of makespan and/or resource needs for the portfolio are also discussed. The authors present a reference model for the RCPoSP and an intelligent framework for modeling and solving the modeled problem based on the original hybrid approach. The opportunity to ask questions, receive answers as well as data representation in the form of facts constitute an invaluable intelligent support to users utilizing this framework. The goal is to provide an intelligent hybrid framework for stating and solving constraint satisfaction or optimization of RCPoSPs. The calculation examples illustrate the capabilities and computational efficiency of the proposed framework.																	1064-1246	1875-8967					2019	37	6					7579	7593		10.3233/JIFS-179364													
J								Distributed authentication system to access data in multiple secure domains	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Security; authentication; access control; keys distribution; kerberos		Currently, distributed systems are used to store information in remote sites. However, these systems are exposed to different types of security risks such as virus, Trojans or ramsomware, and security mechanisms are required to protect the access to these data and guarantee their privacy and integrity. Authentication plays an important role for security issues in a computer system. Authentication is used to prove the user identity, and it is strongly related with the access control to limits the actions and operations that an authenticated user can do in a computer system. However, authentication is a previously step to the access control, and it assumes that authentication of a user has been done successfully. Several cryptography methods can be integrated in an authentication mechanism in order to obtain robust authentication schemes. An authentication scheme based on Kerberos to access data in multiples domains is presented in this paper. A challenge in our authentication scheme is related with the authentication of Kerberos servers. To deal with this problem a keys distribution architecture is added to authentication scheme in order to authenticate the Kerberos servers in a secure way.																	1064-1246	1875-8967					2019	37	6					7595	7606		10.3233/JIFS-179365													
J								Solving dynamic TSP by parallel and adaptive ant colony communities	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Dynamic TSP; Ant Colony Community; PACO; immigrant based colonies; ACO parallel implementation	OPTIMIZATION	The paper verifies the usefulness of a parallel and adaptive Ant Colony Communities (ACC) for solving the dynamic Travelling Salesman Problem (DTSP). ACC consists of a set of client colonies with a server to coordinate their work. Each one of the client colonies implements a standard ACO algorithm. The paper contains a detailed analysis of the operation of ACO for static TSP in order to identify its most dominant parameters. Graph Generator is used to modify the distances in TSP. In order to catch up with the constant changes the ACC should work in parallel and to adopt to the current distances. This is accomplished by modifying the number of iterations and changing the size of its internal prospective solutions buffer. Two implementations of ACC are presented: an asynchronous that works on computers connected through a LAN and a synchronous that uses a Hadoop environment. Numerous experiments clearly indicate, that the adaptive, parallel ACC outperforms both standard version of ACO as well as its versions adopted for DTSP. This is especially true for highly dynamic Graph Generators.																	1064-1246	1875-8967					2019	37	6					7607	7618		10.3233/JIFS-179366													
J								A new reasoning approach combining information systems and interval type-2 fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy sets; fuzzy reasoning; interval type-2 fuzzy sets; data classification; data discovery; pawlak's information system; information granulation; rule induction; fuzzy rules	GRANULATION	In this research, we succeeded in introducing a new reasoning procedure which applies interval type-2 fuzzy sets into a rule induction process. Our proposal allows information granulation which resulted in achieving good experimental results. We introduced decision tables with elements assumed as interval type-2 fuzzy sets which greatly generalize information. Next, by applying corresponding rule induction procedure, we introduced the possibility to generate directly from a benchmark data fuzzy rulebases for type-2 fuzzy inference models. We strongly believe that our reasoning approach will be a proper solution for different research issues such as classification or ranking procedures as well as determining knowledge for fuzzy inference models. The method proposed was tested in a classification problem verified by using medical benchmark data.																	1064-1246	1875-8967					2019	37	6					7619	7630		10.3233/JIFS-179367													
J								Nonlinear grey Bernoulli model based on Fourier transformation and its application in forecasting the electricity consumption in Vietnam	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fourier series; nonlinear grey Bernoulli model; prediction accuracy; residual error; electricity consumption; Vietnam	PREDICTION MODEL; WATER; DEMAND	In recent decades, the Nonlinear Grey Bernoulli Model "NGBM (1, 1)" has been applied in various fields and achieved positive results. However, its prediction results may be inaccurate in different scenarios. In order to expand the field of application and to improve the predictive quality of the NGBM (1, 1) model, this paper proposes an effective model (named Fourier-NGBM (1, 1)). This model includes two main stages; first, we get the error values based on the actual data and predicted value of NGBM (1, 1). Then, we use a Fourier series to filter out and to select the low-frequency error values. To test the superior ability of the proposed model, two numerical data sets were used. One is the historical data of annual water consumption in Wuhan from 2005 to 2012 in He et al. 's paper, and the other is example data from Wang et al. 's paper. The forecasted results prove that the performance of the Fourier-NGBM (1, 1) model is better than three other forecasting models, namely GM (1, 1), NGBM (1, 1) and the improved Grey Regression model. Furthermore, this study also applied the proposed model to forecast the electricity consumption in Vietnam up to the year 2020. The empirical results can offer valuable insights and provide basic information for model building to develop future policies regarding electrical industry management. In subsequent research, more methodologies can be used to reduce the residual error of the NGBM (1, 1) model, such as Markov chain or different kinds of Fourier functions. Additionally, the proposed model can be applied in different industries with fluctuating data and uncertain information.																	1064-1246	1875-8967					2019	37	6					7631	7641		10.3233/JIFS-179368													
J								Empirical evaluation of continuous test-driven development in industrial settings	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										empirical software engineering; agile software development; test-driven development; continuous test-driven development; human-centric experimentation; agile experimentation	PERCENTAGE	BACKGROUND: Continuous Test-Driven Development (CTDD) is, proposed by the authors, enhancement of the well-established Test-Driven Development (TDD) agile software development and design practice. CTDD combines TDD with continuous testing (CT) that essentially perform background testing. The idea is to eliminate the need to execute tests manually by a TDD-inspired developer. OBJECTIVE: The objective is to compare the efficiency of CTDD vs TDD measured by the red-to-green time (RTG time), i.e., time from the moment when the project is rendered not compiling or any of the tests is failing, up until the moment when the project compiles and all the tests are passing. We consider the RTG time to be a possible measurement of efficiency because the shorter the RTG time, the quicker the developer is advancing to the next phase of the TDD cycle. METHOD: We perform single case and small-n experiments in industrial settings presenting how our idea of Agile Experimentation materialise in practice. We analyse professional developers in a real-world software development project employing Microsoft .NET. We extend the contribution presented in our earlier paper by: 1) performing additional experimental evaluation of CTDD and thus collecting additional empirical evidence, 2) giving an extended, detailed example how to use and analyse both a single case and small-n experimental designs to evaluate a new practice (CTDD) in industrial settings taking into account natural constraints one may observe (e.g., a limited number of developers available for research purposes) and presenting how to reach more reliable conclusions using effect size measures, especially PEM and PAND which are more appropriate when data are not normally distributed or there is a large variation between or within phases. RESULTS: We observed reduced variance and trimmed means of the RTG time in CTDD in comparison to TDD. Various effect size measures (including ES, d-index, PEM, and PAND) indicate small, albeit non-zero, effect size due to CTDD. CONCLUSIONS: Eliminating the reoccurring manual task of selecting and executing tests and waiting for the results (by embracing CTDD) may slightly improve the development speed, but this small change on a level of a single developer, multiplied by a number of developers, can potentially lead to savings on the company or industry level.																	1064-1246	1875-8967					2019	37	6					7643	7655		10.3233/JIFS-179369													
J								Fast method of video genre categorization for temporally aggregated broadcast videos	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Content-based video indexing; digital video structures; temporal segmentation; video shot categorization; temporal relations; automatic video genre classification; temporal aggregation; shot length analysis; AVI indexer	CLASSIFICATION; MULTIMEDIA	The automatic detection of video genre is very desirable and necessary for further analysis of videos mainly when the video processing methods should be parameterized according to the specific video features. It improves first of all the efficiency of temporal segmentation. Temporal segmentation is usually the initial stage for the analysis of edited videos, for such processes as highlights detection, removing of undesirable parts like publicity, as well as selection of play segments in sports videos, etc. Then the temporal aggregation method based on the analysis of shot length and consisting in shot grouping into scenes of a given category can be applied to significantly reduce processing time. The analyses and the observations of videos confirm that the editions of videos and the video structures significantly depend on the video genre. Many processes can be better performed if the genre of video is known and the methods and their parameters are adequate to the video genre. The paper presents the analyses and tests in the AVI Indexer showing the impact of shot length on the results of temporal segmentation, temporal aggregation, and genre detection of video edited in a standard and typical way for a given video genre.																	1064-1246	1875-8967					2019	37	6					7657	7667		10.3233/JIFS-179370													
J								Minimizing interpretations in fuzzy description logics under the Godel semantics by using fuzzy bisimulations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										fuzzy description logic; fuzzy bisimulation; bisimilarity; Godel semantics; minimization		We study the problem of minimizing interpretations in fuzzy description logics (DLs) under the Godel semantics by using fuzzy bisimulations. The considered logics are fuzzy extensions of the DL ALC(reg) (a variant of propositional dynamic logic) with additional features among inverse roles, nominals and the universal role. Given a fuzzy interpretation I and for E being the greatest fuzzy auto-bisimulation of I w.r.t. the considered DL, we define the quotient I/(E) of I w.r.t. E and prove that it is minimum w.r.t. certain criteria. Namely, I/(E) is a minimum fuzzy interpretation that validates the same set of fuzzy terminological axioms in the considered DL as I. Furthermore, if the considered DL allows the universal role, then I/(E) is a minimum fuzzy interpretation bisimilar to I, as well as a minimum fuzzy interpretation that validates the same set of fuzzy concept assertions in the considered DL as I.																	1064-1246	1875-8967					2019	37	6					7669	7678		10.3233/JIFS-179371													
J								Adapting ClusTree for more challenging data stream environments	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Concept drift; data streams; ClusTree; on-line learning	SOFTWARE TOOL; ALGORITHMS; KEEL	Data stream mining seeks to extract useful information from quickly-arriving, infinitely-sized and evolving data streams. Although these challenges have been addressed throughout the literature, none of them can be considered "solved." We contribute to closing this gap for the task of data stream clustering by proposing two modifications to the well-known ClusTree data stream clustering algorithm: pruning unused branches and detecting concept drift. Our experimental results show the difficulty in tackling these aspects of data stream mining and the sensitivity of stream mining algorithms to parameter values. We conclude that further research is required to better equip stream learners for the data stream clustering task.																	1064-1246	1875-8967					2019	37	6					7679	7688		10.3233/JIFS-179372													
J								Design an optimized rectifier with TLBO based on threshold voltage cancellation technique	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Vth cancellation technique; rectifier; TLBO; PCE	TAGS	In this paper a full wave rectifier is presented for RFID passive tags working at 960 MHz frequency. For designing the rectifier multi-stage structure, which has high sensitivity and efficiency, is used for the low amplitude input voltage. In order to eliminate the effect of pass transistors threshold voltage, bootstrap circuit which has cross coupled structure is utilized. For optimizing power conversion efficiency (PCE) and gaining high output voltage from the low input voltage the size of the elements and number of stages are modeled and optimized with neural network and TLBO algorithm, respectively. Due to the achieved results from the TLBO algorithm 6 stages are considered for the rectifier designing. For the low input voltage 0.6 V, the power conversion efficiency and the output DC voltage are achieved 50.8% and 1.52 V, respectively. The simulation of the proposed rectifier is done with Cadence software in 0.18 mu m CMOS technology and its layout equals to 0 .005 25mm(2).																	1064-1246	1875-8967					2019	37	6					7691	7698		10.3233/JIFS-151237													
J								A fuzzy AHP-VIKOR approach for evaluation of educational use simulation software packages	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Simulation software package selection; fuzzy; multiple-criteria decision making; AHP; VIKOR	MACHINE-TOOL SELECTION; POWER-PLANT LOCATION; DECISION-MAKING; MULTICRITERIA SELECTION; SITE SELECTION; MCDM METHODS; FRAMEWORK; PROMETHEE; METHODOLOGY; PERFORMANCE	In this study, an intelligent, integrated approach is presented to help educators select the best simulation software package. Selecting the best simulation software package for educational use is a complex multiple criteria decision making (MCDM) problem with several potentially conflicting quantitative and qualitative criteria. In this paper, two fuzzy MCDM methods; fuzzy Analytic Hierarchy Process (F-AHP) and fuzzy VIsekriterijumska optimizacija i KOmpromisno Resenje (F-VIKOR) are integrated to evaluate educational use simulation software package alternatives. In the proposed fuzzy AHP-VIKOR approach, F-AHP is used to determine the fuzzy criteria weights and F-VIKOR is applied to rank simulation software package alternatives with respect to these criteria. A case study is given where several educational use simulation software packages in Turkey are evaluated and ranked.																	1064-1246	1875-8967					2019	37	6					7699	7710		10.3233/JIFS-172290													
J								M-fuzzifying basic inquisitive semantics	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										M-fuzzifying supporting mapping; M-fuzzifying inquisitive content mapping; M-fuzzifying informative content mapping; M-fuzzifying assertive projection operator; M-fuzzifying questioning projection operator	LOGIC	The basic system of inquisitive semantics (InqB) established by Groenendijk et al. is a general inquisitive semantic theory which doesn't concern fuzziness. To explain the fuzzy phenomena in natural languages, this paper extends InqB into the framework of M-fuzzifying setting and establishes a basic system of M-fuzzifying inquisitive semantics. To begin with, the notion of M-fuzzifying supporting mapping is defined, where M is a completely distributive lattice with an involution operator and each subset of the universal set of all possible worlds can be regarded as a support of any well-formed formula to some degree. Then the notions of M-fuzzifying entailment order, M-fuzzifying truth mappings, M-fuzzifying informative content mappings and M-fuzzifying inquisitive content mappings are introduced and their properties are discussed. Further, the degrees of assertiveness, informativeness, inquisitiveness and questioning of a well-formed formula are defined, by which the M-fuzzifying assertive projection operator and the M-fuzzifying questioning projection operator are introduced and characterized. Finally, a necessary and sufficient condition is obtained, where a well-formed formula is exactly the disjunction of its unique M-fuzzifying assertive projection and unique M-fuzzifying questioning projection.																	1064-1246	1875-8967					2019	37	6					7711	7723		10.3233/JIFS-182500													
J								Linked adaptive neuro-fuzzy inference system for biosignal distortion detection system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Linked adaptive neuro fuzzy inference system; biosignal distortion detection; driver healthcare system; sensor fusion	IDENTIFICATION; CLASSIFICATION	This paper proposes a biosignal distortion detection algorithm for a driver healthcare system based on a contact biosensor and a linked adaptive neuro-fuzzy inference system (ANFIS), and demonstrate its superiority using actual vehicle experiments. Contact biosensors are highly sensitive to vehicle vibration and turning. Although vehicle suspension contributes significantly to ride quality, vibration transfers to the driver and contact between the driver and biosensor can become unstable when executing a turn, causing the driver's biosignal to not be measured well. This study estimated the driver's biosignal state using acceleration, angular velocity, and slip ratio measurements obtained from sensor fusion. When the measurement exceeded a defined threshold, the driver healthcare system removed unreliable biosignal data. We adopted ANFIS to improve the proposed sensor fusion algorithm estimate accuracy for the driver's biosignal state and improved the healthcare system robustness to road conditions. The effectiveness of the proposed algorithm was demonstrated experimentally by comparing the system using sensor fusion and linked ANFIS.																	1064-1246	1875-8967					2019	37	6					7725	7735		10.3233/JIFS-182532													
J								A new approach for hybrid multi-attribute variable weight decision making with decision maker's behavioral character	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multiple attribute decision making; variable weight decision method; prospect theory; multiple types of attribute values	INTUITIONISTIC FUZZY NUMBERS; GREEN SUPPLIER SELECTION; PROSPECT-THEORY; RANKING METHOD; TOPSIS; PREFERENCE; INFORMATION; REPRESENTATION; METHODOLOGY; MODEL	In this paper, we present a newly developed methodology for solving hybrid multi-attribute decision making (HMADM) problems with multiple types of attribute values (MTAVs) by introducing the satisfaction degrees of alternatives' closeness to the positive ideal solution for the decision maker (DM) and a compromise-typed variable weight decision method, where the weights of attributes are related to the satisfaction degrees of MTAVs and the DM's behavior characteristics. An example is presented to show the application of the decision process and a detailed comparison analysis is provided to show the applicability and validity of the method proposed.																	1064-1246	1875-8967					2019	37	6					7737	7746		10.3233/JIFS-182539													
J								The hybrid MCDM model with the interval Type-2 fuzzy sets for the software failure analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										MCDM; failures; software; FMEA; type-2 trapezoidal fuzzy numbers	GROUP DECISION-MAKING; INFORMATION-TECHNOLOGY; RISK; TOPSIS; VIKOR; MAINTENANCE; RANKING; COPRAS; FMEA; AHP	Evaluation and analysis of failures which occur in the products or services in different economic areas are an important task of operational management. Solution of this problem leads to the increase of product's/service's quality, but in the same time it also increases business processes effectiveness and business goal's realization. The treated problem is especially important in the information technologies domain. In this paper, the risk factors that may cause failures of the software are defined in compliance with the Failure Mode and Effect Analysis (FMEA) framework and they are assessed during the development and the maintenance phase. The relative importance of these risk factors and their values at the level of each identified failure are described by pre-defined linguistic terms which are modelled by the interval type-2 trapezoidal fuzzy numbers (IT2TrFNs). The weights vector is calculated by using the Fuzzy Analytic Hierarchy Process (FAHP) with interval type-2 fuzzy sets. The rank of failures is obtained by using the complex proportional assessment (COPRAS) method. The example with real life data is illustrated to demonstrate the potential and applicability of the adopted methods.																	1064-1246	1875-8967					2019	37	6					7747	7759		10.3233/JIFS-182541													
J								Copy-move forgery detection based on local intensity order pattern and maximally stable extremal regions	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image forensics; copy-move forgery detection; maximally stable extremal regions; local intensity order pattern	DUPLICATION FORGERY; EFFICIENT	To abandon the use of overlapping block division and resist the tampering factor of illumination change, a novel copy-move forgery detection method is proposed based on Maximally Stable Extremal Regions (MSERs) and the Local Intensity Order Pattern (LIOP), that integrates block-based and keypoints-based methods. The method involves the following steps: first, affine transformation invariant MSERs are used to maintain the geometrical transformation invariance and reduce computational complexity; second, LIOP features are used to describe the texture and resist illumination change; and finally, RANdom SAmple Consensus is applied to remove false matches. The experiments indicate that the proposed method has great performance for scaling, rotation and illumination changes. Moreover, the method has the high robustness to Gaussian noise, Gaussian blur and JPEG compression.																	1064-1246	1875-8967					2019	37	6					7761	7768		10.3233/JIFS-182647													
J								Bootstrapping and multiple imputation ensemble approaches for classification problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Missingness; ensemble; bagging; multiple imputation; expectation maximization	MISSING DATA; INCOMPLETE DATA; REGRESSION; MODELS	Presence of missing values in a dataset can adversely affect the performance of a classifier. Single and Multiple Imputation are normally performed to fill in the missing values. In this paper, we present several variants of combining single and multiple imputation with bootstrapping to create ensembles that can model uncertainty and diversity in the data, and that are robust to high missingness in the data. We present three ensemble strategies: bootstrapping on incomplete data followed by (i) single imputation and (ii) multiple imputation, and (iii) multiple imputation ensemble without bootstrapping. We perform an extensive evaluation of the performance of the these ensemble strategies on eight datasets by varying the missingness ratio. Our results show that bootstrapping followed by multiple imputation using expectation maximization is the most robust method even at high missingness ratio (up to 30%). For small missingness ratio (up to 10%) most of the ensemble methods perform equivalently but better than single imputation. Kappa-error plots suggest that accurate classifiers with reasonable diversity is the reason for this behaviour. A consistent observation in all the datasets suggests that for small missingness (up to 10%), bootstrapping on incomplete data without any imputation produces equivalent results to other ensemble methods.																	1064-1246	1875-8967					2019	37	6					7769	7783		10.3233/JIFS-182656													
J								A new multi-criteria decision making algorithm for medical diagnosis and classification problems using divergence measure of picture fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Picture fuzzy set; picture fuzzy divergence measure; medical diagnosis; classification problem	AGGREGATION OPERATORS; ENTROPY; MODELS; (I	A divergence measure plays an important part in distinguishing two probability distributions and drawing conclusions based on that discrimination. In this paper, we proposed the concept of divergence measure of picture fuzzy sets. We also built some formulas of the proposed divergence measure of picture fuzzy sets anddiscussed some basic properties of this measure.Based on the proposedmeasure, we developed a multi-criteria decision-making algorithm. Finally, we applied the proposed multi-criteria decision-making algorithm in the medical diagnosis problem and the classification problem.																	1064-1246	1875-8967					2019	37	6					7785	7796		10.3233/JIFS-182697													
J								Anti-periodic solutions for quaternion-valued inertial neural networks with time-varying delays	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Quaternion-valued inertial neural networks; anti-periodic solutions; coincidence degree; exponential stability	STABILITY; SYNCHRONIZATION; MODELS	In this paper, we consider a class of quaternion-valued inertial neural networks with time-varying delays. First, by applying a continuation theorem of coincidence degree theory, we establish the existence of anti-periodic solutions of the considered neural networks. Second, by choosing a proper variable substitution, we transform the neural networks into a system of first order differential equations, and by constructing a suitable Lyapunov function, we derive a set of sufficient conditions ensuring the global exponential stability of the system. Finally, we give two examples to illustrate the effectiveness of our results.																	1064-1246	1875-8967					2019	37	6					7797	7813		10.3233/JIFS-182731													
J								Fractal bubble algorithm for simplification of 3D point cloud data	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										3D point cloud; fractal bubble algorithm; data simplification; multi-scale reduction		We present a novel technique for 3D point cloud simplification - the so-called fractal bubble algorithm - to minimize the computational time and overall storage space. The proposed fractal bubble algorithm generates 2D elastic bubbles and copies of themselves through 2D data sets representing planar geometric contours. Each of the bubbles, as it grows, is made to select a single point of its first contact, and all the selected points become the simplified set of points. The fractal bubble algorithm is repeatedly applied to the simplification of planar slices of general 3D point clouds corresponding to 3D geometric objects, leading to the global simplification of 3D point clouds. The benefits of the algorithm are: first the algorithm is computationally light and memory efficient, second it is simple to implement and inherently allows the organized selection of the points of contact and finally it enables us to simplify the point cloud data through a multi-scale fashion by varying a set of user-controlled algorithm parameters. Numerical results verify the effectiveness of the proposed algorithm.																	1064-1246	1875-8967					2019	37	6					7815	7830		10.3233/JIFS-182742													
J								A genetic algorithm for total graph coloring	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Total coloring; total chromatic number; total coloring conjecture; genetic algorithm; crossover; mutation	TOTAL CHROMATIC NUMBER; PLANAR GRAPHS; EVOLUTIONARY ALGORITHMS	A genetic algorithm (GA) belongs to the class of evolutionary algorithms and it is one of the most studied heuristic algorithms to solve graph coloring problems. In this paper, we propose a new GA algorithm for the total graph coloring problem. To the best of our knowledge, no algorithm based on a GA exists in the literature for total graph coloring. In the proposed approach, a novel encoding scheme is introduced, where all the edges and vertices of the graph are represented in a chromosome without any repetition. For the initialization of the population, a greedy algorithm is used to determine the total number of colors required for a total coloring of the graph. The number of colors is used as the fitness value of a chromosome which depends on the sequence of vertices and edges representing the chromosome. We introduce a convergence criteria for GA based on the total coloring conjecture. A two-point crossover and mutation operations, suitable for total coloring, are suggested. The proposed algorithm is applied on some well-known and standard graphs. In our computational tests, graphs are used with a maximum number of 690 vertices and 6650 edges of the graph, respectively. The proposed algorithm determines an optimal solution for 21 graphs among the 27 graphs. The solution of remaining the 6 graphs is near optimal and differs by at most one unit from the optimal value. The results show the effectiveness of the proposed approach.																	1064-1246	1875-8967					2019	37	6					7831	7838		10.3233/JIFS-182816													
J								Acceptance sampling plans for two-stage process for multiple manufacturing lines under neutrosophic statistics	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Neutrosophic statistic; EWMA; sampling plan; producer's and consumer's risks; multistage neutrosophic; average sample number	ROUGH SET MODELS; FUZZY; WEIBULL; DESIGN; (I	In this paper, a new acceptance-sampling plan has been introduced for the two-stage process for multiple lines under the neutrosophic statistics. The parameters of the proposed sampling plan have been determined by satisfying the given risks using the optimization solution under the neutrosophic statistical interval method (NSIM) Using the specific producer's and consumer's risks, the parameters of the proposed plan have also been determined under neutrosophic operating function (NOF). The comparison based on the sample size of the proposed and the existing plans has been given at different plan parameters. The tables are provided, and an industrial example is illustrated for the practical use of the proposed sampling plan. The comparison reveals that the proposed plan is more efficient, flexible, and adequate to be used under uncertainty.																	1064-1246	1875-8967					2019	37	6					7839	7850		10.3233/JIFS-182849													
J								Fuzzy based optimal and traffic-aware restricted access window mechanism for dense IoT networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of Things (IoT); restricted access window (RAW); IEEE 802.11ah; fuzzy logic; Wi-Fi HaLow; fuzzy c-means clustering	IEEE 802.11AH; PROTOCOL; INTERNET; MAC; DCF	IEEE 802.11ah defines amendments to IEEE 802.11 to support the Internet of Things (IoT). IEEE 802.11ah implements restricted access window (RAW) mechanism to reduce the contention and energy consumption in dense IoT networks. The RAW mechanism is a group-based MAC protocol that partitions the devices into various groups and confines the channel access of a group of devices to the restricted time interval known as the RAW slot. However, the standard does not specify, grouping mechanism, duration of RAW slots, and the number of RAW slots while configuring the RAW mechanism. In an IoT network, each device has distinct transmission requirements. Thus, it is necessary to find the optimal number of RAW slots that can maximize the network performance, to group the devices with similar transmission requirements and to assign a RAW slot that adaptively varies with the traffic requirements of the respective group. In this paper, we exploit fuzzy logic to find the optimal number of RAW slots by considering network size, collision probability, and modulation and coding schemes. Further, we propose a traffic-aware adaptive RAW slot allocation (TARA) scheme that uses fuzzy c-means clustering algorithm to group the devices with similar traffic requirements and to assign each group with a RAW slot whose duration adaptively varies with the transmission requirements of the devices. We have also presented a simple yet accurate analytical model to evaluate the performance of the RAW mechanism. Results show that the optimal number of RAW slots found using fuzzy logic significantly enhances the performance of the RAW mechanism in terms of throughput and energy consumption. Further, it is observed that the TARA scheme can effectively meet the traffic requirements of different group of devices when compared to the uniform grouping scheme. Finally, extensive simulations are conducted using ns-3 to validate the analytical results.																	1064-1246	1875-8967					2019	37	6					7851	7864		10.3233/JIFS-182899													
J								Optimal design of fractional order fuzzy PID controller using an intelligent hybrid algorithm for nonlinear power system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fractional-order fuzzy controller; intelligent hybrid algorithm; virus serach colony; bee colony; optimization; low frequency oscillation	SIGNAL STABILITY ANALYSIS; OPTIMIZATION	Generally, increase in energy demand and consequently unwanted events in a current system will lead to instability. To increase the power system reliability, the current system can be expanded, resulting in a high financial burden. Therefore, one of the most effective options is appropriate controllers usage for stability. In order to overcome the defects of classical controllers such as non-linear, complex uncertain systems, an appropriate mathematical model needs to be designed in limited working conditions. In this paper, a new fractional-order fuzzy proportional-integral-deferential (FOPID) controller is proposed. The proposed controller in its structure is an integral, derivative gain with a fractional order. This controller is structurally adjustable with two fractional orders, performing the stability process in a short time. On the other hand, in the proposed controller, the optimal adjustment of the controller gain and membership functions has turned into an optimization problem, which is done by a hybrid algorithm based on the virus colony search (VCS) and artificial bee colony (ABC). Local and final search powers in the proposed hybrid algorithm reduce the possibility of local presence dramatically. Simulation results have shown that the proposed controller achieves the higher robustness, the lower fall time, and the lower frequency oscillations compared to the existing controllers.																	1064-1246	1875-8967					2019	37	6					7865	7882		10.3233/JIFS-182918													
J								New fuzzy approach to facility location problem for extreme environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-objective facility location problem; associated probabilities of a fuzzy measure; fuzzy numbers; choquet integral; multi-attribute group decision making	AGGREGATION; REPRESENTATION; OPERATORS	For the facility location problem under extreme environment a two-stage fuzzy approach is developed. On the first stage, the fuzzy multi-attribute group decision making (MAGDM) model for evaluation of the selection ranking index of a candidate service site is created. For this purpose the triangular fuzzy Choquet averaging (TFCA) operator is constructed. Interaction attributes, influencing the service centers' selection process, are defined. Interaction indexes between attributes and importance values of attributes are taken into account in the construction of the 2-order additive triangular fuzzy valued fuzzy measure (TFVFM). On the second stage, based on the TFCA operator, a new objective function - selection ranking index of candidate sites is constructed. We consider also two classical objective functions - total cost for opening of service centers and number of agents needed to operate the opened service centers. A new objective function together with latter ones creates the multi-objective fuzzy facility location set covering problem. A Pareto front for this problem is constructed. A simulation example of emergency service facility location planning for a city is considered. The example deals with the problem of planning fire station locations for serving emergency situations in specific demand points - critical infrastructure objects.																	1064-1246	1875-8967					2019	37	6					7883	7893		10.3233/JIFS-18723													
J								Mianorm-based logics with n-contraction and n-mingle axioms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mianorms; substructural logic; fuzzy logic; semilinear logic; mianorm-based logic	PSEUDO-UNINORMS; FUZZY	This paper investigates standard completeness for substructural fuzzy logics based on mianorms with n-contraction and n-mingle axioms. For this, first, right and left n-contractive and n-mingle logic systems based on mianorms, their corresponding algebraic structures, and their algebraic completeness results are discussed. Next, completeness with respect to algebras whose lattice reduct is [0, 1], known as standard completeness, is established for these systems via Yang's construction in the style of Jenei-Montagna. Finally, further standard completeness results are introduced for their fixpointed involutive extensions.																	1064-1246	1875-8967					2019	37	6					7895	7907		10.3233/JIFS-190150													
J								Research and comparison of uncertain portfolio selection model with background risk and mental accounts	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertain theory; portfolio selection; background risk; mental account; saving account	OPTIMIZATION; ALGORITHM	Considering the uncertainty of financial market and the investor's different attitudes towards risk caused by the various goals, in this paper, a portfolio selection problem with background risks and mental accounts constraints is studied to explore their impact on investment decisions. Firstly, we establish a model with normal uncertain variables, and the optimal solutions of portfolio models with and without background risk are compared. Secondly, considering that the investors always divide an account into several sub-accounts, we put forward an uncertain portfolio model combining uncertainty theory and mental account theory. Thirdly, a portfolio model with background risk and mental accounts is proposed, and the total expected returns of the models in with different proportions of mental accounts are compared. Finally, some numerical applications are provided to validate the model. The result shows that when the levels of tolerance are the same, the expected return of a portfolio with background risk is lower than that without background risk. In addition, the result also shows that when the percent of savings account decreases and that of the consumption account increases, the total expected return increases.																	1064-1246	1875-8967					2019	37	6					7909	7921		10.3233/JIFS-190157													
J								ELECTRE TRI-C with hesitant outranking functions: Application to supplier development	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										ELECTRE TRI-C; hesitant fuzzy sets; sensitivity analysis; supplier development	MULTICRITERIA DECISION-MAKING; PREFERENCE RELATIONS; RISK-ASSESSMENT; I METHOD; FUZZY; EXTENSION; MANAGEMENT; SELECTION; NUMBERS; MODEL	ELECTRE TRI, a family of multi-criteria methods used to sort alternatives into preference-ordered categories, defines an outranking function to measure the membership degree of an alternative to a category, whenever imprecise evaluations are available. Recent extensions use Hesitant Fuzzy Sets (HFS) to consider uncertain evaluations. However, deterministic parameters are considered, which avoids the application to cases in which non-fuzzy scores and unstable parameters are available. In this study, an approach is proposed for: 1) modeling hesitant outranking functions originated from unstable parameters provided by several DMs; 2) using the HFS to calculate the ELECTRE TRI-C indices; 3) reducing the DMs cognitive effort when they are asked to provide information. An application to supplier development is presented by using ELECTRE TRI-C. Results are compared by using different HFS aggregation operators and sensitivity analysis shows that a robust conclusion can be obtained. Future lines of research are also suggested.																	1064-1246	1875-8967					2019	37	6					7923	7933		10.3233/JIFS-190166													
J								An extended TODIM method and its application in the stock selection under dual hesitant fuzzy linguistic information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-criteria decision-making; dual hesitant fuzzy linguistic element (DHFLE); TODIM method; distance measure; score function	GROUP DECISION-MAKING; OF-THE-ART; PROSPECT-THEORY; TERM SETS; PREFERENCE RELATIONS; MULTICRITERIA; MODEL; REPRESENTATION; CONSISTENCY; CONSENSUS	Uncertain information is inevitable in real life due to decision makers' limited rationality and the complexity of correlative problems. Dual hesitant fuzzy linguistic elements (DHFLEs) can collect all the information from two different viewpoints qualitatively and it has become one of the most effective tools for dealing with uncertain information. In this paper, the main contribution is to propose an extended TODIM method under dual hesitant fuzzy linguistic information and to apply this method to deal with a stock selection problem. Firstly, a new distance measure of DHFLEs is proposed for determining the deviation degree between different DHFLEs. Secondly, in order to distinguish between different DHFLEs effectively, we construct a novel score function of the DHFLE. The score function and the new distance measure are used to complete the extended TODIM method under dual hesitant fuzzy linguistic information. Finally, to show the validity and the practicability, we use the extended TODIM method to solve a practical problem of stock selection.																	1064-1246	1875-8967					2019	37	6					7935	7950		10.3233/JIFS-190194													
J								Optimal fractional order fuzzy based AGC using MODA & NSGA-II to improve dynamic stability of multi-area interconnected power system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										FOFPID-AGC; MODA; NSGA-II; dynamic stability; concomitant optimization scheme; multi-area interconnected power system	AUTOMATIC-GENERATION CONTROL; LOAD FREQUENCY CONTROL; CONTROLLER; DESIGN; ALGORITHM	The prominent responsibility of Automatic Generation Control (AGC) is controlling the interchange power flow deviations toward suppression of both the frequency and tie-line power deviation during the disturbance occurrence in the interconnected multi-area power system. To appropriately deliver an electric power with high quality, AGC system needs to be equipped with an efficient and intelligent controller. In this regard, Fractional Order Fuzzy-PID (FOFPID) is proposed for AGC system to enhance the dynamic stability of multi-area interconnected power system. Due to multi-objective nature and importance of the design problem, the parameters of FOFPID controller have been optimally tuned by two multiobjective optimization algorithms i.e., Multi-Objective Dragonfly Algorithm (MODA) and Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). The damping performance of self-defined FOFPID-AGC has been thoroughly evaluated under different disturbance conditions namely short circuit, load variation and excitation voltage change in the Four-Machine Kundur and Ten-Machine New England. To precisely extract the FOFPID's parameters, concomitant optimization scheme has been scheduled with consideration of all these disturbances. At last, the damping performance of FOFPID-AGC has been mainly approved by MODA and NSGA-II, and its damping capability has been accordingly validated compared to two defined controllers.																	1064-1246	1875-8967					2019	37	6					7951	7964		10.3233/JIFS-190228													
J								Overlapping Community Detection in Bipartite Networks using a Micro-bipartite Network Model: Bi-EgoNet	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Overlapping community; bipartite networks; complex network	ALGORITHM	A bipartite network is a special kind of complex network that consists of two different types of nodes with edges existing only between the different node types. There are numerous real-world examples of bipartite networks, such as scientific collaboration networks and film-actor networks, among many others. Detecting the community structure of bipartite networks not only contributes to a deeper understanding of their hidden structure, but also lays the foundation for research into the personalized recommendation technology. Most existing algorithms, however, only focus on the detection of non-overlapping community structures while ignoring overlapping community structures. In this study, we developed a micro-bipartite network model, Bi-EgoNet along with an algorithm called Overlapping Community Detection using Bi-EgoNet (OCDBEN). This algorithm first extracts the sub-bi-community set from each Bi-EgoNet using similarity within the bipartite network and then constructs a global community structure by merging the sub-bi-communities using the double-merger strategy. We evaluated the OCDBEN algorithm with several synthetic and real-world bipartite networks and compared it with existing state-of-the-art algorithms. The experimental results demonstrated that OCDBEN outperformed existing algorithms in both accuracy and effectiveness.																	1064-1246	1875-8967					2019	37	6					7965	7976		10.3233/JIFS-190320													
J								Approximation of soft ideals by soft relations in semigroups	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soft relations; Soft set approximation; Soft substructures	ROUGH SETS; TOPOLOGY	A soft set handles indeterminate data. By using an equivalence relation, Pawlak introduced the rough set concept for dealing uncertainty to approximate a set. Many authors generalized the concept and used binary relations to approximate a set. A soft set is approximated in this paper by soft binary relations in the context of the aftersets and foresets. Along these lines, we get two sets of soft sets, called the lower approximation and upper approximation with respect to the aftersets and foresets. We applied these concepts on semigroups and approximations of soft subsemigroups, soft left (right) ideals, soft interior ideals and soft bi-ideals of semigroups are studied. Moreover, for the illustration of the concept, some examples are considered.																	1064-1246	1875-8967					2019	37	6					7977	7989		10.3233/JIFS-190328													
J								Reliability allocation method based on maximum entropy ordered weighted average and hesitant fuzzy Linguistic term set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Reliability allocation; feasibility-of-objectives technique (FOO); the maximum entropy ordered weighted averaging method (ME-OWA); CNC machine tools	MULTICRITERIA DECISION-MAKING; SYSTEM; OWA; OPERATORS	Reliability allocation is one of the most important factors to consider when determining the reliability and competitiveness of a product. The feasibility-of-objectives (FOO) technique has become the current standard for assessing reliability designs for military mechanical-electrical systems. However, the FOO method has several drawbacks: For instance, it requires that the value of reliability allocation factors is single linguistic variables, and it does not consider the ordered weight of reliability allocation factors, but simply multiplies the ISPE (Complexity (I), State of Art (S), Performance Time (P), and Environment (E)) values one by one. This can lead to erroneous results. To address these issues, this paper combines the fuzzy allocation method with the maximum entropy ordered weighted averaging method (ME-OWA) to achieve a flexible allocation of system reliability. To verify the effectiveness of the proposed method, the CNC machine tool is taken as an example. The FOO method and the fuzzy allocation method and the proposed method were used to assign reliability to the eight subsystems of a CNC machine tool, and the results were compared to draw conclusions: The proposed method is more flexible and accurate for reliability allocation.																	1064-1246	1875-8967					2019	37	6					7991	8004		10.3233/JIFS-190376													
J								Noncommutative symmetric difference operators for fuzzy logic	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Noncommutative symmetric difference; t-norm; difference operator; pseudo-quasi-metric	ROBUSTNESS ANALYSIS; CONSTRUCTION; SETS	This paper introduces noncommutative symmetric difference operators for fuzzy logics. Structures and properties of these operators are investigated. Finally, pseudo-quasi-metric and pseudo-metric are constructed on [0,1] based on the noncommutative symmetric differences.																	1064-1246	1875-8967					2019	37	6					8005	8013		10.3233/JIFS-190400													
J								Active distribution network planning considering shared demand management	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Active distribution network; energy storage; non-dominated sorting; shared demand management	ENERGY-STORAGE TECHNOLOGIES; SYSTEM; ALLOCATION; OPERATION; LEVEL	Due to the necessity of using distributed generation and storage devices in the operation of power systems and with the advancement of technology and industry in the distribution networks, network operators are trying to transform these systems from passive distribution networks to active ones. To this aim, the present study introduces a novel model to exploit an active distribution network from the cost, operation conditions, and reliability points of view. A shared demand management procedure in the presence of storage devices and price-responsive loads is used to improve operational efficiency, and it is presented using a sensitivity matrix. Probability density functions (PDFs) are used to model uncertainty in power generated by wind systems and PVs, and the fuzzy membership function is used to improve the voltage profile of the network. To optimize the objective function, given that the problem goals are not of the same kind, the multi-objective genetic algorithm, based on the non-dominated concept, is implemented. Proposed optimal planning and exploiting of the active distribution network based on the shared demand management procedure, not only maximize profit, because of peak shaving, upgrade deferral, power exchange, and loss reduction but also, technical indexes and reliability improvement are obtained due to energy storage systems (ESS) and price-responsive loads simultaneous management. The rationality and effectiveness of the proposed method are verified by the simulation results of a 33-bus active distribution network.																	1064-1246	1875-8967					2019	37	6					8015	8028		10.3233/JIFS-190420													
J								Blockchain: Perspectives and issues	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Blockchain; blockchain security; Privacy protection issue; Scalability issues; peer-to-peer network technology (P2P); consensus mechanism; smart contract; Merkle tree	ECONOMY	The tamper-proof digital database on which are written all exchanges between its users since its inception is named Blockchain. The blockchain is a system of storage distributed on a peer-to-peer network (p2p) using consensus mechanism, asymmetric encryption, smart contracts and other key technologies. In an untrusted environment, blockchain comes to associate the implementation of a mechanism for the exchange of information, transfer of value intended to ensure consistency, and integrity data is the cornerstone of the creation of value future of the Internet. Blockchain technology has the following features: decentralized, safe, reliable, open and transparent. Its characteristics make the most attractive blockchain and is the subject of all the debates at the safe level. File storage, healthcare, banking, insurance, field and other scenarios are widely used. However, the technology of the blockchain is still some problems, such as the low flow of transactions, easy disclosure of user information, the confidentiality of the data of transactions and the safe limits of algorithms encryption. His problems that must be resolved urgently in its development and its application. As an emerging technology, the blockchain still has plenty other still undiscovered problems, but it has broad prospects of development and should allow the creation of a new decentralized era and a new world of Central governance without organ of management.																	1064-1246	1875-8967					2019	37	6					8029	8052		10.3233/JIFS-190449													
J								Some separation axioms in L-convex spaces	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										L-convex space; S-1; sub-S-0; S-0; S-1; S-2	(L; CATEGORY; HULL; JHC	In this paper, some low-level separation axioms of L-convex spaces are introduced, including S-1, sub-S-0, S-0, S-1 and S(2)( )separation axioms. Some relevant properties of these separation axioms are discussed. In particular, the relationships between convex spaces and induced L-convex spaces on some separation axioms are investigated.																	1064-1246	1875-8967					2019	37	6					8053	8062		10.3233/JIFS-190471													
J								Wildebeest herd optimization: A new global optimization algorithm inspired by wildebeest herding behaviour	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Global optimization; Wildebeest Herd Optimization Algorithm; biologically inspired metaheuristic; heuristic optimization; heuristic search algorithm	SWARM OPTIMIZATION	This paper proposes a new metaheuristic global optimization algorithm inspired by Wildebeest herding behavior called Wildebeest Herd Optimization (WHO) algorithm. WHO algorithm mimics the way nomadic Wildebeest herds search vast areas of grasslands efficiently for regions of high food density. The WHO algorithm models five principal Wildebeest behaviors: firstly Wildebeests have limited eyesight and can only search for food locally, secondly Wildebeests stick to the herd to escape predators, thirdly Wildebeest herd as a whole migrates to regions of high food availability based on historical knowledge of annual grass growth rates and rainfall patterns, fourthly Wildebeests move out of crowded overgrazed regions and finally Wildebeests move to avoid starvation. The WHO algorithm is compared to Physics inspired, Swarm based, Biologically inspired and Evolution inspired global optimization algorithms on an extended test suite of benchmark optimization problems including rotated, shifted, noisy and high dimensional problems. Extensive simulation results indicate that the WHO algorithm proposed in this paper significantly outperforms state-of-the-art popular metaheuristic optimization algorithms like Particle Swarm Optimization Algorithm (PSO), Genetic Algorithm (GA), Gravitational Search Algorithm (GSA), Artificial Bee Colony Algorithm (ABC) and Simulated Annealing (SA) on shifted, high dimensional and large search range problems.																	1064-1246	1875-8967					2019	37	6					8063	8076		10.3233/JIFS-190495													
J								Novel TOPSIS method for group decision-making based on hesitant m-polar fuzzy model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hesitant fuzzy set; m-polar fuzzy set; hesitant m-polar fuzzy set; decision-making; TOPSIS	N-SOFT SETS; PREFERENCE RELATIONS; EXTENSION; OPERATORS; AGGREGATION	Hesitant fuzzy sets (HFSs) present a general structure to express the uncertain concepts and data that have been served as in most of the generalizations of fuzzy sets. In this research article, we introduce a novel hybrid model called hesitant m-polar fuzzy sets (HmF-sets), which is a reasonable combination of HFSs with m-polar fuzzy sets (mF sets). It is the generalization of the concept HFSs, in which the membership degrees of an element of given set deals the m different numeric and fuzzy values that enables to deal the hesitancy of multipolar information. Hesitancy integrates the conformity for the analysis of given data, and an mF format concedes to severalize the sources of multi-polar information. We highlight and explore some useful properties, construct fundamental operations and investigate comparison laws of HmF-sets. Moreover, we develop the hesitant m-polar fuzzy TOPSIS approach for multi-criteria group decision-making (MCGDM), which is the natural extension of TOPSIS method and used to rank and choose the best alternative under HmF positive and negative ideal solutions to this framework. We describe applications of HmF-sets in group decision-making and apply our proposed method in real life examples to show its efficiency. Finally, we develop an algorithm that implements our decision-making procedure by using computer programming.																	1064-1246	1875-8967					2019	37	6					8077	8096		10.3233/JIFS-190551													
J								Some spherical linguistic Muirhead mean operators with their application to multi-attribute decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Spherical fuzzy set; spherical linguistic set; spherical linguistic muirhead mean; multi-attribute decision making	FUZZY AGGREGATION OPERATORS; SIMILARITY MEASURES; MADM; SETS	This paper aims to propose a new tool to express decision makers' preference information in multi-attribute decision making (MADM) producers. By taking advantages of spherical fuzzy sets (SFSs) and linguistic variables (LVs), we give the definition of spherical linguistic sets (SLSs) and provide operations of spherical linguistic numbers (SLNs). Based on the proposed operations, we incorporate Muirhead mean (MM) into SLSs and introduce novel spherical linguistic aggregation operators. These proposed operators adsorb the inherent advantages of MM, i.e., taking the interrelationship among any numbers of aggregated inputs into account and producing flexible information fusion process. Furthermore, we apply the proposed method in MADM and present the main steps of a new method. In order to show its effectiveness, we use the method to solve an actual MADM problem. The advantages and superiorities of the proposed method are also studied.																	1064-1246	1875-8967					2019	37	6					8097	8111		10.3233/JIFS-190566													
J								A novel mean-variance-maverick DEA prospect cross-efficiency approach for fuzzy portfolio selection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy portfolio selection; data envelopment analysis (DEA); risk attitude; prospect cross-efficiency; meanvariance-maverick model	DATA ENVELOPMENT ANALYSIS; PERFORMANCE EVALUATION; MUTUAL FUNDS; DECISION; MODEL; IMPROVEMENT; FRONTIER; UNITS; RISK	In this paper, a novel framework is proposed for fuzzy portfolio selection based on a combination between Data Envelopment Analysis (DEA) prospect cross-efficiency approach and the maverick index. Although DEA cross-efficiency evaluation is used to an effective tool for portfolio selection, no researcher has yet attempted to combine DEA cross-efficiency method with investors' psychology in fuzzy portfolio selection. To address this problem, two novel prospect cross-efficiency models termed PCE (I) and PCE (II) are developed as the foundations for the construction of a novel fuzzy portfolio model. Because of the uncertain environment of financial market, the returns of assets are characterized as triangular fuzzy numbers. To make our models more comprehensive and practical, five criteria including mean, variance, semi-variance, skewness and entropy are employed in PCE models. Furthermore, based on the PCE evaluation, a novel mean-variance-maverick (MVM) framework is designed for fuzzy portfolio selection, in which the prospect cross-efficiency is viewed as return characteristic, maverick index and variance are considered as risk characteristics. The maverick index, as a novel risk measure, can be used as a good indicator for sensitivity to environment volatility in portfolio selection. Finally, a numerical example is provided to illustrate the effectiveness of our proposed models. The results show that our proposed approach can not only capture the risk attitudes of investors, but also permit well-diversified portfolios and higher risk-adjusted returns than other benchmark portfolios.																	1064-1246	1875-8967					2019	37	6					8113	8130		10.3233/JIFS-190568													
J								Generating priority series via AHP for conducting statistical tests on CAMELS dimension priorities in evaluating bank failure risk	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										AHP; bank's bankruptcy risk; CAMELS; liquidity; asset; capital	BANKRUPTCY PREDICTION; SOUNDNESS; ABILITY; SECTOR	The AHP technique application is usually for determining criteria priorities according to respondents, but no statistical query exists for instance, whether the experts' views significantly change with respect to some characteristics, the criteria priorities significantly differentiate with each other. This research objective is along with determining the priorities of subcriteria in the evaluation of bank failure/bankruptcy risk, to show generating priority series from experts' views for each criteria for carrying out statistical tests with respect to expert subgroups, and then produce information for researchers/decision makers. The research utilises the usage of conducting statistical hypothesis testing on generated priority series and CAMELS approach to bank failure. This study investigates and determines the subcriteria priorities of CAMELS dimensions, and uses the data of study Pekkaya & Erol (2016) for statistical tests by generating priority series of CAMELS dimensions. Since no similar academic study, which uses statistical tests and generates priority series in bank failure/bankruptcy literature via similar approach, is observed; this study can be accepted as paving the way of the usage of AHP technique. The obtained priority values of subcriteria with main criteria of CAMELS dimensions can be used to improve the early warning system for bank failure.																	1064-1246	1875-8967					2019	37	6					8131	8146		10.3233/JIFS-190574													
J								Automatic vehicle detection using spatial time frame and object based classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Vehicle detection; multiple spatial time frames; object oriented classification; rule based method; motion detection	TRACKING; EXTRACTION	This paper presents an automated vehicle identification and classification method from traffic videos. The proposed method unlike other traditional methods combines the multiple time spatial frames to detect moving objects. These moving objects are the potential vehicles however there may be some other moving objects also. Therefore to further improve the accuracy of the proposed method, the moving objects are classified using object oriented classification scheme. The identification of vehicles from traffic videos plays an important role in Intelligent Transport systems (ITS). A virtual line is placed on each frame such that the objects crossing this line are the desired moving objects. The object based classifier makes use of fuzzy rules based on features like area, perimeter, and elongation and so on. These fuzzy rules are used to classify them into vehicle and non-vehicle classes. The second level of classification further classifies the vehicles into two wheeler, four wheeler and six wheeler vehicles. The method can be appropriately used for traffic surveillance as it also computes the speed of vehicles using the time spatial frames. The proposed method is applied on traffic videos of multiple time lengths. A comparative study of the proposed method with the existing methods reveals that the proposed work has higher accuracy. The motion detection, vehicle classification and speed of computation make this method best suited for many ITS applications like traffic surveillance and other similar applications.																	1064-1246	1875-8967					2019	37	6					8147	8157		10.3233/JIFS-190593													
J								Wijsman regularly ideal convergence of double sequences of sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Regularly ideal convergence; double sequence of sets; Wijsman convergence	LACUNARY STATISTICAL CONVERGENCE; CONES	In this paper, we introduce the notions of regularly (I-W2 , I-W)-convergence, regularly (I-W2* , I-W*)-convergence, regularly (I-W2 , I-W) -Cauchy and regularly (I-W2* , I-W*)-Cauchy double sequence of sets and investigate the relationship among them.																	1064-1246	1875-8967					2019	37	6					8159	8166		10.3233/JIFS-190626													
J								Customer's class transformation for profit maximization in multi-class setting of Telecom industry using probability estimation decision trees	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data mining; probability estimation decision trees; actionable knowledge discovery; decision making; profit maximization; Telecom sector	CHURN PREDICTION; ACTIONABLE KNOWLEDGE; INDUCTION; MODELS	Telecom sector is hugely losing profits in different degrees due to various undesired classes of its customers. Churners, a certain class of customers shifting to the competitors, are the most undesired class of customers who are the predominant reason for the losses. Still, there are other classes of customers in this business who stay with the enterprise, but they are inactive in using the services and leading to uncertainty and an insignificant amount of profits. When data mining techniques are applied to such applications they produce customer models in the form of decision trees, etc. and provide customer's class label only such as churner/non-churner. Furthermore, they only focus on improving the technical interestingness measures of prediction models. Thus, very limited research has been carried out on turning the prediction results into useful decision making actions. Consequently, some manual work by domain expert has to be done to postprocess the model to obtain the actionable knowledge for changing the customer from undesired class to the desired one. However, some of the existing works are suggesting the actions to convert the class of the customer from one category to another, but they have limitations in that they do not generalize to more than two classes. In this paper, a novel algorithm, which aptly fits the multi-class setting of Telecom sector, is presented that suggest actions to change the customer from an undesired class to a desirable one with maximum net profit. We explain our proposed method with the help of a case study of the Telecom sector. Empirical tests are conducted on the case study problem and also on UCI benchmark data and shown that our method is effective and scalable. With the help of comparison with state-of-the-art methods and substantial experiments, we demonstrate the efficiency of the proposed method.																	1064-1246	1875-8967					2019	37	6					8167	8197		10.3233/JIFS-190628													
J								On the resummation of series of fuzzy numbers via generalized Dirichlet and generalized factorial series	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										series of fuzzy numbers; Tauberian theorems; fuzzy Fourier series	STATISTICAL CONVERGENCE; SEQUENCES; SUMMABILITY; EQUATIONS; SETS	We introduce semicontinuous summation methods for series of fuzzy numbers and give Tauberian conditions under which summation of a series of fuzzy numbers via generalized Dirichlet series and via generalized factorial series implies its convergence. Besides, we define the concept of level Fourier series of fuzzy valued functions and obtain results concerning the summation of level Fourier series.																	1064-1246	1875-8967					2019	37	6					8199	8206		10.3233/JIFS-190632													
J								Intelligent business analytics using proposed input/output oriented data envelopment analysis DEA and slack based DEA models for US-airlines	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Performance evaluation; date envelopment analysis (DEA); slack based DEA model (SBM); airline industry	REVENUE MANAGEMENT; DISTANCE FUNCTION; EFFICIENCY; PERFORMANCE; PRODUCTIVITY; CARRIERS; MARKET	The study presents Proposed Data Envelopment Analysis (DEA) approaches for intelligent business analytics of the US Airline industry to gain more knowledge, control, flexibility and trade-off among various inputs to the system/industry. Three new proposed DEA models are presented. The results obtained provide credible data analytics and intelligent business suggestions. Firstly the proposed input oriented and the proposed output oriented DEA models are formulated. The proposed input oriented DEA model suggests how resource utilization can be minimized while operating in with the same output level. Further, the proposed output oriented model suggests how output efficiency can be increased to a benchmark level while keeping the input at the same level. Then the proposed Slack Based DEA model SBM is formulated. The proposed models are then solved with the DEA Excel Solver for finding the efficient points in the Pareto Frontier. Representing the Pareto Frontier as a function of total system revenue gives necessary information about the inputs growth and their trade-off with the output. Furthermore, the proposed Slack Based DEA model (SBM) intelligently measures the technical efficiency of each airline. The results show that three airlines namely American, United and Jet Blue are weakly efficient operating below the proposed SBM efficient frontier. Furthermore, the results intelligently suggest the possible inputs reduction and outputs increases to get to the efficient frontier for the management of the concerned airlines. The proposed SBM slacks suggest the possible areas of improvement for future planning and optimal operating input/output levels for the top management of the weakly efficient airlines.																	1064-1246	1875-8967					2019	37	6					8207	8217		10.3233/JIFS-190641													
J								Effective load forecasting for large power consuming industrial customers using long short-term memory recurrent neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Distribution networks; load forecasting; deep learning; machine learning; LSTM-RNN	EXTREME LEARNING-MACHINE; ENSEMBLE; PREDICTION	The study of South African distribution (Dx.) network's load forecasting using recent and state of the art AI (machine learning, deep learning and ensemble deep learning) techniques, is limited. The impact of weather parameters on load forecasting performance of AI techniques in forecasting South African large power users is not well understood. This paper proposes a novel distribution network load forecasting system. The paper further introduces deep learning and ensemble deep learning techniques in forecasting the power consumption of large South African power users. The paper introduces these techniques through an investigation of their performance against that off state of the art machine learning techniques, ANFIS and OP-ELM. The impact of temperature on the performance of these techniques is also investigated. This investigation was conducted on three case studies, with three different industrial large power consumer loads. LSTM-RNN proved to be a more efficient load forecasting technique for the proposed load forecasting system, achieving the lowest load forecasting error in all three case studies. Ensembles of LSTM were found to overall achieve lower errors than the individual techniques' models. This improvement was less than 1%. The inclusion of temperature was found to generally improve the load forecasting performance of ML and DL techniques' models.																	1064-1246	1875-8967					2019	37	6					8219	8235		10.3233/JIFS-190658													
J								Spherical fuzzy soft sets and its applications in decision-making problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soft sets; spherical fuzzy sets; spherical fuzzy soft sets; soft discernibility matrix; decision-making problem	AGGREGATION OPERATORS; MATRIX-THEORY; DISCERNIBILITY MATRIX; THEORETIC APPROACH	In this paper, the definition of spherical fuzzy soft sets and some of its properties are introduced. Spherical fuzzy soft sets are a generalization of soft sets. Notably, we showed DeMorgan's laws that are valid in spherical fuzzy soft set theory. Also, we propose an algorithm to solve the decision-making problem based on adjustable soft discernibility matrix. It gives an order relation between all the objects of our universe. Finally, an illustrative example is discussed to prove that they can be effectively used to solve problems with uncertainties.																	1064-1246	1875-8967					2019	37	6					8237	8250		10.3233/JIFS-190728													
J								Linear system of equations in m-polar fuzzy environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										m-PFN; m-PFLSEs; fully m-PFLSEs; minimal and maximal symmetric solution	DECISION-MAKING; SYMMETRIC-SOLUTIONS; SETS; MODELS	In this paper, we present the notion of m-polar fuzzy number(m-PFN) along with some properties. Further, we describe the m-polar fuzzy linear system of equations (m-PFLSEs) along with weak and strong solutions. Moreover, we characterize a new technique to find the solution of fully m-PFLSE using one-cut extension. By applying this technique, we calculate the minimal and maximal symmetric solutions (SSs) of the fully m-PFLSEs which based on a controllable solution set (CSS) and a tolerable solution set (TSS), respectively. We consider an example to find the solutions of fully m-PFLSEs. In the end, we prove some elementary results on the base of our proposed method.																	1064-1246	1875-8967					2019	37	6					8251	8266		10.3233/JIFS-190744													
J								Ellsberg urn problems with multiple degrees of freedom	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ellsberg urn; uncertainty theory; chance theory; chance measure		An Ellsberg urn is filled with n balls of m different colors and each ball is marked in only one color. However, the number of balls of each color is unknown, which should be regarded as uncertain variable. Moreover, since we randomly draw a ball from the urn, Ellsberg urn problems are a mixture of uncertainty and randomness. In order to deal with such problems, we can employ probability theory, uncertainty theory, and chance theory. Until now only the case m = 3 has been studied. This paper is aimed at studying more general cases where m > 3. It is concluded that the chance of drawing a ball of one color is equal to that of drawing a ball of another color.																	1064-1246	1875-8967					2019	37	6					8267	8273		10.3233/JIFS-190761													
J								Bipolar fuzzy system of linear equations with polynomial parametric form	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Polynomial PBFNs; center; distance; BFSLEs	RESOLUTION; NUMBER	The notions of center and distance between two Parametric Bipolar Fuzzy Numbers (PBFNs) by giving the preference of right dominance as compared to the left dominance are presented. The solution of Bipolar Fuzzy System of Linear Equations (BFSLEs) is discussed with polynomial parametric bipolar fuzzy number coefficients matrix having crisp real variables and the right-hand side is polynomial parametric bipolar fuzzy numbers. Some of their related properties are investigated. It is proved that if the real coefficient matrix considered as crisp in an original system, while the unknown variable vectors and Right Hand Side (RHS) column vector functions are treated as PBFNs, then initially, the general BFSLEs in polynomial parametric form is solved by the addition and subtraction of the vectors of the lower and upper bound, respectively. The solution procedure is computationally efficient in a bipolar fuzzy environment, and our proposed method is simple as well as efficient to handle the BFSLEs.																	1064-1246	1875-8967					2019	37	6					8275	8287		10.3233/JIFS-190764													
J								Soft intersection hyperstructures: Application in Krasner hyperrings	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										left(right) soft int-hypergroup; soft int- polygroup; left(right) soft int-additive hyperring; soft int-hyperideal	SETS; RINGS	In this paper first, we define the notions of left and right soft int-hypergroups and derive some of their basic properties. Second, we study these concepts in the context of complete hypergroups and polygroups. Then, we introduce the notions of left and right soft int-additive hyperrings and soft int-hyperideal. In special case we study these notions for the class of Krasner hyperrings. Finally, a characterization of soft int-hyperideal for the class of Krasner hyperfields is investigated.																	1064-1246	1875-8967					2019	37	6					8289	8297		10.3233/JIFS-190791													
J								Hybrid Kano-fuzzy-DEMATEL model based risk factor evaluation and ranking of cross-border e-commerce SMEs with customer requirement	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cross-border e-commerce; fuzzy set theory; DEMATEL; Kano model; uncertain information evaluation; risk analysis	OPTIMIZATION MODEL; LOGISTICS PROBLEM; TRUST; SATISFACTION; SERVICE; SETS	The advance of "One Belt One Road" initiative and "Internet+" strategy have greatly promoted the development of cross-border e-commerce in the past several years. To gain global competition, the small-and-medium-sized enterprises(SMEs) have focused on the revenues and risk. However, most previous studies have focused on consumer perceived risk, while neglecting the importance of sellers' risk. It is the aim of this article to address the following question: With growing customer requirement in global marketing competition, customer requirement plays an important role in the successful design of SMEs risk mitigation solution as SMEs's performance is highly determined by customer satisfaction in the customer-oriented market. How to identify the most important risk factor of cross-border e-commerce SMEs based on influence of consumer requirements? Therefore, to balance the mutual interests in cross-border e-commerce online transactions. In order to achieve these objectives. We analyse cross-border e-commerce SMEs' risk factor and develop a hybrid method for risk evaluation and ranking, named Kano-fuzzy-DEMATEL. This new method offers a more accurate way to calculate the degree of relation among each risk factor considering the influence of consumer requirements and deal with uncertain information on risk evaluation, as to determine the risk priority for cross-border e-commerce SMEs. The ranking of risk factors can provide basis for decision-making and improve the accuracy of prediction. We supplement existing e-commerce research to assess the overall risk factors from the seller's perspective based on the customer requirement. The study can provide comprehensive insights to mitigate risk.																	1064-1246	1875-8967					2019	37	6					8299	8315		10.3233/JIFS-190830													
J								Consistency analysis and priority weights of multiplicative trapezoidal fuzzy preference relations based on multiplicative consistency and logarithmic least square model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Group decision making; multiplicative trapezoidal fuzzy preference relations; multiplicative consistency; logarithmic least square priority model	GROUP DECISION-MAKING; DERIVING INTERVAL WEIGHTS; GOAL PROGRAMMING METHOD; ADDITIVE CONSISTENCY; CONSENSUS; AHP; COMPATIBILITY; TRANSITIVITY; SELECTION; FUSION	Consistency and the priority vector are two important issues in preference relations. As one of preference relations, multiplicative trapezoidal fuzzy preference relation (MTFPR) is an effective form of vague and imprecise information when decision maker (DM) express his/her opinions by comparing alternatives or criteria with each other in group decision making (GDM). Therefore, it is meaningful to discuss the consistency and the method for deriving priority vector of MTFPRs. In this paper, we define multiplicative consistency of MTFPRs and investigate the necessary and sufficient conditions of multiplicative consistent MTFPRs. Some properties of multiplicative consistent MTFPRs are studied in detail. Based on the necessary and sufficient conditions of multiplicative consistent MTFPRs, two consistent measurement matrices (CMMs) are developed to define an acceptably multiplicative consistent MTFPR. A logarithmic least square model is further constructed for deriving a normalized trapezoidal fuzzy priority vector from a MTFPR. Three numerical examples including a GDM problem are analyzed to demonstrate the validity of the proposed models.																	1064-1246	1875-8967					2019	37	6					8317	8334		10.3233/JIFS-190846													
J								Some existence results for a new class of elliptic Kirchhoff equation with logarithmic source terms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Kirchhoff; existence; sub-supersolution; galerkin approach; elliptic equation	POSITIVE SOLUTIONS; (P X	In this paper, some new existence results of elliptic equation of Kirchhoff-type with changing sign data and logarithmic source terms are proved, by using three different methods: direct variational method, Galerkin approach and sub-super solutions method. Our study is natural extensions from the previous recent works in [2-19, 24, 38], where the authors have already studied the existence of positive solutions for some classes of Laplacian elliptic problems by using one classical method for a certain class of elliptic equations.																	1064-1246	1875-8967					2019	37	6					8335	8344		10.3233/JIFS-190885													
J								Fault tree analysis of a hydraulic system based on the interval model using latin hypercube sampling	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hydraulic system; interval model; fault tree analysis; probability interval; latin hypercube sampling	RELIABILITY-ANALYSIS; PROBABILITY; FTA	There will inevitably be failures during the use of hydraulic systems in armored vehicles because of the detrimental environments in which they operate. In order to improve the reliability of such hydraulic systems, a fault tree model of top event 'hydraulic system failure' is established and analyzed in this study according to the system arrangement and potential fault mechanisms. To properly consider the uncertain probability of each basic failure event in the system, it is necessary to overcome the limitations of the traditional fault tree model. Accordingly, in this study, the importance of the basic event probability interval in describing the failure probability of the top event (hydraulic system failure) was calculated using Latin hypercube sampling through interval modeling. This method offers significant benefits for the reliability assessment of hydraulic systems and can be used to provide guidance for improving system reliability.																	1064-1246	1875-8967					2019	37	6					8345	8355		10.3233/JIFS-190891													
J								An evidential reasoning based approach for GDM with uncertain preference ordinals	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertain preference ordinals; evidential reasoning; group decision making; alternatives ranking	GROUP-DECISION-MAKING; AGGREGATION; CONSENSUS; ALGORITHM; OPERATORS; RULE; OPTIMIZATION; WEIGHTS; CONTEXT	The group-ranking problem is ubiquitous in the real life, which is a group decision-making (GDM) problem and cannot be completed independently by individuals. However, different experts or decision-makers (DMs) may provide their preference information on alternatives in the form of uncertain preference ordinals. This paper developed an evidential reasoning (ER) based method to deal with various types of preference information on alternatives such as precise and imprecise, complete and incomplete, and known and unknown, which may provide by experts or DMs in the process of alternatives ranking. The proposed method allows experts or DMs to express their preferences independently and freely using belief structure, and provides a means based on evidence distance to determine the relative weights of belief structures, which is not need to solve the complex optimization model. Furthermore, the interval ER algorithm is employed to aggregate different types of preference information with a rigorous and systematic framework. The feasibility and rationality of the method are explained and verified by examples.																	1064-1246	1875-8967					2019	37	6					8357	8369		10.3233/JIFS-190915													
J								Disruption management strategy of terminal logistics under accidental travel time delay based on prospect theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Travel time delay; terminal logistics; disruption management; psychological perception; prospect theory	DECISION	Focus on the distribution vehicle scheduling problem under travel time delay, a series of rescue modes generalized from practical vehicle planner' experiences were presented. The measurement method of value function based on prospect theory and the disruption management strategy of customer's psychological perception are put forward in this paper. A multi-objective optimization model for disruption management of logistics distribution is established through multi-objective programming at first. Next, the idea of gradual optimization for the target to obtain the terminal logistics distribution adjustment scheme with minimum disturbance is introduced. Acting the customer's psychological perception time as the reference point, the degree of the customer's psychological perception of the expected time of goods is measured with the prospect theory. In order to optimize the disruption management model and search the optimum solution of multi-objective optimization problem, an improved quantum bacterial foraging algorithm is proposed. The effectiveness and practicability of the proposed method is verified through the comparison and analysis with rescheduling method.																	1064-1246	1875-8967					2019	37	6					8371	8379		10.3233/JIFS-190916													
J								An extended HFACS based risk analysis approach for human error accident with interval type-2 fuzzy sets and prospect theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										HFACS; interval type-2 fuzzy sets; prospect theory; human error accident	CLASSIFICATION-SYSTEM HFACS; DECISION-MAKING METHOD; MODEL; SAFETY; RELIABILITY; MARINE; TODIM; OIL; AGGREGATION; EVALUATE	The traditional Human Factor Analysis and Classification System (HFACS) model has been regarded as one of the most widely used and effective human error accident analysis approaches. However, current HFACS models are insufficient to address accident analysis problem with the high uncertain risk information and inconsistent behavioral preferences. The aim of this paper is to develop an extended HFACS based risk analysis method for human error accident based on interval type-2 fuzzy sets and prospect theory. Firstly, the interval type-2 fuzzy sets are used to express the uncertain evaluation information in the risk analysis process. Secondly, prospect theory is utilized to depict the different risk preferences of experts under uncertain environment. Next, the ordered weighted averaging (OWA) operator for interval type-2 fuzzy number is combined with prospect theory to calculate risk priorities of risk factors. Specially, a ranking method based on possibility mean and variation coefficient is proposed to compare interval type-2 fuzzy numbers. Finally, an illustrative example in marine industry is selected to demonstrate the application of the extended HFACS based risk analysis method. The comparison analysis indicates that the proposed model can achieve relatively reasonable and objective risk evaluation results.																	1064-1246	1875-8967					2019	37	6					8381	8395		10.3233/JIFS-190929													
J								Fuzzy linguistic descriptions for execution trace comprehension and their application in an introductory course in artificial intelligence	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Computational intelligence; linguistic descriptions of data; linguistic modelling of complex phenomena; computer game bots; turing test; computer-assisted assessment	NATURAL-LANGUAGE GENERATION; FEEDBACK; SUMMARIES; TOPOLOGY	Execution traces comprehension is an important topic in computer science since it allows software engineers to get a better understanding of the system behavior. However, traces are usually very large and hence they are difficult to interpret. Parallel, execution traces comprehension is a very important topic into the algorithms learning courses since it allows students to get a better understanding of the algorithm behavior. Therefore, there is a need to investigate ways to help students (and teachers) find and understand important information conveyed in a trace despite the trace being massive. In this paper, we propose a new approximation for execution traces comprehension based on fuzzy linguistic descriptions. A new methodology and a data-driven architecture based on linguistic modelling of complex phenomenon are presented and explained. In particular, they are applied to automatically generate linguistic reports from execution traces generated during the execution of algorithm implemented by the students of an introductory course of artificial intelligence. To the best of our knowledge, it is the first time that linguistic modelling of complex phenomenon is applied to execution traces comprehension. Throughout the article, it is shown how this kind of technology can be employed as a useful computer-assisted assessment tool that provides students and teachers with technical, immediate and personalised feedback about the algorithms that are being studied and implemented. At the same time, they provide us with two useful applications: they are an indispensable pedagogical resource for improving comprehension of execution traces, and they play an important role in the process of measuring and evaluating the "believability" of the agents implemented. To show and explore the possibilities of this new technology, a web platform has been designed and implemented by one of the authors, and it has been incorporated into the process of assessment of an introductory artificial intelligence course. Finally, an empirical evaluation to confirm our hypothesis was performed and a survey directed to the students was carried out to measure the quality of the learning-teaching process by using this methodology enriched with fuzzy linguistic descriptions.																	1064-1246	1875-8967					2019	37	6					8397	8415		10.3233/JIFS-190935													
J								A multi-period regret minimization model for uncertain portfolio selection with bankruptcy constraint	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertain portfolio selection; Regret factor; Multi-objective optimization; Bankruptcy risk; Firefly algorithm	MULTIOBJECTIVE FIREFLY ALGORITHM; RISK INDEX MODEL; OPTIMIZATION PROBLEM; RETURNS	In the complex financial market, there are situations where the security returns have to be evaluated by experienced experts due to the lack of historical data. In this paper, within the framework of uncertainty theory, we propose a multi-period bi-objective regret minimization model for portfolio selection, in which bankruptcy risk and liquidity risk are both considered. Furthermore, in order to solve the proposed multi-objective optimization problem, a novel hybrid algorithm named MFA-SCA is proposed by combining the advantages of the firefly algorithm (FA) and sine cosine algorithm (SCA). Finally, a numerical example is given to illustrate the effectiveness of the proposed approaches.																	1064-1246	1875-8967					2019	37	6					8417	8439		10.3233/JIFS-190936													
J								Pythagorean m-polar fuzzy sets and TOPSIS method for the selection of advertisement mode	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean m-polar fuzzy set; algebraic operations on PmFSs; alpha- & (alpha, beta)-cut; decision making; TOPSIS	DECISION-MAKING; AGGREGATION OPERATORS; MEMBERSHIP GRADES; SOFT TOPOLOGY; OPERATIONS	The prevailing structures like intuitionistic fuzzy sets, Pythagorean fuzzy sets and m-polar fuzzy sets etc. have their own deficiencies and limitations. There are many real life situations where multi-polar information is available and these structures fail to work due to their limitations e.g. in medical diagnosis sometimes some tests are repeated time and again to get multiple readings about symptoms to get a better diagnosis of a diseases. The motivation behind this article is to overcome these deficiencies by introducing a novel sort of set entitled Pythagorean m-polar fuzzy set (PmFS) as hybrid structure of Pythagorean fuzzy set and m-polar fuzzy set, m being some cardinal number. For m = 1, this set dwindles to Pythagorean fuzzy set and becomes Pythagorean bipolar fuzzy set for m = 2. We take the advantage to present a number of algebraic operations and some characteristics of PmFSs. We define some linguistic terms using the notion of product of PmFSs (circle times) by assigning different numeric values to the constant k is an element of [0, infinity [ and present an illustration to determine the values of membership and non-membership functions of PmFSs for lower, middle and upper class regarding economic position. We render an application of PmFSs in decision making problem (DMP) of selection of most appropriate mode of advertisement using the well-known tool TOPSIS.																	1064-1246	1875-8967					2019	37	6					8441	8458		10.3233/JIFS-191087													
J								Innovative capacity-based approach to blue ocean strategies of family firms: An IT2 fuzzy hybrid decision-making analysis for potential investors	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval type-2 fuzzy sets; innovation; blue ocean strategy; family firms; investment; decision making	SOCIOEMOTIONAL WEALTH; KNOWLEDGE; AGENCY	The study aims to measure the weights of innovative capacity and rank the blue ocean strategies for the family firms under the fuzzy environment. A hybrid decision making approach is proposed by considering IT2 (interval type 2) fuzzy DEMATEL (decision making trial and evaluation laboratory) for weighting the criteria and IT2 fuzzy VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje) to rank different alternatives. In this study, family firms, their innovative tendencies and one of the most challenging marketing strategies they can use in combatting with fierce competition, namely, blue ocean strategy have been discussed. Through blue ocean strategies family firms can create undiscovered markets, creates their own demand and give an end to the value/cost trade-off. This strategy is envisaged to make the company attractive for potential investors after the point they start to grow and decide to expand their partnership structure to external players.																	1064-1246	1875-8967					2019	37	6					8459	8470		10.3233/JIFS-191091													
J								Dynamic multi-objective optimization for multi-period emergency logistics network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Emergency logistics network; multi objective optimization; dynamic optimization; evolutionary algorithm; self-adaption	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM	In order to solve the problem of multi-period material supply in emergency logistics network, a dynamic multi-objective optimization mathematical model with constraints is constructed. The model takes the minimum cost and the maximum fill rate of demands as the objectives, and takes the location of distribution centers and the allocation of material as the decision variables. A dynamic self-adaptive multi-objective differential evolution algorithm is proposed to solve the mathematical model, and the feasible non-dominated solutions of the model are obtained. In the improved algorithm, on the one hand, a new environment change detect operator and a new environment change response strategy are adopted so that the traditional static optimization algorithm can be used to solve the dynamic optimization problem. On the other hand, the improved algorithm adopts adaptive mutation strategy to improve the ability of global exploration and local exploitation. Case study shows that the improved strategy greatly improves the performance of the algorithm, and can solve the dynamic multi-objective optimization problem effectively.																	1064-1246	1875-8967					2019	37	6					8471	8481		10.3233/JIFS-191130													
J								Application of fuzzy dematel method to analyse s-CO2 Brayton power systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Super critic; carbon dioxide; Brayton; fuzzy DEMATEL; faults	TURBINE BLADE FAILURE; MANAGEMENT; EVALUATE; ANP; ENERGY; TOPSIS; SELECTION; RISK; AHP	Supercritical carbon dioxide (s-CO2) Brayton power cycle has increasingly attracted attention due to having greater efficiency than conventional power cycles. Thus, s-CO2 systems have begun being tested all around the world, first at the laboratory scale, then as actual medium-capacity systems for ships' main engines, and finally as large terrestrial power systems. In order to understand system performance during these tests, one must know the causes and effects of the operating regime of the system and its failures. In this context, the study analyzes 15 fundamental problems of power system-related components using the fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL) method. The DEMATEL method allows for identifying and analyzing important errors and/or problems in the s-CO2 Brayton cycle according to the cause-and-effect relationship scheme Similarly, fuzzy sets are freed from uncertainty in decision-making and from the verbal comments of experts in DEMATEL. When examining the results, fire protection and/or firefighting problems, generator problems, gearbox problems, and radiator problems appear to have high importance in terms of causes. In addition, turbine problems, electrical problems, catalytic combustion-chamber problems, instrumentation, and control-system problems are also important in terms of effects. The study's obtained results will strongly contribute to the operational safety and prevention of serious high-speed, high-temperature, and high-pressure machinery effects for laboratory-scale, medium-capacity, and actual-terrestrial s-CO2 power systems.																	1064-1246	1875-8967					2019	37	6					8483	8498		10.3233/JIFS-191133													
J								Joint category-level and discriminative feature learning networks for unsupervised domain adaptation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Domain adaptation; deep learning; discriminative feature learning; transfer learning	REGULARIZATION; KERNEL	Unsupervised domain adaptation (UDA) aims to build a classifier for the unlabeled target domain by transferring knowledge from a well-labeled source domain. Recently deep domain adaptation methods can not effectively integrate discriminability with transferability of features, and these methods can only reduce, but not remove, the cross-domain discrepancy. To this end, this paper proposes a new domain adaptation method called Joint Category-Level and Discriminative Feature Learning Network (CDN). CDN not only achieves domain adaptation by minimizing category-level distribution discrepancy between domains but also learns discriminative feature representations via maximizing inter-category distance and selecting transferability samples simultaneously. Moreover, we develop a Transferability Weighting Module (TWM), which is based on a constructed classifier, to further strengthen the discriminability of sample's features. The experimental results demonstrate that CDN can significantly decrease the cross-domain distribution inconsistency and further promote the classification performance.																	1064-1246	1875-8967					2019	37	6					8499	8510		10.3233/JIFS-191136													
J								Fuzzy based decision system for estimation of operator's situation awareness index while surveillance during low ambient lighting conditions	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy decision system; situation awareness index; assistive technology; surveillance; image fusion	IMAGE; FUSION	Surveillance during low ambient lighting conditions is a challenging task due to hampered perception of the scene while using only visible range cameras. As a solution to this, surveillance using fusion of visible and infrared image has been proposed to provide comprehensive information of the scene instantaneously. Fusion of multispectral sensor information leads to enhancement of information for the operator. However, enhancement of information presented to user not always lead to enhancement of situation awareness (SA) level. This study focuses on SA estimation of operator during ground surveillance tasks while used visible-infrared fused imagery. The original reference image and fused image outputs are compared statistically using different image quality parameters like target to background entropy, correlation and structural similarity index. A fuzzy based decision model is developed to give a quantitative and qualitative estimate of operator's SA during surveillance carried out in low ambient lighting conditions. The fuzzy based decision model gives a qualitative estimate of SA as: poor, low, moderate & high, and, quantitative index in range of 1-10 indicating SA level and thus provides alert about the level of details perceivable from the fused image, thereby, helping operator to take decision based on SA index instantaneously.																	1064-1246	1875-8967					2019	37	6					8511	8521		10.3233/JIFS-172095													
J								Lattice-valued betweenness relations and its induced lattice-valued convex structures	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lattice-valued betweenness space; L-fuzzifying convex structure; lattice-valued geometric interval space; L-fuzzifying join space	SPACES; JHC	In this paper, we mainly introduce the concept of lattice-valued betweenness relations and study its relationships with L-fuzzifying convex structures in depth. Such an extended betweenness relation is defined to be a map from X x X x X to a complete lattice provided that fulfills a set of axioms. We also discuss the relationships among them and other algebraic and geometric structures. Moreover, a category of approach is provided to present lattice-valued betweenness relations. It is shown that the category of lattice-valued betweenness spaces and the category of lattice-valued geometric interval spaces are isomorphic.																	1064-1246	1875-8967					2019	37	6					8523	8533		10.3233/JIFS-181395													
J								Takagi-sugeno-kang fuzzy systems with dynamic rule weights	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										TSK fuzzy systems; dynamic rule weights; stacked structure; outlier detection	CLASSIFICATION; NETWORK	Fuzzy rules are very important in Takagi-Sugeno-Kang (TSK) fuzzy systems as they not only provide a mapping mechanism for input patterns but also make fuzzy systems interpretable. Current works further introduce rule weights to restrict/strengthen fuzzy rules for more situations. However, most of the embedded rule weights in fuzzy rules are static. In other words, the rule weights keep unchanged once they have been determined by the learning algorithms. In practical applications, it is often expected that each fuzzy rule should deduce different confidence degrees with respect to different input patterns. In this paper, a new TSK fuzzy system is proposed, in which each fuzzy rule is empowered by an individual dynamic rule weight (DRW). DRW is basically a nonlinear function of the input pattern to reflect the confidence degree (acceptability) the fuzzy rule acting on the input pattern. Furthermore, for an input pattern, its "isolation" level can be measured by the aggregated DRW values of the fuzzy rules in the proposed fuzzy system. Specifically, the proposed fuzzy system can be used to identify outliers whose aggregated DRW values of all fuzzy rules are very small. In order to effectively embed DRW to each fuzzy rule, an analogous stacked structure consisting of a basic input-output unit and an augmenting unit is proposed. The stacked architecture is characterized by three features: (i) the augmented information from the augmenting unit can provide indirect pattern information for DRW learning; (ii) the predictive information from the augmenting unit can be differentiated from the interpretability of fuzzy rules; and (iii) the modeling performance can be improved by the stacked generalization principle which leverages the predictive information in the manifold of the input pattern space in the system approximation process. Experimental results on 16 real-life datasets demonstrate the approximation accuracy, interpretability and outlier detection ability of the proposed fuzzy system.																	1064-1246	1875-8967					2019	37	6					8535	8550		10.3233/JIFS-182561													
J								An extended generalized TODIM method for risk assessment of supply chain in social commerce under interval type-2 fuzzy environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Supply chain of social e-commerce; risk assessment; generalized TODIM; interval type-2 fuzzy set	DECISION-MAKING METHOD; MANAGEMENT; AHP; SELECTION; MEDIA; MODEL	Different from the traditional supply chain, there are more uncertainty in the supply chain of social commerce (SCOSC), which brings new risk. As a fundamental stage in the supply chain risk management (SCRM), risk assessment can effectively control risks in supply chain. To address risk preference of decision makers (DMs) and the criteria ambiguity, this paper proposes an extended generalized TODIM (an acronym in Portuguese of interactive and multi-criteria decision making) method within interval type-2 fuzzy sets (IT2FSs) to evaluate the risk of the SCOSC. First, the generalized TODIM is extended with two different value functions based on two types of criteria, cost and benefit, and the alternatives are ranked according to the distance to the ideal solution and the negative-ideal solution. Second, the IT2FSs is applied to deal with the linguistics ambiguity with the generalized TODIM method. Furthermore, the risk assessment of SCOSC based on the extended generalized TODIM within IT2FSs is proposed. Finally, a case of the risk assessment in SCOSC is presented to validate the effectiveness of the proposed method.																	1064-1246	1875-8967					2019	37	6					8551	8565		10.3233/JIFS-190061													
J								Particle swarm optimization based multi-task parallel reinforcement learning algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-task reinforcement learning; parallel reinforcement learning; particle swarm optimization; transfer learning		Transfer learning has been identified as conducive to improving the speed of machine learning in many areas. In multi-task reinforcement learning, transfer learning can assist the transfer of experiences between different tasks. The research conducted in this article is focused on two aspects. On the one hand, multi-task parallel transfer learning can improve the learning speed of parallel learning tasks. On the other hand, the learning of the current optimal experience can help the target point rewards to be transmitted to the starting point. The value of this self-learning can also accelerate the convergence speed of the reinforcement learning. According to the research into these two aspects, this paper uses the idea of particle swarm optimization (PSO) to conduct self-learning and interactive learning in multi-task parallel learning. In this paper, a new multi-task learning algorithm named PSO-MTPRL (Multi-Task Parallel Reinforcement Learning based on PSO) is proposed. Based on the idea of PSO algorithm, the Boltzmann strategy, Self-Learning Process (SLP) and Interactive Learning Process (ILP) are selected probabilistically. Based on the characteristic exhibited by reinforcement learning, segmented learning model is recommended. In the early learning stages, the complete Boltzmann exploration strategy is applied, and B-SLP-ILP (Boltzmann-SLP- ILP) learning procedure is conducted exclusively in the middle stage of the learning. In the late learning stages, Boltzmann exploration is involved again. The segmented learning model can help ensure the balance of the exploration and exploitation, in addition to ensuring that all tasks convergence.																	1064-1246	1875-8967					2019	37	6					8567	8575		10.3233/JIFS-190209													
J								Airline safety assessment based on fuzzy mathematics and Bayesian networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bayesian network; fuzzy mathematics; fault tree analysis; safety assessment	ANALYTIC HIERARCHY PROCESS; COMPREHENSIVE EVALUATION; AHP	To reduce the airline accident risk probability, this paper proposes a new airline safety assessment method based on fuzzy mathematics and Bayesian networks (BN). Herein, we construct a safety assessment system encompassing five aspects-namely maintenance quality, aircraft technical state, environmental effects, emergency rescue, and safety management-and establish a BN model based on this safety assessment system. The fuzzy mathematical and statistical analyses are used to obtain the prior probabilities, and the data training function in GeNIe2.1 is used to obtain the conditional probabilities. Finally, we apply our method to an unspecified airline. The results indicate that the risk probability was 0.826 for the airline to have an excellent safety status in January 2018; this value was 0.886 according to fault tree analysis (FTA). In addition, by using vertical and horizontal analyses, we investigate the factors affecting airline safety. Thus, our BN-based method is more efficient than FTA because compared with FTA, the BN has the advantages, such as polymorphism and accuracy, particularly in detecting the most risky factors in a complex model.																	1064-1246	1875-8967					2019	37	6					8577	8587		10.3233/JIFS-190273													
J								Accurate characterization of weld appearance induced by T-joint laser stake-welding by integration of ANFIS approach and numerical simulation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Laser stake-welding; T-joints; ANFIS model; numerical simulation; weld appearance	PENETRATION; PREDICTION	The geometrical appearance of weld bead is critically important in terms of directly determining the quality and reliability of T-joints laser stake-welding process. In this regard, this paper puts forward an innovative hybrid modeling scheme integrating the adaptive neural fuzzy interface system (ANFIS) with three-dimensional numerical simulation to accurately characterize the weld bead appearance. First, an ANFIS-based model is developed to identify the weld characteristics by experimental observation and provide the key parameters of hybrid heat source involved in the weld numerical simulation. Second, the weld bead geometry, i.e., weld penetration depth, surface weld width and interface weld width are all computed utilizing the numerical simulation method. The proposed numerical model exhibits good agreements with the experimental results in regard to forecasting the weld characteristics. In the end, the role of various welding conditions on the formation mechanism and T-joints bead profiles of the laser stake-welding are elucidated through the simulation model. The simulated results would help provide a much better understanding of the critical factors which does affect the weld appearance, and lay a solid foundation for optimizing of welding parameters and obtaining a high-quality weld.																	1064-1246	1875-8967					2019	37	6					8589	8601		10.3233/JIFS-190542													
J								Progress of uncertain and fuzzy methods in group decision making: A graphical overview	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Group decision making (GDM); bibliometric method; citespace; citation burst	LINGUISTIC TERM SETS; CONSENSUS MODEL; PREFERENCE RELATIONS; EMERGING TRENDS; FUSION PROCESS; ASSESSMENTS; CONSISTENCY; LAW	Most recently, uncertain and fuzzy factors appear frequently in group decision making (GDM) researches. A more realistic approach may be to use scientometric tools for carrying out the systematic review to disclose the fast-growing uncertain and fuzzy GDM methods. The aim of this paper is to show a comprehensive presentation of the state of these approaches. To achieve this goal, a systematic literature review of researching articles, which have been published in Science Citation Index (SCI) journals since 1965, is proposed. And this study mainly uses the software of CiteSpace which has been frequently used in data visualization field due to its universality in order to map the main trends in this subject. This work considers the leading categories, journals, authors and references, and the results indicate that the GDM research would continue to flourish. Additionally, some possible approaches that could improve the current fuzzy GDM methods are presented.																	1064-1246	1875-8967					2019	37	6					8603	8612		10.3233/JIFS-190631													
J								Learning process: Multi-Agent Tutoring System	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Multiagent systems; education; tutorship; scheduling		A multi-agent architecture has been developed for tutorial assignation scheduling. It has two main types of agents: the students and the teachers. These two are coordinated by an algorithm which assigns the classes in order of arrival. The architecture will provide the necessary tools to the students, so they get the maximum profit from the tutorials. Students and Lecturers can coordinate their tutorial meeting in an efficient way with the help of the multi-agent system.																	2255-2863						2019	8	1					5	12		10.14201/ADCAIJ201981512													
J								Algorithm Analysis in Multi-agent Systems	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Multi-agent systems; optimization		This paper presents a multi-agent system that looks for the most optimum algorithm for its type. For that purpose it will use several agents which will he in charge of testing the algorithms and comparing the outcome to see which is the most efficient. Thanks to this procedure the most optimum procedure can he obtained.																	2255-2863						2019	8	1					13	18		10.14201/ADCAIJ2019811318													
J								CTRANSPORT: Multi-agent-based simulation	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Electric vehicles; Multi-agent Systems; Eco-friendly		Pollution nowadays is a really important issue that must be solved. Big cities suffer from overcrowding which result in traffic congestion and a lot of air pollution. Adapting to the idea of cities bike lane expansion, we design a Multi-agent simulation to distribute among the users green energy vehicles; concretely bikes, scooters and electric cars.																	2255-2863						2019	8	1					19	26		10.14201/ADCAIJ2019811926													
J								Multi-Agent Vehicle Share System	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Multi-Agent; JADE; Vehicle share system; ecological		A multi-agent system is proposed that simulates a network of vehicle rental stations in a city The paper studies the relationship between the agents and the client, analyses the casuistry associated with possible problems that may he encountered in the absence of transport in a given slop, as well as the decisions that could he taken by the interested party.Subsequently an architecture capable of being scalable in terms of functionalities and the number of agents involved in it will be proposed. The aim of this paper is to revise the original paper, which is more focused on the possibility of studying a particular city, raising and solving the problems associated with public vehicle sharing services.																	2255-2863						2019	8	1					27	35		10.14201/ADCAIJ2019812736													
J								Virtual agent organizations to optimize energy consumption in households	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Energy; Consumption; Global Warning; Saving	MULTIAGENT SYSTEM; MANAGEMENT	Global warming affects us all, that is why we must all act to stop it. It has been show; that this undoubted problem can be solved to a large extend if we snake small individual efforts. How can we do this? Making prudent use of electricity. If we manage to make more efficient use of the energy we consume in our homes, we will contribute enormously in this common cause. With the help of virtual agents, we will get a better management of the energy we consume.																	2255-2863						2019	8	1					37	47		10.14201/ADCAIJ2019813748													
J								Multi-agent system for selecting images based on the gender and age	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Multi-Agent; Facial Recognition; Artificial Vision; Biometrics	RECOGNITION SYSTEM; FACE RECOGNITION	This paper presents a multi-agent system that is able to search people on a database of images recognizing patterns of facial features on each person, based on the main features of the face (eyes, nose and mouth). Using that multi-agent architecture, the system can do the work faster applying Fisher faces algorithm for the face recognition and classification. This technology can be used for several purposes like specific ads in each user group to suit better their interests or search for the age and gender of people that usually go to different places like malls or shops.																	2255-2863						2019	8	1					49	54		10.14201/ADCAIJ2019814954													
J								Genetic fuzzy rule-based system using MOGUL learning methodology for energy consumption forecasting	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Electricity consumption; Forecasting; Fuzzy rule based methods; MOGUL		This paper presents the application of a Methodology to Obtain Genetic fuzzy rule-based-systems Under the iterative rule Learning approach (MOGUL) to forecast energy consumption. Historical data referring to the energy consumption gathered from three groups, namely lights, HVAC and electrical socket, are used to train the proposed approach and achieve forecasting results for the future. The performance of the proposed method is compared to that of previous approaches, namely Hybrid Neural Fuzzy Interface System (HyFIS) and Wang and Mendel's Fuzzy Rule Learning Method (WM). Results show that the proposed methodology achieved smaller fore-casting errors for the following hours, with a smaller standard deviation. Thus, the proposed approach is able to achieve more reliable results than the other state of the art methodologies.																	2255-2863						2019	8	1					55	64		10.14201/ADCAIJ2019815564													
J								SWARM OPTIMIZATION ALGORITHM BASED ON THE ANT COLONY LIFE CYCLE	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Biologically inspired algorithm; Ant colony life cycle; Swarm intelligence; Optimization algorithm		Optimization is very important to the success of any business. One technique for solving optimization is swarm intelligence; it has been successfully applied to solve a wide range of optimization problems. We devised a new swarm intelligence optimization algorithm based on the cooperative behavior of three different kinds of ants in a colony. Our algorithm consists of both exploration and exploitation processes to achieve better search performance. A new local search, inspired by the foraging of desert ants, was introduced to help the search move away from the local optima. Performance was evaluated on 23 standard benchmark functions of varying complexity. Our algorithm was able to find the global optima in more than 80 percent of the test functions, whereas the second-place algorithm only found around 10 percent of the functions tested.																	0127-9084						2019					2		1	14		10.22452/mjcs.sp2019no2.1													
J								STARVATION DELAYED DHCP SERVICE FOR ENABLING POOL RECOVERY	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										DHCP; DHCP starvation attack; DHCP discovering; Network security		Dynamic Host Configuration Protocol (DHCP) Internet Protocol (IP) address starvation is a method, used by attackers, to breakdown communication over IP network. In order to solve this problem, a method to detect and recover malicious IP address request by using Internet Control Message Protocol (ICMP) protocol has been proposed. However, the ICMP based was not be able to work faster in detecting and recovering the malicious request than the attack rate. This study proposed an ease and effective authentication method to emphasize on limiting the rate of IP addresses request by malicious client during the DHCP discovering process and prevent the DHCP server from being IP address starved. Experimental results revealed that the proposed method was not only limited to the IP addresses requested time by attackers but also able to prevent the DHCP server from facing the IP address starvation attack.																	0127-9084						2019					2		15	34		10.22452/mjcs.sp2019no2.2													
J								THE NEW METHODOLOGY FOR VEHICULAR NETWORK WITH FUZZY TIME WINDOWS	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										vehicle routing problem; fuzzy membership function; route construction	VEHICLE-ROUTING PROBLEM; ARTIFICIAL BEE COLONY; EVOLUTIONARY ALGORITHM; METAHEURISTICS; SEARCH	This work proposes the new methodology for the vehicular network with fuzzy time windows. The Fuzzy technique is applied to produce an initial population and then the evolutionary algorithm is employed to improve the solution. In this work, the inter-route crossover, infra-route mutation, elitism strategy, and onlooker bee probability selection method were enhanced in the original processes of the evolutionary algorithm. The proposed algorithm is tested on 56 datasets of Solomon. The results from the proposed algorithm are shown in comparison with other algorithms in the literature. The findings from the computational results are very inspiring, it shows that the algorithm is very competitive. Comparing with the algorithm in the literature, the proposed algorithm obtains the best solution in terms of the coefficient of variation values for almost 40 instances from the 56 problem instances. In addition, the information regarding the p-value was resolved by the Wilcoxon signed-rank test for the considered testing instances that display statistically superior performance at the 95% significance level (alpha = 0.05) on comparing algorithms.																	0127-9084						2019					2		35	53		10.22452/mjcs.sp2019no2.3													
J								PIXEL-BASED FOREGROUND DETECTION IN REPETITIVE TIME-SERIES REGION	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										subtraction techniques; change detection algorithms; periodic structures; pattern matching; time series analysis	BACKGROUND SUBTRACTION	Currently, many state-of-the-art background subtraction techniques cannot deal properly with the area of periodic changing background, while some continue classifying them as foreground at intervals, others simply mask that area as a non-region of interest. To cope with this issue, a novel method of detecting repetitive temporal patterns based on the image sequences was proposed in this paper. The main emphasis of the proposed approach is on classifying those pixels as a background and identifying foreground objects in their relevant areas. As for the foreground detection, a model of time series pattern found in each pixel is individually built first; and then, any changes beyond the allowance of model periodicity are then determined as foreground objects. The proposed method could be used and run in parallel with any state-of-the-art background subtraction technique, allowing more accurate foreground-background segmentation. Experimental results showed that using Y channel, the proposed method of detecting time-series background area could achieve 92.9% of recall rate with less than 1% false positives. The recall of foreground detection in an area of repetitive time-series pattern was about 87%; while F-measure was about 0.73 on average. The false positives of foreground detection were also less than 1%. Accordingly, the proposed time-delay detection technique could significantly help to suppress the foreground error on time series background area, especially during the change from one sub pattern to another which causes a camera sensor to capture both sub-pattern values in one frame. Performance comparison with state-of-the-art methods showed that our proposed method was able to reduce 80% of the average false alarm and improve F-measure to 28% while the computational efficiency was reduced by only 1%.																	0127-9084						2019					2		54	78		10.22452/mjcs.sp2019no2.4													
J								SELECTION OF A MINIMAL NUMBER OF SIGNIFICANT PORCINE SNPs BY AN INFORMATION GAIN AND GENETIC ALGORITHM HYBRID MODEL	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Bioinformatics; Feature selection; Information gain; Genetic algorithm; Support vector machine; Swine; Single nucleotide polymorphisms	SUPPORT VECTOR MACHINE; CLASSIFICATION	A panel of a large number of common Single Nucleotide Polymorphisms (SNPs) distributed across an entire porcine genome has been widely used to represent genetic variability of pigs. With the advent of SNP-array technology, a genome-wide genetic profile of a specimen can be easily observed. Among the large number of such variations, there exists a much smaller subset of the SNP panel that could equally be used to correctly identify the corresponding breed. This work presents a SNP selection heuristic that can still be used effectively in the breed classification. The features were selected by combining a filter method and a wrapper method-information gain method and genetic algorithm-plus a feature frequency selection step, while classification used a support vector machine. We were able to reduce the number of significant SNPs to 0.86 % of the total number of SNPs in a swine dataset with 94.80 % classification accuracy.																	0127-9084						2019					2		79	95		10.22452/mjcs.sp2019no2.5													
J								MINING SOCIAL MEDIA CROWD TRENDS FROM THAI TEXT POSTS AND COMMENTS	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Thai text; posts and comments; crowd trends; social media; content extraction; sentiment analysis		Text mining from social media stream has attracted wide interests from both businesses and academics. Very large numbers of self-posts from crowd sources contains hidden frends, which can be valuable to a business enterprise. Crowd trend mining methods, together with an easily understood visual presentation, are thus in great demand. We present an approach to mining crowd trends from Thai text posts. A Thai language preprocessing module was necessary to transform continuous text into series of words. Our method could then mine general unforeseen crowd trends by using an automatic context extraction technique, tf-idf score and an aggregated opinion score calculated from automatically classified sentiments for each post or comment. The best sentiment classifier was chosen based on extensive experiments on the same data source. These scores were combined into one unified term popularity which was visualized as a word cloud on a web application. A case study used a popular Thai discussion website - Pantip.com - and achieved three interwoven desired goals: (1) extraction of general and unforeseen crowd trends from a Thai discussion website, (2) assigning unified popularity scores to each candidate term and (3) presenting those terms to end users in an easily comprehended form.																	0127-9084						2019					2		96	108		10.22452/mjcs.sp2019no2.6													
J								EMOTION ANALYTICS WITH PROCESS MINING	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Process Mining; Smartphone; Fuzzy Miner; Dotted Chart Analysis; Emotion Intelligence; StudentLife project; ProM; Disco Fluxicon	PERFORMANCE; PLEASURE	This research builds on the intersection of 'process mining' and 'emotion analytics' in order to discover and investigate the emotional patterns of students during the StudentLife project based on data collected by smartphones through PAM application (i.e., Photographic Affection Meter). The main objective of the study is to analyze and predict the relationships (or continuity) amongst 16 common emotional indicators based on the Russel's Circumplex Affect Grid' and by means of Fuzzy Miner (supported by Disco Fluxicon) and Dotted Chart Analysis (supported by ProM) process mining tools and techniques. Accordingly, the current work is divided into two main parts. In the first part, a pre-processing (or data preparation/cleansing) approach via Python programming was done in order to change the format of the initially collected event logs from JSON to the appropriate format/structure. In the second part, the emotional datasets were analyzed using the above-mentioned techniques. To do this, new groups/clusters of contexts were categorized and pre-defined. The third part of the study deals with data interpretation and discussion of the obtained findings. The proposed/applied approach was capable of providing 'frequency-based" models/graphs of the students' behavior, both before and after the experiment, in terms of 5 categories: "Minimal, Minor, Moderate, Moderately Severe, and Severe" depression-related emotional trends ranging from Low Severity (NA) to High Severity (PA). This research provides groundwork for further and future studies.																	0127-9084						2019					2		109	131		10.22452/mjcs.sp2019no2.7													
J								A COMPACT OPTIMAL LEARNING MACHINE	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Extreme learning machine (ELM); Analytical incremental learning (AIL); Principal component analysis (PCA); Objective relation learning (ORL); Optimal hidden node determination; Multiple-output architecture; Multiple-output regression	REGRESSION; NETWORKS	Artificial neural networks (ANNs) have been developed and applied to a variety of problems, such as pattern recognition, clustering, function approximation, forecasting, optimization, etc. However, existing ANNs have a high computational cost, since their learning methods are mostly based on a parameter tuning approach. Extreme learning machine (ELM) is a state-of-the-art method that generally dramatically reduces the computational cost. An analysis of the ELM method reveals that there are unsolved key factors, including inefficient hidden node construction, redundant hidden nodes, and unstable results. Therefore, we describe a new learning machine based on analytical incremental learning (AIL) in conjunction with principal component analysis (PCA). This learning machine, PCA-AIL, inherited the advantages from the original one and solved the unsolved key factors of ELM, and also extended AIL capability to serve a multiple-output structure. PCA-AIL was implemented with a single-layer feed-forward neural network architecture, used an adaptive weight determination technique to achieve a compact optimal structure and also used objective relations to support multiple output regression tasks. PCA AIL has two steps: objective relation estimation and multiple optimal hidden node constructions. In the first step, PCA estimated the objective relations from multiple-output residual errors. In the second step, the multiple optimal nodes were obtained from objective relations and added to the model. PCA AIL was tested with 16 multiple-objective regression datasets. PCA AIL mostly outperformed other methods (ELM, EM-ELM, CP-ELM DP-ELM, PCA-ELM, EI-ELM) in terms of fast testing speed - 0.0017 second, a compact model - 19.9 nodes, an accurate performance - RMSE 0.11261, and a stable result - S.D. of RAISE 0.00911: reported in averaged.																	0127-9084						2019					2		132	156		10.22452/mjcs.sp2019no2.8													
J								A ROBUST-TEXTURE CONVOLUTIONAL NEURAL NETWORK	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										RT-CNN; convolutional neural network; robust texture; texture subband; texture classification		AlexNet was a breakthrough for the convolutional neural network (CNN) and showed the greatest successful modified CNN that works well with large-scale images. However, it was unsuccessful in texture classification tasks. To extend CNN's capability, this paper proposes a modified CNN architecture called a robust-texture convolutional neural network (RT-CNN) to serve both complex shape and texture classification tasks, especially in the following challenges: (i) the same class of images naturally contains various viewpoints, scales, uneven illuminations, etc.; (ii) similarly shaped objects with different textures of images are often assigned into different classes; and (iii) different shaped objects with similar textures of images are often assigned into the same class. The proposed scheme embeded a texture-embedded supplementary method, composed of texture compensation and supplement, into the CNN architecture. The texture compensation is constructed from texture subbands decomposed by 2D Littlewood-Paley empirical wavelet transform (2D Littlewood-Paley EWT). Then the texture supplement is constructed from texture subbands by using Gabor wavelet to extract multi-scale and multi-orientation texture features. Based on two challenging datasets, the experimental results show that RT-CNN outperforms all test baseline methods: AlexNet, T-CNN, and wavelet-CNN, in terms of recognition accuracy rate. On a typical dataset, the recognition accuracy rate of the proposed method is still better than those of T-CNN and wavelet-CNN, and is comparable to that of AlexNet.																	0127-9084						2019					2		157	171		10.22452/mjcs.sp2019no2.9													
