PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								iHBP-DeepPSSM: Identifying hormone binding proteins using PsePSSM based evolutionary features and deep learning approach	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Hormone binding proteins; Pseudo position-specific score matrix; Deep neural network; Sequential forward selection; Support vector machine	NEURAL-NETWORKS; WEB SERVER; IDENTIFICATION; PREDICTION; CLASSIFICATION; INFORMATION; DNA; PLASMA	Hormone binding proteins (HBPs) are soluble carrier proteins that can non-covalently and selectively interact with the human hormone. HBPs plays a significant role in human life, but its functions are still unclear. Conventional experimental approaches are deemed ineffective due to its high time processing time and cost. Due to the rapid increases in protein sequences, truly characterization of HBPs has become a challenging task for investigators. Measuring the effectiveness of HBPs on the human body, an accurate and reliable intelligent model is desirable for the identification of hormone-binding proteins. In this paper, high discriminative evolutionary features are extracted using a position-specific scoring matrix (PSSM) and Pseudo position-specific scoring matrix (PsePSSM). On the other hand, to reflect the intrinsic correlation and sequence order information, Series Correlation Pseudo Amino Acid Composition (SC-PseAAC) is also applied. Furthermore, to reduce the computational time and to eradicate irreverent and noisy features, the Sequential forward selection and Support Vector Machine (SFS-SVM) based ensemble approach is applied to select optimal features. Furthermore, various diverse nature of learning algorithms is utilized to select the best operational engine for our proposed model. After evaluating the empirical outcomes, deep neural network using optimal PsePSSM features obtained remarkable results. Whereas, our proposed iHBP-DeepPSSM achieved an accuracy of 94.41%, 92.31%, and 90.48% using the training dataset, and independent datasets (S2 and S3), respectively. It is observed that iHBP-DeepPSSM shows an outstanding improvement compared to literature methods. It is expected that the developed model may be played a useful role in research academia as well as proteomics and drug development. The source code and all datasets are publicly available at https://www.github.com/salman-khan-mrd/iHBP-DeepPSSM																	0169-7439	1873-3239				SEP 15	2020	204								104103	10.1016/j.chemolab.2020.104103													
J								Clustering of mixed datasets using deep learning algorithm	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Deep learning; Mixed data; Generative adversarial networks; Clustering loss		The performance of a clustering algorithm is highly dependent on the quality and quantity of the training dataset. Deep learning is one of the most popular and successful technique for clustering of datasets with high quality. Typically, most of the datasets contain mixed numeric and categorical data attributes. The clustering of such different types of data is a complex issue. Deep learning methods, the state-of-the-art classifiers, with better learning procedures and computational resources, can fill these gaps. To improve the robustness of clusters, we propose a Constraint-Based Deep Convolutional Generative Adversarial Network (CB-DCGANs) framework for generating simulated data to augment the training set to improve the performance of the clustering algorithm. We evaluated the performance of an end-to-end Deep Convolutional Neural Network (DCNN) in detecting the clusters from given datasets. The results from CB-DCGANs with DCNN yielded baseline accuracies of 0.8853 for heart disease dataset. In chemoinformatics datasets proposed algorithm yielded accuracies of 0.965 for kaggle dataset, 0.987 for factors dataset, 0.952 for kinase dataset. This study shows that using generative adversarial networks for clustering augmentation can significantly improve performance, especially in real-life applications.																	0169-7439	1873-3239				SEP 15	2020	204								104123	10.1016/j.chemolab.2020.104123													
J								New reduced kernel PCA for fault detection and diagnosis in cement rotary kiln	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Principal component analysis; Kernel PCA; Reduced KPCA; Redundancy; Euclidean distance; Fault detection; Cement rotary kiln	PRINCIPAL COMPONENT ANALYSIS; SELECTION; NUMBER; KPCA	Fault detection and diagnosis (FDD) based on data-driven techniques play a crucial role in industrial process monitoring. It intends to promptly detect and identify abnormalities and enhance the reliability and safety of the processes. Kernel Principal Component Analysis (KPCA) is a powerful FDD based data-driven method. It has gained much interest due to its ability in monitoring nonlinear systems. However, KPCA suffers from high computing time and large storage space when a large-sized training dataset is used. So, extracting and selecting the more relevant observations could provide a good solution to high computation time and memory requirements costs. In this paper, a new Reduced KPCA (RKPCA) approach is developed to address that issue. It aims to preserve one representative observation for each similar and selected Euclidean distance between training samples. Afterwards, the obtained reduced training dataset is used to build a KPCA model for FDD purposes. The developed RKPCA scheme is tested and evaluated across a numerical example and an actual involuntary process fault and various simulated sensor faults in a cement plant. The obtained results show high monitoring performance with highest robustness to false alarms and maximum fault detection sensitivity compared to conventional PCA, KPCA and other well-established RKPCA techniques. Furthermore, the unified contribution plot method demonstrates superior potentials in identifying faulty variables.																	0169-7439	1873-3239				SEP 15	2020	204								104091	10.1016/j.chemolab.2020.104091													
J								Near-infrared fault detection based on stacked regularized auto-encoder network	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Near infrared spectroscopy; Auto-encoder; Regularization; Softmax regression; Fault detection	SPECTROSCOPY; MACHINE	Near-infrared spectroscopy as a fast, efficient, non-destructive, and non-polluting technology is employed in this paper to acquire process data at the molecular level to detect the process fault. Due to the high intrinsic dimension, information redundancy, multiple linear correlations between variables and high-sensitiveness of spectral data, stacked auto-encoder as an unsupervised learning method is utilized. In addition, for avoiding over fitting of these auto-encoders and making the model more general, L2 regularization term is employed. To realize the detection and classification of multi-class faults, the softmax regression is adopted as a multi-classification algorithm. Finally, the eligibility of the mentioned neural network is investigated through a crude oil desalination and dehydration process. Nonlinear dimensional reduction of spectral data with minimizing the information loss during data compression, high accuracy, and mull-class fault detection with good visualization of its decision edge are three main advantages of the proposed fault detection method comparing with existing methods.																	0169-7439	1873-3239				SEP 15	2020	204								104101	10.1016/j.chemolab.2020.104101													
J								Prediction of a wellhead separator efficiency and risk assessment in a gas condensate reservoir	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Separation percentage; Gas condensate; Wellhead separator; ANN; ANFIS; Risk assessment	ARTIFICIAL NEURAL-NETWORK; FUZZY; OIL	In this paper artificial neural network (ANN) and adaptive neuro-fuzzy inference system (ANFIS) were applied to predict the separation percentage of gas and gas condensate in a wellhead separator in Naar oil field (Boushehr province, IRAN). The operating parameters including valve opening percentage, gas flow, design pressure, and design temperature are considered as the inputs of the models. The accuracy of the proposed models were evaluated using statistical parameters such as correlation coefficient (R-2), average percent relative error (APRE), average absolute percent relative error (AAPRE), and root mean square error (RMSE). Based on the achieved data, R-2 values were 0.9691 and 0.9807 for ANN and ANFIS models, respectively, while the values of RMSE were 6.117 and 4.57 for the applied models, which denote the higher accuracy of ANFIS model. Moreover, risk analyzing and consequence assessment of probable explosion of separator using PHAST (Process Hazard Analysis Software) software showed that inspection of separators is very important. Considering the calculated results, it can be concluded that ANFIS was better than ANN in prediction of gas and gas condensate separation percentages, since its output showed higher affinity to the real data. Generally, the findings obtained from the current work suggest that it is possible to predict the separation efficiency of a wellhead separator using intelligent systems.																	0169-7439	1873-3239				SEP 15	2020	204								104084	10.1016/j.chemolab.2020.104084													
J								A mutual information-based Variational Autoencoder for robust JIT soft sensing with abnormal observations	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Variational autoencoder; Mutual information; Just-in-time learning; Outliers; Missing data	SENSORS; REGRESSION	Considering industrial process with high-dimensional, intrinsic nonlinearities and possibly abnormal observations, a robust deep learning soft sensor model is developed under the just-in-time learning framework. As an unsupervised deep learning approach, Variational Autoencoder (VAE) has been successfully applied to soft sensing problems owing to its ability to describe the latent representations by probability distributions. In this work, to construct high performance soft sensor model, mutual information (MI) is first introduced for input variable selection. By further incorporating MI as weights on variable of the traditional VAE model, a MI-based output-relevant VAE is developed. For each new sample that arrives, by utilizing Symmetric Kullback-Leibler (SKL) divergence, its relevance with historical samples is determined. Based on the SKL divergence, the input samples that are most relevant to the query sample can be collected. The selected historical input samples and corresponding output samples are employed to build a Gaussian process regression (GPR) local model. Expectation maximation (EM) algorithm is utilized to deal with the nonlinearity and abnormal output observations in GPR local model simultaneously for robustness of the soft sensors. Numerical simulations and a benchmark process are employed to validate the effectiveness of the proposed soft sensor, which demonstrates its superior performance over traditional approaches.																	0169-7439	1873-3239				SEP 15	2020	204								104118	10.1016/j.chemolab.2020.104118													
J								Ozone concentrations prediction in Lanzhou, China, using chaotic artificial neural network	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Ozone; Air pollution; Chaotic system; Prediction model; Artificial neural network; Local	GROUND-LEVEL OZONE; NITROGEN-OXIDES; URBAN; MODEL; VOCS; POLLUTION; IMPACT; AREA; NOX	Among the important industrial cities in western China, Lanzhou has made great progress during recent years with its economic development policy. However, it has been accompanied by air pollution, particularly ozone (O-3) concentrations, which have adverse effects on the environment and human beings. Using Lanzhou as a research sample, chaotic artificial neural network (CANN) was used to establish an ozone concentrations prediction model. Meanwhile, several parameters were considered, such as the ozone concentrations by day at four monitoring stations from 2016 to 2018; sulfur dioxide (SO2), nitrogen dioxide (NO2), O-3, fine particulate matter with an aerodynamic diameter of 2.5 mu m or less (PM2.5), and fine particulate matter with an aerodynamic diameter of 10 mu m or less (PM10) concentrations over the same hours; temperature, humidity, instantaneous wind speed and direction. Seventy percent of the data were used to train CANN, 30% for testing, and back propagation (BP) network, artificial neural network (ANN), multiple linear regression (MLR) model were used for comparison. The results show that CANN has the smallest root mean squared error (RMSE) and a coefficient of correlation (R-2) near 1 compared with BP, ANN and MLR, and this means CANN can provide exact prediction result for pollution treatment in Lanzhou.																	0169-7439	1873-3239				SEP 15	2020	204								104098	10.1016/j.chemolab.2020.104098													
J								Sparse Bayesian learning approach for baseline correction	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Raman spectroscopy; Baseline correction; Sparse Bayesian learning (SBL); Sparse representation	SIGNAL RECONSTRUCTION; WAVELET TRANSFORM; ALGORITHMS	Spectral techniques in analytical chemistry are often affected by baselines in practical implementation. Without baseline correction, the accuracy of the qualitative/quantitative analytical results may degrade substantially. Sparse representation has been applied to baseline correction recently, which can provide state-of-the-art performance. However, it suffers from possible performance degradation when realized using l(1)-norm approximation. To significantly improve the performance for baseline correction, a sparse Bayesian learning (SBL) framework for joint pure spectrum fitting and baseline correction is presented in this work. Since the SBL framework provides high flexibility to tackle the minimum l(0)-norm problem instead of the l(1)-norm approximation, it is possible to yield higher baseline correction accuracy. Moreover, the proposed method has an inherent learning capability, so no additional regularization terms are required. Note that the sparse representation performance would degrade if the grid points used in dictionary matrix are not sufficiently dense. Therefore, we further consider grid points as adjustable parameters and then adopt a grid refinement technique to handle the off-grid gap. Results on both simulated and real datasets reveal substantial performance improvement of the proposed SBL-based method over the existing schemes on baseline correction.																	0169-7439	1873-3239				SEP 15	2020	204								104088	10.1016/j.chemolab.2020.104088													
J								Edge detection of heterogeneity in transmission images based on frame accumulation and multiband information fusion	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multiband information fusion; Frame accumulation; Edge detection; Grayscale resolution; Scattering; Breast tumor	BREAST-CANCER; MAMMOGRAPHY; IMPROVEMENT; DIAGNOSIS; CONTRAST; DENSITY; TISSUE; RISK; CCD	In optical transmission imaging, even a very thin layer of a scattering material can appear opaque make the image blurry, which restricts the positioning of heterogeneity in transmission images. Aiming at the problem, an exploratory method based on frame accumulation and multiband information fusion was proposed and proven, which can be used to detect the edge of heterogeneity in transmission images. Treating the trichromatic image as an example, the following experiment was designed: Firstly, the transmission video of phantom is collected by camera. Then, single-band images are decoded at "R\G\B" and further accumulated by frame accumulation technology to obtain the image with high grayscale level, respectively. Lastly, the information fusion of the three single-band images is realized through neural network, and further the edge could be detected by the established model. The experimental result shows that the heterogeneity edge can be detected more accurately by this method. Therefore, the validity of this method for detecting heterogeneity in transmission images is demonstrated. It is expected to provide an idea for the future clinical diagnosis of breast tumor by transmission method.																	0169-7439	1873-3239				SEP 15	2020	204								104117	10.1016/j.chemolab.2020.104117													
J								An ADMM-based algorithm with minimum dispersion regularization for on-line blind unmixing of hyperspectral images	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Hyperspectral imaging; Pushbroom acquisition system; On-line unmixing; Alternating direction method of multipliers; Minimum dispersion regularization	NONNEGATIVE MATRIX FACTORIZATION; MINIMIZATION; PARTS	Pushbroom imaging systems are emerging techniques for real-time acquisition of hyperspectral images. These systems are frequently used in industrial applications to control and sort products on-the-fly. In this paper, the online hyperspectral image blind unmixing is addressed. We propose a new on-line method based on Alternating Direction Method of Multipliers (ADMM) approach, adapted to pushbroom imaging systems. Because of the generally ill-posed nature of the unmixing problem, we impose a minimum endmembers dispersion regularization to stabilize the solution; this regularization can be interpreted as a convex relaxation of the minimum volume regularization and therefore, presents interesting optimization properties. The proposed algorithm presents faster convergence rate and lower computational complexity compared to the algorithms based on multiplicative update rules. Experimental results on synthetic and real datasets, and comparison to state-of-the-art algorithms, demonstrate the effectiveness of our method in terms of rapidity and accuracy.																	0169-7439	1873-3239				SEP 15	2020	204								104090	10.1016/j.chemolab.2020.104090													
J								i6mA-DNC: Prediction of DNA N6-Methyladenosine sites in rice genome based on dinucleotide representation using deep learning	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										DNA N6-methyladenine; Deep learning; Convolution neural network; Dinucleotide	ESCHERICHIA-COLI; MISMATCH REPAIR; METHYLATION; SINGLE; ACID; RESTRICTION; ENZYME; MODES	DNA methylation is a crucial epigenetic process. DNA N6-methyladenine is closely related to a variety of biological processes such as DNA replication, transcription, repair and cellular defense. In genome, N6-methyladenine (6 mA) sites are not uniformly distributed; therefore, it is required to determine the genomic locations of 6 mA for better comprehension of its biological functions. Although various experimental procedures have been used to identify 6 mA sites and yielded positive results, these biochemical techniques are expensive and time-consuming. In order to solve this problem and provide ease for future researches, it is indispensable to develop a robust and accurate computational model to find N6-methyladenine sites. With this regard, we introduce a deep learning-based computational model called i6mA-DNC to detect the N6-methyladenine sites in the rice genome. We split the DNA sequences into dinucleotide components and feed them to the model. This model automatically extracts optimal features from the pre-processed data using convolution neural network (CNN). Our proposed model i6mA-DNC obtained 89.20% of specificity, 88.01% of sensitivity, 88.60% of accuracy, and 0.772 of MCC. These results prove that our intelligent model achieved better success rates in all evaluation metrics than existing methods. Our model i6mA-DNC is expected to become a useful tool for academic research on N6-methyladenine sites identification. A user-friendly webserver has been established and made freely accessible at https://home.jbnu.ac.kr/NSCL/i6mA-DNC.htm.																	0169-7439	1873-3239				SEP 15	2020	204								104102	10.1016/j.chemolab.2020.104102													
J								Feature selection based on chaotic binary black hole algorithm for data classification	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Black hole algorithm; Chaotic map; Feature selection; Chemical model classification	OPTIMIZATION	With the advance of generating high-dimensional data, feature selection is the most significant procedure to guarantee selecting the most discriminative subset of features and to improve the classification performance. As a result, a binary black hole optimization algorithm (CBBHA) has been developed by getting inspired from natural phenomena. In this paper, the most discriminating features are selected by a new chaotic binary black hole algorithm (CBBHA) where chaotic maps embedded with movement of stars in the BBHA. Ten chaotic maps are employed. Experiments on three chemical datasets show the proposed algorithm, CBBHA, has an advantage over the standard BBHA in terms of selecting relevant features with a high classification performance. Additionally the performance of CBBHA is compared with BBHA in term of the computational time efficiency which is revealing that CBBHA outperforms the BBHA.																	0169-7439	1873-3239				SEP 15	2020	204								104104	10.1016/j.chemolab.2020.104104													
J								Investigating the need for preprocessing of near-infrared spectroscopic data as a function of sample size	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Calibration modelling; Preprocessing; Design of experiments; NIR; Spectroscopy; Model maintenance	MULTIVARIATE CALIBRATION; NIR	Preprocessing of near-infrared (NIR) spectra is an essential part of multivariate calibration. It mainly aims to remove artefacts caused during measurement to improve prediction performance or interpretation. However, preprocessing can have undesired side-effects. Additionally, calibration algorithms can learn to deal with artefacts by themselves when enough samples are available. This may influence the effect preprocessing has on prediction performance when the calibration dataset size increases. In this paper we investigate the interaction between the size of the calibration data and preprocessing for NIR calibrations for several datasets. Results show that extending the calibration data with more samples improves prediction performance, regardless of the preprocessing strategy. Although prediction performance almost always benefits from preprocessing, extending the calibration data can reduce the effect of preprocessing on prediction performance. This means the optimal preprocessing strategy may change as a function of the number of samples. It is demonstrated that using a Design of Experiments (DoE) approach to determine the optimal preprocessing strategy leads to equal or better prediction performance for all calibration set sizes compared to the case of not preprocessing at all. Preprocessing is most valuable for small calibration sets, but as the calibration set increases can become obsolete or even harmful. Therefore, we recommend to always evaluate the effect of a preprocessing strategy before making or updating calibration models.																	0169-7439	1873-3239				SEP 15	2020	204								104105	10.1016/j.chemolab.2020.104105													
J								Alternative particle compensation techniques for online water quality monitoring using UV-Vis spectrophotometer	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Particle compensation; Online water quality monitoring; UV254, UV-Vis spectrophotometer	SCATTER-CORRECTION; CALIBRATION; COD; SPECTROSCOPY; PROJECTION; SYSTEM; TSS	Particles in the water can significantly affect UV-Vis absorption measurements. There is a need for the water industry to develop a reliable technique to eliminate particle impact on on-line water quality monitoring using UV-Vis spectroscopy. This study aims to develop and use digital techniques for particle compensation: single wavelength compensation, linear regression compensation and multiplicative scatter correction method for online UV-Vis spectrophotometers. Water quality data were collected from three selected water sources in water treatment plants which represent different water qualities in terms of particles and organic matters. UV254 measurements were determined with these three software compensation techniques in comparison with the proprietary instrument built-in compensation algorithm using Bland-Altman analysis. Linear correction methods were found to be able to adjust the three compensation techniques to achieve acceptable compensated UV254 results, particularly for raw waters. UV254 measurements using single wavelength compensation, linear regression compensation and multiplicative scatter correction techniques with the assistant of linear correction methods were confirmed to be comparable to the instrument built-in compensation method. Our results reveal that these particle compensation techniques can make the UV254 technology reliable for online water quality monitoring in water treatment network. This paper demonstrated the advantage of using software compensation method to establish local compensation and calibration models instead of relying on the predetermined global calibrations for online water quality monitoring.																	0169-7439	1873-3239				SEP 15	2020	204								104074	10.1016/j.chemolab.2020.104074													
J								Logistic principal component analysis via non-convex singular value thresholding	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Binary data; Logistic PCA; Non-convex; Singular value thresholding; Concave penalty	MATRIX; ALGORITHMS	Multivariate binary data is becoming abundant in current biological research. Logistic principal component analysis (PCA) is one of the commonly used tools to explore the relationships inside a multivariate binary data set by exploiting the underlying low rank structure. We re-expressed the logistic PCA model based on the latent variable interpretation of the generalized linear model on binary data. The multivariate binary data set is assumed to be the sign observation of an unobserved quantitative data set, on which a low rank structure is assumed to exist. However, the standard logistic PCA model (using exact low rank constraint) is prone to overfitting, which could lead to the divergence of some estimated parameters towards infinity. We propose to fit a logistic PCA model through non-convex singular value thresholding to alleviate the overfitting issue. An efficient Majorization-Minimization algorithm is implemented to fit the model and a missing value based cross validation (CV) procedure is introduced for the model selection. Our experiments on realistic simulations of imbalanced binary data and low signal-to-noise ratio show that the CV error based model selection procedure is successful in selecting the proposed model. Furthermore, the selected model demonstrates superior performance in recovering the underlying low rank structure compared to models with convex nuclear norm penalty and exact low rank constraint. Finally, a binary copy number aberration data set is used to illustrate the proposed methodology in practice.																	0169-7439	1873-3239				SEP 15	2020	204								104089	10.1016/j.chemolab.2020.104089													
J								Multiblock global orthogonal projections to latent structures for fault diagnosis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multiblock global orthogonal projections to latent structures; Multiblock modified orthogonal projections to latent structures; Fault diagnosis; Process faults	PLS; PCA; DESIGN; MODEL	Although decentralized fault diagnosis based on multiblock regression models has achieved remarkable results, block information calculated a super block level by most traditional methods does not accurately address the subblock information. To address this information, multiblock diagnosis at block level was proposed, however, the model is complicated and fails to diagnose the output faults caused by external interference and/or noise. Therefore, in this paper, we propose a new fault diagnosis method based on multiblock global orthogonal projections to latent structures (MBGOPLS) to comprehensively diagnose process faults with a simple diagnosis scheme. This framework 1) obtains the full block/subblock output-related information using multiblock modified orthogonal projections to latent structures at block level, 2) combines all block/subblock output-unrelated information into a whole to establish appropriate output-unrelated statistics, and 3) further analyzes the input-unrelated information to diagnose input-unrelated faults. Thus, the proposed MBGOPLS results in a simpler model while retaining output-related and output-unrelated fault diagnosis ability, and capable of diagnosis in input-unrelated faults. A numerical example and a case study on a Tennessee Eastman process demonstrate the validity and superiority of the proposed method with respect to existing approaches.																	0169-7439	1873-3239				SEP 15	2020	204								104092	10.1016/j.chemolab.2020.104092													
J								Effect of variable allocation on validation and optimality parameters and on cross-optimization perspectives	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Validation; Design of experiment; Predictivity; Robustness; Goodness-of-fit	REPRESENTATIVE SUBSET; TEST SET; TRAINING SET; SELECTION; CALIBRATION; PREDICTION; DESIGN; MODELS	We investigated the allocation dependence of validation and design of experiment optimality parameters by modifying the pattern of the independent variables. They were scanned from a centre-like to corner-like positions and fixed design of experiment settings were checked, too. The response variable was modelled with multivariate linear regression. The calculations were performed on simulated data in two and 4-dimensional independent variable spaces and on datasets of power plant and QSAR studies. Our results showed that almost all parameters evaluating validation or design of experiment optimality could be tuned by intentionally increasing the number of samples taken close to the edges of the variable domain. We got strong rank correlation among most of the parameters. It coincides well with the primary aim of design of experiment to obtain validatable models, where design of experiment is performed before the model building, or even before the experiments, and validation takes place after the model building. Surprisingly, the best subsets of the selected samples for the different parameters overlapped only weakly. We found a reasonable cross optimization power only for some goodness-of-fit and robustness parameters, the overlap was rather weak concerning predictivity and design of experiment optimality parameters. For the simulated data we calculated the exact integrated error of prediction, its value strongly depended on the error features of the individual subsets and showed minimal cross-optimization power. We might conclude that design of experiment using optimality parameters need not to provide the allocations with best validation parameters in many cases.																	0169-7439	1873-3239				SEP 15	2020	204								104106	10.1016/j.chemolab.2020.104106													
J								Beyond one-against-all (OAA) and one-against-one (OAO): An exhaustive and parallel half-against-half (HAH) strategy for multi-class classification and applications to metabolomics	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Metabolomics; Inherited metabolic diseases (IMDs); Multi-class classification; Exhaustive and parallel half-against-half (EPHAH) decomposition; Orthogonal partial least squares discriminant analysis (OPLS-DA)	MS URINARY METABOLOMICS; SUPPORT VECTOR MACHINE; INBORN-ERRORS; ORTHOGONAL PROJECTIONS; IMPROVEMENT; TREE	Urinary metabolomics coupled with GC-MS has become a leading technology in newborn screening. Because of non-specificity, complexity and high-variety in clinical characteristics and metabolomic profiling, the simultaneous detection of multiple inherited metabolic diseases (IMDs) is often challenging. As a substantial health problem, a competent chemometrics multi-class classification system for the early detection and diagnosis of IMDs would be advantageous. Beyond the commonly used binarization techniques of one-against-all (OAA) and oneagainst-one (OAO), an exhaustive and parallel half-against-half (EPHAH) decomposition is described in this study to deal with multi-class classification. For a K-class problem, EPHAH employs uniform class binary partition strategy to induce the binary classifier evaluating a half of K classes against the other half. With K-class problem exhaustively decomposed into all uniform binary partitions of K classes, EPHAH parallelly arranges the corresponding binary classifiers and aggregates their outputs to obtain the multi-class prediction using max-wins voting strategy. Based on orthogonal partial least squares discriminant analysis (OPLS-DA) with feature selection using particle swarm optimization (PSO) algorithm, EPHAH is investigated by GC-MS urinary metabolomics data among healthy controls and 9 most common IMDs. The results show that EPHAH enables a complete learning of the complex multi-class decision boundaries of 10 classes, exhibiting significant superiority in classification accuracies over OAA, OAO and traditional HAH. Meanwhile, compared with OAO using the same max-wins voting strategy, EPHAH gives an effective break of the fie problem in classification and enhanced resolution in votes.																	0169-7439	1873-3239				SEP 15	2020	204								104107	10.1016/j.chemolab.2020.104107													
J								Soft sensor modeling for fraction yield of crude oil based on ensemble deep learning	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Soft sensor; Ensemble learning; Deep learning; Nuclear magnetic resonance; Crude oil; Fraction yield	NUCLEAR-MAGNETIC-RESONANCE; H-1-NMR SPECTROSCOPY; ALGORITHM; GASOLINE; ARCHITECTURE; KNOWLEDGE; MACHINE	Soft sensor modeling of crude oil physicochemical properties, especially fraction yield, is very important for refinery. Traditional method of properties evaluation in the laboratory is time-consuming and the near-infrared spectrum technology has shortcoming of low accuracy and strict application conditions. To solve these problems, the nuclear magnetic resonance (NMR) is introduced to properties evaluation of crude oil for refining process, where soft sensor modeling between the 1H NMR spectral data and the fraction yield plays an important role. This paper presents an ensemble deep learning based soft sensor modeling approach for fraction yield evaluation of crude oil. In this approach, a pre-processing method of spectral data is proposed by changing data structure, increasing data redundancy and generating virtual samples. Then, the model structure and its implementation are presented, which consists of a deep learning network, a near neighbor learning part and a random vector functional-link (RVFL) network based ensemble strategy. To avoid over-fitting, the training algorithm using k-fold cross-validation of ensemble deep learning model is also given. The experimental results show that the proposed model can learn the 1H NMR spectral data features well and improve accuracy for crude oil fraction yield evaluation.																	0169-7439	1873-3239				SEP 15	2020	204								104087	10.1016/j.chemolab.2020.104087													
J								Face Image Reflection Removal	INTERNATIONAL JOURNAL OF COMPUTER VISION										Reflection removal; Deep learning; Face images; Optical flow		Face images captured through glass are usually contaminated by reflections. The low-transmitted reflections make the reflection removal more challenging than for general scenes because important facial features would be completely occluded. In this paper, we propose and solve the face image reflection removal problem. We recover the important facial structures by incorporating inpainting ideas into a guided reflection removal framework, which takes two images as the input and considers various face-specific priors. We use a newly collected face reflection image dataset to train our model and compare with state-of-the-art methods. The proposed method shows advantages in estimating reflection-free face images for improving face recognition.																	0920-5691	1573-1405															10.1007/s11263-020-01372-5		SEP 2020											
J								MFP: an approach to delay and energy-efficient module placement in IoT applications based on multi-fog	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fog computing; Internet of Things; Multi-fog; Resource placement; Energy-efficient; Delay-efficient	RESOURCE-MANAGEMENT; CLOUD; THINGS; SIMULATION; INTERNET; TOOLKIT	One of the challenges of using fog computing in IoT systems is the efficient placement of resources in IoT applications. This paper presents a resource placement method for fog-based IoT systems to reduce their latency and energy consumption. Given the limited processing power of fog nodes, only a limited number of modules can be run on these nodes. In fog-cloud systems, placing the modules on fog nodes instead of the cloud layer can be expected to reduce system latency. Therefore, to achieve enhanced latency and energy consumption, this paper introduces a multi-zone fog layer architecture where each zone is a multi-fog. The core idea of the proposal is to use the idle processing capacity of fog nodes in each zone through the maximal placement of modules on these nodes. The paper also presents an algorithm called MFP for carrying out this placement. To evaluate the proposed algorithm, it was simulated in iFogSim for two scenarios with different topologies. The simulation results showed that the proposed scheme offers 16.81% lower latency and 17.75% lower energy consumption than the existing schemes.																	1868-5137	1868-5145															10.1007/s12652-020-02525-7		SEP 2020											
J								Maclaurin symmetric mean aggregation operators based on cubic Pythagorean linguistic fuzzy number	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cubic Pythagorean fuzzy set; CPLFDWMSMA operator; CPLFDOWMSMA operator; CPLFDHWMSMA operator; MCDM	GROUP DECISION-MAKING; FUNDAMENTAL PROPERTIES; REPRESENTATION MODEL; MEMBERSHIP GRADES; VARIABLES	The Maclaurin symmetric mean (MSM) and dual Maclaurin symmetric mean (DMSM) operators are two aggregation operators to aggregate the cubic Pythagorean linguistic fuzzy number. The cubic Pythagorean linguistic fuzzy structure is more real to designate fuzzy data in real decision-making problems. The cubic Pythagorean linguistic fuzzy number is more superior and difficult information in the environment of the fuzzy set theory. We describe the score and accuracy function of CPLFN. We define some aggregation operators, including the CPLFAA, CGPLFAA, CPLFGA, CPLFMSM, and CPLFWMSM operators. We present some operators, with the CPLFDWMSMA, CPLFDOWMSMA, CPLFDHWMSMA, CPLFDWMSMG, CPLFDOWMSMG and CPLFDHWMSMG operators. Moreover, some properties and special cases of our proposed methods are also introduced. Then we present multi-attributive group decision-making based on proposed methods. Further, a numerical example is provided to illustrate the flexibility and accuracy of the proposed operators. Last, the proposed methods are compared with existing methods to examine the best developing skill initiatives.																	1868-5137	1868-5145															10.1007/s12652-020-02272-9		SEP 2020											
J								A multi-objective stochastic model for a reverse logistics supply chain design with environmental considerations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Reverse logistics; Waste electrical and electronic equipment; Scenario-based stochastic programming; Augmented epsilon-constraint method	NETWORK DESIGN; OPTIMIZATION MODEL; LIFE VEHICLES; WASTE; END; CONSTRUCTION; UNCERTAINTY; RECOVERY; SALES; WEEE	Some electronic devices have a short lifetime, and variety-seeking and consumerism are increasingly growing in today's societies. Moreover, electronic wastes contain precious substances such as gold, silver, copper, and aluminum. The proper disposal and processing of them by recycling offer considerable advantages to the environment, given the hazardous natures of such devices' substances. The proposed reverse logistics with waste electrical and electronic equipment (WEEE) is an important task considered by researchers, the use of which offers economic benefits and reduces the environmental impacts of wastes. The present study models the electrical and electronic equipment (EEE) reverse logistics process as a bi-objective mixed-integer programming model under uncertainties. The mathematical model investigates two objectives: an economic objective and an environmental objective. The first is minimizing cost, while the second is maximizing the environmental score by reverse logistics processes in recovering and recycling. The parameters of demand and WEEE return rate which is obtained from the customer were considered as two uncertain parameters. A scenario-based stochastic programming (SSP) approach is applied to deal with the uncertainties. A case study of an electronic equipment manufacturer in Esfahan, Iran was included. The model was solved by a nominal approach and an SSP approach via the epsilon-constraint (EC) and augmented epsilon-constraint (AEC) methods to obtain optimal Pareto solutions and compare the methods. Finally, the optimal results of the two approaches were evaluated. The results indicated that the SSP approach using the AEC method had better outcomes.																	1868-5137	1868-5145															10.1007/s12652-020-02538-2		SEP 2020											
J								Automatic detection of non-proliferative diabetic retinopathy in retinal fundus images using convolution neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diabetic retinopathy; Retinal fundus images; CNN		Diabetic retinopathy (DR) is one of the complications of diabetes and a leading cause of blindness in the world. The tiny blood vessels inside the retina are damaged due to diabetes and result in various vision-related problems and it may lead to complete vision loss without early detection and treatment. Diabetic retinopathy may not cause any symptoms during its earlier stage of the disease and many physical tests such as visual acuity tests, pupil dilation, etc., are required to detect diabetic retinopathy disease. So, early detection of diabetic retinopathy disease is required to avoid vision loss. This work aims to automate the detection and grading of non-proliferative Diabetic Retinopathy from retinal fundus images using Convolution Neural Networks. The model was tested on two popular datasets such as MESSIDOR and IDRiD. Before applying the Convolution Neural Network (CNN) layers, the images were pre-processed and resolution was adjusted (256 x 256). The maximum accuracy achieved is 90.89% using MESSIDOR images. The research can be carried forward by applying various preprocessing techniques before putting them through different computational layers.																	1868-5137	1868-5145															10.1007/s12652-020-02518-6		SEP 2020											
J								Some concepts on interval-valued refined neutrosophic sets and their applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neutrosophic set; Interval-valued refined neutrosophic set; Cut set; Extension principle; Algebraic operation	SIMILARITY MEASURES	An interval-valued refined neutrosophic set is simply an extension of interval neutrosophic set and refined neutrosophic set which can be used in statistics, game theory, engineering, and experimental science. In this study, we define the cut set and extension principle based on interval-valued refined neutrosophic sets which is a bridge between interval-valued refined neutrosophic sets and crisp sets. Also, we examine the properties of the cut sets and extension principle of interval-valued refined neutrosophic sets. Finally, according to the extension principle of the interval-valued refined neutrosophic sets, we introduce some algebraic operations over the interval-valued refined neutrosophic sets.																	1868-5137	1868-5145															10.1007/s12652-020-02512-y		SEP 2020											
J								Pose Selection and Feedback Methods in Tandem Combinations of Particle Filters with Scan-Matching for 2D Mobile Robot Localisation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot Localisation; Particle Filters; Scan-matching	NAVIGATION; TRACKING	Robot localisation is predominantly resolved via parametric or non-parametric probabilistic methods. The particle filter, the most common non-parametric approach, is a Monte Carlo Localisation (MCL) method that is extensively used in robot localisation, as it can represent arbitrary probabilistic distributions, in contrast to Kalman filters, which is the standard parametric representation. In particle filters, a weight is internally assigned to each particle, and this weight serves as an indicator of a particle's estimation certainty. Their output, the tracked object's pose estimate, is implicitly assumed to be the weighted average pose of all particles; however, we argue that disregarding low-weight particles from this averaging process may yield an increase in accuracy. Furthermore, we argue that scan-matching, treated as a prosthesis of (or, put differently, fit in tandem with) a particle filter, can also lead to better accuracy. Moreover, we study the effect of feeding back this improved estimate to MCL, and introduce a feedback method that outperforms current state-of-the-art feedback approaches in accuracy and robustness, while alleviating their drawbacks. In the process of formulating these hypotheses we construct a localisation pipeline that admits configurations that are a superset of state-of-the-art configurations of tandem combinations of particle filters with scan-matching. The above hypotheses are tested in two simulated environments and results support our argumentation.																	0921-0296	1573-0409															10.1007/s10846-020-01253-6		SEP 2020											
J								Boosted-DEPICT: an effective maize disease categorization framework using deep clustering	NEURAL COMPUTING & APPLICATIONS										Autoencoders; Boosted-DEPICT; Deep clustering; Image processing; Plant disease	PLANT-DISEASE; TIME	Clustering of plant disease from digital images is an arduous task due to its dynamic nature and change of appearance under different environmental conditions. In most cases, the image captured in the real-time scenario is subjected to added noise, distortion, poor lighting conditions, and other potential factors that results in poor model performance during the process of discriminating between normal and disease-affected samples. It eventually maximizes the margin of the error rate, thereby leading to misclassification of disease of different varieties of plants in the database with other categories. This paper presents an effective deep clustering-based plant disease categorization algorithm, Boosted-Deep Embedded Regularized Clustering (DEPICT). This model integrates the convolutional autoencoder model with locality-preserving constraints and group sparsity into the network, which improves the embedded learning representation of the images. The PlantVillage and PDD image databases are accessed to develop this model for maize crop. The images are segmented by eliminating the background, cropped, augmented before model training. The performance of the system is evaluated by clustering accuracy and normalized mutual information. The proposed Boosted-DEPICT exhibits better performance, attains promising results with an accuracy of 97.73% and 91.25% on PV and PDD datasets, and outperforms state-of-the-art deep clustering algorithms. This system could be further enhanced by automating the entire process and transforming it into a mobile application for real-time analysis to gain instant results from any region.																	0941-0643	1433-3058															10.1007/s00521-020-05303-w		SEP 2020											
J								Convolutional neural network with spatial pyramid pooling for hand gesture recognition	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network (CNN); Spatial pyramid pooling (SPP); Hand gesture recognition; Sign language recognition		Hand gesture provides a means for human to interact through a series of gestures. While hand gesture plays a significant role in human-computer interaction, it also breaks down the communication barrier and simplifies communication process between the general public and the hearing-impaired community. This paper outlines a convolutional neural network (CNN) integrated with spatial pyramid pooling (SPP), dubbed CNN-SPP, for vision-based hand gesture recognition. SPP is discerned mitigating the problem found in conventional pooling by having multi-level pooling stacked together to extend the features being fed into a fully connected layer. Provided with inputs of varying sizes, SPP also yields a fixed-length feature representation. Extensive experiments have been conducted to scrutinize the CNN-SPP performance on two well-known American sign language (ASL) datasets and one NUS hand gesture dataset. Our empirical results disclose that CNN-SPP prevails over other deep learning-driven instances.																	0941-0643	1433-3058															10.1007/s00521-020-05337-0		SEP 2020											
J								Deep CNN Based Lmser and Strengths of Two Built-In Dualities	NEURAL PROCESSING LETTERS										Lmser; Duality in connection weights; Duality in paired neurons		Least mean square error reconstruction for the self-organizing network (Lmser) was proposed in 1991, featured by a bidirectional architecture with several built-in natures. In this paper, we developed Lmser into CNN based Lmser (CLmser), highlighted by new findings on strengths of two major built-in natures of Lmser, namely duality in connection weights (DCW) and duality in paired neurons (DPN). Shown by experimental results on several real benchmark datasets, DCW and DPN bring to us relative strengths in different aspects. While DCW and DPN can both improve the generalization ability of the reconstruction model on small-scale datasets and ease the gradient vanishing problem, DPN plays the main role. Meanwhile, DPN can form shortcut links from the encoder to the decoder to pass detailed information, so it can enhance the performance of image reconstruction and enables CLmser to outperform some recent state-of-the-art methods in image inpainting with irregularly and densely distributed point-shaped masks.																	1370-4621	1573-773X															10.1007/s11063-020-10341-5		SEP 2020											
J								Improved Sparsity of Support Vector Machine with Robustness Towards Label Noise Based on Rescaled alpha-Hinge Loss with Non-smooth Regularizer	NEURAL PROCESSING LETTERS										Support vector machine; Robust statistics; Rescaled alpha-hinge loss; Non-smooth regularizer; Primal-dual proximal method; Sparsity	CLASSIFICATION	Assupport vector machines(SVM) are used extensively in machine learning applications, it becomes essential to obtain a sparse model that is also robust to noise in the data set. Although many researchers have presented different approaches to get a robust SVM, the work onrobust SVM based on rescaled hinge loss function(RSVM-RHHQ) has attracted a great deal of attention. The method of using correntropy with hinge loss function has added a noticeable amount of robustness to the model. However, the sparsity of the model can be further improved. In this work, we focus on enhancing the sparsity of RSVM-RHHQ. As this work is the improved version of the RSVM-RHHQ, we follow the same track (of adding noise in the data) of RSVM-RHHQ with altogether a new problem formulation. We apply correntropy to the alpha-hinge loss function, which results in a better loss function than the rescaled hinge loss function. We use a non-smooth regularizer with a non-convex and non-smooth loss function. We solve this non-smooth and non-convex problem using the primal-dual proximal method. We find that this combination not only adds sparsity to the model, but it is also better than the existing robust SVM methods in terms of robustness towards label noise. We also provide the convergence proof of the proposed approach. In addition, the time complexity of the optimization technique is included. We perform experiments over various publicly available real-world data sets to compare the proposed method with the existing robust SVM methods. For experimentation purposes, we use small data sets, large data sets, and also data sets with significant class imbalance. Experimental results show that the proposed approach outperforms existing methods in sparseness, accuracy, and robustness. We also provide the sensitivity analysis of the regularization parameter for the label noise in the data set.																	1370-4621	1573-773X															10.1007/s11063-020-10346-0		SEP 2020											
J								Forecasting global crude oil price fluctuation by novel hybrid E-STERNN model and EMCCS assessment	SOFT COMPUTING										Forecasting hybrid neural network; Stochastic time strength; Ensemble empirical mode decomposition; Crude oil price series; Multi-scale composite complexity synchronization	FINANCIAL TIME-SERIES; ELMAN NEURAL-NETWORK; PREDICTION; WIND; DECOMPOSITION; ALGORITHM; SYSTEM; MEMORY; EMD; ANN	Energy futures are a very significant part of commodity futures, no less than the influence of the spot market. A novel hybrid neural network (denote by E-STERNN) is proposed through combining Elman recurrent neural network model with stochastic time strength (ST-ERNN), and ensemble empirical mode decomposition (EEMD) is also introduced to improve the performance of forecasting neural network system for energy markets. ST-ERNN model is established for taking into account the weight of energy historical data with time variations. EEMD is an algorithm that decomposes any non-stationary and nonlinear time series into simple and independent time sequence. From the empirical research for four global energy market prices, the proposed hybrid E-STERNN model is verified to have higher prediction accuracy compared with the original ERNN and the ST-ERNN models. Moreover, a new error evaluation approach, called the exponent of multi-scale composite complexity synchronization (EMCCS), is utilized to analyze and estimate the prediction performance, and the demonstration analyses confirm that the hybrid E-STERNN model has higher prediction accuracy for global energy futures indexes.																	1432-7643	1433-7479															10.1007/s00500-020-05327-3		SEP 2020											
J								Optimizing abi-objectivevehicle routing problem that appears in industrial enterprises	EXPERT SYSTEMS										frequency; local search; multi-start algorithm; periodic vehicle routing problem; proximate optimality principle; service choice	ALGORITHM	In this paper, a new solution method is implemented to solve a bi-objective variant of the vehicle routing problem that appears in industry and environmental enterprises. The solution involves designing a set of routes for each day in a period, in which the service frequency is a decision variable. The proposed algorithm, a muti-start multi-objective local search algorithm (MSMLS), minimizes total emissions produced by all vehicles and maximizes the service quality measured as the number of times that a customer is visited by a vehicle in order to be served. The MSMLS is a neighbourhood-based metaheuristic that obtains high-quality solutions and that is capable of achieving better performance than other competitive algorithms. Furthermore, the proposed algorithm is able to perform rapid movements thanks to the easy representation of the solutions.																	0266-4720	1468-0394														e12638	10.1111/exsy.12638		SEP 2020											
J								Envelopes: A new chapter in partial least squares regression	JOURNAL OF CHEMOMETRICS										abundant regressions; high-dimensional regressions; SIMPLS algorithm; sparse regressions; sufficient dimension reduction	DIMENSION REDUCTION; PLS; CHEMOMETRICS; PREDICTION; MODELS	We describe and elaborate on foundations that connect partial least squares regression with recently developed envelope theory and methodology. These foundations explain why PLS regression can work well in high-dimensional regressions where the number of predictors exceeds the number of observations and set it apart from other predictive methodologies. We hope that our foundational perspective will stimulate cross-fertilization between statistics and chemometrics, leading eventually to important methodological advancements.																	0886-9383	1099-128X														e3287	10.1002/cem.3287		SEP 2020											
J								Measuring employee-tourist encounter experience value: A big data analytics approach	EXPERT SYSTEMS WITH APPLICATIONS										Employee-tourist encounter; Model; Big data analytics; Dictionary; Text analytics	USER-GENERATED CONTENT; VALUE CO-CREATION; SCALE DEVELOPMENT; SERVICE; SATISFACTION; ECONOMY; SKILLS	This paper takes a text analytics approach to measuring dimensions of employee-visitor encounters that impact on visitor outcomes. A conceptual model measuring dimensions of employee-tourist encounters is implemented using a big data analytics approach more suited to large-scale online review data than the traditional, limited survey approach. Using a dictionary-based measurement approach and a large sample of reviews for hotels (n = 265,016), we test the model and the importance of the factors for leveraging perceptions of satisfaction, service and value. The results demonstrate the importance of the different dimensions of experiential value in employee-tourist encounters in creating positive tourist perceptions. This knowledge is crucial for tourism companies aiming to create experiential value for visitors, rather than simply delivering service quality. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113450	10.1016/j.eswa.2020.113450													
J								An analysis of boosted ensembles of binary fuzzy decision trees	EXPERT SYSTEMS WITH APPLICATIONS										Ensemble classifiers; Boosting; Fuzzy characterization; Fuzzy decision trees; Rule-based classifiers	ALGORITHMS; INDUCTION; SYSTEMS	Classification is a functionality that plays a central role in the development of modern expert systems, across a wide variety of application fields: using accurate, efficient, and compact classification models is often a prime requirement. Boosting (and AdaBoost in particular) is a well-known technique to obtain robust classifiers from properly-learned weak classifiers, thus it is particularly attracting in many practical settings. Although the use of traditional classifiers as base learners in AdaBoost has already been widely studied, the adoption of fuzzy weak learners still requires further investigations. In this paper we describe FDT-Boost, a boosting approach shaped according to the SAMME-AdaBoost scheme, which leverages fuzzy binary decision trees as multi-class base classifiers. Such trees are kept compact by constraining their depth, without lowering the classification accuracy. The experimental evaluation of FDT-Boost has been carried out using a benchmark containing eighteen classification datasets. Comparing our approach with FURIA, one of the most popular fuzzy classifiers, with a fuzzy binary decision tree, and with a fuzzy multi-way decision tree, we show that FDT-Boost is accurate, getting to results that are statistically better than those achieved by the other approaches. Moreover, compared to a crisp SAMME-AdaBoost implementation, FDT-Boost shows similar performances, but the relative produced models are significantly less complex, thus opening up further exploitation chances also in memory-constrained systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113436	10.1016/j.eswa.2020.113436													
J								Swarm optimized cluster based framework for information retrieval	EXPERT SYSTEMS WITH APPLICATIONS										Information retrieval; Swarm intelligence; Big data clustering; Frequent pattern mining; Unsupervised learning	SEQUENTIAL PATTERNS; GENETIC ALGORITHM; RANKING; BEES	This work explores the integrated power of swarm intelligence and advances in data mining techniques to solve the information retrieval (IR) problem of rapidly growing digital content on the World Wide Web. We propose a swarm optimized cluster based framework with frequent pattern mining techniques to retrieve user-specific knowledge from extensive document collections. In the pre-processing phase, we split the task into two sub-tasks. The first is to decompose the document collection into groups using a bio-inspired K-Flock clustering algorithm, while the second extracts frequent patterns from each cluster using a memory-efficient Recursive Elimination (RElim) algorithm. In the next phase, we implement a cosine similarity based probabilistic model to retrieve query-specific documents from clusters based on the matching scores between the closed frequent patterns of queries and clusters. The performance of a system is evaluated by conducting several experiments which are carried out on five well-known, diverse and variable size datasets viz- TREC 2014-15 CDS (Clinical Decision Support) datasets containing 733,138 records, OHSUMED dataset with 348,566 records from Medline database, NPL dataset with 11,429 records, LISA document collection of 6004 records, CACM (Collection of ACM) dataset of 3204 records. The results show that the proposed IR framework significantly outperforms the traditional sequential IR approach and other state-of-the-art IR approaches, both in terms of the quality of the returned documents and the time of execution. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113441	10.1016/j.eswa.2020.113441													
J								An efficient double adaptive random spare reinforced whale optimization algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Whale optimization; Engineering design; Swarm-intelligence; Global optimization; Nature-inspired computing	PARTICLE SWARM OPTIMIZER; LEVY FLIGHT; ENGINEERING OPTIMIZATION; EVOLUTIONARY; SEARCH; DESIGN; INTELLIGENCE; STRATEGIES; INTEGER; SYSTEM	Whale optimization algorithm (WOA) is a newly developed meta-heuristic algorithm, which is mainly based on the predation behavior of humpback whales in the ocean. In this paper, a reinforced variant called RDWOA is proposed to alleviate the central shortcomings of the original method that converges slowly, and it is easy to fall into local optimum when dealing with multi-dimensional problems. Two strategies are introduced into the original WOA. One is the strategy of random spare or random replacement to enhance the convergence speed of this algorithm. The other method is the strategy of double adaptive weight, which is introduced to improve the exploratory searching trends during the early stages and exploitative behaviors in the later stages. The combination of the two strategies significantly improves the convergence speed and the overall search ability of the algorithm. The advantages of the proposed RDWOA are deeply analyzed and studied by using typical benchmark examples such as unimodal, multi-modal, and fixed multi-modal functions, and three famous engineering design problems. The experimental results show that the exploratory and exploitative tendencies of WOA and its convergence mode have been significantly improved. The RDWOA developed in this paper is a promising improved WOA variant, and it has better efficacy compared to other state-of-the-art algorithms. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113018	10.1016/j.eswa.2019.113018													
J								A conversational recommender system for diagnosis using fuzzy rules	EXPERT SYSTEMS WITH APPLICATIONS										Recommendation; Diagnosis; Fuzzy logic; Critiquing; Formal concept analysis	SCALE; SCHIZOPHRENIA; CLOSURE; ENGINE	Graded implications in the framework of Fuzzy Formal Concept Analysis are used as the knowledge guiding the recommendations. An automated engine based on fuzzy Simplification Logic is proposed to make the suggestions to the users. Conversational recommender systems have proven to be a good approach in telemedicine, building a dialogue between the user and the recommender based on user preferences provided at each step of the conversation. Here, we propose a conversational recommender system for medical diagnosis using fuzzy logic. Specifically, fuzzy implications in the framework of Formal Concept Analysis are used to store the knowledge about symptoms and diseases and Fuzzy Simplification Logic is selected as an appropriate engine to guide the conversation to a final diagnosis. The recommender system has been used to provide differential diagnosis between schizophrenia and schizoaffective and bipolar disorders. In addition, we have enriched the conversational strategy with two strategies (namely critiquing and elicitation mechanism) for a better understanding of the knowledge-driven conversation, allowing user's feedback in each step of the conversation and improving the performance of the method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113449	10.1016/j.eswa.2020.113449													
J								A model for sector restructuring through genetic algorithm and inverse DEA	EXPERT SYSTEMS WITH APPLICATIONS										Mergers; Restructuring; Inverse data envelopment analysis; Genetic algorithm; Cardinality constraint	DATA ENVELOPMENT ANALYSIS; TECHNICAL EFFICIENCY; OPTIMIZATION; MERGERS; EPSILON; GAINS; BANKS; UNIT	The aim of this study is to devise a sector restructuring model in which all the decision making units (DMUs) satisfy a predefined global efficiency level. The proposal makes several realistic assumptions regarding the merging of DMUs under specific circumstances. The model computes the global efficiency target by giving preference to merging DMUs over saving inputs, hence considering that the affected stakeholders may be resistant to restructuring, and this resistance may have overall negative effects on the image and reputation of the companies and organizations. In addition, the number of constituents in the new entities can be limited by the decision maker after the restructuring process, so that the model also considers a constraint on cardinality. The proposal combines the inverse data envelopment analysis (InvDEA), which computes the merger's input savings, and the genetic algorithm (GA), which solves the combinatorial problem of identifying the merging units. The proposal is illustrated by two examples from banking and higher education. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113422	10.1016/j.eswa.2020.113422													
J								A modified Sine Cosine Algorithm with novel transition parameter and mutation operator for global optimization	EXPERT SYSTEMS WITH APPLICATIONS										Optimization; Sine Cosine Algorithm; Exploration and exploitation; Multilayer perceptron; Engineering optimization problems; Algorithm; Benchmark; Grey Wolf Optimizer; Particle Swarm Optimization; Genetic Algorithm	SWARM	Inspired by the mathematical characteristics of sine and cosine trigonometric functions, the Sine Cosine Algorithm (SCA) has shown competitive performance among other meta-heuristic algorithms. However, despite its sufficient global search ability, its low exploitation ability and immature balance between exploitation and exploration remain weaknesses. In order to improve Sine Cosine Algorithm (SCA), this paper presents a modified version of the SCA called MSCA. Firstly, a non-linear transition rule is introduced instead of a linear transition to provide comparatively better transition from the exploration to exploitation. Secondly, the classical search equation of the SCA is modified by introducing the leading guidance based on the elite candidate solution. When the above proposed modified search mechanism fails to provide a better solution, in addition, a mutation operator is used to generate a new position to avoid the situation of getting trapped in locally optimal solutions during the search. Thus, the MSCA effectively maximizes the advantages of proposed strategies in maintaining a comparatively better balance of exploration and exploitation as compared to the classical SCA. The validity of the MSCA is tested on a set of 33 benchmark optimization problems and employed for training multilayer perceptrons. The numerical results and comparisons among several algorithms show the enhanced search efficiency of the MSCA. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113395	10.1016/j.eswa.2020.113395													
J								Interactive example-based finding of text items	EXPERT SYSTEMS WITH APPLICATIONS										Active learning; Text processing; Pattern	REGULAR EXPRESSIONS; EXTRACTION; GENERATION; TOOL	We consider the problem of identifying within a given document all text items which follow a certain pattern to be specified by a user. In particular, we focus on scenarios in which the task is to be completed very quickly and the user is not able to specify the exact pattern of interest. The key use case corresponds to the interactive exploration of documents in search of snippets that do not fit Boolean, word-based search expressions. We propose an interactive framework in which the user provides examples of the items he is interested in, the system identifies items similar to those provided by the user and progressively refines the similarity criterion by submitting selected queries to the user, in an active learning fashion. The fact that the search is to be executed very quickly places severe requirements on the algorithms that can be used by the system, both for identifying the items and for constructing the queries. We propose and assess experimentally in detail a number of different design options for the components of the learning machinery. The results demonstrate the ability of our approach to achieve effectiveness close to state-of-the-art approaches based on regular expressions, while requiring an execution time which is orders of magnitude shorter. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113403	10.1016/j.eswa.2020.113403													
J								Euclidean distance based feature ranking and subset selection for bearing fault diagnosis	EXPERT SYSTEMS WITH APPLICATIONS										Bearing; Euclidean distance; Defect; Envelope analysis; Fault diagnosis; Feature ranking	EMPIRICAL MODE DECOMPOSITION; REMAINING USEFUL LIFE; NEURAL-NETWORK; SYSTEM; ENTROPY	Bearing failure can cause hazardous effects on rotating machinery. The diagnosis of the fault is very critical for reliable operation. The main steps for the machine learning process involve feature extraction, selection, and classification. Feature selection contains an identification of noble features that performs for better classification accuracy with fewer features and with less computational time. For a large feature dimension; a critical study is required to catch the best feature subset for proper diagnosis. So, this paper presents a unique feature ordering and selection technique called Feature Ranking and Subset Selection based on Euclidean distance (FRSSED). Two bearing databases have considered for verification of the robustness of the proposed technique. One database was obtained from the experiment, and the other publicly available database was collected from Case Western Reserve University (CWRU). Initially, the vibration signals have captured from bearings having an individual as well as combined defects in various components along with healthy bearing. EEMD was applied to these signals, and then, the sensitive IMF was selected by the envelope spectrum. In the later stage, the feature extraction was carried out from the selected IMF using fifteen statistical features. Afterward, the extracted features were introduced into FRSSED algorithm for feature ordering. These ordered features were fed into various classifiers. The comparison was made for classification accuracy and time consumption among generalized method (without feature ordering), principal component analysis (PCA), and FRSSED. The diagnostic outcomes describe that the suggested feature reduction technique improves the classification accuracy with fewer feature subset along with considerable time-saving. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113400	10.1016/j.eswa.2020.113400													
J								Integration and comparison of multi-criteria decision making methods in safe route planner	EXPERT SYSTEMS WITH APPLICATIONS										Safe route planner; AHP; TOPSIS; PROMETHEE; Fuzzy; Discounted cumulative gain	ANALYTIC HIERARCHY PROCESS; EXTENT ANALYSIS METHOD; FUZZY TOPSIS METHODS; MULTIPLE CRITERIA; AHP; SYSTEM; PROJECTS; SELECT	Motor vehicle crashes are a leading cause of death in the U.S. In order to reduce death and serious injury, road and traffic engineers manually evaluate road segments and visualize the safety level of roads. These existing risk maps can be confusing and must be manually interpreted by drivers to find the safest path from a source to a given destination; this can result in ignoring the safety of the routes by drivers. In addition, common navigation systems such as Google Maps and Waze present two or three alternative paths from a source to a given destination based on the travel time and distance. A navigation system is required to take the safety level of the road segments into consideration while suggesting a path. This navigation system needs to acquire knowledge from various sources, a user interface to obtain user preferences, and an inference engine to find the best paths. Such a system can still suggest multiple conflicting paths, such as shortest, fastest and safest paths. This paper presents the addition of a multi-criteria decision-making (MCDM) method, Analytical Hierarchy Process, to a previously designed Safe Route Planner to aid users in choosing the most suitable path among M alternative paths. Different MCDM methods can generate different results while applied to the same problem. There are a few comparative studies to compare the results of different Multi-Criteria Decision-Making (MCDM) methods. Therefore, a particular attention is devoted to comparing the results of five decision-making techniques, namely AHP, Fuzzy AHP, TOPSIS, Fuzzy TOPSIS and PROMETHEE through two real-world case studies. In addition, the comparative studies fail to adequately quantify the results of the MCDM methods; consequently, another aim of this research is to investigate the applicability of Spearman's rank correlation coefficient, Average Overlap and Discounted Cumulative Gain techniques to quantify the results of the MCDM methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113399	10.1016/j.eswa.2020.113399													
J								A white-box analysis on the writer-independent dichotomy transformation applied to offline handwritten signature verification	EXPERT SYSTEMS WITH APPLICATIONS										Offline signature verification; Dichotomy transformation; Writer-independent systems; Instance hardness; Transfer learning	IDENTIFICATION; FEATURES; ENSEMBLE	High number of writers, small number of training samples per writer with high intra-class variability and heavily imbalanced class distributions are among the challenges and difficulties of the offline Handwritten Signature Verification (HSV) problem. A good alternative to tackle these issues is to use a writer-independent (WI) framework. In WI systems, a single model is trained to perform signature verification for all writers from a dissimilarity space generated by the dichotomy transformation. Among the advantages of this framework is its scalability to deal with some of these challenges and its ease in managing new writers, and hence of being used in a transfer learning context. In this work, we present a white-box analysis of this approach highlighting how it handles the challenges, the dynamic selection of references through fusion function, and its application for transfer learning. All the analyses are carried out at the instance level using the instance hardness (IH) measure. The experimental results show that, using the IH analysis, we were able to characterize "good" and "bad" quality skilled forgeries as well as the frontier region between positive and negative samples. This enables futures investigations on methods for improving discrimination between genuine signatures and skilled forgeries by considering these characterizations. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 15	2020	154								113397	10.1016/j.eswa.2020.113397													
J								Artificial intelligence and institutional critique 2.0: unexpected ways of seeing with computer vision	AI & SOCIETY										Institutional critique; Computer vision; Error; Image analysis; Contemporary art	BIG DATA	During 2018, as part of a research project funded by the Deviant Practice Grant, artist Bruno Moreschi and digital media researcher Gabriel Pereira worked with the Van Abbemuseum collection (Eindhoven, NL), reading their artworks through commercial image-recognition (computer vision) artificial intelligences from leading tech companies. The main takeaways were: somewhat as expected, AI is constructed through a capitalist and product-focused reading of the world (values that are embedded in this sociotechnical system); and that this process of using AI is an innovative way for doing institutional critique, as AI offers an untrained eye that reveals the inner workings of the art system through its glitches. This paper aims to regard these glitches as potentially revealing of the art system, and even poetic at times. We also look at them as a way of revealing the inherent fallibility of the commercial use of AI and machine learning to catalogue the world: it cannot comprehend other ways of knowing about the world, outside the logic of the algorithm. But, at the same time, due to their "glitchy" capacity to level and reimagine, these faulty readings can also serve as a new way of reading art; a new way for thinking critically about the art image in a moment when visual culture has changed form to hybrids of human-machine cognition and "machine-to-machine seeing".																	0951-5666	1435-5655															10.1007/s00146-020-01059-y		SEP 2020											
J								A survey of attack detection approaches in collaborative filtering recommender systems	ARTIFICIAL INTELLIGENCE REVIEW										Collaborative filtering; Recommender systems; Fake user; Detecting attack; Clustering; Feature extraction; Shilling attack	PROFILE-INJECTION ATTACKS; MATRIX FACTORIZATION; ACCURACY; MODEL	Nowadays, due to the increasing amount of data, the use of recommender systems has increased. Therefore, the quality of the recommendations for the users of these systems is very important. One of the recommender systems models is collaborative filtering (CF) which uses the ratings given by the users to the items. But many of these ratings may be noisy or inaccurate so they reduce the quality of the recommendations. Sometimes users, using fake profiles, try to change the recommendations in their favor. Since satisfaction and trust in such systems are very important and useful, it would be better to find a way to identify these types of users. Despite numerous studies on CF recommender systems, the design of a robust recommender system is still a challenging problem. In this paper, we have analyzed the 25 previous samples of research on collaborative filtering recommender system (CFRS) for attack detection from 2009 to 2019. Most of these papers focus mainly on movie recommendations. According to these analyzes, we have categorized attack detection methods on CFRS in four categories: clustering, classifying, feature extraction and probabilistic approaches. The evaluation measures, the dataset, and attacks features used in the attack detection approaches are discussed.																	0269-2821	1573-7462															10.1007/s10462-020-09898-3		SEP 2020											
J								Rain Rendering for Evaluating and Improving Robustness to Bad Weather	INTERNATIONAL JOURNAL OF COMPUTER VISION										Adverse weather; Vision and rain; Physics-based rendering; Image to image translation; GAN	VISION	Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73% more realistic than the state-of-the-art. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection, 60% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21% on object detection, 37% on semantic segmentation, and 8% on depth estimation.																	0920-5691	1573-1405															10.1007/s11263-020-01366-3		SEP 2020											
J								Application of Graphene Nanofluid/Ultrasonic Atomization MQL System in Micromilling and Development of Optimal Predictive Model for SKH-9 High-Speed Steel Using Fuzzy-Logic-Based Multi-objective Design	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Micromilling; Nanofluid minimum quantity lubrication; Graphene; Ultrasonic atomization MQL; Robust process design; Fuzzy inference systems; Multiple performance characteristic index (MPCI)	MINIMUM QUANTITY LUBRICATION; CUTTING FLUID; OPTIMIZATION; PARAMETERS; QUALITY; TOOL; DRY	This paper focuses on using nanofluid (graphene)/ultrasonic atomization minimum quantity lubrication (MQL) in micromilling for SKH-9 high-speed steel. Utilizing the special properties of graphene, which has excellent thermal conductivity, it is found that it successfully lowers the cutting temperature generated during processing, reduces tool wear, and improves the quality of micromilling products. Using a self-developed ultrasonic atomization system effectively improves the agglomeration of nanoparticles in nanofluids and increases the lubrication efficiency of nanoparticles. The experimental plot is robustly designed, and theL(18)(2(1) x 3(7)) orthogonal table is used to find the optimal combination of parameters. The control factors are the average thickness of the nanographene, density of nanofluid, spindle speed, distance of nozzle, feed rate, amount ultrasonic atomization, air pressure, nozzle angle, and using gray correlation analysis with fuzzy inference to find more heavy quality characteristics. Finally, the optimal parameter combination of multi-quality characteristics enhanced by nanofluid (graphene)/ultrasonic atomization MQL is compared with the base fluid/ultrasonic atomization MQL, nanofluid (MWCNTs)/ultrasonic atomization MQL, whereas the differences in micromilling force, temperature, tool wear, and workpiece burr are discussed. The results indicate that the use of nanofluid (graphene)/ultrasonic atomization MQL has better results than other lubrication methods.																	1562-2479	2199-3211				OCT	2020	22	7					2101	2118		10.1007/s40815-020-00930-w		SEP 2020											
J								Multiplicative Consistency Adjustment Model and Data Envelopment Analysis-Driven Decision-Making Process with Probabilistic Hesitant Fuzzy Preference Relations	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Probabilistic hesitant fuzzy preference relations; Consistency adjustment algorithm; Data envelopment analysis; Decision-making method	PROGRAMMING-MODEL; PRIORITY WEIGHTS; SUPPORT MODEL; EFFICIENCY; CONSENSUS	Unlike other fuzzy modellings, probabilistic fuzzy sets can reflect clearly the importance of different numerical values. In group decision-making (GDM) problems, it is quite common for decision-makers (DMs) to elicit their knowledge with probabilistic hesitant fuzzy preference relations (PHFPRs), in which consistency adjustment and alternatives' weight vector determination play a key role in the decision-making process. This study aims at constructing a decision-making model with PHFPRs. First, several new concepts are introduced, including the multiplicative consistency and consistency index of PHFPRs. Then, we present a construction approach for the multiplicative consistent PHFPRs, and a convergent local consistency improvement process for PHFPRs is designed to detect and improve their consistency when the PHFPRs do not meet the consistency level. The local adjustment strategy is utilized to retain the preference evaluation of DMs as much as possible. Afterwards, based on the obtained efficiency score values, we propose a new data envelopment analysis model to derive the weight values of alternatives. Furthermore, we explore a decision-making method with PHFPRs to obtain the optimal selection from the alternatives. Finally, an applied case about logistics company assessment is presented, and the effectiveness and rationality of the explored method are verified by the comparison with the various approaches.																	1562-2479	2199-3211				OCT	2020	22	7					2319	2332		10.1007/s40815-020-00944-4		SEP 2020											
J								A Fuzzy Dematel Method To Evaluate The Most Common Diseases In Internal Medicine	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy DEMATEL; Internal medicine; Disease; Diagnose	DECISION-MAKING; LOGIC; SYSTEM; KNOWLEDGE; MODEL; MANAGEMENT	Most patients have more than one disease, and these diseases are able to affect one another. In modern medicine, the etiology and pathophysiology of diseases are well known in detail. However, inter-disease relationships are still mysterious. Physicians' knowledge and experience have great importance in such a multi-criteria case. Because medical doctors in internal medicine clinics deal with large numbers of patients with multiple diseases, they have quite a complex approach in treating illness. In this context, exposing the cause-and-effect relationships among diseases frequently seen in internal medicine will contribute to physicians' ability to blend profound theoretical knowledge with experiential results. Therefore, this study presents a fuzzy DEMATEL (Decision-Making Trial-and-Evaluation Laboratory) method to assess the most common diseases in internal medicine outpatient clinics. The DEMATEL method allows one to identify and analyze significant diseases in internal medicine by considering the cause-and-effect relationship diagram. Likewise, fuzzy sets in DEMATEL overcome the uncertainty in making decisions about disease relationships and internal medicine experts' judgments. When investigating the results, we have found dyspepsia, hyperlipidemia, and anemia to be crucial in terms of causes. When evaluating the effects, the most notable diseases are understood to be renal failure, malignancy, and hepatitis. The results indicate that in the presented study, we could successfully apply these methods to reveal the cause-effect of diseases. The results of this study will contribute to understanding the complex multi-criteria relationship among internal diseases using internists' opinions.																	1562-2479	2199-3211				OCT	2020	22	7					2385	2395		10.1007/s40815-020-00921-x		SEP 2020											
J								An Efficient Algorithm to Solve Transshipment Problem in Uncertain Environment	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Intuitionistic fuzzy set; Intuitionistic fuzzy number; Intuitionistic fuzzy optimal solution; Fully intuitionistic fuzzy transshipment problem		Transshipment problems are special type of transportation problems in which goods are transported from a source to a destination through various intermediate nodes (sources/destinations), possibly to change the modes of transportation or consolidation of smaller shipments into larger or deconsolidation of shipments. These problems have found great applications in the era of e-commerce. The formulation of transshipment problems involves knowledge of parameters like demand, available supply, related cost, time, warehouse space, budget, etc. However, several types of uncertainties are encountered in formulating transshipment problem mathematically due to factors like lack of exact information, hesitation in defining parameters, unobtainable information or whether conditions. These types of uncertainty can be handled amicably by representing the related parameters as intuitionistic fuzzy numbers. In this article, a fully fuzzy transshipment problem is considered in which the related parameters (supply, demand and cost) are assumed to be represented as trapezoidal intuitionistic fuzzy numbers. The proposed method is based on ambiguity and vagueness indices, thereby taking into account hesitation margin in defining the values precisely. These indices are then used to rank fuzzy numbers to derive a fuzzy optimal solution. The technique described in this paper has an edge as it directly produces a fuzzy optimal solution without finding an initial basic feasible solution. The method can easily be employed to fuzzy transshipment problems involving trapezoidal intuitionistic, triangular intuitionistic, trapezoidal, triangular, interval valued fuzzy numbers and real numbers. The proposed technique is supported by numerical illustrations and it has been shown that the method described in the paper is computationally much more efficient than already existing method and is applicable to a larger set of problems.																	1562-2479	2199-3211															10.1007/s40815-020-00923-9		SEP 2020											
J								A Fuzzy Design Decision Model for New Healthcare Service Conceptualization	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Design decision-making; Service conceptualization; Service design	CUSTOMER INVOLVEMENT; PRODUCT SERVICE; QUALITY; INNOVATION; SYSTEMS; NEEDS; LIBRARIES; SELECTION; INDUSTRY; CREATION	This paper purpose a structural design decision model by using service blueprint, failure mode effect analysis (FMEA), Fuzzy method, and the theory of inventive problem solving (TRIZ). In "service process analysis" stage, the service blueprint approach is used to analyze the potential service failure points in the service process. Then, FMEA is employed to diagnose the possible causes and effects of the service failure model in "service failure diagnosis" stage. The service failure models are prioritized according to the calculated integrated Risk Priority Number (RPN) and fuzzy number. In "innovative principle generation" stage, the innovative principles are generated by utilizing the TRIZ matrix. Finally, in "innovative solution conceptualization" stage, the TRIZ inventive principles are used to inspire the new solution for new service design. An empirical survey of home-care service agencies in Beijing is conducted to demonstrate the applicability of the proposed 2S2I model to improve home-care services, medical-equipment designs, and service delivery. The advantages, implications, and contributions of 2S2I model are also concluded.																	1562-2479	2199-3211															10.1007/s40815-020-00942-6		SEP 2020											
J								Fuzzy-Logic-Inspired Zone-Based Clustering Algorithm for Wireless Sensor Networks	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Wireless sensor network; Clustering; Fuzzy logic; Energy efficiency; Lifetime	PROTOCOL	With the increasing use of wireless terminals, wireless sensor networks (WSNs) have received significant attention owing to their wide usage in monitoring harsh environments, crucial surveillance and security applications, and other real-life applications. Sensor nodes in operation are equipped with batteries that are not rechargeable in most of the cases. Attaining the lifetime maximization of these nodes has drawn the attention of researchers in recent years. The clustering mechanism is highly successful in conserving energy resources for network activities and has become a promising field for researches to overcome the issues of battery-constrained WSNs. This research work implements zone-based clustering with the fuzzy-logic approach for dynamic cluster head (CH) selection. It aims to resolve the problem of unbalanced energy dissipation among the CHs in the network. The experimental results show that the proposed protocol outperforms the existing protocols in terms of maximizing the network lifetime.																	1562-2479	2199-3211															10.1007/s40815-020-00929-3		SEP 2020											
J								The influence of color temperature and illuminance on the touch motivation and preference of craft	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Willingness of touch; Color temperature; Illuminance; Sense of vision; Ceramic product; Silver product	HAPTIC INFORMATION	This study aims to explore the influence of different light color temperatures and lighting illuminances on two kinds of crafts regarding touch motivation, preference, and product evaluation. A total of 40 subjects are recruited in this study. The independent variables include lighting color temperature (3500 K, 4500 K, and 5500 K) and illuminance (500 lx, 1000 lx, 1500 lx, and 2000 lx), and three different kinds of dependent variables are measured, including motivation to touch, preference, and 10 product evaluation questions, where each is ranked on a five point scale. The experimental results show that a pottery teapot is suitable for 3500 K at 1000 lx and a silver teapot is suitable for 4500 K at 1500 lx, while a color temperature of 5500 K or illuminance of 500 lx produces the lowest touch motivation and preference. The results of multiple regression analysis show that, if the lighting illuminance can improve viewers' preference for the sample or show good touch on the surface, then their touch motivation will be stronger. The findings of this study can provide insight into the motivation of touch, and further provide some guidelines and recommendations regarding display lighting, in order to increase the competitive advantages of works.																	1868-5137	1868-5145															10.1007/s12652-020-02529-3		SEP 2020											
J								A combined big data analytics and Fuzzy DEMATEL technique to improve the responsiveness of automotive supply chains	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Supply chain responsiveness (SCR); Sentiment analysis; Demand forecasting; Neural network; Fuzzy DEMATEL technique	ONLINE REVIEWS; MODEL; OPTIMIZATION; DYNAMICS; IMPACT; ANP	The vital task of improving the Responsiveness of the automotive supply chains is to forecast the demand and analyze the vehicle's most influential attributes. The purpose of this paper is to develop a model to forecast the demand and analyzing the vehicle attributes using a combined approach of big data analytics and fuzzy decision-making trial and evaluation laboratory (DEMATEL) technique. The forecasting process includes the sentiment analysis of product review and creating a predictive model using an artificial neural network algorithm. The most influential attributes of the vehicle were extracted from online customer reviews and these attributes were analyzed using the Fuzzy DEMATEL method. A newly introduced vehicle in the Mid- SUV segment of the Indian automotive sector has been chosen as a case to illustrate the developed model. The forecasted demand shows an accuracy of 95.5% and the price of the vehicle and safety features are identified as attributes with higher prominence value.																	1868-5137	1868-5145															10.1007/s12652-020-02524-8		SEP 2020											
J								Suppression of low frequency oscillations in power equipment at the harbor based on NSGA-II	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Low-frequency oscillation; NSGA2; Power system stabilizer; Static var compensator		When the power company is cutting and sending electricity, low frequency oscillation will occur in the power equipment at the harbor, which causes equipments to fail to function properly. This paper proposes an improved NSGA-II to coordinately control the power system, with the power system stabilizer and static reactive power compensator. The classical single machine system is simulated and analyzed in Matlab/Simulink software. The Strength Pareto Evolutionary Algorithm is selected to compare and analyze the different performance with the improved genetic algorithm which proposed in this paper. The results show that the PSS and SVC optimized by NSGA-II can improve the damping ratio of power system and restrain the low-frequency oscillation more effectively.																	1868-5137	1868-5145															10.1007/s12652-020-02540-8		SEP 2020											
J								HO pi in Coq	JOURNAL OF AUTOMATED REASONING										Higher-order process calculus; Howe's method; Coq	CALCULUS; BISIMULATION; EQUIVALENCE; CONGRUENCE; SYSTEM	We present a formalization of HO pi in Coq, a process calculus where messages carry processes. Such a higher-order calculus features two very different kinds of binder: process input, similar to lambda-abstraction, and name restriction, whose scope can be expanded by communication. For the latter, we compare four approaches to represent binders: locally nameless, de Bruijn indices, nominal, and Higher-Order Abstract Syntax. In each case, we formalize strong context bisimilarity and prove it is compatible, i.e., closed under every context, using Howe's method, based on several proof schemes we developed in a previous paper.																	0168-7433	1573-0670															10.1007/s10817-020-09553-0		SEP 2020											
J								Cross-domain recommender system using generalized canonical correlation analysis	KNOWLEDGE AND INFORMATION SYSTEMS										Collaborative filtering; Cross-domain recommender system; Generalized canonical correlation analysis; Transfer learning	SPARSITY	Recommender systems provide personalized recommendations to the users from a large number of possible options in online stores. Matrix factorization is a well-known and accurate collaborative filtering approach for recommender system, which suffers from cold-start problem for new users and items. When new users join the system, it will take some time before they enter some ratings in the system, until that time, there are not enough ratings to learn the matrix factorization model. Using auxiliary data such as user's demographic, ratings and reviews in relevant domains, is an effective solution to reduce the new user problem. In this paper, we used the data of users activity from auxiliary domains to build domain-independent users representation that could be used to predict users ratings in the target domains. We proposed an iterative method which applied MAX-VAR generalized canonical correlation analysis (GCCA) on user's latent factors learned from matrix factorization on each domain. Also, to improve the capability of GCCA to learn latent factors for new users, we propose a generalized canonical correlation analysis by inverse sum of selection matrices (GCCA-ISSM) approach, which provides better recommendations in cold-start scenarios. The proposed approach is extended using content-based features like topic models extracted from user's reviews. We demonstrate the accuracy and effectiveness of the proposed approaches on cross-domain rating predictions using comprehensive experiments on Amazon and MovieLens datasets.																	0219-1377	0219-3116															10.1007/s10115-020-01499-4		SEP 2020											
J								Benchmarking deep network architectures for ethnicity recognition using a new large face dataset	MACHINE VISION AND APPLICATIONS										Ethnicity recognition; Face analysis; Soft biometrics; Dataset; Deep learning; Benchmark	FEATURES; DATABASE; GENDER; AGE	Although in recent years we have witnessed an explosion of the scientific research in the recognition of facial soft biometrics such as gender, age and expression with deep neural networks, the recognition of ethnicity has not received the same attention from the scientific community. The growth of this field is hindered by two related factors: on the one hand, the absence of a dataset sufficiently large and representative does not allow an effective training of convolutional neural networks for the recognition of ethnicity; on the other hand, the collection of new ethnicity datasets is far from simple and must be carried out manually by humans trained to recognize the basic ethnicity groups using the somatic facial features. To fill this gap in the facial soft biometrics analysis, we propose the VGGFace2 Mivia Ethnicity Recognition (VMER) dataset, composed by more than 3,000,000 face images annotated with 4 ethnicity categories, namely African American, East Asian, Caucasian Latin and Asian Indian. The final annotations are obtained with a protocol which requires the opinion of three people belonging to different ethnicities, in order to avoid the bias introduced by the well-known other race effect. In addition, we carry out a comprehensive performance analysis of popular deep network architectures, namely VGG-16, VGG-Face, ResNet-50 and MobileNet v2. Finally, we perform a cross-dataset evaluation to demonstrate that the deep network architectures trained with VMER generalize on different test sets better than the same models trained on the largest ethnicity dataset available so far. The ethnicity labels of the VMER dataset and the code used for the experiments are available upon request at.																	0932-8092	1432-1769				SEP 14	2020	31	7-8							67	10.1007/s00138-020-01123-z													
J								A deep learning-based driver distraction identification framework over edge cloud	NEURAL COMPUTING & APPLICATIONS										Driver distraction detection; Convolutional neural networks (CNNs); Deep learning; VGG16; Cloud computing; Raspberry Pi	SYSTEM	Currently, the number of traffic accidents has been increased globally. One of the main reasons for this increase is the distraction of the driver on the road. Distracted driving can cause collisions and cause injury, death, or property damage. New techniques can help to mitigate this problem, and one of the recent approaches is to employ body wearable sensors or camera sensors in the vehicle for real-time monitoring and detection of drivers' distraction and behaviors, such as cell phone use, talking, eating, drinking, radio tuning, navigation interaction, or even combing hair while driving. However, this type of approach requires not only a powerful training module but also a lightweight module for real-time detection and analyzing the captured data. Data need to be collected from specific wearable or camera sensors in order to detect drivers' distraction and ensure immediate feedback by the administrator for safe driving. Therefore, in this paper, we propose an effective camera-based framework for real-time identification of drivers' distraction by using a deep learning approach with edge and cloud computing technologies. More specifically, the framework consists of three modules, including the distraction detection module deployed on edge devices in the vehicle environment, the training module deployed in the cloud environment, and finally the analyzing module implemented in the monitoring environment (administrator side) connected with a telecommunication network. The proposed framework is developed using two deep learning models. The first is a custom deep convolutional neural network (CDCNN) model, and the second one is a visual geometry group-16 (VGG16)-based fine-tuned model. Several experiments are conducted on a public large-scale driver distraction dataset to evaluate the two models. The experimental results show that the accuracy rates were 99.64% for the first model and 99.73% for the second model using a holdout test set of 10%. In addition, the first and second models have achieved accuracy rates of 99.36% and 99.57% using a holdout test set of 30%. The results confirmed the applicability and appropriateness of the adopted deep learning models for designing the proposed driver distraction detection framework.																	0941-0643	1433-3058															10.1007/s00521-020-05328-1		SEP 2020											
J								Prescribed-time convergent and noise-tolerant Z-type neural dynamics for calculating time-dependent quadratic programming	NEURAL COMPUTING & APPLICATIONS										Z-type neural dynamics; Nonlinear activation function; Prescribed-time convergence; Noise tolerance; Time-dependent QP	NETWORK; OPTIMIZATION; ZNN; DESIGN	Neural-dynamics methods for solving quadratic programming (QP) have been studied for decades. The main feature of a neural-dynamics solver is that it can generate a continuous path from the initial point, and the final path will converge to the solution. In particular, the Z-type neural dynamics (ZND) that has emerged in recent years shows that it can completely converge to the ideal solution for the real-time-dependent QP in the ideal situation, i.e., without noise. It is worth noting that noise substantially influences the accuracy of neural-dynamics models in the process of solving the problems. Nevertheless, the existing neural-dynamics methods show limited capacity of noise tolerance, which may seriously affect their application in practical problems. By exploiting the Z-type design formula and two nonlinear activation functions, this work proposes a prescribed-time convergent and noise-tolerant ZND (PTCNTZND) model for calculating real-time-dependent QPs under noisy environments. Theoretical analyses of the PTCNTZND model show that it can be accelerated to prescribed-time convergence to the time-dependent optimal solution, and has natural anti-noise ability. The upper bound of the convergence time is also derived theoretically. Finally, the performance of the PTCNTZND model was verified by experiments, and the results substantiate the excellent robustness and convergence characteristics of the proposed PTCNTZND model for calculating real-time-dependent QPs, as compared with the existing ZND models.																	0941-0643	1433-3058															10.1007/s00521-020-05356-x		SEP 2020											
J								Impact of data smoothing on semantic segmentation	NEURAL COMPUTING & APPLICATIONS										Semantic segmentation; SegNet; Smoothing; Deep learning	GENETIC ALGORITHM; COLOR; CATEGORIZATION; IMAGES	Semantic segmentation is the process to classify each pixel of an image. The current state-of-the-art semantic segmentation techniques use end-to-end trainable deep models. Generally, the training of these models is controlled by some external hyper-parameters rather to use the variation in data. In this paper, we investigate the impact of data smoothing on the training and generalization of deep semantic segmentation models. A mechanism is proposed to select the best level of smoothing to get better generalization of the deep semantic segmentation models. Furthermore, a smoothing layer is included in the deep semantic segmentation models to automatically adjust the level of smoothing. Extensive experiments are performed to validate the effectiveness of the proposed smoothing strategies.																	0941-0643	1433-3058															10.1007/s00521-020-05341-4		SEP 2020											
J								The distributed ledger technology as a measure to minimize risks of poor-quality pharmaceuticals circulation	PEERJ COMPUTER SCIENCE										Blockchain; Distributed ledger technology; Pharmacology; Pharmacy; Supply chain; Medicine		Background. In the modern world, millions of people suffer from fake and poor-quality medical products entering the market. Violation of the rules of transportation of drugs makes them ineffective and even dangerous. The relationship between the various parts of the supply chain, production and regulation of drugs is too hard and has many problems. Distributed ledger technology is a distributed database, the properties of which allow us to track the entire path of medical products from the manufacturer to consumer, to improve the current model of the supply chain, to transform the pharmaceutical industry and prevent falsified drugs reach the market. Objective. The aim of the article is to analyze the distributed ledger technology as an innovative means of poor-quality pharmaceuticals prevention to reach the market as well as their forehanded detection. Methods. Content analysis of web sites of companies developing distributed ledger technology solutions had been performed. Five examples found with a google search engine by keywords "distributed ledger technology'', "blockchain'', "pharmaceuticals'' and "supply chain'' were examined. Analysis of relative scientific publications had been made. With the help of generalization and systematization methods, services provided by these companies were analyzed. The visual model of the supply chain was created with Microsoft Visio software. Results. The analysis results contain a principle scheme of distributed ledger technology implementation to achieve the objectives. The analysis of present-day pharmaceuticals supply chain structure and the distributed ledger technology capacities to improve pharmaceutical companies has been carried out and presented. Furthermore, the article allows getting acquainted with today's projects released to the market as well as the prognosis of the distributed ledger technology in pharmaceutical industry enhancement in the future.																	2376-5992					SEP 14	2020									e292	10.7717/peerj-cs.292													
J								Managing slow-moving item: a zero-inflated truncated normal approach for modeling demand	PEERJ COMPUTER SCIENCE										Demand during lead time; Inventory models; Zero-inflated truncated normal statistical distribution	INTERMITTENT DEMAND; OPTIMIZATION; SYSTEMS	This paper proposes a slow-moving management method for a system using of intermittent demand per unit time and lead time demand of items in service enterprise inventory models. Our method uses zero-inflated truncated normal statistical distribution, which makes it possible to model intermittent demand per unit time using mixed statistical distribution. We conducted numerical experiments based on an algorithm used to forecast intermittent demand over fixed lead time to show that our proposed distributions improved the performance of the continuous review inventory model with shortages. We evaluated multi-criteria elements (total cost, fill-rate, shortage of quantity per cycle, and the adequacy of the statistical distribution of the lead time demand) for decision analysis using the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). We confirmed that our method improved the performance of the inventory model in comparison to other commonly used approaches such as simple exponential smoothing and Croston's method. We found an interesting association between the intermittency of demand per unit of time, the square root of this same parameter and reorder point decisions, that could be explained using classical multiple linear regression model. We confirmed that the parameter of variability of the zero-inflated truncated normal statistical distribution used to model intermittent demand was positively related to the decision of reorder points. Our study examined a decision analysis using illustrative example. Our suggested approach is original, valuable, and, in the case of slow-moving item management for service companies, allows for the verification of decision-making using multiple criteria.																	2376-5992					SEP 14	2020									e298	10.7717/peerj-cs.298													
J								Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media	PEERJ COMPUTER SCIENCE										Language modelling; Social media; Text mining; Complex networks; Cognition and language; STEM education; Cognitive network science; Social computing	COMPLEX NETWORKS; NORMS; BIAS; INFORMATION; SEMANTICS; TWITTER; VALENCE; AROUSAL	Mindset reconstruction maps how individuals structure and perceive knowledge, a map unfolded here by investigating language and its cognitive reflection in the human mind, i.e., the mental lexicon. Textual forma mentis networks (TFMN) are glass boxes introduced for extracting and understanding mindsets' structure (in Latin forma mentis) from textual data. Combining network science, psycholinguistics and Big Data, TFMNs successfully identified relevant concepts in benchmark texts, without supervision. Once validated, TFMNs were applied to the case study of distorted mindsets about the gender gap in science. Focusing on social media, this work analysed 10,000 tweets mostly representing individuals' opinions at the beginning of posts. "Gender'' and "gap'' elicited a mostly positive, trustful and joyous perception, with semantic associates that: celebrated successful female scientists, related gender gap to wage differences, and hoped for a future resolution. The perception of "woman'' highlighted jargon of sexual harassment and stereotype threat (a form of implicit cognitive bias) about women in science "sacrificing personal skills for success''. The semantic frame of "man'' highlighted awareness of the myth of male superiority in science. No anger was detected around "person'', suggesting that tweets got less tense around genderless terms. No stereotypical perception of "scientist'' was identified online, differently from real-world surveys. This analysis thus identified that Twitter discourse mostly starting conversations promoted a majorly stereotype-free, positive/trustful perception of gender disparity, aimed at closing the gap. Hence, future monitoring against discriminating language should focus on other parts of conversations like users' replies. TFMNs enable new ways for monitoring collective online mindsets, offering data-informed ground for policy making.																	2376-5992					SEP 14	2020									e295	10.7717/peerj-cs.295													
J								Creation of mutants by using centrality criteria in social network analysis	PEERJ COMPUTER SCIENCE										Mutation analysis; Finite state machine; Social network analysis; W method; Centrality	MUTATION; DESIGN	Mutation testing is a method widely used to evaluate the effectiveness of the test suite in hardware and software tests or to design new software tests. In mutation testing, the original model is systematically mutated using certain error assumptions. Mutation testing is based on well-defined mutation operators that imitate typical programming errors or which form highly successful test suites. The success of test suites is determined by the rate of killing mutants created through mutation operators. Because of the high number of mutants in mutation testing, the calculation cost increases in the testing of finite state machines (FSM). Under the assumption that each mutant is of equal value, random selection can be a practical method of mutant reduction. However, in this study, it was assumed that each mutant did not have an equal value. Starting from this point of view, a new mutant reduction method was proposed by using the centrality criteria in social network analysis. It was assumed that the central regions selected within this frame were the regions from where test cases pass the most. To evaluate the proposed method, besides the feature of detecting all failures related to the model, the widely-used W method was chosen. Random and proposed mutant reduction methods were compared with respect to their success by using test suites. As a result of the evaluations, it was discovered that mutants selected via the proposed reduction technique revealed a higher performance. Furthermore, it was observed that the proposed method reduced the cost of mutation testing.																	2376-5992					SEP 14	2020									e293	10.7717/peerj-cs.293													
J								Hydrographic data inspection and disaster monitoring using shipborne radar small range images with electronic navigation chart	PEERJ COMPUTER SCIENCE										Shipborne radar; Information fusion; Electronic navigation chart; Oil spill	DISPLAY	Shipborne radars cannot only enable navigation and collision avoidance but also play an important role in the fields of hydrographic data inspection and disaster monitoring. In this paper, target extraction methods for oil films, ships and coastlines from original shipborne radar images are proposed. First, the shipborne radar video images are acquired by a signal acquisition card. Second, based on remote sensing image processing technology, the radar images are preprocessed, and the contours of the targets are extracted. Then, the targets identified in the radar images are integrated into an electronic navigation chart (ENC) by a geographic information system. The experiments show that the proposed target segmentation methods of shipborne radar images are effective. Using the geometric feature information of the targets identified in the shipborne radar images, information matching between radar images and ENC can be realized for hydrographic data inspection and disaster monitoring.																	2376-5992					SEP 14	2020									e290	10.7717/peerj-cs.290													
J								A Context-Based Disambiguation Model for Sentiment Concepts Using a Bag-of-Concepts Approach	COGNITIVE COMPUTATION										Sentiment analysis; Sentiment concept; Disambiguation; Commonsense knowledge; Bag-of-concepts; ConceptNet; Numberbatch; Word embedding	EMBEDDINGS; NETWORK	With the widespread dissemination of user-generated content on different web sites, social networks, and online consumer systems such as Amazon, the quantity of opinionated information available on the Internet has been increased. Sentiment analysis of user-generated content is one of the main cognitive computing branches; hence, it has attracted the attention of many scholars in recent years. One of the main tasks of the sentiment analysis is to detect polarity within a text. The existing polarity detection methods mainly focus on keywords and their naive frequency counts; however, they less regard the meanings and implicit dimensions of the natural concepts. Although background knowledge plays a critical role in determining the polarity of concepts, it has been disregarded in polarity detection methods. This study presents a context-based model to solve ambiguous polarity concepts using commonsense knowledge. First, a model is presented to generate a source of ambiguous sentiment concepts based on SenticNet by computing the probability distribution. Then, the model uses a bag-of-concepts approach to remove ambiguities and semantic augmentation with the ConceptNet handling to overcome lost knowledge. ConceptNet is a large-scale semantic network with a large number of commonsense concepts. In this paper, the point mutual information (PMI) measure is used to select the contextual concepts having strong relationships with ambiguous concepts. The polarity of the ambiguous concepts is precisely detected using positive/negative contextual concepts and the relationship of the concepts in the semantic knowledge base. The text representation scheme is semantically enriched using Numberbatch, which is a word embedding model based on the concepts from the ConceptNet semantic network. In this regard, the cosine similarity metric is used to measure similarity and select a concept from the ConceptNet network for semantic augmentation. Pre-trained concepts vectors facilitate the more effective computation of semantic similarity among the concerned concepts. The proposed model is evaluated by applying a corpus of product reviews, called Semeval. The experimental results revealed an accuracy rate of 82.07%, representing the effectiveness of the proposed model.																	1866-9956	1866-9964															10.1007/s12559-020-09729-1		SEP 2020											
J								Internet of Things assisted radio frequency identification based mine safety management platform	COMPUTATIONAL INTELLIGENCE										Internet of Things; mine safety management; radio frequency identification	EARLY WARNING SYSTEM	The production of any mine depends heavily on the secure relation between human and mining equipment. Protecting and optimizing the use of these tools and workers from potential accidents by real-time monitoring and control will greatly improve mine safety and productivity. As an emerging technology, the Internet of Things (IoT) and radio frequency identification (RFID) provides a new strategy for underground production of mine security and safety. The main technologies of the Internet of Things are introduced in line with the required of mine safety production. The innovation used in this process is RFID technology, sensor technology for the conceiving of things, intelligent technology for thinking about things, and inherent security technology. In this article, the adaptive heuristic mathematical model based on IoT and RFID real-time monitoring systems has been proposed for the production of mine safety and analysis. This system allows for real-time tracking, detecting suspicious incidents, and verification of the position of a miner within the harsh underground mining environment. The major involvement of this research is the cost-efficient framework to efficiently promote the protection of underground coalmines. This system helps solve serviceability, accessibility, flexibility, and interoperability problems in coal mines.																	0824-7935	1467-8640															10.1111/coin.12369		SEP 2020											
J								A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains	INTERNATIONAL JOURNAL OF COMPUTER VISION										Weakly supervised semantic segmentation; Self-supervised Learning; Natural imaging; Digital pathology; Satellite imaging; Deep learning; Convolutional neural network	RECOGNITION; FEATURES	Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub:.																	0920-5691	1573-1405															10.1007/s11263-020-01373-4		SEP 2020											
J								Entropy-based shadowed set approximation of intuitionistic fuzzy sets	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										entropy; intuitionistic fuzzy sets; k-nearest neighbors; machine learning; shadowed sets	THEORETIC 3-WAY APPROXIMATIONS; EXTENSIONS; ALGORITHM; SYSTEMS; MODEL	We propose a method to approximate Intuitionistic Fuzzy Sets (IFSs) with Shadowed Sets that could be used, in decision making or similar tasks, when the full information about membership values is not necessary, is difficult to process or to interpret. Our approach is based on an information-theoretic perspective and aims at preserving the uncertainty, represented through an entropy measure, in the original IFS by minimizing the difference between the entropy in the input IFS and the output Shadowed Set. We propose three different efficient optimization algorithms that retain Fuzziness, Lack of Knowledge, or both, and illustrate their computation through an illustrative example. We also evaluate the application of the proposed approximation methods in the Machine Learning setting by showing that the approximation, through the proposed methods, of IFSk-Nearest Neighbors is able to outperform, in terms of running time, the standard algorithm.																	0884-8173	1098-111X				DEC	2020	35	12					2117	2139		10.1002/int.22287		SEP 2020											
J								Recommendation of users in social networks: A semantic and social based classification approach	EXPERT SYSTEMS										classification; commitment; credibility; K-means; K-Nearest Neighbours; recommendation of users; semantic filtering; social filtering; trust; Yelp social network	SYSTEMS; TRUST; SIMILARITY	Recently, the study of social network-based recommender systems has become an active research topic. The integration of the social relationships that exist between users can improve the accuracy of recommendation results since the users' preferences are similar or influenced by their connected friends. We focus in this article on the recommendation of users in social networks. Our approach is based on semantic and social representations of the users' profiles. We have formalized and illustrated these two dimensions using the Yelp social network. The novelty of our approach concerns the modelling of the credibility of the user, through his/her trust and commitment in the social network. Moreover, in order to optimize the performance of the recommendation process, we have used two classification techniques: an unsupervised technique that uses the K-means algorithm (applied initially to all users); and a supervised technique that uses the K-Nearest Neighbours algorithm (applied to newly added users). A recommendation algorithm has been proposed taking into account the cold-start and sparsity problems. A prototype of a recommender system has been developed and tested using two publicly available datasets: the Yelp database and the Rich Epinions database. The comparative evaluation results show the effectiveness of combining the semantic, the social and the credibility information in an approach that appropriately uses the K-means and K-Nearest Neighbours algorithms.																	0266-4720	1468-0394														e12634	10.1111/exsy.12634		SEP 2020											
J								The design and research of a new pharmaceuticals-vending machine based on online medical service	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Interaction design; Online medical treatment; Pharmaceuticals-vending machine; Community management; Industrial design	VALIDATION; POSTURE	Under the pressure of fast-paced life, people's demand for pharmaceuticals increases, and 24-h pharmacies begin to increase. But these pharmacies cannot really provide pharmaceuticals anytime and anywhere, and at the same time, the existing pharmaceuticals-vending machine (PM) has only single function and imperfect management. The mature automated medicine distribution equipment is only used in hospitals, and the cost is higher. In order to solve the above problems, a new pharmaceuticals-vending machine (NPM) based on mobile terminal machine combines body temperature, pulse detection with pharmaceuticals purchase was designed, which breaks the traditional manual retail mode and realizes 24 h safe and efficient pharmaceuticals purchase. The new pharmaceuticals-vending machine (NPM) has some functions such as self-detecting body temperature and pulse, remote doctor consultation, accurate positioning of pharmaceuticals machine location and rapid search and purchase. Users can purchase pharmaceuticals efficiently, accurately and safely by it. The new pharmaceuticals-vending machine (NPM) can connect with the hospitals, communities and patients through APP, which can not only meet the personalized needs of users, but also manage the machine effectively.																	1868-5137	1868-5145															10.1007/s12652-020-02482-1		SEP 2020											
J								An Elastica-Driven Digital Curve Evolution Model for Image Segmentation	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Multigrid convergence; Digital estimator; Curvature; Shape optimization; Image segmentation	EULERS ELASTICA	Geometric priors have been shown to be useful in image segmentation to regularize the results. For example, the classical Mumford-Shah functional uses region perimeter as prior. This has inspired much research in the last few decades, with classical approaches like the Rudin-Osher-Fatemi and most graph-cut formulations, which all use a weighted or binary perimeter prior. It has been observed that this prior is not suitable in many applications, for example for segmenting thin objects or some textures, which may have high perimeter/surface ratio. Mumford observed that an interesting prior for natural objects is the Euler elastical model, which involves the squared curvature. In other areas of science, researchers have noticed that some physical binarization processes, like emulsion unmixing, can be well-approximated by curvature-related flow like the Willmore flow. However, curvature-related flows are not easy to compute because curvature is difficult to estimate accurately, and the underlying optimization processes are not convex. In this article, we propose to formulate a digital flow that approximates an Elastica-related flow using a multigrid-convergent curvature estimator, within a discrete variational framework. We also present an application of this model as a post-processing step to a segmentation framework.																	0924-9907	1573-7683															10.1007/s10851-020-00983-4		SEP 2020											
J								Smart healthcare solutions using the internet of medical things for hand gesture recognition system	COMPLEX & INTELLIGENT SYSTEMS										Gesture recognition; IoMT; Machine learning; Patient monitoring; Smart healthcare		Patient gesture recognition is a promising method to gain knowledge and assist patients. Healthcare monitoring systems integrated with the Internet of Things (IoT) paradigm to perform the remote solutions for the acquiring inputs. In recent years, wearable sensors, and information and communication technologies are assisting for remote monitoring and recommendations in smart healthcare. In this paper, the dependable gesture recognition (DGR) using a series learning method for identifying the action of patient monitoring through remote access is presented. The gesture recognition systems connect to the end-user (remote) and the patient for instantaneous gesture identification. The gesture is recognized by the analysis of the intermediate and structuring features using series learning. The proposed gesture recognition system is capable of monitoring patient activities and differentiating the gestures from the regular actions to improve the convergence. Gesture recognition through remote monitoring is indistinguishable due to the preliminary errors. Further, it is convertible using series learning. Therefore, the misdetections and classifications are promptly identified using the DGR and verified by comparative analysis and experimental study. From the analysis, the proposed DGR approach attains 94.92% high precision for the varying gestures and 89.85% high accuracy for varying mess factor. The proposed DGR reduces recognition time to 4.97 s and 4.93 s for the varying gestures and mess factor, respectively.																	2199-4536	2198-6053															10.1007/s40747-020-00194-9		SEP 2020											
J								Deep understanding of shopper behaviours and interactions using RGB-D vision	MACHINE VISION AND APPLICATIONS										Intelligent retail environment; RGB-D camera; Deep learning; Human behaviour analysis	REAL-TIME TRACKING; MULTIPLE HUMANS; PEOPLE; SENSOR; ROBUST	In retail environments, understanding how shoppers move about in a store's spaces and interact with products is very valuable. While the retail environment has several favourable characteristics that support computer vision, such as reasonable lighting, the large number and diversity of products sold, as well as the potential ambiguity of shoppers' movements, mean that accurately measuring shopper behaviour is still challenging. Over the past years, machine-learning and feature-based tools for people counting as well as interactions analytic and re-identification were developed with the aim of learning shopper skills based on occlusion-free RGB-D cameras in a top-view configuration. However, after moving into the era of multimedia big data, machine-learning approaches evolved into deep learning approaches, which are a more powerful and efficient way of dealing with the complexities of human behaviour. In this paper, a novel VRAI deep learning application that uses three convolutional neural networks to count the number of people passing or stopping in the camera area, perform top-view re-identification and measure shopper-shelf interactions from a single RGB-D video flow with near real-time performances has been introduced. The framework is evaluated on the following three new datasets that are publicly available: TVHeads for people counting, HaDa for shopper-shelf interactions and TVPR2 for people re-identification. The experimental results show that the proposed methods significantly outperform all competitive state-of-the-art methods (accuracy of 99.5% on people counting, 92.6% on interaction classification and 74.5% on re-id), bringing to different and significative insights for implicit and extensive shopper behaviour analysis for marketing applications.																	0932-8092	1432-1769				SEP 13	2020	31	7-8							66	10.1007/s00138-020-01118-w													
J								The brain, the artificial neural network and the snake: why we see what we see	AI & SOCIETY										Human vision; Machine vision; Brain; Artificial neural network; Visual appreciation		For millions of years, biological creatures have dealt with the world without being able to see it; however, the change in the atmospheric condition during the Cambrian period and the subsequent increase of light, triggered the sudden evolution of vision and the consequent evolutionary benefits. Nevertheless, how from simple organisms to more complex animals have been able to generate meaning from the light who fell in their eyes and successfully engage the visual world remains unknown. As shown by many psychophysical experiments, biological visual systems cannot measure the physical properties of the world. The light projected onto the retina is, in fact, unable to specify the physical properties of the world in which humans and other visually 'intelligent' animals behave; however, visual behaviours are habitually successful. Through psychophysical evidence, examples of the functioning of Artificial Neural Networks (ANNs) and a reflection upon visual appreciation in the cultural and artistic context, this paper shows (a) how vision emerged by randomtrial and errorduring evolution and lifetime learning; (b) how the functioning of ANNs may provide evidence and insights on how machine and human vision works; and (c) how rethinking vision theory in terms of trial and error may offer a new approach to better understand vision-biological and artificial-and reveal new insights intowhy we like what we like.																	0951-5666	1435-5655															10.1007/s00146-020-01065-0		SEP 2020											
J								An enhanced class topper algorithm based on particle swarm optimizer for global optimization	APPLIED INTELLIGENCE										Adaptive performance adjustment; Class topper optimization; Intensive crowded sorting; Particle swarm optimization	SEARCH; EVOLUTIONARY	Class topper optimization (CTO) algorithm divides the initial swarm into several sub-swarms, and this causes it to possess a strong exploration ability throughout optimization. It however randomly selects best-ranked particles as section toppers (ST's) and class topper (CT), and the inability of every particle to directly learn from the CT causes slow convergence during the latter stages of iterations. To overcome the algorithm's deficiency and find a good balance between exploration and exploitation, this study proposes an enhanced CTO (ECTPSO) based on the social learning characteristics of particle swarm optimization (PSO). We created an external archive called the assertive repository (AR) to store best-ranked particles and employed the Karush-Kuhn-Tucker (KKT) proximity measure to assist in the selection of STs and CT. Also, the intensive crowded sorting (ICS) is developed to truncate the AR when it exceeds its maximum size limit. To further encourage exploitation and avert particles from getting trapped in local optimum, we incorporated an adaptive performance adjustment strategy (APA) into our framework to activate particles when they are stagnated. The CEC2017 test suite is employed to evaluate the effectiveness of the proposed algorithm and four other benchmark peer algorithms. The results show that our proposed method possesses a better capability to elude local optima with faster convergence than the other peer algorithms. Furthermore, the algorithms were applied to economic load dispatch (ELD), of which our proposed algorithm demonstrated its effectiveness and competitiveness to address optimization problems.																	0924-669X	1573-7497															10.1007/s10489-020-01856-4		SEP 2020											
J								A novel discretization algorithm based on multi-scale and information entropy	APPLIED INTELLIGENCE										Data mining; Discretization; Information entropy; Multi-scale; MDLPC criterion	CONTINUOUS ATTRIBUTES	Discretization is one of the data preprocessing topics in the field of data mining, and is a critical issue to improve the efficiency and quality of data mining. Multi-scale can reveal the structure and hierarchical characteristics of data objects, the representation of the data in different granularities will be obtained if we make a reasonable hierarchical division for a research object. The multi-scale theory is introduced into the process of data discretization and a data discretization method based on multi-scale and information entropy called MSE is proposed. MSE first conducts scale partition on the domain attribute to obtain candidate cut point set with different granularity. Then, the information entropy is applied to the candidate cut point set, and the candidate cut point with the minimum information entropy is selected and detected in turn to determine the final cut point set using the MDLPC criterion. In such way, MSE avoids the problem that the candidate cut points are limited to only certain limited attribute values caused by considering only the statistical attribute values in the traditional discretization methods, and reduces the number of candidates by controlling the data division hierarchy to an optimal range. Finally, the extensive experiments show that MSE achieves high performance in terms of discretization efficiency and classification accuracy, especially when it is applied to support vector machines, random forest, and decision trees.																	0924-669X	1573-7497															10.1007/s10489-020-01850-w		SEP 2020											
J								COVID-19 detection and disease progression visualization: Deep learning on chest X-rays for classification and coarse localization	APPLIED INTELLIGENCE										Activation maps; COVID-19; Deep neural networks; Transfer learning		Chest X-rays are playing an important role in the testing and diagnosis of COVID-19 disease in the recent pandemic. However, due to the limited amount of labelled medical images, automated classification of these images for positive and negative cases remains the biggest challenge in their reliable use in diagnosis and disease progression. We implemented a transfer learning pipeline for classifying COVID-19 chest X-ray images from two publicly available chest X-ray datasets(1,2). The classifier effectively distinguishes inflammation in lungs due to COVID-19 and Pneumonia from the ones with no infection (normal). We have used multiple pre-trained convolutional backbones as the feature extractor and achieved an overall detection accuracy of 90%, 94.3%, and 96.8% for the VGG16, ResNet50, and EfficientNetB0 backbones respectively. Additionally, we trained a generative adversarial framework (a CycleGAN) to generate and augment the minority COVID-19 class in our approach. For visual explanations and interpretation purposes, we implemented a gradient class activation mapping technique to highlight the regions of the input image that are important for predictions. Additionally, these visualizations can be used to monitor the affected lung regions during disease progression and severity stages.																	0924-669X	1573-7497															10.1007/s10489-020-01867-1		SEP 2020											
J								Machine learning model-based two-dimensional matrix computation model for human motion and dance recovery	COMPLEX & INTELLIGENT SYSTEMS										Computation model; Machine learning; Human motion; Two-dimensional matrix; Neural		Many regions of human movement capturing are commonly used. Still, it includes a complicated capturing method, and the obtained information contains missing information invariably due to the human's body or clothing structure. Recovery of motion that aims to recover from degraded observation and the underlying complete sequence of motion is still a difficult task, because the nonlinear structure and the filming property is integrated into the movements. Machine learning model based two-dimensional matrix computation (MM-TDMC) approach demonstrates promising performance in short-term motion recovery problems. However, the theoretical guarantee for the recovery of nonlinear movement information lacks in the two-dimensional matrix computation model developed for linear information. To overcome this drawback, this study proposes MM-TDMC for human motion and dance recovery. The advantages of the machine learning-based Two-dimensional matrix computation model for human motion and dance recovery shows extensive experimental results and comparisons with auto-conditioned recurrent neural network, multimodal corpus, low-rank matrix completion, and kinect sensors methods.																	2199-4536	2198-6053															10.1007/s40747-020-00186-9		SEP 2020											
J								Path Tracking Control for Autonomous Harvesting Robots Based on Improved Double Arc Path Planning Algorithm	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Autonomous harvesting robot; Double arc; Path tracking; Pure pursuit; Control strategy	TRAJECTORY TRACKING; VEHICLE	Focusing on the problems of big overshoot and long convergence time due to the large initial heading error, a new path tracking control strategy for autonomous harvesting robots based on improved double arc path planning is designed. Firstly, the improved double arc path planning algorithm includes global path planning and local planning. The shortest path distance is taken as the goal, so the optimal tangent arc path from the initial point to the reference straight line can be automatically planned by the global path planning. Due to the problems such as control delay in harvesting robot, the global path cannot be effectively tracked. By analyzing the influence of the non-mutable steering angle and control delay on path tracking, a new local path planning algorithm is proposed. Then, the two preview points obtained in global planning can be optimized dynamically, and the actual reliability of the algorithm can be enhanced. Secondly, the error compensation model is designed to compensate the installation error of the positioning antenna in real time, to satisfy the simplified condition of the two-wheeled vehicle model. Finally, by combining the large angle steering control method and the pure pursuit algorithm, a new hybrid control strategy is designed to solve the steering angle, and the path tracking function of harvesting robots is realized. Experiment results demonstrate that the hybrid control strategy can reduce the oscillation of driving path, enhance the convergence, then, improve working quality and efficiency of robots.																	0921-0296	1573-0409															10.1007/s10846-020-01257-2		SEP 2020											
J								A POI recommendation approach integrating social spatio-temporal information into probabilistic matrix factorization	KNOWLEDGE AND INFORMATION SYSTEMS										Point of interest; Location-based social network; Probabilistic matrix factorization	OF-INTEREST	In recent years, point of interest (POI) recommendation has gained increasing attention all over the world. POI recommendation plays an indispensable role in assisting people to find places they are likely to enjoy. The exploitation of POIs recommendation by existing models is inadequate due to implicit correlations among users and POIs and cold start problem. To overcome these problems, this work proposed a social spatio-temporal probabilistic matrix factorization (SSTPMF) model that exploits POI similarity and user similarity, which integrates different spaces including the social space, geographical space and POI category space in similarity modelling. In other words, this model proposes a multivariable inference approach for POI recommendation using latent similarity factors. The results obtained from two real data sets, Foursquare and Gowalla, show that taking POI correlation and user similarity into account can further improve recommendation performance. In addition, the experimental results show that the SSTPMF model performs better in alleviating the cold start problem than state-of-the-art methods in terms of normalized discount cumulative gain on both data sets.																	0219-1377	0219-3116															10.1007/s10115-020-01509-5		SEP 2020											
J								Ambient self-powered cluster-based wireless sensor networks for industry 4.0 applications	SOFT COMPUTING										Smart grids; Energy harvesting; Routing protocols; Clustering; Wireless sensor networks; Ambient energy sources; Industry 4; 0; IIoT; Smart factory	LEADER ELECTION; PROTOCOLS; LIFETIME; DESIGN	Smart grid is one of the major prospective candidates in the Industrial Internet of Things family that ensures smooth and efficient power distribution, restoration in times of emergency, and usage control for the consumers. Electric power generators contribute at the core of smart grid along with the transmission lines and transformers. Extensive research works are conducted to optimize different parameters such as efficient energy usage, automated demand response, and emergency grid failure recovery. However, the component status analysis of the electric generators within a smart grid is still in the nascent stage. In this paper, we propose a novel routing protocol for supervised device-data transfer from smart grid generators to the command and control center using wireless ad hoc and sensor networks. Our protocol assumes various sensor devices (temperature sensors, oil level sensor, turbine status sensors, etc.) to be employed on each generator due to their mechanical sophistication. Additionally, we introduce the ambient energy harvesting for the sensors energy replenishment to accommodate tolerable node outage. Our simulation results demonstrate promising outcome with respect to different key parameters such as message flow, energy consumption, outage frequency, remaining energy, and harvested energy.																	1432-7643	1433-7479															10.1007/s00500-020-05259-y		SEP 2020											
J								Adaptive super-twisting sliding mode control for micro gyroscope based on double loop fuzzy neural network structure	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Micro gyroscope; Double loop fuzzy neural network control; Adaptive control; Super-twisting sliding mode control	TIME	In this paper, a new adaptive super-twisting sliding mode control (STSMC) scheme based on a double loop fuzzy neural network (DLFNN) is proposed to solve the problem of the external disturbances and approximate the unknown model for a micro gyroscopes. The STSMC algorithm can effectively suppress chattering since it can hide the high-frequency switching part in the high-order derivative of the sliding mode variable and transfer the discrete control law to the high-order sliding mode surface. Because it not only combines the advantages of fuzzy systems, but also incorporates the advantages of neural network control, the proposed double loop fuzzy neural network can better approximate the system model with excellent approximation. Moreover, it has the advantage of full adjustment, and the initial values of all parameters in the network can be arbitrarily set, then the parameters can be adjusted to the optimal stable value adaptively according to the adaptive algorithm. Finally, the superiority of the STSMC algorithm is also discussed. Simulation results verify the superiority of the STSMC algorithm, showing it can improve system performance and estimate unknown models more accurately compared with conventional neural network sliding mode control (CNNSMC).																	1868-8071	1868-808X															10.1007/s13042-020-01191-7		SEP 2020											
J								Fuzzy decision support modeling for internet finance soft power evaluation based on sine trigonometric Pythagorean fuzzy information	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Pythagorean fuzzy sets; Sine trigonometric operational laws; Sine trigonometric aggregation operators; Multi-criteria decision-making technique	AGGREGATION OPERATORS; SETS; NUMBERS; TODIM	The Pythagorean fuzzy set (PFS) is one of the most important concepts to accommodate more uncertainties than the intuitionistic fuzzy sets, fuzzy sets and hence its applications are more extensive. The well-known sine trigonometric function ensures the periodicity and symmetry of the origin in nature and thus satisfies the expectations of decision-makers over the parameters of the multi-time process. Keeping the features of sine function and the importance of the PFS, introduce the novel sine trigonometric operational laws (STOLs) under Pythagorean Fuzzy Settings. In addition, novel sine-trigonometric Pythagorean fuzzy aggregation operators are established based on these STOLs. The core of the study is the decision-making algorithm for addressing multi-attribute decision-making problems based on the proposed aggregation operators with unknown weight information of the given criteria. Finally, an illustrative example on internet finance soft power evaluation is provided to verify the effectiveness. Sensitivity and comparative analyses are also implemented to assess the stability and validity of our method.																	1868-5137	1868-5145															10.1007/s12652-020-02471-4		SEP 2020											
J								Hybrid nonlinear convolution filters for image recognition	APPLIED INTELLIGENCE										Gaussian convolution; Point-wise nonlinearity; Nonlinear features; Image recognition	RECEPTIVE-FIELDS	Typical convolutional filter only extract features linearly. Although nonlinearities are introduced into the feature extraction layer by using activation functions and pooling operations, they can only provide point-wise nonlinearity. In this paper, a Gaussian convolution for extracting nonlinear features is proposed, and a hybrid nonlinear convolution filter consisting of baseline convolution, Gaussian convolution and other nonlinear convolutions is designed. It can efficiently achieve the fusion of linear features and nonlinear features while preserving the advantages of traditional linear convolution filter in feature extraction. Extensive experiments on the benchmark datasets MNIST, CIFAR10, and CIFAR100 show that the hybrid nonlinear convolutional neural network has faster convergence and higher image recognition accuracy than the traditional baseline convolutional neural network.																	0924-669X	1573-7497															10.1007/s10489-020-01845-7		SEP 2020											
J								Sensor fusion based manipulative action recognition	AUTONOMOUS ROBOTS												Manipulative action recognition is one of the most important and challenging topic in the fields of image processing. In this paper, three kinds of sensor modules are used for motion, force and object information capture in the manipulative actions. Two fusion methods are proposed. Further, the recognition accuracy can be improved by using object as context. For the feature-level fusion method, significant features are chosen first. Then the Hidden Markov Models are built with these selected features to characterize the temporal sequence. For the decision-level fusion method, HMMs are built for each feature group. Then the decisions are fused. On top of these two fusion methods, the object/action context is modeled using Bayesian network. Assembly tasks are used for algorithm evaluation. The experimental results prove that the proposed approach is effective on manipulative action recognition task. The recognition accuracy of the decision-level, feature-level fusion methods and the Bayesian model are 72%, 80% and 90% respectively.																	0929-5593	1573-7527															10.1007/s10514-020-09943-8		SEP 2020											
J								Improved chaotic binary grey wolf optimization algorithm for workflow scheduling in green cloud computing	EVOLUTIONARY INTELLIGENCE										Meta-heuristic; Grey wolf optimization; Green cloud computing; Workflow scheduling	GENETIC ALGORITHM; SCIENTIFIC WORKFLOW; SEARCH ALGORITHM; KRILL HERD; ENVIRONMENT	The workflow scheduling in the cloud computing environment is a well-known NP-complete problem, and metaheuristic algorithms are successfully adapted to solve this problem more efficiently. Grey wolf optimization (GWO) is a recently proposed interesting metaheuristic algorithm to deal with continuous optimization problems. In this paper, we proposed IGWO, an improved version of the GWO algorithm which uses the hill-climbing method and chaos theory to achieve better results. The proposed algorithm can increase the convergence speed of the GWO and prevents falling into the local optimum. Afterward, a binary version of the proposed IGWO algorithm, using various S functions and V functions, is introduced to deal with the workflow scheduling problem in cloud computing data centers, aiming to minimize their executions' cost, makespan, and the power consumption. The proposed workflow scheduling scheme is simulated using the CloudSim simulator and the results show that our scheme can outperform other scheduling approaches in terms of metrics such as power consumption, cost, and makespan.																	1864-5909	1864-5917															10.1007/s12065-020-00479-5		SEP 2020											
J								Underdetermined blind source extraction of early vehicle bearing faults based on EMD and kernelized correlation maximization	JOURNAL OF INTELLIGENT MANUFACTURING										Blind source extraction; Signal decomposition; Kernelized correlation; Fault diagnosis	INDEPENDENT COMPONENT ANALYSIS; EMPIRICAL MODE DECOMPOSITION; SOURCE SEPARATION; TIME-SERIES; SIGNALS; DIAGNOSIS; ALGORITHMS; CORRENTROPY; MACHINES; MIXTURE	The incipient bearing fault diagnosis is crucial to the industrial machinery maintenance. Developed based on the blind source separation, blind source extraction (BSE) has recently become the focus of intensive research work. However, owing to certain industrial restrictions, the number of sensors is usually less than that of the source signals, which is defined as an underdetermined BSE problem to identify the fault signals. The kernelized methods are found to be robust to the noise, especially in the presence of outliers, which makes it a suitable tool to extract fault signatures submerged in the strong environment noise. Thus, this paper proposes a new underdetermined BSE method based on the empirical mean decomposition and kernelized correlation. The experimental results indicate that the extracted fault signature presents more obvious periodicity. Two important parameters of this method, including the multi-shift number and the kernel size are investigated to improve the algorithm performance. Furthermore, performance comparisons with underdetermined BSE based on the second order correlation are made to emphasize the advantage of the presented method. The application of the proposed method is validated using the simulated signal and the rolling element bearing signal of the train vehicle axle.																	0956-5515	1572-8145															10.1007/s10845-020-01655-1		SEP 2020											
J								A study of deep learning approaches for classification and detection chromosomes in metaphase images	MACHINE VISION AND APPLICATIONS										Chromosome; Classification; Deep learning; Metaphase; Detection		Chromosome analysis is an important approach to detecting genetic diseases. However, the process of identifying chromosomes in metaphase images can be challenging and time-consuming. Therefore, it is important to use automatic methods for detecting chromosomes to aid diagnosis. This work proposes a study of deep learning approaches for classification and detection of chromosome in metaphase images. Furthermore, we propose a method for detecting chromosomes, which includes new stages for preprocessing and reducing false positives and false negatives. The proposed method is evaluated using 74 chromosome images in the metaphase stage, which were obtained from the CRCN-NE database, resulting in 2174 chromosome regions. We undertake three types of evaluation: segmentation; classification of cropped regions of chromosomes; and detection of chromosomes in the original images. For the segmentation analysis, we evaluated the Otsu, adaptive, fuzzy and fuzzy-adaptive methods. For classification and detection, we evaluated the following state-of-the-art algorithms: VGG16, VGG19, Inception v3, MobileNet, Xception, Sharma and MiniVGG. The classification results showed that the proposed approach, using segmented images, obtained better results than using RGB images. Furthermore, when analyzing deep learning approaches, the VGG16 algorithm obtained the best results, using fine tuning, with a sensitivity of 0.98, specificity of 0.99 and AUC of 0.955. The results also showed that the proposed negative reduction method increased sensitivity by 18%, while maintaining the specificity value. Deep learning methods have been proved to be efficient at detecting chromosomes, but preprocessing and post-processing are important to avoid false negatives. Therefore, using binary images and adding stages for reducing false positives and false negatives are necessary in order to increase the quality of the images of the chromosomes detected.																	0932-8092	1432-1769				SEP 11	2020	31	7-8							65	10.1007/s00138-020-01115-z													
J								Somun: entity-centric summarization incorporating pre-trained language models	NEURAL COMPUTING & APPLICATIONS										Automatic text summarization; Language models; Harmonic centrality	FEATURE-EXTRACTION; CENTRALITY	Text summarization resolves the issue of capturing essential information from a large volume of text data. Existing methods either depend on the end-to-end models or hand-crafted preprocessing steps. In this study, we propose an entity-centric summarization method which extracts named entities and produces a small graph with a dependency parser. To extract entities, we employ well-known pre-trained language models. After generating the graph, we perform the summarization by ranking entities using the harmonic centrality algorithm. Experiments illustrate that we outperform the state-of-the-art unsupervised learning baselines by improving the performance more than 10% for ROUGE-1 and more than 50% for ROUGE-2 scores. Moreover, we achieve comparable results to recent end-to-end models.																	0941-0643	1433-3058															10.1007/s00521-020-05319-2		SEP 2020											
J								Developing artificial neural networks to estimate real-time onboard bus ride comfort	NEURAL COMPUTING & APPLICATIONS										Ride comfort; Bus passenger; Artificial neural network; Autonomous bus; Machine learning	SCALE; PREDICTION; PASSENGERS; TRANSPORT	The ride comfort of bus passengers is a critical factor that is recognised to attract greater ridership towards a sustainable public transport system. However, it is challenging to estimate bus passenger comfort onboard while travelling due to the complex non-linear interaction among various factors. A practicable method to collect real-time comfort ratings by passengers is also not readily available. This study developed an artificial neural network (ANN) model with three layers to precisely estimate real-time ride comfort of bus passengers. The inputs are vehicle-related parameters (speed, acceleration and jerk), passenger-related features (posture, location, facing, gender, age, weight and height), ride comfort index in ISO 2631-1997 (vibration dose value and maximum transient vibration value), and output is passenger rating (collected from a specialised mobile application). The ANN model provided a satisfactory performance and good correlation between inputs and output with an average MSE = 0.03 andR-value = 0.83, respectively. Sensitivity analysis was also conducted to quantify the relative contribution of each variable in the ANN model, revealing similar contributions among all influencing factors in the range of 4-6%. On average, passenger-related factors contribute slightly higher than vehicle-related factors to the ride comfort estimation based on the connection weight approach. The development of ANN model which can precisely estimate bus ride comfort is important as a considerable amount of machine learning and artificial intelligence are utilised to guide autonomous bus (AB). The present findings can help AB designers and engineers in improving AB technology to achieve a higher level of passengers' onboard comfort.																	0941-0643	1433-3058															10.1007/s00521-020-05318-3		SEP 2020											
J								Locate the Bounding Box of Neural Networks with Intervals	NEURAL PROCESSING LETTERS										Neural networks; Genetic algorithms; Intervals; Optimization	OPTIMIZATION; MODEL	A novel hybrid method is proposed for neural network training. The method consists of two phases: in the first phase the bounds for the neural network parameters are estimated using a genetic algorithm that uses intervals as chromosomes. In the second phase a genetic algorithm is used to train the neural network inside the bounding box located by the first phase. The proposed method is tested on a series of well-known datasets from the relevant literature and the results are reported.																	1370-4621	1573-773X															10.1007/s11063-020-10347-z		SEP 2020											
J								Adaptive Decision Threshold-Based Extreme Learning Machine for Classifying Imbalanced Multi-label Data	NEURAL PROCESSING LETTERS										Multi-label classification; Class imbalance learning; Stochastic optimization; Decision threshold moving; Extreme learning machine	SUPPORT VECTOR MACHINE; CLASSIFICATION; CLASSIFIERS; FRAMEWORK; SMOTE; ELM	Multi-label learning is a popular area of machine learning research as it is widely applicable to many real-world scenarios. In comparison with traditional binary and multi-classification tasks, the multi-label data are more easily impacted or destroyed by an imbalanced data distribution. This paper describes an adaptive decision threshold-based extreme learning machine algorithm (ADT-ELM) that addresses the imbalanced multi-label data classification problem. Specifically, the macro and micro F-measure metrics are adopted as the optimization functions for ADT-ELM, and the particle swarm optimization algorithm is employed to determine the optimal decision threshold combination. We use the optimized thresholds to make decision for future multi-label instances. Twelve baseline multi-label data sets are used in a series of experiments o verify the effectiveness and superiority of the proposed algorithm. The experimental results indicate that the proposed ADT-ELM algorithm is significantly superior to many state-of-the-art multi-label imbalance learning algorithms, and it generally requires less training time than more sophisticated algorithms.																	1370-4621	1573-773X															10.1007/s11063-020-10343-3		SEP 2020											
J								Stable roommates with narcissistic, single-peaked, and single-crossing preferences	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Stable matching; Incomplete preferences; Preferences with ties; Restricted preference domains; NP-completeness; Polynomial-time algorithms	ALGORITHM; STABILITY; CHOICE	The classicalStable Roommatesproblem is to decide whether there exists a matching of an even number of agents such that no two agents which are not matched to each other would prefer to be with each other rather than with their respectively assigned partners. We investigateStable Roommateswith complete (i.e., every agent can be matched with any other agent) or incomplete preferences, with ties (i.e., two agents are considered of equal value to some agent) or without ties. It is known that in general allowing ties makes the problem NP-complete. We provide algorithms forStable Roommatesthat are, compared to those in the literature, more efficient when the input preferences are complete and have some structural property, such as being narcissistic, single-peaked, and single-crossing. However, when the preferences are incomplete and have ties, we show that being single-peaked and single-crossing does not reduce the computational complexity-Stable Roommatesremains NP-complete.																	1387-2532	1573-7454				SEP 11	2020	34	2							53	10.1007/s10458-020-09470-x													
J								Machine learning assisted OSP approach for improved QoS performance on 3D charge-trap based SSDs	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										3D charge-trap based SSD; I; O throughput; one-shot-programming; reinforcement learning; QoS performance		Three-dimensional (3D) charge-trap based solid-state-drivers (SSDs) have become an emerging storage solution in recent years. One-shot-programming in 3D charge-trap based SSDs could deliver a maximized system input/output (I/O) throughput at the cost of degraded Quality-of-Service (QoS) performance. This paper proposes reinforcement-learning based one-shot-programming (RLOSP), a reinforcement learning based approach to improve the QoS performance for 3D charge-trap based SSDs. By learning the I/O patterns of the workload environments as well as the device internal status, the proposed approach could properly choose requests in the device queue, and allocate physical addresses for these requests during one-shot-programming. In this manner, the storage device could deliver an improved QoS performance. Experimental results reveal that the proposed approach could reduce the worst-case latency at the 99.9th percentile by 37.5%-59.2%, with an optimal system I/O throughput.																	0884-8173	1098-111X				DEC	2020	35	12					2103	2116		10.1002/int.22286		SEP 2020											
J								Data in the time of COVID-19: a general methodology to select and secure a NoSQL DBMS for medical data	PEERJ COMPUTER SCIENCE										NoSQL databases; Database security; Key-value stores NoSQL systems; Document-based stores NoSQL systems; Column-based stores NoSQL systems; Graph stores NoSQL systems; Object Store NoSQL systems; COVID-19 patients' data		Background. As the COVID-19 crisis endures and the virus continues to spread globally, the need for collecting epidemiological data and patient information also grows exponentially. The race against the clock to find a cure and a vaccine to the disease means researchers require storage of increasingly large and diverse types of information; for doctors following patients, recording symptoms and reactions to treatments, the need for storage flexibility is only surpassed by the necessity of storage security. The volume, variety, and variability of COVID-19 patient data requires storage in NoSQL database management systems (DBMSs). But with a multitude of existing NoSQL DBMSs, there is no straightforward way for institutions to select the most appropriate. And more importantly, they suffer from security flaws that would render them inappropriate for the storage of confidential patient data. Motivation. This paper develops an innovative solution to remedy the aforementioned shortcomings. COVID-19 patients, as well as medical professionals, could be subjected to privacy-related risks, from abuse of their data to community bullying regarding their medical condition. Thus, in addition to being appropriately stored and analyzed, their data must imperatively be highly protected against misuse. Methods. This paper begins by explaining the five most popular categories of NoSQL databases. It also introduces the most popular NoSQL DBMS types related to each one of them. Moreover, this paper presents a comparative study of the different types of NoSQL DBMS, according to their strengths and weaknesses. This paper then introduces an algorithm that would assist hospitals, and medical and scientific authorities to choose the most appropriate type for storing patients' information. This paper subsequently presents a set of functions, based on web services, offering a set of endpoints that include authentication, authorization, auditing, and encryption of information. These functions are powerful and effective, making them appropriate to store all the sensitive data related to patients. Results and Contributions. This paper presents an algorithm to select the most convenient NoSQL DBMS for COVID-19 patients, medical staff, and organizations data. In addition, the paper proposes innovative security solutions that eliminate the barriers to utilizing NoSQL DBMSs to store patients' data. The proposed solutions resolve several security problems including authentication, authorization, auditing, and encryption. After implementing these security solutions, the use of NoSQL DBMSs will become a much more appropriate, safer, and affordable solution to storing and analyzing patients' data, which would contribute greatly to the medical and research effort against COVID-19. This solution can be implemented for all types of NoSQL DBMSs; implementing it would result in highly securing patients' data, and protecting them from any downsides related to data leakage.																	2376-5992					SEP 10	2020									e297	10.7717/peerj-cs.297													
J								Protein structure prediction in an atomic model with differential evolution integrated with the crowding niching method	NATURAL COMPUTING										Protein structure prediction; Differential evolution; Evolutionary computing niching methods; Crowding niching method	ALGORITHM	A hybrid version between differential evolution and the fragment replacement technique was defined for protein structure prediction. The coarse-grained atomic model of the Rosetta system was used for protein representation. The high-dimensional and multimodal nature of protein energy landscapes requires an efficient search for obtaining the native structures with minimum energy. However, the energy model of Rosetta presents an additional difficulty, since the best energy area in the landscape does not necessarily correspond to the closest conformations to the native structure. A strategy is to obtain a diverse set of protein conformations that correspond to different minima in the landscape. The incorporation of the crowding niching method into the hybrid evolutionary algorithm allows addressing the problem of the energy landscape deceptiveness, allowing to obtain a set of optimized and diverse protein folds.																	1567-7818	1572-9796															10.1007/s11047-020-09801-7		SEP 2020											
J								Hybridizing ant lion with whale optimization algorithm for compressed sensing MR image reconstruction vial(1)minimization: an ALWOA strategy	EVOLUTIONARY INTELLIGENCE										Ant lion optimization algorithm (ALOA); Compressed sensing; Convergence; Image reconstruction; Whale optimization algorithm (WOA)		In the field of image compression, the compressed sensing image reconstruction has made great achievements due to proper use of the image sparsity without the Nyquist sampling law constraint. For image information distribution, great deals of researches indicate that there exists obvious structural and statistical prior's regularity and by the traditional compression algorithm it is difficult to achieve. In this paper, we propose a compressed sensing image reconstruction algorithm based on hybrid ALWOA strategy, which combines the ant lion optimization algorithm and the whale optimization algorithm. The hybrid algorithm produces a global search with faster convergence. By continuously learning the proposed hybrid method can find optimal solutions. The objective function for the image reconstruction process is taken as thel(1)minimization problem. The reconstructed image is obtained by solving thel(1)minimization problem. Extensive simulations have been conducted and the results show that the proposed method has achieved better performance when compared with traditional reconstruction algorithms.																	1864-5909	1864-5917															10.1007/s12065-020-00475-9		SEP 2020											
J								Detection of Parkinson's disease from handwriting using deep learning: a comparative study	EVOLUTIONARY INTELLIGENCE										HandPDMultiMC dataset; Parkinson's disease (PD); CNN; CNN-BLSTM; Handwriting; Data augmentation; Transfer learning		Degenerative disorders such as Parkinson's disease (PD) have an influence on daily activities due to rigidity of muscles, tremor or cognitive impairment. Micrographia, speech intensity, and deficient generation of voluntary saccadic eye movements (Pretegiani and Optican in Front Neurol 8:592, 2017) are manifestations of PD that can be used to devise noninvasive and low cost clinical tests. In this context, we have collected a multimodal dataset that we call Parkinson's disease Multi-Modal Collection (PDMultiMC), which includes online handwriting, speech signals, and eye movements recordings. We present here the handwriting dataset that we call HandPDMultiMC that will be made publicly available. The HandPDMultiMC dataset includes handwriting samples from 42 subjects (21 PD and 21 controls). In this work we investigate the application of various Deep learning architectures, namely the CNN and the CNN-BLSTM, to PD detection through time series classification. Various approaches such as Spectrograms have been applied to encode pen-based signals into images for the CNN model, while the raw time series are directly used in the CNN-BLSTM. In order to train these models for PD detection on large scale data, various data augmentation approaches for pen-based signals are proposed. Experimental results on our dataset show that the best performance for early PD detection (97.62% accuracy) is reached by a combination of CNN-BLSTM models trained with Jittering and Synthetic data augmentation approaches. We also illustrate that deep architectures can surpass the models trained on pre-engineered features even though the available data is small.																	1864-5909	1864-5917															10.1007/s12065-020-00470-0		SEP 2020											
J								Volume Sweeping: Learning Photoconsistency for Multi-View Shape Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION										Multi view stereo reconstruction; Learned photoconsistency; Performance capture; Volume sweeping	STEREO	We propose a full study and methodology for multi-view stereo reconstruction with performance capture data. Multi-view 3D reconstruction has largely been studied with general, high resolution and high texture content inputs, where classic low-level feature extraction and matching are generally successful. However in performance capture scenarios, texture content is limited by wider angle shots resulting in smaller subject projection areas, and intrinsically low image content of casual clothing. We present a dedicated pipeline, based on a per-camera depth map sweeping strategy, analyzing in particular how recent deep network advances allow to replace classic multi-view photoconsistency functions with one that is learned. We show that learning based on a volumetric receptive field around a 3D depth candidate improves over using per-view 2D windows, giving the photoconsistency inference more visibility over local 3D correlations in viewpoint color aggregation. Despite being trained on a standard dataset of scanned static objects, the proposed method is shown to generalize and significantly outperform existing approaches on performance capture data, while achieving competitive results on recent benchmarks.																	0920-5691	1573-1405															10.1007/s11263-020-01377-0		SEP 2020											
J								J(A)over-capA-Net: Joint Facial Action Unit Detection and Face Alignment Via Adaptive Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION										Joint learning; Facial AU detection; Face alignment; Adaptive attention learning	REPRESENTATION; 3D	Facial action unit (AU) detection and face alignment are two highly correlated tasks, since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. However, most existing AU detection works handle the two tasks independently by treating face alignment as a preprocessing, and often use landmarks to predefine a fixed region or attention for each AU. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared feature is learned firstly, and high-level feature of face alignment is fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment feature and global feature for AU detection. Extensive experiments demonstrate that our framework (i) significantly outperforms the state-of-the-art AU detection methods on the challenging BP4D, DISFA, GFT and BP4D+ benchmarks, (ii) can adaptively capture the irregular region of each AU, (iii) achieves competitive performance for face alignment, and (iv) also works well under partial occlusions and non-frontal poses. The code for our method is available at https://github.com/ZhiwenShao/PyTorch-JAANet.																	0920-5691	1573-1405															10.1007/s11263-020-01378-z		SEP 2020											
J								Beyond Covariance: SICE and Kernel Based Visual Feature Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Covariance matrix; Structure sparsity; Sparse inverse covariance estimate; Kernel matrix; Visual representation	REGION COVARIANCE; DIFFUSION TENSOR; CLASSIFICATION; RECOGNITION; SELECTION; MATRICES	The past several years have witnessed increasing research interest on covariance-based feature representation. Originally proposed as a region descriptor, it has now been used as a general representation in various recognition tasks, demonstrating promising performance. However, covariance matrix has some inherent shortcomings such as singularity in the case of small sample, limited capability in modeling complicated feature relationship, and a single, fixed form of representation. To achieve better recognition performance, this paper argues that more capable and flexible symmetric positive definite (SPD)-matrix-based representation shall be explored, and this is attempted in this work by exploiting prior knowledge of data and nonlinear representation. Specifically, to better deal with the issues of small number of feature vectors and high feature dimensionality, we propose to exploit the structure sparsity of visual features and exemplify sparse inverse covariance estimate as a new feature representation. Furthermore, to effectively model complicated feature relationship, we propose to directly compute kernel matrix over feature dimensions, leading to a robust, flexible and open framework of SPD-matrix-based representation. Through theoretical analysis and experimental study, the proposed two representations well demonstrate their advantages over the covariance counterpart in skeletal human action recognition, image set classification and object classification tasks.																	0920-5691	1573-1405															10.1007/s11263-020-01376-1		SEP 2020											
J								Toward better prediction of recurrence for Cushing's disease: a factorization-machine based neural approach	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Cushing's disease; Recurrence; Deep learning; Transsphenoidal surgery; Local interpretable model-agnostic explanations	LONG-TERM REMISSION; TRANSSPHENOIDAL SURGERY; POSTOPERATIVE CORTISOL; EXPERIENCE; MORTALITY	Cushing's disease (CD) is a rare disease that occurs in 1.2-1.4 persons per million population per year. Recurrence prediction after transsphenoidal surgery (TSS) is important for determining individual treatment and follow-up strategies. Between 2000 and 2017, 354 CD patients with initial postoperative remission and long-term follow-up data were enrolled from Peking union medical college hospital (PUMCH) to predict recurrence, and PUMCH is one of the largest CD treatment centers in the world. We first investigated the effect of a factorization machine (FM)-based neural network to predict recurrence after TSS for CD. This method could automatically reduce a portion of the cross-feature selection work with acceptable parameters. We conducted a performance comparison of various algorithms on the collected dataset. To address the lack of interpretability of neural network models, we also used the local interpretable model-agnostic explanations approach, which provides an explanation in the form of relevant features of the predicted results by approximating the model behavior of the variables in a local manner. Compared with existing methods, the DeepFM model obtained the highest AUC value (0.869) and the lowest log loss value (0.256). According to the importance of each feature, three top features for the DeepFM model were postoperative morning adrenocorticotropic hormone level, age, and postoperative morning serum cortisol nadir. In the post hoc explanation phase, the above-mentioned importance-leading features made a great contribution to the prediction probability. The results showed that deep learning-based models could better aid neurosurgeons in recurrence prediction after TTS for patients with CD, and could contribute to determining individual treatment strategies.																	1868-8071	1868-808X															10.1007/s13042-020-01192-6		SEP 2020											
J								Multi-label text classification with latent word-wise label information	APPLIED INTELLIGENCE										Multi-label text classification; Labeled topic model; Word-wise label information; abel-to-label structure		Multi-label text classification (MLTC) is a significant task that aims to assign multiple labels to each given text. There are usually correlations between the labels in the dataset. However, traditional machine learning methods tend to ignore the label correlations. To capture the dependencies between the labels, the sequence-to-sequence (Seq2Seq) model is applied to MLTC tasks. Moreover, to reduce the incorrect penalty caused by the Seq2Seq model due to the inconsistent order of the generated labels, a deep reinforced sequence-to-set (Seq2Set) model is proposed. However, the label generation of the Seq2Set model still relies on a sequence decoder, which cannot eliminate the influence of the predefined label order and exposure bias. Therefore, we propose an MLTC model with latent word-wise label information (MLC-LWL), which constructs effective word-wise labeled information using a labeled topic model and incorporates the label information carried by the word and label context information through a gated network. With the word-wise label information, our model captures the correlations between the labels via a label-to-label structure without being affected by the predefined label order or exposure bias. Extensive experimental results illustrate the effectiveness and significant advantages of our model compared with the state-of-the-art methods.																	0924-669X	1573-7497															10.1007/s10489-020-01838-6		SEP 2020											
J								Attention augmented multi-scale network for single image super-resolution	APPLIED INTELLIGENCE										Single image super-resolution; Attention mechanism; Multi-scale convolution; Feature recalibration and aggregation; Local hierarchical feature fusion; Global hierarchical feature fusion		Multi-scale convolution can be used in a deep neural network (DNN) to obtain a set of features in parallel with different perceptive fields, which is beneficial to reduce network depth and lower training difficulty. Also, the attention mechanism has great advantages to strengthen representation power of a DNN. In this paper, we propose an attention augmented multi-scale network (AAMN) for single image super-resolution (SISR), in which deep features from different scales are discriminatively aggregated to improve performance. Specifically, the statistics of features at different scales are first computed by global average pooling operation, and then used as a guidance to learn the optimal weight allocation for the subsequent feature recalibration and aggregation. Meanwhile, we adopt feature fusion at two levels to further boost reconstruction power, one of which is intra-group local hierarchical feature fusion (LHFF), and the other is inter-group global hierarchical feature fusion (GHFF). Extensive experiments on public standard datasets indicate the superiority of our AAMN over the state-of-the-art models, in terms of not only quantitative and qualitative evaluation but also model complexity and efficiency.																	0924-669X	1573-7497															10.1007/s10489-020-01869-z		SEP 2020											
J								Forecasting of e-commerce transaction volume using a hybrid of extreme learning machine and improved moth-flame optimization algorithm	APPLIED INTELLIGENCE										E-commerce transaction; Prediction; Machine learning; Extreme learning machine; Moth-flame optimization	MODEL; PREDICTION; PARAMETERS	The rapid development of e-commerce has resulted in optimization of the industrial structure of Chinese enterprises and has improved the Chinese economy. E-commerce transaction volume is an evaluation index used to determine the development level of e-commerce. This study proposed a model for forecasting e-commerce transaction volume. First, a hybrid moth-flame optimization algorithm (HMFO) was proposed. The convergence ability of the HMFO algorithm was analyzed on the basis of test functions. Second, using data provided by the China Internet Network Information Center, factors influencing e-commerce transaction volume were analyzed. The input variables of the e-commerce transaction volume prediction model were selected by analyzing correlation coefficients. Finally, a hybrid extreme learning machine and hybrid-strategy-based HMFO (ELM-HMFO) method was proposed to predict the volume of e-commerce transactions. The prediction results revealed that the root mean square error of the proposed ELM-HMFO model was smaller than 0.5, and the determination coefficient was 0.99, which indicated that the forecast e-commerce transaction volume was satisfactory. The proposed ELM-HMFO model can promote the industrial upgrading and development of e-commerce in China.																	0924-669X	1573-7497															10.1007/s10489-020-01840-y		SEP 2020											
J								Optimal reconfiguration and DG integration in distribution networks considering switching actions costs using tabu search algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Reconfiguration; Distributed generation (DG); Particle swarm optimization (PSO); Tabu search algorithm (TSA); Distribution network (DN)	DISTRIBUTION FEEDER RECONFIGURATION; POWER-SYSTEM RECONFIGURATION; SOFT OPEN POINT; LOSS MINIMIZATION; RELIABILITY IMPROVEMENT; GENERATION PLACEMENT; MULTIOBJECTIVE APPROACH; LOSS REDUCTION; CUCKOO SEARCH; OPTIMIZATION	The penetration of distributed generation (DG) units has been steadily increasing in distribution networks (DNs). However, oversizing and improper locating or operating of them can increase the losses or deteriorate the voltage profile. In this regard, distribution network reconfiguration (DNR) can be envisioned as a solution to maximize the DG penetration while improving the voltage profile and minimizing the losses. So, a study on DNR with the presence of DGs is necessary, in which the switching action costs are taken into consideration, since they can impose an extra cost on the daily operations. This study proposes a solution to solve the siting, sizing, and operating of DGs and DNR problems simultaneously considering switching action costs, losses costs, and reactive power generation of DGs. To consider the reactive power limits, their capability curve (P-Q curve) is included in the model. DNR is a combinatorial optimization problem, while the entire problem is modeled as a mixed-integer nonlinear programming problem. To solve that, tabu search algorithm (TSA) as one of the most efficient global solver for combinatorial problems is used, and its results are validated by particle swarm optimization (PSO) algorithm. To enlighten the effectiveness of the proposed approach, 5 scenarios are defined on IEEE 33-bus and IEEE 69-bus test systems. The results are also compared to previous studies. It is shown that, thanks to embedding the reactive power generation and switching costs at the same time, maximum loss reductions can be realized, while the results are more realistic and reliable.																	1868-5137	1868-5145															10.1007/s12652-020-02511-z		SEP 2020											
J								Social mining-based clustering process for big-data integration	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social mining; Ambient intelligence; Big-data integration; Cluster process; Healthcare; Machine learning; Lifecare	RECOMMENDATION	With the development of information technology, ambient intelligence has been combined with various application areas so as to create new convergence service industries. Through IT convergence, human-oriented technologies for improving people's quality of life has continued to be developed. Healthcare service that has been provided along with the development of various smart IT devices makes it possible to realize more efficient healthcare of people. Therefore, along with such a medical service, the advanced lifecare service for physical and mental health has been demanded. In order to meet the healthcare demands, an advanced healthcare platform has been developed. Lifecare service has been expanded to healthcare, the disease with the highest mortality induced by complications so that the service for disease survivals have been offered. Accordingly, a big-data integration and advanced healthcare platform based on patients' life logs are developed in order for health service. In this platform, it is possible to establish an optimized model with the knowledge base and predict diseases and complications and judge a degree of risk with the use of information filtering. The conventional filtering based on a data model using scatter life logs makes use of user attribute information only for clustering so that it has low accuracy. Also, in calculating the similarity of actual users, such a method does not apply social relationships. Therefore, this study proposes a social mining based cluster process for big-data integration. The proposed method uses conventional static model information and the information extracted from the social network in order to create reliable user modeling and applies a different level of weight depending on users' relations. In the clustering process for disease survivals' health conditions, it is possible to predict their health risk. Based on the risk and expectation of healthcare event occurrence, their health conditions can be improved. Lifecare forecasting model that uses social relation performs social sequence mining using PrefixSpan to complement the weak point that spends a long time to scan it repeatedly in the candidate pattern. For performance evaluation, the social mining based cluster process was compared with a conventional cluster method. More specifically, the estimation accuracy of the conventional model-based cluster method was compared with the accuracy of the social mining based cluster process. As a result, the proposed method in the mining-based healthcare platform had better performance than the conventional model-based cluster method.																	1868-5137	1868-5145															10.1007/s12652-020-02042-7		SEP 2020											
J								On the conjunction of possibility measures under intuitionistic evidence sets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intuitionistic basic probability assignment; Possibility measures; Intuitionistic evidence sets	MULTICRITERIA DECISION-MAKING; FUZZY-SETS; PROGRAMMING METHODOLOGY; FAILURE MODE; KNOWLEDGE; NUMBERS; LOGIC	There are many uncertain issues. To address these problems, many theories and models has been proposed. The conjunction of possibility measures can conjunct different possibility measures and deal with issues of decision making, which is an interesting model and has promising prospect. Meanwhile, the intuitionistic evidence sets is based on the intuitionistic basic probability assignment, which means that the intuitionistic evidence sets can degenerate into classical basic probability assignment in some special cases. The intuitionistic evidence sets is more flexible and effective than the classical basic probability assignment. However, the conjunction of possibility measures has not been applied to intuitionistic evidence sets. So, what the conjunction of possibility measures to intuitionistic evidence sets is still an issue that need be discussed. To address this issue, this paper proposes the conjunction of possibility measures under intuitionistic evidence sets. Numerical comparison experiments are illustrated to prove the validity of the conjunction of possibility measures under intuitionistic evidence sets. The experimental results prove the proposed model can not only apply the conjunction of possibility measures to the intuitionistic evidence sets effectively, but also solve effectively the issues of decision making than other similar models.																	1868-5137	1868-5145															10.1007/s12652-020-02508-8		SEP 2020											
J								Fast and energy-efficient approximate motion estimation architecture for real-time 4 K UHD processing	JOURNAL OF REAL-TIME IMAGE PROCESSING										Approximate computing; Approximate adders; SAD; Motion estimation; Video coding; HEVC; Energy-efficiency		Approximate computing techniques exploit the characteristics of error-tolerant applications either to provide faster implementations of their computational structures or to achieve substantial improvements in terms of energy efficiency. In video encoding, the motion estimation (ME) stage, including the Integer ME (IME) and the Fractional ME (FME) steps, is the most computational intensive task and it is highly resilient to controlled losses of accuracy. In accordance, this article proposes the exploitation of approximate computing techniques to implement energy efficient dedicated hardware structures targeting the motion estimation stage of current video encoders. The designed ME architecture supports IME and FME and is able to real-time process 4 K UHD videos (3840 x 2160 pixels) at 30 frames per second, while dissipating 108.92 mW. When running at its maximum operation frequency, the architecture can process 8 K UHD videos (7680 x 4320 pixels) at 120 frames per second. The solution described in this article presents the highest throughput and the highest energy efficiency among all state-of-the-art compared works, showing that the use of approximate computing is a promising solution when implementing video encoders in dedicated hardware.																	1861-8200	1861-8219															10.1007/s11554-020-01014-6		SEP 2020											
J								Geometric consistency of triangular fuzzy multiplicative preference relation and its application to group decision making	KNOWLEDGE AND INFORMATION SYSTEMS										Triangular fuzzy multiplicative preference relation; Geometric consistency; Group decision making; Interval multiplicative preference relation	EXTENT ANALYSIS METHOD; UNCERTAINTY; MODEL; AHP	The triangular fuzzy multiplicative preference relation (TFMPR) has attracted the attention of many scholars. This paper investigates the geometric consistency of TFMPR and applies it to group decision making (GDM). Firstly, by introducing two parameters, a triangular fuzzy number is transformed into an interval. According to the geometric consistency of interval multiplicative preference relation (IMPR), the geometric consistency of TFMPR is defined. Then, two corresponding IMPRs are extracted from the TFMPR by programming models in the majority case and minority case, respectively. Using the constructed linear programming models, two interval priority weight vectors are obtained from the two extracted IMPRs, respectively. Combining two interval priority weight vectors, a linear programming model is established to derive the triangular fuzzy priority weights. Subsequently, the closeness degrees of alternatives by experts are defined to obtain the group utility indices and individual regret indices of alternatives. Then, the compromise indices of alternatives are calculated considering experts' compromise attitude. By minimizing the compromise indices of alternatives, a multi-objective programming model is constructed to obtain experts' weights. By aggregating the individual TFMPRs, the collective TFMPR is obtained to derive the triangular fuzzy priority weights. Using the arithmetic mean values, the ranking order of alternatives is generated. Therefore, a method is proposed to solve GDM with TFMPRs. Finally, a performance evaluation example of precise poverty alleviation is provided to illustrate the advantage of the proposed method.																	0219-1377	0219-3116															10.1007/s10115-020-01507-7		SEP 2020											
J								A real-time and precise ellipse detector via edge screening and aggregation	MACHINE VISION AND APPLICATIONS										Ellipse detection; Tangent lines constraint; Real time	RANDOMIZED HOUGH TRANSFORM; CALIBRATION	A fast and precise method for ellipse detection is proposed in this paper. The method aims at clearly removing the lines and curves which are not ellipse edges to improve the ellipse fitting. In arc extraction, the arcs are divided into four categories according to the gradient, and the size constraint is exploited to remove the interference lines. Then, the arc relative position constraints and the tangent lines constraint are employed to exactly group the arcs that belong to the same ellipse into a set. Finally, a post-processing approach is developed to remove the invalid ellipses. Due to the effective removal of the interference edges and the designed geometric multi-constraint, the computational costs of arc grouping and parameter estimation are dramatically reduced, and the fitting results are finely agreeable to the actual ellipse contours. The performance is evaluated with 3600 synthetic images and 1517 real images, and the experimental results demonstrate that the proposed method runs much faster than the current speed leading methods with the comparable or higherF-measure.																	0932-8092	1432-1769				SEP 10	2020	31	7-8							64	10.1007/s00138-020-01113-1													
J								Distributed cooperative control algorithm for optimal power sharing for AC microgrids using Cournot game theory	NEURAL COMPUTING & APPLICATIONS										Microgrid; Distributed control; Game theory; Islanded; Power sharing		Research on the optimal power allocation of large-scale distributed generator (DG) units based on user power generation to access microgrids (MGs) in a multi-agent system framework has recently become the focus of modern grid and energy concerns. In this paper, according to the Cournot oligopoly game, the Nash equilibrium point between the power generation company and power generation user of the MG operating in island mode is obtained. According to the obtained Nash equilibrium point, the optimal ratio of power generated by the power generation company and by the power generation user in the model is calculated. At the same time, to achieve the maximum benefit and stable operation of the MG, a distributed cooperative control algorithm based on consensus theory is proposed. This control algorithm can cause each DG to generate power according to the total consumption load. The optimal power generation ratio distribution based on the Nash equilibrium point eliminates the steady-state frequency deviation of each DG in the MG, thereby ensuring the user's power quality. The simulation results show that the control algorithm can achieve the above research goals.																	0941-0643	1433-3058															10.1007/s00521-020-05315-6		SEP 2020											
J								Finite Time Anti-synchronization of Quaternion-Valued Neural Networks with Asynchronous Time-Varying Delays	NEURAL PROCESSING LETTERS										Anti-synchronization; Asynchronous; Finite time; Quaternion-valued neural network; Time delay; Unbounded	GLOBAL EXPONENTIAL STABILITY; MIXED DELAYS; STABILIZATION	In this paper, we consider the finite time anti-synchronization (A-SYN) of master-slave coupled quaternion-valued neural networks, where the time-varying delays can be asynchronous and unbounded. Without adopting the general decomposition method, the quaternion-valued state is considered as a whole, which greatly reduces the hassle of further analysis and calculations. The designed controller is delay-free, and the terms with time delay do not need to be bounded globally. Several sufficient conditions for ensuring the finite time A-SYN are obtained under 1-norm and 2-norm respectively. The A-SYN error will be proved to evolve from the initial value to 1 in finite time, and evolve from 1 to 0 also in finite time, hence the finite time A-SYN is proved, which is called two-phases-method. Moreover, adaptive rules for control strengths are also designed to realize the finite time A-SYN. Lastly, a numerical example is presented to demonstrate the correctness and effectiveness of our obtained results.																	1370-4621	1573-773X															10.1007/s11063-020-10348-y		SEP 2020											
J								Self-adaptive potential-based stopping criteria for Particle Swarm Optimization with forced moves	SWARM INTELLIGENCE										Particle Swarm Optimization; Stopping criterion; Heuristics	CONVERGENCE	We study the variant of Particle Swarm Optimization that applies random velocities in a dimension instead of the regular velocity update equations as soon as the so-calledpotentialof the swarm falls below a certain small bound in this dimension, arbitrarily set by the user. In this case, the swarm performs aforced move. In this paper, we are interested in how, by counting the forced moves, the swarm can decide for itself to stop its movement because it is improbable to find better candidate solutions than the already-found best solution. We formally prove that when the swarm is close to a (local) optimum, it behaves like a blind-searching cloud and that the frequency of forced moves exceeds a certain, objective function-independent value. Based on this observation, we define stopping criteria and evaluate them experimentally showing that good candidate solutions can be found much faster than setting upper bounds on the iterations and better solutions compared to applying other solutions from the literature.																	1935-3812	1935-3820															10.1007/s11721-020-00185-z		SEP 2020											
J								NetHALOC: A learned global image descriptor for loop closing in underwater visualSLAM	EXPERT SYSTEMS										convolutional neural network; global image descriptor; simultaneous localization and mapping; visual loop closing	PLACE RECOGNITION; LOCALIZATION	This article presents the experimental assessment of a hash-based loop closure detection methodology for visualsimultaneous localization and mapping(SLAM), addressed to underwater autonomous vehicles. This methodology uses a new global image descriptor callednet hash-based loop closure(NetHALOC), which is learned with a simple and fastconvolutional neural network. The results using NetHALOC have been compared with the results using three different top-quality state-of-the-art global image descriptors:Net vector of locally aggregated descriptors,hash-based loop closureanddeep loop closure. The transformation from images to hashes implies a considerable reduction of data to be processed, shared, compared and transmitted in global navigation, localization or mapping tasks. Complete tests with an extensive set of real underwater imagery were done to compare the performance of the defined loop closure detection approach using the aforementioned four hashing techniques, in order to conclude which of them is the more adequate for visual SLAM applications under the sea.																	0266-4720	1468-0394														e12635	10.1111/exsy.12635		SEP 2020											
J								Evaluation of temperature compensation methods for a near-infrared calibration to predict the viscosity of micellar liquids	JOURNAL OF CHEMOMETRICS										in-line NIR; micellar liquids; PLS; temperature compensation; viscosity	INDUCED SPECTRAL VARIATION; WORMLIKE MICELLES; NIR SPECTROSCOPY; MODEL; VISCOELASTICITY; STRATEGY; MOISTURE; SUGAR	Near-infrared (NIR) spectroscopy is a popular technique for the measurement of chemical and physical properties in-line using predictive models. The success of these models in industrial settings, in terms of accuracy and precision, often relies on the removal or avoidance of non-linear spectral changes associated with fluctuating process parameters like temperature. In this work, a NIR calibration model developed to predict the viscosity of micellar liquids in-line is used to evaluate various methods designed to account for temperature fluctuations. The viscosity of these liquids can vary on average by +/- 0.5 Pa s with a 1 degrees change in temperature. The methods trialled include global linear techniques, a multivariate filter (generalised least squares weighting [GLSW]) and direct standardisation. The performances of these techniques were compared against one another based on root mean square error of prediction (RMSEP), prediction bias and rank. The best method was found to be GLSW, which was the least complex (five latent variables) and showed the lowest RMSEP (0.429 Pa s). This study provides insight into the use of recognised methods to remove temperature-induced spectral variation in a PLS model developed to predict viscosity, where both NIR spectra and the property of viscosity itself are sensitive to temperature.																	0886-9383	1099-128X														e3301	10.1002/cem.3301		SEP 2020											
J								A neurodynamic algorithm to optimize residential demand response problem of plug-in electric vehicle q	NEUROCOMPUTING											ENERGY-STORAGE; CHARGING METHOD; SMART; MANAGEMENT; DISPATCH																		0925-2312	1872-8286				SEP 10	2020	405						1	11		10.1016/j.neucom.2020.04.128													
J								WiFi-based driver?s activity recognition using multi -layer classification	NEUROCOMPUTING											SUPPORT VECTOR MACHINES; MODEL SELECTION; SYSTEM; BEHAVIOR; DISTRACTIONS; INFORMATION; PERFORMANCE; ALGORITHM; FEATURES; FATIGUE																		0925-2312	1872-8286				SEP 10	2020	405						12	25		10.1016/j.neucom.2020.04.133													
J								Synthetic-Neuroscore: Using a neuro-AI interface for evaluating generative adversarial networks	NEUROCOMPUTING											COUPLED COMPUTER VISION; TASK-DIFFICULTY; MODELS; P300																		0925-2312	1872-8286				SEP 10	2020	405						26	36		10.1016/j.neucom.2020.04.069													
J								Retinal optical coherence tomography image classification with label smoothing generative adversarial network	NEUROCOMPUTING											DIABETIC MACULAR EDEMA; SD-OCT IMAGES; AUTOMATIC SEGMENTATION; GEOGRAPHIC ATROPHY; LAYER BOUNDARIES; DEGENERATION; PATHOLOGY; PATTERNS; DISEASES; BURDEN																		0925-2312	1872-8286				SEP 10	2020	405						37	47		10.1016/j.neucom.2020.04.044													
J								Remote -sensing image retrieval with tree -triplet -classification networks	NEUROCOMPUTING											SIMILARITY																		0925-2312	1872-8286				SEP 10	2020	405						48	61		10.1016/j.neucom.2020.04.038													
J								A convolutional fuzzy min -max neural network	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 10	2020	405						62	71		10.1016/j.neucom.2020.04.003													
J								Joint group and residual sparse coding for image compressive sensing	NEUROCOMPUTING											ADAPTIVE SPARSITY; RECONSTRUCTION; REPRESENTATION; RECOVERY																		0925-2312	1872-8286				SEP 10	2020	405						72	84		10.1016/j.neucom.2020.04.065													
J								Dissipativity of impulsive matrix -valued neural networks with leakage delay and mixed delays	NEUROCOMPUTING											GLOBAL EXPONENTIAL STABILITY; COMPLEX; SYNCHRONIZATION																		0925-2312	1872-8286				SEP 10	2020	405						85	95		10.1016/j.neucom.2020.03.042													
J								Leader -following bipartite consensus of multiple uncertain Euler-Lagrange systems over signed switching digraphs	NEUROCOMPUTING											MULTIAGENT SYSTEMS; NETWORKS; TRACKING																		0925-2312	1872-8286				SEP 10	2020	405						96	102		10.1016/j.neucom.2020.04.050													
J								A biologically inspired visual integrated model for image classification	NEUROCOMPUTING											CONVOLUTIONAL NEURAL-NETWORK; OBJECT RECOGNITION; DORSAL; DECAY; INFORMATION; STREAMS; AGE																		0925-2312	1872-8286				SEP 10	2020	405						103	113		10.1016/j.neucom.2020.04.081													
J								A novel industrial process monitoring method based on improved local tangent space alignment algorithm	NEUROCOMPUTING											NONLINEAR DIMENSIONALITY REDUCTION; ROOT CAUSE DIAGNOSIS; FAULT-DETECTION; PROJECTION; SYSTEMS																		0925-2312	1872-8286				SEP 10	2020	405						114	125		10.1016/j.neucom.2020.04.053													
J								No -reference stereoscopic image quality evaluator with segmented monocular features and perceptual binocular features	NEUROCOMPUTING											SELF-SIMILARITY; PREDICTION																		0925-2312	1872-8286				SEP 10	2020	405						126	137		10.1016/j.neucom.2020.04.049													
J								RFP-Net: Receptive field -based proposal generation network for object detection	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 10	2020	405						138	148		10.1016/j.neucom.2020.04.106													
J								Sentiment aware word emb e ddings using refinement and senti-contextualized learning approach	NEUROCOMPUTING										Word embedding; Sentiment analysis; Deep learning	NEURAL-NETWORK; CLASSIFICATION; EMBEDDINGS	Most pre-trained word embeddings are achieved from context-based learning algorithms trained over a large text corpus. This leads to learning similar vectors for words that share most of their contexts, while expressing different meanings. Therefore, the complex characteristics of words cannot be fully learned by using such models. One of the natural language processing applications that suffers from this problem is sentiment analysis. In this task, two words with opposite sentiments are not distinguished well by using common pre-trained word embeddings. This paper addresses this problem and proposes two simple, but empirically effective, approaches to learn word embeddings for sentiment analysis. The both approaches exploit sentiment lexicons and take into account the polarity of words in learning word embeddings. While the first approach encodes the sentiment information of words into existing pre-trained word embeddings, the second one builds synthetic sentimental contexts for embedding models along with other semantic contexts. The word embeddings obtained from the both approaches are evaluated on several sentiment classification tasks using Skip-gram and GloVe models. Results show that both approaches improve state-of-the-art results using basic deep learning models over sentiment analysis benchmarks. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 10	2020	405						149	160		10.1016/j.neucom.2020.03.094													
J								Cascade architecture with rhetoric long short-term memory for complex sentence sentiment analysis	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 10	2020	405						161	172		10.1016/j.neucom.2020.04.055													
J								Adaptive optimal control for reference tracking independent of exo-system dynamics	NEUROCOMPUTING											DISCRETE-TIME-SYSTEMS; LINEAR-SYSTEMS; STABILIZATION																		0925-2312	1872-8286				SEP 10	2020	405						173	185		10.1016/j.neucom.2020.04.140													
J								Deep residual networks with a fully connected reconstruction layer for single image super -resolution	NEUROCOMPUTING											SUPERRESOLUTION; INTERPOLATION																		0925-2312	1872-8286				SEP 10	2020	405						186	199		10.1016/j.neucom.2020.04.030													
J								Deep attentive and semantic preserving video summarization	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 10	2020	405						200	207		10.1016/j.neucom.2020.04.132													
J								Fault detection and identification of rolling element bearings with Attentive Dense CNN	NEUROCOMPUTING											DIAGNOSIS																		0925-2312	1872-8286				SEP 10	2020	405						208	217		10.1016/j.neucom.2020.04.143													
J								ModMRF: A modularity -based Markov Random Field method for community detection	NEUROCOMPUTING											MODEL																		0925-2312	1872-8286				SEP 10	2020	405						218	228		10.1016/j.neucom.2020.04.067													
J								Wireless -sensor -network -based target localization: A semidefinite relaxation approach with adaptive threshold correction	NEUROCOMPUTING											PASSIVE LOCALIZATION; NLOS IDENTIFICATION; MITIGATION; TRACKING																		0925-2312	1872-8286				SEP 10	2020	405						229	238		10.1016/j.neucom.2020.04.046													
J								A novel versatile window function for memristor model with application in spiking neural network	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 10	2020	405						239	246		10.1016/j.neucom.2020.04.111													
J								Reducing human efforts in video segmentation annotation with reinforcement learning	NEUROCOMPUTING											ENERGY MINIMIZATION																		0925-2312	1872-8286				SEP 10	2020	405						247	258		10.1016/j.neucom.2020.02.127													
J								Attentional coarse -and -fine generative adversarial networks for image inpainting	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 10	2020	405						259	269		10.1016/j.neucom.2020.03.090													
J								An efficient ACO-PSO-based framework for data classification and preprocessing in big data	EVOLUTIONARY INTELLIGENCE										ACO; PSO; SAW; Big data; Content based classification	PARTICLE SWARM OPTIMIZATION; PARAMETER SELECTION	Big data is prominent for the systematic extraction and analysis of a huge or complex dataset. It is also helpful in the management of data as compared to the traditional data-processing mechanisms. In this paper, an efficient ant colony optimization (ACO) and particle swarm optimization (PSO)-based framework have been proposed for data classification and preprocessing in the big data environment. It shows that the content part can be collaborated and fetched for analysis from the volume and velocity integration. Then weight marking has been done through the volume and the data variety. In the end, the ranking has been done through the velocity and variety aspects of big data. Data preprocessing has been performed from weights assigned on the basis of size, content, and keywords. ACO and PSO are then applied considering different computation aspects like uniform distribution, random initialization, epochs, iterations, and time constraint in case of both minimization and maximization. The weight assignments have been done automatically and through an unbiased random mechanism. It has been done on a scale of 0-1 for all the separated data. Then simple adaptive weight (SAW) method has been applied for prioritization and ranking. The overall average classification accuracy obtained in the case of PSO-SAW is 98%, and in the case of ACO-SAW, it is 95%. PSO-SAW approach outperforms in all cases, in comparison to ACO-SAW.																	1864-5909	1864-5917															10.1007/s12065-020-00477-7		SEP 2020											
J								Online summarization of dynamic graphs using subjective interestingness for sequential data	DATA MINING AND KNOWLEDGE DISCOVERY										Graph summarization; Maximum entropy principle; Subjective interestingness; Dynamic graphs	ALGORITHMS; CLIQUES	Many real-world phenomena can be represented as dynamic graphs, i.e., networks that change over time. The problem of dynamic graph summarization, i.e., to succinctly describe the evolution of a dynamic graph, has been widely studied. Existing methods typically use objective measures to find fixed structures such as cliques, stars, and cores. Most of the methods, however, do not consider the problem ofonlinesummarization, where the summary is incrementally conveyed to the analyst as the graph evolves, and (thus) do not take into account the knowledge of the analyst at a specific moment in time. We address this gap in the literature through a novel, generic framework for subjective interestingness for sequential data. Specifically, we iteratively identify atomic changes, called 'actions', that provide most information relative to the current knowledge of the analyst. For this, we introduce a novelinformation gainmeasure, which is motivated by the minimum description length (MDL) principle. With this measure, our approach discovers compact summaries without having to decide on the number of patterns. As such, we are the first to combine approaches for data mining based on subjective interestingness (using the maximum entropy principle) with pattern-based summarization (using the MDL principle). We instantiate this framework for dynamic graphs and dense subgraph patterns, and present DSSG, a heuristic algorithm for the online summarization of dynamic graphs by means of informative actions, each of which represents an interpretable change to the connectivity structure of the graph. The experiments on real-world data demonstrate that our approach effectively discovers informative summaries. We conclude with a case study on data from an airline network to show its potential for real-world applications.																	1384-5810	1573-756X															10.1007/s10618-020-00714-8		SEP 2020											
J								Complex Pythagorean Dombi fuzzy operators using aggregation operators and theirdecision-making	EXPERT SYSTEMS										complex Pythagorean Dombi fuzzy weighted arithmetic averaging operator; complex Pythagorean fuzzy sets; Dombi operations on complex Pythagorean fuzzy numbers; multi criteria decision-making; <mml; math altimg="urn; x-wiley; 02664720; media; exsy12626; exsy12626-math-0001" display="inline" overflow="scroll"><mml; mi mathvariant="script">T</mml; mi></mml; math>-norms	DECISION-MAKING; MEMBERSHIP GRADES; TOPSIS	A complex Pythagorean fuzzy set, an extension of Pythagorean fuzzy set, is a powerful tool to handle two dimension phenomenon. Dombi operators with operational parameters have outstanding flexibility. This article presents certain aggregation operators under complex Pythagorean fuzzy environment, including complex Pythagorean Dombi fuzzy weighted arithmetic averaging (CPDFWAA) operator, complex Pythagorean Dombi fuzzy weighted geometric averaging (CPDFWGA) operator, complex Pythagorean Dombi fuzzy ordered weighted arithmetic averaging (CPDFOWAA) operator and complex Pythagorean Dombi fuzzy ordered weighted geometric averaging (CPDFOWGA) operator. Moreover, this paper explores some fundamental properties of these operators with appropriate elaboration. A decision-making numerical example related to the selection of bank to purchase loan is given to demonstrate the significance of our proposed approach. Finally, a comparative analysis with existing operators is given to demonstrate the peculiarity of our proposed operators.																	0266-4720	1468-0394														e12626	10.1111/exsy.12626		SEP 2020											
J								Item Response Theory Based Ensemble in Machine Learning	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Classification; ensemble learning; item response theory; machine learning; expectation maximization (EM) algorithm	SUPPORT VECTOR MACHINE; CLASSIFIER; RECOGNITION; PREDICTION; ALGORITHM	In this article, we propose a novel probabilistic framework to improve the accuracy of a weighted majority voting algorithm. In order to assign higher weights to the classifiers which can correctly classify hard-to-classify instances, we introduce the item response theory (IRT) framework to evaluate the samples' difficulty and classifiers' ability simultaneously. We assigned the weights to classifiers based on their abilities. Three models are created with different assumptions suitable for different cases. When making an inference, we keep a balance between the accuracy and complexity. In our experiment, all the base models are constructed by single trees via bootstrap. To explain the models, we illustrate how the IRT ensemble model constructs the classifying boundary. We also compare their performance with other widely used methods and show that our model performs well on 19 datasets.																	1476-8186	1751-8520				OCT	2020	17	5					621	636		10.1007/s11633-020-1239-y		SEP 2020											
J								DiscoStyle: Multi-level Logistic Ranking for Personalized Image Style Preference Inference	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Facial preference; feature representation; logistic regression; face recommendation; transfer learning	CONVOLUTIONAL NEURAL-NETWORKS; FACE RECOGNITION; DEEP; MACHINE	Learning based on facial features for detection and recognition of people's identities, emotions and image aesthetics has been widely explored in computer vision and biometrics. However, automatic discovery of users' preferences to certain of faces (i.e., style), to the best of our knowledge, has never been studied, due to the subjective, implicative, and uncertain characteristic of psychological preference. Therefore, in this paper, we contribute to an answer to whether users' psychological preference can be modeled and computed after observing several faces. To this end, we first propose an efficient approach for discovering the personality preference related facial features from only a very few anchors selected by each user, and make accurate predictions and recommendations for users. Specifically, we propose to discover the style of faces (DiscoStyle) for human's psychological preference inference towards personalized face recommendation system/application. There are four merits of our DiscoStyle: 1) Transfer learning is exploited from identity related facial feature representation to personality preference related facial feature. 2) Appearance and geometric landmark feature are exploited for preference related feature augmentation. 3) A multi-level logistic ranking model with on-line negative sample selection is proposed for online modeling and score prediction, which reflects the users' preference degree to gallery faces. 4) A large dataset with different facial styles for human's psychological preference inference is developed for the first time. Experiments show that our proposed DiscoStyle can well achieve users' preference reasoning and recommendation of preferred facial styles in different genders and races.																	1476-8186	1751-8520				OCT	2020	17	5					637	651		10.1007/s11633-020-1244-1		SEP 2020											
J								The Governance of Unmanned Aircraft Systems (UAS): Aviation Law, Human Rights, and the Free Movement of Data in the EU	MINDS AND MACHINES										Aviation law; Co-regulation; Coordination mechanism; Data protection; Governance; Legal regulation; Middle-out approach; Soft law; Unmanned aircraft system (UAS)		The paper deals with the governance of Unmanned Aircraft Systems (UAS) in European law. Three different kinds of balance have been struck between multiple regulatory systems, in accordance with the sector of the governance of UAS which is taken into account. The first model regards the field of civil aviation law and its European Union (EU)'s regulation: the model looks like a traditional mix of top-down regulation and soft law. The second model concerns the EU general data protection law, the GDPR, which has set up a co-regulatory framework summed up with the principle of accountability also, but not only, in the field of drones. The third model of governance has been adopted by the EU through methods of legal experimentation and coordination mechanisms for UAS. The overall aim of the paper is to elucidate the ways in which such three models interact, insisting on differences and similarities with other technologies (e.g. self-driving cars), and further legal systems (e.g. the US).																	0924-6495	1572-8641															10.1007/s11023-020-09541-8		SEP 2020											
J								Adaptive dialogue management using intent clustering and fuzzy rules	EXPERT SYSTEMS										conversational systems; dialogue management; dialogue rules; evolving classifiers; clustering; user modelling	USER SIMULATION; SYSTEMS	Conversational systems have become an element of everyday life for billions of users who use speech-based interfaces to services, engage with personal digital assistants on smartphones, social media chatbots, or smart speakers. One of the most complex tasks in the development of these systems is to design the dialogue model, the logic that provided a user input selects the next answer. The dialogue model must also consider mechanisms to adapt the response of the system and the interaction style according to different groups and user profiles. Rule-based systems are difficult to adapt to phenomena that were not taken into consideration at design-time. However, many of the systems that are commercially available are based on rules, and so are the most widespread tools for the development of chatbots and speech interfaces. In this article, we present a proposal to: (a) automatically generate the dialogue rules from a dialogue corpus through the use of evolving algorithms, (b) adapt the rules according to the detected user intention. We have evaluated our proposal with several conversational systems of different application domains, from which our approach provided an efficient way for adapting a set of dialogue rules considering user utterance clusters.																	0266-4720	1468-0394														e12630	10.1111/exsy.12630		SEP 2020											
J								Causality, poetics, and grammatology: the role of computation in machine seeing	AI & SOCIETY										Materiality; Causality; Digital humanities; Computation; Interface		Digitised collections and born digital items, such as photos or video, exist beyond the scale of human viewing. New methods are required to read, understand and work with the data, resulting in computation becoming increasingly central to both creation of a cultural reality and as the interpretative tool and practice. If artists' look, then how might a machine see as a critical tool? Developing work on computational culture and the Next Rembrandt project as unstable digital object, this paper considers how the medium affects computational critical practice. Drawing on Heidegger's view of causality and Derrida's grammatology, this paper explores how the medium acts a locus between the human and machine readings and the remediations that occur within the reading. This is developed as through a reading of how the interface translates the signs and symbols and how this affects the reading. By reconsidering the critical assemblage and using it to think with, the human and the machine are seen as critical partners. Attending to the materialities of the reading through a playful approach that decentres potential meaning encourages us to glimpse beneath the surface and gestures towards a critical practice as understanding both computation and its materiality.																	0951-5666	1435-5655															10.1007/s00146-020-01061-4		SEP 2020											
J								Artificial moral and legal personhood	AI & SOCIETY										Moral personhood; Legal personhood; Moral status; Legal status; Civil law rules of robotics; EU Parliament; Robot rights; AI robots	RIGHTS	This paper considers the hotly debated issue of whether one should grant moral and legal personhood to intelligent robots once they have achieved a certain standard of sophistication based on such criteria as rationality, autonomy, and social relations. The starting point for the analysis is the European Parliament's resolution on Civil Law Rules on Robotics (2017) and its recommendation that robots be granted legal status and electronic personhood. The resolution is discussed against the background of the so-called Robotics Open Letter, which is critical of the Civil Law Rules on Robotics (and particularly of </n>59 f.). The paper reviews issues related to the moral and legal status of intelligent robots and the notion of legal personhood, including an analysis of the relation between moral and legal personhood in general and with respect to robots in particular. It examines two analogies, to corporations (which are treated as legal persons) and animals, that have been proposed to elucidate the moral and legal status of robots. The paper concludes that one should not ascribe moral and legal personhood to currently existing robots, given their technological limitations, but that one should do so once they have achieved a certain level at which they would become comparable to human beings.																	0951-5666	1435-5655															10.1007/s00146-020-01063-2		SEP 2020											
J								Seeing threats, sensing flesh: human-machine ensembles at work	AI & SOCIETY										Human-machine interfaces; Visual enskillment; Sensory anthropology; Facial recognition; X-ray scanning; Robotic surgery		Based on detailed descriptions of human-machine ensembles, this article explores how humans and machines work together to see specific things and unsee others, and how they come to co-configure one another. For seeing is not an automated function; whether one is a human or a machine, vision is gradually enskilled and mutually co-constituted. The analysis intersects three different ways of human-machine seeing to shed further light on the workings of each one: an airport, where facial recognition algorithms collaborate with border guards to grant passage to particular travellers and not to others; a luggage-scanning system, where potential security threats are assessed by a complex of X-rays and human intro-spection; and a hospital operating room, where human-machinic surgical robots find their way and operate on the insides of human bodies, touching only by seeing. In these examples, human and machine ways of seeing merge together, seeing in particular apparatuses of material, political, organisational, economic and fleshy components. The article analyses the practical work of human-machinic collaboration and explores how the different material and social constituents, not necessarily always working from the same agenda, come to configure what can be seen and sensed and what cannot.																	0951-5666	1435-5655															10.1007/s00146-020-01064-1		SEP 2020											
J								Integrity verification and behavioral classification of a large dataset applications pertaining smartOSvia blockchain and generative models	EXPERT SYSTEMS										android security; malware detection; deep learning; DCGAN	MALICIOUS EXECUTABLES; MALWARE DETECTION; NETWORKS; DETECT	Malware analysis and detection over the Android have been the focus of considerable research, during recent years, as customer adoption of Android attracted a corresponding number of malware writers. Antivirus companies commonly rely on signatures and are error-prone. Traditional machine learning techniques are based on static, dynamic, and hybrid analysis; however, for large scale Android malware analysis, these approaches are not feasible. Deep neural architectures are able to analyze large scale static details of the applications, but static analysis techniques can ignore many malicious behaviors of applications. The study contributes to the documentation of various approaches for detection of malware, traditional and state-of-the-art models, developed for analysis that facilitates the provision of basic insights for researchers working in malware analysis, and the study also provides a dynamic approach that employs deep neural network models for detection of malware. Moreover, the study uses Android permissions as a parameter to measure the dynamic behavior of around 16,900 benign and intruded applications. A dataset is created which encompasses a large set of permissions-based dynamic behavior pertaining applications, with an aim to train deep learning models for prediction of behavior. The proposed architecture extracts representations from input sequence data with no human intervention. The state-of-the-art Deep Convolutional Generative Adversarial Network extracted deep features and accomplished a general validation accuracy of 97.08% with an F1-score of 0.973 in correctly classifying input. Furthermore, the concept of blockchain is utilized to preserve the integrity of the dataset and the results of the analysis.																	0266-4720	1468-0394														e12611	10.1111/exsy.12611		SEP 2020											
J								Formalising Sigma-Protocols and Commitment Schemes Using CryptHOL	JOURNAL OF AUTOMATED REASONING										Cryptography; Sigma protocols; Commitment schemes; Isabelle/HOL; Provable security	SECURITY	Machine-checked proofs of security are important to increase the rigour of provable security. In this work we present a formalised theory of two fundamental two party cryptographic primitives: Sigma-protocols and Commitment Schemes. Sigma-protocols allow a prover to convince a verifier that they possess some knowledge without leaking information about the knowledge. Commitment schemes allow a committer to commit to a message and keep it secret until revealing it at a later time. We use CryptHOL (Lochbihler in Archive of formal proofs, 2017) to formalise both primitives and prove secure multiple examples namely; the Schnorr, Chaum-Pedersen and Okamoto Sigma-protocols as well as a construction that allows for compound (AND and OR) Sigma-protocols and the Pedersen and Rivest commitment schemes. A highlight of the work is a formalisation of the construction of commitment schemes from Sigma-protocols (Damgard in Lecture notes, 2002). We formalise this proof at an abstract level using the modularity available in Isabelle/HOL and CryptHOL. This way, the proofs of the instantiations come for free.																	0168-7433	1573-0670															10.1007/s10817-020-09581-w		SEP 2020											
J								Artificial neural networks training acceleration through network science strategies	SOFT COMPUTING										Network science; Artificial neural networks; Multilayer perceptron; Revise phase		The development of deep learning has led to a dramatic increase in the number of applications of artificial intelligence. However, the training of deeper neural networks for stable and accurate models translates into artificial neural networks (ANNs) that become unmanageable as the number of features increases. This work extends our earlier study where we explored the acceleration effects obtained by enforcing, in turn, scale freeness, small worldness, and sparsity during the ANN training process. The efficiency of that approach was confirmed by recent studies (conducted independently) where a million-node ANN was trained on non-specialized laptops. Encouraged by those results, our study is now focused on some tunable parameters, to pursue a further acceleration effect. We show that, although optimal parameter tuning is unfeasible, due to the high non-linearity of ANN problems, we can actually come up with a set of useful guidelines that lead to speed-ups in practical cases. We find that significant reductions in execution time can generally be achieved by setting the revised fraction parameter (zeta) to relatively low values.																	1432-7643	1433-7479															10.1007/s00500-020-05302-y		SEP 2020											
J								A genetic algorithm for the fuzzy shortest path problem in a fuzzy network	COMPLEX & INTELLIGENT SYSTEMS										Fuzzy graph; Shortest path problem; Fuzzy shortest path problem; Genetic algorithm	ROBUST	The shortest path problem (SPP) is an optimization problem of determining a path between specified source vertexsand destination vertextin a fuzzy network. Fuzzy logic can handle the uncertainties, associated with the information of any real life problem, where conventional mathematical models may fail to reveal proper result. In classical SPP, real numbers are used to represent the arc length of the network. However, the uncertainties related with the linguistic description of arc length in SPP are not properly represented by real number. We need to address two main matters in SPP with fuzzy arc lengths. The first matter is how to calculate the path length using fuzzy addition operation and the second matter is how to compare the two different path lengths denoted by fuzzy parameter. We use the graded mean integration technique of triangular fuzzy numbers to solve this two problems. A common heuristic algorithm to solve the SPP is the genetic algorithm. In this manuscript, we have introduced an algorithmic method based on genetic algorithm for determining the shortest path between a source vertex s and destination vertex t in a fuzzy graph with fuzzy arc lengths in SPP. A new crossover and mutation is introduced to solve this SPP. We also describe the QoS routing problem in a wireless ad hoc network.																	2199-4536	2198-6053															10.1007/s40747-020-00195-8		SEP 2020											
J								A novel hybrid whale optimization algorithm with flower pollination algorithm for feature selection: Case study Email spam detection	COMPUTATIONAL INTELLIGENCE										email spam detection; feature selection; flower pollination algorithm; opposition-based learning; optimization; whale optimization algorithm	DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; FEATURE SUBSET; CLASSIFICATION; OPPOSITION; SEARCH; COLONY	Feature selection (FS) in data mining is one of the most challenging and most important activities in pattern recognition. In this article, a new hybrid model of whale optimization algorithm (WOA) and flower pollination algorithm (FPA) is presented for the problem of FS based on the concept of opposition-based learning (OBL) which name is HWOAFPA. The procedure is that the WOA is run first and at the same time during the run, the WOA population is changed by the OBL. And, to increase the accuracy and speed of convergence, it is used as the initial population of FPA. To evaluate the performance of the proposed method, experiments were carried out in two steps. The experiments were performed on 10 datasets from the UCI data repository and Email spam detection datasets. The results obtained from the first step showed that the proposed method was more successful in terms of the average size of selection and classification accuracy than other basic metaheuristic algorithms. In addition, the results from the second step showed that the proposed method which was a run on the Email spam dataset performed much more accurately than other similar algorithms in terms of accuracy of Email spam detection.																	0824-7935	1467-8640															10.1111/coin.12397		SEP 2020											
J								Sibilant consonants classification comparison with multi- and single-class neural networks	EXPERT SYSTEMS										deep learning; machine learning; neural networks; sibilant consonants; speech and language therapy	SPEECH-THERAPY; RECOGNITION; APHASIA	Many children with speech sound disorders cannot pronounce the sibilant consonants correctly. We have developed a serious game, which is controlled by the children's voices in real time, with the purpose of helping children on practicing the production of European Portuguese (EP) sibilant consonants. For this, the game uses a sibilant consonant classifier. Since the game does not require any type of adult supervision, children can practice producing these sounds more often, which may lead to faster improvements of their speech. Recently, the use of deep neural networks has given considerable improvements in the classification of a variety of use cases, from image classification to speech and language processing. Here, we propose to use deep convolutional neural networks to classify sibilant phonemes of EP in our serious game for speech and language therapy. We compared the performance of several different artificial neural networks that used Mel frequency cepstral coefficients or log Mel filterbanks. Our best deep learning model achieves classification scores of 95.48% using a 2D convolutional model with log Mel filterbanks as input features. Such results are then further improved for specific classes with simple binary classifiers.																	0266-4720	1468-0394														e12620	10.1111/exsy.12620		SEP 2020											
J								EMoSOA: a new evolutionary multi-objective seagull optimization algorithm for global optimization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Seagull Optimization Algorithm; Multi-objective Optimization; Evolutionary; Pareto; Engineering Design Problems; Convergence; Diversity	SPOTTED HYENA OPTIMIZER; COMPUTATIONAL INTELLIGENCE; DESIGN OPTIMIZATION; PLACEMENT; MODEL; COST	This study introduces the evolutionary multi-objective version of seagull optimization algorithm (SOA), entitled Evolutionary Multi-objective Seagull Optimization Algorithm (EMoSOA). In this algorithm, a dynamic archive concept, grid mechanism, leader selection, and genetic operators are employed with the capability to cache the solutions from the non-dominatedPareto. The roulette-wheel method is employed to find the appropriate archived solutions. The proposed algorithm is tested and compared with state-of-the-art metaheuristic algorithms over twenty-four standard benchmark test functions. Four real-world engineering design problems are validated using proposedEMoSOAalgorithm to determine its adequacy. The findings of empirical research indicate that the proposed algorithm is better than other algorithms. It also takes into account those optimal solutions from theParetowhich shows high convergence.																	1868-8071	1868-808X															10.1007/s13042-020-01189-1		SEP 2020											
J								System for monitoring road slippery based on CCTV cameras and convolutional neural networks	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Machine learning; Convolutional neural networks; Transfer learning; Road safety		The slipperiness of the surface is essential for road safety. The growing number of CCTV cameras opens the possibility of using them to automatically detect the slippery surface and inform road users about it. This paper presents a system of developed intelligent road signs, including a detector based on convolutional neural networks (CNNs) and the transfer-learning method employed to the processing of images acquired with video cameras. Based on photos taken in different light conditions by CCTV cameras located at the roadsides in Poland, four network topologies have been trained and tested: Resnet50 v2, Resnet152 v2, Vgg19, and Densenet201. The last-mentioned network has proved to give the best result with 98.34% accuracy of classification dry, wet, and snowy roads.																	0925-9902	1573-7675				DEC	2020	55	3					521	534		10.1007/s10844-020-00618-5		SEP 2020											
J								Approach to derive golden paths based on machine sequence patterns in multistage manufacturing process	JOURNAL OF INTELLIGENT MANUFACTURING										Multistage manufacturing process (MMP); Product quality; Golden path; Machine sequence pattern (MSP); Quality engineering	QUALITY; PRODUCTIVITY; OPTIMIZATION; IMPROVEMENT; PREDICTION; SYSTEMS	A multistage manufacturing process (MMP) consists of several consecutive process stages, each of which has multiple machines performing the same functions in parallel. A manufacturing path (simply referred to as path) is defined as an ordered set indicating a record of machines assigned to a product at each process stage of an MMP. An MMP usually produces products through various paths. In practice, multiple machines in a process stage have different operational performances, which accumulate during production and affect the quality of products. This study proposes a heuristic approach to derive the golden paths that produce products whose quality exceeds the desired level. The proposed approach consists of the searching phase and the merging phase. The searching phase extracts two types of machine sequence patterns (MSPs) from a path dataset in an MMP. An MSP is a subset of the path that is defined as an ordered set of assigned machines from several process stages. The two extracted types of MSPs are: (1) superior MSP, which affects the production of superior-quality products, and (2) inferior MSP, which affects the production of inferior-quality products, called inferior MSP. The merging phase derives the golden paths by combining superior MSPs and excluding inferior MSPs. The proposed approach is verified by applying it to a hypothetical path dataset and the semiconductor tool fault isolation (SETFI) dataset. This verification shows that the proposed approach derives the golden paths that exceed the predefined product quality level. This outcome demonstrates the practical viability of the proposed approach in an MMP.																	0956-5515	1572-8145															10.1007/s10845-020-01654-2		SEP 2020											
J								Critical joint identification for efficient sequencing	JOURNAL OF INTELLIGENT MANUFACTURING										Critical joint; Sequence; Machine learning; PCA; Assembly; SVM	WELDING SEQUENCE; GEOMETRY ASSURANCE; OPTIMAL-DESIGN; DIGITAL TWIN; OPTIMIZATION; RESPECT	Identifying the optimal sequence of joining is an exhaustive combinatorial optimization problem. On each assembly, there is a specific number of weld points that determine the geometrical deviation of the assembly after joining. The number and sequence of such weld points play a crucial role both for sequencing and assembly planning. While there are studies on identifying the complete sequence of welding, identifying such joints are not addressed. In this paper, based on the principles of machine intelligence, black-box models of the assembly sequences are built using the support vector machines (SVM). To identify the number of the critical weld points, principle component analysis is performed on a proposed data set, evaluated using the SVM models. The approach has been applied to three assemblies of different sizes, and has successfully identified the corresponding critical weld points. It has been shown that a small fraction of the weld points of the assembly can reduce more than 60% of the variability in the assembly deviation after joining.																	0956-5515	1572-8145															10.1007/s10845-020-01660-4		SEP 2020											
J								Deep learning based breast cancer detection and classification using fuzzy merging techniques	MACHINE VISION AND APPLICATIONS										Breast cancer; Saliency detection; Nuclei segmentation; Anisotropic diffusion; Convolutional neural network; Deep learning	COMPUTER-AIDED DETECTION; SEGMENTATION; NETWORKS; HYBRID	Automatic identification of abnormal and normal cells is a critical step in computer-assisted pathology, owing to certain heterogeneous characteristics of cancer cells. However, automated nuclei detection is problematic in unevenly shaped, overlapping and touching nuclei. It is, consequently, essential to detect single and overlapping nuclei and distinguish them from single ones for a reasonable quantitative analysis. Diagnosis is improved by introducing a computer-aided diagnosis system to automatically detect breast cancer tissue nuclei from whole slide images of hematoxylin and eosin stains. We propose a method for the automatic cell nuclei detection, segmentation, and classification of breast cancer using a deep convolutional neural network (Deep-CNN) approach. The main contribution of this work is the detection of nuclei using anisotropic diffusion in a filter and applying a novel multilevel saliency nuclei detection model in ductal carcinoma of breast cancer tissue. The detected nuclei are classified into benign and malignant cells by applying the new Deep-CNN model. Finally, the novel multilevel saliency nuclei detection technique is integrated with the Deep-CNN to produce an nMSDeep-CNN model that turns out to be the most accurate results with very less computation time. The accuracy, sensitivity and specificity of the proposed system are 98.62%, 0.947 and 0.964, respectively. The classification for benign and malignant cells is evaluated by applying 10 fold cross-validation. Thus, the system can be clinically used for an objective, accurate, and rapid diagnosis of abnormal tissue. The effectiveness of the suggested framework is demonstrated through experiments on several datasets.																	0932-8092	1432-1769				SEP 9	2020	31	7-8							63	10.1007/s00138-020-01122-0													
J								DeepTable: a permutation invariant neural network for table orientation classification	DATA MINING AND KNOWLEDGE DISCOVERY										Information discovery; Tabular data; Table orientation classification; Deep learning; Machine learning		Tables are a common way to present information in an intuitive and concise manner. They are used extensively in media such as scientific articles or web pages. Automatically analyzing the content of tables bears special challenges. One of the most basic tasks is determination of the orientation of a table: In column tables, columns represent one entity with the different attribute values present in the different rows; row tables are vice versa, and matrix tables give information on pairs of entities. In this paper, we address the problem of classifying a given table into one of the three layouts horizontal (for row tables), vertical (for column tables), and matrix. We describe DeepTable, a novel method based on deep neural networks designed for learning from sets. Contrary to previous state-of-the-art methods, this basis makes DeepTable invariant to the permutation of rows or columns, which is a highly desirable property as in most tables the order of rows and columns does not carry specific information. We evaluate our method using a silver standard corpus of 5500 tables extracted from biomedical articles where the layout was determined heuristically. DeepTable outperforms previous methods in both precision and recall on our corpus. In a second evaluation, we manually labeled a corpus of 300 tables and were able to confirm DeepTable to reach superior performance in the table layout classification task. The codes and resources introduced here are available at.																	1384-5810	1573-756X				NOV	2020	34	6					1963	1983		10.1007/s10618-020-00711-x		SEP 2020											
J								CrawlSN: community-aware data acquisition with maximum willingness in online social networks	DATA MINING AND KNOWLEDGE DISCOVERY										Social networks; Graph algorithm; Data acquisition	GROUP QUERIES; SEARCH	Real social network datasets with community structures are critical for evaluating various algorithms in Online Social Networks (OSNs). However, obtaining such community data from OSNs has recently become increasingly challenging due to privacy issues and government regulations. In this paper, we thus make our first attempt to address two important factors, i.e., user willingness and existence of community structure, to obtain more complete OSN data. We formulate a new research problem, namelyCommunity-aware Data Acquisition with Maximum Willingness in Online Social Networks(CrawlSN), to identify a group of users from an OSN, such that the group is a socially tight community and the users' willingness to contribute data is maximized. We prove that CrawlSN is NP-hard and inapproximable within any factor unless, and propose an effective algorithm, namedCommunity-aware Group Identification with Maximum Willingness(CIW) with various processing strategies. We conduct an evaluation study with 1093 volunteers to validate our problem formulation and demonstrate that CrawlSN outperforms the other alternatives. We also perform extensive experiments on 7 real datasets and show that the proposed CIW outperforms the other baselines in both solution quality and efficiency.																	1384-5810	1573-756X															10.1007/s10618-020-00709-5		SEP 2020											
J								Artificial intelligence and the value of transparency	AI & SOCIETY										Transparency; Explainability; Contestability; Machine learning; Bias		Some recent developments in Artificial Intelligence-especially the use of machine learning systems, trained on big data sets and deployed in socially significant and ethically weighty contexts-have led to a number of calls for "transparency". This paper explores the epistemological and ethical dimensions of that concept, as well as surveying and taxonomising the variety of ways in which it has been invoked in recent discussions. Whilst "outward" forms of transparency (concerning the relationship between an AI system, its developers, users and the media) may be straightforwardly achieved, what I call "functional" transparency about the inner workings of a system is, in many cases, much harder to attain. In those situations, I argue that contestability may be a possible, acceptable, and useful alternative so that even if we cannot understand how a system came up with a particular output, we at least have the means to challenge it.																	0951-5666	1435-5655															10.1007/s00146-020-01066-z		SEP 2020											
J								Metric transfer learning via geometric knowledge embedding	APPLIED INTELLIGENCE										Metric learning; Transfer learning; Geometric knowledge embedding; Mahalanobis distance metric	KERNEL	The usefulness of metric learning in image classification has been proven and has attracted increasing attention in recent research. In conventional metric learning, it is assumed that the source and target instances are distributed identically, however, real-world problems may not have such an assumption. Therefore, for better classifying, we need abundant labeled images, which are inaccessible due to the high cost of labeling. In this way, the knowledge transfer could be utilized. In this paper, we present a metric transfer learning approach entitled as "Metric Transfer Learning via Geometric Knowledge Embedding (MTL-GKE)" to actuate metric learning in transfer learning. Specifically, we learn two projection matrices for each domain to project the source and target domains to a new feature space. In the new shared sub-space,Mahalanobisdistance metric is learned to maximize inter-class and minimize intra-class distances in target domain, while a novel instance reweighting scheme based on the graph optimization is applied, simultaneously, to employ the weights of source samples for distribution matching. The results of different experiments on several datasets on object and handwriting recognition tasks indicate the effectiveness of the proposedMTL-GKEcompared to other state-of-the-arts methods.																	0924-669X	1573-7497															10.1007/s10489-020-01853-7		SEP 2020											
J								Time-aware sequence model for next-item recommendation	APPLIED INTELLIGENCE										Recommendation; Sequence Modeling; Time-Aware; Long Short-Term Memory	FACTORIZATION	The sequences of users' behaviors generally indicate their preferences, and they can be used to improve next-item prediction in sequential recommendation. Unfortunately, users' behaviors may change over time, making it difficult to capture users' dynamic preferences directly from recent sequences of behaviors. Traditional methods such as Markov Chains (MC), Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks only consider the relative order of items in a sequence and ignore important time information such as the time interval and duration in the sequence. In this paper, we propose a novel sequential recommendation model, named Interval- and Duration-aware LSTM with Embedding layer and Coupled input and forget gate (IDLSTM-EC), which leverages time interval and duration information to accurately capture users' long-term and short-term preferences. In particular, the model incorporates global context information about sequences in the input layer to make better use of long-term memory. Furthermore, the model introduces the coupled input and forget gate and embedding layer to further improve efficiency and effectiveness. Experiments on real-world datasets show that the proposed approaches outperform the state-of-the-art baselines and can handle the problem of data sparsity effectively.																	0924-669X	1573-7497															10.1007/s10489-020-01820-2		SEP 2020											
J								SOS 2.0: an evolutionary approach for SOS algorithm	EVOLUTIONARY INTELLIGENCE										Metaheuristics; Optimization; Symbiotic organisms search; Benchmark function; Engineering design optimization	SYMBIOTIC ORGANISMS SEARCH; BIOGEOGRAPHY-BASED OPTIMIZATION; AUGMENTED LAGRANGE MULTIPLIER; DIFFERENTIAL EVOLUTION; CHAOS; STRATEGY	With the shortcomings on the solution given for most-recent optimization problems, decision-makers from different fields yearn the existence of tenacious breakthrough. In fact, they all shared the same obligation to optimize work efficiency, whether to minimize cost, consumption or to maximize the profit acquirement. Metaheuristic search is the more-advanced method proven to be useful for difficult optimization tasks. Moreover, development records also signalized rapid development of these algorithms, contributing several notable and powerful optimization algorithms. Among them, Symbiotic Organisms Search (SOS) received noticeable attention due to its simplicity and also its parameter-less nature. Nonetheless, several considerable issues are still challenging for further development. For instance, local optima and premature convergence issues found from any improper and inefficiency computational procedure on higher dimensional problems. Also, exploitation and exploration trade-off is another essential issue involving stability for optimal performance. In that case, this work proposed a new evolutionary approach named SOS 2.0. There are two distinct features associated with the evolution: Self-Parameter-Updating (SPU) technique and chaotic maps sequencing. Both features are integrated for a better balance of exploration and exploitation in which SPU focuses on exploration and chaotic map focuses on exploitation instead. This work also applied benchmarks function tests and engineering design optimization problem in advance for validation purpose of the performance. The experimental results showed that SOS 2.0 delivers not only better performance from its predecessor and also several recent SOS modifications which can be concluded as one successive approach for better SOS algorithm, but also enhances the computation efficiency and capability of searching optimal solution.																	1864-5909	1864-5917															10.1007/s12065-020-00476-8		SEP 2020											
J								Improved human-object interaction detection through skeleton-object relations	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Human-object interaction; interaction pattern; spatial relation; skeleton-object relation		Current methods for human-object interaction detection often use the spatial relation between a human and an object as an interaction pattern. However, this strategy is relatively simple and has low discrimination in similar interactions. To solve this drawback, the spatial relation between skeletons and objects is proposed to model the interaction pattern and improve the detection accuracy. First, the skeleton-object interaction pattern image is extracted for each interaction proposal. Second, a deep neural network is applied to learn the interaction features from these images. Finally, the interaction feature is added to the human-object interaction detection network by a multistream structure. In the experiments, we evaluate the proposed method on the HICO-DET and V-COCO datasets. Experimental results show that the proposed method can achieve the best performance compared with state-of-art methods.																	0952-813X	1362-3079															10.1080/0952813X.2020.1818293		SEP 2020											
J								A novel transfer learning fault diagnosis method based on Manifold Embedded Distribution Alignment with a little labeled data	JOURNAL OF INTELLIGENT MANUFACTURING										Transfer learning; Bidirectional gated recurrent unit; Manifold Embedded Distribution Alignment	DENOISING AUTOENCODER; NETWORK; EEMD	Accurate identification of rolling bearing faults is quite significant for the stable operation of mechanical systems. However, for practical diagnosis issues, it is difficult to obtain abundant labeled data due to the change of operating conditions and complex working environment, which puts forward higher requirements on the ability of the diagnosis methods. To tackle the mentioned problem, a novel transfer learning method based on a little labeled data is proposed, which uses bidirectional gated recurrent unit (BiGRU) and Manifold Embedded Distribution Alignment (MEDA). Firstly, frequency spectrum datasets are utilized to remove the redundant information of raw vibration signals. Secondly, the BiGRU network is constructed to generate auxiliary samples that are utilized as source domain. Finally, MEDA, as the most powerful non-deep transfer learning method, is applied to align the distribution of these auxiliary samples generated by BiGRU and the unlabeled samples from target domain. Experiment results indicate the excellent performance of the proposed method under a little labeled data.																	0956-5515	1572-8145															10.1007/s10845-020-01657-z		SEP 2020											
J								CGA: a new feature selection model for visual human action recognition	NEURAL COMPUTING & APPLICATIONS										Human action recognition; Cooperative genetic algorithm; Feature selection; Coalition game; Pearson correlation coefficient; Weizmann; KTH; UCF11; HMDB51; UCI HAR	SEARCH ALGORITHM; SYSTEM; KERNEL; VIDEOS	Recognition of human actions from visual contents is a budding field of computer vision and image understanding. The problem with such a recognition system is the huge dimensions of the feature vectors. Many of these features are irrelevant to the classification mechanism. For this reason, in this paper, we propose a novel feature selection (FS) model called cooperative genetic algorithm (CGA) to select some of the most important and discriminating features from the entire feature set to improve the classification accuracy as well as the time requirement of the activity recognition mechanism. In CGA, we have made an effort to embed the concepts of cooperative game theory in GA to create a both-way reinforcement mechanism to improve the solution of the FS model. The proposed FS model is tested on four benchmark video datasets named Weizmann, KTH, UCF11, HMDB51, and two sensor-based UCI HAR datasets. The experiments are conducted using four state-of-the-art feature descriptors, namely HOG, GLCM, SURF, and GIST. It is found that there is a significant improvement in the overall classification accuracy while considering very small fraction of the original feature vector.																	0941-0643	1433-3058															10.1007/s00521-020-05297-5		SEP 2020											
J								Asynchronous l(2)-l(infinity) Filtering for Discrete-Time Fuzzy Markov Jump Neural Networks with Unreliable Communication Links	NEURAL PROCESSING LETTERS										Fuzzy Markov jump neural networks; Asynchronous l(2) - l(infinity) filter; Data packet loss; Signal quantization	GLOBAL ASYMPTOTIC STABILITY; STATE ESTIMATION; FEEDBACK-CONTROL; SYSTEMS; DELAY; SYNCHRONIZATION	This paper investigates the problem of l(2)-l(infinity) asynchronous filtering for a class of discrete-time fuzzy neural networks subject to Markov jump parameters and unreliable communication links. Due to the fact that neural networks possess the nonlinear dynamic characteristic, it is difficult to deal with such a nonlinear characteristic directly, so the Takagi-Sugeno fuzzy model is introduced to approximate the system. Directed against the unreliable communication links, the data packet loss depicted by a stochastic variable with Bernoulli distribution and the signal quantization phenomenon occurring in communication channels are taken into consideration simultaneously. The attention of this paper is mainly centered on devising an asynchronous l(2)-l(infinity) filter for ensuring the l(2)-l(infinity) performance of the studied system under asynchronous conditions. Some sufficient conditions for the existence of the asynchronous l(2)-l(infinity) filter are presented. Finally, a numerical example is given to carry out the simulation experiment, which can verify the effectiveness of the obtained results.																	1370-4621	1573-773X															10.1007/s11063-020-10337-1		SEP 2020											
J								Deep neural de-raining model based on dynamic fusion of multiple vision tasks	SOFT COMPUTING										Deep neural network; Single-image de-raining; Screen blend model; Multi-task learning; Dynamic scheme; Evolutionary algorithm	SINGLE-IMAGE; NOISE REMOVAL	Image quality is relevant to the performance of computer vision applications. The interference of rain streaks often greatly depreciates the visual effect of images. It is a traditional and critical vision challenge to remove rain streaks from rainy images. In this paper, we introduce a deep connectionist screen blend model for single-image rain removal research. The novel deep structure is mainly composed of shortcut connections, and ends with sibling branches. The specific architecture is designed for joint optimization of heterogeneous but related tasks. In particular, a feature-level task is design to preserve object edges which tend to be lost in de-rained images. Moreover, a comprehensive image quality assessment is an additional vision task for further improvement on de-rained results. Instead of using rules of thumb, we propose an actionable method to dynamically assign appropriate weighting coefficients for all vision tasks we use. On the other hand, various factors such as haze also give rise to weak visual appeal of rainy images. To remove these adverse factors, we develop an image enhancement framework which enables the hyperparameters to be optimized in an adaptive way, and efficiently improves the perceived quality of de-rained results. The effectiveness of the proposed de-raining system has been verified by extensive experiments, and most results of our method are impressive. The source code and more de-rained results will be available online.																	1432-7643	1433-7479															10.1007/s00500-020-05291-y		SEP 2020											
J								Blood glucose level prediction for diabetes based on modified fuzzy time series and particle swarm optimization	COMPUTATIONAL INTELLIGENCE										blood glucose level prediction; fuzzy time series; particle swarm optimization; Type-1 diabetes	FORECASTING ENROLLMENTS; NEURAL-NETWORK; DIAGNOSIS; MODELS; SENSORS	Blood glucose control is an essential goal for the patients who have Type-1 diabetes (T1D). The prediction of the blood glucose levels for the next 30-minute is crucial. If the predicted blood glucose level is in the critical ranges, and these predictions can be known in advance, then the patients can take the necessary cautions to prevent from it. In this article, we propose a modified fuzzy particle swarm optimization algorithm for the prediction of blood glucose levels of 30-minute after the last measurement. We form the average and patient-specific models to predict the blood glucose level of the patients. Both models are tested on two different datasets which contain patients with T1D. The experimental results are evaluated in terms of root mean squared error and Clarke error grid analysis metrics. The results indicate that our proposed modified algorithm is feasible to be applied to the prediction of blood glucose levels. In addition, this approach can assist patients with T1D for their blood glucose control.																	0824-7935	1467-8640															10.1111/coin.12396		SEP 2020											
J								Accuracy improvement in air-quality forecasting using regressor combination with missing data imputation	COMPUTATIONAL INTELLIGENCE										air-quality forecasting; deep learning; missing data imputation; regressor combination	NEURAL-NETWORK; POLLUTION; MODELS; PREDICTION	This article proposes a hybrid model based on regressor combination to improve the accuracy of air-quality forecasting. The expectation-maximization algorithm was used to impute the missing values of the dataset. The optimal hyperparameter values for the regressors were found by the grid search approach, depending on the mean absolute error (MAE), in the training session. The regressors having the minimum MAE were then globally combined for prediction. The output of the regressor with the minimum absolute error between the actual and predicted values was chosen as the prediction result of the hybrid model. The performance of the proposed model was compared with that of sequential deep learning methods, namely long short-term memory and gated recurrent unit, in terms of MAE, mean relative error (MRE), and squared correlation coefficient (SCC) metrics. The imputed dataset was divided into training and testing subsets of different durations. According to the experimental results, our hybrid model performed better than the deep learning methods in terms of MAE, MRE, and SCC metrics, irrespective of the training data length. Furthermore, the Akaike's information criterion and the Bayesian information criterion values suggested that the quality of the hybrid model was better than that of the deep learning models.																	0824-7935	1467-8640															10.1111/coin.12399		SEP 2020											
J								Detection of anomalous episodes in urban Ozone maps	EXPERT SYSTEMS										air quality; autoencoders; convolutional neural network; DBSCAN; deep learning; outlier detection	AIR-POLLUTION; SENSOR NETWORKS; MORTALITY	In addition to classification and regression, outlier detection has emerged as a relevant activity in deep learning. In comparison with previous approaches where the original features of the examples were used for separating the examples with high dissimilarity from the rest of the examples, deep learning can automatically extract useful features from raw data, thus removing the need for most of the feature engineering efforts usually required with classical machine learning approaches. This requires training the deep learning algorithm with labels identifying the examples or with numerical values. Although outlier detection in deep learning has been usually undertaken by training the algorithm with categorical labels-classifier-, it can also be performed by using the algorithm as regressor. Nowadays numerous urban areas have deployed a network of sensors for monitoring multiple variables about air quality. The measurements of these sensors can be treated individually-as time series-or collectively. Collectively, a variable monitored by a network of sensors can be transformed into a map. Maps can be used as images in machine learning algorithms-including computer vision algorithms-for outlier detection. The identification of anomalous episodes in air quality monitoring networks allows later processing this time period with finer-grained scientific packages involving fluid dynamic and chemical evolution software, or the identification of malfunction stations. In this work, a Convolutional Neural Network is trained-as a regressor-using as input Ozone-urban images generated from the Air Quality Monitoring Network of Madrid (Spain). The learned features are processed by Density-based Spatial Clustering of Applications with Noise (DBSCAN) algorithm for identifying anomalous maps. Comparisons with other deep learning architectures are undertaken, for instance, autoencoders-undercomplete and denoizing-for learning salient features of the maps and later to use as input of DBSCAN. The proposed approach is able efficiently find maps with local anomalies compared to other approaches based on raw images or latent features extracted with autoencoders architectures with DBSCAN.																	0266-4720	1468-0394														e12636	10.1111/exsy.12636		SEP 2020											
J								Semantic segmentation and colorization of grayscale aerial imagery withW-Netmodels	EXPERT SYSTEMS										fully-convolutional neural networks; processing grayscale aerial photos; semantic segmentation of remotely sensed imagery; W-Net architecture	NETWORKS; DYNAMICS; MAP	The semantic segmentation of remotely sensed aerial imagery is nowadays an extensively explored task, concerned with determining, for each pixel in an input image, the most likely class label from a finite set of possible labels. Most previous work in the area has addressed the analysis of high-resolution modern images, although the semantic segmentation of historical grayscale aerial photos can also have important applications. Examples include supporting the development of historical road maps, or the development of dasymetric disaggregation approaches leveraging historical building footprints. Following recent work in the area related to the use of fully-convolutional neural networks for semantic segmentation, and specifically envisioning the segmentation of grayscale aerial imagery, we evaluated the performance of an adapted version of the W-Net architecture, which has achieved very good results on other types of image segmentation tasks. Our W-Net model is trained to simultaneously segment images and reconstruct, or predict, the colour of the input images from intermediate representations. Through experiments with distinct data sets frequently used in previous studies, we show that the proposed W-Net architecture is quite effective in colouring and segmenting the input images. The proposed approach outperforms a baseline corresponding to the U-Net model for the segmentation of both coloured and grayscale imagery, and it also outperforms some of the other recently proposed approaches when considering coloured imagery.																	0266-4720	1468-0394														e12622	10.1111/exsy.12622		SEP 2020											
J								An integrated information systems architecture for the agri-food industry	EXPERT SYSTEMS										agri-food industry; information systems; IS architecture; mushroom production	FIRM PERFORMANCE; TECHNOLOGY; AGRICULTURE; ADOPTION; INVESTMENTS; INNOVATION; STRATEGY	As information systems and technologies grow in usage in the agri-food industry, the same has happened to the relevance of Information Systems (IS) that allow for a parallel control, monitoring and management of the organizations' activities and business processes. As the literature proves, the benefits of implementing adequate and interoperable IS are very numerous and tend to represent a significant determinant regarding the organizations' overall success. Despite this, to the best of our knowledge there currently is no IS architecture designed to serve the specificities of the agri-food industry. With this study a novel information systems architecture for the agri-food sector is proposed. The artefact is composed by 12 integrated main components and a set of subcomponents aimed at supporting all the monitoring, control and management activities. In order to validate the proposed architecture a case study was implemented at a mushroom production organization. This allowed us to perceive the ability of our artefact to serve as the basis for the development of IS that address all of the organization's business and environmental needs.																	0266-4720	1468-0394														e12599	10.1111/exsy.12599		SEP 2020											
J								A Liu estimator for the beta regression model and its application to chemical data	JOURNAL OF CHEMOMETRICS										beta regression; Liu estimator; Monte Carlo methods; multicollinearity; relative MSE	RIDGE-REGRESSION; HAZELNUTS; MOISTURE; FLAVOR; COLOR	Beta regression has become a popular tool for performing regression analysis on chemical, environmental, or biological data in which the dependent variable is restricted to the interval [0, 1]. For the first time, in this paper, we propose a Liu estimator for the beta regression model with fixed dispersion parameter that may be used in several realistic situations when the degree of correlation among the regressors differs. First, we show analytically that the new estimator outperforms the maximum likelihood estimator (MLE) using the mean square error (MSE) criteria. Second, using a 'simulation study, we investigate the properties in finite samples of six different suggested estimators of the shrinkage parameter and compare it with the MLE. The simulation results indicate that in the presence of multicollinearity, the Liu estimator outperforms the MLE uniformly. Finally, using an empirical application on chemical data, we show the benefit of the new approach to applied researchers.																	0886-9383	1099-128X														e3300	10.1002/cem.3300		SEP 2020											
J								Investigation of the Sense of Agency in Social Cognition, Based on Frameworks of Predictive Coding and Active Inference: A Simulation Study on Multimodal Imitative Interaction	FRONTIERS IN NEUROROBOTICS										sense of agency; predictive coding; active inference; multimodal perception; human-robot interaction; recurrent neural network; variational Bayes	NEURAL-NETWORK MODEL; MIRROR NEURONS; TIME SCALES; ROBOT; RECOGNITION; PERCEPTION; DYNAMICS	When agents interact socially with different intentions (or wills), conflicts are difficult to avoid. Although the means by which social agents can resolve such problems autonomously has not been determined, dynamic characteristics of agency may shed light on underlying mechanisms. Therefore, the current study focused on the sense of agency, a specific aspect of agency referring to congruence between the agent's intention in acting and the outcome, especially in social interaction contexts. Employing predictive coding and active inference as theoretical frameworks of perception and action generation, we hypothesize that regulation of complexity in the evidence lower bound of an agent's model should affect the strength of the agent's sense of agency and should have a significant impact on social interactions. To evaluate this hypothesis, we built a computational model of imitative interaction between a robot and a human via visuo-proprioceptive sensation with a variational Bayes recurrent neural network, and simulated the model in the form of pseudo-imitative interaction using recorded human body movement data, which serve as the counterpart in the interactions. A key feature of the model is that the complexity of each modality can be regulated differently by changing the values of a hyperparameter assigned to each local module of the model. We first searched for an optimal setting of hyperparameters that endow the model with appropriate coordination of multimodal sensation. These searches revealed that complexity of the vision module should be more tightly regulated than that of the proprioception module because of greater uncertainty in visual information flow. Using this optimally trained model as a default model, we investigated how changing the tightness of complexity regulation in the entire network after training affects the strength of the sense of agency during imitative interactions. The results showed that with looser regulation of complexity, an agent tends to act more egocentrically, without adapting to the other. In contrast, with tighter regulation, the agent tends to follow the other by adjusting its intention. We conclude that the tightness of complexity regulation significantly affects the strength of the sense of agency and the dynamics of interactions between agents in social settings.																	1662-5218					SEP 7	2020	14								61	10.3389/fnbot.2020.00061													
J								Democratizing AI: non-expert design of prediction tasks	PEERJ COMPUTER SCIENCE										Citizen science; Supervised learning; Predictive models; Randomized control trial; Amazon mechanical turk; Novel data collection; Crowdsourcing; Interactive machine learning; Automatic machine learning; AutoML		Non-experts have long made important contributions to machine learning (ML) by contributing training data, and recent work has shown that non-experts can also help with feature engineering by suggesting novel predictive features. However, non-experts have only contributed features to prediction tasks already posed by experienced ML practitioners. Here we study how non-experts can design prediction tasks themselves, what types of tasks non-experts will design, and whether predictive models can be automatically trained on data sourced for their tasks. We use a crowdsourcing platform where non-experts design predictive tasks that are then categorized and ranked by the crowd. Crowdsourced data are collected for top-ranked tasks and predictive models are then trained and evaluated automatically using those data. We show that individuals without ML experience can collectively construct useful datasets and that predictive models can be learned on these datasets, but challenges remain. The prediction tasks designed by non-experts covered a broad range of domains, from politics and current events to health behavior, demographics, and more. Proper instructions are crucial for non-experts, so we also conducted a randomized trial to understand how different instructions may influence the types of prediction tasks being proposed. In general, understanding better how non-experts can contribute to ML can further leverage advances in Automatic machine learning and has important implications as ML continues to drive workplace automation.																	2376-5992					SEP 7	2020									e296	10.7717/peerj-cs.296													
J								A dictionary for translation from natural to formal data model language	COMPUTATIONAL INTELLIGENCE										data modeling; dictionary; formal language; knowledge-based system; linguistic corpus; machine translation; pattern recognition	ENTITY-RELATIONSHIP; SYSTEM	The paper describes our current research activities and results related to developing knowledge-based systems to support the creation of entity-relationship (ER) models. The authors based obtaining an ER model in textual form on translation from one language into another, that is, from an English controlled natural language into the formalized language of an ER data model. Our translation method consisted of creating translation rules of sentential form parts into ER model constructs based on the textual and character patterns detected in the business descriptions. To enable the computer analyses necessary for creating translation mechanisms, we created a linguistic corpus that contains lists of the business descriptions and the texts of other business materials. From the corpus, we then created a specific dictionary and linguistic rules to automate the business descriptions' translation into the ER data model language. Before that, however, the corpus was enriched by adding annotations to the words related to ER data model constructs. In this paper, we also present the main issues uncovered during the translation process and offer a possible solution with utility evaluation: applying information-extraction performance measures to a set of sentences from the corpus.																	0824-7935	1467-8640															10.1111/coin.12393		SEP 2020											
J								Finding the ground state of spin Hamiltonians with reinforcement learning	NATURE MACHINE INTELLIGENCE											QUANTUM; OPTIMIZATION; MODEL; ALGORITHM; GO; EFFICIENT; GAME	Reinforcement learning has become a popular method in various domains, for problems where an agent must learn what actions must be taken to reach a particular goal. An interesting example where the technique can be applied is simulated annealing in condensed matter physics, where a procedure is determined for slowly cooling a complex system to its ground state. A reinforcement learning approach has been developed that can learn a temperature scheduling protocol to find the ground state of spin glasses, magnetic systems with strong spin-spin interactions between neighbouring atoms. Reinforcement learning (RL) has become a proven method for optimizing a procedure for which success has been defined, but the specific actions needed to achieve it have not. Using a method we call 'controlled online optimization learning' (COOL), we apply the so-called 'black box' method of RL to simulated annealing (SA), demonstrating that an RL agent based on proximal policy optimization can, through experience alone, arrive at a temperature schedule that surpasses the performance of standard heuristic temperature schedules for two classes of Hamiltonians. When the system is initialized at a cool temperature, the RL agent learns to heat the system to 'melt' it and then slowly cool it in an effort to anneal to the ground state; if the system is initialized at a high temperature, the algorithm immediately cools the system. We investigate the performance of our RL-driven SA agent in generalizing to all Hamiltonians of a specific class. When trained on random Hamiltonians of nearest-neighbour spin glasses, the RL agent is able to control the SA process for other Hamiltonians, reaching the ground state with a higher probability than a simple linear annealing schedule. Furthermore, the scaling performance (with respect to system size) of the RL approach is far more favourable, achieving a performance improvement of almost two orders of magnitude onL= 14(2)systems. We demonstrate the robustness of the RL approach when the system operates in a 'destructive observation' mode, an allusion to a quantum system where measurements destroy the state of the system. The success of the RL agent could have far-reaching impacts, from classical optimization, to quantum annealing and to the simulation of physical systems.																		2522-5839				SEP	2020	2	9					509	517		10.1038/s42256-020-0226-x		SEP 2020											
J								A multi-layer and multi-ensemble stock trader using deep learning and deep reinforcement learning	APPLIED INTELLIGENCE										Deep learning; Deep reinforcement learning; Intraday stock trading	TIME-SERIES; PREDICTION	The adoption of computer-aided stock trading methods is gaining popularity in recent years, mainly because of their ability to process efficiently past information through machine learning to predict future market behavior. Several approaches have been proposed to this task, with the most effective ones using fusion of a pile of classifiers decisions to predict future stock values. However, using prices information in single supervised classifiers has proven to lead to poor results, mainly because market history is not enough to be an indicative of future market behavior. In this paper, we propose to tackle this issue by proposing a multi-layer and multi-ensemble stock trader. Our method starts by pre-processing data with hundreds of deep neural networks. Then, a reward-based classifier acts as a meta-learner to maximize profit and generate stock signals through different iterations. Finally, several metalearner trading decisions are fused in order to get a more robust trading strategy, using several trading agents to take a final decision. We validate the effectiveness of the approach in a real-world trading scenario, by extensively testing it on theStandard & Poor's 500future market and theJ.P. MorganandMicrosoftstocks. Experimental results show that the proposed method clearly outperforms all the considered baselines (which still performs very well in the analysed period), and even the conventionalBuy-and-Holdstrategy, which replicates the market behaviour.																	0924-669X	1573-7497															10.1007/s10489-020-01839-5		SEP 2020											
J								A Reservoir Computing Approach to Word Sense Disambiguation	COGNITIVE COMPUTATION										Reservoir computing; Echo state network; Word sense disambiguation; Word embeddings		Reservoir computing (RC) has emerged as an alternative approach for the development of fast trainable recurrent neural networks (RNNs). It is considered to be biologically plausible due to the similarity between randomly designed artificial reservoir structures and cortical structures in the brain. The paper continues our previous research on the application of a member of the family of RC approaches-the echo state network (ESN)-to the natural language processing (NLP) task of Word Sense Disambiguation (WSD). A novel deep bi-directional ESN (DBiESN) structure is proposed, as well as a novel approach for exploiting reservoirs' steady states. The models also make use of ESN-enhanced word embeddings. The paper demonstrates that our DBiESN approach offers a good alternative to previously tested BiESN models in the context of the word sense disambiguation task having smaller number of trainable parameters. Although our DBiESN-based model achieves similar accuracy to other popular RNN architectures, we could not outperform the state of the art. However, due to the smaller number of trainable parameters in the reservoir models, in contrast to fully trainable RNNs, it is to be expected that they would have better generalization properties as well as higher potential to increase their accuracy, which should justify further exploration of such architectures.																	1866-9956	1866-9964															10.1007/s12559-020-09758-w		SEP 2020											
J								InceptionTime: Finding AlexNet for time series classification	DATA MINING AND KNOWLEDGE DISCOVERY										Time series classification; Deep learning; Scalable model; Inception	DISTANCE	This paper brings deep learning at the forefront of research into time series classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last fewdecades ofwork in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in O(N-2 center dot T-4) for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with N = 1500 time series of short length T = 46. Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime-an ensemble of deep Convolutional Neural Network models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1500 time series in one hour but it can also learn from 8M time series in 13 h, a quantity of data that is fully out of reach of HIVE-COTE.																	1384-5810	1573-756X				NOV	2020	34	6					1936	1962		10.1007/s10618-020-00710-y		SEP 2020											
J								A framework for evaluating ontology meta-matching approaches	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Ontology matching; Evolutionary ontology matching; Ontology meta-matching; Metaheuristics; Data provenance	MEMETIC ALGORITHM; ALIGNMENTS	Ontology matching has become a key issue to solve problems of semantic heterogeneity. Several researchers propose diverse techniques that can be used in distinct scenarios. Ontology meta-matching approaches are a specialization of ontology matching and have achieved good results in pairs of ontologies with different types of heterogeneities. However, developing a new ontology meta-matcher can be a costly process and a lot of experiments are often carried out to analyze the behavior of the matcher. This article presents a modularized framework that covers the main stages of the ontology meta-matching evaluation process. This framework aims to aid researchers to develop and analyze algorithms for ontology meta-matching, mainly metaheuristic-based supervised and unsupervised approaches. As the main contribution of the research, the framework proposed will facilitate the evaluation of ontology meta-matching approaches and, as the secondary contribution, a data provenance model that captures the main information generated and consumed throughout experiments is presented in the framework.																	0925-9902	1573-7675															10.1007/s10844-020-00615-8		SEP 2020											
J								Learning cell embeddings for understanding table layouts	KNOWLEDGE AND INFORMATION SYSTEMS										Tabular data; Table layout; Cell embeddings; Representation learning; Cell classification; Semi-supervised learning		There is a large amount of data on the web in tabular form, such as Excel sheets, CSV files, and web tables. Often, tabular data is meant for human consumption, using data layouts that are difficult for machines to interpret automatically. Previous work uses the stylistic features of tabular cells (such as font size, border type, and background color) to classify tabular cells by their role in the data layout of the document (top attribute, data, metadata, etc.). In this paper, we propose a deep neural network model which can embed semantic and contextual information about tabular cells in a low-dimensional cell embedding space. We pre-train this cell embedding model on a large corpus of tabular documents from various domains. We then propose a classification technique based on recurrent neural networks (RNNs) to use our pre-trained cell embeddings, combining them with stylistic features introduced in previous work, in order to improve the performance of cell type classification in complex documents. We evaluate the performance of our system on three datasets containing documents with various data layouts, in two settings: in-domain and cross-domain training. Our evaluation result shows that our proposed cell vector representations in combination with our RNN-based classification technique significantly improve cell type classification performance.																	0219-1377	0219-3116															10.1007/s10115-020-01508-6		SEP 2020											
J								The State Space of Artificial Intelligence	MINDS AND MACHINES										Artificial intelligence; Deep learning; Self-learning; Semantic grounding; State space of AI; Self-x-property; Self-x-capacity	NEURAL-NETWORKS; NEUROSCIENCE; BRAINS; GAME; GO	The goal of the paper is to develop and propose a general model of the state space of AI. Given the breathtaking progress in AI research and technologies in recent years, such conceptual work is of substantial theoretical interest. The present AI hype is mainly driven by the triumph of deep learning neural networks. As the distinguishing feature of such networks is the ability to self-learn, self-learning is identified as one important dimension of the AI state space. Another dimension is recognized as generalization, the possibility to go over from specific to more general types of problems. A third dimension is semantic grounding. Our overall analysis connects to a number of known foundational issues in the philosophy of mind and cognition: the blockhead objection, the Turing test, the symbol grounding problem, the Chinese room argument, and use theories of meaning. It shall finally be argued that the dimension of grounding decomposes into three sub-dimensions. And the dimension of self-learning turns out as only one of a whole range of "self-x-capacities" (based on ideas of organic computing) that span the self-x-subspace of the full AI state space.																	0924-6495	1572-8641															10.1007/s11023-020-09538-3		SEP 2020											
J								Region Stability and Stabilization of Recurrent Neural Network with Parameter Disturbances	NEURAL PROCESSING LETTERS										Region stability; Stabilization; Recurrent neural networks; Adaptive control	GLOBAL EXPONENTIAL STABILITY; ASYMPTOTIC STABILITY; LAGRANGE STABILITY; ROBUST STABILITY; GENERAL-CLASS; TIME; DELAYS; MULTISTABILITY; INVARIANT; CRITERIA	This paper mainly focuses on global region stability and stabilization analysis for recurrent neural networks with certain or uncertain parameter disturbances. Firstly, it presents global region stability results for recurrent neural networks with certain parameter disturbances by state partition and mathematical analysis methods. Next, it designs one adaptive controller to stabilize network states to the desired region for recurrent neural networks with uncertain parameter disturbances. At last, it gives two numerical examples for verifying obtained results.																	1370-4621	1573-773X															10.1007/s11063-020-10344-2		SEP 2020											
J								Robust Biometrics from Motion Wearable Sensors Using a D-vector Approach	NEURAL PROCESSING LETTERS										Person identification and verification; D-vector approach; Gait recognition; Motion wearable sensors	PERSON IDENTIFICATION; FEATURE-EXTRACTION	This paper proposes a d-vector approach for extracting robust biometrics from inertial signals recorded with wearable sensors. The d-vector approach generates identity representations using a deep learning architecture composed of Convolutional Neural Networks. This architecture includes two convolutional layers for learning features from the inertial signal spectrum. These layers were pretrained using data from 154 subjects. After that, additional fully connected layers were attached to perform user identification and verification, considering 36 new subjects. This paper compares the proposed d-vector approach with previous proposed algorithms using in-the-wild recordings in different scenarios. The results demonstrated the robustness of the proposed d-vector approach for in-the-wild conditions: 97.69% and 94.16% accuracies (for user identification) and 99.89% and 99.67% Areas Under the Curve (for user verification) were obtained using one (walking) or several activities (walking, jogging and stairs) respectively. These results were also verified in laboratory conditions improving the performance reported in previous works. All the analyses were carried out using public datasets recorded at the Wireless Sensor Data Mining laboratory.																	1370-4621	1573-773X															10.1007/s11063-020-10339-z		SEP 2020											
J								Characterization and computation of ancestors in reaction systems	SOFT COMPUTING										Reaction systems; Ancestor computation; Computational complexity; Causality relations	LAC OPERON; NETWORK	In reaction systems, preimages andnth ancestors are sets of reactants leading to the production of a target set of products in either 1 ornsteps, respectively. Many computational problems on preimages and ancestors, such as finding all minimum-cardinalitynth ancestors, computing their size or counting them, are intractable. In this paper, we characterize allnth ancestors using a Boolean formula that can be computed in polynomial time. Once simplified, this formula can be exploited to easily solve all preimage and ancestor problems. This allows us to directly relate the difficulty of ancestor problems to the cost of the simplification so that new insights into computational complexity investigations can be achieved. In particular, we focus on two problems: (i) deciding whether a preimage/nth ancestor exists and (ii) finding a preimage/nth ancestor of minimal size. Our approach is constructive, it aims at finding classes of reactions systems for which the ancestor problems can be solved in polynomial time, in exact or approximate way.																	1432-7643	1433-7479															10.1007/s00500-020-05300-0		SEP 2020											
J								A simulated annealing optimization algorithm based nonlinear model predictive control strategy with application	EVOLVING SYSTEMS										Exothermic batch reactor; Nonlinear model predictive control; Runaway reaction; Simulated annealing	OPTIMAL TEMPERATURE PROFILES; TRACKING; POLYMERIZATION	Batch reactors are widely used in the production of fine chemicals, polymers, pharmaceuticals and other specialty products. For certain exothermic reactions, the transient operation of the reactor with respect to small changes in critical parameters like coolant temperature and initial composition of the reactants can lead runaway condition of the reactor. In order to avoid the hazards associated with runaway situations, it is imperative to operate the reactor by means of an efficient controller. This work presents a nonlinear model predictive control (NMPC) strategy based on simulated annealing (SA) for the temperature control of a batch reactor involving a highly exothermic runaway reaction. The efficacy of the proposed strategy is studied through simulation for the temperature control of the reactor in which a highly parametric sensitive exothermic reaction of hydrolysis of acetic anhydride with sulfuric acid as catalyst and acetic acid as a solvent is carried out. The controller is found effective in averting the runaway behavior with the smooth and quick attainment of the desired operating condition. The results demonstrate the better performance of the SA based NMPC over the linear model predictive controller (LMPC).																	1868-6478	1868-6486															10.1007/s12530-020-09354-1		SEP 2020											
J								Low power consumption and reliability of wireless communication network in intelligent parking system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent parking; Internet of things; Magnetic sensor; Wireless communication network; Low power consumption		With the rapid increase in the number of vehicles in our country, parking and parking management are becoming more and more difficult. Use information technology to improve the use of parking spaces and improve the efficiency of parking management, it is an effective way to alleviate the parking problem. In 2015, the research on "Internet + parking" accelerated rapidly in China. And various intelligent parking system models are introduced. Firstly, the parking conditions at home and abroad are studied, and the existing parking systems are analyzed. It is found that the current parking system can hardly be used in public parking areas such as residential areas, roadside and square. In order to solve the core part of intelligent parking system in public parking IoT Low energy consumption parking space control. Using new detection technology, parking lock control technology, wireless sensor network technology, solar charging technology, power management technology. Parking space detection module, parking lock control module and wireless sensor network module and power control management module with additional solar charging are designed. There are three problems specifically for intelligent parking in public parking lots: parking detection and automatic control, low-energy wireless signal transmission, efficient power management, effectively improve the performance of intelligent parking system and reduce energy consumption. This paper uses advanced wireless technology to construct and design intelligent parking system. Through the research on the development and management characteristics and application examples of intelligent transportation in developed countries such as Europe and America, based on China's basic national conditions and advanced wireless communication technology, radio frequency technology and computer technology research, establish an integrated system. Finally, the performance and power consumption of the intelligent parking system are tested, to ensure the feasibility of the system, this kind of system has certain application value in the scenario of resource sharing and intelligent management of public parking spaces.																	1868-5137	1868-5145															10.1007/s12652-020-02183-9		SEP 2020											
J								Overlapping gait pattern recognition using regression learning for elderly patient monitoring	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Conditional analysis; Gait detection; Patient monitoring; Pattern recognition; Regression learning	EVENT DETECTION; MACHINE	Gait recognition in elderly patient monitoring is a standard process that employs medical healthcare systems, wearable sensors, motion capturing devices, and Information and Communication Technologies (ICT). The patterns of the patient movement are observed at different time instances for identifying the abnormality in gaits to provide better assistance. In this article, a novel Overlapping Gait Pattern Recognition method based on Regression Learning (RL) is introduced. This method classifies the gait pattern based on the direction of movement and angle of deviation of the patient at the initial stage. The analyses of differentiation are performed using RL for identifying the errors and differences in gait patterns through correlation. The errors are recurrently analyzed through different iterates for approximating the recognition accuracy in a reduced time. The classification of patterns through correlation and conditional analysis of the regression helps identify the errors through intense learning and deviation identification. The proposed method is found to achieve better recognition accuracy, fewer error rates, and smaller recognition delays for different gait patterns.																	1868-5137	1868-5145															10.1007/s12652-020-02503-z		SEP 2020											
J								Equivalence between Digital Well-Composedness and Well-Composedness in the Sense of Alexandrov onn-D Cubical Grids	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Digital topology; Discrete surfaces; Well-composed images		Among the different flavors ofwell-composednesseson cubical grids, two of them, called, respectively,digital well-composedness(DWCness) andwell-composedness in the sense of Alexandrov(AWCness), are known to be equivalent in 2D and in 3D. The former means that a cubical set does not contain critical configurations, while the latter means that the boundary of a cubical set is made of a disjoint union of discrete surfaces. In this paper, we prove that this equivalence holds inn-D, which is of interest because today images are not only 2D or 3D but also 4D and beyond. The main benefit of this proof is that the topological properties available for AWC sets, mainly their separation properties, are also true for DWC sets, and the properties of DWC sets are also true for AWC sets: an Euler number locally computable, equivalent connectivities from a local or global point of view. This result is also true for gray-level images thanks to cross section topology, which means that the sets of shapes of DWC gray-level images make a tree like the ones of AWC gray-level images.																	0924-9907	1573-7683				NOV	2020	62	9					1285	1333		10.1007/s10851-020-00988-z		SEP 2020											
J								Pythagorean fuzzy combined compromise solution method integrating the cumulative prospect theory and combined weights for cold chain logistics distribution center selection	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										cold chain distribution center; combined compromise solution method; cumulative prospect theory; green logistics; Pythagorean fuzzy sets	DECISION-MAKING; ENVIRONMENTAL-PROTECTION; SUPPLIER SELECTION; MEMBERSHIP GRADES; LOCATION; TRANSITION; MANAGEMENT; TODIM; MODEL; MCDM	The evaluation and selection of cold chain logistics distribution centers are of vital importance for third-party logistics companies which want to build green cold chain logistics networks. To select distribution centers, the conflicts among multiple criteria should be considered. The combined compromise solutions (CoCoSo) method can help enterprises make a structural decision; however, in the original CoCoSo method, the evaluation information was expressed by crisp numbers. Nevertheless, in many cases, because of the imprecision and incompleteness of information, it may be more flexible for evaluators to provide imprecise and fuzzy values rather than crisp numbers. In addition, the judgment values are often expressed based on decision-makers' psychological expectations. The evaluation criteria of alternatives have relevance to some extent, which would influence the evaluation results. Based on these concerns, this study presents a modified CoCoSo method in the Pythagorean fuzzy environment in which evaluators can express psychological expectations on alternatives. To achieve this goal, the cumulative prospect theory is introduced to obtain the Pythagorean fuzzy prospect weights. Then, an objective weight determination method of criteria under the Pythagorean fuzzy environment is proposed to eliminate the influence of homogeneity of criteria. Based on the Pythagorean fuzzy prospect weights and the combined weights, the original CoCoSo method is extended to the Pythagorean fuzzy environment. A case of selection logistics distribution center is investigated to demonstrate the practicality of the proposed method. The advantages of the proposed method are verified by comparative analysis.																	0884-8173	1098-111X				DEC	2020	35	12					2009	2031		10.1002/int.22281		SEP 2020											
J								Genetic algorithm based approaches to solve the order batching problem and a case study in a distribution center	JOURNAL OF INTELLIGENT MANUFACTURING										Genetic algorithms; Metaheuristics; Order batching problem; Warehousing; Logistics	NEIGHBORHOOD SEARCH; MULTIPLE PICKERS; PICKING; OPTIMIZATION; HYBRID; WAREHOUSE; ASSIGNMENT	The order batching problem is a combinatorial optimization problem that arises in the warehouse order picking process. In the order batching problem, the aim is to find groups of orders and picking routes of these groups to minimize distance travelled by the order picker. This problem is encountered especially in manual order picking systems where the capacity of picking vehicle is limited. Solving the order batching problem becomes more important when the size of the problem (e.g. number of storage locations, number of aisles, number of customer orders, etc.) is large. The content of the batch and picking route affect the retrieval-time of the orders. Therefore, an effective batching and routing approach is essential in reducing the time needed to collect ordered items. The main objective of this study is to develop fast and effective metaheuristic approaches to solve the order batching problem. For this purpose, two genetic algorithm based metaheuristic approaches are proposed. The numerical test of the proposed algorithms is performed with generated data sets. The proposed methods are thought to be useful to solve real-life problems in different warehouse configurations. Accordingly, a real case study is conducted in the distribution center of a well-known retailer in Turkey. The case study includes the storage assignment process of incoming products. The results demonstrate that developed algorithms are practical and useful in real-life problems.																	0956-5515	1572-8145															10.1007/s10845-020-01653-3		SEP 2020											
J								Attribute reduction of SE-ISI concept lattices for incomplete contexts	SOFT COMPUTING										Incomplete context; Concept lattice; SE-ISI formal concept; Attribute reduction; Discernibility matrix	APPROXIMATE CONCEPT CONSTRUCTION; KNOWLEDGE REDUCTION; RULE ACQUISITION; FORMAL CONTEXTS; DECISION CONTEXTS; 3-WAY DECISIONS; CONNECTIONS; OBJECT	Three-way concept analysis in incomplete contexts lays the theory dealing with the data in incomplete contexts, especially three kinds of partially known formal concepts including SE-ISI formal concept, ISE-SI formal concept and ISE-ISI formal concept. Generally speaking, not every attribute is essential in an incomplete context since the purpose of research is different. Thus, we propose four kinds of attribute reduction of SE-ISI concept lattices based on different criteria. Then, we discuss the relationships among the four kinds of attribute reduction, including the relationships among the consistent sets and relationships among the reducts. Finally, based on discernibility matrices and discernibility functions, the approaches to obtaining these attribute reduction are presented.																	1432-7643	1433-7479				OCT	2020	24	20					15143	15158		10.1007/s00500-020-05271-2		SEP 2020											
J								Applying deep neural networks for user intention identification	SOFT COMPUTING										Intention identification; Intention mining; Deep learning; CNN; LSTM; Product reviews; Social media services		The social media revolution has provided the online community an opportunity and facility to communicate their views, opinions and intentions about events, policies, services and products. The intent identification aims at detecting intents from user reviews, i.e., whether a given user review contains intention or not. The intent identification, also called intent mining, assists business organizations in identifying user's purchase intentions. The prior works have focused on using only the CNN model to perform the feature extraction without retaining the sequence correlation. Moreover, many recent studies have applied classical feature representation techniques followed by a machine learning classifier. We examine the intention review identification problem using a deep learning model with an emphasis on maintaining the sequence correlation and also to retain information for a long time span. The proposed method consists of the convolutional neural network along with long short-term memory for efficient detection of intention in a given review, i.e., whether the review is an intent vs non-intent. The experimental results depict that the performance of the proposed system is better with respect to the baseline techniques with an accuracy of 92% for Dataset1 and 94% for Dataset2. Moreover, statistical analysis also depicts the effectiveness of the proposed method with respect to the comparing methods.																	1432-7643	1433-7479															10.1007/s00500-020-05290-z		SEP 2020											
J								A context-aware recommendation approach based on feature selection	APPLIED INTELLIGENCE										Recommender system; Context-aware; Feature selection; Restaurant recommendation	CLASSIFICATION; REGRESSION; SYSTEMS	Contextual information can be used in recommender systems to make recommendation more efficient. Recent research has made progress in combining contextual information into representation models for recommendations. However, the existing approaches do not well address the problem of data sparsity, and they suffer from context redundancy. To deal with these problems, this paper proposes a context-aware recommendation approach based on embedded feature selection. It gets rid of context redundancy by generating a minimum subset of all contextual information and allocates the weight to each context appropriately. Experiments on the restaurant recommendation shows that the proposed approach has better performance.																	0924-669X	1573-7497															10.1007/s10489-020-01835-9		SEP 2020											
J								Data-guided multi-granularity selector for attribute reduction	APPLIED INTELLIGENCE										Attribute reduction; Data-guided; Parameterized multi-granularity	KNOWLEDGE GRANULATION; INFORMATION FUSION; ROUGH SETS; ACCELERATOR; APPROXIMATION; DISTANCE; MODEL	Presently, the greedy searching strategy has been widely accepted for obtaining reduct in the field of rough set. In the framework of greedy searching, the evaluation of the candidate attribute is crucial, because the evaluation can determine the final result of reduct to a large extent. However, most of the previous evaluations are designed by considering one and only one fixed granularity, which fails to make the multi-view based evaluation possible. To fill such gap, a Parameterized Multi-granularity Attribute Selector is proposed for obtaining reduct in this paper. Our attribute selector consists of two parts: one is the multi-granularity attribute selector which evaluates and selects attributes through using the information provided by multiple different granularities; the other is the data-guided parameterized granularity selector which generates multiple different parameterized granularities through taking the characteristics of data into account. The experimental results over 15 UCI data sets show the following: 1) compared with the state of the art approaches for obtaining reducts, our proposed attribute selector can contribute to reduct with higher stability; 2) our proposed attribute selector will not provide the reduct with poorer classification performance. This research suggests a new trend for the multi-granularity mechanism in the problem of attribute reduction.																	0924-669X	1573-7497															10.1007/s10489-020-01846-6		SEP 2020											
J								Video detection of foreign objects on the surface of belt conveyor underground coal mine based on improved SSD	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING												Aiming at the problem of belt conveyor damage caused by the presence of foreign objects on the belt conveyor in coal mines, this paper proposed that video detection of foreign objects on the belt surface was performed based on SSD. Improvements on SSD network are made from following aspects. Firstly, the deep separable convolution method is used to reduce the amount of parameters in the SSD algorithm and improve the speed. Then, GIOU loss function is adopted instead of the position loss function in the original SSD to improve the detection accuracy. Finally, the extracted position of the feature map and the proportion of the default boxes are optimized to improve the detection accuracy. The experiment results show that the improved algorithm proposed in this paper is superior to the original SSD algorithm, the average accuracy rate has been increased from 87.1 to 90.2%, and the detection frame rate has been increased from 32 to 41 FPS.																	1868-5137	1868-5145															10.1007/s12652-020-02495-w		SEP 2020											
J								A node to node security for sensor nodes implanted in cross cover multi-layer architecture using Mc-Nie algorithm	EVOLUTIONARY INTELLIGENCE										Cross covered multi-layer architecture; Cross-layered architecture; Energy consumption in WSN; Mc-Nie cryptosystem; Multi-layer architecture; Network lifetime; Sensor nodes; Wireless sensor networks	NETWORKS; LIFETIME; ATTACK; PROTOCOL	Security, and lifetime enhancement are the two key factors in the field of wireless sensor networks. Since the sensor nodes are placed in critical regions and are irreplaceable, it is essential to improve their lifetime through avoiding the energy consumption unnecessarily. Moreover, it is found that the sensor nodes are easily vulnerable due to their random placement. We propose a cross covered multi-layer architecture with an efficient Mc-Nie security algorithm. Moreover, the details of various attacks are also given in this paper. It is found that the proposed approach is performing well when compared to the recent state-of-the-art approaches. We have considered various parameters such as energy consumption, detecting the attacks, attack detection percentage, packet loss, and packet overhead in order to evaluate the performance of the proposed system. The improvement in energy consumption rate is around 81% when compared to the non-cross covered architecture.																	1864-5909	1864-5917															10.1007/s12065-020-00478-6		SEP 2020											
J								A group evaluation based binary PSO algorithm for feature selection in high dimensional data	EVOLUTIONARY INTELLIGENCE										Binary particle swarm optimization; Group evaluation; Feature selection; Stability indices; High dimensional data	ROUGH SETS; OPTIMIZATION	This paper proposes a group evolution based feature selection technique using Binary PSO, which is an essential tool of pre-processing for solving classification problem. A new updating mechanism for calculating Pbest and Gbest are also proposed and the relevance and redundancy of the selected feature subsets are considered as an objective function. The proposed algorithm is tested and compared with four existing feature selection algorithms. In this study, a decision tree classifier is employed to evaluate the classification accuracy of the selected feature subsets on five benchmark datasets. The result shows that proposed algorithm can be successfully used to improve classification accuracy and to improve stability indices as well. It is also observed that with increased weight on relevance of the function, there is a significant reduction on the cardinality of features and increase in classification accuracy. The existing four algorithms usually select a smaller feature subset while the proposed algorithm can achieves higher classification accuracy on most of the test datasets.																	1864-5909	1864-5917															10.1007/s12065-020-00482-w		SEP 2020											
J								Approximation of two-variable functions using high-order Takagi-Sugeno fuzzy systems, sparse regressions, and metaheuristic optimization	SOFT COMPUTING										Fuzzy systems; Sparse regression; Metaheuristic optimization	PARTICLE SWARM OPTIMIZATION; RULE-BASED SYSTEMS; GENETIC ALGORITHM; MODEL ALGORITHM; IDENTIFICATION; SELECTION; PSO	This paper proposes a new hybrid method for training high-order Takagi-Sugeno fuzzy systems using sparse regressions and metaheuristic optimization. The fuzzy system is considered with Gaussian fuzzy sets in the antecedents and high-order polynomials in the consequents of fuzzy rules. The fuzzy sets can be chosen manually or determined by a metaheuristic optimization method (particle swarm optimization, genetic algorithm or simulated annealing), while the polynomials are obtained using ordinary least squares, ridge regression or sparse regressions (forward selection, least angle regression, least absolute shrinkage and selection operator, and elastic net regression). A quality criterion is proposed that expresses a compromise between the prediction ability of the fuzzy model and its sparsity. The conducted experiments showed that: (a) the use of sparse regressions and/or metaheuristic optimization can reduce the validation error compared with the reference method, and (b) the use of sparse regressions may simplify the fuzzy model by zeroing some of the coefficients.																	1432-7643	1433-7479				OCT	2020	24	20					15113	15127		10.1007/s00500-020-05238-3		SEP 2020											
J								A DFIG-based wind energy conversion system (WECS) for LVRT enhancement using a hybrid approach: an efficient MEHRFA technique	SOFT COMPUTING										WECS; LVDT; DFIG; Modified elephant herding algorithm; Random forest algorithm; MEHFRA	SLIDING-MODE CONTROL; THROUGH CAPABILITY ENHANCEMENT; RANDOM FOREST; FAULT-RIDE; FEEDBACK LINEARIZATION; PREDICTIVE CONTROL; OPTIMIZATION; ALGORITHM; TURBINE; CONTROLLER	This paper proposes a hybrid algorithm-based control model for enhancing the low voltage ride through (LVRT) capability of the doubly fed induction generator (DFIG) system. The hybrid algorithm is the combined execution of both modified elephant herding algorithm (MEHA) and random forest (RF) algorithm, and hence the technique is named as MEHRFA. The purpose of the proposed control technique ensures the LVRT capability in the DFIG-based WECS at voltage drop and fault conditions. Here, MEHA approach is used as offline manner to identify the ideal solutions from the available searching space and creates the training dataset. In the MEHA technique, multiple parameters are considered which are related to LVRT such as, voltage, real and reactive powers and current. Utilizing these parameters, the objective function is characterized and solved by MEHA method. In light of the accomplished dataset, the RF performs and predicts the best possible control signals for machine-side converter and grid-side converter. In this manner, the LVRT capability of DFIG system and power quality issues has been enhanced with the assistance of proposed technique. The proposed procedure is implemented in MATLAB/Simulink working platform, and the exhibition is assessed by utilizing the comparison analysis with existing techniques.																	1432-7643	1433-7479															10.1007/s00500-020-05276-x		SEP 2020											
J								Toward Alzheimer's disease classification through machine learning	SOFT COMPUTING										Multivariate linear regression; Logistic regression; Support vector machine (SVM); Feature scaling; Normalization; ADNI	MILD COGNITIVE IMPAIRMENT; PREDICTORS; DEMENTIA	Alzheimer's disease (AD) and cognitive impairment due to aging are the recently prevailing diseases among aged inhabitants due to an increase in the aging population. Several demographic characters, structural and functional neuroimaging investigations, cardio-vascular studies, neuropsychiatric symptoms, cognitive performances, and biomarkers in cerebrospinal fluids are the various predictors for AD. These input features can be considered for the prediction of symptoms whether they belong to AD or normal cognitive impairment due to aging. In the proposed study, the hypothesis is derived for supervised learning methods such as multivariate linear regression, logistic regression, and SVM. Feature scaling and normalization are performed with features as initial steps for applying the parameters to derive the hypothesis. Performance metrics are analyzed with the implementation results. The present work is applied to 1000 baseline assessment data from Alzheimer's Disease Neuroimaging Initiative (ADNI) studies that give conversion prediction. The comparison of results in the literature suggests that the efficiency of the proposed study is highly advantageous in differentiating AD pathology from cognitive impairment due to aging.																	1432-7643	1433-7479															10.1007/s00500-020-05292-x		SEP 2020											
J								An improved optimization technique using Deep Neural Networks for digit recognition	SOFT COMPUTING										Recurrent deep neural networks; Hybrid mini-batch and stochastic Hessian-free optimization; Pattern recognition; Predictive analysis; Classification and validation		In the world of information retrieval, recognizing hand-written digits stands as an interesting application of machine learning (deep learning). Though this is already a matured field, a way to recognize digits using an effective optimization using soft computing technique is a challenging task. Training such a system with larger data often fails due to higher computation and storage. In this paper, a recurrent deep neural network with hybrid mini-batch and stochastic Hessian-free optimization (MBSHF) is for accurate and faster convergence of predictions as outputs. A second-order approximation is used for achieving better performance for solving quadratic equations which greatly depends on computation and storage. Also, the proposed technique uses an iterative minimization algorithm for faster convergence using a random initialization though huge additional parameters are involved. As a solution, a convex approximation of MBSHF optimization is formulated and its performance on experimenting with the standard MNIST dataset is discussed. A recurrent deep neural network till a depth of 20 layers is successfully trained using the proposed MBSHF optimization, resulting in a better quality performance in computation and storage. The results are compared with other standard optimization techniques like mini-batch stochastic gradient descent (MBSGD), stochastic gradient descent (SGD), stochastic Hessian-free optimization (SHF), Hessian-free optimization (HF), nonlinear conjugate gradient (NCG). The proposed technique produced higher recognition accuracy of 12.2% better than MBSGD, 27.2% better than SHF, 35.4% better than HF, 40.2% better than NCG and 32% better than SGD on an average when applied to 50,000 testing sample size.																	1432-7643	1433-7479															10.1007/s00500-020-05262-3		SEP 2020											
J								Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network	APPLIED INTELLIGENCE										DeTraC; Covolutional neural networks; COVID-19 detection; Chest X-ray images; Data irregularities		Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural networks (CNNs) for image recognition and classification. However, due to the limited availability of annotated medical images, the classification of medical images remains the biggest challenge in medical diagnosis. Thanks to transfer learning, an effective mechanism that can provide a promising solution by transferring knowledge from generic object recognition tasks to domain-specific tasks. In this paper, we validate and a deepCNN, called Decompose, Transfer, and Compose (DeTraC), for the classification of COVID-19 chest X-ray images.DeTraCcan deal with any irregularities in the image dataset by investigating its class boundaries using a class decomposition mechanism. The experimental results showed the capability ofDeTraCin the detection of COVID-19 cases from a comprehensive image dataset collected from several hospitals around the world. High accuracy of 93.1% (with a sensitivity of 100%) was achieved byDeTraCin the detection of COVID-19 X-ray images from normal, and severe acute respiratory syndrome cases.																	0924-669X	1573-7497															10.1007/s10489-020-01829-7		SEP 2020											
J								Robust semi-supervised support vector machines with Laplace kernel-induced correntropy loss functions	APPLIED INTELLIGENCE										Robustness; Correntropy; Semi-supervised classification; Laplacian support vector machine	FEATURE-SELECTION; REGRESSION; REGULARIZATION; CLASSIFICATION; FRAMEWORK	The insufficiency and contamination of supervision information are the main factors affecting the performance of support vector machines (SVMs) in real-world applications. To address this issue, novel correntropy loss functions and Laplacian SVM (LapSVM) are utilized for robust semi-supervised classification. It is known that correntropy loss functions have been used in robust learning and achieved promising results. However, the potential for more diverse priors has not been extensively explored. In this paper, a correntropy loss function induced from Laplace kernel function, called LK-loss, is applied to LapSVM for the construction of robust semi-supervised classifier. Properties of LK-loss are demonstrated including robustness, symmetry, boundedness, Fisher consistency and asymptotic approximation behaviors. Moreover, the asymmetric version of LK-loss is introduced to further improve the performance. Concave-convex procedure (CCCP) technique is used to handle the non-convexity of Laplace kernel-induced correntropy loss functions iteratively. Experimental results show that in most cases, the proposed methods have better generalization performance than the comparing ones, which demonstrate the feasibility and effectiveness of the proposed semi-supervised classification framework.																	0924-669X	1573-7497															10.1007/s10489-020-01865-3		SEP 2020											
J								Deep network compression with teacher latent subspace learning and LASSO	APPLIED INTELLIGENCE										Deep neural network; Compression; Pruning; Subspace learning; LASSO	NEURAL-NETWORKS	Deep neural networks have been shown to excel in understanding multimedia by using latent representations to learn complex and useful abstractions. However, they remain unpractical for embedded devices due to memory constraints, high latency, and considerable power consumption at runtime. In this paper, we propose the compression of deep models based on learning lower dimensional subspaces from their latent representations while maintaining a minimal loss of performance. We leverage on the premise that deep convolutional neural networks extract many redundant features to learn new subspaces for feature representation. We construct a compressed model by reconstruction from representations captured by an already trained large model. As compared to state-of-the-art, the proposed approach does not rely on labeled data. Moreover, it allows the use of sparsity inducing LASSO parameter penalty to achieve better compression results than when used to train models from scratch. We perform extensive experiments using VGG-16 and wide ResNet models on CIFAR-10, CIFAR-100, MNIST and SVHN datasets. For instance, VGG-16 with 8.96M parameters trained on CIFAR-10 was pruned by 81.03 % with only 0.26 % generalization performance loss. Correspondingly, the size of the VGG-16 model is reduced from 35MB to 6.72MB to facilitate compact storage. Furthermore, the associated inference time for the same VGG-16 model is reduced from 1.1 secs to 0.6 secs so that inference is accelerated. Particularly, the proposed student models outperform state-of-the-art approaches and the same models trained from scratch.																	0924-669X	1573-7497															10.1007/s10489-020-01858-2		SEP 2020											
J								Hierarchical classification with multi-path selection based on granular computing	ARTIFICIAL INTELLIGENCE REVIEW										Granular computing; Hierarchical classification; Inter-level error propagation; Multi-path selection	CLASSIFIERS	Hierarchical classification is a research hotspot in machine learning due to the widespread existence of data with hierarchical class structures. Existing hierarchical classification methods based on granular computing can effectively reduce the computational complexity by considering the granularity of classes. However, their predictive accuracy is affected by inter-level error propagation within the hierarchy. In this paper, we propose a hierarchical classification method with multi-path selection based on coarse- and fine-grained class relationships, which mitigates the inter-level error propagation problem. Firstly, we use a top-down recursive method to calculate the probabilities of the hierarchical classes by logistic regression classification. Secondly, the current class probability is calculated by combining the parent and current classes probabilities. We select multiple possible fine-grained classes at the current level according to their sibling relationships. Compared with existing methods, the proposed method reduces the possibility of misclassification from the upper layer. Finally, the multi-path prediction result is provided to a classical classifier for final prediction. Our hierarchical classification method is evaluated on six benchmark datasets to demonstrate that it provides better classification performance than existing state-of-the-art hierarchical methods.																	0269-2821	1573-7462															10.1007/s10462-020-09899-2		SEP 2020											
J								Multipath feature recalibration DenseNet for image classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										DenseNet; Image classification; Multipath DenseBlocks; Feature recalibration; MFR-DenseNet		Recently, deep neural networks have demonstrated their efficiency in image classification tasks, which are commonly achieved by an extended depth and width of network architecture. However, poor convergence, over-fitting and gradient disappearance might be generated with such comprehensive architectures. Therefore, DenseNet is developed to address these problems. Although DenseNet adopts bottleneck technique in DenseBlocks to avoid relearning feature-maps and decrease parameters, this operation may lead to the skip and loss of important features. Besides, it still takes oversized computational power when the depth and width of the network architecture are increased for better classification. In this paper, we propose a variate of DenseNet, named Multipath Feature Recalibration DenseNet (MFR-DenseNet), to stack convolution layers instead of adopting bottleneck for improving feature extraction. Meanwhile, we build multipath DenseBlocks with Squeeze-Excitation (SE) module to represent the interdependencies of useful feature-maps among different DenseBlocks. Experiments in CIFAR-10, CIFAR-100, MNIST and SVHN reveal the efficiency of our network, with further reduced redundancy whilst maintaining the high accuracy of DenseNet.																	1868-8071	1868-808X															10.1007/s13042-020-01194-4		SEP 2020											
J								Improvement of a cement rotary kiln performance using artificial neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cement rotary kiln; Process control; Artificial neural network; Optimization	IDENTIFICATION	In order to investigate the effect of parameters and system optimization, the processes must be modeled first. Cement rotary kiln systems are complex because of non-linear, time invariant and full of behavioral uncertainty where the mathematical modeling of the plant is impossible. Artificial neural network (ANN) is one of the best tools for improving the performance of such processes. In this study, the operational data from a cement factory are gathered and the relationships between variables analyzed via using ANN via MATLAB toolbox. ANN proposed 2.7 and 865 rpm for kiln and fan motor speed respectively and 4599.7 Ncm/h for total grate flowrate as optimum values. This research shows that using ANN for improving the performance of rotary kiln is effective and by optimization of operational parameters through ANN and applying them in the rotary kiln, higher production in the cement industry is accessible.																	1868-5137	1868-5145															10.1007/s12652-020-02501-1		SEP 2020											
J								Molecular cancer classification method on microarrays gene expression data using hybrid deep neural network and grey wolf algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cancer classification; DNA Microarray; Deep neural networks; Grey wolf algorithm; Deep learning	FEATURE-SELECTION	Gene selection methods are critical in cancer classification, which depends on the expression of a small number of biomarker genes, which have been a significant issue of enormous recent studies. Microarray technology allows generating tumors gene expression datasets. Cancer classification based on these datasets commonly has a kind of small sample size against the number of genes involved and includes multiclass categories. In this paper, grey wolf algorithm was used for extracting notable features in the pre-processing stage, and deep neural network (DNN) was used as deep learning for improving the accuracy degree of cancer detection from three datasets, i.e., STAD (Stomach adenocarcinoma), LUAD (lung adenocarcinoma) and BRCA (breast invasive carcinoma). The proposed method achieved the highest accuracy for these three datasets. The proposed method was able to achieve accuracy close to 100. Furthermore, the proposed method was compared with linear support vector machine classification, RBF, the nearest neighbor, linear regression, one vs. all, Naive Bayes, and decision tree algorithms. The proposed method had 0.57 improvement on the LUAD dataset, 1.11 optimization on the STAD dataset, and 0.78 development on the BRCA dataset.																	1868-5137	1868-5145															10.1007/s12652-020-02478-x		SEP 2020											
J								A watermarking scheme using intellectual encoding-encryption based blind reversible integer wavelet-singular value decomposition transform for authenticity detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Watermarking; Encoding; RSA encryption; IWT; SVD; QR code		Image processing plays vital role in analysis, storage and transmission of medical information. Digital watermarking (DWM) is considered as one of the more robust technique to protect patients' Medical Health Record (MHR) in the field of telemedicine against various malicious attacks. In this paper, a blind reversible Integer Wavelet-Singular Value Decomposition (IWT-SVD) transform watermarking technique based on Encoded-encrypted Watermark is proposed for security authentication. Prior to watermarking, a novel encoding algorithm based on prime number theory with RSA encryption is used and followed by the QR code generation; is embedded into the patient health record. In this paper, different watermark embedding algorithms are discussed and a blind reversible IWT-SVD transform based DWM with novel encoding algorithm based on prime number theory is proposed with the suitable embedding algorithm along with RSA encryption applied for PHR watermark to be embedded in medical images and medical videos. QR code is also used to encode encrypted data so as to ensure reduced data payload. The proposed Encoded-Encrypted based DWM is applied to the medical images Ultra sound image, MRI image & CT scan images and 2-D & 3D medical videos. The proposed method is tested for various key values and it has achieved a PSNR on an average 80 db, SSIM of 0.95, MSE of 0.007, NAE of 0.006, NCC of almost 1 and the time of evaluation is 22 to50 seconds. The proposed technique based on novel encoding algorithm is compared with Huffman, Arithmetic coding, and with some classical watermarking systems.																	1868-5137	1868-5145															10.1007/s12652-020-02496-9		SEP 2020											
J								Chatter detection for milling using novelp-leader multifractal features	JOURNAL OF INTELLIGENT MANUFACTURING										Chatter detection; Milling processes; Multifractal features; p-leader; Feature selection	FEATURE-SELECTION; WAVELET LEADERS; IDENTIFICATION	Chatter in machining results in poor workpiece surface quality and short tool life. An accurate and reliable chatter detection method is needed before its complete development. This paper applies a novelp-leader multifractal formalism for chatter detection in milling processes. This novel formalism can discover internal singularities rising on unstable signals due to chatter without prior knowledge of the natural frequencies of the machining system. Thep-leader multifractal features are selected by using a multivariate filter method for feature selection, and verified by both numerical simulations and experimental studies with detailed parameter selection discussions when applying this formalism. The proposed method is assessed in terms of their dynamic monitoring abilities and classification accuracies under wide cutting conditions. The results show that the multifractal features can successfully detect chatter with high accuracies and short computation time. For further verification, the proposed method is compared with two commonly-used methods, which indicates that the proposed method gives better classification accuracies, especially when identifying unstable tests.																	0956-5515	1572-8145															10.1007/s10845-020-01651-5		SEP 2020											
J								Automatic solar cell diagnosis and treatment	JOURNAL OF INTELLIGENT MANUFACTURING										Photovoltaics; Solar cell manufacturing; Automatic inspection; Defect classification; Electroluminescence imaging; Random forest; PCA; Gabor filters	DEFECT DETECTION; IMAGES	Solar cells represent one of the most important sources of clean energy in modern societies. Solar cell manufacturing is a delicate process that often introduces defects that reduce cell efficiency or compromise durability. Current inspection systems detect and discard faulty cells, wasting a significant percentage of resources. We introduceCell Doctor, a new inspection system that uses state of the art techniques to locate and classify defects in solar cells and performs a diagnostic and treatment process to isolate or eliminate the defects.Cell Doctoruses a fully automatic process that can be included in a manufacturing line. Incoming solar cells are first moved with a robotic arm to an Electroluminescence diagnostic station, where they are imaged and analysed with a set of Gabor filters, a Principal Component Analysis technique, a Random Forest classifier and different image processing techniques to detect possible defects in the surface of the cell. After the diagnosis, a laser station performs an isolation or cutting process depending on the detected defects. In a final stage, the solar cells are characterised in terms of their I-V Curve and I-V Parameters, in a Solar Simulator station. We validated and testedCell Doctorwith a labelled dataset of images of monocrystalline silicon cells, obtaining an accuracy and recall above 90% forCracks,Area DefectsandFinger interruptions; and precision values of 77% for Finger Interruptions and above 90% forCracksandArea Defects.Which allowsCell Doctorto diagnose and repair solar cells in an industrial environment in a fully automatic way.																	0956-5515	1572-8145															10.1007/s10845-020-01642-6		SEP 2020											
J								Topological Properties of the First Non-Local Digitally Well-Composed Interpolation onn-D Cubical Grids	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Well-composed images; Critical configurations; Digital topology; Tree of shapes; Mathematical morphology	COHOMOLOGY	In discrete topology, we like digitally well-composed (shortly DWC) interpolations because they remove pinches in cubical images. Usual well-composed interpolations are local and sometimes self-dual (they treat in a same way dark and bright components in the image). In our case, we are particularly interested inn-D self-dual DWC interpolations to obtain a purely self-dual tree of shapes. However, it has been proved that we cannot have ann-D interpolation which is at the same time local, self-dual and well-composed. By removing the locality constraint, we have obtained ann-D interpolation with many properties in practice: it is self-dual, DWC, and in-between (this last property means that it preserves the contours). Since we did not publish the proofs of these results before, we propose to provide in a first time the proofs of the two last properties here (DWCness and in-betweeness) and a sketch of the proof of self-duality (the complete proof of self-duality requires more material and will come later). Some theoretical and practical results are given.																	0924-9907	1573-7683				NOV	2020	62	9					1256	1284		10.1007/s10851-020-00989-y		SEP 2020											
J								Prediction of atherosclerosis diseases using biosensor-assisted deep learning artificial neuron model	NEURAL COMPUTING & APPLICATIONS										Deep learning concepts; Cardiovascular diseases; Atherosclerosis disease and artificial neuron	MACHINE; CLASSIFICATION	In the present medical era, the major cause of the rise in death rate worldwide is atherosclerosis disease and this diagnosis is complicated because initial signs are unattended. To reduce the costs of treatment and prevent serious events, it is necessary to improve the prediction accuracy of cardiovascular diseases during plaque formation. This proposal is intended to create a support system for the biosensor-assisted deep learning concepts for detecting atherosclerosis disease. With the clinical data, this mathematical model can predict heart disease based on deep learning-assistedk-means geometric distribution artificial neuron model. The atherosclerotic plaque formation mathematical model explains the early atherosclerotic lesion development in a more accurate manner. Further, the creation of the atherosclerotic plate, the test performs numerical simulations with idealized two-dimensional carotid artery bifurcation geometry. The proposed system has been analyzed using a variety of similarity tests such as the coefficient Matthews's correlation (CMC). Furthermore, the results have reached 95.66% accuracy and 0.93 CMC, which are significantly higher than published conventional research.																	0941-0643	1433-3058															10.1007/s00521-020-05317-4		SEP 2020											
J								Machine Learning Guidance for Connection Tableaux	JOURNAL OF AUTOMATED REASONING										Connection tableaux; Internal guidance; Monte Carlo	PROVING SYSTEM COMPETITION; LOGIC	Connection calculi allow for very compact implementations of goal-directed proof search. We give an overview of our work related to connection tableaux calculi: first, we show optimised functional implementations of connection tableaux proof search, including a consistent Skolemisation procedure for machine learning. Then, we show two guidance methods based on machine learning, namely reordering of proof steps with Naive Bayesian probabilities, and expansion of a proof search tree with Monte Carlo Tree Search.																	0168-7433	1573-0670															10.1007/s10817-020-09576-7		SEP 2020											
J								Experimental Study on Motion Control of Rope-Driven Snake Manipulator Using Velocity Mapping Method	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Snake manipulator; Jacobin matrix; Velocity mapping; Kinematic optimization	CONTINUUM; SYSTEM; ROBOT; ACTUATORS; DESIGN; MODEL	The rope-driven snake manipulator is a bionic mechanism with hyper redundant DOFs and can be applied in narrow and confined environments, such as surgery, spacecraft, nuclear plant, etc. The kinematic mapping expressed by the rope length, joint angle and end pose is highly nonlinear and difficult to be calculated. Moreover, the control methods with rope length as input are prone to redundant driving ropes getting stuck due to differences in model and actual mechanism. Therefore, the perfect kinematic mapping of the rope-driven snake manipulator is necessary for designing high-efficiency motion controllers. In this paper, an analytical mapping about the velocities of ropes, joints and end is established and verified. Firstly, a prototype inspired by the biological snake spine is designed. And then the Jacobian matrix representing the velocity mapping is derived and analyzed in detail. The joint and rope velocities are optimized by configuring the null space vector of the Jacobian matrix. Based on the velocity mapping and optimization, a motion control scheme for the snake manipulator is established to realize servo control of the joints and end. Finally, the trajectory tracking simulation and experiment are executed to verify the velocity mapping theory and control scheme. This research can provide solutions for the complex motion control problems of subsequent snake manipulators.																	0921-0296	1573-0409															10.1007/s10846-020-01249-2		SEP 2020											
J								Recurrent neural network with pooling operation and attention mechanism for sentiment analysis: A multi-task learning approach	KNOWLEDGE-BASED SYSTEMS										Sentiment analysis; Opinion mining; Recurrent neural network; Multi-task; Attention mechanism		Sentiment analysis is designed to classify documents into a fixed number of pre-defined categories that represent different sentiments. Focusing on the limitation of insufficient training data, multi-task learning models based on deep learning have recently achieved significant progress in this field. In general, these models leverage multiple datasets annotated for different tasks to improve the performance on each individual dataset. The improvement is particularly evident on tasks with limited training data. However, most of these models suffer from two limitations. First, they use the final output of the hidden layer as the overall representation of the text, which initially loses a certain amount of semantic information. Second, although some of them utilize a certain gate mechanism to select shared features, some irrelevant shared features are erroneously used owing to polysemy. To address these two limitations, we integrate a pooling layer into a Bi-directional Recurrent Neural Network (BRNN) to extract semantic information comprehensively. We then apply the attention mechanism between shared layers and task-specific layers to identify the effective shared features, and propose an Attention-based Separate Pooling BRNN (ASP-BRNN) model. We conduct experiments to show the effectiveness of our models on four datasets (SST1, SST2, SUBJ, and IMDB), and the accuracy of our models increases steadily by approximately 0.5% for each model. It proves the effectiveness of every newly added component in solving the two problems. A further evaluation on eight datasets shows our proposed ASP-BRNN model outperforms current state-of-the-art models, such as ASP-MTL model (at least +0.2% on Electronics and at most +6.9% on IMDB), MT-ARC-II model (at least +0.2% on SST2 and at most +3.8% on DVDs). (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				SEP 5	2020	203								105856	10.1016/j.knosys.2020.105856													
J								Hashtag our stories: Hashtag recommendation for micro-videos via harnessing multiple modalities	KNOWLEDGE-BASED SYSTEMS										Micro-videos; Hashtag recommendation; Multiple modalities; Deep neural network	SENTIMENT ANALYSIS	Due to the short attention span and instant gratification phenomenon, micro-videos are growing exponentially while gaining more and more concerns. Yet the sheer number of micro-videos leads to severe information overload issues, making it difficult for users to identify their desired micro-videos. The hashtag, mainly utilized in the domain of the microblog or the image, is the indicator or the core idea of the target content and can be applied to various information retrieval scenarios (e.g., search, browse, and categorization). So far, however, little attention has been paid to perform the hashtag recommendation for micro-videos via harnessing multiple modalities. In this article, we devise a neural network-based solution, LOGO (short for "muLti-mOdal-based hashtaG recOmmendation"), to recommend hashtags for micro-videos by utilizing multiple modalities. The proposed LOGO approach first represents each modality as the combination of sequential units in it, weighted by the attention mechanism. In this way, the sequential and attentive features are captured simultaneously. After that, the LOGO integrates the representations of all modalities via a multi-view representation learning framework, which projects the representations into a common space under the restriction of the modality similarity. Ultimately, the LOGO feed the projections of three modalities in the common space and the embeddings of hashtags into a customized neural collaborative filtering framework to perform the hashtag recommendation. Extensive experiments on the scope of both overall performance comparison and micro-level analyses have well-justified the effectiveness and rationality of our proposed approach. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				SEP 5	2020	203								106114	10.1016/j.knosys.2020.106114													
J								Quantum-inspired ant lion optimized hybrid k-means for cluster analysis and intrusion detection	KNOWLEDGE-BASED SYSTEMS										Intrusion detection; K-means; Quantum-inspired ant lion optimized; Cluster analysis	COLONY OPTIMIZATION; SWARM INTELLIGENCE; DETECTION SYSTEM; ALGORITHM; NETWORKS; PROTOCOL; MODEL	Intrusion detection maintains network security by detecting intrusion behaviors. There are many clustering algorithms that can be used directly for intrusion detection. K-means is a simple and efficient method used in data clustering. However, k-means has a tendency to converge to local optima and depends on the initial value of cluster centers. Therefore, we present an efficient hybrid clustering algorithm referred to as QALO-K, whereby, we combine k-means with quantum-inspired ant lion optimized. This algorithm combines the advantages of quantum computing and swarm intelligence algorithms to improve the k-means algorithm and make the k-means algorithm converge towards the global optimal direction. Our proposed algorithm is tested on several standard datasets from UCI Machine Learning Repository for cluster analysis and its performance is compared with other well-known algorithms. The proposed method was applied on KDD Cup 99 large datasets for intrusion detection. The simulation results infer that the proposed algorithms can be efficiently used for data clustering and intrusion detection. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106167	10.1016/j.knosys.2020.106167													
J								Auto-Regressive Time Delayed jump neural network for blood glucose levels forecasting	KNOWLEDGE-BASED SYSTEMS										Time series forecasting; Neural networks; Precision medicine; Diabetes; Bio-medical patterns	PREDICTION	Diabetes mellitus is a widespread chronic disease and is one of the main causes of death worldwide. In order to improve the quality of life of people with diabetes and reduce the occurrence of complications, it is fundamental to prevent glycemic levels from exceeding the physiologic range. With this purpose, many works in recent years have been developed to forecast future glycemic trends using machine learning algorithms that exploit the reading of continuous glucose monitoring sensors, which gather glycemic data from diabetic patients 24 h a day. However, their application is limited in practice by the fact that they usually require a large amount of training data and other heterogeneous features gathered from patients. For this reason, in this work we present a novel neural network capable of predicting future glycemic levels using only the past glucose values as input while needing a small amount of training data. The model is a jump neural network with the addition of feedback connections from the output to the hidden layer, and time delays for each of the input-to-hidden, output-to-hidden and input-to-output connections. Experiments were conducted on a private and a public dataset. We evaluated performance in terms of RMSE and of adverse event detection. The proposed model outperforms other methods suited for time series forecasting, as well as models for blood glucose level prediction present in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106134	10.1016/j.knosys.2020.106134													
J								Instance-based weighting filter for superparent one-dependence estimators	KNOWLEDGE-BASED SYSTEMS										Bayesian network classifiers; Weighting; Instance-based weighting filter	NAIVE BAYES; FEATURE-SELECTION; EFFICIENT; ASSUMPTION	Bayesian network classifiers remain of great interest in recent years, among which semi-naive Bayesian classifiers which utilize superparent one-dependence estimators (SPODEs) have shown superior predictive power. Linear weighting schemes are effective and efficient ones for linearly combining SPODEs, whereas it is a challenging task for averaged one-dependence estimators (AODE) to find globally optimal and fixed weights for its SPODE members. The joint probability distribution of SPODE may not always fit different test instances to the same extent, thus a flexible rather than rigid weighting scheme would be a feasible solution for the final AODE to approximate the true joint probability distribution. Based on this promise, we propose a novel instance-based weighting filter, which can flexibly assign discriminative weights to each single SPODE for different test instances. Meanwhile, the weight considers not only the mutual dependence between the superparent and class variable, but also the conditional dependence between the superparent and non-superparent attributes. Experimental comparison on 30 publicly available datasets shows that SPODE with instance-based weighting filter outperforms state-of-the-art BNCs with and without weighting methods in terms of zero-one loss, bias and variance with minimal additional computation. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106085	10.1016/j.knosys.2020.106085													
J								An improved Dragonfly Algorithm for feature selection	KNOWLEDGE-BASED SYSTEMS										Feature selection; Binary Dragonfly Algorithm; Selected features; Classification accuracy; V-shaped transfer function; Optimization	OPTIMIZATION ALGORITHM; CLASSIFICATION; SYSTEM; COLONY	Dragonfly Algorithm (DA) is a recent swarm-based optimization method that imitates the hunting and migration mechanisms of idealized dragonflies. Recently, a binary DA (BDA) has been proposed. During the algorithm iterative process, the BDA updates its five main coefficients using random values. This updating mechanism can be improved to utilize the survival-of-the-fittest principle by adopting different functions such as linear, quadratic, and sinusoidal. In this paper, a novel BDA is proposed. The algorithm uses different strategies to update the values of its five main coefficients to tackle Feature Selection (FS) problems. Three versions of BDA have been proposed and compared against the original DA. The proposed algorithms are Linear-BDA, Quadratic-BDA, and Sinusoidal-BDA. The algorithms are evaluated using 18 well-known datasets. Thereafter, they are compared in terms of classification accuracy, the number of selected features, and fitness value. The results show that Sinusoidal-BDA outperforms other proposed methods in almost all datasets. Furthermore, Sinusoidal-BDA exceeds three swarm-based methods in all the datasets in terms of classification accuracy and it excels in most datasets when compared in terms of the fitness function value. In a nutshell, the proposed Sinusoidal-BDA outperforms the comparable feature selection algorithms and the proposed updating mechanism has a high impact on the algorithm performance when tackling FS problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106131	10.1016/j.knosys.2020.106131													
J								Hidden behavior prediction of complex system based on time-delay belief rule base forecasting model	KNOWLEDGE-BASED SYSTEMS										BRB model; Hidden behavior; Time series prediction; Time-delay	ALGORITHM; IMPACT	The hidden belief rule base model (HBRB) which can utilize both hybrid expert's experience and experimental data has become a useful method for hidden behavior forecasting of complex system in many applications. But the traditional HBRB model is one-step forecasting model, which means that it can only utilize the limited information in very short time instant. In fact, one-step forecasting method is incomplete because the future behavior of a complex system is generated through multiple historical stats in different time instant. Therefore, a time-delay hidden BRB forecasting model (THBRB) is designed, where the input with multiple time instant of the HBRB model is considered, and the corresponding reasoning process is also designed. Further, an optimization method based on projection covariance matrix adaption evolution strategy (P-CMA-ES) algorithm is used to train the initial THBRB model. A case study is established to prove the advantage of the proposed method, and the experiment results show that the proposed THBRB model can predict the future hidden behavior of complex system effectively. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106147	10.1016/j.knosys.2020.106147													
J								A deep learning model to effectively capture mutation information in multivariate time series prediction	KNOWLEDGE-BASED SYSTEMS										Multivariate time series prediction; Mutation information; LSTM with transformation mechanism; Attention mechanism		In real-world complex multivariate time series data, mutation phenomena can significantly affect variation rules of target series. Meanwhile, there is no specific learning mechanism for the current deep learning model to capture mutation information in time series prediction. To this end, we propose a new deep learning model to capture mutation information between data. To capture the impact of mutation information on target series, a new function mapping is designed in the attention mechanism of the encoder to process the fusion of historical hidden state and cell state information; and an LSTM with transformation mechanism is proposed in the encoder to process the input information flow and learn the mutation information. In addition, an adaptive self-paced curriculum learning mechanism is designed to obtain mutation information that may be ignored among mini-batch samples. Finally, we define an objective function for multivariate time series prediction, which can extract the influence of temporal correlation information and mutation information within the data on target series. Our model can achieve superior performance than all baseline methods on five real-world datasets in different fields. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106139	10.1016/j.knosys.2020.106139													
J								Robust multi-label feature selection with dual-graph regularization	KNOWLEDGE-BASED SYSTEMS										Feature selection; Multi-label learning; Graph regularization; Classification	SPARSE REGRESSION; CLASSIFICATION; FRAMEWORK	Multi-label learning is facing great challenges due to high-dimensional feature space, complex label correlations and noises in multi-label data. Feature selection techniques have attracted considerable attention to address the problems. In this paper, we design our method based on dual-graph regularization, i.e., feature graph regularization and label graph regularization. The feature graph regularization is used to preserve the geometric structure of features, while label graph regularization intends to explore the correlations of labels. Furthermore, the l(2,1)-norm is imposed on the loss function to enhance the robust of feature selection methods. As a result, a new feature selection method termed Robust Multi-label Feature Selection based on Dual-graph (DRMFS) is proposed. Particularly, only one unknown variable, feature weight matrix, is incorporated in our proposed method, which can reach global optimum. Additionally, we impose both l(2,1)-norm and non-negative constraints onto the feature weight matrix to enhance the property of row-sparse. Finally, we design an optimization scheme to solve the proposed method, and offer the convergence proof of the optimization scheme. Extensive experimental results demonstrate the superiority of the proposed method in comparison to the-state-of-art multi-label feature selection methods. Finally, some insightful discussions with respect to the convergence analysis, complexity analysis and parameter sensitivity analysis are presented. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106126	10.1016/j.knosys.2020.106126													
J								Recognition of opinion leaders coalitions in online social network using game theory	KNOWLEDGE-BASED SYSTEMS										Game theory; Opinion leader; Online Social Network; Shapley value	STRUCTURAL HOLES; SHAPLEY VALUE; TRUST; ALGORITHM; POWER	Nowadays, human decision-making power and attitude revolutionize over time and change their viewpoint towards substantial convinced values because of the influence of social media. Social network plays a crucial role in human life with the progression of technology. In the online social network, the responsibility of opinion leaders is also vital in information diffusion and promotion purposes. In this manuscript, we proposed a different Game theory-based Opinion Leader Detection (GOLD) algorithm to identify the group of users having the maximum synergy declared as the coalition of opinion leaders. For implementing purpose, we used the game theory approach in which all the users behave like a player, and a trustor-trustee tree formed comprised coalitions based on trust and conditional probability. We proposed an inventive and distinctive solution to measures the individual payoff using the distance-based centrality parameter. We also computed the Shapley value for each user to identify the maximum marginal contribution and determined the maximum synergy of each coalition. The main advantage of the proposed approach is to strengthen the power of the coalition and produced the synergetic outcome. The proposed approach delivered around 90% accuracy and decreased 37% execution time. Therefore, it provides superior results regarding the accuracy, precision, time complexity, rate of convergence, and computational time with other SNA (Social Network Analysis) measures. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106158	10.1016/j.knosys.2020.106158													
J								Two-stage sentiment classification based on user-product interactive information	KNOWLEDGE-BASED SYSTEMS										Review document; Sentiment classification; Coarse-to-fine classification; User-product interactive information; Deep learning; Interpretability		Document-level review sentiment classification aims to predict the sentiment category for given review documents written by users for products. Most of the existing methods focus on generating a good review document representation and classifying the review document directly. However, on the one hand, as document-level review sentiment classification usually includes many sentiment categories and the difference between these sentiment categories is not obvious, it may be difficult to obtain satisfying result by direct classification. On the other hand, this once classification process with review representation may fail to well interpret how the results are achieved. In addition, although some information such as user preference and product characteristics are incorporated when building models, the interactive information between user and product are usually ignored. In this paper, inspired by the deductive reasoning strategy of human doing multiple choice questions, we are motivated to propose a Two-Stage Sentiment Classification (TSSC) model to classify review documents in two stages: (1) Coarse classification stage, where model mainly adopts user-product interactive information to pre-judge the sentiment tendency of the review document without considering the review information; (2) Fine classification stage, where model uses text information of the review document for further classification based on the sentiment tendency obtained in coarse classification stage. Finally, the sentiment classification task is accomplished by combining both the results of coarse classification and fine classification. The experimental results demonstrate that our TSSC model significantly outperforms most of the related models (e.g., Trigram and NSC+UPA) on IMDB and Yelp datasets in terms of classification accuracy. When compared with the state-of-the-art HUAPA model, our TSSC model not only achieves slightly more accurate performance, but also has lower time complexity and stronger interpretability. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106091	10.1016/j.knosys.2020.106091													
J								A novel Domain Adaptive Residual Network for automatic Atrial Fibrillation Detection	KNOWLEDGE-BASED SYSTEMS										Atrial Fibrillation Detection; ECG; Domain adaptation; Residual Block	OBSTRUCTIVE SLEEP-APNEA; COMPUTER-AIDED DIAGNOSIS; NEURAL-NETWORK; RECOGNITION; ARRHYTHMIAS; PERSPECTIVE; STATISTICS; TRANSFORM; STROKE	Atrial fibrillation (AF) is the most common cardiac arrhythmia and shows a rising trend with the increase of aged. Currently, existing intelligent AF detection methods have achieved good results in massive labeled data. However, it is time-consuming and undesirable to label ECG signals in real applications. Meanwhile, due to distribution discrepancy by different testing conditions, it is unsatisfied for directly applying trained model to other datasets. Inspired by the domain adaptation techniques, this paper proposes a novel Domain Adaptive Residual Network (DARN) to detect AF of unlabeled datasets with the aid of detection knowledge of labeled dataset. Firstly, residual blocks are adopted to extract informative deep features from the ECG signals automatically. Then, deep features are fed into feature classifier to acquire final detection result. Further, the multi-layer multi-kernel maximum mean discrepancy is combined into the training process to reduce distribution discrepancy of different domains, which imposes constraints on network parameters. Finally, the proposed method was evaluated with the data from MIT-BIH Atrial Fibrillation Database (AFDB), MIT-BIH Arrhythmia Database and 2017 Physionet challenge dataset. The experimental results show that the proposed domain adaptive approach improves the accuracy by 4.50% on average and the F1 score by 4.28% on average using the knowledge of AFDB. Additionally, comparison experiment shows that the proposed feature extractor and classifier achieved 98.97%, 98.75%, and 98.84% for the sensitivity, specificity, and accuracy on the AFDB, respectively. Consequently, the proposed method is provided with high application potential as a valuable auxiliary tool for clinical AF detection. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106122	10.1016/j.knosys.2020.106122													
J								SurvLIME: A method for explaining machine learning survival models	KNOWLEDGE-BASED SYSTEMS										Interpretable model; Explainable Al; Survival analysis; Censored data; Convex optimization; The Cox model	VARIABLE SELECTION; REGRESSION; FORESTS; LASSO; PREDICTION; EXTENSIONS	A new method called SurvLIME for explaining machine learning survival models is proposed. It can be viewed as an extension or modification of the well-known method LIME. The main idea behind the proposed method is to apply the Cox proportional hazards model to approximate the survival model at the local area around a test example. The Cox model is used because it considers a linear combination of the example covariates such that coefficients of the covariates can be regarded as quantitative impacts on the prediction. Another idea is to approximate cumulative hazard functions of the explained model and the Cox model by using a set of perturbed points in a local area around the point of interest. The method is reduced to solving an unconstrained convex optimization problem. A lot of numerical experiments demonstrate the SurvLIME efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106164	10.1016/j.knosys.2020.106164													
J								A mixed integer linear programming support vector machine for cost-effective feature selection	KNOWLEDGE-BASED SYSTEMS										Feature selection; Support vector machine; Mixed integer linear programming; Robust optimization; Feature cost; Cost uncertainty	ROBUST SOLUTIONS; TEST STRATEGIES; KNOWLEDGE; TREES	In the era of big data, feature selection is indispensable as a dimensional reduction technique to lower data complexity and enhance machine learning performances. However, traditional feature selection methods mainly focus on classification performances, while they exclude the impact of associated feature costs; e.g., price, risk, and computational complexity for feature acquisition. In this research, we extend the l(1) norm support vector machine (l(1)-SVM) to address the feature costs, by incorporating a budget constraint to preserve classification accuracy with the least expensive features. Furthermore, we formulate its robust counterpart to address the uncertainty of the feature costs. To enhance computational efficiency, we also develop an algorithm to tighten the bound of the weight vector in the budget constraint. Through the experimental study on a variety of benchmark and synthetic datasets, our proposed mixed integer linear programming (MILP) models show that they can achieve competitive outcomes in terms of predictive and economic performances. Also, the algorithm that tightens the budget constraint helps to curtail computational complexity. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106145	10.1016/j.knosys.2020.106145													
J								Granule description in knowledge granularity and representation	KNOWLEDGE-BASED SYSTEMS										Knowledge granularity; Knowledge representation; Granular computing; Granule description; Definable granule	ROUGH SET; CONCEPT LATTICE; 3-WAY DECISION; CONTEXTS	Granule description is an important problem in knowledge granularity and representation. In the theoretical study of granule description, two basic sub-problems need to be solved: (1) check whether a target granule is definable, and try to offer an exact description for the target granule if definable; (2) an appropriate approximate description should also be provided to the target granule if indefinable. Up to now, there have been some preliminary studies on these two sub-problems. However, the existing findings are not enough or incomplete since sufficient and necessary conditions of definable granules have not been explored, and there have not been effective ways to find better approximate descriptions of indefinable granules. Motivated by these discussions, this paper puts forward granule description methods for definable and indefinable granules. First of all, we propose the notions of covering elements and inserting elements of a target granule by which sufficient and necessary conditions of boolean AND-definable and (boolean AND, (sic))-definable granules are constructed. And then, the smallest definable granule containing the target indefinable granule and the largest definable granule included in the target indefinable granule are investigated, which are used to solve the problem of finding better approximate descriptions for indefinable granules. Finally, a comparison is made to show that the existing work on complete sets should be improved for providing feasible solutions to the granule description problem.																	0950-7051	1872-7409				SEP 5	2020	203								106160	10.1016/j.knosys.2020.106160													
J								Sketch-then-Edit Generative Adversarial Network	KNOWLEDGE-BASED SYSTEMS										Generative adversarial network; Vanishing gradient problem; Non-negative matrix factorization		Generative Adversarial Network (GAN) has been widely used to generate impressively plausible data. However, it is a non-trivial task to train the original GAN model in practice due to the vanishing gradient problem. This is because the JS divergence could be a constant (i.e., log2) when original data distribution and generated data distribution hold a negligible overlapping area. Under such a scenario, the gradient of generator is 0. Most efforts have been devoted to designing a more proper difference measure while few attentions have been paid to the former aspect of the issue. In this paper, we propose a new method to design a noise distribution having a guaranteed non-negligible overlapping area with raw data distribution. The key idea is to transform the noise from the randomized space into the raw data space. We propose to obtain the transformation as the basis matrix in non-negative matrix factorization because the basis matrix has the underlying features of the raw data. The proposed idea is instantiated as Sketch-then-Edit GAN (SEGAN) where sketches are the noises after transformation and are adopted as the name since they contains basic features of the raw data. Moreover, a new generator for editing the sketches into realistic-like data is designed. We mathematically prove that SEGAN solves the gradient vanishing problem, and conduct extensive experiments on the MNIST, CIFARIO, SVHN and Celeba datasets to demonstrate the effectiveness of SEGAN. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106102	10.1016/j.knosys.2020.106102													
J								Lightweight multi-scale residual networks with attention for image super-resolution	KNOWLEDGE-BASED SYSTEMS										Super-resolution; Deep convolutional neural networks; Dilated convolutions; Attention mechanism; Residual networks		In recent years, constructing various deep convolutional neural networks (CNNs) for single-image super-resolution (SISR) tasks has made significant progress. Despite their high performance, numerous CNNs are limited in practical applications, owing to the requirement of heavy computation. This paper proposes a lightweight network for SISR, known as attention-based multi-scale residual network (AMSRN). In detail, a residual atrous spatial pyramid pooling (ASPP) block as well as a spatial and channel-wise attention residual (SCAR) block is stacked alternately to support the main framework of the entire network. The residual ASPP block utilizes parallel dilated convolutions of different dilation rates to achieve the purpose of capturing multi-scale features. The SCAR block adds the channel attention (CA) and spatial attention (SA) mechanisms based on a double-layer convolution residual block. In addition, group convolution is introduced in the SCAR block to further reduce the parameters while preventing over-fitting. Moreover, a multi-scale feature attention module is designed to provide instructive multi-scale attention information for shallow features. Particularly, we propose a novel upscale module, which adopts dual paths to upscale the features by jointly using sub-pixel convolution and nearest interpolation layers, instead of using deconvolution layer or sub-pixel convolution layer alone. The experimental results demonstrate that our method achieves comparable performance to the state-of-the-art methods, both quantitatively and qualitatively. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106103	10.1016/j.knosys.2020.106103													
J								Generating attentive goals for prioritized hindsight reinforcement learning	KNOWLEDGE-BASED SYSTEMS										Attentive goals generation; Prioritized hindsight model; Hindsight experience replay; Reinforcement learning	GO	Typical reinforcement learning (RL) performs a single task and does not scale to problems in which an agent must perform multiple tasks, such as moving a robot arm to different locations. The multigoal framework extends typical RL using a goal-conditional value function and policy, whereby the agent pursues different goals in different episodes. By treating a virtual goal as the desired one, and frequently giving the agent rewards, hindsight experience replay has achieved promising results in the sparse-reward setting of multi-goal RL. However, these virtual goals are uniformly sampled after the replay state from experiences, regardless of their significance. We propose a novel prioritized hindsight model for multi-goal RL in which the agent is provided with more valuable goals, as measured by the expected temporal-difference (TD) error. An attentive goals generation (AGG) network, which consists of temporal convolutions, multi-head dot product attentions, and a last-attention network, is structured to generate the virtual goals to replay. The AGG network is trained by following the gradient of TD-error calculated by an actor-critic model, and generates goals to maximize the expected TD-error with replay transitions. The whole network is fully differentiable and can be learned in an end-to-end manner. The proposed method is evaluated on several robotic manipulating tasks and demonstrates improved sample efficiency and performance. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106140	10.1016/j.knosys.2020.106140													
J								CNN-FCM: System modeling promotes stability of deep learning in time series prediction	KNOWLEDGE-BASED SYSTEMS										Fuzzy cognitive maps; Deep neural networks; Time series prediction; System modeling	FUZZY COGNITIVE MAPS; DESIGN	Time series data are usually non-stationary and evolve over time. Even if deep learning has been found effective in dealing with sequential data, the stability of deep neural networks in coping with the situations unseen during the training stage is also important. This paper deals with this problem based on a fuzzy cognitive block (FCB) which embeds the learning of high-order fuzzy cognitive maps into the deep learning architecture. Thereafter, computers can automatically model the complex system that produces the observation rather than simply regress the available data. Respectively, we design a deep neural network termed CNN-FCM which has combined the available convolution network with FCB. To validate the advantages of our design and verify the effectiveness of FCB, twelve benchmark datasets are employed and classic deep learning architectures are introduced as the comparison. The experimental results show that the performance of many current popular deep learning architectures declines when handling data deviated from the training set. FCB plays an important role in promoting the performance of CNN-FCM in the corresponding experiments. Thereafter, we conclude that system modeling can promote the stability of deep learning in time series prediction. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106081	10.1016/j.knosys.2020.106081													
J								A robust time series prediction method based on empirical mode decomposition and high-order fuzzy cognitive maps	KNOWLEDGE-BASED SYSTEMS										Time series prediction; High-order fuzzy cognitive maps; Empirical mode decomposition; Bayesian ridge regression	NEURAL-NETWORK; MULTIVARIATE	Fuzzy cognitive maps (FCMs) have been widely used in time series prediction due to the excellent performance in dynamic system modeling. However, existing time series prediction methods based on FCMs have some defects, such as low precision and sensitivity to hyper parameters. Therefore, more accurate and robust methods remain to be proposed for handling non-stationary and large-scale time series. To address this issue, in this paper, a novel time series prediction method based on empirical mode decomposition (EMD) and high-order FCMs (HFCMs) is proposed, termed as EMD-HFCM. First, EMD is applied to extract features from the original sequence to obtain multiple sequences to represent the nodes of HFCM. To learn HFCM efficiently and accurately, a robust learning method based on Bayesian ridge regression is employed, which can estimate the regular parameters from data instead of being set manually. Then, prediction can be performed based on the iterative characteristics of HFCM. To compare EMD-HFCM with existing methods, extensive experiments are conducted on eight benchmark datasets and the results validate the performance of the proposal in handling large-scale and non-stationary time series. Furthermore, the experiments also show that the proposed method is much more robust and insensitive to hyper parameters than the state of art methods. Finally, nonparametric statistical tests are carried out and the superiority of the proposed method is verified in the statistical sense. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106105	10.1016/j.knosys.2020.106105													
J								A hybrid model to support decision making in emergency department management	KNOWLEDGE-BASED SYSTEMS										DEMATEL method; Emergency department; PROMETHEE II method; Healthcare; Decision making; Hybrid model; Patient overcrowding	LENGTH-OF-STAY; HEALTH-CARE; INTEGRATED SIMULATION; PROMETHEE; OPTIMIZATION; PERFORMANCE; SELECTION; INTERVENTIONS; SERVICES; EVALUATE	The Emergency Department (ED) plays a key role in restoring the health of patients. Ensuring the availability of the ED and achieving rational use of its resources is critical to avoiding ED overcrowding by patients. Given this, the critical question is how ED managers can design and select improvement actions that reduce ED overcrowding. Designing and selecting enhancement actions are viewed as a Multiple Criteria Decision Making (MCDM) problem. Thus, this work provides a hybrid MCDM model combining Decision-Making Trial and Evaluation Laboratory (DEMATEL) and Preference Ranking Organization (PROMETHEE II) methods to help ED managers design improvement actions and make decisions that reduce ED overcrowding. In the model, the role of DEMATEL method is to generate knowledge to support the design of improvement actions from the causal relationships among the criteria governing the management of the patient care and treatment process in ED units. However, as EDs have costly resources, actions need to be prioritized. Therefore, the PROMETHEE II method composes the model to prioritize improvement actions that reduce short-term ED overcrowding. The model was validated by applying it in the ED of one of the largest hospitals in the state of Parana, Brazil, that exclusively serves patients with the Brazilian federal government's single healthcare system (Sistema Unico de Saude - SUS). The model was easily understood by the ED managers due to its ease of use, and the integration among these managers necessitated by its development and application enriched the discussion of the overcrowding problem faced by the ED. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106148	10.1016/j.knosys.2020.106148													
J								A new iterated local search algorithm for the cyclic bandwidth problem	KNOWLEDGE-BASED SYSTEMS										Heuristic; Computational methods; Cyclic bandwidth minimization; Graph labeling; Combinatorial optimization	GRAPHS; BOUNDS	The Cyclic Bandwidth Problem is an important graph labeling problem with numerous applications. This work aims to advance the state-of-the-art of practically solving this computationally challenging problem. We present an effective heuristic algorithm based on the general iterated local search framework and integrating dedicated search components. Specifically, the algorithm relies on a simple, yet powerful local optimization procedure reinforced by two complementary perturbation strategies. The local optimization procedure discovers high-quality solutions in a particular search zone while the perturbation strategies help the search to escape local optimum traps and explore unvisited areas. We present intensive computational results on 113 benchmark instances from 8 different families, and show performances that are never achieved by current best algorithms in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106136	10.1016/j.knosys.2020.106136													
J								Supervised link prediction in multiplex networks	KNOWLEDGE-BASED SYSTEMS										Link prediction; Multiplex networks; Complex networks; Supervised learning; Feature extraction	COMPLEX NETWORKS; SOCIAL NETWORKS; ORGANIZATION	In recent years, multiplex networks have been introduced to describe real complex systems, where the same group of entities make different types of interaction. In a multiplex network, each layer expresses one distinct type of interaction. Link prediction is a research hotspot in complex network analysis. A large number of link prediction methods have been proposed, but only a few were designed for multiplex networks. In this paper, we focus on the link prediction problem in multiplex networks. In our opinion, an approach in which link prediction is performed by simultaneously considering the information from all layers is advisable, because the formation of links in one layer can be affected by links of the same node pairs in other layers. A supervised method is proposed in this study to implement link prediction in multiplex networks, which regards link prediction as a binary classification problem. In the proposed method, a classification model is fed by a set of elaborate structural features of node pairs that are extracted from all layers. Extensive experiments are conducted on six networks to analyze the effectiveness of the proposed method. The results demonstrate that the proposed method outperforms the compared methods significantly. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106168	10.1016/j.knosys.2020.106168													
J								Remote sensing image captioning via Variational Autoencoder and Reinforcement Learning	KNOWLEDGE-BASED SYSTEMS										Transformer; Variational Autoencoder; Transfer learning; Remote sensing image captioning; Self-attention mechanisms; Convolutional neural network; Reinforcement learning	MODELS	Image captioning, i.e., generating the natural semantic descriptions of given image, is an essential task for machines to understand the content of the image. Remote sensing image captioning is a part of the field. Most of the current remote sensing image captioning models suffered the overfitting problem and failed to utilize the semantic information in images. To this end, we propose a Variational Autoencoder and Reinforcement Learning based Two-stage Multi-task Learning Model (VRTMM) for the remote sensing image captioning task. In the first stage, we finetune the CNN jointly with the Variational Autoencoder. In the second stage, the Transformer generates the text description using both spatial and semantic features. Reinforcement Learning is then applied to enhance the quality of the generated sentences. Our model surpasses the previous state of the art records by a large margin on all seven scores on Remote Sensing Image Caption Dataset. The experiment result indicates our model is effective on remote sensing image captioning and achieves the new state-of-the-art result. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								105920	10.1016/j.knosys.2020.105920													
J								Comprehensive relative importance analysis and its applications to high dimensional gene expression data analysis	KNOWLEDGE-BASED SYSTEMS										Collinearity; Feature ranking; High dimensional; Small sample size; Relative importance; Singularity	CANCER CLASSIFICATION; VARIABLE SELECTION; MODEL SELECTION; REGRESSION; PREDICTORS; WEIGHT	Identification of important genes is challenging not only because of its high dimensional nature, but also because the expressions of genes from the same pathway are often highly correlated. A large number of feature selection methods have been proposed to select a subset of genes for interpretation and prediction of certain phenotypes. Among them, the L-1 penalization-based methods, such as lasso, adaptive lasso and elastic net, gain most attentions. However, the L-1 penalty employed by these methods is known to have difficulties in selection of a group of highly correlated features. The issue of identifying important highly correlated features, on the other hand, is well studied in the multiple regression analysis with a sufficient sample size. In particular, relative weight analysis is known effective in measuring the relative importance of correlated features. But the relative weight analysis suffers from the postulation of a full-column-rank feature matrix and is infeasible for high dimensional problems. In this research, a comprehensive relative importance analysis is proposed and proven valid without sample size and matrix rank restraints. Simulation and real cases are used to show the effectiveness of the proposed method in selecting relevant features especially for the high dimensional data. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106120	10.1016/j.knosys.2020.106120													
J								Evolving fuzzy neural hydrocarbon networks: A model based on organic compounds	KNOWLEDGE-BASED SYSTEMS										Fuzzy inference system; Artificial hydrocarbon networks; Autonomous partitioning; Evolving models	ENSEMBLE METHODS; CLASSIFICATION; REGRESSION; HEALTH	This paper presents a new evolving intelligent model capable of combining the techniques and concepts of artificial neural networks, fuzzy systems and artificial hydrocarbon networks, in which the latter aggregates concepts of organic chemistry to carry out the training of intelligent models. The proposed model has three layers where the first two form a fuzzy inference system and the third layer is responsible for the defuzzification process through concepts based on the bond between carbons and hydrogens. The fuzzification process of the model is based on the techniques of an autonomous data partitioning algorithm that can elicit the number and centers of the clouds that make up the fuzzy neurons in the first layer of the model. Thereby, an evolving algorithm is employed, which uses the data set as a stream in a single-pass incremental mode (allowing fast processing). This is achieved in an unsupervised manner, and thus, to eliminate possible overfitting problems in the subsequent supervised training process, a Bayesian pruning technique is used to identify the neurons that are finally most relevant to the actual supervised approximation and/or classification problem. To validate the proposed approach, binary pattern classification tests, multi-class classification problems and regression problems were performed. The results obtained and compared with other intelligent models in the literature prove that the approach becomes a model capable of extracting knowledge from data sets and using concepts of organic chemistry to perform learning tasks with high degree of reliability. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106099	10.1016/j.knosys.2020.106099													
J								Ensemble feature selection in high dimension, low sample size datasets: Parallel and serial combination approaches	KNOWLEDGE-BASED SYSTEMS										Data mining; Ensemble learning; Feature selection; High dimension low sample size; Machine learning	SUPPORT VECTOR MACHINES; CLASSIFICATION; PREDICTION; CANCER	Feature selection in high dimension, low sample size (HDLSS) data is always an important data pre-processing task. In the literature, the concept of ensemble learning has been applied to improve single feature selection methods, the so-called ensemble feature selection techniques. The most widely used approach is to combine multiple feature selection methods and their selection results via some sort of aggregation function in a parallel manner. Another ensemble strategy is based on the serial combination approach where the selection results of the first feature selection stage are used as input for the second stage of feature selection to produce the final output. The aim of this paper is to fully explore the performance of parallel and serial combination approaches for ensemble feature selection over HDLSS data. In particular, we strive to answer two research questions: whether parallel and serial based ensemble feature selection can outperform single feature selection and which combination approach is the better choice for ensemble feature selection. The experimental results based on comparing nine parallel and nine serial combinations, as well as three single baseline feature selection methods, including principal component analysis (PCA), genetic algorithm (GA), and C4.5 decision tree, show that ensemble feature selection performs better than single feature selection in terms of classification accuracy. However, there are no significant differences in performance between the single best baseline method (i.e. GA) and the top three parallel and serial combinations. On the other hand, the serial combination approach produces the largest feature reduction rate. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106097	10.1016/j.knosys.2020.106097													
J								Tag-informed collaborative topic modeling for cross domain recommendations	KNOWLEDGE-BASED SYSTEMS										Collaborative filtering; Recommender system; Cross domain; Topic model	REGRESSION	Collaborative topic modeling is powerful to alleviate data sparsity in recommender systems owing to the incorporation of collaborative filtering and topic models. However sufficient textual data is not always available. On the other hand, tags serving as supplementary description of items can reflect users' interests in item attributes. But previous works only mine the effect of tags on ratings in one domain and ignore that in related domains items can be related in attributes. Tags encode similar properties of items and can be transferred across domains to mutually benefit recommendations for both domains. In this study we propose a TagCDCTR (Tag-informed Cross Domain Collaborative Topic Regression) model, which exploits shared tags as bridges to link related domains through an extended collaborative topic modeling framework. The model exploits the inter-domain relations by encoding cross domain item-item similarity based on common tags and jointly learning a shared set of topics from all domains together. Collectively factorizing the rating matrices of multiple domains into common user latent factors and domain-specific item latent factors, so that the learned item latent factors are linked through the inter-domain relations, helping to capture the items more comprehensively. The rich information reused in multiple domains alleviates data sparsity and the semantic advantage of topics and tags provides a better interpretability of recommendations. The experiments conducted on three datasets demonstrate that TagCDCTR outperforms state-of-the-art collaborative-topic-based models and cross-domain-based models. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106119	10.1016/j.knosys.2020.106119													
J								A prospect theory-based three-way decision model	KNOWLEDGE-BASED SYSTEMS										Three-way decision; Prospect theory; Thresholds; Reference point; Parameter estimation	SHADOWED SETS; MAKING METHOD; ROUGH SETS; APPROXIMATIONS; INFORMATION; GRANULATION; CONSENSUS	In three-way decision, how to reflect the risk attitude in determining decision rules is an important issue. In the classical three-way decision model, loss functions are used to measure risks and determine the minimum-cost decision rules. On this basis, the utility function has been used as a new risk measurement to determine the maximum-utility decision rules. However, some studies show that utility theory may produce some paradoxes and cannot reflect the real risk attitude. To address this problem, we propose a prospect theory-based three-way decision model. In this scenario, we use prospect theory to describe decision-makers' risk attitudes and utilize the value function as a new risk measurement. The decision rules are induced based on the principle of prospect value maximization. Then, we analyze and prove the existence and uniqueness of thresholds. Two specific analytic solutions of thresholds are calculated and the simplified decision rules are derived. A case study is presented to illustrate the effectiveness of our model and make comparisons with other related models. Finally, the results of the experimental analysis are reported to validate the feasibility and performance of our proposed model. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106129	10.1016/j.knosys.2020.106129													
J								IA-SUWO: An Improving Adaptive semi-unsupervised weighted oversampling for imbalanced classification problems	KNOWLEDGE-BASED SYSTEMS										Imbalanced classification; Least squares support numerical spectrum; Minority samples weights; Oversampling; k* information nearest neighbors	SMOTE; MACHINE	As the essence of machine learning, classification is widely used in real life, however, imbalanced data has brought great challenges to classification problems. This is because standard classifiers tend to favor the majority instances and ignore the minority instances. The new oversampling algorithms (e.g. A-SUWO) based on the improving majority weighted minority oversampling (IMWMO) method assign weights through the Euclidean distances from majority instances to hard-to-learn minority instances, and then guide the synthesis of minority samples according to the weights to address the offset of the classification hyperplanes. A-SUWO has achieved better results than traditional oversampling algorithms (e.g. SMOTE and MWMOTE, etc.), when its parameters are well adjusted. However, A-SUWO may give minority training samples inappropriate weights in some irregularly distributed scenarios and make learning tasks even more harder. Additionally, A-SUWO's knn synthesizing method may not obtain wider and more effective instances. Therefore, we propose an improving adaptive semi-unsupervised weighted oversampling (IA-SUWO) technique to address the imbalanced classification problems more effectively. The improvement of IA-SUWO mainly focuses on the following two aspects: (1) comprehensively considering the least squares support numerical spectrum values and the IMWMO method to assign weights to minority instances, and (2) synthesizing new instances using the k* information nearest neighbors (k*INN) method. IA-SUWO aims to maximize the probability that all important minority samples will be drawn and generates more efficient (more scattered) boundary instances. Results demonstrate that IA-SUWO achieves significantly better results in most datasets compared with other 10 oversampling algorithms and 2 ensemble algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106116	10.1016/j.knosys.2020.106116													
J								Improving the robustness of machine reading comprehension model with hierarchical knowledge and auxiliary unanswerability prediction	KNOWLEDGE-BASED SYSTEMS										Machine reading comprehension; Hierarchical knowledge enrichment; Multi-task learning; Model robustness		Machine Reading Comprehension (MRC) aims to understand a passage and answer a series of related questions. With the development of deep learning and the release of large-scale MRC datasets, many end-to-end MRC neural networks have achieved remarkable success. However, these models are fragile and lack of robustness when there are some imperceptible adversarial perturbations in the input. In this paper, we propose an MRC model which has two main components to improve the robustness. On the one hand, we enhance the representation of the model by leveraging hierarchical knowledge from external knowledge bases. On the other hand, we introduce an auxiliary unanswerability prediction module and perform supervised multi-task learning along with a span prediction task. Experimental results on benchmark datasets show that our model can achieve consistent improvement compared with other strong baselines. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106075	10.1016/j.knosys.2020.106075													
J								Secure collaborative few-shot learning	KNOWLEDGE-BASED SYSTEMS										Few-shot learning; Meta-learning; Differential privacy; Additively homomorphic encryption	NEURAL-NETWORKS; PRIVACY; NOISE	Few-shot learning aims at training a model that can effectively recognize novel classes with extremely limited training examples. Few-shot learning via meta-learning can improve the performance on novel tasks by leveraging previously acquired knowledge as a prior when the training examples are extremely limited. However, most of these existing few-shot learning methods involve parameter transfer, which usually requires sharing models trained on the examples for specific tasks, thus posing a potential threat to the privacy of data owners. To tackle this, we design a novel secure collaborative few-shot learning framework. More specifically, we incorporate differential privacy into few-shot learning through adding the calibrated Gaussian noise to its optimization process to prevent sensitive information in the training set from being leaked. To prevent potential privacy disclosure to other participants and the central server, homomorphic encryption is integrated while calculating global loss functions and interacting with a central server. Furthermore, we implement our framework on the classical few-shot learning methods such as MAML and Reptile, and extensively evaluate its performance on Omniglot, Mini-ImageNet and Fewshot-CIFAR100 datasets. The experimental results demonstrate the effectiveness of our framework in both utility and privacy. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106157	10.1016/j.knosys.2020.106157													
J								SiamAtt: Siamese attention network for visual tracking	KNOWLEDGE-BASED SYSTEMS										Visual tracking; Siamese network; Attention network		Visual attention has recently achieved great success and wide application in deep neural networks. Existing methods based on Siamese network have achieved a good accuracy-efficiency trade-off in visual tracking. However, the training time of Siamese trackers becomes longer for the deeper network and larger training data. Further, Siamese trackers cannot predict the target location well in fast motion, full occlusion, camera motion, and similar object scenarios. Due to these difficulties, we develop an end-to-end Siamese attention network for visual tracking. Our approach is to introduce an attention branch in the region proposal network that contains a classification branch and a regression branch. We perform foreground-background classification by combining the scores of the classification branch and the attention branch. The regression branch predicts the bounding boxes of the candidate regions based on the classification results. Furthermore, the proposed tracker achieves the experimental results comparable to the state-of-the-art tracker on six tracking benchmarks. In particular, the proposed method achieves an AUC score of 0.503 on LaSOT, while running at 40 frames per second (FPS). (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106079	10.1016/j.knosys.2020.106079													
J								Multi-component transfer metric learning for handling unrelated source domain samples	KNOWLEDGE-BASED SYSTEMS										Transfer learning; Metric learning; Component; Mahalanobis distance; Weight matrix		Transfer learning (TL) is a machine learning paradigm designed for the problem where the training and test data are from different domains. Existing TL approaches mostly assume that training data from the source domain are collected from multiple views or devices. However, in practical applications, a sample in a target domain often only corresponds to a specific view or device. Without the ability to mitigate the influence of the many unrelated samples, the performance of existing TL approaches may deteriorate for such learning tasks. This problem will be exacerbated if the intrinsic relationships among the source domain samples are unclear. Currently, there is no mechanism for determining the intrinsic characteristics of samples in order to treat them differently during TL. The source domain samples that are not related to the test data not only incur computational overhead, but may result in negative transfer. We propose the multi-component transfer metric learning (MCTML) method to address this challenging research problem. Unlike previous metric-based transfer learning which are only capable of using one metric to transform all the samples, MCTML automatically extracts distinct components from the source domain and learns one metric for each component. For each component, MCTML learns the importance of that component in terms of its predictive power based on the Mahalanobis distance metrics. The optimized combination of components are then used to predict the test data collaboratively. Extensive experiments on public datasets demonstrates its effectiveness in knowledge transfer under this challenging condition. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106132	10.1016/j.knosys.2020.106132													
J								Cross-modal learning with prior visual relation knowledge	KNOWLEDGE-BASED SYSTEMS										Visual relation reasoning; Relation embedding; Anisotropic graph convolutional networks; Visual question answering; Cross-modal information retrieval		Visual relational reasoning is a central component in recent cross-modal analysis tasks, which aims at reasoning about the visual relationships between objects and their properties. These relationships provide rich semantics and help to enhance the visual representation for improving cross-modal learning. Previous works have succeeded in modeling latent visual relationships or rigid-categorized visual relationships. However, these kinds of methods leave out the problem of ambiguity inherent in the visual relationships because of the diverse relational semantics of different visual appearances. In this work, we explore to model the visual relationships by context-aware representations based on human prior knowledge. Based on such representations, we novelly propose a plug-and-play visual relational reasoning module to enhance image encoding. Specifically, we design an Anisotropic Graph Convolution to utilize the information of relation embeddings and relation directionality between objects for generating relation-aware image representations. We demonstrate the effectiveness of the relational reasoning module by applying it to both Visual Question Answering (VQA) and Cross-Modal Information Retrieval (CMIR) tasks. Extensive experiments are conducted on VQA 2.0 and CMPlaces datasets and superior performance is reported when comparing with state-of-the-art works. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				SEP 5	2020	203								106150	10.1016/j.knosys.2020.106150													
J								Attention-aware scoring learning for person re-identification	KNOWLEDGE-BASED SYSTEMS										Person re-identification; Attention module; Score learning head	NETWORK	Person re-identification (re-ID) refers to matching people across multiple camera views at different times and locations. The challenge is mainly about the huge variance of visual appearance of a specific pedestrian owing to pose variations, illumination changes and various camera-styles. In this paper, an Attention-Aware Scoring Learning (AASL) framework is proposed to address these issues. The proposed AASL framework consists of two attention modules and a score learning head. Specifically, the two modules, Spatial Attention Grid and Channel Attention Grid, embedded respectively in the shallow and deep layer in the convolutional neural network, are put forward to help the network learn the most discriminative visual features. Furthermore, an adaptive module termed score learning head is proposed to optimize the parameters of the attention modules. The present paper carries out extensive experiments on three large-scale datasets, including Market-1501, DukeMTMC-reID and CUHK03, after which it is found that our Attention-Aware Scoring Learning framework significantly outperforms the baseline model and achieves a competitive performance compared with the state-of-the-art person re-ID methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106154	10.1016/j.knosys.2020.106154													
J								A weighted hybrid ensemble method for classifying imbalanced data	KNOWLEDGE-BASED SYSTEMS										Data imbalance; Binary classification; Boosting algorithm; Data sampling methods; Base classifiers		In real datasets, most are unbalanced. Data imbalance can be defined as the number of instances in some classes greatly exceeds the number of instances in other classes. Whether in the field of data mining or machine learning, data imbalance can have adverse effects. At present, the methods to solve the problem of data imbalance can be divided into data-level methods, algorithm-level methods and hybrid methods. In this paper, we propose a weighted hybrid ensemble method for classifying imbalanced data in binary classification tasks, called WHMBoost. In the framework of the boosting algorithm, the presented method combines two data sampling methods and two base classifiers, and each sampling method and each base classifier is assigned corresponding weights, which makes them have better complementary advantages. The performance of WHMBoost has been evaluated on 40 benchmark imbalanced datasets with state of the art ensemble methods like AdaBoost, RUSBoost, SMOTEBoost using AUC, F-Measure and Geometric Mean as the performance evaluation criteria. Experimental results show significant improvement over the other methods and it can be concluded that WHMBoost is a promising and effective algorithm to deal with imbalance datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106087	10.1016/j.knosys.2020.106087													
J								A structure optimization method for extended belief-rule-based classification system	KNOWLEDGE-BASED SYSTEMS										Extended belief-rule-based system; Structure optimization; Attribute optimization; Rule activation; Classification; High dimension	WEIGHT CALCULATION; EXPERT-SYSTEM; ACTIVATION; INFORMATION	The widely applied belief-rule-based(BRB) system has demonstrated its advantages in handling both qualitative and quantitative information. As an extension of BRB system, the extended beliefrule-based(EBRB) system bridges the rule-based methods and data-driven methods by efficiently transforming data into extended belief rules(EBRs). Many works have been done to apply EBRB system in addressing classification problems. However, the problems of making use of all attributes indiscri-minately and activating almost all EBRs still affect the accuracy and computational efficiency of EBRB system. In this paper, a structure optimization method for EBRB(SO-EBRB) system, including attribute optimization and rule activation, is proposed to address aforementioned problems. In the attribute optimization, a weighted minimum redundancy maximum relevance(MRMR) method is proposed, where the relevance between attributes and label as well as the redundancy among attributes are used to evaluate attributes. Afterwards, the proposed attribute weight calculation method is utilized to assign attribute weights for the EBRB system. In rule activation, an improved minimum centre distance rule activation(MCDRA) method, which considering the weights of attributes in distance calculation, is used to activate customized EBRs for input query data. 15 benchmark classification data sets are utilized to verify the effectiveness of the proposed SO-EBRB method. The results show that, compared with conventional EBRB system, the SO-EBRB system achieves higher classification accuracy, lower rule activation ratio and less response time. Additionally, comparison between the proposed method and some state-of-art machine learning algorithms demonstrates that the SO-EBRB system achieves prominent performance in addressing classification problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				SEP 5	2020	203								106096	10.1016/j.knosys.2020.106096													
J								Multivariate time series analysis from a Bayesian machine learning perspective	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Multivariate analysis; Bayesian inference; Structural time series; Feature selection; Prediction	MODEL	In this paper, we perform multivariate time series analysis from a Bayesian machine learning perspective through the proposed multivariate Bayesian time series (MBTS) model. The multivariate structure and the Bayesian framework allow the model to take advantage of the association structure among target series, select important features, and train the data-driven model at the same time. Extensive analyses on both simulated data and empirical data indicate that the MBTS model is able to, cover the true values of regression coefficients in 90%credible intervals, select the most important predictors, and boost the prediction accuracy with higher correlation in absolute value of the target series, and consistently yield superior performance over the univariate Bayesian structural time series (BSTS) model, the autoregressive integrated moving average with regression (ARIMAX) model, and the multivariate ARIMAX (MARIMAX) model, in one-step-ahead forecast and ten-steps-ahead forecast.																	1012-2443	1573-7470				OCT	2020	88	10					1061	1082		10.1007/s10472-020-09710-6		SEP 2020											
J								Context-aware next location prediction using data mining and metaheuristics	EVOLUTIONARY INTELLIGENCE										Spatio-temporal data; Location prediction; Metaheuristics; Location context; HMM; Markov model		Due to the heavy use of smartphones and other GPS enabled devices, researchers have easy access to substantial mobility data. Many existing techniques predict the next location of the users based on their mobility traces which includes only geographical coordinates in the form of spatio-temporal data. These raw mobility traces possess hidden information known as location context. Contextual information of any location means its name, time spent there, associated activity, preferred visit time and many such parameters. Enriching raw mobility traces with such contextual information adds more value to it and more sense to it's applications. The proposed model performs geographical, contextual and behavioural enrichment of raw trajectories. It also assigns relevant tag to each identified location automatically using metaheuristic approach. This paper proposes a model CANLoc to perform data collection, trajectory enrichment and location prediction. Performance of the proposed model is verified using two datasets: GeoLife and Mobi-India, which shows significant improvement in the location prediction accuracy.																	1864-5909	1864-5917															10.1007/s12065-020-00469-7		SEP 2020											
J								Fast automatic camera network calibration through human mesh recovery	JOURNAL OF REAL-TIME IMAGE PROCESSING										Camera calibration; Pose estimation; Human mesh recovery; 3D matching	SELF-CALIBRATION	Camera calibration is a necessary preliminary step in computer vision for the estimation of the position of objects in the 3D world. Despite the intrinsic camera parameters can be easily computed offline, extrinsic parameters need to be computed each time a camera changes its position, thus not allowing for fast and dynamic network re-configuration. In this paper we present an unsupervised and automatic framework for the estimation of the extrinsic parameters of a camera network, which leverages on optimised 3D human mesh recovery from a single image, and which does not require the use of additional markers. We show how it is possible to retrieve the real-world position of the cameras in the network together with the floor plane, exploiting regular RGB images and with a weak prior knowledge of the internal parameters. Our framework can also work with a single camera and in real-time, allowing the user to add, re-position, or remove cameras from the network in a dynamic fashion.																	1861-8200	1861-8219															10.1007/s11554-020-01002-w		SEP 2020											
J								Occluded suspect search via channel-guided mechanism	NEURAL COMPUTING & APPLICATIONS										Person search; Occluded patterns; Channel-aware attention; Channel-guided mechanism; Convolutional neural network; Surveillance system	PERSON REIDENTIFICATION; PEDESTRIAN DETECTION; MULTIPLE; NETWORK	To elude from the camera, suspects often hide behind other things or persons, leading to a series of occlusion patterns. These suspects are notoriously hard to search due to the substantially various appearance in the intricate occlusion patterns. Existing methods solving occlusion problem depend on learning several frequent patterns separately. It brings not only high consumption but also less coverage of patterns in real application scenarios. Different from the current researches which only concern certain patterns that do not synthesize the occlusion patterns in practical applications, we consider a wide range of occlusion patterns which conform the real application scenarios in one coherent model with less interference of both the occlusion and background areas and without redundant computation. Consequently, we propose a channel-guided mechanism (CGM) for occluded suspect search in this paper. The core idea is that different body areas have been activated via different channels in convolutional neural networks. By suppressing the effects of the interference areas, such as occlusion and background areas, we can filter out the visible areas which are the essential elements for the occlusion patterns. Channel-aware attention is introduced to learn the relation between areas and channels. Furthermore, we can identify suspects using a rule which focuses more on the visible area and focuses less on the occluded area in the specific occlusion pattern. Extensive evaluations on two challenging datasets confirm the effectiveness of the proposed CGM.																	0941-0643	1433-3058															10.1007/s00521-020-05314-7		SEP 2020											
J								Regional land planning based on BPNN and space mining technology	NEURAL COMPUTING & APPLICATIONS										BPNN; Space mining technology; Regional planning; Land planning; Modeling		The rationality of regional land planning needs to be evaluated through intelligent technology and continuously optimized. At present, most land planning is temporarily adjusted according to actual needs, so it does not have real-time dynamics. In order to improve the rationality of regional land planning, based on BP neural network, this study combined the spatial mining technology to construct a regional land planning model, and used BP neural network and SVM technology to establish a relationship model between the impact factor value and distance scale factor. Moreover, based on the gray system theory, this study uses the gray correlation model to measure the coupling degree between industrial structure and land use, and analyzes the correlation between various factors. In addition, the artificial neural network after learning and testing can be used for dynamic simulation calculations. The research results show that this algorithm has some practical effects.																	0941-0643	1433-3058															10.1007/s00521-020-05316-5		SEP 2020											
J								Feature Selection of Network Data VIAl(2,p)Regularization	COGNITIVE COMPUTATION										Feature selection; Sparsity regularization; l(2p) (0 < p < 1) norm]; Smoothing trust region	UNSUPERVISED FEATURE-SELECTION; FRAMEWORK; NONSMOOTH	Feature selection is the process of selecting a subset of relevant features from the original feature set, and it plays an important role in handling high-dimensional data. In recent years, sparse learning-based feature selection approaches have been widely studied, and different regularizers have been proposed. Among these regularizers, it has been found thatl(2,p)(0 2,porm-based feature selection to deal with network data in an unsupervised scenario, and design an iterative algorithm using the framework of the alternating direction method of multipliers. In order to deal with the nonsmooth and non-Lipschitz continuous subproblem caused byl(2,p), we design a nonmonotone smoothing trust region algorithm and present its global convergence analysis. The extensive numerical experiments on real-world network datasets validate the effectiveness of the proposed model and algorithm.																	1866-9956	1866-9964															10.1007/s12559-020-09763-z		SEP 2020											
J								Community detection and co-author recommendation in co-author networks	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Co-author network; Community detection; Modularity; Link analysis; Hilltop algorithm	SELECTION	With the increasing complexity of scientific research and the expanding scale of projects, scientific research cooperation is an important trend in large-scale research. The analysis of co-authorship networks is a big data problem due to the expanding scale of the literature. Without sufficient data mining, research cooperation will be limited to a similar group, namely, a "small group", in the co-author networks. This "small group" limits the research results and openness. However, the researchers are not aware of the existence of other researchers due to insufficient big data support. Considering the importance of discovering communities and recommending potential collaborations from a large body of literature, we propose an enhanced clustering algorithm for detecting communities. It includes the selection of an initial central node and the redefinition of the distance and iteration of the central node. We also propose a method that is based on the hilltop algorithm, which is an algorithm that is used in search engines, for recommending co-authors via link analysis. The co-author candidate set is improved by screening and scoring. In screening, the expert set formation of the hilltop algorithm is added. The score is calculated from the durations and quantity of the collaborations. Via experiments, communities can be extracted, and co-authors can be recommended from the big data of the scientific research literature.																	1868-8071	1868-808X															10.1007/s13042-020-01190-8		SEP 2020											
J								Reasoning about group social commitments in multi-agent systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-agent systems; Communicative social commitments; Group social commitments; Correspondence theory; Soundness; Completeness	MODEL CHECKING; AGENT COMMUNICATION; KNOWLEDGE; LOGIC; SEMANTICS; LANGUAGES; TOOL	This paper aims to analayze and reason about group communicating social commitments in Multi-Agent Systems (MASs). In fact, this paper presents Computation Tree Logic Group Commitments (CTLGC), a temporal logic of group commitments for agent communications which extends computation tree logic (CTL) to reason about group social commitments and their fulfillments simultaneously. To do so, we classify groups of communicating agents into divisible and indivisible. After that, we divide group commitments into two categories: one-to-group and group-to-one commitments. Then, we advocate the necessary social accessibility relations that are needed to capture the semantics of each type of group commitments. Thereafter, we use Benthem's Correspondence Theory for modal logics to prove the soundness and completeness of the proposed CTLGC logic. Particularly, we present a set of reasoning postulates in CTLGC and correspond them to their related classes of frames. We use the NetBill protocol, a concrete example from business domain to illustrate each reasoning postulate. Furthermore, we adopt the interpreted systems as an underlying formalism over which our developed postulates are interpreted. When correspondence exists, this confirms that the CTLGC logic generated by any subset of the proposed postulates is sound and complete with respect to models that are built on the corresponding frames. By so doing, we provide a novel, consistent, formal and computationally grounded semantic to reason about group communicating social commitments and their fulfillments in MASs and prove the soundness and completeness of the proposed logic.																	1868-5137	1868-5145															10.1007/s12652-020-02498-7		SEP 2020											
J								A predictive risk level classification of diabetic patients using deep learning modified neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning modified neural network (DLMNN); Hadoop distributed file system (HDFS); Naive bayes (NB); K-means clustering (KMC); Improved K-means clustering (IKMC) algorithm		In health care firm, data mining (DM) has an effectual role in predicting the diseases. Today, diabetes is the chief global health issue. Several algorithms are introduced for predicting the diabetes disease and its accuracy estimation. Yet, there is no effectual algorithm for providing the severity of diabetes in respect of ratio which interprets the impact of diabetes on different organs of the human body. To overcome such drawbacks, predictive and risk level classification of diabetes patients using DLMNN and Naive Bayes (NB) classification methods is system model. This system model system comprises 2 phases namely, phase-1: diabetic disease prediction model, and phase-2: risk analysis. In phase-1, the patient data are taken as of the dataset. Then, from this patient dataset repeated data are removed using HDFS Map Reduce (). Next, as the preprocessing stage, the missing attributes are replaced by averaging the considered data. After that, from the preprocessed data the disease is predicted using DLMNN classification method which results in obtaining the diabetic patient data. Then, the diabetic patient data are sent to phase-2. In phase 2, the missing attributes are replaced using the same average method. Next, the patient data is sorted centered on age utilizing recursive K-means clustering algorithm. Finally, the clustered patient data is classified using the NB classifier algorithm. Experiential results contrasted the system model modified deep learning algorithm with the existing IKMC algorithm in rapports of precision, accuracy, F-measure, and recall. The outcomes confirmed that the system model diabetes prediction and analysis model shows better results on considering the existent methods.																	1868-5137	1868-5145															10.1007/s12652-020-02490-1		SEP 2020											
J								An efficient comparison of two indexing-based deep learning models for the formation of a web-application based IoT-cloud network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep belief network; Convolutional neural network; Similarity-based indexing; Web-based application; Client-server model	IMAGE RETRIEVAL; COLOR	To search a particular image file amongst a large and massive encrypted database is very time consuming and hectic task. Many encryption and searching techniques have been used but they did not prove effective to support smart devices in order to provide input image and retrieve the required results on the personal gadgets of the user. Therefore, based on these facts, an intelligent and advanced multimodal system has been developed in this paper which is based on encrypted content-based search. Thus, in order to perform content-based search two type of novel deep learning techniques, namely cluster-based deep belief network and supervised similarity-based convolutional neural network have been used. The proposed models have been influenced by special indexing techniques to retrieve the best relevant and similar images in very less time. In order to secure the entire images of the database, confusion-diffusion technique based on chaotic map encryption has been used. In order to develop the internet of things model and to support smart device users, a web based application has also been developed using Apache Tomcat server and linking between java and MATLAB has been done using MATLAB engine. Analysis of many parameters like precision, recall, f-score, entropy, correlation coefficient and time has been done here. Also, the proposed system has been compared to many latest and related techniques by using two benchmark and renowned datasets namely WANG and COIL-100.																	1868-5137	1868-5145															10.1007/s12652-020-02500-2		SEP 2020											
J								Secure and efficient firmware update for increasing IoT-enabled smart devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; Hybrid update scheme; Firmware distribution; Firmware validation; Blockchain; Peer-to-peer; Smart contract	BLOCKCHAIN; INTERNET; SYSTEM; THINGS	With the rapidly increasing number of Internet of Things (IoT) devices, various interconnected devices have become targets of growing cyberattacks. Keeping the firmware of an IoT device up-to-date is one feasible way to protect the device against cyberattacks. The existing approaches of firmware update (including distribution and validation) are not scalable in distribution to increasing numbers of devices, however, let alone able to provide reliable validation. To address the above issues, this study proposes a hybrid update scheme, including distributed membership-based firmware sharing for firmware distribution and smart-contract-enabled firmware validation via a blockchain (BC). This hybrid update approach leverages the advantages of a peer-to-peer network and a blockchain. Evaluation of the study has shown that the proposed distributed membership-based firmware sharing is more secure and scalable to an increasing number of devices. The proposed smart-contract-enabled firmware validation is more efficient than the firmware validation in existing studies since it can effectively reduce unwanted repeat validation via a smart contract in a blockchain. In addition, it can help make sure all members of an IoT-enabled service having the right firmware before providing the service to users. Such designs can improve service quality and also reduce as much human intervention to leverage the strengths of an IoT-enabled application or system.																	1868-5137	1868-5145															10.1007/s12652-020-02492-z		SEP 2020											
J								Recognition of plant leaf diseases based on computer vision	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Plant disease recognition; Image segmentation; Image classification; Color space; Convolutional neural networks	IMAGE; SEGMENTATION; SELECTION; IMPACT; COLOR; SIZE	Agriculture is one of the most important sources of income for people in many countries. However, plant disease issues influence many farmers, as diseases in plants often naturally occur. If proper care is not taken, diseases can have hazardous effects on plants and influence the product quality, quantity or productivity. Therefore, the detection and prevention of plant diseases are serious concerns and should be considered to increase productivity. An effective identification technology can be beneficial for monitoring plant diseases. Generally, the leaves of plants show the first signs of plant disease, and most diseases can be detected from the symptoms that appear on the leaves. Therefore, this paper introduces a novel method for the detection of plant leaf diseases. The method is divided into two parts: image segmentation and image classification. First, a hue, saturation and intensity-based and LAB-based hybrid segmentation algorithm is proposed and used for the disease symptom segmentation of plant disease images. Then, the segmented images are input into a convolutional neural network for image classification. The validation accuracy obtained using this approach was approximately 15.51% higher than that for the conventional method. Additionally, the detection results showed that the average detection rate was 75.59% under complex background conditions, and most of the diseases were effectively detected. Thus, the approach of combined segmentation and classification is effective for plant disease identification, and our empirical research validates the advantages of the proposed method.																	1868-5137	1868-5145															10.1007/s12652-020-02505-x		SEP 2020											
J								Imbalanced regression and extreme value prediction	MACHINE LEARNING										Supervised learning; Imbalanced domain learning; Imbalanced regression; Extreme value prediction	NEURAL-NETWORKS; MODEL	Research in imbalanced domain learning has almost exclusively focused on solving classification tasks for accurate prediction of cases labelled with a rare class. Approaches for addressing such problems in regression tasks are still scarce due to two main factors. First, standard regression tasks assume each domain value as equally important. Second, standard evaluation metrics focus on assessing the performance of models on the most common values of data distributions. In this paper, we present an approach to tackle imbalanced regression tasks where the objective is to predict extreme (rare) values. We propose an approach to formalise such tasks and to optimise/evaluate predictive models, overcoming the factors mentioned and issues in related work. We present an automatic and non-parametric method to obtain relevance functions, building on the concept of relevance as the mapping of target values into non-uniform domain preferences. Then, we proposeSERA, a new evaluation metric capable of assessing the effectiveness and of optimising models towards the prediction of extreme values while penalising severe model bias. An experimental study demonstrates howSERAprovides valid and useful insights into the performance of models in imbalanced regression tasks.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1803	1835		10.1007/s10994-020-05900-9		SEP 2020											
J								Ada-boundary: accelerating DNN training via adaptive boundary batch selection	MACHINE LEARNING										Batch selection; Acceleration; Convergence; Decision boundary		Neural networks converge faster with help from a smart batch selection strategy. In this regard, we proposeAda-Boundary, a novel and simple adaptive batch selection algorithm that constructs an effective mini-batch according to the learning progress of the model. Our key idea is to exploitconfusingsamples for which the model cannot predict labels with high confidence. Thus, samples near the current decision boundary are considered to be the most effective for expediting convergence. Taking advantage of this design,Ada-Boundarymaintained its dominance for various degrees of training difficulty. We demonstrate the advantage ofAda-Boundaryby extensive experimentation using CNNs with five benchmark data sets.Ada-Boundarywas shown to produce a relative improvement in test errors by up to 31.80% compared with the baseline for a fixed wall-clock training time, thereby achieving a faster convergence speed.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1837	1853		10.1007/s10994-020-05903-6		SEP 2020											
J								A decision-theoretic approach for model interpretability in Bayesian framework	MACHINE LEARNING										Interpretable machine learning; Bayesian predictive models		A salient approach to interpretable machine learning is to restrict modeling to simple models. In the Bayesian framework, this can be pursued by restricting the model structure and prior to favor interpretable models. Fundamentally, however, interpretability is about users' preferences, not the data generation mechanism; it is more natural to formulate interpretability as a utility function. In this work, we propose an interpretability utility, which explicates the trade-off between explanation fidelity and interpretability in the Bayesian framework. The method consists of two steps. First, a reference model, possibly a black-box Bayesian predictive model which does not compromise accuracy, is fitted to the training data. Second, a proxy model from an interpretable model family that best mimics the predictive behaviour of the reference model is found by optimizing the interpretability utility function. The approach is model agnostic-neither the interpretable model nor the reference model are restricted to a certain class of models-and the optimization problem can be solved using standard tools. Through experiments on real-word data sets, using decision trees as interpretable models and Bayesian additive regression models as reference models, we show that for the same level of interpretability, our approach generates more accurate models than the alternative of restricting the prior. We also propose a systematic way to measure stability of interpretabile models constructed by different interpretability approaches and show that our proposed approach generates more stable models.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1855	1876		10.1007/s10994-020-05901-8		SEP 2020											
J								Skew Gaussian processes for classification	MACHINE LEARNING										Skew Gaussian Process; Nonparametric; Classifier; Probit; Conjugate; Skew	BAYES; COMPUTATION; REGRESSION	Gaussian processes (GPs) are distributions over functions, which provide a Bayesian nonparametric approach to regression and classification. In spite of their success, GPs have limited use in some applications, for example, in some cases a symmetric distribution with respect to its mean is an unreasonable model. This implies, for instance, that the mean and the median coincide, while the mean and median in an asymmetric (skewed) distribution can be different numbers. In this paper, we propose skew-Gaussian processes (SkewGPs) as a non-parametric prior over functions. A SkewGP extends the multivariateunified skew-normaldistribution over finite dimensional vectors to a stochastic processes. The SkewGP class of distributions includes GPs and, therefore, SkewGPs inherit all good properties of GPs and increase their flexibility by allowing asymmetry in the probabilistic model. By exploiting the fact that SkewGP and probit likelihood are conjugate model, we derive closed form expressions for the marginal likelihood and predictive distribution of this new nonparametric classifier. We verify empirically that the proposed SkewGP classifier provides a better performance than a GP classifier based on either Laplace's method or expectation propagation.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1877	1902		10.1007/s10994-020-05906-3		SEP 2020											
J								Weak approximation of transformed stochastic gradient MCMC	MACHINE LEARNING										Stochastic gradient MCMC; Transform; Convergence analysis; Ito process		Stochastic gradient Langevin dynamics (SGLD) is a computationally efficient sampler for Bayesian posterior inference given a large scale dataset and a complex model. Although SGLD is designed for unbounded random variables, practical models often incorporate variables within a bounded domain, such as non-negative or a finite interval. The use of variable transformation is a typical way to handle such a bounded variable. This paper reveals that several mapping approaches commonly used in the literature produce erroneous samples from theoretical and empirical perspectives. We show that the change of random variable in discretization using an invertible Lipschitz mapping function overcomes the pitfall as well as attains the weak convergence, while the other methods are numerically unstable or cannot be justified theoretically. Experiments demonstrate its efficacy for widely-used models with bounded latent variables, including Bayesian non-negative matrix factorization and binary neural networks.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1903	1923		10.1007/s10994-020-05904-5		SEP 2020											
J								Statistical hierarchical clustering algorithm for outlier detection in evolving data streams	MACHINE LEARNING										Big data; Clustering; Anomaly detection; Fraud detection		Anomaly detection is a hard data analysis process that requires constant creation and improvement of data analysis algorithms. Using traditional clustering algorithms to analyse data streams is impossible due to processing power and memory issues. To solve this, the traditional clustering algorithm complexity needed to be reduced, which led to the creation of sequential clustering algorithms. The usual approach is two-phase clustering, which usesonlinephase to relax data details and complexity, andofflinephase to cluster concepts created in theonlinephase. Detecting anomalies in a data stream is usually solved in theonlinephase, as it requires unreduced data. Contrarily, producing good macro-clustering is done in theofflinephase, which is the reason why two-phase clustering algorithms have difficulty being equally good in anomaly detection and macro-clustering. In this paper, we propose a statistical hierarchical clustering algorithm equally suitable for both detecting anomalies and macro-clustering. The proposed algorithm is single-phased and uses statistical inference on the input data stream, resulting in statistical distributions that are constantly updated. This makes the classification adaptable, allowing agglomeration of outliers into clusters, tracking population evolution, and to be used without knowing the expected number of clusters and outliers. The proposed algorithm was tested against typical clustering algorithms, including two-phase algorithms suitable for data stream analysis. A number of typical test cases were selected, to show the universality and qualities of the proposed clustering algorithm.																	0885-6125	1573-0565															10.1007/s10994-020-05905-4		SEP 2020											
J								Detection of 3D bounding boxes of vehicles using perspective transformation for accurate speed measurement	MACHINE VISION AND APPLICATIONS										Traffic surveillance; 3D object detection; Deep learning; Perspective transformation	CAMERA CALIBRATION	Detection and tracking of vehicles captured by traffic surveillance cameras is a key component of intelligent transportation systems. We present an improved version of our algorithm for detection of 3D bounding boxes of vehicles, their tracking and subsequent speed estimation. Our algorithm utilizes the known geometry of vanishing points in the surveilled scene to construct a perspective transformation. The transformation enables an intuitive simplification of the problem of detecting 3D bounding boxes to detection of 2D bounding boxes with one additional parameter using a standard 2D object detector. Main contribution of this paper is an improved construction of the perspective transformation which is more robust and fully automatic and an extended experimental evaluation of speed estimation. We test our algorithm on the speed estimation task of the BrnoCompSpeed dataset. We evaluate our approach with different configurations to gauge the relationship between accuracy and computational costs and benefits of 3D bounding box detection over 2D detection. All of the tested configurations run in real time and are fully automatic. Compared to other published state-of-the-art fully automatic results, our algorithm reduces the mean absolute speed measurement error by 32% (1.10 km/h to 0.75 km/h) and the absolute median error by 40% (0.97 km/h to 0.58 km/h).																	0932-8092	1432-1769				SEP 4	2020	31	7-8							62	10.1007/s00138-020-01117-x													
J								Low-Rank Discriminative Adaptive Graph Preserving Subspace Learning	NEURAL PROCESSING LETTERS										Low-rank constraints; Graph preserving; Subspace learning; Feature extraction	FACE RECOGNITION; DIMENSIONALITY REDUCTION; REPRESENTATION; EIGENFACES	The global and local geometric structures of data play a key role in subspace learning. Although many manifold-based subspace learning methods have been proposed for preserving the local geometric structure of data, they usually use a predefined neighbor graph to characterize it. However, the predefined neighbor graph might be not optimal since it keeps fixed during the subsequent subspace learning process. Moreover, most manifold-based subspace learning methods ignore the global structure of data. To address these issues, we propose a low-rank discriminative adaptive graph preserving (LRDAGP) subspace learning method for image feature extraction and recognition by integrating the low-rank representation , adaptive manifold learning, and supervised regularizer into a unified framework. To capture the optimal local geometric structure of data for subspace learning, LRDAGP adopts an adaptive manifold learning strategy that the neighbor graph is adaptively updated during the subspace learning process. To capture the optimal global structure of data for subspace learning, LRDAGP also seeks the low-rank representations of data in a low-dimensional subspace during the subspace learning process. Moreover, for improving the discrimination ability of the learned subspace, a supervised regularizer is designed and incorporated into the LRDAGP model. Experimental results on several image datasets show that LRDAGP is effective for image feature extraction and recognition.																	1370-4621	1573-773X															10.1007/s11063-020-10340-6		SEP 2020											
J								An LMI Based State Estimation for Fractional-Order Memristive Neural Networks with Leakage and Time Delays	NEURAL PROCESSING LETTERS										Caputo's fractional derivative; Fractional-order memristive neural networks; Lyapunov-Krasvoskii functional; Linear matrix inequalities (LMIs)	GLOBAL ASYMPTOTIC STABILITY; EXPONENTIAL STABILITY; DISCRETE; SYSTEM; SYNCHRONIZATION	This paper investigates the state estimation problem for a class of fractional-order memristive neural networks (FOMNNs) with leakage and time delay. The main objective of this study is to construct an efficient estimator such that the state of the corresponding estimation error is globally stable. Distinct to the previous studies, the state estimation problem of FOMNNs is investigated through fractional-order Lyapunov direct method. The sufficient conditions that ensure the global stability of the error system has been derived as a set of solvable linear matrix inequalities. In order to validate the effectiveness of the proposed theoretical results, two numerical examples have been illustrated.																	1370-4621	1573-773X															10.1007/s11063-020-10338-0		SEP 2020											
J								On decision evaluation functions in three-way decision spaces derived from overlap and grouping functions	SOFT COMPUTING										(Semi-)three-way decision spaces; Decision evaluation functions; Overlap functions; Grouping functions	FUZZY IMPLICATIONS; INTERVAL OVERLAP; CLASSIFICATION; SYSTEMS; SETS; MODELS; WEB	Overlap and grouping functions, as two kinds of particular binary aggregation functions, have been continuously discussed in the literature for their vast preponderance in some real applications. Meanwhile, after Hu introduced the notion of three-way decision spaces, the decision evaluation functions in three-way decision spaces constructed from the so-called semi-decision evaluation functions have been investigated consistently. This paper continues to consider this research topic and mainly focuses the decision evaluation functions in three-way decision spaces obtained from overlap and grouping functions. Firstly, based on overlap and grouping functions, we give several methods of constructing semi-decision evaluation functions in semi-three-way decision spaces. Secondly, we show some novel semi-decision evaluation functions which are related to fuzzy sets, interval-valued fuzzy sets, fuzzy relations and hesitant fuzzy sets, respectively. Finally, using overlap and grouping functions, we get some ways to construct decision evaluation functions in three-way decision spaces from semi-decision evaluation functions in semi-three-way decision spaces.																	1432-7643	1433-7479				OCT	2020	24	20					15159	15178		10.1007/s00500-020-05283-y		SEP 2020											
J								Ask a(n)droid to tell you the odds: probabilistic security-by-contract for mobile devices	SOFT COMPUTING										Security-by-contract; Android; Statistical analysis; Probabilistic model checking	MALWARE; SYSTEM	Security-by-contract is a paradigm proposed for the secure installation, usage, and monitoring of apps into mobile devices, with the aim of establishing, controlling, and, if necessary, enforcing security-critical behaviors. In this paper, we extend this paradigm with new functionalities allowing for a quantitative estimation of such behaviors, in order to reveal in real time the more and more challenging subtleties of new-generation malware and repackaged apps. The novel paradigm is based on formal means and techniques ranging from statistical analysis to probabilistic model checking. The framework, deployed in the Android environment, is evaluated by examining both its effectiveness with respect to a benchmark of real-world malware and its effect on the execution of genuine, secure apps.																	1432-7643	1433-7479															10.1007/s00500-020-05299-4		SEP 2020											
J								ModPSO-CNN: an evolutionary convolution neural network with application to visual recognition	SOFT COMPUTING										Particle swarm optimization; Convolution neural network; Backpropagation; Visual recognition	IDENTIFICATION; SEGMENTATION; MLP	Training optimization plays a vital role in the development of convolution neural network (CNN). CNNs are hard to train because of the presence of multiple local minima. The optimization problem for a CNN is non-convex, hence, has multiple local minima. If any of the chosen hyper-parameters are not appropriate, it will end up at bad local minima, which leads to poor performance. Hence, proper optimization of the training algorithm for CNN is the key to converge to a good local minimum. Therefore, in this paper, we introduce an evolutionary convolution neural network (ModPSO-CNN) algorithm. The proposed algorithm results in the fusion of modified particle swarm optimization (ModPSO) along with backpropagation (BP) and convolution neural network (CNN). The training of CNN involves ModPSO along with backpropagation (BP) algorithm to encourage performance improvement by avoiding premature convergence and local minima. The ModPSO have adaptive, dynamic and improved parameters, to handle the issues in training CNN. The adaptive and dynamic parameters bring a proper balance between the global and local search ability, while an improved parameter keeps the diversity of the swarm. The proposed ModPSO algorithm is validated on three standard mathematical test functions and compared with three variants of the benchmark PSO algorithm. Furthermore, the performance of the proposed ModPSO-CNN is also compared with other training algorithms focusing on the analysis of computational cost, convergence and accuracy based on a standard problem specific to classification applications, such as CIFAR-10 dataset and face and skin detection dataset.																	1432-7643	1433-7479															10.1007/s00500-020-05288-7		SEP 2020											
J								Complex q-rung orthopair fuzzy 2-tuple linguistic Maclaurin symmetric mean operators and its application to emergency program selection	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										complex q-rung orthopair fuzzy 2-tuple linguistic set; emergency management program evaluation; Maclaurin symmetric mean operator; multiattribute group decision-making	AGGREGATION OPERATORS; BONFERRONI OPERATORS; SETS; NUMBERS; TOPSIS; MODEL	This essay designs an innovate approach to work out linguistic multiattribute group decision-making (MAGDM) issues with complex q-rung orthopair fuzzy 2-tuple linguistic (Cq-ROF2TL) evaluation information. To begin with, the conception of Cq-ROF2TL set is propounded to express uncertain and fuzzy assessment information. Meanwhile, the score and accuracy function, a comparison approach, Cq-ROF2TL weighted averaging, and Cq-ROF2TL weighted geometric operator are put forward. Furthermore, to take into consideration the correlation among multiple input data, the Cq-ROF2TL Maclaurin symmetric mean (MSM) operator, the Cq-ROF2TL dual MSM operator and their weighted forms are presented. Several attractive characteristics and particular instances of the developed operators are also explored at length. Later, an innovative MAGDM methodology is designed based upon the propounded operators to settle the emergency program evaluation issue under the Cq-ROF2TL circumstance. Consequently, the efficiency and outstanding superiority of the created approach are severally substantiated by parameter exploration and detailed comparative analysis.																	0884-8173	1098-111X				NOV	2020	35	11					1749	1790		10.1002/int.22271		SEP 2020											
J								Reactive VNS algorithm for the maximum k-subset intersection problem	JOURNAL OF HEURISTICS										Reactive VNS; Metaheuristics; Heuristics; Maximum intersection ofk-subsets problem	VARIABLE NEIGHBORHOOD SEARCH; VEHICLE-ROUTING PROBLEM; GRASP	This paper proposes a reactive VNS metaheuristic for the maximum intersection ofk-subsets problem (kMIS). The kMIS is defined as: Given a set of elements, a subset family of the first set and an integerk. The problem consists of findingksubset so that the intersection is maximum. Our VNS metaheuristic incorporates strategies used in GRASP metaheuristics, such as the GRASP construction phase and the Reactive GRASP. Both were used in the shaking phase as a reactive components to VNS. We also propose what we call teh Dynamic Step, a new way to increase the VNS neighborhood. All of these strategies, as well as the Skewed VNS, were added to our Reactive VNS algorithm for kMIS. Computational results showed that the new algorithm outperforms the state-of-the-art algorithm.																	1381-1231	1572-9397				DEC	2020	26	6					913	941		10.1007/s10732-020-09452-y		SEP 2020											
J								Iris Segmentation Using Feature Channel Optimization for Noisy Environments	COGNITIVE COMPUTATION										Iris segmentation; U-Net; Dense block; Dilated convolution; Feature channel optimization	U-NET	In recent years, iris recognition has been widely used in various fields. As the first step of iris recognition, segmentation accuracy is of great significance to the final recognition. However, iris images exhibit a variety of noise in the real world, which leads to lower segmentation accuracy than the ideal case. To address this problem, this paper proposes an iris segmentation method using feature channel optimization for noisy images. The method for non-ideal environments with noise is more suitable for practical applications. We add dense blocks and dilated convolutional layers to the encoder so that the information gradient flow obtained by different layers can be reused, and the receptive field can be expanded. In the decoder, based on Jensen-Shannon (JS) divergence, we first recalculate the weight of the feature channels obtained from each layer, which enhances the useful information and suppresses the interference information in the noisy environments to boost the segmentation accuracy. The proposed architecture is validated in the CASIA v4.0 interval (CASIA) and IIT Delhi v1.0 datasets (IITD). For CASIA, the mean error rate is 0.78%, and the F-measure value is 98.21%. For IITD, the mean error rate is 0.97%, and the F-measure value is 97.87%. Experimental results show that the proposed method outperforms other state-of-art methods under noisy environments, such as Gaussian blur, Gaussian noise, and salt and pepper noise.																	1866-9956	1866-9964															10.1007/s12559-020-09759-9		SEP 2020											
J								Video frame interpolation via optical flow estimation with image inpainting	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										frame interpolation; image inpainting; optical flow; video processing	SHAPE-BASED INTERPOLATION; CROWD EVACUATION	As we all know, video frame rate determines the quality of the video. The higher the frame rate, the smoother the movements in the picture, the clearer the information expressed, and the better the viewing experience for people. Video interpolation aims to increase the video frame rate by generating a new frame image using the relevant information between two consecutive frames, which is essential in the field of computer vision. The traditional motion compensation interpolation method will cause holes and overlaps in the reconstructed frame, and is easily affected by the quality of optical flow. Therefore, this paper proposes a video frame interpolation method via optical flow estimation with image inpainting. First, the optical flow between the input frames is estimated via combined local and global-total variation (CLG-TV) optical flow estimation model. Then, the intermediate frames are synthesized under the guidance of the optical flow. Finally, the nonlocal self-similarity between the video frames is used to solve the optimization problem, to fix the pixel loss area in the interpolated frame. Quantitative and qualitative experimental results show that this method can effectively improve the quality of optical flow estimation, generate realistic and smooth video frames, and effectively increase the video frame rate.																	0884-8173	1098-111X				DEC	2020	35	12					2087	2102		10.1002/int.22285		SEP 2020											
J								An Improved Particle Swarm Optimization Algorithm forOptimal Allocation of Distributed Generation Units in Radial Power Systems	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING											DISTRIBUTION NETWORKS; OPTIMAL PLACEMENT; MODELS; MINIMIZATION	In this paper, an improved particle swarm optimization method (PSO) is proposed to optimally size and place a DG unit in an electrical power system so as to improve voltage profile and reduce active power losses in the system. An IEEE 34 distribution bus system is used as a case study for this research. A new equation of weight inertia is proposed so as to improve the performance of the PSO conventional algorithm. This development is done by controlling the inertia weight which affects the updating velocity of particles in the algorithm. Matlab codes are developed for the adapted electrical power system and the improved PSO algorithm. Results show that the proposed PSO algorithm successfully finds the optimal size and location of the desired DG unit with a capacity of 1.6722 MW at bus number 10. This makes the voltage magnitude of the selected bus equal to 1.0055 pu and improves the status of the electrical power system in general. The minimum value of fitness losses using the applied algorithm is found to be 0.0.0406 while the average elapsed time is 62.2325 s. In addition to that, the proposed PSO algorithm reduces the active power losses by 31.6%. This means that the average elapsed time is reduced by 21% by using the proposed PSO algorithm as compared to the conventional PSO algorithm that is based on the liner inertia weight equation.																	1687-9724	1687-9732				SEP 3	2020	2020								8824988	10.1155/2020/8824988													
J								Group recommendation with noisy subjective preferences	COMPUTATIONAL INTELLIGENCE										computational social choice; group recommendation; noisy subjective preferences		Social choice theory provides a principled framework for the aggregation of individuals' preferences in support of group decision-making and recommendation. Much of this work, however, either assumes that individuals' subjective preferences (and thus, their votes) are correctly specified by the individuals themselves, or alternatively that the votes of individuals are noisy estimates of some underlying ground truth over rankings of alternatives. We argue that neither model appropriately addresses some of the issues which arise in the context of group-recommendation domains where individuals have subjective preferences but for some reason (eg, the high cognitive burden, concerns about privacy, etc.) may instead vote using a noisy estimate of their subjective preference rankings. In this paper, we propose a general probabilistic framework for modeling noisy subjective preferences, and explore the accuracy and reliability of four well-studied voting rules under various noise models. Our results demonstrate that there is no single reliable method amongst the examined methods. Specifically, we observe the change in noise distribution can flip one method from being the most reliable to the least.																	0824-7935	1467-8640															10.1111/coin.12398		SEP 2020											
J								A roadmap to neural automatic post-editing: an empirical approach	MACHINE TRANSLATION										Automatic post-editing; Neural post-editing; Multi-source; Deep learning; Empirical evaluation; Machine Translation		In a translation workflow, machine translation (MT) is almost always followed by a human post-editing step, where the raw MT output is corrected to meet required quality standards. To reduce the number of errors human translators need to correct, automatic post-editing (APE) methods have been developed and deployed in such workflows. With the advances in deep learning, neural APE (NPE) systems have outranked more traditional, statistical, ones. However, the plethora of options, variables and settings, as well as the relation between NPE performance and train/test data makes it difficult to select the most suitable approach for a given use case. In this article, we systematically analyse these different parameters with respect to NPE performance. We build an NPE "roadmap" to trace the different decision points and train a set of systems selecting different options through the roadmap. We also propose a novel approach for APE with data augmentation. We then analyse the performance of 15 of these systems and identify the best ones. In fact, the best systems are the ones that follow the newly-proposed method. The work presented in this article follows from a collaborative project between Microsoft and the ADAPT centre. The data provided by Microsoft originates from phrase-based statistical MT (PBSMT) systems employed in production. All tested NPE systems significantly increase the translation quality, proving the effectiveness of neural post-editing in the context of a commercial translation workflow that leverages PBSMT.																	0922-6567	1573-0573				SEP	2020	34	2-3					67	96		10.1007/s10590-020-09249-7		SEP 2020											
J								Machine-learning for automatic prediction of flatness deviation considering the wear of the face mill teeth	JOURNAL OF INTELLIGENT MANUFACTURING										Face milling; Wear; Tool life; Tool condition monitoring; Flatness deviation; Cutting power; Random forest; SMOTE	SURFACE-ROUGHNESS; TOOL WEAR; ARTIFICIAL-INTELLIGENCE; NEURAL-NETWORKS; MODELS; ONLINE; SYSTEM; IMPROVEMENT; PARAMETERS; OPERATIONS	The acceptance of the machined surfaces not only depends on roughness parameters but also in the flatness deviation (Delta(fl)). Hence, before reaching the threshold of flatness deviation caused by the wear of the face mill, the tool inserts need to be changed to avoid the expected product rejection. As current CNC machines have the facility to track, in real-time, the main drive power, the present study utilizes this facility to predict the flatness deviation-with proper consideration to the amount of wear of cutting tool insert's edge. The prediction of deviation from flatness is evaluated as a regression and a classification problem, while different machine-learning techniques like Multilayer Perceptrons, Radial Basis Functions Networks, Decision Trees and Random Forest ensembles have been examined. Finally, Random Forest ensembles combined with Synthetic Minority Over-sampling Technique (SMOTE) balancing technique showed the highest performance when the flatness levels are discretized taking into account industrial requirements. The SMOTE balancing technique resulted in a very useful strategy to avoid the strong limitations that small experiment datasets produce in the accuracy of machine-learning models.																	0956-5515	1572-8145															10.1007/s10845-020-01645-3		SEP 2020											
J								Indirect tool monitoring in drilling based on gap sensor signal and multilayer perceptron feed forward neural network	JOURNAL OF INTELLIGENT MANUFACTURING										Indirect tool monitoring; Drilling; Supervised learning; Multilayer perceptron feed forward neural network; Statistical analysis	WEAR; MACHINE; CLASSIFICATION; PERFORMANCE; PREDICTION; FAILURE	In this study, an indirect tool monitoring was developed based on the installation of a gap sensor in measuring the signal related to the tool behavior during the drilling process. Eleven types of twist drills with different tool conditions were utilized to differentiate the sensorial signals based on the tool states. A statistical analysis was conducted in the signal processing, by extracting the gap sensor signal associates from each tool condition, using the skewness and kurtosis features. Multi-class classification was conducted using the multilayer perceptron (MLP) feed forward neural network (FF-NN) model to classify and predict the tool condition based on the skewness and kurtosis data. The architectures of the MLP FF-NN models were varied to optimize the classification accuracy. This study found that the tool condition was correlated to the displacement of the drill machine spindle because the runout occurred when the sensor signal displayed fluctuation and irregularity trends. The peak intensity of the gap sensor signals increased with increasing wear severity of the twist drill. An ideal MLP FF-NN structure was achieved when the classification performance was optimized to be consistent with the learning curve.																	0956-5515	1572-8145															10.1007/s10845-020-01635-5		SEP 2020											
J								MLDroid-framework for Android malware detection using machine learning techniques	NEURAL COMPUTING & APPLICATIONS										Permissions; API calls; Feature selection methods; Android apps; Machine learning		This research paper presents MLDroid-a web-based framework-which helps to detect malware from Android devices. Due to increase in the popularity of Android devices, malware developers develop malware on daily basis to threaten the system integrity and user's privacy. The proposed framework detects malware from Android apps by performing its dynamic analysis. To detect malware from real-world apps, we trained our proposed framework by selecting features which are gained by implementing feature selection approaches. Further, these selected features help to build a model by considering different machine learning algorithms. Experiment was performed on 5,00,000 plus Android apps. Empirical result reveals that model developed by considering all the four distinct machine learning algorithms parallelly (i.e., deep learning algorithm, farthest first clustering, Y-MLP and nonlinear ensemble decision tree forest approach) and rough set analysis as a feature subset selection algorithm achieved the highest detection rate of 98.8% to detect malware from real-world apps.																	0941-0643	1433-3058															10.1007/s00521-020-05309-4		SEP 2020											
J								Enhanced synchronization-inspired clustering for high-dimensional data	COMPLEX & INTELLIGENT SYSTEMS										Synchronization-inspired; Clustering; High-dimensional dataset; Local density	METRICS; PCA	The synchronization-inspired clustering algorithm (Sync) is a novel and outstanding clustering algorithm, which can accurately cluster datasets with any shape, density and distribution. However, the high-dimensional dataset with high dimensionality, high noise, and high redundancy brings some new challenges for the synchronization-inspired clustering algorithm, resulting in a significant increase in clustering time and a decrease in clustering accuracy. To address these challenges, an enhanced synchronization-inspired clustering algorithm, namely SyncHigh, is developed in this paper to quickly and accurately cluster the high-dimensional datasets. First, a PCA-based (Principal Component Analysis) dimension purification strategy is designed to find the principal components in all attributes. Second, a density-based data merge strategy is constructed to reduce the number of objects participating in the synchronization-inspired clustering algorithm, thereby speeding up clustering time. Third, the Kuramoto Model is enhanced to deal with mass differences between objects caused by the density-based data merge strategy. Finally, extensive experimental results on synthetic and real-world datasets show the effectiveness and efficiency of our SyncHigh algorithm.																	2199-4536	2198-6053															10.1007/s40747-020-00191-y		SEP 2020											
J								Predicting irregularities in arrival times for transit buses with recurrent neural networks using GPS coordinates and weather data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent transportation systems; ITS; Traffic flow; Neural networks; GPS locations; Weather conditions		Intelligent transportation systems (ITS) play an important role in the quality of life of citizens in any metropolitan city. Despite various policies and strategies incorporated to increase the reliability and quality of service, public transportation authorities continue to face criticism from commuters largely due to irregularities in bus arrival times, most notably manifested in early or late arrivals. Due to these irregularities, commuters may miss important appointments, wait for too long at the bus stop, or arrive late for work. Therefore, accurate prediction models are needed to build better customer service solutions for transit systems, e.g. building accurate mobile apps for trip planning or sending bus delay/cancel notifications. Prediction models will also help in developing better appointment scheduling systems for doctors, dentists, and other businesses to take into account transit bus delays for their clients. In this paper, we seek to predict the occurrence of arrival time irregularities by mining GPS coordinates of transit buses provided by the Toronto Transit Commission (TTC) along with hourly weather data and using this data in machine learning models that we have developed. In our study, we compared the performance of a Long Short Term Memory Recurrent Neural Network (LSTM) model against four baseline models, an Artificial Neural Network (ANN), Support Vector Regression (SVR), Autoregressive Integrated Moving Average (ARIMA) and historical averages. We found that our LSTM model demonstrates the best prediction accuracy. The improved accuracy achieved by the LSTM model may lend itself to its ability to adjust and update the weights of neurons while accounting for long-term dependencies. In addition, we found that weather conditions play a significant role in improving the accuracy of our models. Therefore, we built a prediction model that combines an LSTM model with a Recurrent Neural Network Model (RNN) that focuses on the weather condition. Our findings also reveal that in nearly 37% of scheduled arrival times, buses either arrive early or late by a margin of more than 5 min, suggesting room for improvement in the current strategies employed by transit authorities.																	1868-5137	1868-5145															10.1007/s12652-020-02507-9		SEP 2020											
J								Finding hard faces with better proposals and classifier	MACHINE VISION AND APPLICATIONS										Face detection; Convolutional neural networks; Object detection	SAR IMAGES; RECOGNITION; TEXTURE; NETWORK	Recent studies witnessed that deep CNNs significantly improve the performance of face detection in the wild. However, detecting faces with small scales, large pose variations, and occlusions is still challenging. In this paper, to detect challenging faces, we present a boosted faster RCNN (F-RCN) version with an enhanced region proposal network (eRPN) module and newly introduced hard example mining strategies. The eRPN module generates better proposals than traditional RPN by integarating semantic information into the input feature maps. Two hard example mining strategies, i.e., online hard proposal mining (OHPM) and offline hard image mining (OHIM), are proposed to train better classifier. The OHPM can effectively sample quality and diversity of hard positive examples, which is important for detecting hard faces like tiny faces. The OHIM further boosts the classifier to detect hard faces via an auxiliary fine-tuning on a small proportion of training data. Experimental results on the FDDB, WIDER FACE, Pascal Faces, and AFW datasets show that our method significantly improves the faster-RCNN face detector and achieves performance superior or comparable to the state-of-the-art face detectors.																	0932-8092	1432-1769				SEP 3	2020	31	7-8							61	10.1007/s00138-020-01110-4													
J								A novel Pythagorean fuzzy AHP and fuzzy TOPSIS methodology for green supplier selection in the Industry 4.0 era	SOFT COMPUTING										Green supplier selection; Industry 4; 0; PFAHP; PFTOPSIS	CRITERIA DECISION-MAKING; OCCUPATIONAL-HEALTH; RISK-ASSESSMENT; MODEL; SAFETY; EXTENSION; FRAMEWORK	Advances in information and communication technology have created innovator technologies such as cloud computing, Internet of Things, big data analysis and artificial intelligence. These technologies have penetrated production systems and converted them smart. However, this transformation did not only affect production systems, but also differentiated supplier selection processes. In the supplier selection process, the usage of new technologies along with traditional and green criteria extensively has been investigated in recent years. This paper aims to develop a new group decision-making approach based on Industry 4.0 components for selecting the best green supplier by integrating AHP and TOPSIS methods under the Pythagorean fuzzy environment. In the proposed approach, judgments of different experts are expressed by linguistic terms based on Pythagorean fuzzy numbers. The interval-valued Pythagorean Fuzzy AHP method is utilized to determine the criteria weights. The Pythagorean Fuzzy TOPSIS method based on the distances of suppliers is applied to obtain the ranking of the suppliers and determine the most suitable one. Finally, a real case study on an agricultural tools and machinery company is presented to indicate the effectiveness and accuracy of the proposed selection approach.																	1432-7643	1433-7479															10.1007/s00500-020-05294-9		SEP 2020											
J								Elliptic entropy of uncertain random variables with application to portfolio selection	SOFT COMPUTING										Uncertainty theory; Elliptic entropy; Uncertain random variable; Chance theory; Mean-entropy model; Diversification index	SUPPLY CHAIN; CROSS-ENTROPY; MODEL; OPTIMIZATION; DEVIATION; RISK; DIVERSIFICATION; ALGORITHM	This paper investigates an elliptic entropy of uncertain random variables and its application in the area of portfolio selection. We first define the elliptic entropy to characterize the uncertainty of uncertain random variables and give some mathematical properties of the elliptic entropy. Then we derive a computational formula to calculate the elliptic entropy of function of uncertain random variables. Furthermore, we use the elliptic entropy to characterize the risk of investment and establish a mean-entropy portfolio selection model, in which the future security returns are described by uncertain random variables. Based on the chance theory, the equivalent form of the mean-entropy model is derived. To show the performance of the mean-entropy portfolio selection model, several numerical experiments are presented. We also numerically compare the mean-entropy model with the mean-variance model, the equi-weighted portfolio model, and the most diversified portfolio model by using three kinds of diversification indices. Numerical results show that the mean-entropy model outperforms the mean-variance model in selecting diversified portfolios no matter of using which diversification index.																	1432-7643	1433-7479															10.1007/s00500-020-05266-z		SEP 2020											
J								Digitized droop control of a high gain primitive converter-General performance analysis for smart city lighting application	COMPUTATIONAL INTELLIGENCE										analysis; control; converter; DC droop; simulation	DECENTRALIZED CONTROL; VOLTAGE REGULATION; DC	This article presents the detailed behavioral analysis of the reduced equation DC droop controller being adapted to the primitive high gain converter designed for DC loads. The article discusses the operation in its open and closed loop modes, voltage, and current control modes as part of the simulation study. The converter mode of operation is continuous in all. Graphical portrayals of statistical results describe the effectiveness of the controller against a standard proportional integral (PI) controller in time and frequency domain. The results substantiate that the DC droop control is superior in terms of efficiency, line regulation, dynamism, and current sharing and stability.																	0824-7935	1467-8640															10.1111/coin.12381		SEP 2020											
J								Unsupervised detail-preserving network for high quality monocular depth estimation	NEUROCOMPUTING										Unsupervised network; Monocular; Depth estimation; Rectangle convolution; Learned composite proximal operator		In this paper, we propose an unsupervised learning framework to address the problems of the inaccurate inference of depth details and the loss of spatial information for monocular depth estimation. First, as an unsupervised technique, the proposed framework takes easily collected stereo image pairs instead of ground truth depth data as inputs for training. Second, we design a rectangle convolution to capture global dependencies between neighboring pixels across entire rows or columns in an image, which can bring significant promotion on depth details inference. Third, we propose a learned depth refinement module including a color-guided refinement layer and a learned composite proximal operator to preserve depth discontinuities and obtain high quality depth map. The proposed network is fully differentiable and end-to-end trainable. Extensive experiments evaluated on KITTI, Cityscapes and Make3D dataset demonstrate our state-of-the-art performance and good cross-dataset generalization ability. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						1	13		10.1016/j.neucom.2020.05.015													
J								Non-rigid retinal image registration using an unsupervised structure-driven regression network	NEUROCOMPUTING										Retinal image registration; Unsupervised learning; Convolution neural networks; Deformable registration	APPEARANCE; FRAMEWORK	Retinal image registration is clinically significant to help clinicians obtain more complete details of the retinal structure by correlating the properties of the retina. However, existing methods suffer from great challenges due to time-consuming optimization and lack of ground truth. In this paper, we propose an unsupervised learning framework for non-rigid retinal image registration, which directly learns the mapping from a retinal image pair to their corresponding deformation field without any supervision such as ground truth registration fields. Specifically, we formulate the complex mapping as a parameterized deformation function, which can be represented and optimized by a deep neural network. Furthermore, the Structure-Driven Regression Network (SDRN) framework is applied to compute the multi-scale similarity combined with contextual structures (e.g., vessel distribution, optic disk appearance, and edge information) to guide the end-to-end learning procedure more effectively with unlabeled data. Given a new pair of images, our method can quickly register images by directly evaluating the parametric function using the learned parameters, which runs faster than traditional registration algorithms. Experimental results, performed on the public challenging dataset (FIRE), show that our method achieves an average Dice similarity coefficient (DSC) of 0.753 with short execution times (0.021 s), which is more accurate and robust than existing approaches and promises to significantly speed up retinal image analysis and processing. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						14	25		10.1016/j.neucom.2020.04.122													
J								Disturbance-observer based consensus of linear multi-agent systems with exogenous disturbance under intermittent communication	NEUROCOMPUTING										Linear multi-agent systems; Consensus; Disturbance-observer; Intermittent control	ADAPTIVE CONSENSUS; SYNCHRONIZATION; CONTAINMENT; NETWORKS; TRACKING; CLUSTER	This paper investigates consensus of linear multi-agent systems (MASs) with exogenous disturbances and intermittent communication. Based on a disturbance-observer method, both distributed state feedback and output feedback intermittent consensus protocols are proposed by using estimated disturbance and local relative information. And the consensus conditions are obtained by using Lyapunov stability theory. Finally, two numerical simulation examples are given to illustrate the proposed theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						26	33		10.1016/j.neucom.2020.04.051													
J								Learning blur invariant binary descriptor for face recognition	NEUROCOMPUTING										Blur robust face recognition; Feature learning; Binary descriptor	PATTERN	Binary representations have demonstrated remarkable performance in face recognition for its robustness to local changes and computation efficiency. However, the performance of face recognition based on most binary descriptors are not satisfactory when dealing with blurred face images. To solve this problem, we propose a novel blur invariant binary descriptor for face recognition. Particularly, we maximize the correlation between the binary codes of sharp face images and blurred face images of positive image pairs for learning the projection matrix. After that, we use the learned projection matrix to obtain blur-robust binary codes by quantizing projected pixel difference vectors (PDVs) in the testing stage. Experiment results on FERET and CMU-PIE show that our method achieves better recognition performance than representative binary descriptors LBP and CBFD. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						34	40		10.1016/j.neucom.2020.04.082													
J								Optimal topology for consensus using genetic algorithm	NEUROCOMPUTING										Distributed control; Optimal topology; Consensus; Two-dimensional Genetic Algorithm; Multi-Agent System (MAS)	HETEROGENEOUS MULTIAGENT SYSTEMS; GLOBAL OPTIMAL CONSENSUS; NETWORKS; AGENTS	In the Multi-Agent Systems (MAS), graph network topologies play a crucial role in building consensus among the connected agents. Consensus may be achieved on many network graphs using distributed control theory. However, the optimal network topology is not addressed in most of the literature, which is an important part of building stable consensus among networked agents. In this paper, the optimal topology is obtained irrespective of the agent dynamics by using two-dimensional Genetic Algorithm (GA), which is a new approach in this context. Simulation result for agents with first, and secondorder linear dynamic is obtained. These results show that the proposed method achieves consensus using the optimal network topology satisfactorily. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						41	49		10.1016/j.neucom.2020.04.107													
J								Lightweight image super-resolution with feature enhancement residual network	NEUROCOMPUTING										Lightweight image super-resolution; Non-locally enhanced module; Structure-aware channel attention	ATTENTION	Recently, single image super-resolution (SR) methods based on deep convolutional neural network (CNN) have demonstrated remarkable progress. The essence of most CNN-based models is to learn the nonlinear mapping between low-resolution patches and corresponding high-resolution ones. However, numerous convolutions are applied to implement this mapping, which directly contributes to large model sizes and huge graphics memory consumption. In this paper, we propose a lightweight feature enhancement residual network (FERN) to achieve prominent performance by incorporating lightweight non-local operations into the residual block. By taking advantage of utilizing this non-locally enhanced residual block, the proposed model can capture long-range dependencies. For further improving performance, we design the structure-aware channel attention layer that explicitly boosts feature maps with more structural and textural details. Extensive experiments suggest that the proposed approach performs favorably against state-of-the-art SR algorithms in terms of visual quality and inference time. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						50	60		10.1016/j.neucom.2020.05.008													
J								A heterogeneous branch and multi-level classification network for person re-identification	NEUROCOMPUTING										Person re-identification; Convolutional neural networks; Feature representation; Heterogeneous branch; Multi-level classification		Convolutional neural networks with multiple branches have recently been proved highly effective in person re-identification (re-ID). Researchers design multi-branch networks using part models, yet they always attribute the effectiveness to multiple parts. In addition, existing multi-branch networks always have isomorphic branches, which lack structural diversity. In order to improve this problem, we propose a novel Heterogeneous Branch and Multi-level Classification Network (HBMCN), which is designed based on the pre-trained ResNet-50 model. A new heterogeneous branch, SE-Res-Branch, is proposed based on the SE-Res module, which consists of the Squeeze-and-Excitation block and the residual block. Furthermore, a new multi-level classification joint objective function is proposed for the supervised learning of HBMCN, whereby multi-level features are extracted from multiple high-level layers and concatenated to represent a person. Based on three public person re-ID benchmarks (Market1501, DukeMTMC-reID and CUHK03), experimental results show that the proposed HBMCN reaches 94.4%, 85.7% and 73.8% in Rank-1, and 85.7%, 74.6% and 69.0% in mAP, achieving a state-of-the-art performance. Further analysis demonstrates that the specially designed heterogeneous branch performs better than an isomorphic branch, and multi-level classification provides more discriminative features compared to single-level classification. As a result, HBMCN provides substantial further improvements in person reID tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						61	69		10.1016/j.neucom.2020.05.007													
J								Multi-source urban data fusion for property value assessment: A case study in Philadelphia	NEUROCOMPUTING										Property value assessment; Data fusion; Deep neural network; Boosted regression trees	HOUSE PRICE; APPRAISAL; VALUATION; DETERMINANTS; MODEL	The property value assessment in the real estate market still remains as a challenges due to incomplete and insufficient information, as well as the lack of efficient algorithms. House attributes, such as size and number of bedrooms, are currently being employed to perform the estimation by professional appraisers and researchers. Numerous algorithms have been proposed; however, a better assessment performance is still expected by the market. Nowadays, there are more available relevant data from various sources in urban areas, which have a potential impact on the house value. In this paper, we propose to fuse urban data, i.e., metadata and imagery data, with house attributes to unveil the market value of the property in Philadelphia. Specifically, two deep neural networks, i.e., metadata fusion network and image appraiser, are proposed to extract the representations, i.e., expected levels, from metadata and street-view images, respectively. A boosted regression tree (BRT) is adapted to estimate the market values of houses with the fused metadata and expected levels. The experimental results with the data collected from the city of Philadelphia demonstrate the effectiveness of the proposed model. The research presented in this paper also provides the real estate industry a new reference to the property value assessment with the data fusion methodology. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						70	83		10.1016/j.neucom.2020.05.013													
J								Adaptive neural consensus tracking control for a class of 2-order multi-agent systems with nonlinear dynamics	NEUROCOMPUTING										Adaptive neural control; Backstepping; Neural networks; Finite-time stability; Nonlinear MASs	FINITE-TIME CONSENSUS; SYNCHRONIZATION; ALGORITHM; PROTOCOL; LEADER	This paper investigates the problem of finite-time consensus tracking control for a class of 2-order non-linear multi-agent systems (MASs). In order to present a consensus control protocol by adaptive neural control approach, a novel fast finite-time stability criterion is first set up, which provides a theoretical basis for applying approximation-based adaptive control approaches to solve the finite-time control issues. Furthermore, an adaptive neural fast finite-time consensus tracking controller is constructed based on the developed finite-time stability criterion. The suggested adaptive neural backstepping control design scheme successfully avoids the problem of singularity of controllers. Under the action of the presented protocol, the output of each follower tracks the reference signal and other signals of the closed-loop system remain bounded in finite time. The efficacy of the proposed control scheme is confirmed by simulation study. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						84	92		10.1016/j.neucom.2020.05.004													
J								An analysis on the use of autoencoders for representation learning: Fundamentals, learning task case studies, explainability and challenges	NEUROCOMPUTING										Representation learning; Autoencoders; Deep learning; Feature extraction	DIMENSIONALITY REDUCTION; COMPONENT ANALYSIS; NEURAL-NETWORKS; MODELS	In many machine learning tasks, learning a good representation of the data can be the key to building a well-performant solution. This is because most learning algorithms operate with the features in order to find models for the data. For instance, classification performance can improve if the data is mapped to a space where classes are easily separated, and regression can be facilitated by finding a manifold of data in the feature space. As a general rule, features are transformed by means of statistical methods such as principal component analysis, or manifold learning techniques such as Isomap or locally linear embedding. From a plethora of representation learning methods, one of the most versatile tools is the autoencoder. In this paper we aim to demonstrate how to influence its learned representations to achieve the desired learning behavior. To this end, we present a series of learning tasks: data embedding for visualization, image denoising, semantic hashing, detection of abnormal behaviors and instance generation. We model them from the representation learning perspective, following the state of the art methodologies in each field. A solution is proposed for each task employing autoencoders as the only learning method. The theoretical developments are put into practice using a selection of datasets for the different problems and implementing each solution, followed by a discussion of the results in each case study and a brief explanation of other six learning applications. We also explore the current challenges and approaches to explainability in the context of autoencoders. All of this helps conclude that, thanks to alterations in their structure as well as their objective function, autoencoders may be the core of a possible solution to many problems which can be modeled as a transformation of the feature space. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						93	107		10.1016/j.neucom.2020.04.057													
J								A novel end-to-end 1D-ResCNN model to remove artifact from EEG signals	NEUROCOMPUTING										Electroencephalogram (EEG); Artifacts removal; Deep learning; End-to-end; One-dimensional residual convolutional neural networks model (1D-ResCNN)	BLIND SOURCE SEPARATION; OCULAR ARTIFACTS; NEURAL-NETWORK; AUTOMATIC CORRECTION; EOG ARTIFACTS; ICA	Electroencephalography (EEG) signals are an important tool in the field of clinical medicine, brain research and the study of neurological diseases. EEG is very susceptible to a variety of physiological signals, which brings great difficulties to the research and analysis of EEG signals. Therefore, removing noise from EEG signals is a key prerequisite for analyzing EEG signals. In this paper, a one-dimensional residual Convolutional Neural Networks (1D-ResCNN) model for raw waveform-based EEG denoising is proposed to solve the above problem. An end-to-end (i.e. waveform in and waveform out) manner is used to map a noisy EEG signal to a clean EEG signal. In the training stage, an objective function is often adopted to optimize the model parameters and in the test stage, the trained 1D-ResCNN model is used as a filter to automatically remove noise from the contaminated EEG signal. The proposed model is evaluated on the EEG signal from the CHB-MIT Scalp EEG Database, and the added noise signals are obtained from the database. We compared the proposed model with the independent of the composite analysis (ICA), the fast independent composite analysis (FICA),Recursive least squares(RLS) filter,Wavelet Transform (WT) and Deep neural network(DNN) models. Experimental Results show that the proposed model can yield cleaner waveforms and achieve significant improvement in SNR and RMSE.Meanwhile, the proposed model can also preserve the nonlinear characteristics of EEG signals. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				SEP 3	2020	404						108	121		10.1016/j.neucom.2020.04.029													
J								Generating electrocardiogram signals by deep learning	NEUROCOMPUTING										Deep learning; Evaluation approach; Generative model; Synthetic ECG; Short-term Fourier transform; Stationary wavelet transform; mu-Law companding transformation	ATRIAL-FIBRILLATION; QUANTITATIVE-ANALYSIS; MODEL	Given the importance of a diverse and vast amount of realistic and labeled electrocardiogram (ECG) signals in improving the performance of biomedical signal processing algorithms, and the situation of severe lack of the signals, three generative models based on deep learning are introduced for the generation of ECG signals: The WaveNet-based model, the SpectroGAN model, and the WaveletGAN model. The WaveNet-based model adopts mu-law companding transformation as a preprocessing method and then is followed by a sequence of convolutional layers with dilation; SpectroGAN and WaveletGAN use short-term Fourier transform (STFT) and stationary wavelet transform (SWT) respectively to obtain suitable input form for the generative adversarial networks (GAN). Our proposed models are capable of generating ECG signals containing three different heartbeat types: normal beat, left bundle branch block beat and right bundle branch block beat. The synthetic ECG signals generated by our models are more realistic since deep artificial neural networks can discover intricate structure and characteristics of real ECG signals instead of manually setting specific parameters for synthesis. Besides, ECG signals produced by one of our proposed models could be naturally continuous and be up to more than 20 seconds. Furthermore, we first provide an evaluation approach for quantitatively demonstrating the performance of ECG generative models. The study demonstrates that deep learning is a feasible and effective method for ECG generation. Our proposed ECG generative models can be utilized to assess biomedical signal processing algorithms so as to improve their performance in clinical trials. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						122	136		10.1016/j.neucom.2020.04.076													
J								Online optimal consensus control of unknown linear multi-agent systems via time-based adaptive dynamic programming	NEUROCOMPUTING										Linear multi-agent systems (MASs); Adaptive dynamic programming (ADP); Consensus control; Discrete-time (DT) system	NEURAL-NETWORK; SYNCHRONIZATION; ALGORITHMS; DESIGN; GAMES	This paper considers the online optimal consensus control problem for unknown linear discrete-time (DT) multi-agent systems (MASs). Based on time-based adaptive dynamic programming (ADP) method, the control policies are designed by utilizing the current and recorded data of unknown MASs. The critic-actor NN frameworks are employed to approximate the performance indexes and optimal control policies, respectively. The NN weights are updated once at the sampling instant to produce real-time online control. Furthermore, the control policies are proved to effectively drive the MASs to achieve consistency and satisfy the Nash equilibrium. Finally, a numerical example is implemented to shown the feasibility of the control scheme. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						137	144		10.1016/j.neucom.2020.04.119													
J								Adaptive fuzzy discrete-time fault-tolerant control for permanent magnet synchronous motors based on dynamic surface technology	NEUROCOMPUTING										Adaptive fuzzy control; Dynamic surface control; Permanent magnet synchronous motors; Discrete-time; Fault-tolerant control	STOCHASTIC NONLINEAR-SYSTEMS; POSITION TRACKING CONTROL; INDUCTION-MOTORS; VEHICLE; DESIGN; DELAY	The discrete-time fault-tolerant control method for permanent magnet synchronous motors (PMSMs) is investigated for the first time in this paper based on adaptive fuzzy theory, where the designed controllers consider faults containing both loss of effectiveness and bias. Firstly, with the help of Euler method, the discrete-time model of PMSMs is obtained. Secondly, adaptive fuzzy theory is used to realize fault-tolerant control of PMSMs. Next, dynamic surface control is used to resolve the problems of "explosion of complexity" and noncausal in discrete-time systems caused by backstepping. It is proved that the proposed method can guarantee all signals and states in the closed-loop system are semi-global uniform ultimate bounded when the faults occur and the position tracking error can converge to a small neighborhood of the origin. Finally, simulation results show that the proposed control method has strong fault-tolerant performance and robustness. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						145	153		10.1016/j.neucom.2020.04.009													
J								Human scanpath prediction based on deep convolutional saccadic model	NEUROCOMPUTING										Scanpath prediction; Inhibition of return; Deep learning; Visual attention; Visual search	VISUAL-ATTENTION; EYE-MOVEMENTS; SALIENCY; FACE	Human scanpath represents the sequence of human eye fixations, revealing the dynamic process of saccadic eye movement when a natural scene is freely viewed by humans. It is valuable to have an in-depth understanding of the dynamic visual attention and visual search behavior. In this paper, a deep convolutional saccadic model (DCSM) is proposed to predict human scanpath. The model simultaneously predicts the foveal saliency maps and fixation durations with considering on modeling the inhibition of return, which is a well recognized physiological mechanism to mimic human saccadic behavior. Both the foveal saliency and fixation durations are predicted by convolutional neural networks, which associate the inhibition of return with image content from spatial and temporal aspects. With the proposed DCSM, fixations of a scanpath are sequentially predicted with only a single image as input. Our method is capable of handling the challenges of temporal dependency and spatial association with image content. Experimental results on MIT1003 and FIGRIM datasets demonstrate the effectiveness of our proposed method when compared with state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						154	164		10.1016/j.neucom.2020.03.060													
J								Visually grounded paraphrase identification via gating and phrase localization	NEUROCOMPUTING										Visual grounded paraphrases; Gating; Phrase localization; Vision and language		Visually grounded paraphrases (VGPs) describe the same visual concept but in different wording. Previous studies have developed models to identify VGPs from language and visual features. In these existing methods, language and visual features are simply fused. However, our detailed analysis indicates that VGPs with different lexical similarities require different weights on language and visual features to maximize identification performance. This motivates us to propose a gated neural network model to adaptively control the weights. In addition, because VGP identification is closely related to phrase localization, we also propose a way to explicitly incorporate phrase-object correspondences. From our evaluation in detail, we confirmed our model outperforms the state-of-the-art model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						165	172		10.1016/j.neucom.2020.04.066													
J								Learning crowd behavior from real data: A residual network method for crowd simulation	NEUROCOMPUTING										Crowd evacuation; Cohesiveness; Collectiveness; Residual network; Scene-independent	DRIVEN; EVACUATION; MODEL; REGRESSION	Traditional methods of crowd evacuation simulation reduce the visual realism of crowd motion modeling due to hypothetical scenarios and rules. Data-driven methods are an effective way to enhance the visual realism of crowd simulation. However, the existing work mainly focuses on training models for specific scenarios and applies them to the same scenario, so it lacks consideration of scene adaptability. To address this problem, we present a residual network based scene-independent crowd simulation (ResNet-SICS) framework to simulate crowd motion. First, a data-driven crowd properties quantization (DCPQ) method is proposed. This method divides crowd properties into physical properties and social properties: the physical properties (such as the position and the velocity) are extracted from a large number of real videos, and the social properties (such as local cohesiveness and global collectiveness) are then quantified according to the physical properties of crowds. Second, a residual network for crowd behavior properties learning (ResNet-CBPL) motion prediction model is established. This model takes the crowd properties as parameters to construct the residual network, and uses real data to learn the movement rules of the crowd. The ResNet-CBPL motion prediction model can fit the movement behavior of the crowd more accurately than can other models. Finally, we implement a crowd simulation system based on ResNet-CBPL to visualize the results of theoretical analysis in a graphical way. The experimental results show that the proposed method can simulate the crowd motion more realistically, and the trained crowd simulation framework can be applied to various scenarios. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				SEP 3	2020	404						173	185		10.1016/j.neucom.2020.04.141													
J								Hierarchical structure correlation inference for pose estimation	NEUROCOMPUTING											PICTORIAL STRUCTURES																		0925-2312	1872-8286				SEP 3	2020	404						186	197		10.1016/j.neucom.2020.04.108													
J								Deep multi -view residual attention network for crowd flows prediction	NEUROCOMPUTING											MODEL; LSTM																		0925-2312	1872-8286				SEP 3	2020	404						198	212		10.1016/j.neucom.2020.04.124													
J								EyesGAN: Synthesize human face from human eyes	NEUROCOMPUTING											RECOGNITION																		0925-2312	1872-8286				SEP 3	2020	404						213	226		10.1016/j.neucom.2020.04.121													
J								SCLNet: Spatial context learning network for congested crowd counting	NEUROCOMPUTING											CONVOLUTIONAL NEURAL-NETWORK																		0925-2312	1872-8286				SEP 3	2020	404						227	239		10.1016/j.neucom.2020.04.139													
J								Security control of multi -agent systems under false data injection attacks	NEUROCOMPUTING											CYBER-PHYSICAL SYSTEMS; CONSENSUS; SYNCHRONIZATION; SUBJECT																		0925-2312	1872-8286				SEP 3	2020	404						240	246		10.1016/j.neucom.2020.04.109													
J								Network pruning using sparse learning and genetic algorithm	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 3	2020	404						247	256		10.1016/j.neucom.2020.03.082													
J								Structured feature for multi -label learning	NEUROCOMPUTING											CLASSIFICATION																		0925-2312	1872-8286				SEP 3	2020	404						257	266		10.1016/j.neucom.2020.04.134													
J								Group consensus of multi -agent networks with hybrid interactions	NEUROCOMPUTING											LEADER-FOLLOWING CONSENSUS; BIPARTITE CONSENSUS; SYSTEMS																		0925-2312	1872-8286				SEP 3	2020	404						267	275		10.1016/j.neucom.2020.04.112													
J								Asymptotic stability and synchronization for nonlinear distributed -order system with uncertain parameters	NEUROCOMPUTING											NETWORKS; SUBJECT																		0925-2312	1872-8286				SEP 3	2020	404						276	282		10.1016/j.neucom.2020.05.011													
J								SCNet: Scale -aware coupling -structure network for efficient video object detection	NEUROCOMPUTING											SEGMENTATION																		0925-2312	1872-8286				SEP 3	2020	404						283	293		10.1016/j.neucom.2020.03.110													
J								Event -triggered constrained robust control for partly -unknown nonlinear systems via ADP	NEUROCOMPUTING											APPROXIMATION; ITERATION																		0925-2312	1872-8286				SEP 3	2020	404						294	303		10.1016/j.neucom.2020.05.012													
J								Learning with privileged information for photo aesthetic assessment	NEUROCOMPUTING																													0925-2312	1872-8286				SEP 3	2020	404						304	316		10.1016/j.neucom.2020.04.142													
J								Exponential synchronization of complex -valued memristor-based delayed neural networks via quantized intermittent control	NEUROCOMPUTING											TIME-VARYING DELAYS; STABILITY ANALYSIS; ROBUST STABILITY; STABILIZATION; DISSIPATIVITY																		0925-2312	1872-8286				SEP 3	2020	404						317	328		10.1016/j.neucom.2020.04.097													
J								A trajectory planning method for robot scanning system uuuusing mask R -CNN for scanning objects with unknown model	NEUROCOMPUTING											SEGMENTATION																		0925-2312	1872-8286				SEP 3	2020	404						329	339		10.1016/j.neucom.2020.04.059													
J								Deep subspace clustering to achieve jointly latent feature extraction and discriminative learning	NEUROCOMPUTING											ALGORITHM																		0925-2312	1872-8286				SEP 3	2020	404						340	350		10.1016/j.neucom.2020.04.120													
J								AdaBoost-CNN: An adaptive boosting algorithm for convolutional neural networks to classify multi -class imbalanced datasets using transfer learning	NEUROCOMPUTING											ENSEMBLE																		0925-2312	1872-8286				SEP 3	2020	404						351	366		10.1016/j.neucom.2020.03.064													
J								Synchronization of inertial memristive neural networks with time -varying delays via static or dynamic event -triggered control	NEUROCOMPUTING											EXPONENTIAL SYNCHRONIZATION; FINITE-TIME; STABILITY; SYSTEM; MULTISYNCHRONIZATION; COMMUNICATION; STABILIZATION																		0925-2312	1872-8286				SEP 3	2020	404						367	380		10.1016/j.neucom.2020.04.099													
J								Lifelong generative modeling	NEUROCOMPUTING											REPLAY																		0925-2312	1872-8286				SEP 3	2020	404						381	400		10.1016/j.neucom.2020.02.115													
J								Evolutionary computation for bottom-up hypothesis generation on emotion and communication	CONNECTION SCIENCE										Emotion; affective communication; evolutionary computation; genetic algorithms; neural oscillations	MODEL	Through evolutionary computation, affective models may emerge autonomously in unanticipated ways. We explored whether core affect would be leveraged through communication with conspecifics (e.g. signalling danger or foraging opportunities). Genetic algorithms served to evolve recurrent neural networks controlling virtual agents in an environment with fitness-increasing food and fitness-reducing predators. Previously, neural oscillations emerged serendipitously, with higher frequencies for positive than negative stimuli, which we replicated here in the fittest agent. The setup was extended so that oscillations could be exapted for the communication between two agents. An adaptive communicative function evolved, as shown by fitness benefits relative to (1) a non-communicative reference simulation and (2) lesioning of the connections used for communication. An exaptation of neural oscillations for communication was not observed but a simpler type of communication developed than was initially expected. The agents approached each other in a periodic fashion and slightly modified these movements to approach food or avoid predators. The coupled agents, though controlled by separate networks, appeared to self-assemble into a single vibrating organism. The simulations (a) strengthen an account of core affect as an oscillatory modulation of neural-network competition, and (b) encourage further work on the exaptation of core affect for communicative purposes.																	0954-0091	1360-0494															10.1080/09540091.2020.1814203		SEP 2020											
J								Multivariate calibration: Identification of phenolic compounds in PROPOLIS using FT-NIR	JOURNAL OF CHEMOMETRICS										antioxidants; Chemometrics; iPLS; phenolic compounds; second derivative	NEAR-INFRARED SPECTROSCOPY; ANTIOXIDANT ACTIVITY; ANTIBACTERIAL ACTIVITY; EXTRACTS; IPLS; CONSTITUENTS; BACTERIA; TOOL	The chemical composition of propolis is complex and varies according to location, seasonality, and vegetation where the hives are located. Raw and macerated propolis produced in different regions of Brazil were assessed and produced iPLS models from FTNIR combined to multivariate calibration to determine total phenolic content (TPC). The spectra of raw propolis were obtained analyzing samples directly, without treatment or processing, while macerated propolis was submitted to the milling process using liquid nitrogen. Concerning the results, the TPC ranged from 3.61 to 90.4 mg GAE g(-1), which indicates a significant difference among samples. It was possible to conclude that macerated propolis presented the best multivariate calibration obtained from the second derivative and SNV pre-processing. The error values RMSECV and RMSEP were 3.79 and 6.17 mg GAE g(-1)respectively, while the determination coefficients in the calibration, cross-validation, and external validation were 0.97, 0.87, and 0.82, respectively.																	0886-9383	1099-128X														e3296	10.1002/cem.3296		SEP 2020											
J								What is beautiful is not always good: influence of machine learning-derived photo attractiveness on intention to initiate social interactions in mobile dating applications	CONNECTION SCIENCE										Photo attractiveness; social interaction; mobile dating application; machine learning	CONSTRUAL-LEVEL THEORY; PHYSICAL ATTRACTIVENESS; MATE PREFERENCES; COMPLEMENTARY NEEDS; SEX-DIFFERENCES; GENDER; INTELLIGENCE; SELECTION; CHOICES; US	The popularity of mobile dating applications has reconstructed how people initiate new romantic relationships. Photo attractiveness, the most prominent information provided in the online dating context before social interactions, has attracted considerable attention but reached inconsistent conclusions. By considering the location, gender, and attractiveness difference between each dyad of participants (hunter and target) together, this study attempts to re-examine the contingent impact of physical attractiveness in initiating social interactions. Multi-methods (machine-learning and panel logistic regression) were used to empirically analyse the large-scale field data. The results show that: (1) A target's photo attractiveness can promote a hunter's willingness to leave a message; (2) The impact of photo attractiveness will be attenuated when two users have a larger location difference; (3) Male users are more likely to be impacted by photo attractiveness in their social interactions than female users; (4) A hunter with low attractiveness is inclined to initiate social interactions with a more attractive target; whereas a hunter with high attractiveness does not. This study deepens the theoretical implications in relevant fields, provides new methodology insight, and offers personalised marketing strategies for mobile dating applications.																	0954-0091	1360-0494															10.1080/09540091.2020.1814204		SEP 2020											
J								IFODPSO-based multi-level image segmentation scheme aided with Masi entropy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Color image; Particle Swarm Optimization; Masi entropy; Segmentation	PARTICLE SWARM OPTIMIZATION; MINIMUM CROSS-ENTROPY; TSALLIS ENTROPY; PSO ALGORITHM; COLOR; EVOLUTIONARY; SELECTION	This article presents an improved version of Fractional Order Darwinian PSO (IFODPSO) for segmenting 3D histogram-based color images at multiple levels of Berkley Segmentation Dataset (BSDS500). The success of convergence and accuracy rate of FODPSO algorithm depends on the value of fractional coefficient. This concept may provide drawback to the algorithm specially for multilevel problems of large dataset. So, to overcome the full dependency on fractional coefficient, delta potential model of quantum mechanics has been incorporated with FODPSO for updating the particle's present as well as global position by destroying the worst particles (solutions), formulated using the introduction of the context parameter. Multi-level Massi Entropy (MME), of current interest, has been chosen here as the objective function for finding the threshold values in combination with IFODPSO. Further, the small segmented regions have been removed or merged into bigger regions for showing the better discrimination between different segmented objects. The effectiveness of the proposed MME-IFODPSO algorithm has been extensively investigated in terms of statistically and qualitatively in terms of the fidelity parameters with the state-of-art approaches and it has been found that the proposed method has improved at least 2-5% to the conventional methods in terms of accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02506-w		SEP 2020											
J								Using product data management technology to semiconductor manufacturing industry	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Semiconductor manufacturing industry; Product data Management (PDM); Integrated circuit (IC); Electronic design automation (EDA); Circuit design procedures	X-ARCHITECTURE; SUPPLY CHAIN; ALGORITHM	Many enterprises cannot cost down and quality improve. So, speeding up production time is a new competition ability. Most Product Data Management (PDM) can automate to input files and data through known procedures. But some technology cannot satisfy the requirement for production information in precision, clearness and customization. This paper proposed a technique using application product data management on Integrated Circuit (IC) Design. Using internet, network and circuit simulation for solution problems, resulting from R&D process and the compatibility of customer required products. Using the proposed model in PDM to reduce the correction rate after the IC design and production in order to satisfy customers' demands. The results of this paper are the participating teams of strategic alliance can develop IC product to integrate enterprise subject to the related product quality. Support correct information of product for customer demands to develop relative products. Through the function of IC circuit simulation. The proposed model can verify customers' product instantly to reduce time and cost, and to increase the design efficiency and quality. The customer can use ICs products from building up IC courses. Using this technology can forecast customers' demands, development business trend and strengthen communicate with customers to build customer relationship, put IC products into the commercial market more quickly and enhance customer competition ability. Therefore, for ICs designer can get better product quality, shorten product R&D time and strengthen the competition ability.																	1868-5137	1868-5145															10.1007/s12652-020-02504-y		SEP 2020											
J								High-flexible hardware and instruction of composite Galois field multiplication targeted at symmetric crypto processor	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Composite Galois field multiplication; Reconfigurable hardware; Flexible structure; Dedicated instruction; Symmetric crypto processor	DESIGN	Composite Galois field multiplication is one of the most important and complex nonlinear arithmetic unit in symmetric cipher algorithms. However, current hardware implementations are hard to maintain high performance and flexibility. Based on reconfigurable technology, we propose a flexible architecture of composite Galois field multiplication (RCGFM) and dedicated instructions of composite Galois filed multiplication (ICGFM) over GF((2(n))(m)), where n = 8, m = 1, 2, 3, 4. The RCGFM adopts a serial-parallel mixed structure, which can achieve different Galois field multiplications with good parallelism and scalability. By extending the x(k)B multiplications of serial chain, where k = 1, 2, 3, the RCGFM can concurrently support the composite Galois filed multiplications with higher orders, such as GF((2(8))(m)), where m >= 5, m is an element of Z(+). Moreover, in order to reduce the instruction overhead of target symmetric crypto processor, the ICGFM is specially designed, which is composed of operation and configuration instructions for x(k)B and A x B over GF((2(n))(m)). The ICGFM can be applied to RCGFM structure efficiently and flexibly by configuring the corresponding parameters. The experimental results show that under 0.18 mu m CMOS technology, the maximum clock frequency is 625 MHz, while the area of circuit is 11.2 kilo gates. Compared with current researches, the RCGFM structure can improve the throughput rate more than a factor of 1.36x-9.19x, when normalized to the same technology and per kilo gates, the technology-scaled throughput rate increases more than a factor of 1.25x-4.4x, while the area overhead does not increase significantly. In addition, the ICGFM can reduce 1-2 orders of magnitude the number of instructions compared with other works. At last, the reconfigurable architecture we proposed supports different composite Galois field multiplications over GF((2(n))(m)) with more flexibility and efficiency.																	1868-5137	1868-5145															10.1007/s12652-020-02497-8		SEP 2020											
J								A neural network ensemble method for effective crack segmentation using fully convolutional networks and multi-scale structured forests	MACHINE VISION AND APPLICATIONS										Crack image segmentation; Fully convolutional network; Multi-scale structured forests; Anti-symmetrical bi-orthogonal wavelet	MACHINE VISION SYSTEM; DEEP; RECOGNITION; INSPECTION	Crack image segmentation has recently become a major research topic in nondestructive inspection. However, the image segmentation methods are not robust to variations such as illumination, weather, noise and the segmentation accuracy which cannot meet the requirements of practical applications. Therefore, a neural network ensemble method is proposed for effective crack segmentation in this paper, which consists of fully convolution networks (FCN) and multi-scale structured forests for edge detection (SFD). In order to improve the accuracy of crack segmentation and reduce the error mark under complex background, a new network model based on FCN model is proposed to address the problems that lose local information and the capacity of partial refinement, which are frequently encountered in FCN model in the crack segmentation. In addition, SFD is combined with the half-reconstruction method of anti-symmetrical bi-orthogonal wavelet to overcome the limitation of crack edge detection. Finally, the result of the two maps is merged after resizing to the original image dimensions. Qualitative and quantitative evaluations of the proposed methods are performed, showing that they can obtain better results than certain existing methods for crack segmentation.																	0932-8092	1432-1769				SEP 2	2020	31	7-8							60	10.1007/s00138-020-01114-0													
J								Authente-Kente: enabling authentication for artisanal economies with deep learning	AI & SOCIETY										Human-machine collaboration; Machine learning; Artisanal economy; Generative justice; Industrial symbiosis; Ethnocomputing		The economy for artisanal products, such as Navajo rugs or Pashmina shawls are often threatened by mass-produced fakes. We propose the use of AI-based authentication as one part of a larger system that would replace extractive economies with generative circulation. In this case study we examine initial experiments towards the development of a cell phone-based authentication app for kente cloth in West Africa. We describe the context of weavers and cloth sales; an initial test of amachine learningalgorithm for distinguishing between real and fake kente, and an outline of the next stages of development.																	0951-5666	1435-5655															10.1007/s00146-020-01055-2		SEP 2020											
J								Failure of chatbot Tay was evil, ugliness and uselessness in its nature or do we judge it through cognitive shortcuts and biases?	AI & SOCIETY										Tay; Chatbot; Artificial intelligence; Cognitive distortion		This study deals with the failure of one of the most advanced chatbots called Tay, created by Microsoft. Many users, commentators and experts strongly anthropomorphised this chatbot in their assessment of the case around Tay. This view is so widespread that we can identify it as a certain typical cognitive distortion or bias. This study presents a summary of facts concerning the Tay case, collaborative perspectives from eminent experts: (1) Tay did not mean anything by its morally objectionable statements because, in principle, it was not able to think; (2) the controversial content spread by this AI was interpreted incorrectly-not as a mere compilation of meaning (parroting), but as its disclosure; (3) even though chatbots are not members of the symbolic order of spatiotemporal relations of the human world, we treat them in this way in many aspects.																	0951-5666	1435-5655															10.1007/s00146-020-01053-4		SEP 2020											
J								Expanding hermeneutics to the world of technology	AI & SOCIETY										Hermeneutics; Being-in-the-world; Ready-to-hand; Technology; Technoscience explanation; Understanding	SCIENCE	In this essay, I first analyze the extension of hermeneutical interpretation in the Heideggerian sense to products of contemporary technology which are components of our "lifeworld". Products of technology, such as airplanes, laptops, cellular phones, washing machines, or vacuum cleaners might be compared with what Heidegger calls the "Ready-to-hand" (das Zuhandene) with regard to utilitarian objects such as a hammer, planer, needle and door handle inBeing and Time. Our life with our equipment, which represents the "Ready-to-hand" in Heidegger's sense of the word, is determined by temporalization (Zeitigung) which cannot be separated and isolated from the wholeness of things in the world. In the second part of my paper, I explore the positive achievement of material hermeneutics (Don Ihde) with regard to its extension to technoscience and the discussion of how such hermeneutics can contribute to the preservation of our threatened lifeworld, but also to explore the possibilities of how technical inventions, medical innovations could improve our way of life.																	0951-5666	1435-5655															10.1007/s00146-020-01052-5		SEP 2020											
J								Design of fault diagnosis algorithm for electric fan based on LSSVM and Kd-Tree	APPLIED INTELLIGENCE										Least Square Support Vector Machine; Kd-Tree; Data preprocessing; Fuzzy C-Means clustering algorithm; Model optimization	ROTATING MACHINERY	Currently, the complexity of mechanical equipment is increasing rapidly together with the poor working environment. If a fault occurs, how to find the fault in time becomes a poser. Motivated by this existing problem, based on the analysis of the fault characteristics of electric fans, a fault diagnosis algorithm model based on Least Square Support Vector Machine (LSSVM) and Kd-Tree was proposed. This algorithm was based on the LSSVM optimized by the Cuckoo Search (CS). This paper used the "one-to-many" principle and the sigma threshold method to introduce k-Nearest Neighbor (kNN) which was implemented by Kd-Tree as a secondary classifier to optimize the model. In data preprocessing, the data based on time series was first processed by Empirical Mode Decomposition (EMD) and the energy ratios were calculated, and the the above results were degraded by Principal Component Analysis (PCA) and normalized. On top of that, in case of the uncertain fault types, the Fuzzy C-Means clustering algorithm (FCM) optimized by Particle Swarm Optimization (PSO) was proposed to provide a priori knowledge for the model. In this paper, the algorithm model, FCM and other parts were verified to prove that the performance and generality of the algorithm were better than those of general classification algorithms, and relevant experiments were conducted for different data processing methods to expand the universality of the algorithm.																	0924-669X	1573-7497															10.1007/s10489-020-01830-0		SEP 2020											
J								Discriminative feature extraction for video person re-identification via multi-task network	APPLIED INTELLIGENCE										Attribute; Center loss; Feature representation; Person re-identification; Video	ATTRIBUTE; DEEP	The goal of video-based person re-identification is to match different pedestrians in various image sequences across non-overlapping cameras. A critical issue of this task is how to exploit the useful information provided by videos. To solve this problem, we propose a novel feature learning framework for video-based person re-identification. The proposed method aims at capturing the most significant information in the spatial and temporal domains and then building a discriminative and robust feature representation for each sequence. More specifically, to learn more effective frame-wise features, we apply several attributes to the video-based task and build a multi-task network for the identity and attribute classifications. In the training phase, we present a multi-loss function to minimize intra-class variances and maximize inter-class differences. After that, the feature aggregation network is employed to aggregate frame-wise features and extract the temporal information from the video. Furthermore, considering that adjacent frames typically have similar appearance features, we propose the concept of "non-redundant appearance feature extraction" to obtain the sequence-level appearance descriptors of pedestrians. Based on the complementarity between the temporal feature and the non-redundant appearance feature, we combine them in the distance learning phase by assigning them different distance-weighted coefficients. Extensive experiments are conducted on three video-based datasets and the results demonstrate the superiority and effectiveness of our method.																	0924-669X	1573-7497															10.1007/s10489-020-01844-8		SEP 2020											
J								A hybrid model for financialtime-seriesforecasting based on mixed methodologies	EXPERT SYSTEMS										ARIMA; EEMD; financial time series; forecasting; Taylor expansion	NEURAL-NETWORKS; VECTOR MACHINES; SERIES; DECOMPOSITION; NONSTATIONARY; ARIMA	This paper proposes a hybrid model that combines ensemble empirical mode decomposition (EEMD), autoregressive integrated moving average (ARIMA), and Taylor expansion using a tracking differentiator to forecast financial time series. Specifically, the financial time series is decomposed by EEMD into some subseries. Then, the linear portion of each subseries is forecasted by the linear ARIMA model, while the nonlinear portion is predicted by the nonlinear Taylor expansion model. The forecasting results of the linear and nonlinear models are combined into the predicted result of each subseries. The final prediction result is obtained by combining the prediction values of all the subseries. The empirical results with real financial time series data demonstrate that this new hybrid approach outperforms the benchmark hybrid models considered in this paper.																	0266-4720	1468-0394														e12633	10.1111/exsy.12633		SEP 2020											
J								Mechanisation of the AKS Algorithm	JOURNAL OF AUTOMATED REASONING										Theorem-proving; Automated reasoning; AKS algorithm; Number theory; Finite fields computational complexity; Writer monad; Machine model	FERMATS	The AKS algorithm (by Agrawal, Kayal and Saxena) is a significant theoretical result, establishing "PRIMES in P" by a brilliant application of ideas from finite fields. This paper describes an implementation of the AKS algorithm in our theorem prover HOL4, together with a proof of its correctness and its computational complexity. The complexity analysis is based on a conservative computation model using a writer monad. The method is elementary, but enough to show that our implementation of the AKS algorithm takes a number of execution steps bounded by a polynomial function of the input size. This establishes formally that the AKS algorithm indeed shows "PRIMES is in P".																	0168-7433	1573-0670															10.1007/s10817-020-09563-y		SEP 2020											
J								A high-speed feature matching method of high-resolution aerial images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image matching; Corner detection; Scale estimation; Parallel computing; CPU-FPGA cooperative processing	STEREO; REGISTRATION	This paper presents a novel corner detection and scale estimation algorithm for image feature description and matching. Inspired by Adaboost's weak classifier, a series of sub-detectors is elaborately designed to obtain reliable corner pixels. The new corner detection algorithm is more robust than the FAST and HARRIS algorithm, and it is especially suitable for the implementation in FPGA. The new scale estimation method can be directly implemented in the original image without building Gaussian pyramid and searching max response value in each level, which not only increase computational efficiency but also greatly reduces memory requirement. Based on the proposed algorithm, a CPU-FPGA cooperative parallel processing architecture is presented. The architecture overcomes the memory space limitation of FPGA and achieves high-speed feature matching for massive high-resolution aerial images. The speed of the CPU-FPGA cooperative process is hundred times faster than SIFT algorithm running on CPU, and dozens of times faster than SIFT running in CPU + GPU system.																	1861-8200	1861-8219															10.1007/s11554-020-01012-8		SEP 2020											
J								Adaptive virtual-inertia control and chicken swarm optimizer for frequency stability in power-grids penetrated by renewable energy sources	NEURAL COMPUTING & APPLICATIONS										Frequency control; Adaptive virtual-inertia control; Optimization algorithms; Renewable energy sources	DIFFERENTIAL EVOLUTION; FUZZY-LOGIC; SYSTEMS; ALGORITHM	In this article, a control scheme based on chicken swarm optimizer (CSO) in cooperation with adaptive virtual-inertia control (AVIC) is investigated. The proposed control scheme aims at improving the frequency stability of an interconnected power system which is penetrated by renewable energy sources. The CSO is applied to produce the best values of the gains of the adapted standard proportional-integral-derivative (PID) controllers and required parameters of AVICs. Various scenarios are addressed in this study such as applications of sudden step load disturbances and severe variations in the inertia of the system. In addition, realistic conditions such as uncertainties of tidal power source and random load disturbances are demonstrated. Compulsory assessments with subsequent discussions to evaluate the results of the CSO are made. The proposed CSO-AVIC based control method is verified by comparisons with well-matured interesting algorithms such as differential evolution and particle swarm optimizers. Various quality specifications of the dynamic responses and the demonstrated results indicate clearly the viability of the proposed CSO-AVIC based on control scheme. It can be emphasized that the utilization of AVIC along with PID controllers are significantly improved the system dynamic performances and their dynamic response specifications meet the terms of standard acceptable criteria's.																	0941-0643	1433-3058															10.1007/s00521-020-05054-8		SEP 2020											
J								Multi-cue based 3D residual network for action recognition	NEURAL COMPUTING & APPLICATIONS										Action recognition; Multi-cue; 3D convolution; Salient motion cue; Residual	HASHING-BASED APPROACH; SERVICE RECOMMENDATION; ATTENTION	Convolutional neural network (CNN) is a natural structure for video modelling that has been successfully applied in the field of action recognition. The existing 3D CNN-based action recognition methods mainly perform 3D convolutions on individual cues (e.g. appearance and motion cues) and rely on the design of subsequent networks to fuse these cues together. In this paper, we propose a novel multi-cue 3D convolutional neural network (M3D), which integrates three individual cues (i.e. an appearance cue, a direct motion cue, and a salient motion cue) directly. Different from the existing methods, the proposed M3D model directly performs 3D convolutions on multiple cues instead of a single cue. Compared with the previous methods, this model can obtain more discriminative and robust features by integrating three different cues as a whole. Further, we propose a novel residual multi-cue 3D convolution model (R-M3D) to improve the representation ability to obtain representative video features. Experimental results verify the effectiveness of proposed M3D model, and the proposed R-M3D model (pre-trained on the Kinetics dataset) achieves competitive performance compared with the state-of-the-art models on UCF101 and HMDB51 datasets.																	0941-0643	1433-3058															10.1007/s00521-020-05313-8		SEP 2020											
J								Conditional Generative Adversarial Networks with Multi-scale Discriminators for Prostate MRI Segmentation	NEURAL PROCESSING LETTERS										Magnetic resonance images (MRI); Generative adversarial networks; Generator; Feature matching loss	ATLAS-BASED SEGMENTATION; LABEL FUSION; LEVEL SET	Accurate prostate MR image segmentation is a necessary preprocessing stage for computer-assisted diagnostic algorithms. Convolutional neural network, as a research focus in recent years, has been proven to be powerful in computer vision field. Recently, the most effective prostate MRI segmentation technology mainly relies on full convolutional network which has been widely used in semantic segmentation task. However, it's independent and identically distributed assumption neglect the structural regularity present in MR images and miss information between pixels. In this paper, we propose an MRI-conditional generative adversarial networks for prostate segmentation. Our adversarial training make it context aware and the use of adversarial loss functions learn high-level structural information. The network consist of a generator and a discriminator. The generator consists of a contraction channel and an expansion channel like U-Net. The method we proposed uses a multi-scale discriminator which consist of two discriminators with the same structure but different input sizes. The objective function has two parts: one is the adversarial loss, the other is feature matching loss which stabilizes the training and get better convergence. The experiment show that our network can accurately segment the prostate MRI and outperforms most existing methods.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1251	1261		10.1007/s11063-020-10303-x		SEP 2020											
J								An improved multi-sensor D-S rule for conflict reassignment of failure rate of set	SOFT COMPUTING										Information fusion; Priori probability; Confidence interval; Multi-sensor fusion; D-S evidence theory	INFORMATION FUSION; NETWORKS; MODELS	In order to realize high-precision sensor sensing system for unmanned vehicle, a method based on D-S (Dempster-Shafer) evidence theory was presented. D-S evidence theory is a method of evidence processing. However, because of the limitation of multiplication rule, it cannot deal with the evidence of high conflict caused by the failure of a sensor in the system. In the existing D-S theory framework, the prior probability is used to measure the sensor's failure rate. The correlation matrix is obtained by the mass distribution of evidence, and the interval classification is carried out accordingly. Bayes formula is used to adjust the prior probability of the sensor by synthesizing the risk distribution function of different intervals, and the calculation rule of correction report using convolution rule is proposed. Compared to existing methods, it can reduce the conflict degree and information entropy of the system, so that a reasonable decision can be made.																	1432-7643	1433-7479				OCT	2020	24	20					15179	15188		10.1007/s00500-020-05298-5		SEP 2020											
J								Chaotic lightning search algorithm	SOFT COMPUTING										Metaheuristic; Lightning search algorithm; Chaos; Chaotic maps	HARMONY SEARCH; KRILL HERD; OPTIMIZATION	Metaheuristics have proven their efficiency in treating complex optimization problems. Generally, they produce good results quite close to optimal despite some weaknesses such as premature convergence and stagnation in the local optima. However, some techniques are used to improve the obtained results, one of them is the adoption of chaos theory. Including chaotic sequences in metaheuristics has proven its efficiency in previous studies by improving the performance and quality of the results obtained. In this study, we propose an improvement of the metaheuristic lightning search algorithm (LSA) by using chaos theory. In fact, the idea is to replace the values of random variables with a chaotic sequences generator. To prove the success of the metaheuristic-chaos theory association, we tested five chaotic version of lightning search algorithm on a benchmark of seven functions. Experimental results show that sine or singer map are the best choices to improve the efficiency of LSA, in particular with the lead projectile update.																	1432-7643	1433-7479															10.1007/s00500-020-05273-0		SEP 2020											
J								Cesaro summability of sequences in intuitionistic fuzzy normed spaces and related Tauberian theorems	SOFT COMPUTING										Intuitionistic fuzzy normed space; Cesaro summability; Tauberian theorem; Slow oscillation	SUMMABLE DOUBLE SEQUENCES; STATISTICAL CONVERGENCE	We define the concept of Cesaro summability method in intuitionistic fuzzy normed spaces and prove a related Tauberian theorem. Also, we define slowly oscillating sequences in intuitionistic fuzzy normed spaces, prove related theorems and show that Cesaro summability of slowly oscillating sequences implies ordinary convergence in intuitionistic fuzzy normed spaces. Finally, we give an analogue of classical two-sided Tauberian theorem due to Hardy by using the concept of q-boundedness.																	1432-7643	1433-7479															10.1007/s00500-020-05301-z		SEP 2020											
J								Matheuristics to optimize refueling and maintenance planning of nuclear power plants	JOURNAL OF HEURISTICS										Hybrid heuristics; Matheuristics; Mixed integer programming; Maintenance planning; Nuclear power plants; Optimization in energy	UNIT COMMITMENT PROBLEM; MANAGEMENT; ALGORITHM; OUTAGES	Planning the maintenance of nuclear power plants is a complex optimization problem, involving a joint optimization of maintenance dates, fuel constraints and power production decisions. This paper investigates Mixed Integer Linear Programming (MILP) matheuristics for this problem, to tackle large size instances used in operations with a time scope of 5 years, and few restrictions with time window constraints for the latest maintenance operations. Several constructive matheuristics and a Variable Neighborhood Descent local search are designed. The matheuristics are shown to be accurately effective for medium and large size instances. The matheuristics give also results on the design of MILP formulations and neighborhoods for the problem. Contributions for the operational applications are also discussed. It is shown that the restriction of time windows, which was used to ease computations, induces large over-costs and that this restriction is not required anymore with the capabilities of matheuristics or local searches to solve such size of instances. Our matheuristics can be extended to a bi-objective optimization extension with stability costs, for the monthly re-optimization of the maintenance planning in the real-life application.																	1381-1231	1572-9397															10.1007/s10732-020-09450-0		SEP 2020											
J								Automating smart Internet of Things devices in modern homes using context-based fuzzy logic	COMPUTATIONAL INTELLIGENCE										centralized control; context processing; fuzzy logic; IoT; smart home		Internet of Things (IoT) is mainly used to connect different embedded objects over the internet to make communication between them possible. With the help of IoT, devices find a way to interact, work together, and study from each other's experiences just like humans do. IoT finds its way in applications such as smart home, smart city, healthcare, agriculture, and so on. The name smart home arises due to the automation of the normal home appliances to make it smart. When the devices of the normal smart home are connected via the internet, they become a part of the IoT. The smart home should ensure the following characteristics such as security, comfort, convenience, and energy saving. The article presents a technique for IoT controlled devices in a smart home using context-based fuzzy logic. Fuzzy logic is mainly used to monitor and analyze the real-time data collected from the sensors in the smart homes from various environments. Context-based fuzzy logic uses a multivalued logic principle which differs from the normal Boolean logic, where the truth value lies between only zero and one (ie, true or false). The proposed smart home is implemented in a real case scenario where it yields an accuracy of 90.5%, response time of 6.41 milliseconds, and an F-measure of 97%.																	0824-7935	1467-8640															10.1111/coin.12370		SEP 2020											
J								From mobility data to habits and common pathways	EXPERT SYSTEMS										Gaussian mixture model; habits; longest common sub-sequence; mobility; pattern; spatio-temporal clustering	PATTERNS	Many aspects of our lives are associated with places and the activities we perform on a daily basis. Most of them are recurrent and demand displacement of the individual between regular places like going to work, school or other important personal locations. To accomplish these recurrent daily activities, people tend to follow regular paths with similar temporal and spatial characteristics, especially because humans are frequently looking for uniformity to support their decisions and make their actions easier or even automatic. In this work, we propose a method for discovering common pathways across users' habits from human mobility data. By using a density-based clustering algorithm, we identify the most preferable locations the users visit, we apply a Gaussian mixture model over these places to automatically separate among all traces, the trajectories that follow patterns in order to discover the representations of individual's habits. By using the longest common sub-sequence algorithm, we search for the trajectories that are more similar over the set of users' habits trips by considering the distance that pairs of users or habits share on the same path. The proposed method is evaluated over two real-world GPS datasets and the results show that the approach is able to detect the most important places in a user's life, detect the routine activities and identify common routes between users that have similar habits paving the way for research techniques in carpooling, recommendation and prediction systems.																	0266-4720	1468-0394														e12627	10.1111/exsy.12627		SEP 2020											
J								ANN Based Execution Time Prediction Model and Assessment of Input Parameters through ISM	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Cloud computing; neural network; Prediction model; Resource selection		Cloud computing is on-demand network access model which provides dynamic resource provisioning, selection and scheduling. The performance of these techniques extensively depends on the prediction of various factors e.g., task execution time, resource trust value etc., As the accuracy of prediction model absolutely depends on the input data that are fed into the network, Selection of suitable inputs also plays vital role in predicting the appropriate value. Based on predicted value, Scheduler can choose the suitable resource and perform scheduling for efficient resource utilization and reduced makespan estimates. However, precise prediction of execution time is difficult in cloud environment due to heterogeneous nature of resources and varying input data. As each task has different characteristic and execution criteria, the environment must be intelligent enough to select the suitable resource. To solve these issues, an Artificial Neural Network (ANN) based prediction model is proposed to predict the execution time of tasks. First, input parameters are identified and selected through Interpretive Structural Modeling (ISM) approach. Second, a prediction model is proposed for predicting the task execution time for varying number of inputs. Third, the proposed model is validated and provides 21.72% reduction in mean relative error compared to other state-of-the-art methods.																	1683-3198					SEP	2020	17	5					683	691		10.34028/iajit/17/5/1													
J								Otsu's Thresholding Method Based on Plane Intercept Histogram and Geometric Analysis	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										3-D Otsu's method; threshold selection; Otsu's method; 3-D histogram; image segmentation	SEGMENTATION	The Three-Dimensional (3-D) Otsu's method is an effective improvement on the traditional Otsu's method. However, it not only has high computational complexity, but also needs to improve its anti-noise ability. This paper presents a new Otsu's method based on 3-D histogram. This method transforms 3-D histogram into a 1-D histogram by a plane that is perpendicular to the main diagonal of the 3-D histogram, and designs a new maximum variance criterion for threshold selection. In order to enhance its anti-noise ability, a method based on geometric analysis, which can correct noise, is used for image segmentation. Simulation experiments show that this method has stronger anti-noise ability and less time consumption, comparing with the conventional 3-D Otsu's method, the recursive 3-D Otsu's method, the 3-D Otsu's method with SFLA, the equivalent 3-D Otsu's method and the improved 3-D Otsu's method.																	1683-3198					SEP	2020	17	5					692	701		10.34028/iajit/17/5/2													
J								Saliency Cuts: Salient Region Extraction based on Local Adaptive Thresholding for Image Information Recognition of the Visually Impaired	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Saliency region extraction; saliency map; saliency cuts; local adaptive thresholding; the visually impaired		In recent years, there has been an increased scope for assistive software and technologies, which help the visually impaired to perceive and recognize natural scene images. In this article, we propose a novel saliency cuts approach using local adaptive thresholding to obtain four regions from a given saliency map. The saliency cuts approach is an effective tool for salient object detection. First, we produce four regions for image segmentation using a saliency map as an input image and applying an automatic threshold operation. Second, the four regions are used to initialize an iterative version of the Grab Cut algorithm and to produce a robust and high-quality binary mask with a full resolution. Lastly, based on the binary mask and extracted salient object, outer boundaries and internal edges are detected by Canny edge detection method. Extensive experiments demonstrate that the proposed method correctly detects and extracts the main contents of the image sequences for delivering visually salient information to the visually impaired people compared to the results of existing salient object segmentation algorithms.																	1683-3198					SEP	2020	17	5					713	720		10.34028/iajit/17/5/4													
J								A Novel Feature Selection Method Based on Maximum Likelihood Logistic Regression for Imbalanced Learning in Software Defect Prediction	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Software defect prediction; Machine learning; Class imbalance; Maximum-likelihood logistic regression		The most frequently used machine learning feature ranking approaches failed to present optimal feature subset for accurate prediction of defective software modules in out-of-sample data. Machine learning Feature Selection (FS) algorithms such as Chi-Square (CS), Information Gain (IG), Gain Ratio (GR), RelieF (RF) and Symmetric Uncertainty (SU) perform relatively poor at prediction, even after balancing class distribution in the training data. In this study, we propose a novel FS method based on the Maximum Likelihood Logistic Regression (MLLR). We apply this method on six software defect datasets in their sampled and unsampled forms to select useful features for classification in the context of Software Defect Prediction (SDP). The Support Vector Machine (SVM) and Random Forest (RaF) classifiers are applied on the FS subsets that are based on sampled and unsampled datasets. The performance of the models captured using Area Ander Receiver Operating Characteristics Curve (AUC) metrics are compared for all FS methods considered. The Analysis Of Variance (ANOVA) F-test results validate the superiority of the proposed method over all the FS techniques, both in sampled and unsampled data. The results confirm that the MLLR can be useful in selecting optimal feature subset for more accurate prediction of defective modules in software development process.																	1683-3198					SEP	2020	17	5					721	730		10.34028/iajit/17/5/5													
J								Text Similarity Computation Model for Identifying Rumor Based on Bayesian Network in Microblog	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Microblog Rumor; Similarity; Bayesian Network		The research of text similarity, especially for rumor texts, which constructed the calculation model by known rumors and calculated its similarity. From which, people can recognize the rumor in advance, and improve their vigilance to effectively block and control rumors dissemination. Based on the Bayesian network, the similarity calculation model of microblog rumor texts was built. At the same time, taking into account not only the rumor texts have similar characters, but also the rumor producers have similar characters, and therefore the similarity calculation model of rumor texts makers was constructed. Then, the similarity between the text and the user was integrated, and the microblog similarity calculation model was established. Finally, also experimentally studied the performance of the proposed model on the microblog rumor text and the user data set. The experimental results indicated that the similarity algorithm proposed in this paper could be used to identify the rumors of texts and predict the characters of users more accurately and effectively.																	1683-3198					SEP	2020	17	5					731	741		10.34028/iajit/17/5/6													
J								Enhanced Latent Semantic Indexing Using Cosine Similarity Measures for Medical Application	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Arabic Text; Latent Semantic Indexing; Search Engine; Dimensionality Reduction; Text Classification	INFORMATION-RETRIEVAL; ALGORITHM	The Vector Space Model (VSM) is widely used in data mining and Information Retrieval (IR) systems as a common document representation model. However, there are some challenges to this technique such as high dimensional space and semantic looseness of the representation. Consequently, the Latent Semantic Indexing (LSI) was suggested to reduce the feature dimensions and to generate semantic rich features that can represent conceptual term-document associations. In fact, LSI has been effectively employed in search engines and many other Natural Language Processing (NLP) applications. Researchers thereby promote endless effort seeking for better performance. In this paper, we propose an innovative method that can be used in search engines to find better matched contents of the retrieving documents. The proposed method introduces a new extension for the LSI technique based on the cosine similarity measures. The performance evaluation was carried out using an Arabic language data collection that contains 800 medical related documents, with more than 47,222 unique words. The proposed method was assessed using a small testing set that contains five medical keywords. The results show that the performance of the proposed method is superior when compared to the standard LSI.																	1683-3198					SEP	2020	17	5					742	749		10.34028/iajit/17/5/7													
J								Performance Evaluation of Industrial Firms Using DEA and DECORATE Ensemble Method	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Data Envelopment Analysis; Decision Trees; Ensemble Methods; Financial Variables; Financial Ratios	DATA ENVELOPMENT ANALYSIS; MACHINE	This study introduces an approach of combining Data Envelopment Analysis (DEA) and ensemble Methods in order to classify and predict the efficiency of Decision Making Units (DMU). The approach includes applying DEA in the first stage to compute the efficiency score for each DMU, then a variables' ranker was utilized to extract the most important variables that affect the DMU's performance, then J48 was adopted to build a classifier whose outcomes will be enhanced by Diverse Ensemble Creation by Oppositional Relabeling of Artificial Training Examples (DECORATE) Ensemble method. To examine the approach, this study utilizes a dataset from firms' financial statements that are listed on Amman Stock Exchange. The dataset was preprocessed and turned out to include 53 indusfrial firms for the years 2012 to 2015. The dataset includes 11 input variables and 11 output ratios. The examination of financial variables and ratios play a vital role in the financial analysis practice. This paper shows that financial variable and ratio averages are points of reference to evaluate and measure firms' future financial performance as well as that of other similar firms in the same sector. In addition, the results of this work are for comparative analyses of the financial performance of the indusfrial sector.																	1683-3198					SEP	2020	17	5					750	757		10.34028/iajit/17/5/8													
J								Ontology-Based Transformation and Verification of UML Class Model	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										UML Class Model Verification; Dependency Relationship; XOR Association Constraints		Software models describe structures, relationships and features of the software system. Especially, in Model Driven Engineering (MDE), they are considered as first-class elements instead of programming code and all software development activities move around these models. In MDE, programming code is automatically generated by the models and models' defects can implicitly transfer to the code. These defects can harder to discover and rectify. Model verification is a promising solution to the problem. The Unified Modelling Language (UML) class model is an important part of UML and is used in both analysis and design. However, UML only provides graphical elements without any formal foundation. Therefore, verification of formal properties such as consistency, satisfiability and consequences are not possible in UML. This paper mainly focuses on ontology-based transformation and verification of the UML class model elements which have not been addressed in any existing verification methods e.g. xor association constraint, and dependencies relationships. We validate the scalability and effectiveness of the proposed solution using various UML class models. The empirical study shows that the proposed approach scales in the presence of the large and complex model.																	1683-3198					SEP	2020	17	5					758	768		10.34028/iajit/17/5/9													
J								Improved Streaming Quotient Filter: A Duplicate Detection Approach for Data Streams	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Bloom filters; Computer Network; Data stream; Duplicate detection; False positive rates	BLOOM FILTER	The unprecedented development and popularization of the Internet, combined with the emergence of a variety of modern applications, such as search engines, online transactions, climate warning systems and so on, enables the worldwide storage of data to grow unprecedented. Efficient storage, management and processing of such huge amounts of data has become an important academic research topic. The detection and removal of duplicate and redundant data from such multi-trillion data, while ensuring resource and computational efficiency, has constituted a challenging area of research.Because of the fact that all the data of potentially unbounded data streams can not be stored, and the need to delete duplicated data as accurately as possible, intelligent approximate duplicate data detection algorithms are urgently required. Many well-known methods based on the bitmap structure, Bloom Filter and its variants are listed in the literature. In this paper, we propose a new data structure, Improved Streaming Quotient Filter (ISQF), to efficiently detect and remove duplicate data in a data stream. ISQF intelligently stores the signatures of elements in a data stream, while using an eviction strategy to provide near zero error rates. We show that ISQF achieves near optimal performance with fairly low memory requirements, making it an ideal and efficient method for repeated data detection. It has a very low error rate. Empirically, we compared ISQF with some existing methods (especially Steaming Quotient Filter (SQF)). The results show that our proposed method outperforms the-existing methods in terms of memory usage and accuracy. We also discuss the parallel implementation of ISQF.																	1683-3198					SEP	2020	17	5					769	777		10.34028/iajit/17/5/10													
J								A Concept-based Sentiment Analysis Approach for Arabic	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Arabic Sentiment Analysis; Concept-based Sentiment Analysis; Machine Learning and Ensemble Learning		Concept-Based Sentiment Analysis (CBSA) methods are considered to be more advanced and more accurate when it compared to ordinary Sentiment Analysis methods, because it has the ability of detecting the emotions that conveyed by multi-word expressions concepts in language. This paper presented a CBSA system for Arabic language which utilizes both of machine learning approaches and concept-based sentiment lexicon. For extracting concepts from Arabic, a rule-based concept extraction algorithm called semantic parser is proposed. Different types of feature extraction and representation techniques are experimented among the building prosses of the sentiment analysis model for the presented Arabic CBSA system. A comprehensive and comparative experiments using different types of classification methods and classifier fusion models, together with different combinations of our proposed feature sets, are used to evaluate and test the presented CBSA system. The experiment results showed that the best performance for the sentiment analysis model is achieved by combined Support Vector Machine-Logistic Regression (SVM-LR) model where it obtained a F-score value of 93.23% using the Concept-Based-Features + Lexicon-Based-Features + Word2vec-Features (CBF + LEX+ W2V) features combinations.																	1683-3198					SEP	2020	17	5					778	788		10.34028/iajit/17/5/11													
J								The Performance of Penalty Methods on Tree-Seed Algorithm for Numerical Constrained Optimization Problems	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Constrained optimization; penalty functions; penalty approaches; tree-seed algorithm	EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHMS	The constraints are the most important part of many optimization problems. The metaheuristic algorithms are designed for solving continuous unconstrained optimization problems initially. The constraint handling methods are integrated into these algorithms for solving constrained optimization problems. Penalty approaches are not only the simplest way but also as effective as other constraint handling techniques. In literature, there are many penalty approaches and these are grouped as static, dynamic and adaptive. In this study, we collect them and discuss the key benefits and drawbacks of these techniques. Tree-Seed Algorithm (TSA) is a recently developed metaheuristic algorithm, and in this study, nine different penalty approaches are integrated with the TSA. The performance of these approaches is analyzed on well-known thirteen constrained benchmark functions. The obtained results are compared with state-of-art algorithms like Differential Evolution (DE), Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC), and Genetic Algorithm (GA). The experimental results and comparisons show that TSA outperformed all of them on these benchmark functions.																	1683-3198					SEP	2020	17	5					799	807		10.34028/iajit/17/5/13													
J								Advanced Analysis of the Integrity of Access Control Policies: the Specific Case of Databases	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Access Control; Databases Security; Formal Validation; Integrity Analysis; Conformity Verification		Databases are considered as one of the most compromised assets according to 2014-2016 Verizon Data Breach Reports. The reason is that databases are at the heart of Information Systems (IS) and store confidential business or private records. Ensuring the integrity of sensitive records is highly required and even vital in critical systems (e-health, clouds, e-government, big data, e-commerce, etc.,). The access control is a key mechanism for ensuring the integrity and preserving the privacy in large scale and critical infrastructures. Nonetheless, excessive, unused and abused access privileges are identified as most critical threats in the top ten database security threats according to 2013-2015 Imperva Application Defense Center reports. To address this issue, we focus in this paper on the analysis of the integrity of access control policies within relational databases. We propose a rigorous and complete solution to help security architects verifying the correspondence between the security planning and its concrete implementation. We define a formal framework for detecting non-compliance anomalies in concrete Role Based Access Control (RBAC) policies. We rely on an example to illustrate the relevance of our contribution.																	1683-3198					SEP	2020	17	5					808	815		10.34028/iajit/17/5/14													
J								A Fast High Precision Skew Angle Estimation of Digitized Documents	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Skew angle estimation; document images; Hough transform; Binarization; edge detection; RLSA		In this paper, we treated the problem of automatic skew angle estimation of scanned documents. The skew of document occurs very often, due to incorrect positioning of the documents or a manipulation error during scanning. This has negative consequences on the steps of automatic analysis and recognition of text. It is therefore essential to verify, before proceeding to these steps, the presence of skew on the document to be processed and to correct it. The difficulty of this verification is associated to the presence of graphic zones, sometimes dominant, that have a considerable impact on the accuracy of the text skew angle estimation. We also noted the importance of preprocessing to improve the accuracy and the calculation cost of skew estimation approaches. These two elements have been taken into consideration in our design and development of a new approach of skew angle estimation and correction. Our approach is based on local binarization followed by horizontal smoothing by the Run Length Smoothing Algorithm (RLSA) method, detection of horizontal contours and the Hierarchical Hough Transform (HHT). The algorithms involved in our approach have been chosen to guarantee a skew estimation: accurate, fast and robust, especially to graphic dominance and real time application. The experimental tests show the effectiveness of our approach on a representative database of the Document Image Skew Estimation Contest (DISEC) contest International Conference on Document Analysis and Recognition (ICDAR).																	1683-3198					SEP	2020	17	5					825	831		10.34028/iajit/17/5/16													
J								Ensemble neural network approach detecting pain intensity from facial expressions	ARTIFICIAL INTELLIGENCE IN MEDICINE										Ensemble neural network; Pain detection; Facial expression; Deep learning	CLASSIFICATION	This paper reports on research to design an ensemble deep learning framework that integrates fine-tuned, three-stream hybrid deep neural network (i.e., Ensemble Deep Learning Model, EDLM), employing Convolutional Neural Network (CNN) to extract facial image features, detect and accurately classify the pain. To develop the approach, the VGGFace is fine-tuned and integrated with Principal Component Analysis and employed to extract features in images from the Multimodal Intensity Pain database at the early phase of the model fusion. Subsequently, a late fusion, three layers hybrid CNN and recurrent neural network algorithm is developed with their outputs merged to produce image-classified features to classify pain levels. The EDLM model is then benchmarked by means of a single-stream deep learning model including several competing models based on deep learning methods. The results obtained indicate that the proposed framework is able to outperform the competing methods, applied in a multi-level pain detection database to produce a feature classification accuracy that exceeds 89 %, with a receiver operating characteristic of 93 %. To evaluate the generalization of the proposed EDLM model, the UNBC-McMaster Shoulder Pain dataset is used as a test dataset for all of the modelling experiments, which reveals the efficacy of the proposed method for pain classification from facial images. The study concludes that the proposed EDLM model can accurately classify pain and generate multi-class pain levels for potential applications in the medical informatics area, and should therefore, be explored further in expert systems for detecting and classifying the pain intensity of patients, and automatically evaluating the patients' pain level accurately.																	0933-3657	1873-2860				SEP	2020	109								101954	10.1016/j.artmed.2020.101954													
J								Reinforcement learning for intelligent healthcare applications: A survey	ARTIFICIAL INTELLIGENCE IN MEDICINE										Artificial intelligence; Reinforcement learning; Healthcare; Personalized medicine	MARKOV DECISION-PROCESSES; STRATEGIES; DESIGN; MODEL; OPTIMIZATION; ANESTHESIA; CONTROLLER; CHALLENGES; INTERFACE	Discovering new treatments and personalizing existing ones is one of the major goals of modern clinical research. In the last decade, Artificial Intelligence (AI) has enabled the realization of advanced intelligent systems able to learn about clinical treatments and discover new medical knowledge from the huge amount of data collected. Reinforcement Learning (RL), which is a branch of Machine Learning (ML), has received significant attention in the medical community since it has the potentiality to support the development of personalized treatments in accordance with the more general precision medicine vision. This report presents a review of the role of RL in healthcare by investigating past work, and highlighting any limitations and possible future contributions.																	0933-3657	1873-2860				SEP	2020	109								101964	10.1016/j.artmed.2020.101964													
J								Recommendations for enhancing the usability and understandability of process mining in healthcare	ARTIFICIAL INTELLIGENCE IN MEDICINE										Process mining; Healthcare processes; Event log; Process execution data; Health information system; Hospital information system; Process analysis; Process improvement	DECISION-SUPPORT-SYSTEMS; DEPARTMENT PERFORMANCE-MEASURES; EVIDENCE-BASED MEDICINE; PREDICTIVE ANALYTICS; CONFORMANCE CHECKING; CLINICAL GUIDELINES; PROCESS DISCOVERY; DOMAIN KNOWLEDGE; DESIGN SCIENCE; FRAMEWORK	Healthcare organizations are confronted with challenges including the contention between tightening budgets and increased care needs. In the light of these challenges, they are becoming increasingly aware of the need to improve their processes to ensure quality of care for patients. To identify process improvement opportunities, a thorough process analysis is required, which can be based on real-life process execution data captured by health information systems. Process mining is a research field that focuses on the development of techniques to extract process-related insights from process execution data, providing valuable and previously unknown information to instigate evidence-based process improvement in healthcare. However, despite the potential of process mining, its uptake in healthcare organizations outside case studies in a research context is rather limited. This observation was the starting point for an international brainstorm seminar. Based on the seminar's outcomes and with the ambition to stimulate a more widespread use of process mining in healthcare, this paper formulates recommendations to enhance the usability and understandability of process mining in healthcare. These recommendations are mainly targeted towards process mining researchers and the community to consider when developing a new research agenda for process mining in healthcare. Moreover, a limited number of recommendations are directed towards healthcare organizations and health information systems vendors, when shaping an environment to enable the continuous use of process mining.																	0933-3657	1873-2860				SEP	2020	109								101962	10.1016/j.artmed.2020.101962													
J								A dynamic prediction engine to prevent chemotherapy-induced nausea and vomiting	ARTIFICIAL INTELLIGENCE IN MEDICINE										Chemotherapy; CINV; Side effects; Data mining; Data science; Prediction; Clinical decision support	QUALITY-OF-LIFE; CANCER STATISTICS; ROUTINE PRACTICE; IMPACT; ANTIEMETICS; VALIDATION; PROTECTION; MORTALITY; ETIOLOGY; THERAPY	Background: Cancer remains the second major cause of death in the United States over the last decade. Chemotherapy is a core component of nearly every cancer treatment plan. Chemotherapy-Induced Nausea and Vomiting (CINV) are the two most dreadful and unpleasant side-effects of chemotherapy for cancer patients. Several patient-specific factors affect the risk of CINV. However, none of the guidelines consider those factors. Not all of the patients have the similar emetic risk of CINV. Despite the improvements in CINV management, as many as two-thirds of chemotherapy patients still experience some degree of CINV. As a result, physicians use their personal experiences for CINV treatment, which leads to inconsistent managements of CINV. Objective: The overall objective of this study is to improve the prevention of CINV using precise, personalized and evidence-based antiemetic treatment before chemotherapy. In CINV prediction, one of the interesting factors is that CINV has two distinct and complex pathophysiologic phases: acute and delayed. In addition, the risk factors and their associations are different for different emetogenic chemotherapies (e.g., low, moderate, and high). There are six contexts considering the combination of phases and emetogenicity levels. This will require the creation of six different models. Instead, our objective was to describe a single framework named "prediction engine" that can perform prediction query without losing the sensitivity to each context. The prediction engine discovers how the patient-related variables and the emetogenecity of chemotherapy are associated with the risk of CINV for each phase. Methods: This was a single-center retrospective study. The data were collected by retrospective record review from the electronic medical record system used at the University of Missouri Ellis Fischel Cancer Center. An association rule-based dynamic and context-sensitive Prediction Engine has been developed. Physicians receive feedback about CINV risks of patients from the CINV decision support system based on patient-specific factors. Results: The prediction performance of the system outperformed many popular prediction methods and all the results of CINV risk prediction published in the literature. Best prediction performance was achieved using the rule-ranking approach. The accuracy, sensitivity, and specificity were 87.85 %, 87.54 %, and 88.2 %, respectively. Conclusions: The system used the patient-specific risk factors for making personalized treatment recommendations for CINV. It solved a real clinical problem that will shorten the gap between clinical practices and evidence-based guidelines for CINV management leading to the practice of personalized and precise treatment recommendation, better life quality of patient, and reduced healthcare cost. The approach presented in this article can be applied to any other clinical predictions.																	0933-3657	1873-2860				SEP	2020	109								101925	10.1016/j.artmed.2020.101925													
J								Skeletal scintigraphy image enhancement based neutrosophic sets and salp swarm algorithm	ARTIFICIAL INTELLIGENCE IN MEDICINE										Nuclear medicine; Bone scintigraphy; Neutrosophic domain; Neutrosophic similarity score; Salp swarm algorithm	SIMILARITY SCORE; NUCLEAR-MEDICINE; SEGMENTATION; QUALITY	Recently, several schemes are proposed for enhancing the dark regions of the skeletal scintigraphy image. Nevertheless, most of them are flawed by some performance problems. This paper presents an adaptive scheme based on Salp Swarm algorithm (SSA) and a neutrosophic set (NS) under multi-criteria to enhance the dark regions of the skeletal scintigraphy image efficiently. Enhancing the dark regions is first converted into an optimization problem. The SSA algorithm is used to find the best improvement for each image separately, and then the neutrosophic algorithm is used to find similarity score to each image with adaptive weight coefficients obtained by the SSA algorithm. The proposed algorithm is applied to an Egyptian medical dataset collected from Menoufia University Hospital and it is a no-reference image. The experiments are done using 3 different resolutions 512*512, 256*256, and 128*128 and compared with Gamma Correction, the NS algorithm and the local enhance algorithm. The results demonstrate that the proposed algorithm achieves superior performance in almost criteria fitness function, entropy, eumber of edges, nNaturalness image quality Evaluator, sharpness, sharpness index, and contrast-distorted images using contrast enhancement. The results showed the idea of integration between the falsity membership of the neutrosophic set and the Salp swarm algorithm can be used to Skeletal Scintigraphy enhancement. This paper proved that it can depend on falsity membership of the neutrosophic set in the Image Enhancement field.																	0933-3657	1873-2860				SEP	2020	109								101953	10.1016/j.artmed.2020.101953													
J								Exploiting complex medical data with interpretable deep learning for adverse drug event prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Deep learning; Text mining; Explainable AI; Adverse drug events; Medical records	RISK-FACTORS	A variety of deep learning architectures have been developed for the goal of predictive modelling and knowledge extraction from medical records. Several models have placed strong emphasis on temporal attention mechanisms and decay factors as a means to include highly temporally relevant information regarding the recency of medical event occurrence while facilitating medical code-level interpretability. In this study we utilise such models with a large Electronic Patient Record (EPR) data set consisting of diagnoses, medication, and clinical text data for the purpose of adverse drug event (ADE) prediction. The first contribution of this work is an empirical evaluation of two state-of-the-art medical-code based models in terms of objective performance metrics for ADE prediction on diagnosis and medication data. Secondly, as an extension of previous work, we augment an interpretable deep learning architecture to permit numerical risk and clinical text features and demonstrate how this approach yields improved predictive performance compared to the other baselines. Finally, we assess the importance of attention mechanisms in regards to their usefulness for medical code-level and text-level interpretability, which may facilitate novel insights pertaining to the nature of ADE occurrence within the health care domain.																	0933-3657	1873-2860				SEP	2020	109								101942	10.1016/j.artmed.2020.101942													
J								Grouping attributes zero-shot learning for tongue constitution recognition	ARTIFICIAL INTELLIGENCE IN MEDICINE										Traditional Chinese Medicine; Deep learning; Tongue images; Constitution recognition; Zero-shot learning; Grouping attributes	COLOR; SUSCEPTIBILITY; IDENTIFICATION; TEXTURE	Traditional Chinese Medicine (TCM) considers that the personal constitution determines the occurrence trend and therapeutic effects of certain diseases, which can be recognized by machine learning through tongue images. However, current machine learning methods are confronted with two challenges. First, there are not some larger tongue image databases available. Second, they do not use the domain knowledge of TCM, so that the imbalance of constitution categories cannot be solved. Therefore, this paper proposes a new constitution recognition method based on the zero-shot learning with the knowledge of TCM. To further improve the performance, a new zero-shot learning method is proposed by grouping attributes and learning discriminant latent features, which can better solve the imbalance problem of constitution categories. Experimental results on our constructed databases validate the proposed methods.																	0933-3657	1873-2860				SEP	2020	109								101951	10.1016/j.artmed.2020.101951													
J								Extracting deep features from short ECG signals for early atrial fibrillation detection	ARTIFICIAL INTELLIGENCE IN MEDICINE										Medical knowledge engineering; Deep features extraction; Early atrial fibrillation detection; Data mining	TIME-SCALE DECOMPOSITION; ENERGY	Atrial Fibrillation (AF) at an early stage has a short duration and is sometimes asymptomatic, making it difficult to detect. Although the use of mobile sensing devices has provided the possibility of real-time cardiac detection, it is highly susceptible to the noise signals generated by body movement. Therefore, it is of great importance to study early AF detection for mobile terminals with noise immunity. Extracting effective features is critical to AF detection, but most existing studies used shallow time, frequency or time-frequency energy (TFE) features with weak representation that need to rely on long ECG signals to capture the variation in information and cannot sensitively capture the subtle variation caused by early AF. In addition, most studies only considered the discrimination of AF from normal sinus rhythm (SR) signals, ignoring the interference of noise and other signals. This study proposes three new deep features that can accurately capture the subtle variation in short ECG segments caused by early AF, examines the interference of noise and other signals generated by the mobile terminal and proposes a new feature set for early AF detection. We use six popular classifiers to evaluate the relative effectiveness of the deep features we developed against the features extracted by two conventional time-frequency methods, and the performance of the proposed feature set for detecting early AF. Our study shows that the best results for classifying AF and SR are obtained by Random Forest (RF), with 0.96 F1 score. The best results for classifying four types of signal are obtained by Extreme Gradient Boosting (XGBoost), with overall F1 score 0.88 and the individual F1 score for classifying SR, AF, Other and Noisy with 0.91, 0.90, 0.73, and 0.96, respectively.																	0933-3657	1873-2860				SEP	2020	109								101896	10.1016/j.artmed.2020.101896													
J								A Mixed-Integer and Asynchronous Level Decomposition with Application to the Stochastic Hydrothermal Unit-Commitment Problem	ALGORITHMS										stochastic programming; stochastic hydrothermal UC problem; parallel computing; asynchronous computing; level decomposition	OPTIMIZATION	Independent System Operators (ISOs) worldwide face the ever-increasing challenge of coping with uncertainties, which requires sophisticated algorithms for solving unit-commitment (UC) problems of increasing complexity in less-and-less time. Hence, decomposition methods are appealing options to produce easier-to-handle problems that can hopefully return good solutions at reasonable times. When applied to two-stage stochastic models, decomposition often yields subproblems that are embarrassingly parallel. Synchronous parallel-computing techniques are applied to the decomposable subproblem and frequently result in considerable time savings. However, due to the inherent run-time differences amongst the subproblem's optimization models, unequal equipment, and communication overheads, synchronous approaches may underuse the computing resources. Consequently, asynchronous computing constitutes a natural enhancement to existing methods. In this work, we propose a novel extension of the asynchronous level decomposition to solve stochastic hydrothermal UC problems with mixed-integer variables in the first stage. In addition, we combine this novel method with an efficient task allocation to yield an innovative algorithm that far outperforms the current state-of-the-art. We provide convergence analysis of our proposal and assess its computational performance on a testbed consisting of 54 problems from a 46-bus system. Results show that our asynchronous algorithm outperforms its synchronous counterpart in terms of wall-clock computing time in 40% of the problems, providing time savings averaging about 45%, while also reducing the standard deviation of running times over the testbed in the order of 25%.																		1999-4893				SEP	2020	13	9							235	10.3390/a13090235													
J								A Brain-Inspired Hyperdimensional Computing Approach for Classifying Massive DNA Methylation Data of Cancer	ALGORITHMS										algorithms in biology; bioinformatics; machine learning; classification; hyperdimensional computing; cancer; DNA methylation	CLASSIFICATION; IMPACT; CPG	The recent advancements in cancer genomics have put under the spotlight DNA methylation, a genetic modification that regulates the functioning of the genome and whose modifications have an important role in tumorigenesis and tumor-suppression. Because of the high dimensionality and the enormous amount of genomic data that are produced through the last advancements in Next Generation Sequencing, it is very challenging to effectively make use of DNA methylation data in diagnostics applications, e.g., in the identification of healthy vs diseased samples. Additionally, state-of-the-art techniques are not fast enough to rapidly produce reliable results or efficient in managing those massive amounts of data. For this reason, we propose HD-classifier, an in-memory cognitive-based hyperdimensional (HD) supervised machine learning algorithm for the classification of tumor vs non tumor samples through the analysis of their DNA Methylation data. The approach takes inspiration from how the human brain is able to remember and distinguish simple and complex concepts by adopting hypervectors and no single numerical values. Exactly as the brain works, this allows for encoding complex patterns, which makes the whole architecture robust to failures and mistakes also with noisy data. We design and develop an algorithm and a software tool that is able to perform supervised classification with the HD approach. We conduct experiments on three DNA methylation datasets of different types of cancer in order to prove the validity of our algorithm, i.e., Breast Invasive Carcinoma (BRCA), Kidney renal papillary cell carcinoma (KIRP), and Thyroid carcinoma (THCA). We obtain outstanding results in terms of accuracy and computational time with a low amount of computational resources. Furthermore, we validate our approach by comparing it (i) to BIGBIOCL, a software based on Random Forest for classifying big omics datasets in distributed computing environments, (ii) to Support Vector Machine (SVM), and (iii) to Decision Tree state-of-the-art classification methods. Finally, we freely release both the datasets and the software on GitHub.																		1999-4893				SEP	2020	13	9							233	10.3390/a13090233													
J								A Fast Image Thresholding Algorithm for Infrared Images Based on Histogram Approximation and Circuit Theory	ALGORITHMS										infrared image; image thresholding; histogram approximation; transient response of a first-order linear circuit; gray-level histogram; threshold value	SEGMENTATION; ENTROPY	Image thresholding is one of the fastest and most effective methods of detecting objects in infrared images. This paper proposes an infrared image thresholding method based on the functional approximation of the histogram. The one-dimensional histogram of the image is approximated to the transient response of a first-order linear circuit. The threshold value for the image segmentation is formulated using combinational analogues of standard operators and principles from the concept of the transient behavior of the first-order linear circuit. The proposed method is tested on infrared images gathered from the standard databases and the experimental results are compared with the existing state-of-the-art infrared image thresholding methods. We realized through the experimental results that our method is well suited to perform infrared image thresholding.																		1999-4893				SEP	2020	13	9							207	10.3390/a13090207													
J								A Simulated Annealing Algorithm for Solving Two-Echelon Vehicle Routing Problem with Locker Facilities	ALGORITHMS										vehicle routing problem; locker facilities; integer linear programming	CUT ALGORITHM	We consider the problem of utilizing the parcel locker network for the logistics solution in the metropolitan area. Two-echelon distribution systems are attractive from an economic standpoint, whereas the product from the depot can be distributed from or to intermediate facilities. In this case, the intermediate facilities are considered as locker facilities present in an accessible location in the vicinity of the final customers. In addition, the utilization of locker facilities can reduce the cost caused by the unattended deliveries. The problem is addressed as an optimization model that formulated into an integer linear programming model denoted as the two-echelon vehicle routing problem with locker facilities (2EVRP-LF). The objective is to minimize the cost of transportation with regards to the vehicle travelling cost, the intermediate facilities renting cost, and the additional cost to compensate the customer that needs to travel to access the intermediate facilities. Because of its complexity, a simulated annealing algorithm is proposed to solve the problem. On the other hand, the modelling approach can be conducted by generating two-phase optimization model approaches, which are the p-median problem and the capacitated vehicle routing problem. The results from both methods are compared in numerical experiments. The results show the effectiveness of 2EVRP-LF compared to the two-phase optimization. Furthermore, the simulated annealing algorithm showed an effective performance in solving 2EVRP-LF.																		1999-4893				SEP	2020	13	9							218	10.3390/a13090218													
J								Fused Gromov-Wasserstein Distance for Structured Objects	ALGORITHMS										optimal transport; GRAPHS and Structured objects; Wasserstein and Gromov-Wasserstein distances	ALGORITHM	Optimal transport theory has recently found many applications in machine learning thanks to its capacity to meaningfully compare various machine learning objects that are viewed as distributions. The Kantorovitch formulation, leading to the Wasserstein distance, focuses on the features of the elements of the objects, but treats them independently, whereas the Gromov-Wasserstein distance focuses on the relations between the elements, depicting the structure of the object, yet discarding its features. In this paper, we study the Fused Gromov-Wasserstein distance that extends the Wasserstein and Gromov-Wasserstein distances in order to encode simultaneously both the feature and structure information. We provide the mathematical framework for this distance in the continuous setting, prove its metric and interpolation properties, and provide a concentration result for the convergence of finite samples. We also illustrate and interpret its use in various applications, where structured objects are involved.																		1999-4893				SEP	2020	13	9							212	10.3390/a13090212													
J								Arrhythmias Prediction Using an Hybrid Model Based on Convolutional Neural Network and Nonlinear Regression	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										ECG; arrhythmia; detection; classification; prediction; convolutional neural networks; nonlinear regression models	AUTOMATED DETECTION; CLASSIFICATION	In biomedical signal processing, artificial intelligence techniques are used for identifying and extracting relevant information. However, it lacks effective solutions based on machine learning for the prediction of cardiac arrhythmias. The heart diseases diagnosis rests essentially on the analysis of various properties of ECG signal. The arrhythmia is one of the most common heart diseases. A cardiac arrhythmia is a disturbance of the heart rhythm. It occurs when the heart beats too slowly, too fast or anarchically, with no apparent cause. The diagnosis of cardiac arrhythmias is based on the analysis of the ECG properties, especially, the durations (P, QRS, T), the amplitudes (P, Q, R, S, T), the intervals (PQ, QT, RR), the cardiac frequency and the rhythm. In this paper we propose a system of arrhythmias diagnosis assistance based on the analysis of the temporal and frequential properties of the ECG signal. After the features extraction step, the ECG properties are then used as input for a convolutional neural network to detect and classify the arrhythmias. Finally, the classification results are used to perform a prediction of arrhythmias with nonlinear regression model. The method is illustrated using the MIT-BIH database.																	1469-0268	1757-5885				SEP	2020	19	3							2050024	10.1142/S1469026820500248													
J								Improved Flower Pollination Algorithm for Optimal Groundwater Management	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Groundwater management; simulation-optimization model; flower pollination algorithm	MULTIOBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHMS; WATER-RESOURCES; DESIGN	Groundwater management problems are typically of a large-scale nature, involving complex nonlinear objective functions and constraints, which are commonly evaluated through the use of numerical simulation models. Given these complexities, metaheuristic optimization algorithms have recently become popular choice for solving such complex problems which are difficult to solve by traditional methods. However, the practical applications of metaheuristics are severely challenged by the requirement of large number of function evaluations to achieve convergence. To overcome this shortcoming, many new metaheuristics and different variants of existing ones have been proposed in recent years. In this study, a recently developed algorithm called power pollination algorithm (FPA) is investigated for optimal groundwater management. The FPA is improved, combined with the widely used groundwater flow simulation model MODFLOW, and applied to solve two groundwater management problems. The proposed algorithm, denoted as IFPA, is first tested on a hypothetical aquifer system, to minimize the total pumping to contain contaminated groundwater within a capture zone. IFPA is then applied to maximize the total annual pumping from existing wells in Rhis-Nekor unconfined coastal aquifer on the northern of Morocco. The obtained results indicate that IFPA is a promising method for solving groundwater management problems as it outperforms the standard FPA and other algorithms applied to the case studies considered, both in terms of convergence rate and solution quality.																	1469-0268	1757-5885				SEP	2020	19	3							2050022	10.1142/S1469026820500224													
J								Mine Pressure Prediction Study Based on Fuzzy Cognitive Maps	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Fuzzy cognitive maps; mine pressure behavior; weight regularization; dropout regularization; genetic algorithm; prediction		The study on the prediction of mine pressure, while exploiting in coal mine, is a critical and technical guarantee for coal mine safety and production. In this paper, primarily due to the actual demand for the prediction of mine pressure, a practical prediction model Mine Pressure Prediction (MPP) was proposed based on fuzzy cognitive maps (FCMs). The Real Coded Genetic Algorithm (RCGA) was proposed to solve the problem by introducing the weight regularization and dropout regularization. A numerical example involving in-situ monitoring data is studied. Mean Square Error (MSE) and fitness function were used to evaluate the applicability of MPP model which is trained by RCGA, Regularization Genetic Algorithm (RGA) and Weight and Dropout RGA optimization algorithms. The numerical results demonstrate that the proposed Weight and Dropout RGA is better than the other two algorithms, and realizing the requirement for prediction of mine pressure in the coal mine production.																	1469-0268	1757-5885				SEP	2020	19	3							2050023	10.1142/S1469026820500236													
J								A Novel Energy-Efficient Clustering Protocol Using Two-Stage Genetic Algorithm for Improving the Lifetime of Wireless Sensor Networks	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Wireless sensor network; clustering; genetic algorithm; energy-efficient WSN	ROUTING ALGORITHMS; HEAD SELECTION	Wireless sensor networks (WSNs) are beginning to be deployed at an accelerated pace, and they have attracted significant attention in a broad spectrum of applications. WSNs encompass a large number of sensor nodes enabling a base station (BS) to sense and transmit data over the area where WSN is spread. As most sensor nodes have a limited energy capacity and at the same time transmit critical information, enhancing the lifetime and the reliability of WSNs are essential factors in designing these networks. Among many approaches, clustering of sensor nodes has proved to be an effective method of reducing energy consumption and increasing lifetime of WSNs. In this paper, a new energy-efficient clustering protocol is implemented using a two-step Genetic Algorithm (GA). In the first step of GA, cluster heads (CHs) are selected, and in the second step, cluster members are chosen based on their distance to the selected CHs. Compared to other clustering protocols, the lifetime of WSNs in the proposed clustering is improved. This improvement is the consequence of the fact that this clustering considers energy efficient parameters in clustering protocol.																	1469-0268	1757-5885				SEP	2020	19	3							2050019	10.1142/S1469026820500194													
J								Optimistic Variants of Single-Objective Bilevel Optimization for Evolutionary Algorithms	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Bilevel optimization; evolutionary computation; intermarriage-crossover; optimistic and pessimistic; decision-making	GENETIC ALGORITHM; MODEL	Single-objective bilevel optimization is a specialized form of constraint optimization problems where one of the constraints is an optimization problem itself. These problems are typically non-convex and strongly NP-Hard. Recently, there has been an increased interest from the evolutionary computation community to model bilevel problems due to its applicability in real-world applications for decision-making problems. In this work, a partial nested evolutionary approach with a local heuristic search has been proposed to solve the benchmark problems and have outstanding results. This approach relies on the concept of intermarriage-crossover in search of feasible regions by exploiting information from the constraints. A new variant has also been proposed to the commonly used convergence approaches, i.e., optimistic and pessimistic. It is called an extreme optimistic approach. The experimental results demonstrate the algorithm converges differently to known optimum solutions with the optimistic variants. Optimistic approach also outperforms pessimistic approach. Comparative statistical analysis of our approach with other recently published partial to complete evolutionary approaches demonstrates very competitive results.																	1469-0268	1757-5885				SEP	2020	19	3							2050020	10.1142/S1469026820500200													
J								A Fuzzy Logic-Based Method to Avert Intrusions in Wireless Sensor Networks Using WSN-DS Dataset	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Fuzzy; WSN-DS; intrusion; prevention; malicious	FEATURE-SELECTION	Intrusion is one of the biggest problems in wireless sensor networks. Because of the evolution in wired and wireless mechanization, various archetypes are used for communication. But security is the major concern as networks are more prone to intrusions. An intrusion can be dealt in two ways: either by detecting an intrusion in a wireless sensor network or by preventing an intrusion in a wireless sensor network. Many researchers are working on detecting intrusions and less emphasis is given on intrusion prevention. One of the modern techniques for averting intrusions is through fuzzy logic. In this paper, we have defined a fuzzy rule-based system to avert intrusions in wireless sensor network. The proposed system works in three phases: feature extraction, membership value computation and fuzzified rule applicator. The proposed method revolves around predicting nodes in three categories as "red", "orange" and "green". "Red" represents that the node is malicious and prevents it from entering the network. "Orange" represents that the node "might be malicious" and marks it suspicious. "Green" represents that the node is not malicious and it is safe to enter the network. The parameters for the proposed FzMAI are packet send to base station, energy consumption, signal strength, a packet received and PDR. Evaluation results show an accuracy of 98.29% for the proposed system. A detailed comparative analysis concludes that the proposed system outperforms all the other considered fuzzy rule-based systems. The advantage of the proposed system is that it prevents a malicious node from entering the system, thus averting intrusion.																	1469-0268	1757-5885				SEP	2020	19	3							2050018	10.1142/S1469026820500182													
J								EEG Feature Extraction Using Genetic Programming for the Classification of Mental States	ALGORITHMS										EEG; classification; genetic programming; feature extraction; mental states	CLASSIFIERS; SELECTION; SEIZURE	The design of efficient electroencephalogram (EEG) classification systems for the detection of mental states is still an open problem. Such systems can be used to provide assistance to humans in tasks where a certain level of alertness is required, like in surgery or in the operation of heavy machines, among others. In this work, we extend a previous study where a classification system is proposed using a Common Spatial Pattern (CSP) and Linear Discriminant Analysis (LDA) for the classification of two mental states, namely a relaxed and a normal state. Here, we propose an enhanced feature extraction algorithm (Augmented Feature Extraction with Genetic Programming, or+FEGP) that improves upon previous results by employing a Genetic-Programming-based methodology on top of the CSP. The proposed algorithm searches for non-linear transformations that build new features and simplify the classification task. Although the proposed algorithm can be coupled with any classifier, LDA achieves 78.8% accuracy, the best predictive accuracy among tested classifiers, significantly improving upon previously published results on the same real-world dataset.																		1999-4893				SEP	2020	13	9							221	10.3390/a13090221													
J								Fuzzy Preference Programming Framework for Functional assessment of Subway Networks	ALGORITHMS										fuzzy ANP; fuzzy preference programming; subway network; impacts of failure; infrastructure assets; criticality index	EXPERT-SYSTEM	The 2019 Canadian Infrastructure report card identified 60% of the subway system to be in a very poor to a poor condition. With multiple assets competing for the limited fund, new methodologies are required to prioritize assets for rehabilitation. The report suggested that adopting an Asset Management Plan would assist municipalities in maintaining and operating infrastructure effectively. ISO 55000 emphasized the importance of risk assessment in assessing the value of an organization's assets. Subway risk assessment models mainly focus on structural failures with minimum focus on functional failure impacts and network criticality attributes. This research presents two modules to measure the functional failure impacts of a subway network, given financial, social, and operational perspectives, in addition to the station criticality. The model uses the Fuzzy Analytical Network Process with application to Fuzzy Preference Programming to calculate the weights for seven failure impact attributers and seven criticality attributes. Data are collected using questionnaires and unstructured/structured interviews with municipality personnel. The analysis identified social impacts to have the highest score of 38%, followed by operational and financial impacts at 34% and 27.65%, respectively. The subway station criticality revealed station location to have the highest impact at 35%, followed by station nature of use and station characteristics at 30.5% and 31.82%, respectively. When integrated with probability of failure, this model provides a comprehensive risk index to optimize stations for rehabilitation.																		1999-4893				SEP	2020	13	9							220	10.3390/a13090220													
J								A New Chaotic-Based Approach for Multi-Objective Optimization	ALGORITHMS										multi-objective optimization; large-scale optimization; Chaotic search; Tchebychev scalarization	ALGORITHM	Multi-objective optimization problems (MOPs) have been widely studied during the last decades. In this paper, we present a new approach based on Chaotic search to solve MOPs. Various Tchebychev scalarization strategies have been investigated. Moreover, a comparison with state of the art algorithms on different well known bound constrained benchmarks shows the efficiency and the effectiveness of the proposed Chaotic search approach.																		1999-4893				SEP	2020	13	9							204	10.3390/a13090204													
J								More Time-Space Tradeoffs for Finding a Shortest Unique Substring	ALGORITHMS										shortest unique substring; k-mismatch SUS; time-space tradeoff; Karp-Rabin; sketching; suffix trees	CONSTRUCTION	We extend recent results regarding finding shortest unique substrings (SUSs) to obtain new time-space tradeoffs for this problem and the generalization of finding k-mismatch SUSs. Our new results include the first algorithm for finding a k-mismatch SUS in sublinear space, which we obtain by extending an algorithm by Senanayaka (2019) and combining it with a result on sketching by Gawrychowski and Starikovskaya (2019). We first describe how, given a text T of length n and m words of workspace, with high probability we can find an SUS of length L in O(n(L/m) log L) time using random access to T, or in O(n(L/m) log(2) (L) log log sigma) time using O((L/m) log(2) L) sequential passes over T. We then describe how, for constant k, with high probability, we can find a k-mismatch SUS in O(n(1+epsilon) L/m) time using O(n(epsilon)L/m) sequential passes over T, again using only m words of workspace. Finally, we also describe a deterministic algorithm that takes O (n tau log sigma log n) time to find an SUS using O(n/tau) words of workspace, where tau is a parameter.																		1999-4893				SEP	2020	13	9							234	10.3390/a13090234													
J								Hierarchical and Unsupervised Graph Representation Learning with Loukas's Coarsening	ALGORITHMS										graph representation learning; Graph2Vec; graph convolutional networks; graph coarsening; unsupervised learning; mutual information maximization		We propose a novel algorithm for unsupervised graph representation learning with attributed graphs. It combines three advantages addressing some current limitations of the literature: (i) The model is inductive: it can embed new graphs without re-training in the presence of new data; (ii) The method takes into account both micro-structures and macro-structures by looking at the attributed graphs at different scales; (iii) The model is end-to-end differentiable: it is a building block that can be plugged into deep learning pipelines and allows for back-propagation. We show that combining a coarsening method having strong theoretical guarantees with mutual information maximization suffices to produce high quality embeddings. We evaluate them on classification tasks with common benchmarks of the literature. We show that our algorithm is competitive with state of the art among unsupervised graph representation learning methods.																		1999-4893				SEP	2020	13	9							206	10.3390/a13090206													
J								A Linear-Time Algorithm for the Isometric Reconciliation of Unrooted Trees	ALGORITHMS										reconciliation; phylogeny; gene tree	GENE DUPLICATION; PHYLOGENY	In the reconciliation problem, we are given two phylogenetic trees. A species tree represents the evolutionary history of a group of species, and a gene tree represents the history of a family of related genes within these species. A reconciliation maps nodes of the gene tree to the corresponding points of the species tree, and thus helps to interpret the gene family history. In this paper, we study the case when both trees are unrooted and their edge lengths are known exactly. The goal is to root them and to find a reconciliation that agrees with the edge lengths. We show a linear-time algorithm for finding the set of all possible root locations, which is a significant improvement compared to the previousO(N-3 log N) algorithm.																		1999-4893				SEP	2020	13	9							225	10.3390/a13090225													
J								Finding Top-k Nodes for Temporal Closeness in Large Temporal Graphs	ALGORITHMS										temporal graph; link stream; temporal path; temporal closeness; montecarlo method	CENTRALITY; TIME	The harmonic closeness centrality measure associates, to each node of a graph, the average of the inverse of its distances from all the other nodes (by assuming that unreachable nodes are at infinite distance). This notion has been adapted to temporal graphs (that is, graphs in which edges can appear and disappear during time) and in this paper we address the question of finding the top-knodes for this metric. Computing the temporal closeness for one node can be done inO(m)time, wheremis the number of temporal edges. Therefore computing exactly the closeness for all nodes, in order to find the ones with top closeness, would requireO(nm)time, wherenis the number of nodes. This time complexity is intractable for large temporal graphs. Instead, we show how this measure can be efficiently approximated by using a "backward" temporal breadth-first search algorithm and a classical sampling technique. Our experimental results show that the approximation is excellent for nodes with high closeness, allowing us to detect them in practice in a fraction of the time needed for computing the exact closeness of all nodes. We validate our approach with an extensive set of experiments.																		1999-4893				SEP	2020	13	9							211	10.3390/a13090211													
J								An Image Hashing Algorithm for Authentication with Multi-Attack Reference Generation and Adaptive Thresholding	ALGORITHMS										reference hashing; adaptive thresholding; image authentication	TRANSFORM; PATTERN	Image hashing-based authentication methods have been widely studied with continuous advancements owing to the speed and memory efficiency. However, reference hash generation and threshold setting, which are used for similarity measures between original images and corresponding distorted version, are important but less considered by most of existing models. In this paper, we propose an image hashing method based on multi-attack reference generation and adaptive thresholding for image authentication. We propose to build the prior information set based on the help of multiple virtual prior attacks, and present a multi-attack reference generation method based on hashing clusters. The perceptual hashing algorithm was applied to the reference/queried image to obtain the hashing codes for authentication. Furthermore, we introduce the concept of adaptive thresholding to account for variations in hashing distance. Extensive experiments on benchmark datasets have validated the effectiveness of our proposed method.																		1999-4893				SEP	2020	13	9							227	10.3390/a13090227													
J								Solving the Urban Transit Routing Problem Using a Cat Swarm Optimization-Based Algorithm	ALGORITHMS										cat swarm optimization; urban transit routing problem; public transportation; real world application	NETWORK DESIGN; GENETIC ALGORITHM; SYSTEMS	Presented in this research paper is an attempt to apply a cat swarm optimization (CSO)-based algorithm to the urban transit routing problem (UTRP). Using the proposed algorithm, we can attain feasible and efficient (near) optimal route sets for public transportation networks. It is, to our knowledge, the first time that cat swarm optimization (CSO)-based algorithm is applied to cope with this specific problem. The algorithm's efficiency and excellent performance are demonstrated by conducting experiments with both real-world as well as artificial data. These specific data have also been used as test instances by other researchers in their publications. Computational results reveal that the proposed cat swarm optimization (CSO)-based algorithm exhibits better performance, using the same evaluation criteria, compared to most of the other existing approaches applied to the same test instances. The differences of the proposed algorithm in comparison with other published approaches lie in its main process, which is a modification of the classic cat swarm optimization (CSO) algorithm applied to solve the urban transit routing problem. This modification in addition to a variation of the initialization process, as well as the enrichment of the algorithm with a process of improving the final solution, constitute the innovations of this contribution. The UTRP is studied from both passenger and provider sides of interest, and the algorithm is applied in both cases according to necessary modifications.																		1999-4893				SEP	2020	13	9							223	10.3390/a13090223													
J								Study on Multi-Objective Optimization-Based Climate Responsive Design of Residential Building	ALGORITHMS										building climate responsive design; multi-objective optimization; energy saving optimal; cost-optimal	ENERGY-CONSUMPTION; OFFICE BUILDINGS; PERFORMANCE; ENVELOPE; RETROFIT; COST	This paper proposes an optimization process based on a parametric platform for building climate responsive design. Taking residential buildings in six typical American cities as examples, it proposes thermal environment comfort (Discomfort Hour, DH), building energy demand (BED) and building global cost (GC) as the objective functions for optimization. The design variables concern building orientation, envelope components, and window types, etc. The optimal solution is provided from two different perspectives of the public sector (energy saving optimal) and private households (cost-optimal) respectively. By comparing the optimization results with the performance indicators of the reference buildings in various cities, the outcome can give the precious indications to rebuild the U.S. residential buildings with a view to energy-efficiency and cost optimality depending on the location.																		1999-4893				SEP	2020	13	9							238	10.3390/a13090238													
J								A Class of Spline Functions for Solving 2-Order Linear Differential Equations with Boundary Conditions	ALGORITHMS										spline function; truncation error; convergence; second order differential equations	NUMERICAL-SOLUTION	In this paper, we exploit an numerical method for solving second order differential equations with boundary conditions. Based on the theory of the analytic solution, a series of spline functions are presented to find approximate solutions, and one of them is selected to approximate the solution automatically. Compared with the other methods, we only need to solve a tri-diagonal system, which is much easier to implement. This method has the advantages of high precision and less computational cost. The analysis of local truncation error is also discussed in this paper. At the end, some numerical examples are given to illustrate the effectiveness of the proposed method.																		1999-4893				SEP	2020	13	9							231	10.3390/a13090231													
J								Policy-Based Composition and Embedding of Extended Virtual Networks and SFCs for IIoT	ALGORITHMS										NFV; SFC; VNE; IIoT		The autonomic composition of Virtual Networks (VNs) and Service Function Chains (SFCs) based on application requirements is significant for complex environments. In this paper, we use graph transformation in order to compose an Extended Virtual Network (EVN) that is based on different requirements, such as locations, low latency, redundancy, and security functions. The EVN can represent physical environment devices and virtual application and network functions. We build a generic Virtual Network Embedding (VNE) framework for transforming an Application Request (AR) to an EVN. Subsequently, we define a set of transformations that reflect preliminary topological, performance, reliability, and security policies. These transformations update the entities and demands of the VN and add SFCs that include the required Virtual Network Functions (VNFs). Additionally, we propose a greedy proactive heuristic for path-independent embedding of the composed SFCs. This heuristic is appropriate for real complex environments, such as industrial networks. Furthermore, we present an Industrail Internet of Things (IIoT) use case that was inspired by Industry 4.0 concepts, in which EVNs for remote asset management are deployed over three levels; manufacturing halls and edge and cloud computing. We also implement the developed methods in Alevin and show exemplary mapping results from our use case. Finally, we evaluate the chain embedding heuristic while using a random topology that is typical for such a use case, and show that it can improve the admission ratio and resource utilization with minimal overhead.																		1999-4893				SEP	2020	13	9							240	10.3390/a13090240													
J								Perturbative-Iterative Computation of Inertial Manifolds of Systems of Delay-Differential Equations with Small Delays	ALGORITHMS										delay-differential equations; inertial manifold; functional iteration; small-delay expansion	STEADY-STATE APPROXIMATION; SLOW INVARIANT-MANIFOLDS; UNSTABLE MANIFOLDS; GENE-EXPRESSION; MODEL-REDUCTION; NITRIC-OXIDE; SIMPLIFICATION; MECHANISM; ENZYME; CONSTRUCTION	Delay-differential equations belong to the class of infinite-dimensional dynamical systems. However, it is often observed that the solutions are rapidly attracted to smooth manifolds embedded in the finite-dimensional state space, called inertial manifolds. The computation of an inertial manifold yields an ordinary differential equation (ODE) model representing the long-term dynamics of the system. Note in particular that any attractors must be embedded in the inertial manifold when one exists, therefore reducing the study of these attractors to the ODE context, for which methods of analysis are well developed. This contribution presents a study of a previously developed method for constructing inertial manifolds based on an expansion of the delayed term in small powers of the delay, and subsequent solution of the invariance equation by the Fraser functional iteration method. The combined perturbative-iterative method is applied to several variations of a model for the expression of an inducible enzyme, where the delay represents the time required to transcribe messenger RNA and to translate that RNA into the protein. It is shown that inertial manifolds of different dimensions can be computed. Qualitatively correct inertial manifolds are obtained. Among other things, the dynamics confined to computed inertial manifolds display Andronov-Hopf bifurcations at similar parameter values as the original DDE model.																		1999-4893				SEP	2020	13	9							209	10.3390/a13090209													
J								A Jacobi-Davidson Method for Large Scale Canonical Correlation Analysis	ALGORITHMS										canonical correlation analysis; Jacobi-Davidson; generalized eigenvalue problems; convergence	ALGORITHMS; CHEBYSHEV	In the large scale canonical correlation analysis arising from multi-view learning applications, one needs to compute canonical weight vectors corresponding to a few of largest canonical correlations. For such a task, we propose a Jacobi-Davidson type algorithm to calculate canonical weight vectors by transforming it into the so-called canonical correlation generalized eigenvalue problem. Convergence results are established and reveal the accuracy of the approximate canonical weight vectors. Numerical examples are presented to support the effectiveness of the proposed method.																		1999-4893				SEP	2020	13	9							229	10.3390/a13090229													
J								Spatially Adaptive Regularization in Image Segmentation	ALGORITHMS										image segmentation; spatially adaptive regularization; nonsmooth optimization; split bregman method	GRADIENT METHODS; PARAMETER; RESTORATION; ALGORITHMS; SELECTION; CUT	We present a total-variation-regularized image segmentation model that uses local regularization parameters to take into account spatial image information. We propose some techniques for defining those parameters, based on the cartoon-texture decomposition of the given image, on the mean and median filters, and on a thresholding technique, with the aim of preventing excessive regularization in piecewise-constant or smooth regions and preserving spatial features in nonsmooth regions. Our model is obtained by modifying a well-known image segmentation model that was developed by T. Chan, S. Esedoglu, and M. Nikolova. We solve the modified model by an alternating minimization method using split Bregman iterations. Numerical experiments show the effectiveness of our approach.																		1999-4893				SEP	2020	13	9							226	10.3390/a13090226													
J								Distributed Graph Diameter Approximation	ALGORITHMS										graph analytics; parallel graph algorithms; weighted graph decomposition; weighted diameter approximation; MapReduce	MAPREDUCE	We present an algorithm for approximating the diameter of massive weighted undirected graphs on distributed platforms supporting a MapReduce-like abstraction. In order to be efficient in terms of both time and space, our algorithm is based on a decomposition strategy which partitions the graph into disjoint clusters of bounded radius. Theoretically, our algorithm uses linear space and yields a polylogarithmic approximation guarantee; most importantly, for a large family of graphs, it features a round complexity asymptotically smaller than the one exhibited by a natural approximation algorithm based on the state-of-the-art Delta-stepping SSSP algorithm, which is its only practical, linear-space competitor in the distributed setting. We complement our theoretical findings with a proof-of-concept experimental analysis on large benchmark graphs, which suggests that our algorithm may attain substantial improvements in terms of running time compared to the aforementioned competitor, while featuring, in practice, a similar approximation ratio.																		1999-4893				SEP	2020	13	9							216	10.3390/a13090216													
J								Fast Spectral Approximation of Structured Graphs with Applications to Graph Filtering	ALGORITHMS										graph signal processing; graph Fourier transform; approximate graph Fourier transform; divide-and-conquer	SEPARATOR THEOREM; DESIGN	To analyze and synthesize signals on networks or graphs, Fourier theory has been extended to irregular domains, leading to a so-called graph Fourier transform. Unfortunately, different from the traditional Fourier transform, each graph exhibits a different graph Fourier transform. Therefore to analyze the graph-frequency domain properties of a graph signal, the graph Fourier modes and graph frequencies must be computed for the graph under study. Although to find these graph frequencies and modes, a computationally expensive, or even prohibitive, eigendecomposition of the graph is required, there exist families of graphs that have properties that could be exploited for an approximate fast graph spectrum computation. In this work, we aim to identify these families and to provide a divide-and-conquer approach for computing an approximate spectral decomposition of the graph. Using the same decomposition, results on reducing the complexity of graph filtering are derived. These results provide an attempt to leverage the underlying topological properties of graphs in order to devise general computational models for graph signal processing.																		1999-4893				SEP	2020	13	9							214	10.3390/a13090214													
J								R2D2: A Dbpedia Chatbot Using Triple-Pattern Like Queries	ALGORITHMS										chatbots; DBpedia; conversation agents		Chatbots, also known as conversation agents, are programs that are able to simulate and reproduce an intelligent conversation with humans. Although this type of program is not new, the explosion of the available information and the rapid increase of the users seeking this information have renewed the interest in their development. In this paper, we present R2D2, an intelligent chatbot relying on semantic web technologies and offering an intelligent controlled natural language interface for accessing the information available in DBpedia. The chatbot accepts structured input, allowing users to enter triple-pattern like queries, which are answered by the underlying engine. While typing, an auto-complete service guides users on creating the triple patterns, suggesting resources available in the DBpedia. Based on user input (in the form of triple-pattern like queries), the corresponding SPARQL queries are automatically formulated. The queries are submitted to the corresponding DBpedia SPARQL endpoint, and then the result is received by R2D2 and augmented with maps and visuals and eventually presented to the user. The usability evaluation performed shows the advantages of our solution and its usefulness.																		1999-4893				SEP	2020	13	9							217	10.3390/a13090217													
J								A Simheuristic Algorithm for Solving the Stochastic Omnichannel Vehicle Routing Problem with Pick-up and Delivery	ALGORITHMS										omnichannel retail stores; vehicle routing problem; pick-up and delivery; biased-randomized heuristics; simheuristics	LARGE NEIGHBORHOOD SEARCH; TIME WINDOWS; GENETIC ALGORITHM; SIMULATION; MULTICHANNEL; INTELLIGENCE; OPTIMIZATION; SYSTEMS; BRANCH; CROSS	Advances in information and communication technologies have made possible the emergence of new shopping channels. The so-called 'omnichannel' retailing mode allows customers to shop for products online and receive them at home. This paper focuses on the omnichannel delivery concept for the retailing industry, which addresses the replenishment of a set of retail stores and the direct shipment of the products to customers within an integrated vehicle routing formulation. Due to itsNP-Hardness, a constructive heuristic, which is extended into a biased-randomized heuristic and which is embedded into a multi-start procedure, is introduced for solving the large-sized instances of the problem. Next, the problem is enriched by considering a more realistic scenario in which travel times are modeled as random variables. For dealing with the stochastic version of the problem, a simheuristic algorithm is proposed. A series of computational experiments contribute to illustrate how our simheuristic can provide reliable and low-cost solutions under uncertain conditions.																		1999-4893				SEP	2020	13	9							237	10.3390/a13090237													
J								Relaxed Rule-Based Learning for Automated Predictive Maintenance: Proof of Concept	ALGORITHMS										Predictive Maintenance; failure prediction; rule learning; Decision Tree; Machine Learning	FAULT-DIAGNOSIS; ALGORITHMS; ENSEMBLE	In this paper we propose a novel approach of rule learning called Relaxed Separate-and- Conquer (RSC): a modification of the standard Separate-and-Conquer (SeCo) methodology that does not require elimination of covered rows. This method can be seen as a generalization of the methods of SeCo and weighted covering that does not suffer from fragmentation. We present an empirical investigation of the proposed RSC approach in the area of Predictive Maintenance (PdM) of complex manufacturing machines, to predict forthcoming failures of these machines. In particular, we use for experiments a real industrial case study of a Continuous Compression Moulding (CCM) machine which manufactures the plastic bottle closure (caps) in the beverage industry. We compare the RSC approach with a Decision Tree (DT) based and SeCo algorithms and demonstrate that RSC significantly outperforms both DT based and SeCo rule learners. We conclude that the proposed RSC approach is promising for PdM guided by rule learning.																		1999-4893				SEP	2020	13	9							219	10.3390/a13090219													
J								Detecting Traffic Incidents Using Persistence Diagrams	ALGORITHMS										persistence diagram; bottleneck distance; anomaly detection; bagging; incident detection	TOPOLOGICAL DATA-ANALYSIS; HOMOLOGY; ROUTE	We introduce a novel methodology for anomaly detection in time-series data. The method uses persistence diagrams and bottleneck distances to identify anomalies. Specifically, we generate multiple predictors by randomly bagging the data (reference bags), then for each data point replacing the data point for a randomly chosen point in each bag (modified bags). The predictors then are the set of bottleneck distances for the reference/modified bag pairs. We prove the stability of the predictors as the number of bags increases. We apply our methodology to traffic data and measure the performance for identifying known incidents.																		1999-4893				SEP	2020	13	9							222	10.3390/a13090222													
J								A Survey on Shortest Unique Substring Queries	ALGORITHMS										string algorithms; shortest unique substring; repeats; compact data structures	PALINDROME; ALGORITHM; ALIGNMENT	The shortest unique substring (SUS) problem is an active line of research in the field of string algorithms and has several applications in bioinformatics and information retrieval. The initial version of the problem was proposed by Pei et al. [ICDE'13]. Over the years, many variants and extensions have been pursued, which include positional-SUS, interval-SUS, approximate-SUS, palindromic-SUS, range-SUS, etc. In this article, we highlight some of the key results and summarize the recent developments in this area.																		1999-4893				SEP	2020	13	9							224	10.3390/a13090224													
J								A Hybrid Genetic Algorithm-Based Fuzzy Markovian Model for the Deterioration Modeling of Healthcare Facilities	ALGORITHMS										fuzzy logic; Markov chain; deterioration prediction; genetic algorithm; healthcare facilities	MANAGEMENT	Healthcare facilities are constantly deteriorating due to tight budgets allocated to the upkeep of building assets. This entails the need for improved deterioration modeling of such buildings in order to enforce a predictive maintenance approach that decreases the unexpected occurrence of failures and the corresponding downtime elapsed to repair or replace the faulty asset components. Currently, hospitals utilize subjective deterioration prediction methodologies that mostly rely on age as the sole indicator of degradation to forecast the useful lives of the building components. Thus, this paper aims at formulating a more efficient stochastic deterioration prediction model that integrates the latest observed condition into the forecasting procedure to overcome the subjectivity and uncertainties associated with the currently employed methods. This is achieved by means of developing a hybrid genetic algorithm-based fuzzy Markovian model that simulates the deterioration process given the scarcity of available data demonstrating the condition assessment and evaluation for such critical facilities. A nonhomogeneous transition probability matrix (TPM) based on fuzzy membership functions representing the condition, age and relative deterioration rate of the hospital systems is utilized to address the inherited uncertainties. The TPM is further calibrated by means of a genetic algorithm to circumvent the drawbacks of the expert-based models. A sensitivity analysis was carried out to analyze the possible changes in the output resulting from predefined modifications to the input parameters in order to ensure the robustness of the model. The performance of the deterioration prediction model developed is then validated through a comparison with a state-of-art stochastic model in contrast to real hospital datasets, and the results obtained from the developed model significantly outperformed the long-established Weibull distribution-based deterioration prediction methodology with mean absolute errors of 1.405 and 9.852, respectively. Therefore, the developed model is expected to assist decision-makers in creating more efficient maintenance programs as well as more data-driven capital renewal plans.																		1999-4893				SEP	2020	13	9							210	10.3390/a13090210													
J								Simulated Annealing with Exploratory Sensing for Global Optimization	ALGORITHMS										simulated annealing; exploration; intensification; sensing search; search memory	REAL-PARAMETER OPTIMIZATION; PARTICLE SWARM OPTIMIZER; CODED GENETIC ALGORITHMS; CMA EVOLUTION STRATEGY; DIFFERENTIAL EVOLUTION; COLONY OPTIMIZATION; SEARCH; PERFORMANCE; MEMORY	Simulated annealing is a well-known search algorithm used with success history in many search problems. However, the random walk of the simulated annealing does not benefit from the memory of visited states, causing excessive random search with no diversification history. Unlike memory-based search algorithms such as the tabu search, the search in simulated annealing is dependent on the choice of the initial temperature to explore the search space, which has little indications of how much exploration has been carried out. The lack of exploration eye can affect the quality of the found solutions while the nature of the search in simulated annealing is mainly local. In this work, a methodology of two phases using an automatic diversification and intensification based on memory and sensing tools is proposed. The proposed method is called Simulated Annealing with Exploratory Sensing. The computational experiments show the efficiency of the proposed method in ensuring a good exploration while finding good solutions within a similar number of iterations.																		1999-4893				SEP	2020	13	9							230	10.3390/a13090230													
J								Low-Power FPGA Implementation of Convolution Neural Network Accelerator for Pulse Waveform Classification	ALGORITHMS										convolution neural network (CNN); traditional Chinese medicine (TCM); pulse waveform classification; field-programmable gate array (FPGA)	DIAGNOSIS; PRESSURE; SYSTEM	In pulse waveform classification, the convolution neural network (CNN) shows excellent performance. However, due to its numerous parameters and intensive computation, it is challenging to deploy a CNN model to low-power devices. To solve this problem, we implement a CNN accelerator based on a field-programmable gate array (FPGA), which can accurately and quickly infer the waveform category. By designing the structure of CNN, we significantly reduce its parameters on the premise of high accuracy. Then the CNN is realized on FPGA and optimized by a variety of memory access optimization methods. Experimental results show that our customized CNN has high accuracy and fewer parameters, and the accelerator costs only 0.714 W under a working frequency of 100 MHz, which proves that our proposed solution is feasible. Furthermore, the accelerator classifies the pulse waveform in real time, which could help doctors make the diagnosis.																		1999-4893				SEP	2020	13	9							213	10.3390/a13090213													
J								Feasibility Analysis and Application of Reinforcement Learning Algorithm Based on Dynamic Parameter Adjustment	ALGORITHMS										reinforcement learning; control system; parameter adjustment		Reinforcement learning, as a branch of machine learning, has been gradually applied in the control field. However, in the practical application of the algorithm, the hyperparametric approach to network settings for deep reinforcement learning still follows the empirical attempts of traditional machine learning (supervised learning and unsupervised learning). This method ignores part of the information generated by agents exploring the environment contained in the updating of the reinforcement learning value function, which will affect the performance of the convergence and cumulative return of reinforcement learning. The reinforcement learning algorithm based on dynamic parameter adjustment is a new method for setting learning rate parameters of deep reinforcement learning. Based on the traditional method of setting parameters for reinforcement learning, this method analyzes the advantages of different learning rates at different stages of reinforcement learning and dynamically adjusts the learning rates in combination with the temporal-difference (TD) error values to achieve the advantages of different learning rates in different stages to improve the rationality of the algorithm in practical application. At the same time, by combining the Robbins-Monro approximation algorithm and deep reinforcement learning algorithm, it is proved that the algorithm of dynamic regulation learning rate can theoretically meet the convergence requirements of the intelligent control algorithm. In the experiment, the effect of this method is analyzed through the continuous control scenario in the standard experimental environment of "Car-on-The-Hill" of reinforcement learning, and it is verified that the new method can achieve better results than the traditional reinforcement learning in practical application. According to the model characteristics of the deep reinforcement learning, a more suitable setting method for the learning rate of the deep reinforcement learning network proposed. At the same time, the feasibility of the method has been proved both in theory and in the application. Therefore, the method of setting the learning rate parameter is worthy of further development and research.																		1999-4893				SEP	2020	13	9							239	10.3390/a13090239													
J								A Comparison of Ensemble and Dimensionality Reduction DEA Models Based on Entropy Criterion	ALGORITHMS										data envelopment analysis; dimensionality reduction; ensembles; exhaustive state space search; entropy	DATA ENVELOPMENT ANALYSIS; EFFICIENCY ESTIMATION; TECHNICAL EFFICIENCY; VARIABLES; SELECTION; INPUTS	Dimensionality reduction research in data envelopment analysis (DEA) has focused on subjective approaches to reduce dimensionality. Such approaches are less useful or attractive in practice because a subjective selection of variables introduces bias. A competing unbiased approach would be to use ensemble DEA scores. This paper illustrates that in addition to unbiased evaluations, the ensemble DEA scores result in unique rankings that have high entropy. Under restrictive assumptions, it is also shown that the ensemble DEA scores are normally distributed. Ensemble models do not require any new modifications to existing DEA objective functions or constraints, and when ensemble scores are normally distributed, returns-to-scale hypothesis testing can be carried out using traditional parametric statistical techniques.																		1999-4893				SEP	2020	13	9							232	10.3390/a13090232													
J								Hierarchical Fuzzy Expert System for Organizational Performance Assessment in the Construction Industry	ALGORITHMS										organization performance; analytic hierarchy process; hierarchical fuzzy expert system; construction industry	BENCHMARKING; INNOVATION; IMPLEMENTATION; PERSPECTIVE; MANAGEMENT	Organizations have been trying to increase their efficiency and improve their performance in order to achieve their goals. Various factors determine organizational success. The construction industry is a project-based industry which is exceptionally dynamic. The need to identify the weak points and search for solutions to improve the performance of the construction organization is extremely crucial. The industry has always focused on the measure of project success. Previous research works have primarily focused on the measurement of financial or tangible assets. However, there is a lack of understanding of qualitative factors and their combined effect on organizational performance. Therefore, the objectives of this paper are to identify and study the success factors-both financial and non-financial factors. The potential success factors are collected from the literature review and construction experts through a questionnaire to evaluate their effect on organizational performance. The collected data have been analyzed using the Analytic Hierarchy Process (AHP) to shortlist the critical success factors. Thereafter, the Hierarchical Fuzzy Expert System has been used to build a prediction model based on the selected factors. The developed research/model benefits both researchers and practitioners to predict accurate company performance.																		1999-4893				SEP	2020	13	9							205	10.3390/a13090205													
J								A Two-Phase Approach for Semi-Supervised Feature Selection	ALGORITHMS										feature selection; semi-supervised datasets; classification; clustering; correlation	RECOGNITION	This paper proposes a novel approach for selecting a subset of features in semi-supervised datasets where only some of the patterns are labeled. The whole process is completed in two phases. In the first phase, i.e., Phase-I, the whole dataset is divided into two parts: The first part, which contains labeled patterns, and the second part, which contains unlabeled patterns. In the first part, a small number of features are identified using well-known maximum relevance (from first part) and minimum redundancy (whole dataset) based feature selection approaches using the correlation coefficient. The subset of features from the identified set of features, which produces a high classification accuracy using any supervised classifier from labeled patterns, is selected for later processing. In the second phase, i.e., Phase-II, the patterns belonging to the first and second part are clustered separately into the available number of classes of the dataset. In the clusters of the first part, take the majority of patterns belonging to a cluster as the class for that cluster, which is given already. Form the pairs of cluster centroids made in the first and second part. The centroid of the second part nearest to a centroid of the first part will be paired. As the class of the first centroid is known, the same class can be assigned to the centroid of the cluster of the second part, which is unknown. The actual class of the patterns if known for the second part of the dataset can be used to test the classification accuracy of patterns in the second part. The proposed two-phase approach performs well in terms of classification accuracy and number of features selected on the given benchmarked datasets.																		1999-4893				SEP	2020	13	9							215	10.3390/a13090215													
J								Online Topology Inference from Streaming Stationary Graph Signals with Partial Connectivity Information	ALGORITHMS										network topology inference; graph signal processing; proximal gradient algorithm; online optimization; streaming data	THRESHOLDING ALGORITHM; OPTIMIZATION	We develop online graph learning algorithms from streaming network data. Our goal is to track the (possibly) time-varying network topology, and affect memory and computational savings by processing the data on-the-fly as they are acquired. The setup entails observations modeled as stationary graph signals generated by local diffusion dynamics on the unknown network. Moreover, we may have a priori information on the presence or absence of a few edges as in the link prediction problem. The stationarity assumption implies that the observations' covariance matrix and the so-called graph shift operator (GSO-a matrix encoding the graph topology) commute under mild requirements. This motivates formulating the topology inference task as an inverse problem, whereby one searches for a sparse GSO that is structurally admissible and approximately commutes with the observations' empirical covariance matrix. For streaming data, said covariance can be updated recursively, and we show online proximal gradient iterations can be brought to bear to efficiently track the time-varying solution of the inverse problem with quantifiable guarantees. Specifically, we derive conditions under which the GSO recovery cost is strongly convex and use this property to prove that the online algorithm converges to within a neighborhood of the optimal time-varying batch solution. Numerical tests illustrate the effectiveness of the proposed graph learning approach in adapting to streaming information and tracking changes in the sought dynamic network.																		1999-4893				SEP	2020	13	9							228	10.3390/a13090228													
J								Dimensional Synthesis for Multi-Linkage Robots Based on a Niched Pareto Genetic Algorithm	ALGORITHMS										niched Pareto genetic algorithm; multi-linkage robots; dimensional synthesis; density function; maneuverability; energy expenditure	OPTIMIZATION	The dimensional synthesis of multi-linkage robots has great significance for improving flexibility and efficiency. With the increase of the degree of freedom and restrictions on special occasions, the solution of dimensional synthesis becomes complicated and time-consuming. Theory of workspace density function, maneuverability, and energy expenditure had been studied. With high flexibility and low energy consumption as the design goal, the method for dimensional and joint angle synthesis of multi-linkage robots was proposed based on a niched Pareto genetic algorithm. The Pareto solution set has been obtained. The method was verified by two application examples, which is occlusion of the solar salt evaporation pool and the secondary scattering of solid 2,2 '-azobis(2,4-dimethylvaleronitrile). Through the application of NPGA (niched Pareto genetic algorithm) compared with KPCA (kernel principal component analysis), it can save 12.37% time in occlusion of one evaporating pool and reduce energy consumption by 3.85%; it can save 9.96% time in scattering of remain materials per barrel and reduce energy consumption by 1.77%. The study reduces the labor intensity of manual workers in the salt making industry, ensures the safe production of dangerous chemicals, and provides new ideas and methods for the dimensional synthesis of multi-linkage robots.																		1999-4893				SEP	2020	13	9							203	10.3390/a13090203													
J								Mining arguments in scientific abstracts with discourse-level embeddings	DATA & KNOWLEDGE ENGINEERING												Argument mining consists in the automatic identification of argumentative structures in texts. In this work we leverage existing discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. We propose a new annotation schema and use it to augment a corpus of computational linguistics abstracts that had previously been annotated with discourse units and relations. Our initial experiments with the enriched corpus confirm the potential value of incorporating discourse information in argument mining tasks. In order to tackle the limitations posed by the lack of corpora containing both discourse and argumentative annotations we explore two transfer learning approaches in which discourse parsing is used as an auxiliary task when training argument mining models. In this case, as no discourse information is used as input, the resulting models could be used to predict the argumentative structure of unannotated texts.																	0169-023X	1872-6933				SEP	2020	129								101840	10.1016/j.datak.2020.101840													
J								Natural logic knowledge bases and their graph form	DATA & KNOWLEDGE ENGINEERING										Natural Logic; Knowledge management applications; Ontologies; Query; Metalogic; Bioinformatics databases		This paper describes how knowledge bases can be represented in and reasoned with in natural logic. Natural logic is a regimented fragment of natural language possessing a well-defined logical semantics. As such, natural logic may be considered an attractive alternative among the various knowledge representation logics such as description logics. Our version of natural logic expands formal ontologies with affirmative propositions expressing a variety of relationships between concepts. It comprises (nested) restrictive relative clauses and prepositional phrases and, as a new construct, adverbial prepositional phrases. The natural logic knowledge base is to be used for deductive query answering applying inference rules. This is facilitated by introduction of DATALOG as an embedding meta-logic. The inference rules are stated in DATALOG and act directly on the natural logic formulations. The knowledge base propositions are decomposed into a graph form enabling path finding between concepts. The examples in the paper are derived from text source life-science descriptions.																	0169-023X	1872-6933				SEP	2020	129								101848	10.1016/j.datak.2020.101848													
J								Design and implementation of ETL processes using BPMN and relational algebra	DATA & KNOWLEDGE ENGINEERING										Data Warehousing; OLAP; ETL; BPMN		Extraction, transformation, and loading (ETL) processes are used to extract data from internal and external sources of an organization, transform these data, and load them into a data warehouse. The Business Process Modeling and Notation (BPMN) has been proposed for expressing ETL processes at a conceptual level. A different approach is studied in this paper, where relational algebra (RA), extended with update operations, is used for specifying ETL processes. In this approach, data tasks in an ETL workflow can be automatically translated into SQL queries to be executed over a DBMS. To illustrate this study, the paper addresses the problem of updating Slowly Changing Dimensions (SCDs) with dependencies, that is, the case when updating a SCD table impacts on associated SCD tables. Tackling this problem requires extending the classic RA with update operations. The paper also shows the implementation of a portion of the TPC-DI benchmark that results from both approaches. Thus, the paper presents three implementations: (a) An SQL implementation based on the extended RA-based specification of an ETL process expressed in BPMN4ETL; and (b) Two implementations of workflows that follow from BPMN4ETL, one that uses the Pentaho DI tool, and another one that uses Talend Open Studio for DI. Experiments over these implementations of the TPC-DI benchmark for different scale factors were carried out, and are described and discussed in the paper, showing that the extended RA approach results in more efficient processes than the ones produced by implementing the BPMN4ETL specification over the mentioned ETL tools. The reasons for this result are also discussed.																	0169-023X	1872-6933				SEP	2020	129								101837	10.1016/j.datak.2020.101837													
J								Mo.Re.Farming: A hybrid architecture for tactical and strategic precision agriculture	DATA & KNOWLEDGE ENGINEERING										BI 2.0; Precision agriculture; Data integration	BIG DATA; DESIGN; SYSTEM	In this paper we propose an innovative architecture, called Mo.Re.Farming, for handling agricultural data in an integrated fashion and supporting decision making in the precision agriculture domain. This architecture is oriented to data analysis and is inspired by Business Intelligence 2.0 approaches. It is hybrid in that it couples traditional and big data technologies to integrate heterogeneous data, at different levels of detail, from several owned and open data sources; its goal is to demonstrate that such integration is feasible and beneficial in supporting situ-specific and large-scale analyses. The proposed architecture has been developed in the context of the Mo.Re.Farming project, aimed at providing a Decision Support System for agricultural technicians in the Emilia-Romagna region and to enable analyses related to the use of water and chemical resources. The architecture is fully deployed and serves as a hub for agricultural data in Emilia-Romagna; the integrated data are made available in open access mode and can be accessed through web interfaces and through a set of web services. The paper describes the architecture from the technological and functional points of view and discusses the Mo.Re.Farming project outcomes and lessons learnt.																	0169-023X	1872-6933				SEP	2020	129								101836	10.1016/j.datak.2020.101836													
J								An analytical model for information gathering and propagation in social networks using random graphs	DATA & KNOWLEDGE ENGINEERING										Social networks; Data models; Information gathering; Data sharing; Business intelligence; Node discovery in graphs	ALGORITHMS	In this paper, we propose an analytical model for information gathering and propagation in social networks using random sampling. We represent the social network using the Erdos-Renyi model of the random graph. When a given node is selected in the social network, information about itself and all of its neighbors are obtained and these nodes are considered to be discovered. We provide an analytical solution for the expected number of nodes that are discovered as a function of the number of nodes randomly sampled in the graph. We use the concepts of combinatorics, probability, and inclusion-exclusion principle for computing the number of discovered nodes. This is a computationally-intensive problem with combinatorial complexity. This model is useful when crawling and mining of the social network graph is prohibited. Our work finds application in several important real-world decision support scenarios such as survey sample selection, construction of public directory, and crowdsourced databases using social networks, targeted advertising, and recommendation systems. It can also be used for finding a randomized dominating set of a graph that finds applications in computer networks, document summarization, and biological networks. We have evaluated the performance both analytically as well as by means of simulation, and the results are comparable. The results have an accuracy of around 96% for random graphs and above 87% for the power-law graphs.																	0169-023X	1872-6933				SEP	2020	129								101852	10.1016/j.datak.2020.101852													
J								Optimal Consensus Recovery of Multi-agent System Subjected to Agent Failure	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Multi-agent system; consensus recovery; algebraic connectivity; Laplacian	FLOCKING; NETWORKS; ALGORITHMS; STATE	Multi-Agent Systems are susceptible to external disturbances, sensor failures or collapse of communication channel/media. Such failures disconnect the agent network and thereby hamper the consensus of the system. Quick recovery of consensus is vital to continue the normal operation of an agent-based system. However, only limited works in the past have investigated the problem of recovering the consensus of an agent-based system in the event of a failure. This work proposes a novel algorithmic approach to recover the lost consensus, when an agent-based system is subject to the failure of an agent. The main focus of the algorithm is to reconnect the multi-agent network in a way so as to increase the connectivity of the network, post recovery. The proposed algorithm may be applied to both linear and non-linear continuous-time consensus protocols. To verify the efficiency of the proposed algorithm, it has been applied and tested on two multi-agent networks. The results, thus obtained, have been compared with other state-of-the-art recovery algorithms. Finally, it has been established that the proposed algorithm achieves better connectivity and therefore, faster consensus when compared to the other state-of-the-art.																	0218-2130	1793-6349				SEP	2020	29	6							2050017	10.1142/S0218213020500177													
J								A Novel Online Change Point Detection Using an Approximate Random Blanket and the Line Process Energy	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Change point; machine learning; data mining; Markov random fields; edge detection; binary classification	JUMP-SPARSE; RECONSTRUCTION; MINIMIZATION; ALGORITHMS	In this paper, we focus on the problem of change point detection in piecewise constant signals. This problem is central to several applications such as human activity analysis, speech or image analysis and anomaly detection in genetics. We present a novel window-sliding algorithm for an online change point detection. The proposed approach considers a local blanket of a global Markov Random Field (MRF) representing the signal and its noisy observation. For each window, we define and solve the local energy minimization problem to deduce the gradient on each edge of the MRF graph. The gradient is then processed by an activation function to filter the weak features and produce the final jumps. We demonstrate the effectiveness of our method by comparing its running time and several detection metrics with state of the art algorithms.																	0218-2130	1793-6349				SEP	2020	29	6							2050018	10.1142/S0218213020500189													
J								A Holistic Approach for Automatic Deep Understanding and Protection of Technical Documents	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Technical document processing; natural language understanding; system diagram understanding; system simulator; stochastic petri-nets modeling	DESIGN	A Technical Document (TD) is mainly composed by a set of modalities appropriately structured and associated. These modalities could be NL-text, block diagrams, formulas, tables, graphics, pictures etc. A deep understanding of a TD will be based on the synergistic understanding and associations of these modalities. This paper offers a novel methodology for the implementation of a holistic approach for deep understanding of technical documents by understanding and associating these modalities. This approach is based on the homogeneous expression (mapping) of the technical document modalities into the same medium, which in this case is the Stochastic Petri-nets (SPN). Then, these modalities are associated to each other generating new knowledge about the technical document topic and a SPN simulator is created to offer additional information about the functional behavior of the system described in the document. Some results from our studies are provided to prove the overall concept.																	0218-2130	1793-6349				SEP	2020	29	6							2050007	10.1142/S0218213020500074													
J								Analytical and Simple Form of Shrinkage Functions for Non-Convex Penalty Functions in Fused Lasso Algorithm	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Fused lasso method; non-convex penalty function; wavelet transform	WAVELET; RECONSTRUCTION; REGULARIZATION	In some circumstances, the performance of machine learning (ML) tasks are based on the quality of signal (data) that is processed in these tasks. Therefore, the pre-processing techniques, such as reconstruction and denoising methods, are important techniques in ML tasks. In reconstructed (estimated) method, the fused lasso algorithm with non-convex penalty function is an efficient method when the signal corrupted by additive white Gaussian noise (AWGN) is considered. Therefore, this paper proposes new shrinkage functions for non-convex penalty functions, modified arctangent and exponential models, in fused lasso formulation. A lot of works present the shrinkage function for arctangent penalty function. Unfortunately, there is no closed-form solution. The numerical solution is required for shrinkage function of this penalty function. However, the analytical solution is derived in this paper. Moreover, the shrinkage function of modified exponential penalty function is proposed. This shrinkage function obtains from simple iterative method, fixed-point algorithm. We demonstrate the proposed methods through simulations with standard one-dimensional signals contaminated by AWGN. The proposed techniques are compared with traditional estimation methods, such as total variation (TV) and wavelet denoising methods. In experimental results, our proposed methods outperform several exiting methods both visual quality and in terms of root mean square error (RMSE). In fact, the proposed methods can better preserve the feature of noise-free signal than the compared methods. The denoised signals produced by the proposed methods are less smooth than the denoised signals produced by the compared methods.																	0218-2130	1793-6349				SEP	2020	29	6							2050020	10.1142/S0218213020500207													
J								A Hybrid Concession Mechanism for Negotiating Software Agents in Competitive Environments	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Multiagent systems; concession strategy; automated negotiation; imitation mechanisms	AUTOMATED NEGOTIATION; STRATEGY; SINGLE; MODEL	This paper presents a new hybrid concession mechanism for negotiating agents. It considers both the current concession behavior of the proposing agent and the concession offered by its opponent in the last counteroffer to create a new offer. The proposed mechanism is a kind of imitating offer generation tactic. The difference is that it uses the first order difference between the two last counteroffers received from the opponent as its current reservation value which is one of the important inputs in generating a new offer. In this paper, a bilateral negotiation over a single issue is considered where agents have adverse interests over the issue such as price. Four negotiation environmental settings are used to test the proposed offer generating mechanism. The experimental results show that the proposed hybrid concession mechanism outperforms the time-dependent concession tactic in terms of utility rate while performing lower in one negotiation environment and similarly in most of negotiation environments.																	0218-2130	1793-6349				SEP	2020	29	6							2050016	10.1142/S0218213020500165													
J								A Persian Medical Question Answering System	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Medical question answering; Persian language processing; question processing		A question answering system is a type of information retrieval that takes a question from a user in natural language as the input and returns the best answer to it as the output. In this paper, a medical question answering system in the Persian language is designed and implemented. During this research, a dataset of diseases and drugs is collected and structured. The proposed system includes three main modules: question processing, document retrieval, and answer extraction. For the question processing module, a sequential architecture is designed which retrieves the main concept of a question by using different components. In these components, rule-based methods, natural language processing, and dictionary-based techniques are used. In the document retrieval module, the documents are indexed and searched using the Lucene library. The retrieved documents are ranked using similarity detection algorithms and the highest-ranked document is selected to be used by the answer extraction module. This module is responsible for extracting the most relevant section of the text in the retrieved document. During this research, different customized language processing tools such as part of speech tagger and lemmatizer are also developed for Persian. Evaluation results show that this system performs well for answering different questions about diseases and drugs. The accuracy of the system for 500 sample questions is 83.6%.																	0218-2130	1793-6349				SEP	2020	29	6							2050019	10.1142/S0218213020500190													
J								CLASSIFICATION OF HIGH RESOLUTION SATELLITE IMAGES USING IMPROVED U-NET	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										satellite image classification; deep learning; U-Net; spatial pyramid pooling	CONVOLUTIONAL NEURAL-NETWORKS	Satellite image classification is essential for many socio-economic and environmental applications of geographic information systems, including urban and regional planning, conservation and management of natural resources, etc. In this paper, we propose a deep learning architecture to perform the pixel-level understanding of high spatial resolution satellite images and apply it to image classification tasks. Specifically, we augment the spatial pyramid pooling module with image-level features encoding the global context, and integrate it into the U- Net structure. The proposed model solves the problem consisting in the fact that U-Net tends to lose object boundaries after multiple pooling operations. In our experiments, two public datasets are used to assess the performance of the proposed model. Comparison with the results from the published algorithms demonstrates the effectiveness of our approach.																	1641-876X	2083-8492				SEP	2020	30	3					399	413		10.34768/amcs-2020-0030													
J								IMPLEMENTATION AND EVALUATION OF MEDICAL IMAGING TECHNIQUES BASED ON CONFORMAL GEOMETRIC ALGEBRA	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										medical image segmentation; medical image registration; computational geometry; Clifford algebra; conformal geometric algebra	REGISTRATION; SEGMENTATION	Medical imaging tasks, such as segmentation, 3D modeling, and registration of medical images, involve complex geometric problems, usually solved by standard linear algebra and matrix calculations. In the last few decades, conformal geometric algebra (CGA) has emerged as a new approach to geometric computing that offers a simple and efficient representation of geometric objects and transformations. However, the practical use of CGA-based methods for big data image processing in medical imaging requires fast and efficient implementations of CGA operations to meet both real-time processing constraints and accuracy requirements. The purpose of this study is to present a novel implementation of CGA-based medical imaging techniques that makes them effective and practically usable. The paper exploits a new simplified formulation of CGA operators that allows significantly reduced execution times while maintaining the needed result precision. We have exploited this novel CGA formulation to re-design a suite of medical imaging automatic methods, including image segmentation, 3D reconstruction and registration. Experimental tests show that the re-formulated CGA-based methods lead to both higher precision results and reduced computation times, which makes them suitable for big data image processing applications. The segmentation algorithm provides the Dice index, sensitivity and specificity values of 98.14%, 98.05% and 97.73%, respectively, while the order of magnitude of the errors measured for the registration methods is 10(-5).																	1641-876X	2083-8492				SEP	2020	30	3					415	433		10.34768/amcs-2020-0031													
J								AN INTELLIGENT MULTIMODAL FRAMEWORK FOR IDENTIFYING CHILDRENWITH AUTISM SPECTRUM DISORDER	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										autism spectrum disorder; eye fixation; facial expression; cognitive level; improved random forest	YOUNG-CHILDREN; INDIVIDUALS; ATTENTION; RECOGNITION; ADULTS	Early identification can significantly improve the prognosis of children with autism spectrum disorder (ASD). Yet existing identification methods are costly, time consuming, and dependent on the manual judgment of specialists. In this study, we present a multimodal framework that fuses data on a child's eye fixation, facial expression, and cognitive level to automatically identify children with ASD, to improve the identification efficiency and reduce costs. The proposed methodology uses an optimized random forest (RF) algorithm to improve classification accuracy and then applies a hybrid fusion method based on the data source and time synchronization to ensure the reliability of the classification results. The classification accuracy of the framework was 91%, which is higher than that of the RF, support vector machine, and discriminant analysis methods. The results suggest that data on a child's eye fixation, facial expression, and cognitive level are useful for identifying children with ASD. Because the proposed framework can separate ASD children from typically developing (TD) children, it can facilitate the early identification of ASD and may improve intervention programs for children with ASD.																	1641-876X	2083-8492				SEP	2020	30	3					435	448		10.34768/amcs-2020-0032													
J								MATHEMATICAL METHODS OF SIGNAL ANALYSIS APPLIED IN MEDICAL DIAGNOSTIC	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										classification; decision support system; signal filtering; data fusion; temporal analysis	DEEP BRAIN-STIMULATION; SUBTHALAMIC NUCLEUS; PARKINSON-DISEASE; CLASSIFICATION; SUBTERRITORIES; RECORDINGS	Digital signal processing, such as filtering, information extraction, and fusion of various results, is currently an integral part of advanced medical therapies. It is especially important in neurosurgery during deep-brain stimulation procedures. In such procedures, the surgical target is accessed using special electrodes while not being directly visible. This requires very precise identification of brain structures in 3D space throughout the surgery. In the case of deep-brain stimulation surgery for Parkinson's disease (PD), the target area-the subthalamic nucleus (STN)-is located deep within the brain. It is also very small (just a few millimetres across), which makes this procedure even more difficult. For this reason, various signals are acquired, filtered, and finally fused, to provide the neurosurgeon with the exact location of the target. These signals come from preoperative medical imaging (such as MRI and CT), and from recordings of brain activity carried out during surgery using special brain-implanted electrodes. Using the method described in this paper, it is possible to construct a decision-support system that, during surgery, analyses signals recorded within the patient's brain and classifies them as recorded within the STN or not. The constructed classifier discriminates signals with a sensitivity of 0.97 and a specificity of 0.96. The described algorithm is currently used for deep-brain stimulation surgeries among PD patients.																	1641-876X	2083-8492				SEP	2020	30	3					449	462		10.34768/amcs-2020-0033													
J								RECOGNITION OF SPECIES AND GENERA OF BACTERIA BY MEANS OF THE PRODUCT OF WEIGHTS OF THE CLASSIFIERS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										pattern recognition; recognition of bacterial cells; classifiers; product of weights of the classifiers	IDENTIFICATION; SYSTEM	In microbiology, computer methods are applied in the analysis and recognition of laboratory-acquired microscopic images concerning, for example, bacterial cells or other microorganisms. Proper recognition of the species and genera of bacteria is a key stage in the microbiological diagnostics process, because it allows a quick start of the appropriate therapy. The original method proposed in the paper concerns the automatic recognition of selected species and genera of bacteria presented in digital images. The classification was made on the basis of the analysis of the physical characteristics of bacterial cells using the product of classifier confidence weights. The end result of the classification process is the classification list, sorted in descending order according to the weights of the classifiers. In addition to the correct classification, a list of other possible results of the analysis is obtained. The method thus allows not only the classification, but also an analysis of the confidence level of the selection made. The proposed method can be used to recognize not only bacterial cells, but also other microorganisms, for example, fungi that exhibit similar morphological characteristics. In addition, the use of the method does not require the application of specialized computer equipment, which widens the scope of applications regardless of the laboratory IT infrastructure, not only in microbiological diagnostics, but also in other diagnostic laboratories.																	1641-876X	2083-8492				SEP	2020	30	3					463	473		10.34768/amcs-2020-0034													
J								APPROXIMATE STATE-SPACE AND TRANSFER FUNCTION MODELS FOR 2x2 LINEAR HYPERBOLIC SYSTEMS WITH COLLOCATED BOUNDARY INPUTS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										distributed parameter system; hyperbolic equations; approximation model; state space; transfer function	EQUATION; PDES	Two approximate representations are proposed for distributed parameter systems described by two linear hyperbolic PDEs with two time- and space-dependent state variables and two collocated boundary inputs. Using the method of lines with the backward difference scheme, the original PDEs are transformed into a set of ODEs and expressed in the form of a finite number of dynamical subsystems (sections). Each section of the approximation model is described by state-space equations with matrix-valued state, input and output operators, or, equivalently, by a rational transfer function matrix. The cascade interconnection of a number of sections results in the overall approximation model expressed in finite-dimensional state-space or rational transfer function domains, respectively. The discussion is illustrated with a practical example of a parallel-flow double-pipe heat exchanger. Its steady-state, frequency and impulse responses obtained from the original infinite-dimensional representation are compared with those resulting from its approximate models of different orders. The results show better approximation quality for the "crossover" input-output channels where the in-domain effects prevail as compared with the "straightforward" channels, where the time-delay phenomena are dominating.																	1641-876X	2083-8492				SEP	2020	30	3					475	491		10.34768/amcs-2020-0035													
J								GLOBAL STABILITY OF NONLINEAR FEEDBACK SYSTEMS WITH FRACTIONAL POSITIVE LINEAR PARTS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										global; stability; fractional; nonlinear; feedback; positive; system		The global (absolute) stability of nonlinear systems with fractional positive and not necessarily asymptotically stable linear parts and feedbacks is addressed. The characteristics u = f(e) of the nonlinear parts satisfy the condition k1e <= f(e) <= k2e for some positive k1 and k2. It is shown that the fractional nonlinear systems are globally asymptotically stable if the Nyquist plots of the fractional positive linear parts are located on the right-hand side of the circles (-1/k(1),-1/k(2)).																	1641-876X	2083-8492				SEP	2020	30	3					493	499		10.34768/amcs-2020-0036													
J								FRACTIONAL ORDER TUBE MODEL REFERENCE ADAPTIVE CONTROL FOR A CLASS OF FRACTIONAL ORDER LINEAR SYSTEMS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										fractional order linear system; model reference adaptive control; fractional adaptive control; optimization; performance tube; fractional order TMRAC	(PID-MU)-D-LAMBDA CONTROLLER; LYAPUNOV FUNCTIONS; STABILITY; CALCULUS; CONVERGENCE	We introduce a novel fractional order adaptive control design based on the tube model reference adaptive control (TMRAC) scheme for a class of fractional order linear systems. By considering an adaptive state feedback control configuration, the main idea is to replace the classical reference model with a single predetermined trajectory by a fractional order performance tube guidance model allowing a set of admissible trajectories. Besides, an optimization problem is formulated to compute an on-line correction control signal within specified bounds in order to update the system performance while minimizing a control cost criterion. The asymptotic stability of the closed loop fractional order control system is demonstrated using an extension of the Lyapunov direct method. The dynamical performance of the fractional order tube model reference adaptive control (FOTMRAC) is compared with the standard fractional order model reference adaptive control (FOMRAC) strategy, and the simulation results show the effectiveness of the proposed control method.																	1641-876X	2083-8492				SEP	2020	30	3					501	515		10.34768/amcs-2020-0037													
J								DISCRETE-TIME SLIDING MODE CONTROL OF LINEAR SYSTEMS WITH INPUT SATURATION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										discrete-time sliding mode control; super-twisting controller; input saturation; disturbance compensation	DESIGN; PLANTS	The paper proposes a discrete-time sliding mode controller for single input linear dynamical systems, under requirements of the fast response without overshoot and strong robustness to matched disturbances. The system input saturation is imposed during the design due to inevitable limitations of most actuators. The system disturbances are compensated by employing nonlinear estimation by integrating the signum of the sliding variable. Hence, the proposed control structure may be regarded as a super-twisting-like algorithm. The designed system stability is analyzed as well as the sliding manifold convergence conditions are derived using a discrete-time model of the system in the d-domain. The results obtained theoretically have been verified by computer simulations.																	1641-876X	2083-8492				SEP	2020	30	3					517	528		10.34768/amcs-2020-0038													
J								T-S FUZZY BIBO STABILISATION OF NON-LINEAR SYSTEMS UNDER PERSISTENT PERTURBATIONS USING FUZZY LYAPUNOV FUNCTIONS AND NON-PDC CONTROL LAWS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										linear matrix inequalities; Takagi-Suegno fuzzy systems; fuzzy Lyapunov functions; integral delayed Lyapunov functions (IDLFs); non-parallel distributed fuzzy controllers (non-PDC); generalised inescapable ellipsoids	H-INFINITY CONTROL; TAKAGI-SUGENO MODELS; SUFFICIENT CONDITIONS; STABILITY CONDITIONS; LOCAL STABILIZATION; LMI CONDITIONS; DESIGN; REGULATORS	This paper develops an innovative approach for designing non-parallel distributed fuzzy controllers for continuous-time non-linear systems under persistent perturbations. Non-linear systems are represented using Takagi-Sugeno fuzzy models. These non-PDC controllers guarantee bounded input bounded output stabilisation in closed-loop throughout the computation of generalised inescapable ellipsoids. These controllers are computed with linear matrix inequalities using fuzzy Lyapunov functions and integral delayed Lyapunov functions. LMI conditions developed in this paper provide non-PDC controllers with a minimum *-norm (upper bound of the 1-norm) for the T-S fuzzy system under persistent perturbations. The results presented in this paper can be classified into two categories: local methods based on fuzzy Lyapunov functions with guaranteed bounds on the first derivatives of membership functions and global methods based on integral-delayed Lyapunov functions which are independent of the first derivatives of membership functions. The benefits of the proposed results are shown through some illustrative examples.																	1641-876X	2083-8492				SEP	2020	30	3					529	550		10.34768/amcs-2020-0039													
J								DISTRIBUTED FAULT ESTIMATION OF MULTI-AGENT SYSTEMS USING A PROPORTIONAL-INTEGRAL OBSERVER: A LEADER-FOLLOWING APPLICATION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										multiagent systems; fault estimation; state and fault observers; linear matrix inequalities	COOPERATIVE CONTROL; LPV SYSTEMS; DESIGN; CONSENSUS	This paper proposes a methodology for observer-based fault estimation of leader-following linear multi-agent systems subject to actuator faults. First, a proportional-integral distributed fault estimation observer is developed to estimate both actuator faults and states of each follower agent by considering directed and undirected graph topologies. Second, based on the proposed quadratic Lyapunov equation, sufficient conditions for the asymptotic convergence of the observer are obtained as a set of linear matrix inequalities. Finally, a numerical example is provided to illustrate the proposed approach.																	1641-876X	2083-8492				SEP	2020	30	3					551	560		10.34768/amcs-2020-0040													
J								ANT-BASED CLUSTERING FOR FLOWGRAPH MINING	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										possibly certain sequences; flow graphs; rough sets; fuzzy sets; ant-based clustering	INFORMATION-SYSTEMS; FREQUENT EPISODES; FLOW-GRAPHS; TOOL	The paper is devoted to the problem of mining graph data. The goal of this process is to discover possibly certain sequences appearing in data. Both rough set flow graphs and fuzzy flow graphs are used to represent sequences of items originally arranged in tables representing information systems. Information systems are considered in the Pawlak sense, as knowledge representation systems. In the paper, an approach involving ant based clustering is proposed. We show that ant based clustering can be used not only for building possible large groups of similar objects, but also to build larger structures (in our case, sequences) of objects to obtain or preserve the desired properties.																	1641-876X	2083-8492				SEP	2020	30	3					561	572		10.34768/amcs-2020-0041													
J								TWO META-HEURISTIC ALGORITHMS FOR SCHEDULING ON UNRELATED MACHINES WITH THE LATE WORK CRITERION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										late work minimization; unrelated machines; tabu search; genetic algorithm	WEIGHTED LATE WORK; 2-MACHINE FLOWSHOP PROBLEM; SINGLE-MACHINE; JOB-SHOP; APPROXIMATION SCHEME; PARALLEL PROCESSORS; GENETIC ALGORITHM; MINIMIZE	A scheduling problem in considered on unrelated machines with the goal of total late work minimization, in which the late work of a job means the late units executed after its due date. Due to the NP-hardness of the problem, we propose two meta-heuristic algorithms to solve it, namely, a tabu search (TS) and a genetic algorithm (GA), both of which are equipped with the techniques of initialization, iteration, as well as termination. The performances of the designed algorithms are verified through computational experiments, where we show that the GA can produce better solutions but with a higher time consumption. Moreover, we also analyze the influence of problem parameters on the performances of these meta-heuristics.																	1641-876X	2083-8492				SEP	2020	30	3					573	584		10.34768/amcs-2020-0042													
J								LINE SEGMENTATION OF HANDWRITTEN TEXT USING HISTOGRAMS AND TENSOR VOTING	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										document image processing; handwritten text line segmentation; projection profile; text string; off-line cursive script recognition; ICDAR 2009 competition	WATER-FLOW ALGORITHM; RECOGNITION	There are a large number of historical documents in libraries and other archives throughout the world. Most of them are written by hand. In many cases they exist in only one specimen and are hard to reach. Digitization of such artifacts can make them available to the community. But even digitized, they remain unsearchable, and an important task is to draw the contents in the computer readable form. One of the first steps in this direction is to recognize where the lines of the text are. Computational intelligence algorithms can be used to solve this problem. In the present paper, two groups of algorithms, namely, projection-based and tensor voting-based, are compared. The performance is evaluated on a data set and with the procedure proposed by the organizers of the ICDAR 2009 competition.																	1641-876X	2083-8492				SEP	2020	30	3					585	596		10.34768/amcs-2020-0043													
J								REAL-TIME HIERARCHICAL PREDICTIVE RISK ASSESSMENT AT THE NATIONAL LEVEL: MUTUALLY AGREED PREDICTED SERVICE DISRUPTION PROFILES	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										risk assessment; cyber security; hierarchical approach; service disruption profiles; coordination.	MANAGEMENT	We present a real-time hierarchical approach to an on-line risk assessment at the national level taking into account both local risk analyses performed by key service operators and relevant interdependencies between those services. For this purpose we define mutually agreed predicted service disruption profiles and then propose a coordination mechanism to align those profiles. A simple, four-entity example is provided to illustrate the coordination.																	1641-876X	2083-8492				SEP	2020	30	3					597	609		10.34768/amcs-2020-0044													
J								Theory of Hidden Oscillations and Stability of Control Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											LOCK-IN RANGES; LYAPUNOV DIMENSION; DYNAMICAL MODEL; LIMIT-CYCLES; COSTAS LOOP; PULL-IN; ATTRACTORS; COUNTEREXAMPLES; COMPUTATION; MULTISTABILITY	The development of the theory of absolute stability, the theory of bifurcations, the theory of chaos, theory of robust control, and new computing technologies has made it possible to take a fresh look at a number of well-known theoretical and practical problems in the analysis of multidimensional control systems, which led to the emergence of the theory of hidden oscillations, which represents the genesis of the modern era of Andronov's theory of oscillations. The theory of hidden oscillations is based on a new classification of oscillations as self-excited or hidden. While the self-excitation of oscillations can be effectively investigated analytically and numerically, revealing a hidden oscillation requires the development of special analytical and numerical methods and also it is necessary to determine the exact boundaries of global stability, to analyze and reduce the gap between the necessary and sufficient conditions for global stability, and distinguish classes of control systems for which these conditions coincide. This survey discusses well-known theoretical and engineering problems in which hidden oscillations (their absence or presence and location) play an important role.																	1064-2307	1555-6530				SEP	2020	59	5					647	668		10.1134/S1064230720050093													
J								Development and Research of Selectively Invariant Electromechanical Systems with the Adaptation of Regulators to Velocity Level Changes	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												In this paper, we develop structural solutions of the selectively invariant electromechanical systems (SI EMSs) that adapt the regulators presented in the canonical form of controllability (CFC) and in the canonical form of observability (CFO) to changes in the velocity level. Their capabilities to provide the specified quality indicators in a wide velocity range are comparatively analyzed. The obtained results are verified by the digital modeling of the synthesized systems and full-scale tests using real technological equipment. Specific recommendations for designing systems in various applications are elaborated.																	1064-2307	1555-6530				SEP	2020	59	5					669	683		10.1134/S1064230720050032													
J								Diagnostic Model for a Distributed Computer System in Real Time	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												An approach to test the diagnostics of distributed computing systems based on the introduction of redundancy in the system in order to simplify the test construction process and the diagnostic process is investigated. The introduced redundancy is a diagnostic model of the system, which runs in parallel with the main software of the system. For this model, sufficient conditions for controllability and observability are formulated.																	1064-2307	1555-6530				SEP	2020	59	5					684	694		10.1134/S1064230720050068													
J								Decomposition Method for Solving a Three-Index Planar Assignment Problem	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The classical assignment problem is considered. A third index is introduced, which can characterize, for example, the location of the work carried out. An iterative decomposition algorithm is proposed. At each step, a problem is solved with three constraints from different groups of conditions and with one connecting variable. The triples of problems are also solved with one restriction, and according to certain rules, the coefficients of the objective functions change. The iterative process that is monotonic in the objective function either arrives at the exact optimum solution of the original problem or indicates the nonuniqueness of the solution. In the latter case, a simple procedure finds the optima.																	1064-2307	1555-6530				SEP	2020	59	5					695	698		10.1134/S1064230720050056													
J								Binding Cryptographic Keys into Biometric Data: Optimization	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												Cryptography and biometry are important components of contemporary access control systems. Cryptographic systems themselves are highly reliable but they require the exact reproduction of access keys; this cannot be done by humans, while the corresponding devices might be lost or stolen. Biometric data are always with the person; however, they vary: it is impossible to obtain the same feature values. In this paper, a way is proposed to link the cryptographic key and the biometric features of the iris. This yields a two-component key such that no original component can be extracted until the biometric features close to the original ones, i.e., the data of the same person, are presented. The connecting method (coder) and the extracting method (decoder) consist of several separate steps executed successively. To select the parameters, we solve the following discrete optimization problem: under the given threshold of the false accept rate, we minimize the value of the false reject rate. The restrictions of this optimization problem are the minimal size of the coded key and the maximal size of the final key. Numerical experiments are conducted on open-access databases (DBs).																	1064-2307	1555-6530				SEP	2020	59	5					699	711		10.1134/S1064230720050135													
J								Computational Method for Recognizing Situations and Objects in the Frames of a Continuous Video Stream Using Deep Neural Networks for Access Control Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											HUMAN LOCALIZATION; ALGORITHM; GAS	An effective (performance- and accuracy-wise) computational method for pattern recognition in a continuous video stream using deep neural networks for access control systems is proposed. The class of recognition problems solved by the method using a sequence of video stream frames is identified: the vehicle itself and the characters on its license plate (LP), faces of people, and abnormal situations. In contrast to the known solutions, a classification with a subsequent reinforcement based on multiple frames of a video stream and with an algorithm for the automatic annotation of images is used. Neural network architectures with independent recurrent layers for classifying video fragments adapted for the problems, a dual network for face recognition, and a deep neural network for vehicle character recognition are proposed. New databases for neural network training are created. A schematic diagram of an intelligent access control system for ensuring the security of an enterprise, a distinctive feature of which is the use of a multirotor unmanned aerial vehicle with a computing unit, is proposed. Field experiments are carried out, and the accuracy and performance of the computational method in solving each problem are assessed. Software modules in the Python language for solving tasks of the intelligent access control system are developed.																	1064-2307	1555-6530				SEP	2020	59	5					712	727		10.1134/S1064230720050020													
J								Three-Link Mechanism as a Model of a Person on a Swing	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												We model movements of a person swinging on a swing. We consider a flat three-link hinged mechanism as the main mechanical model of the person sitting on the swing. The first, second, and third links model the human body, two hips that are rigidly connected to the swing, and two shins, respectively. The hinge between the first and second links models two hip joints, while the hinge between the second and third links models two knee joints. In each of the interlink hinges, a control moment, limited in magnitude, is applied. At the point of the support of the swing, the moment of viscous friction acts. A mathematical model of a controlled three-link mechanism is built. In solving the problem of synthesizing the control of a three-link model, the law of the control of a simpler (auxiliary) two-link swing model is preconstructed. Then this law is used to control the three-link model. Motion equations with the control built in the form of feedback, have periodic orbitally asymptotically stable solutions. Depending on the parameters of the model, such solutions describe the oscillations of a swing with a constant amplitude or rotation. The control that damps the swing is also built.																	1064-2307	1555-6530				SEP	2020	59	5					728	744		10.1134/S1064230720050081													
J								Analysis of Critical Damage in the Communication Network: I. Model and Computational Experiment	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												In the computational experiments on a flow model of a communication and control network, changes in the system's functional characteristics under destructive effects are studied. The removal of a subset of edges is considered to be critical damage if it results in the absence of connection paths for at least one pair of source-sink nodes. For each case of damage, both the total number of disconnected pairs and all communication directions, the throughput of which is less than the specified standard value, are determined. Based on the data obtained, multiparameter diagrams are constructed for assessing changes in the maximum flows for each dividing cut and all pairs of vertices. The boundary points on the diagrams correspond to the most dangerous damage not dominated by at least one damage indicator. The results of the effects of destructive effects on network systems with various structural features are analyzed.																	1064-2307	1555-6530				SEP	2020	59	5					745	754		10.1134/S106423072005010X													
J								Deep Neural Networks for Determining the Parameters of Buildings from Single-Shot Satellite Imagery	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											SHADOWS	The height of a building is a basic characteristic needed for analytical services. It can be used to evaluate the population and functional zoning of a region. The analysis of the height structure of urban territories can be useful for understanding the population dynamics. In this paper, a novel method for determining a building's height from a single-shot Earth remote sensing oblique image is proposed. The height is evaluated by a simulation algorithm that uses the masks of shadows and the visible parts of the walls. The image is segmented using convolutional neural networks that makes it possible to extract the masks of roofs, shadows, and building walls. The segmentation models are integrated into a completely automatic system for mapping buildings and evaluating their heights. The test dataset containing a labeled set of various buildings is described. The proposed method is tested on this dataset, and it demonstrates the mean absolute error of less than 4 meters.																	1064-2307	1555-6530				SEP	2020	59	5					755	767		10.1134/S106423072005007X													
J								Linear-Cubic Locally Optimal Control of Linear Systems and Its Application for Aircraft Guidance	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												In the paper, the requirements for aircraft guidance methods are considered. It is noted that the existing methods of optimization of linear control systems based mainly on the minimization of quadratic functionals of quality do not allow providing the set of requirements for interception systems of aerial objects. It is proposed to use new local quadratic-biquadratic quality functionals, whose minimization makes it possible to obtain more general linear-cubic control laws that are adequate for modern requirements. The biquadratic terminal part of the functional contains fourth-degree summands. An example of the synthesis of a specific guidance method is considered and analyzed.																	1064-2307	1555-6530				SEP	2020	59	5					768	780		10.1134/S1064230720050123													
J								Calculation of Controls, Not Generating Singularities for Four Control Moment Gyrodynes	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												This paper demonstrates the technology for calculating spacecraft's (SC's) programmed laws of orientation that does not generate special, singular, states of the control moment gyro systems and consists of two collinear gyrodyne pairs. The calculation of the programmed orientation laws that do not contain singular states for two pairs of gyrodynes is possible due to the use of the kinematic configuration of the system of the control moment gyrodynes in the new technology for calculating the SC's controls, which allows, if required, adjusting the position of the virtual flywheel axes for new attitude control problems. Today, the laws of programmed attitude control are calculated as a solution to the direct problem of dynamics. These laws contain singular states of the gyro system in which the SC remains uncontrollable. The controllability is restored by withdrawing the gyro system from these states using an additional gyro system and additional control. At the same time, the attitude control process of the SC is interrupted for the duration of the output. These material and temporary losses require searching for attitude control laws that do not contain special states of the gyro system. The calculation of such laws is demonstrated in solving the problem of orienting a space telescope with two collinear gyrodyne pairs.																	1064-2307	1555-6530				SEP	2020	59	5					781	795		10.1134/S1064230720050044													
J								On Improving the Maneuverability of a Space Vehicle Managed by Inertial Executive Bodies	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											AXIALLY-SYMMETRIC SPACECRAFT; OPTIMAL TURN PROBLEM	The problem of increasing the maneuverability of a spacecraft (SC) by minimizing the duration of rotations around the center of mass is solved. The case is studied when the orientation is controlled using inertial actuators (power gyroscopes, gyrodynamics). The problem of the fastest possible turn of an SC using gyrodynes from an arbitrary initial angular position to the desired final angular position is considered in detail. Using the maximum principle of L.S. Pontryagin, as well as quaternion models and methods for solving the problems of controlling the motion of an SC, a solution to the problem is obtained. The conditions of the optimality of the reorientation mode without unloading the gyrosystem are written in an analytical form and the properties of the optimal motion are studied. Formalized equations and calculation expressions are presented for constructing an optimal control program taking into account the possible perturbations. The key relationships that determine the optimal values of the parameters of the rotation control law are given. The results of mathematical modeling of the motion of an SC with the optimal control are presented, demonstrating the practical feasibility of the developed algorithm for controlling the spatial orientation of an SC. A condition is formulated for determining the moment of the start of braking from measurements of the current motion parameters, which significantly increases the accuracy of bringing an SC into a predetermined resting position in the presence of restrictions on the control moment.																	1064-2307	1555-6530				SEP	2020	59	5					796	815		10.1134/S1064230720020094													
J								A new intelligent pattern classifier based on deep-thinking	NEURAL COMPUTING & APPLICATIONS										Deep-thinking pattern classifier; Bayesian inference; Unsupervised learning; Correlation principle	MACHINE	A new intelligent pattern classifier based on the human being's thinking logics is developed in this paper, aiming to approximate the optimal design process and avoid the matrix inverse computation in conventional classifier designs. It is seen that the proposed classifier has no parameters to be determined via mathematical optimization. Instead, it is built by using the correlation principles to construct the clusters at first. The middle-level feature vectors can then be extracted from the statistical information of the correlations between the input data and the ones in each pattern cluster. For accurate classification purpose, the advanced feature vectors are generated with the moments' information of the middle-level feature vectors. After that, Bayesian inference is implemented to make decisions from the weighted sum of the advanced feature components. In addition, a real-time fine-tuning loop (layer) is designed to adaptively "widen" the border of each pattern clustering region such that the input data can be directly classified once they are located in one of the clustering regions. An experiment for the classification of the handwritten digit images from the MNIST database is performed to show the excellent performance and effectiveness of the proposed intelligent pattern classifier.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14247	14261		10.1007/s00521-019-04479-0													
J								Inverse partitioned matrix-based semi-random incremental ELM for regression	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; The minimum norm least-squares solution; Inefficient nodes; Convergence rate; Inverse partitioned matrix	EXTREME LEARNING-MACHINE; FEEDFORWARD NETWORKS; HIDDEN NEURONS; APPROXIMATION; CLASSIFICATION; NUMBER; BOUNDS	Incremental extreme learning machine has been verified that it has the universal approximation capability. However, there are two major issues lowering its efficiency: one is that some "random" hidden nodes are inefficient which decrease the convergence rate and increase the structural complexity, the other is that the final output weight vector is not the minimum norm least-squares solution which decreases the generalization capability. To settle these issues, this paper proposes a simple and efficient algorithm in which the parameters of even hidden nodes are calculated by fitting the residual error vector in the previous phase, and then, all existing output weights are recursively updated based on inverse partitioned matrix. The algorithm can reduce the inefficient hidden nodes and obtain a preferable output weight vector which is always the minimum norm least-squares solution. Theoretical analyses and experimental results show that the proposed algorithm has better performance on convergence rate, generalization capability and structural complexity than other incremental extreme learning machine algorithms.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14263	14274		10.1007/s00521-019-04289-4													
J								Audiovisual cross-modal material surface retrieval	NEURAL COMPUTING & APPLICATIONS										Cross-modal retrieval; Local receptive fields-based extreme learning machine; Canonical correlation analysis; Material analysis	EXTREME LEARNING-MACHINE; LOCAL RECEPTIVE-FIELDS; PERCEPTION	Cross-modal retrieval is developed rapidly because it can process the data among different modalities. Aiming at solving the problem that the text and image sometimes cannot perform the true and accurate analysis of the material, a system of audiovisual cross-modal retrieval on material surface is proposed. First, we use local receptive fields-based extreme learning machine to extract sound and image features, and then the sound and image features are mapped to the subspace using canonical correlation analysis and retrieved by Euclidean distance. Finally, the process of audiovisual cross-modal retrieval is realized by the system. The experimental results show that the proposed system has a good application effect on wood. The designed system provides a new idea for research in the field of material identification.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14301	14309		10.1007/s00521-019-04476-3													
J								Reinforcement learning and adaptive optimization of a class of Markov jump systems with completely unknown dynamic information	NEURAL COMPUTING & APPLICATIONS										Markov jump linear systems (MJLSs); Adaptive optimal control; Online; Reinforcement learning (RL); Coupled algebraic Riccati equations (AREs)	DISCRETE-TIME-SYSTEMS; SLIDING MODE CONTROL; DESIGN; ALGORITHM	In this paper, an online adaptive optimal control problem of a class of continuous-time Markov jump linear systems (MJLSs) is investigated by using a parallel reinforcement learning (RL) algorithm with completely unknown dynamics. Before collecting and learning the subsystems information of states and inputs, the exploration noise is firstly added to describe the actual control input. Then, a novel parallel RL algorithm is used to parallelly compute the correspondingNcoupled algebraic Riccati equations by online learning. By this algorithm, we will not need to know the dynamic information of the MJLSs. The convergence of the proposed algorithm is also proved. Finally, the effectiveness and applicability of this novel algorithm is illustrated by two simulation examples.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14311	14320		10.1007/s00521-019-04180-2													
J								Object affordance detection with relationship-aware network	NEURAL COMPUTING & APPLICATIONS										Object affordance detection; Convolutional neural network; Relationship-aware; Online sequential extreme learning machine	SEQUENTIAL LEARNING ALGORITHM; MACHINE	Object affordance detection, which aims to understand functional attributes of objects, is of great significance for an autonomous robot to achieve a humanoid object manipulation. In this paper, we propose a novel relationship-aware convolutional neural network, which takes the symbiotic relationship between multiple affordances and the combinational relationship between the affordance and objectness into consideration, to predict the most probable affordance label for each pixel in the object. Different from the existing CNN-based methods that rely on separate and intermediate object detection step, our proposed network directly produces the pixel-wise affordance maps from an input image in an end-to-end manner. Specifically, there are three key components in our proposed network: Coord-ASPP module introducing CoordConv in atrous spatial pyramid pooling (ASPP) to refine the feature maps, relationship-aware module linking the affordances and corresponding objects to explore the relationships, and online sequential extreme learning machine auxiliary attention module focusing on individual affordances further to assist relationship-aware module. The experimental results on two public datasets have shown the merits of each module and demonstrated the superiority of our relationship-aware network against the state of the arts.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14321	14333		10.1007/s00521-019-04336-0													
J								Hierarchical attentive Siamese network for real-time visual tracking	NEURAL COMPUTING & APPLICATIONS										Visual tracking; Siamese networks; Attention mechanism; Hierarchical features	OBJECT TRACKING	Visual tracking is a fundamental and highly useful component in various tasks of computer vision. Recently, end-to-end off-line training Siamese networks have demonstrated great success in visual tracking with high performance in terms of speed and accuracy. However, Siamese trackers usually employ visual features from the last simple convolutional layers to represent the targets while ignoring the fact that features from different layers characterize different representation capabilities of the targets, and hence this may degrade tracking performance in the presence of severe deformation and occlusion. In this paper, we present a novel hierarchical attentive Siamse (HASiam) network for high-performance visual tracking, which exploits different kinds of attention mechanisms to effectively fuse a series of attentional features from different layers. More specifically, we combine a deeper network with a shallow one to take full advantage of the features from different layers and apply spatial and channel-wise attentions on different layers to better capture visual attentions on multi-level semantic abstractions, which is helpful to enhance the discriminative capacity of the model. Furthermore, the top-layer feature maps have low resolution that may affect localization accuracy if each feature is treated independently. To address this issue, a non-local attention module is also adopted on the top layer to force the network to pay more attention to the structural dependency of features at all locations during off-line training. The proposed HASiam is trained off-line in an end-to-end manner and needs no online updating the network parameters during tracking. Extensive evaluations demonstrate that our HASiam has achieved favorable results with AUC scores of64.6%and EAO scores of 0.227 while having a speed of 60 fps on the OTB2013, OTB100 and VOT2017 real-time experiments, respectively. Our tracker with high accuracy and real-time speed can be applied to numerous vision applications like visual surveillance systems, robotics and augmented reality.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14335	14346		10.1007/s00521-019-04238-1													
J								Novel direct remaining useful life estimation of aero-engines with randomly assigned hidden nodes	NEURAL COMPUTING & APPLICATIONS										Remaining useful life (RUL); Aero-engines; Extreme learning machine (ELM)	EXTREME LEARNING-MACHINE; ADAPTIVE NEURAL-CONTROL; HEALTH-MANAGEMENT; NONLINEAR-SYSTEMS; PROGNOSTICS; CLASSIFICATION; CAPABILITY; PREDICTION; REGRESSION	This paper aims to improve data-driven prognostics by presenting a novel approach of directly estimating the remaining useful life (RUL) of aero-engines without requiring setting any failure threshold information or estimating degradation states. Specifically, based on the sensory data, RUL estimations are directly obtained through the universal function approximation capability of the extreme learning machine (ELM) algorithm. To achieve this, the features related with the RUL are first extracted from the sensory data as the inputs of the ELM model. Besides, to optimize the number of observed sensors, three evaluation metrics of correlation, monotonicity and robustness are defined and combined to automatically select the most relevant sensor values for more effective and efficient remaining useful life predictions. The validity and superiority of the proposed approach is evaluated by the widely used turbofan engine datasets from NASA Ames prognostics data repository. The proposed approach shows improved RUL estimation applicability at any time instant of the degradation process without determining the failure thresholds. This also simplifies the RUL estimation procedure. Moreover, the random properties of hidden nodes in the ELM learning mechanisms ensures the simplification and efficiency for real-time implementation. Therefore, the proposed approach suits to real-world applications in which prognostics estimations are required to be fast.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14347	14358		10.1007/s00521-019-04478-1													
J								A machine-learning-enhanced hierarchical multiscale method for bridging from molecular dynamics to continua	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; Hierarchical multiscale method; Molecular model; Continuum model	CONSISTENT CLUSTERING ANALYSIS; CAUCHY-BORN RULE; DISLOCATION NUCLEATION; MODEL; POTENTIALS; HOMOGENIZATION; SIMULATIONS; FRAMEWORK; IMPLEMENTATION; DEFORMATION	In the community of computational materials science, one of the challenges in hierarchical multiscale modeling is information-passing from one scale to another, especially from the molecular model to the continuum model. A machine-learning-enhanced approach, proposed in this paper, provides an alternative solution. In the developed hierarchical multiscale method, molecular dynamics simulations in the molecular model are conducted first to generate a dataset, which represents physical phenomena at the nanoscale. The dataset is then used to train a material failure/defect classification model and stress regression models. Finally, the well-trained models are implemented in the continuum model to study the mechanical behaviors of materials at the macroscale. Multiscale modeling and simulation of a molecule chain and an aluminum crystalline solid are presented as the applications of the proposed method. In addition to support vector machines, extreme learning machines with single-layer neural networks are employed due to their computational efficiency.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14359	14373		10.1007/s00521-019-04480-7													
J								An event recommendation model using ELM in event-based social network	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; Event-based social network; Event recommendation	EXTREME LEARNING-MACHINE; WEB	In recent years, event-based social network (EBSN) platforms have increasingly entered people's daily life and become more and more popular. In EBSNs, event recommendation is a typical problem which recommends interested events to users. Different from traditional social networks, both online and off-line factors play an important role in EBSNs. However, the existing methods do not make full use of the online and off-line information, which may lead to a low accuracy, and they are also not efficient enough. In this paper, we propose a novel event recommendation model to solve the above shortcomings. At first, a feature extraction phase is constructed to make full use of the EBSN information, including spatial feature, temporal feature, semantic feature, social feature and historical feature. And then, we transform the recommendation problem to a classification problem and ELM is extended as the classifier in the model. Extensive experiments are conducted on real EBSN datasets. The experimental results demonstrate that our approach is efficient and has a better performance than the existing methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14375	14384		10.1007/s00521-019-04344-0													
J								An experimental evaluation of extreme learning machines on several hardware devices	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; Hardware; Multi-core; GPU; FPGA	ELM; APPROXIMATION; REGRESSION; FRAMEWORK; NETWORKS	As an important learning algorithm, extreme learning machine (ELM) is known for its excellent learning speed. With the expansion of ELM's applications in the field of classification and regression, the need for its real-time performance is increasing. Although the use of hardware acceleration is an obvious solution, how to select the appropriate acceleration hardware for ELM-based applications is a topic worthy of further discussion. For this purpose, we designed and evaluated the optimized ELM algorithms on three kinds of state-of-the-art acceleration hardware, i.e., multi-core CPU, Graphics Processing Unit (GPU), and Field-Programmable Gate Array (FPGA) which are all suitable for matrix multiplication optimization. The experimental results showed that the speedup ratio of these optimized algorithms on acceleration hardware achieved 10-800. Therefore, we suggest that (1) use GPU to accelerate ELM algorithms for large dataset, and (2) use FPGA for small dataset because of its lower power, especially for some embedded applications. We also opened our source code.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14385	14397		10.1007/s00521-019-04481-6													
J								Adaptive neural tracking control for automotive engine idle speed regulation using extreme learning machine	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; Adaptive neural control; Uncertain nonlinearity; Engine idle speed regulation		The automotive engine idle speed control problem is a compromise among low engine speed for fuel saving, minimum emissions and disturbance rejection ability to prevent engine stall. However, idle speed regulation is very challenging due to the presence of high nonlinearity and aging-caused uncertainties in the engine dynamics. Therefore, the engine idle speed system is a typical uncertain nonlinear system. To address the problems of inherent nonlinearity and uncertainties in idle speed regulation, an extreme learning machine (ELM)-based adaptive neural control algorithm is proposed for tracking the target idle speed adaptively. The purpose of ELM is to rapidly deal with the uncertain nonlinear engine system. Since the original ELM is not designed for adaptive control, a new adaptation law is designed to update the weights of ELM in the sense of Lyapunov stability. Experiment is conducted to validate the performance of the proposed control method. Experimental result indicates that the ELM-based adaptive neural control outperforms the classical proportional-integral-derivative (PID), fuzzy-PID and back-propagation-neural-network-based controllers in terms of tracking performance under the variation of engine load.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14399	14409		10.1007/s00521-019-04482-5													
J								ELM-based driver torque demand prediction and real-time optimal energy management strategy for HEVs	NEURAL COMPUTING & APPLICATIONS										Hybrid electric vehicle; Energy optimization; Extreme learning machine; Connected vehicles; Driver demand prediction	HYBRID; OPTIMIZATION; NETWORKS	In hybrid electric vehicles, the energy economy depends on the coordination between the internal combustion engine and the electric machines under the constraint that the total propulsion power satisfies the driver demand power. To optimize this coordination, not only the current power demand but also the future one is needed for real-time distribution decision. This paper presents a prediction-based optimal energy management strategy. Extreme learning machine algorithm is exploited to provide the driver torque demand prediction for realizing the receding horizon optimization. With an industrial used traffic-in-the-loop powertrain simulation platform, an urban driving route scenario is built for the source data collection. Both of one-step-ahead and multi-step-ahead predictions are investigated. The prediction results show that for the three-step-ahead prediction, the 1st step can achieve unbiased estimation and the minimum root-mean-square error can achieve 100, 150 and 160 of the 1st, 2nd and 3rd steps, respectively. Furthermore, integrating with the learning-based prediction, a real-time energy management strategy is designed by solving the receding horizon optimization problem. Simulation results demonstrate the effect of the proposed scheme.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14411	14429		10.1007/s00521-019-04240-7													
J								Fast nonsingular terminal sliding mode control for permanent-magnet linear motor via ELM	NEURAL COMPUTING & APPLICATIONS										ELM; Equivalent control; FNTSM; Permanent-magnet linear motor	NONLINEAR-SYSTEMS; NEURAL-CONTROL; MOTION CONTROL; COMPENSATION; FRICTION; NETWORKS	In this paper, a novel fast nonsingular terminal sliding mode (FNTSM) control strategy using extreme learning machine (ELM) is proposed for permanent-magnet linear motor systems. It is shown that the developed FNTSM controller is composed of an equivalent control via ELM technique, a compensation control and a reaching control. Distinguished from the traditional ELM for pattern classification, output weights of the proposed ELM are adaptively adjusted by the adaptive law in Lyapunov sense from the global stability point of view, such that the equivalent control of the proposed controller can be flexibly estimated via ELM. Not only can the strong robustness and the faster convergence rate of the closed-loop control be guaranteed, but also the dependence of system dynamics can be further alleviated in the controller design due to the implementation of the ELM. Comparative simulation results are given to validate the robust control performance of the developed controller for both step tracking and sinusoidal tracking purposes.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14447	14457		10.1007/s00521-019-04502-4													
J								Robust surface reconstruction from highly noisy point clouds using distributed elastic networks	NEURAL COMPUTING & APPLICATIONS										Surface reconstruction; Random weights network; Elastic regularization; Sparsity; Distributed ADMM	APPROXIMATION; SELECTION	In this paper, a novel distributed elastic random weights network (DERWN) is proposed to achieve robust surface reconstruction from highly noisy point clouds sampled from real surface. The designed elastic regularization withl1 penalty items makes the network more resilient to noise and effectively capture the intrinsic shape of surface. Sparsity constraints of output weight vectors and threshold-based nodes removal are conducive to determining appropriate number of hidden nodes of network and optimizing the distribution of hidden nodes. The distributed optimization manner in DERWN on the basis of alternating direction method of multipliers solves the problem that traditional RWN learning algorithm suffers from the limitation of memory with large-scale data. The proposed DERWN achieves a solution to global problem by solving local subproblems coordinately. Experimental results show that the proposed DERWN algorithm can robustly reconstruct the unknown surface in case of highly noisy data with satisfying accuracy and smoothness.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14459	14470		10.1007/s00521-019-04409-0													
J								Hyperspectral image super-resolution using recursive densely convolutional neural network with spatial constraint strategy	NEURAL COMPUTING & APPLICATIONS										Hyperspectral image; Super-resolution; Deep convolutional network; Recursion; Dense connection	RESOLUTION	Hyperspectral images (HSIs) have been widely applied in real life, such as remote sensing, geological exploration, and so on. Many deep networks have been proposed to raise the resolution of HSIs for their better applications. But training their huge number of model parameters (weights and biases) needs more memory for storage and computation, which may bring some difficulties when they are applied in mobile terminal devices. In order to condense the deep networks and still keep the reconstruction effect, this paper proposes a compact deep network for HSI super-resolution (SR) by fusing the idea of recursion, dense connection, and spatial constraint (SCT) strategy. We name this method as recursive densely convolutional neural network with a spatial constraint strategy (SCT-RDCNN). The proposed method uses a novel designed recursive densely convolutional neural network (RDCNN) to learn the mapping relation between the low-resolution (LR) HSI and the high-resolution (HR) HSI and then adopts the SCT to improve the determined HR HSI. Compared with some existing deep-network-based HSI SR methods, the proposed method can use much less parameters (weight and bias) to attain or exceed the performance of methods with similar convolution layers because of the recursive structure and dense connection. It is significant and meaningful for the practical applications of the network in HSI SR due to the limitations of hardware devices. Some experiments on three HSI databases illustrate that our proposed SCT-RDCNN method outperforms several state-of-the-art HSI SR methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14471	14481		10.1007/s00521-019-04484-3													
J								A multi-target corner pooling-based neural network for vehicle detection	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Vehicle detection; Multi-target corner pooling; Intelligent transportation system	TRACKING; SALIENT	Convolutional neural network has shown strong capability to improve performance in vehicle detection, which is one of the main research topics of intelligent transportation system. Aiming to detect the blocked vehicles efficiently in actual traffic scenes, we propose a novel convolutional neural network based on multi-target corner pooling layers. The hourglass network, which could extract local and global information of the vehicles in the images simultaneously, is chosen as the backbone network to provide vehicles' features. Instead of using the max pooling layer, the proposed multi-target corner pooling (MTCP) layer is used to generate the vehicles' corners. And in order to complete the blocked corners that cannot be generated by MTCP, a novel matching corners method is adopted in the network. Therefore, the proposed network can detect blocked vehicles accurately. Experiments demonstrate that the proposed network achieves an AP of 43.5% on MS COCO dataset and a precision of 93.6% on traffic videos, which outperforms the several existing detectors.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14497	14506		10.1007/s00521-019-04486-1													
J								Extreme-learning-machine-based FNTSM control strategy for electronic throttle	NEURAL COMPUTING & APPLICATIONS										Electronic throttle; Extreme learning machine; Fast nonsingular terminal sliding mode; Lumped uncertainty bound	SLIDING MODE CONTROL; NONLINEAR-SYSTEMS; NEURAL-CONTROL; DESIGN	A novel extreme-learning-machine-based robust control scheme for automotive electronic throttle systems with uncertain dynamics is presented in this paper. It is shown that the well-known extreme learning machine (ELM) is used to estimate the upper bound of the lumped uncertainty while a fast nonsingular terminal sliding mode feedback controller is designed to achieve global stability and finite-time convergence for the closed-loop system. Although the ELM used in this paper has the same structure as the one in the conventional least-square-based ELM used for pattern classifications, i.e., the input weights are randomly chosen, the ELM adopted in the closed-loop control system is designed to achieve global control purpose. The output weights of the ELM will be adaptively adjusted in Lyapunov sense from the perspective of global stability of the closed-loop system, rather than local optimization in conventional ELM. The proposed control can thus not only realize the finite-time error convergence but also needs no prior knowledge of lumped uncertainty. Simulation results are demonstrated to verify the excellent tracking performance of the proposed control in comparison with other existing control methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14507	14518		10.1007/s00521-019-04446-9													
J								Perceptual image quality using dual generative adversarial network	NEURAL COMPUTING & APPLICATIONS										Image processing; Perceptual quality; Data distribution; Generative adversarial network; Classification	SUPERRESOLUTION	Generative adversarial networks have received a remarkable success in many computer vision applications for their ability to learn from complex data distribution. In particular, they are capable to generate realistic images from latent space with a simple and intuitive structure. The main focus of existing models has been improving the performance; however, there is a little attention to make a robust model. In this paper, we investigate solutions to the super-resolution problems-in particular perceptual quality-by proposing a robust GAN. The proposed model unlike the standard GAN employs two generators and two discriminators in which, a discriminator determines that the samples are from real data or generated one, while another discriminator acts as classifier to return the wrong samples to its corresponding generators. Generators learn a mixture of many distributions from prior to the complex distribution. This new methodology is trained with the feature matching loss and allows us to return the wrong samples to the corresponding generators, in order to regenerate the real-look samples. Experimental results in various datasets show the superiority of the proposed model compared to the state of the art methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14521	14531		10.1007/s00521-019-04239-0													
J								Human Feedback as Action Assignment in Interactive Reinforcement Learning	ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS										Interactive machine learning; reinforcement learning; reward shaping; learning from human teachers	ROBOT	Teaching by demonstrations and teaching by assigning rewards are two popular methods of knowledge transfer in humans. However, showing the right behaviour (by demonstration) may appear more natural to a human teacher than assessing the learner's performance and assigning a reward or punishment to it. In the context of robot learning, the preference between these two approaches has not been studied extensively. In this article, we propose a method that replaces the traditional method of reward assignment with action assignment (which is similar to providing a demonstration) in interactive reinforcement learning. The main purpose of the suggested action is to compute a reward by seeing if the suggested action was followed by the self-acting agent or not. We compared action assignment with reward assignment via a user study conducted over the web using a two-dimensional maze game. The logs of interactions showed that action assignment significantly improved users' ability to teach the right behaviour. The survey results showed that both action and reward assignment seemed highly natural and usable, reward assignment required more mental effort, repeatedly assigning rewards and seeing the agent disobey commands caused frustration in users, and many users desired to control the agent's behaviour directly.																	1556-4665	1556-4703				SEP	2020	14	4							14	10.1145/3404197													
J								Finding the Largest Successful Coalition under the Strict Goal Preferences of Agents	ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS										Coalitional resource games; goal preferences of agents; successful coalition; network flows; genetic algorithm; heuristic	COOPERATIVE GAMES; STRUCTURE GENERATION; TASK ALLOCATION; OPTIMIZATION; ALGORITHMS; SYSTEMS	Coalition formation has been a fundamental form of resource cooperation for achieving joint goals in multiagent systems. Most existing studies still focus on the traditional assumption that an agent has to contribute its resources to all the goals, even if the agent is not interested in the goal at all. In this article, a natural extension of the traditional coalitional resource games (CRGs) is studied from both theoretical and empirical perspectives, in which each agent has uncompromising, personalized preferences over goals. Specifically, a new CRGs model with agents' strict preferences for goals is presented, in which an agent is willing to contribute its resources only to the goals that are in its own interest set. The computational complexity of the basic decision problems surrounding the successful coalition is reinvestigated. The results suggest that these problems in such a strict preference way are complex and intractable. To find the largest successful coalition for possible computation reduction or potential parallel processing, a flow-network-based exhaust algorithm, called FNetEA, is proposed to achieve the optimal solution. Then, to solve the problem more efficiently, a hybrid algorithm, named 2D-HA, is developed to find the approximately optimal solution on the basis of genetic algorithm, two-dimensional (2D) solution representation, and a heuristic for solution repairs. Through extensive experiments, the 2D-HA algorithm exhibits the prominent ability to provide reassurances that the optimal solution could be found within a reasonable period of time, even in a super-large-scale space.																	1556-4665	1556-4703				SEP	2020	14	4							15	10.1145/3412370													
J								UAVs vs. Pirates: An Anticipatory Swarm Monitoring Method Using an Adaptive Pheromone Map	ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS										UAV swarm; piracy; pheromone map; reservation mechanism; adaptive	SEARCH	For the rising hazard of pirate attacks, unmanned aerial vehicle (UAV) swarm monitoring is a promising countermeasure. Previous monitoring methods have deficiencies in either adaptivity to dynamic events or simple but effective path coordination mechanisms, and they are inapplicable to the large-area, low-target-density, and long-duration persistent counter-piracy monitoring. This article proposes a self-organized UAV swarm counter-piracy monitoring method. Based on the pheromone map, this method is characterized by (1) a reservation mechanism for anticipatory path coordination and (2) a ship-adaptive mechanism for adapting to merchant ship distributions. A heuristic depth-first branch and bound search algorithm is designed for solving individual path planning. Simulation experiments are conducted to study the optimal number of plan steps and adaptivity scaling factor for different numbers of UAVs. Results show that merely decreasing revisit intervals cannot effectively reduce pirate attacks. Without the ship-adaptive mechanism, the proposed method reduces up to 87.2%, 43.2%, and 5.5% of revisit intervals compared to the Levy Walk method, the sweep method, and the baseline self-organized method, respectively, but cannot reduce pirate attacks; while with the ship-adaptive mechanism, the proposed method can reduce pirate attacks by up to 6.7% compared to the best of the baseline methods.																	1556-4665	1556-4703				SEP	2020	14	4							13	10.1145/3380782													
J								Challenges of Human-Aware AI Systems	AI MAGAZINE												From its inception, artificial intelligence (AI) has had a rather ambivalent relationship to humans - swinging between their augmentation and their replacement. Now, as AI technologies enter our everyday lives at an ever-increasing pace, there is a greater need for AI systems to work synergistically with humans. To do this effectively, AI systems must pay more attention to aspects of intelligence that help humans work with each other - including social intelligence. I will discuss the research challenges in designing such human-aware AI systems, including modeling the mental states of humans-in-the-loop and recognizing their desires and intentions, providing proactive support, exhibiting explicable behavior, giving cogent explanations on demand, and engendering trust. I will survey the progress made so far on these challenges, and highlight some promising directions. I will also touch on the additional ethical quandaries that such systems pose. I will end by arguing that the quest for human-aware AI systems broadens the scope of AI enterprise; necessitates and facilitates true interdisciplinary collaborations; and can go a long way toward increasing public acceptance of AI technologies.																	0738-4602					FAL	2020	41	3					3	17															
J								Conversational Intelligence Challenge: Accelerating Research with Crowd Science and Open Source	AI MAGAZINE												Development of conversational systems is one of the most challenging tasks in natural language processing, and it is especially hard in the case of open-domain dialogue. The main factors that hinder progress in this area are lack of training data and difficulty of automatic evaluation. Thus, to reliably evaluate the quality of such models, one needs to resort to time-consuming and expensive human evaluation. We tackle these problems by organizing the Conversational Intelligence Challenge (ConvAI) - open competition of dialogue systems. Our goals are threefold: to work out a good design for human evaluation of open-domain dialogue, to grow open-source code base for conversational systems, and to harvest and publish new datasets. Over the course of ConvAI1 and ConvAI2 competitions, we developed a framework for evaluation of chatbots in messaging platforms and used it to evaluate over 30 dialogue systems in two conversational tasks - discussion of short text snippets from Wikipedia and personalized small talk. These large-scale evaluation experiments were performed by recruiting volunteers as well as paid workers. As a result, we succeeded in collecting a dataset of around 5,000 long meaningful human-to-bot dialogues and got many insights into the organization of human evaluation. This dataset can be used to train an automatic evaluation model or to improve the quality of dialogue systans. Our analysis of ConvAI1 and ConvAI2 competitions shows that the future work in this area should be centered around the more active participation of volunteers in the assessment of dialogue systems. To achieve that; we plan to make the evaluation setup more engaging.																	0738-4602					FAL	2020	41	3					18	27															
J								Adaptable Conversational Machines	AI MAGAZINE											DOMAIN ADAPTATION; SPOKEN; SYSTEMS	In recent years we have witnessed a surge in machine learning methods that provide machines with conversational abilities. Most notably, neural-network-based systems have set the state of the art for difficult tasks such as speech recognition, semantic understanding, dialogue management, language generation, and speech synthesis. Still, unlike for the ancient game of Go for instance, we are far from achieving human-level performance in dialogue. The reasons for this are numerous. One property of human-human dialogue that stands out is the infinite number of possibilities of expressing oneself during the conversation, even when the topic of the conversation is restricted. A typical solution to this problem was scaling-up the data. The most prominent mantra in speech and language technology has been "There is no data like more data." However, the researchers now are focused on building smarter algorithms - algorithms that can learn efficiently from just a few examples. This is an intrinsic property of human behavior: an average human sees during their lifetime a fraction of data that we nowadays present to machines. A human can even have an intuition about a solution before ever experiencing an example solution. The human-inspired ability to adapt may just be one of the keys in pushing dialogue systems toward human performance. This article reviews advancements in dialogue systems research with a focus on the adaptation methods for dialogue modeling, and ventures to have a glance at the future of research on adaptable conversational machines.																	0738-4602					FAL	2020	41	3					28	44															
J								Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study	AI MAGAZINE												In this article, we present an end-to-end automated helpdesk email ticket assignment system driven by high accuracy, coverage, business continuity, scalability, and optimal usage of computational resources. The primary objective of the system is to determine the problem mentioned in an incoming email ticket and then automatically dispatch it to an appropriate resolver group with high accuracy. While meeting this objective, it should also meet the objective of being able to operate at desired accuracy levels in the face of changing business needs by automatically adapting to the changes. The proposed system uses a system of classifiers with separate strategies for handling frequent and sparse resolver groups augmented with a semiautomatic rule engine and retraining strategies to ensure that it is accurate, robust; and adaptive to changing business needs. Our system has been deployed in the production of six major service providers in diverse service domains and currently assigns 100,000 emails per month, on an average, with an accuracy close to ninety percent and covering at least ninety percent of email tickets. This translates to achieving human-level accuracy and results in a net savings of more than 50,000 man-hours of effort per annum. To date, our deployed system has already served more than two million tickets in production.																	0738-4602					FAL	2020	41	3					45	62															
J								Large-Scale Personalized Categorization of Financial Transactions	AI MAGAZINE												A major part of financial accounting involves organizing business transactions using a customizable filing system that accountants call a "chart of accounts." This task must be carried out for every financial transaction, and hence automation is of significant value to the users of accounting software. In this article we present a large-scale recommendation system used by millions of small businesses in the USA, UK, Australia, Canada, India, and France to organize billions of financial transactions each year. The system uses machine learning to combine fragments of information from millions of users in a manner that allows us to accurately recommend chart-of-accounts categories even when users have created their own or named them using abbreviations or in foreign languages. Transactions are handled even if a given user has never categorized a transaction like that before. The development of such a system and testing it at scale over billions of transactions is a first in the financial industry.																	0738-4602					FAL	2020	41	3					63	77															
J								Improving the Accuracy and Transparency of Underwriting with Artificial Intelligence to Transform the Life-Insurance Industry	AI MAGAZINE												Life insurance provides trillions of dollars of financial security for hundreds of millions of individuals and families worldwide. To simultaneously offer affordable products while managing this financial ecosystem, life-insurance companies use an underwriting process to assess the mortality risk posed by individual applicants. Traditional underwriting is largely based on examining an applicant's health and behavioral profile. This manual process is incompatible with expectations of a rapid customer experience through digital capabilities. Fortunately, the availability of large historical data sets and the emergence of new data sources provide an unprecedented opportunity for artificial intelligence to transform underwriting in the life-insurance industry with standard measures of mortality risk. We combined one of the largest application data sets in the industry with a responsible artificial intelligence framework to develop a mortality model and life score. We describe how the life score serves as the primary risk-driving engine of deployed algorithmic underwriting systems and demonstrate its high level of accuracy, yielding a nine-percent reduction in claims within the healthiest pool of applicants. Additionally, we argue that, by embracing transparency, the industry can build consumer trust and respond to a dynamic regulatory environment focused on algorithmic decision-making. We present a consumer-facing tool that uses a state-of-the-art method for interpretable machine learning to off er transparency into the life score.																	0738-4602					FAL	2020	41	3					78	93															
J								Scaling-Up Data-Driven Pilot Projects	AI MAGAZINE											ANALYTICS	Conducting pilot projects are a common approach among organizations to test and evaluate new technology. A pilot project is often conducted to remove uncertainties from a large-scale project and should be limited in time and scope. Nowadays, several organizations are testing and evaluating artificial intelligence techniques and more advanced forms of analytics via pilot projects. Unfortunately, many organizations are experiencing problems in scaling-up the findings from pilot projects to the rest of the organization. Hence, results from pilot projects become siloed with limited business value. In this article, we present an overview of barriers for conducting and scaling-up data-driven pilot projects. Lack of senior management support is a frequently mentioned top barrier in the literature. In response to this, we present our recommendations on what type of activities can be performed, to increase the chances of getting a positive response from senior management regarding scaling-up the usage of artificial intelligence and advanced analytics within an organization.																	0738-4602					FAL	2020	41	3					94	102															
J								The Reproducibility Crisis Is Real	AI MAGAZINE												The reproducibility crisis is real, and it is not only the field of psychology that has to deal with it. All the sciences are affected; the field of artificial intelligence is not an exception.																	0738-4602					FAL	2020	41	3					103	106															
J								Towards Making Fuzzy Techniques More Adequate for Combining Knowledge of Several Experts	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fuzzy techniques; combining expert knowledge; non-normalized membership functions		In medical and other applications, expert often use rules with several conditions, each of which involve a quantity within the domain of expertise of a different expert. In such situations, to estimate the degree of confidence that all these conditions are satisfied, we need to combine opinions of several experts - i.e., in fuzzy techniques, combine membership functions corresponding to different experts. In each area of expertise, different experts may have somewhat different membership functions describing the same natural language ("fuzzy") term like small. It is desirable to present the user with all possible conclusions corresponding to all these membership functions. In general, even if, for each area of expertise, we have only a 1-parametric family characterizing different membership function, then for rules with 3 conditions, we already have a difficult-to-interpret 3-parametric family of possible consequences. It is thus desirable to limit ourselves to the cases when the resulting family is still manageable - e.g., is 1-parametric. In this paper, we provide a full description of all such families. Interestingly, it turns out that such families are possible only if we allow non-normalized membership functions, i.e., functions for which the maximum may be smaller than 1. We argue that this is a way to go, since normalization loses some information that we receive from the experts.																	1343-0130	1883-8014				SEP	2020	24	5					583	588															
J								A New (Simplified) Derivation of Nash's Bargaining Solution	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										group decision making; Nash's bargaining; solution; partial order		According to the Nobelist John Nash, if a group of people wants to selects one of the alternatives in which all of themget a better deal than in a status quo situations, then they should select the alternative that maximizes the product of their utilities. In this paper, we provide a new (simplified) derivation of this result, a derivation which is not only simpler - it also does not require that the preference relation between different alternatives be linear.																	1343-0130	1883-8014				SEP	2020	24	5					589	592															
J								How to Describe Conditions Like 2-out-of-5 in Fuzzy Logic: A Neural Approach	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fuzzy logic; neural networks; 2-out-of-5 conditions; medical applications		In many medical applications, we diagnose a disease and/or apply a certain remedy if, e.g., two out of five conditions are satisfied. In the fuzzy case, i.e., when we only have certain degrees of confidence that each of n statement is satisfied, how do we estimate the degree of confidence that k out of n conditions are satisfied? In principle, we can get this estimate if we use the usual methodology of applying fuzzy techniques: we represent the desired statement in terms of "and" and "or," and use fuzzy analogues of these logical operations. The problem with this approach is that for large n, it requires too many computations. In this paper, we derive the fastest-to-compute alternative formula. In this derivation, we use the ideas from neural networks.																	1343-0130	1883-8014				SEP	2020	24	5					593	598															
J								How to Combine (Dis)Utilities of Different Aspects into a Single (Dis)Utility Value, and How This Is Related to Geometric Images of Happiness	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS												In many practical situations, a user needs our help in selecting the best out of a large number of alternatives. To be able to help, we need to understand the user's preferences. In decision theory, preferences are described by numerical values known as utilities. It is often not feasible to ask the user to provide utilities of all possible alternatives, so we must be able to estimate these utilities based on utilities of different aspects of these alternatives. In this paper, we provide a general formula for combining utilities of aspects into a single utility value. The resulting formula turns out to be in good accordance with the known correspondence between geometric images and different degrees of happiness.																	1343-0130	1883-8014				SEP	2020	24	5					599	603															
J								Theoretical Explanation of Recent Empirically Successful Code Quality Metrics	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										code quality metrics; Laplace indeterminacy principle; order statistics		Millions of lines of code are written every day, and it is not practically possible to perfectly thoroughly test all this code on all possible situations. In practice, we need to be able to separate codes which aremore probable to contain bugs - and which thus need to be tested more thoroughly - from codes which are less probable to contain flaws. Several numerical characteristics known as code quality metrics - have been proposed for this separation. Recently, a new efficient class of code quality metrics have been proposed, based on the idea to assign consequent integers to different levels of complexity and vulnerability: we assign 1 to the simplest level, 2 to the next simplest level, etc. The resulting numbers are then combined - if needed, with appropriate weights. In this paper, we provide a theoretical explanation for the above idea.																	1343-0130	1883-8014				SEP	2020	24	5					604	608															
J								Modeling a Fuzzy System for Diagnosis of Syndromes with Integrated Eastern and Western Medicine Using the Importance of Symptoms	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fuzzy systems; integrated Eastern and Western medicine diagnosis; importance of symptoms	LOGIC	In this study, we present an approach to include the importance of symptoms for the diagnosis of syndromes with integrated eastern and western medicine. We also focus on knowledge representation and inference engine of our proposed system using the importance of symptoms. The innovative point of this study is combining the degree of diagnosis of syndrome of Eastern medicine with that of disease of Western medicine when both medicines are associated to a common "disease" name to obtain more accurate diagnosis. Moreover, the importance of symptoms in the inference rules in medical expert systems still has an important role in the diagnosis of syndromes. Based on this approach, the system can adapt more with real clinical practice of integrated eastern and western medicine diagnosis. Finally, examples are provided to demonstrate the advantage of this approach.																	1343-0130	1883-8014				SEP	2020	24	5					609	614															
J								Improving Fairness in IEEE 802.11 EDCA Ad Hoc Networks Based on Fuzzy Logic	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										IEEE 802.11; fuzzy logic; MAC control; QoS; EDCA		Wireless ad hoc network is a self-configurable and dynamically distributed network in which stations can move freely. In the ad hoc network, some flows have difficulty in accessing the channel due to contention at both the medium-access control (MAC) and link layers. The IEEE 802.11 protocol is currently the de facto standard for wireless networks. It uses enhanced distributed channel access (EDCA) method to access the transmission environment of each type of data flow. The size of the contention window (CW) in EDCA is related to the probability of accessing the channel of each flow. In our approach, useful information is obtained from the physical, MAC, and link layers. A fuzzy logic system is then used to adjust the size of CW to rely on such value, thereby improving the fairness index of data flows (voice, video, best effort) in IEEE 802.11 EDCA. The simulation results show that the proposed method can improve the throughput and fairness index of data flows.																	1343-0130	1883-8014				SEP	2020	24	5					615	620															
J								Navigation Model for a Robot as a Human Group Member to Adapt to Changing Conditions of Personal Space	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										social robotics; human-aware navigation; human-robot interaction; robot navigation; group norm		In the present paper, we propose a robotic model to help determine a robot's position under the changing conditions of human personal space in a human-robot group. Recently, several attempts have been made to develop personal robots suitable for human communities. Determining a robot's position is important not only to avoid collisions with humans but also to maintain a socially acceptable distance from them. Interpersonal space maintained by persons in a community depends on the particular context and situations. Therefore, robots need to determine their own positions while considering the positions of other persons and evaluating the changes made in their personal space. To address this problem, we proposed a robot navigation model and examined whether the experiment participants could distinguish the robot's trajectory from the human's trajectory in the experimental scenario. We prepared a scenario in which robots in a group needed to keep an appropriate distance in a three-dimensional space. The experiment participants provided their impressions on robot movements while watching the records representing the scenario. The results indicate that (1) a robot using the proposed model is able to follow the other group members and (2) the experiment participants were not sure whether the trajectories of the robots were controlled by humans and by the proposed model. Therefore, we conclude that the proposed model generates suitable trajectories in robot groups.																	1343-0130	1883-8014				SEP	2020	24	5					621	629															
