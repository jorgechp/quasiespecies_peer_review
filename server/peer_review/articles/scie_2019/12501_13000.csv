PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								INTELLIGENT CONVOLUTIONAL MESH-BASED ENCRYPTION TECHNIQUE AUGMENTED WITH FUZZY MASKING OPERATIONS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Encryption technique; Convolutional mesh-based mapping; Substitution/Inverse substitution methods; Block encoding/decoding; Block diffusion; Key expansion	AES	In this digitally-led world, users either digitally store or transmit their sensitive information. This information is vulnerable to all types of threats unless properly secured. In this paper, we address the problem by presenting a new encryption technique. The proposed technique introduces several processes and operations that make it extremely effective. First, the substitution process makes deep changes to its input, thereby greatly weakening the relationship between the input and the output. Second, the diffusion processes highly increase the avalanche effect. Third, the technique intelligently inserts random noises in the encryption to further increase the ciphertext confusion. Fourth, the technique presents highly effective encoding operation that adds further diffusion to the ciphertext. The conducted experiments on our prototype implementation showed that our technique is effective and passed important security tests.																	1349-4198	1349-418X				FEB	2020	16	1					257	282		10.24507/ijicic.16.01.257													
J								A TRANSFORMED SALP SWARM ALGORITHM ON CONTAINER DEPLOYMENT PROBLEM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Transformed salp swarm algorithm; Tent chaotic distribution; Nonlinear convergence factor; Microservice architecture; Container deployment	OPTIMIZER	Salp Swarm Algorithm (SSA) is a meta-heuristic algorithm for solving single-objective optimization problems with satisfying performance in the exploration phase. However, the accuracy in the exploitation phase still remains a problem to be solved. The paper presented a Transformed SSA (TSSA). Firstly, the tent chaotic sequence was used to guarantee the uniformity of solution distribution. Secondly, the exploration mode was changed to improve the accuracy of solutions. Lastly, an improved exploitation mode was introduced to supplant the original way. The TSSA was compared with PSO algorithm and multiple newly-proposed heuristic algorithms in 29 widely-accepted test functions. We also applied the proposed TSSA to container deployment problem in a microservice architecture; in this scenario, each microservice ran in an isolated docker container. The results of these experiments demonstrated that TSSA performed better than other algorithms in most test functions and the container deployment problem.																	1349-4198	1349-418X				FEB	2020	16	1					283	299		10.24507/ijicic.16.01.283													
J								VISUAL TRACKING-BASED HAND GESTURE RECOGNITION USING BACKPROPAGATION NEURAL NETWORK	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Hand gesture; Color filter; Visual tracking; Backpropagation neural network		This article proposed a hand gesture recognition using backpropagation neural network based on visual tracking. The proposed algorithm is passed through three stages, namely, hand region extraction, hand feature extraction and hand gesture recognition. The hand region is extracted from the background using skin color detection based on YCbCr color filter. The extracted hand region is then converted to gray scale and binary image to speed up processing time. The hand feature is obtained from the binary image of the detected hand region by dividing the feature into six regions. The last stage is the recognition of the hand gesture which is performed based on the backpropagation neural network. The six regions of the hand feature of each gesture become the input data of the neural network. In order to accommodate the six regions of the hand feature, the proposed method implemented six nodes on the input layer. Number of the hidden layers is two with ten nodes on each. The output layer has four nodes to accommodate ten output of the recognitions process. The hand gestures to be recognized are a set of ten hand gestures, namely: Stop, One, Two, Three, Four, Hello, Yes, No, OK and Call. The experiments on each hand gesture showed that our proposed algorithm can reach the good performance of recognition rate with minimum result of 80%, maximum result of 947 and average result of 86.67%.																	1349-4198	1349-418X				FEB	2020	16	1					301	313		10.24507/ijicic.16.01.301													
J								3D ONLINE PATH PLANNING OF UAV BASED ON IMPROVED DIFFERENTIAL EVOLUTION AND MODEL PREDICTIVE CONTROL	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Path planning; Model predictive control; Artificial potential field; Differential evolution; Unmanned aerial vehicle		This paper presents an efficient 3D online path planning algorithm for UAV flying in partially known environment. The algorithm integrates model predictive control (MPC) and differential evolution (DE) as the planning strategy. In the initial stage, the artificial potential field (APF) model is developed to describe the mutual effect between the UAV and the surrounding environments. Afterwards, a novel objective function is proposed to address the optimization problem of multi-objective and multi-constraints, which take into account the path length, the smoothness degree of a path and the safety of a path. In addition, the multiple constraints based on the realistic scenarios are taken into account, including maximum acceleration, maximum velocity, map and threat constraints. Then, the improved differential evolution algorithm based on the theory of MPC, is developed to optimize the objective function to find the optimal path. Finally, to show the high performance of the proposed method, we compare the proposed algorithm with the existing optimization algorithms and several extended algorithms. The results reveal that the proposed algorithm not only produces an optimal plan for UAV in a local known 3D environment, but also has better performances in terms of running time and stability.																	1349-4198	1349-418X				FEB	2020	16	1					315	329		10.24507/ijicic.16.01.315													
J								A NEW APPROACH TO GENERATE MULTI S-BOXES BASED ON RNA COMPUTING	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Multi S-box; RNA; Bio-computing; DNA map rules	ENCRYPTION; CHAOS	Many scientists have tried to design new security methods in the domains of cryptography and steganography that are inspired by biological techniques, such as Deoxyribose Nucleic Acid (DNA), Ribonucleic Acid (RNA), Messenger RNA (mRNA), and other bio-molecular methods. Encryption techniques are typically applied to preserving the confidentiality of data, and this work aims to increase the protection of data through modern cryptographic methods. Based on DNA computing, our approach increases confusion through the design of multiple S-boxes. First, a new mRNA-based S-box generates different S-boxes for each user, and a secret key recursively creates these S-boxes until obtaining the required number of S-boxes. This proposed design passes S-box test criteria effectively, including invertibility, balance, completeness, avalanche, and strict avalanche. When changing only one bit in the key used to generate the S-boxes, the results show the newly constructed S-box changes with approximately 99% differences from the original. With comparisons between our technique to other searches, we found it converges to an optimal solution faster in abstracted steps. This new method can be leveraged in block cipher algorithms, such as AES, DES, and Ghost.																	1349-4198	1349-418X				FEB	2020	16	1					331	348		10.24507/ijicic.16.01.331													
J								STRATEGIC ANALYSIS IN MARKOVIAN QUEUES WITH A SINGLE WORKING VACATION AND MULTIPLE VACATIONS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Queueing systems; Equilibrium strategies; Multiple vacations; Working vacation; Social welfare	EQUILIBRIUM BALKING STRATEGIES; SOCIAL OPTIMIZATION; QUEUING SYSTEM; M/M/1 QUEUE; BEHAVIOR; SERVER	This paper considers the customers' equilibrium behavior in Markovian queues with a single working vacation and multiple vacations. That is, if there are customers in the queue upon the end of the working vacation, the new regular busy period will start. Otherwise, the server begins a classical vacation. Arriving customers decide whether to join the system or balk based on the system states and a linear reward-cost structure. Firstly, we study the expected sojourn time of customers in the fully observable and almost observable queues. Secondly, we derive equilibrium strategies for the customers for two cases and analyze the customers' strategic behavior and social welfare under these strategies. Finally, the effect of the information levels as well as system parameters on equilibrium strategies and social welfare are illustrated by numerical examples. We observe that equilibrium thresholds for the almost observable queues are contained within the range between thresholds for the fully observable queues. Moreover, we also find that the state of the server informed to the customers is not necessarily beneficial to social welfare.																	1349-4198	1349-418X				FEB	2020	16	1					349	366		10.24507/ijicic.16.01.349													
J								THE DIGITAL IMAGE WATERMARKING SCHEME USING LOW FREQUENCY CONSTRUCTION AND HISTOGRAM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Image watermarking; Robust watermarking; Histogram; Low frequency construction	ROBUST; DCT; EXPANSION; TRANSFORM; DWT; SVD	In order to protect the copyright authority of important digital images, the non-blind watermarking scheme is allowed. In this paper, we propose a histogram based non-blind watermarking scheme with high robustness. We consider constructing or enhancing the feature of low frequency in the carrier image when embedding the watermark by modifying the value of pixels according to the histogram. In the traditional histogram based watermarking scheme, the selection of modified pixels often adopts the random selection method, which will cause the noise texture in the image. The traditional transform domain based watermarking scheme will also generate objectionable texture features in the carrier image. By calculating the weights of pixels at different positions, we choose the most suitable pixel to modify. The modified pixel will be equal or close to the surrounding pixels after the modifying. The region with equal or close pixel values can form a feature of low frequency. And the feature of low frequency is very robust to many attacks. Experiments show that the proposed method can effectively deal with compression, scaling, and noise attacks, and can resist rotation attacks in a certain extent.																	1349-4198	1349-418X				FEB	2020	16	1					367	384		10.24507/ijicic.16.01.367													
J								CONTROLLABILITY AND OBSERVABILITY FOR A CLASS OF DYNAMIC SYSTEMS WITH INTERNAL CAUSES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Dynamic system; Internal cause; Controllability; Observability	LOCAL OBSERVABILITY	In this paper, controllability and observability are investigated for a dynamic system with an internal cause. The dynamic system is represented as a nonlinear model which is linearized to a linear one by a Taylor expansion method. Moreover, Gramian and rank criteria are developed to analyze the controllability and observability for the dynamic system. Based on the developed criteria, logical relationships of input-state and state-output can be analyzed profoundly for the dynamic system. Finally, an example is provided to verify the controllability and observability conclusions.																	1349-4198	1349-418X				FEB	2020	16	1					385	393		10.24507/ijicic.16.01.385													
J								ROUTING RESOURCES REDUCTION OF VIRTUAL FPGA FOR SPECIFIC APPLICATION SETS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Reconfigurable computing; Overlay; Virtual FPGA; Routing	ARCHITECTURE	A fine-grained FPGA overlay is a virtual device layer (vFPGA) implemented on a physical commercial FPGA. A vFPGA can provide advantages such as bitstream portability and design that works with third-party CAD tools. However, present vFPGAs impose large resource and performance overheads on the underlying FPGA. In this work, we focus on reducing these resource overheads by implementing an application-specific routing architecture for vFPGAs. Because the routing circuits occupy the majority of resources of typical FPGAs, removing routing channels not used for a given application set can significantly reduce the resources required for a vFPGA. By evaluating the proposed application-specific routing, using our previously proposed scalable-logic-module (SLM) architecture on 5 x 5 tile array of 7-input logic elements, our approach reduces the number of lookup tables and flip-flop gates required on the physical FPGA by 40% and 39%, respectively.																	1349-4198	1349-418X				FEB	2020	16	1					395	403		10.24507/ijicic.16.01.395													
J								GENERATION OF ADVERSARIAL EXAMPLES USING ADAPTIVE DIFFERENTIAL EVOLUTION	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Adversarial examples; Differential evolution; JADE		Deep neural networks (DNNs) were shown to be vulnerable to adversarial examples which contain small perturbations. Attacks based on adversarial examples can be classified as either white-box or black-box attacks. In the white box attacks the adversary has complete knowledge about the model being attacked. Black-box attacks are perfromed without any internal model information. As one of the black-box attacks on computer vision, a method of generating adversarial examples using differential evolution (DE) has been reported. DE is as a stochastic direct search method using population, and enables to generate adversarial examples without having access to any information about the network parameter values or their gradients. In this paper, we generate adversarial examples using JADE, which is a variant of adaptive DE. JADE employs a control parameter adaptation mechanism and exhibits high accuracy and rapid convergence in various optimization problems. The effectiveness of the generation of adversarial examples using JADE is examined and discussed by experiments of adversarial attacks against state-of-the-art DNNs.																	1349-4198	1349-418X				FEB	2020	16	1					405	414		10.24507/ijicic.16.01.405													
J								Multi-label zero-shot human action recognition via joint latent ranking embedding	NEURAL NETWORKS										Human action recognition; Multi-label learning; Zero-shot learning; Joint latent ranking embedding; Weakly supervised learning		Human action recognition is one of the most challenging tasks in computer vision. Most of the existing works in human action recognition are limited to single-label classification. A real-world video stream, however, often contains multiple human actions. Such a video stream is usually annotated collectively with a set of relevant human action labels, which leads to a multi-label learning problem. Furthermore, there are a great number of meaningful human actions in reality but it would be extremely difficult, if not impossible, to collect/annotate sufficient video clips regarding all these human actions for training a supervised learning model. In this paper, we formulate a real-world human action recognition task as a multi-label zero-shot learning problem. To address this problem, a joint latent ranking embedding framework is proposed. Our framework holistically tackles the issue of unknown temporal boundaries between different actions within a video clip for multi-label learning and exploits the side information regarding the semantic relationship between different human actions for zero-shot learning. Specifically, our framework consists of two component neural networks for visual and semantic embedding respectively. Thus, multi-label zero-shot recognition is done by measuring relatedness scores of concerned action labels to a test video clip in the joint latent visual and semantic embedding spaces. We evaluate our framework in different settings, including a novel data split scheme designed especially for evaluating multi-label zero-shot learning. The experimental results on two weakly annotated multi-label human action datasets (i.e. Breakfast and Charades) demonstrate the effectiveness of our framework. Crown Copyright (c) 2019 Published by Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						1	23		10.1016/j.neunet.2019.09.029													
J								Joint Ranking SVM and Binary Relevance with robust Low-rank learning for multi-label classification	NEURAL NETWORKS										Multi-label classification; Rank-SVM; Binary Relevance; Robust Low-rank learning; Kernel methods	FRAMEWORK; LIBRARY	Multi-label classification studies the task where each example belongs to multiple labels simultaneously. As a representative method, Ranking Support Vector Machine (Rank-SVM) aims to minimize the Ranking Loss and can also mitigate the negative influence of the class-imbalance issue. However, due to its stacking-style way for thresholding, it may suffer error accumulation and thus reduces the final classification performance. Binary Relevance (BR) is another typical method, which aims to minimize the Hamming Loss and only needs one-step learning. Nevertheless, it might have the class-imbalance issue and does not take into account label correlations. To address the above issues, we propose a novel multi-label classification model, which joints Ranking support vector machine and Binary Relevance with robust Low-rank learning (RBRL). RBRL inherits the ranking loss minimization advantages of Rank-SVM, and thus overcomes the disadvantages of BR suffering the class-imbalance issue and ignoring the label correlations. Meanwhile, it utilizes the hamming loss minimization and one-step learning advantages of BR, and thus tackles the disadvantages of Rank-SVM including another thresholding learning step. Besides, a low-rank constraint is utilized to further exploit high-order label correlations under the assumption of low dimensional label space. Furthermore, to achieve nonlinear multi-label classifiers, we derive the kernelization RBRL. Two accelerated proximal gradient methods (APG) are used to solve the optimization problems efficiently. Extensive comparative experiments with several state-of-the-art methods illustrate a highly competitive or superior performance of our method RBRL. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						24	39		10.1016/j.neunet.2019.10.002													
J								A smoothing neural network for minimization l(1)-l(p) in sparse signal reconstruction with measurement noises	NEURAL NETWORKS										Neural network; l(1)-norm minimization; l(p)-norm (2 >= p >= 1); Smoothing approximation	GRADIENT PROJECTION; OPTIMIZATION METHOD; RECOVERY; REPRESENTATION; NONSMOOTH; SELECTION	This paper investigates a smoothing neural network (SNN) to solve a robust sparse signal reconstruction in compressed sensing (CS), where the objective function is nonsmooth l(1)-norm and the feasible set satisfies an inequality of l(p)-norm (2 >= p >= 1) which is used for measuring residual errors. With a smoothing approximate technique, the non-smooth and non-Lipschitz continuous issues of the l(1)-norm and the gradient of l(p)-norm can be solved efficiently. We propose a SNN which is modeled by a differential equation and give its circuit implementation. In this case, we prove the proposed SNN converges to the optimal of considered problem. Simulation results are discussed to demonstrate the efficiency of the proposed algorithm. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						40	53		10.1016/j.neunet.2019.10.006													
J								A broad class of discrete-time hypercomplex-valued Hopfield neural networks	NEURAL NETWORKS										Hopfield neural network; Hypercomplex-valued neural network; Stability analysis; Clifford algebra; Cayley-Dickson algebra	ASSOCIATIVE MEMORY; CLASSIFICATION	In this paper, we address the stability of a broad class of discrete-time hypercomplex-valued Hopfield-type neural networks. To ensure the neural networks belonging to this class always settle down at a stationary state, we introduce novel hypercomplex number systems referred to as real-part associative hypercomplex number systems. Real-part associative hypercomplex number systems generalize the well-known Cayley-Dickson algebras and real Clifford algebras and include the systems of real numbers, complex numbers, dual numbers, hyperbolic numbers, quaternions, tessarines, and octonions as particular instances. Apart from the novel hypercomplex number systems, we introduce a family of hypercomplex-valued activation functions called B-projection functions. Broadly speaking, a B-projection function projects the activation potential onto the set of all possible states of a hypercomplex-valued neuron. Using the theory presented in this paper, we confirm the stability analysis of several discrete-time hypercomplex-valued Hopfield-type neural networks from the literature. Moreover, we introduce and provide the stability analysis of a general class of Hopfield-type neural networks on Cayley-Dickson algebras. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						54	67		10.1016/j.neunet.2019.09.040													
J								Constructing multilayered neural networks with sparse, data-driven connectivity using biologically-inspired, complementary, homeostatic mechanisms	NEURAL NETWORKS										Synaptogenesis; Apoptosis; Brain development; Energy efficient; Unsupervised learning; Neural network	NEUROTROPHIC FACTOR; CELL-DEATH; SYNAPSES; NEURONS; OUTGROWTH; BRAIN	The immense complexity of the brain requires that it be built and controlled by intrinsic, self-regulating mechanisms. One such mechanism, the formation of new connections via synaptogenesis, plays a central role in neuronal connectivity and, ultimately, performance. Adaptive synaptogenesis networks combine synaptogenesis, associative synaptic modification, and synaptic shedding to construct sparse networks. Here, inspired by neuroscientific observations, novel aspects of brain development are incorporated into adaptive synaptogenesis. The extensions include: (i) multiple layers, (ii) neuron survival and death based on information transmission, and (iii) bigrade growth factor signaling to control the onset of synaptogenesis in succeeding layers and to control neuron survival and death in preceding layers. Also guiding this research is the assumption that brains must achieve a compromise between good performance and low energy expenditures. Simulations of the network model demonstrate the parametric and functional control of both performance and energy expenditures, where performance is measured in terms of information loss and classification errors, and energy expenditures are assumed to be a monotonically increasing function of the number of neurons. Major insights from this study include (a) the key role a neural layer between two other layers has in controlling synaptogenesis and neuron elimination, (b) the performance and energy-savings benefits of delaying the onset of synaptogenesis in a succeeding layer, and (c) how the elimination of neurons in a preceding layer provides energy savings, code compression, and can be accomplished without significantly degrading information transfer or classification performance. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						68	93		10.1016/j.neunet.2019.09.025													
J								A new learning paradigm for random vector functional-link network: RVFL	NEURAL NETWORKS										RVFL; KRVFL; Learning using privileged information; The Rademacher complexity; SVM; Random vector functional link networks	NEURAL-NETWORKS; PRIVILEGED INFORMATION; CLASSIFICATION; WEIGHTS; PLUS; SVM; NET	In school, a teacher plays an important role in various classroom teaching patterns. Likewise to this human learning activity, the learning using privileged information (LUPI) paradigm provides additional information generated by the teacher to 'teach' learning models during the training stage. Therefore, this novel learning paradigm is a typical Teacher-Student Interaction mechanism. This paper is the first to present a random vector functional link (RVFL) network based on the LUPI paradigm, called RVFL+. The novel RVFL+ incorporates the LUPI paradigm that can leverage additional source of information into the RVFL, which offers an alternative way to train the RVFL. Rather than simply combining two existing approaches, the newly-derived RVFL+ fills the gap between classical randomized neural networks and the newfashioned LUPI paradigm. Moreover, the proposed RVFL+ can perform in conjunction with the kernel trick for highly complicated nonlinear feature learning, termed KRVFL+. Furthermore, the statistical property of the proposed RVFL+ is investigated, and the authors present a sharp and high-quality generalization error bound based on the Rademacher complexity. Competitive experimental results on 14 real-world datasets illustrate the great effectiveness and efficiency of the novel RVFL+ and KRVFL+, which can achieve better generalization performance than state-of-the-art methods. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						94	105		10.1016/j.neunet.2019.09.039													
J								Effects of infinite occurrence of hybrid impulses with quasi-synchronization of parameter mismatched neural networks	NEURAL NETWORKS										Neural networks; Quasi synchronization; Hybrid impulses; Parameter mismatch; Mixed time-varying delays	EXPONENTIAL SYNCHRONIZATION; DYNAMICAL NETWORKS; DELAYS; DISCRETE; STABILITY; ARRAY	This article is deeply concerned with the effects of hybrid impulses on quasi-synchronization of neural networks with mixed time-varying delays and parameter mismatches. Hybrid impulses allow synchronizing as well as desynchronizing impulses in one impulsive sequence, so their infinite time occurrence with the system may destroy the synchronization process. Therefore, the effective hybrid impulsive controller has been designed to deal with the difficulties in achieving the quasi-synchronization under the effects of hybrid impulses, which occur all the time, but their density of occurrence gradually decrease. In addition, the new concepts of average impulsive interval and average impulsive gain have been applied to cope with the simultaneous existence of synchronizing and desynchronizing impulses. Based on the Lyapunov method together with the extended comparison principle and the formula of variation of parameters for mixed time-varying delayed impulsive system, the delay-dependent sufficient criteria of quasi-synchronization have been derived for two separate cases, viz., T-alpha < infinity and T-alpha = infinity. Finally, the efficiency of the theoretical results has been illustrated by providing two numerical examples. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						106	116		10.1016/j.neunet.2019.10.007													
J								Regularized correntropy criterion based semi-supervised ELM	NEURAL NETWORKS										Semi-supervised learning; Extreme learning machine; Regularized correntropy criterion; Mean square error	EXTREME LEARNING-MACHINE; KERNEL; REGRESSION	Along with the explosive growing of data, semi-supervised learning attracts increasing attention in the past years due to its powerful capability in labeling unlabeled data and knowledge mining. As an emerging method, the semi-supervised extreme learning machine (SSELM), that builds on ELM, has been developed for data classification and shown superiorities in learning efficiency and accuracy. However, the optimization of SSELM as well as most of the other ELMs is generally based on the mean square error (MSE) criterion, which has been shown less effective in dealing with non-Gaussian noises. In this paper, a robust regularized correntropy criterion based SSELM (RC-SSELM) is developed. The optimization of the output weight matrix of RC-SSELM is derived by the fixed-point iteration based approach. A convergent analysis of the proposed RC-SSELM is presented based on the half-quadratic optimization technique. Experimental results on 4 synthetic datasets and 13 benchmark UCI datasets are provided to show the superiorities of the proposed RC-SSELM over SSELM and other state-of-the-art methods. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						117	129		10.1016/j.neunet.2019.09.030													
J								Spiking Neural Networks applied to the classification of motor tasks in EEG signals	NEURAL NETWORKS										Spiking Neural Network; Izhikevich model; EEG signals; Motor imagery; Power Spectral Density; Wavelet Decomposition	SPATIAL-PATTERNS; BCI SYSTEM; IMAGERY; RHYTHM; RECOGNITION; INTERFACE; MU	Motivated by the recent progress of Spiking Neural Network (SNN) models in pattern recognition, we report on the development and evaluation of brain signal classifiers based on SNNs. The work shows the capabilities of this type of Spiking Neurons in the recognition of motor imagery tasks from EEG signals and compares their performance with other traditional classifiers commonly used in this application. This work includes two stages: the first stage consists of comparing the performance of the SNN models against some traditional neural network models. The second stage, compares the SNN models performance in two input conditions: input features with constant values and input features with temporal information. The EEG signals employed in this work represent five motor imagery tasks: i.e. rest, left hand, right hand, foot and tongue movements. These EEG signals were obtained from a public database provided by the Technological University of Graz (Brunner et al., 2008). The feature extraction stage was performed by applying two algorithms: power spectral density and wavelet decomposition. Likewise, this work uses raw EEG signals for the second stage of the problem solution. All of the models were evaluated in the classification between two motor imagery tasks. This work demonstrates that with a smaller number of Spiking neurons, simple problems can be solved. Better results are obtained by using patterns with temporal information, thereby exploiting the capabilities of the SNNs. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						130	143		10.1016/j.neunet.2019.09.037													
J								A consensus algorithm based on collective neurodynamic system for distributed optimization with linear and bound constraints	NEURAL NETWORKS										Distributed optimization; Collective neurodynamic system; Consensus algorithm; Lyapunov function	RECURRENT NEURAL-NETWORK; CONVEX-OPTIMIZATION	In this paper, an algorithm based on collective neurodynamic system is investigated for distributed constrained convex optimization, whose objective function is a sum of smooth convex functions and non-smooth L-1-norm functions. Inspired by recent advances in distributed convex optimization, the continuous-time and discrete-time distributed optimization algorithms described by collective neurodynamic systems are proposed. In the systems, each of the smooth objective functions is allocated to each node as well as each of the L-1-norm functions. However, the L-1-norm functions are realized by projection operators. Meanwhile, each node satisfies the local linear and bound constraints. Then a connected network is constituted from all the nodes with consensus to find the optimal solutions. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						144	151		10.1016/j.neunet.2019.10.008													
J								l(1)-gain filter design of discrete-time positive neural networks with mixed delays	NEURAL NETWORKS										Positive neural networks; Mixed delays; Filtering analysis; l(1)-gain performance	INFINITY STATE ESTIMATION; EXPONENTIAL STABILITY; LEAKAGE DELAY; CONVERGENCE; EQUILIBRIUM; BIFURCATION; CRITERIA	This paper mainly focuses on the filter design with l(1)-gain disturbance attenuation performance for a class of discrete-time positive neural networks. Discrete and distributed time-varying delays occurring in neuron transmission are taken into account. Especially, the probabilistic distribution of distributed delays is described by a Bernoulli random process in the system model. First, criteria on the positiveness and the unique equilibrium of discrete-time neural networks are presented. Second, through linear Lyapunov method, sufficient conditions for globally asymptotic stability with l(1)-gain disturbance attenuation performance of positive neural networks are proposed. Third, using the results obtained above, criteria on l(1)-gain stability of the established filtering error system are presented, based on which a linear programming (LP) approach is put forward to design the desired positive filter. Finally, two examples of applications to water distribution network and genetic regulatory network are given to demonstrate the effectiveness and applicability of the derived results. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						152	162		10.1016/j.neunet.2019.10.004													
J								A human-in-the-loop deep learning paradigm for synergic visual evaluation in children	NEURAL NETWORKS										Evaluating the visual acuity of children; Human-in-the-loop; Deep learning; Object localization; Image identification; Integration of software and hardware	INFANTS; CLASSIFICATION; ACUITY; SYSTEM	Visual development during early childhood is a vital process. Examining the visual acuity of children is essential for early detection of visual abnormalities, but performing visual examination in children is challenging. Here, we developed a human-in-the-loop deep learning (DL) paradigm that combines traditional vision examination and DL with integration of software and hardware, thus facilitating the execution of vision examinations, offsetting the shortcomings of human doctors, and improving the abilities of both DL and doctors to evaluate the vision of children. Because this paradigm contains two rounds (a human round and DL round), doctors can learn from DL and the two can mutually supervise each other such that the precision of the DL system in evaluating the visual acuity of children is improved. Based on DL-based object localization and image identification, the experiences of doctors and the videos captured in the first round, the DL system in the second round can simulate doctors in evaluating the visual acuity of children with a final accuracy of 75.54%. For comparison, we also assessed an automatic deep learning method that did not consider the experiences of doctors, but its performance was not satisfactory. This entire paradigm can evaluate the visual acuity of children more accurately than humans alone. Furthermore, the paradigm facilitates automatic evaluation of the vision of children with a wearable device. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						163	173		10.1016/j.neunet.2019.10.003													
J								Learning Cascade Attention for fine-grained image classification	NEURAL NETWORKS										Deep Convolutional Neural Network; Fine-grained image classification; Attention model		Fine-grained image classification is a challenging task due to the large inter-class difference and small intra-class difference. In this paper, we propose a novel Cascade Attention Model using the Deep Convolutional Neural Network to address this problem. Our method first leverages the Spatial Confusion Attention to identify ambiguous areas of the input image. Two constraint loss functions are proposed: the Spatial Mask loss and the Spatial And loss; Second, the Cross-network Attention, applying different pre-train parameters to the two stream architecture. Also, two novel loss functions called Cross-network Similarity loss and Satisfied Rank loss are proposed to make the two-stream networks reinforce each other and get better results. Finally, the Network Fusion Attention merges intermediate results with the novel entropy add strategy to obtain the final predictions. All of these modules can work together and can be trained end to end. Besides, different from previous works, our model is fully weak-supervised and fully paralleled, which leads to easier generalization and faster computation. We obtain the state-of-the-art performance on three challenge benchmark datasets (CUB-200-2011, FGVC-Aircraft and Flower 102) with results of 90.8%, 92.1%, and 98.5%, respectively. The model will be publicly available at https://github.com/billzyx/LCA-CNN. Published by Elsevier Ltd.																	0893-6080	1879-2782				FEB	2020	122						174	182		10.1016/j.neunet.2019.10.009													
J								The robustness-fidelity trade-off in Grow When Required neural networks performing continuous novelty detection	NEURAL NETWORKS										Novelty detection; Self-organised neural networks; Unsupervised learning	ONLINE NOVELTY; MOBILE	Novelty detection allows robots to recognise unexpected data in their sensory field and can thus be utilised in applications such as reconnaissance, surveillance, self-monitoring, etc. We assess the suitability of Grow When Required Neural Networks (GWRNNs) for detecting novel features in a robot's visual input in the context of randomised physics-based simulation environments. We compare, for the first time, several GWRNN architectures, including new Plastic architectures in which the number of activated input connections for individual neurons is adjusted dynamically as the robot senses a varying number of salient environmental features. The networks are studied in both one-shot and continuous novelty reporting tasks and we demonstrate that there is a trade-off, not unique to this type of novelty detector, between robustness and fidelity. Robustness is achieved through generalisation over the input space which minimises the impact of network parameters on performance, whereas high fidelity results from learning detailed models of the input space and is especially important when a robot encounters multiple novelties consecutively or must detect that previously encountered objects have disappeared from the environment. We propose a number of improvements that could mitigate the robustness-fidelity trade-off and demonstrate one of them, where localisation information is added to the input data stream being monitored. (c) 2019 Published by Elsevier Ltd.																	0893-6080	1879-2782				FEB	2020	122						183	195		10.1016/j.neunet.2019.10.015													
J								On the accuracy and computational cost of spiking neuron implementation	NEURAL NETWORKS										Spiking neuron; Accuracy; Computational cost; Numerical method; Simulation; Time step	ABSOLUTE ERROR MAE; LARGE-SCALE MODEL; NUMERICAL-INTEGRATION; HUXLEY EQUATIONS; SIMULATION; NETWORKS; 3RD-GENERATION; PLASTICITY; HODGKIN; RMSE	Since more than a decade ago, three statements about spiking neuron (SN) implementations have been widely accepted: 1) Hodgkin and Huxley (HH) model is computationally prohibitive, 2) Izhikevich (IZH) artificial neuron is as efficient as Leaky Integrate-and-Fire (LIF) model, and 3) IZH model is more efficient than HH model (Izhikevich, 2004). As suggested by Hodgkin and Huxley (1952), their model operates in two modes: by using the a's and beta's rate functions directly (HH model) and by storing them into tables (HHT model) for computational cost reduction. Recently, it has been stated that: 1) HHT model (HH using tables) is not prohibitive, 2) IZH model is not efficient, and 3) both HHT and IZH models are comparable in computational cost (Skocik & Long, 2014). That controversy shows that there is no consensus concerning SN simulation capacities. Hence, in this work, we introduce a refined approach, based on the multiobjective optimization theory, describing the SN simulation capacities and ultimately choosing optimal simulation parameters. We have used normalized metrics to define the capacity levels of accuracy, computational cost, and efficiency. Normalized metrics allowed comparisons between SNs at the same level or scale. We conducted tests for balanced, lower, and upper boundary conditions under a regular spiking mode with constant and random current stimuli. We found optimal simulation parameters leading to a balance between computational cost and accuracy. Importantly, and, in general, we found that 1) HH model (without using tables) is the most accurate, computationally inexpensive, and efficient, 2) IZH model is the most expensive and inefficient, 3) both LIF and HHT models are the most inaccurate, 4) HHT model is more expensive and inaccurate than HH model due to a's and beta's table discretization, and 5) HHT model is not comparable in computational cost to IZH model. These results refute the theory formulated over a decade ago (Izhikevich, 2004) and go more in-depth in the statements formulated by Skocik and Long (2014). Our statements imply that the number of dimensions or FLOPS in the SNs are theoretical but not practical indicators of the true computational cost. The metric we propose for the computational cost is more precise than FLOPS and was found to be invariant to computer architecture. Moreover, we found that the firing frequency used in previous works is a necessary but an insufficient metric to evaluate the simulation accuracy. We also show that our results are consistent with the theory of numerical methods and the theory of SN discontinuity. Discontinuous SNs, such LIF and IZH models, introduce a considerable error every time a spike is generated. In addition, compared to the constant input current, the random input current increases the computational cost and inaccuracy. Besides, we found that the search for optimal simulation parameters is problem-specific. That is important because most of the previous works have intended to find a general and unique optimal simulation. Here, we show that this solution could not exist because it is a multiobjective optimization problem that depends on several factors. This work sets up a renewed thesis concerning the SN simulation that is useful to several related research areas, including the emergent Deep Spiking Neural Networks. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						196	217		10.1016/j.neunet.2019.09.026													
J								A complementary learning systems approach to temporal difference learning	NEURAL NETWORKS										Complementary learning systems; Reinforcement learning; Hippocampus	VENTRAL STRIATUM; PREDICTION; HIPPOCAMPUS; ENCODE; MODELS	Complementary Learning Systems (CLS) theory suggests that the brain uses a 'neocortical' and a 'hippocampal' learning system to achieve complex behaviour. These two systems are complementary in that the 'neocortical' system relies on slow learning of distributed representations while the 'hippocampal' system relies on fast learning of pattern-separated representations. Both of these systems project to the striatum, which is a key neural structure in the brain's implementation of Reinforcement Learning (RL). Current deep RL approaches share similarities with a 'neocortical' system because they slowly learn distributed representations through backpropagation in Deep Neural Networks (DNNs). An ongoing criticism of such approaches is that they are data inefficient and lack flexibility. CLS theory suggests that the addition of a 'hippocampal' system could address these criticisms. In the present study we propose a novel algorithm known as Complementary Temporal Difference Learning (CTDL), which combines a DNN with a Self-Organizing Map (SOM) to obtain the benefits of both a 'neocortical' and a 'hippocampal' system. Key features of CTDL include the use of Temporal Difference (TD) error to update a SOM and the combination of a SOM and DNN to calculate action values. We evaluate CTDL on Grid World, Cart-Pole and Continuous Mountain Car tasks and show several benefits over the classic Deep Q-Network (DQN) approach. These results demonstrate (1) the utility of complementary learning systems for the evaluation of actions, (2) that the TD error signal is a useful form of communication between the two systems and (3) that our approach extends to both discrete and continuous state and action spaces. (c) 2019 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).																	0893-6080	1879-2782				FEB	2020	122						218	230		10.1016/j.neunet.2019.10.011													
J								Multistability and attraction basins of discrete-time neural networks with nonmonotonic piecewise linear activation functions	NEURAL NETWORKS										Multistability; Discrete-time; Activation function; Instability; Attraction basin; Neural networks	MULTIPERIODICITY	This paper is concerned with multistability and attraction basins of discrete-time neural networks with nonmonotonic piecewise linear activation functions. Under some reasonable conditions, the addressed networks have (2m + 1)(n) equilibrium points. (m + 1)(n) of which are locally asymptotically stable, and the others are unstable. The attraction basins of the locally asymptotically stable equilibrium points are given in the form of hyperspherical regions. These results here, which include existence, uniqueness, locally asymptotical stability, instability and attraction basins of the multiple equilibrium points, generalize and improve the earlier publications. Finally, an illustrative example with numerical simulation is given to show the feasibility and the effectiveness of the theoretical results. The theoretical results and illustrative example indicate that the activation functions improve the storage capacity of neural networks significantly. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						231	238		10.1016/j.neunet.2019.10.005													
J								Multistability of switched neural networks with sigmoidal activation functions under state-dependent switching	NEURAL NETWORKS										Multistability; Switched neural network; State-dependent; Sigmoidal activation function	TIME-VARYING DELAY; STABILITY ANALYSIS; EXPONENTIAL SYNCHRONIZATION; NONLINEAR-SYSTEMS; ROBUST STABILITY; STABILIZATION; CONVERGENCE; DESIGN; MEMORY	This paper presents theoretical results on the multistability of switched neural networks with commonly used sigmoidal activation functions under state-dependent switching. The multistability analysis with such an activation function is difficult because state-space partition is not as straightforward as that with piecewise-linear activations. Sufficient conditions are derived for ascertaining the existence and stability of multiple equilibria. It is shown that the number of stable equilibria of an n-neuron switched neural networks is up to 3(n) under given conditions. In contrast to existing multistability results with piecewise-linear activation functions, the results herein are also applicable to the equilibria at switching points. Four examples are discussed to substantiate the theoretical results. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						239	252		10.1016/j.neunet.2019.10.012													
J								Perceptrons from memristors	NEURAL NETWORKS										Perceptron; Memristor; Backpropagation; Delta rule; Neural network	NEURAL-NETWORKS; BEHAVIOR; DESIGN	Memristors, resistors with memory whose outputs depend on the history of their inputs, have been used with success in neuromorphic architectures, particularly as synapses and non-volatile memories. However, to the best of our knowledge, no model for a network in which both the synapses and the neurons are implemented using memristors has been proposed so far. In the present work we introduce models for single and multilayer perceptrons based exclusively on memristors. We adapt the delta rule to the memristor-based single-layer perceptron and the backpropagation algorithm to the memristor-based multilayer perceptron. Our results show that both perform as expected for perceptrons, including satisfying Minsky-Papert's theorem. As a consequence of the Universal Approximation Theorem, they also show that memristors are universal function approximators. By using memristors for both the neurons and the synapses, our models pave the way for novel memristor-based neural network architectures and algorithms. A neural network based on memristors could show advantages in terms of energy conservation and open up possibilities for other learning systems to be adapted to a memristor-based paradigm, both in the classical and quantum learning realms. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						273	278		10.1016/j.neunet.2019.10.013													
J								Partition level multiview subspace clustering	NEURAL NETWORKS										Multi-view learning; Subspace clustering; Partition space; Information fusion	SPARSE	Multiview clustering has gained increasing attention recently due to its ability to deal with multiple sources (views) data and explore complementary information between different views. Among various methods, multiview subspace clustering methods provide encouraging performance. They mainly integrate the multiview information in the space where the data points lie. Hence, their performance may be deteriorated because of noises existing in each individual view or inconsistent between heterogeneous features. For multiview clustering, the basic premise is that there exists a shared partition among all views. Therefore, the natural space for multiview clustering should be all partitions. Orthogonal to existing methods, we propose to fuse multiview information in partition level following two intuitive assumptions: (i) each partition is a perturbation of the consensus clustering; (ii) the partition that is close to the consensus clustering should be assigned a large weight. Finally, we propose a unified multiview subspace clustering model which incorporates the graph learning from each view, the generation of basic partitions, and the fusion of consensus partition. These three components are seamlessly integrated and can be iteratively boosted by each other towards an overall optimal solution. Experiments on four benchmark datasets demonstrate the efficacy of our approach against the state-of-the-art techniques. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						279	288		10.1016/j.neunet.2019.10.010													
J								Affinity and class probability-based fuzzy support vector machine for imbalanced data sets	NEURAL NETWORKS										Imbalanced data; Fuzzy support vector machine (FSVM); Affinity; Class probability; Kernelknn	CONDENSED NEAREST-NEIGHBOR; CLASSIFICATION; SMOTE; DIAGNOSIS; MODEL; SVM	The learning problem from imbalanced data sets poses a major challenge in data mining community. Although conventional support vector machine can generally show relatively robust performance in dealing with the classification problems of imbalanced data sets, it treats all training samples with the same contribution for learning, which results in the final decision boundary biasing toward the majority class especially in the presence of outliers or noises. In this paper, we propose a new affinity and class probability-based fuzzy support vector machine technique (ACFSVM). The affinity of a majority class sample is calculated according to support vector description domain (SVDD) model trained only by the given majority class training samples in kernel space similar to that used for FSVM learning. The obtained affinity can be used for identifying possible outliers and some border samples existing in the majority class training samples. In order to eliminate the effect of noises, we employ the kernel k-nearest neighbor method to determine the class probability of the majority class samples in the same kernel space as before. The samples with lower class probabilities are more likely to be noises and their contribution for learning seems to be reduced by their low memberships constructed by combining the affinities and the class probabilities. Thus, ACFSVM can pay more attention to the majority class samples with higher affinities and class probabilities while reducing their effects of the ones with lower affinities and class probabilities, eventually skewing the final classification boundary toward the majority class. In addition, the minority class samples are assigned relative high memberships to guarantee their importance for the model learning. The extensive experimental results on the different imbalanced datasets from UCI repository demonstrate that the proposed approach can achieve better generalization performance in terms of G-Mean, F-Measure, and AUC as compared to the other existing imbalanced dataset classification techniques. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						289	307		10.1016/j.neunet.2019.10.016													
J								Model-based optimized phase-deviation deep brain stimulation for Parkinson 's disease	NEURAL NETWORKS										Deep brain stimulation; Synchronization level; Parkinson's disease; Phase deviation	BASAL GANGLIA; SUBTHALAMIC NUCLEUS; CONSTANT-CURRENT; NETWORK; PATHOPHYSIOLOGY; SYNCHRONIZATION; DYNAMICS; PATTERNS; FIDELITY	High-frequency deep brain stimulation (HF-DBS) of the subthalamic nucleus (STN), globus pallidus interna (GPi) and globus pallidus externa (GPe) are often considered as effective methods for the treatment of Parkinson's disease (PD). However, the stimulation of a single nucleus by HF-DBS can cause specific physical damage, produce side effects and usually consume more electrical energy. Therefore, we use a biophysically-based model of basal ganglia-thalamic circuits to explore more effective stimulation patterns to reduce adverse effects and save energy. In this paper, we computationally investigate the combined DBS of two nuclei with the phase deviation between two stimulation waveforms (CDBS). Three different stimulation combination strategies are proposed, i.e., STN and GPe CDBS (SED), STN and GPi CDBS (SID), as well as GPi and GPe CDBS (GGD). Resultantly, it is found that anti-phase CDBS is more effective in improving parkinsonian dynamical properties, including desynchronization of neurons and the recovery of the thalamus relay ability. Detailed simulation investigation shows that anti-phase SED and GGD are superior to SID. Besides, the energy consumption can be largely reduced by SED and GGD (72.5% and 65.5%), compared to HF-DBS. These results provide new insights into the optimal stimulation parameter and target choice of PD, which may be helpful for the clinical practice. (c) 2019 Published by Elsevier Ltd.																	0893-6080	1879-2782				FEB	2020	122						308	319		10.1016/j.neunet.2019.11.001													
J								New approach to global Mittag-Leffler synchronization problem of fractional-order quaternion-valued BAM neural networks based on a new inequality	NEURAL NETWORKS										Fractional-order system; Quaternion-valued neural networks; Bidirectional associative memory; Mittag-Leffler synchronization	ROBUST STABILITY ANALYSIS; TIME-VARYING DELAYS; EXPONENTIAL STABILITY; IMPULSIVE CONTROL; STATE ESTIMATOR; DISCRETE; DESIGN; STABILIZATION; EXISTENCE; CRITERIA	In this paper, a novel kind of neural networks named fractional-order quaternion-valued bidirectional associative memory neural networks (FQVBAMNNs) is formulated. On one hand, applying Hamilton rules in quaternion multiplication which is essentially non-commutative, the system of FQVBAMNNs is separated into eight fractional-order real-valued systems. Meanwhile, the activation functions are considered to be quaternion-valued linear threshold ones which help to reduce the unnecessary computational complexity. On the other hand, based on fractional-order Lyapunov technology, a new fractional-order derivative inequality is established. Mainly by employing the new inequality technique, constructing three novel Lyapunov-Krasovskii functionals (LKFs) and designing simple linear controllers, the global Mittag-Leffler synchronization problems are investigated and the corresponding criteria are acquired for the system of FQVBAMNNs and its special cases such as fractional-order complex-valued BAM neural networks (FCVBAMNNs) and fractional-order real-valued BAM neural networks (FRVBAMNNs), respectively. Finally, two numerical examples are given to show the effectiveness and availability of the proposed results. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						320	337		10.1016/j.neunet.2019.10.017													
J								Local distinguishability aggrandizing network for human anomaly detection	NEURAL NETWORKS										Human anomaly detection; Local input; Distinguishability; Aggrandizing network	EVENT DETECTION	With the growing demand for an intelligent system to prevent abnormal events, many methods have been proposed to detect and locate anomalous behaviors in surveillance videos. However, most of these methods contain two shortcomings mainly: distraction of the network and insufficient discriminating ability. In this paper, we propose a local distinguishability aggrandizing network (LDA-Net) in a supervised manner, consisting of a human detection module and an anomaly detection module. In the human detection module, we obtain segmented patches of specific human subjects and take them as the input of the latter module to focus the network on learning motion characteristics of each person. In addition, considering that the auxiliary information, such as the specific type of an action, can aggrandize the whole network to extract distinguishable detail features of normal and abnormal behaviors, the proposed anomaly detection module comprises a primary binary classification sub-branch and an auxiliary distinguishability aggrandizing sub-branch, through which we can jointly detect anomalies and recognize actions. To further reduce the misclassification of the extremely imbalanced datasets, we design a novel inhibition loss function and embed it into the auxiliary sub-branch of the anomaly detection module. Experiments on several public benchmark datasets for frame-level and pixel-level anomaly detection show that the proposed supervised LDA-Net achieves state-of-the-art results on UCSD Ped2 and Subway Exit datasets. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						364	373		10.1016/j.neunet.2019.11.002													
J								Generative adversarial networks with mixture of t-distributions noise for diverse image generation	NEURAL NETWORKS										Image generation; Generate adversarial networks; Diversity; Mixture of t-distributions; Class codeword		Image generation is a long-standing problem in the machine learning and computer vision areas. In order to generate images with high diversity, we propose a novel model called generative adversarial networks with mixture of t-distributions noise (tGANs). In tGANs, the latent generative space is formulated using a mixture of t-distributions. Particularly, the parameters of the components in the mixture of t-distributions can be learned along with others in the model. To improve the diversity of the generated images in each class, each noise vector and a class codeword are concatenated as the input of the generator of tGANs. In addition, a classification loss is added to both the generator and the discriminator losses to strengthen their performances. We have conducted extensive experiments to compare tGANs with a state-of-the-art pixel by pixel image generation approach, pixelCNN, and related GAN-based models. The experimental results and statistical comparisons demonstrate that tGANs perform significantly better than pixleCNN and related GAN-based models for diverse image generation. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						374	381		10.1016/j.neunet.2019.11.003													
J								Global Mittag-Leffler stability and synchronization of discrete-time fractional-order complex-valued neural networks with time delay	NEURAL NETWORKS										Mittag-Leffler stability; Discrete-time; Synchronization; Fractional-order complex-valued neural networks; Lyapunov's direct method; Time delay	EXPONENTIAL SYNCHRONIZATION; DYNAMICAL ANALYSIS; SYSTEMS	Without decomposing complex-valued systems into real-valued systems, this paper investigates existence, uniqueness, global Mittag-Leffler stability and global Mittag-Leffler synchronization of discrete-time fractional-order complex-valued neural networks (FCVNNs) with time delay. Inspired by Lyapunov's direct method on continuous-time systems, a class of discrete-time FCVNNs is further discussed by employing the fractional-order extension of Lyapunov's direct method. Firstly, by means of contraction mapping theory and Cauchy's inequality, a sufficient condition is presented to ascertain the existence and uniqueness of the equilibrium point for discrete-time FCVNNs. Then, based on the theory of discrete fractional calculus, discrete Laplace transform, the theory of complex functions and discrete Mittag-Leffler functions, a sufficient condition is established for global Mittag-Leffler stability of the proposed networks. Additionally, by applying the Lyapunov's direct method and designing a effective control scheme, the sufficient criterion is derived to ensure the global Mittag-Leffler synchronization of discrete-time FCVNNs. Finally, two numerical examples are also presented to manifest the feasibility and validity of the obtained results. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						382	394		10.1016/j.neunet.2019.11.004													
J								Simultaneously learning affinity matrix and data representations for machine fault diagnosis	NEURAL NETWORKS										Extreme learning machine; Autoencoder; Geometry information; Affinity matrix learning; Representation learning; Machine fault diagnosis		Recently, preserving geometry information of data while learning representations have attracted increasing attention in intelligent machine fault diagnosis. Existing geometry preserving methods require to predefine the similarities between data points in the original data space. The predefined affinity matrix, which is also known as the similarity matrix, is then used to preserve geometry information during the process of representations learning. Hence, the data representations are learned under the assumption of a fixed and known prior knowledge, i.e., similarities between data points. However, the assumed prior knowledge is difficult to precisely determine the real relationships between data points, especially in high dimensional space. Also, using two separated steps to learn affinity matrix and data representations may not be optimal and universal for data classification. In this paper, based on the extreme learning machine autoencoder (ELM-AE), we propose to learn the data representations and the affinity matrix simultaneously. The affinity matrix is treated as a variable and unified in the objective function of ELM-AE. Instead of predefining and fixing the affinity matrix, the proposed method adjusts the similarities by taking into account its capability of capturing the geometry information in both original data space and non-linearly mapped representation space. Meanwhile, the geometry information of original data can be preserved in the embedded representations with the help of the affinity matrix. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed method, and the empirical study also shows it is an efficient tool on machine fault diagnosis. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						395	406		10.1016/j.neunet.2019.11.007													
J								Person identification using fusion of iris and periocular deep features	NEURAL NETWORKS										Person identification; Iris recognition; Periocular recognition; Data augmentation; Deep learning; Rank-level fusion	RECOGNITION	A novel method for person identification based on the fusion of iris and periocular biometrics has been proposed in this paper. The challenges for image acquisition for Near-Infrared or Visual Wavelength lights under constrained and unconstrained environments have been considered here. The proposed system is divided into image preprocessing data augmentation followed by feature learning for classification components. In image preprocessing an annular iris, the portion is segmented out from an eyeball image and then transformed into a fixed-sized image region. The parameters of iris localization have been used to extract the local periocular region. Due to different imaging environments, the images suffer from various noise artifacts which create data insufficiency and complicate the recognition task. To overcome this situation, a novel method for data augmentation technique has been introduced here. For features extraction and classification tasks well-known, VGG16, ResNet50, and Inception-v3 CNN architectures have been employed. The performance due to iris and periocular are fused together to increase the performance of the recognition system. The extensive experimental results have been demonstrated in four benchmark iris databases namely: MMU1, UPOL, CASIA-Iris-distance, and UBIRIS.v2. The comparison with the state-of-the-art methods with respect to these databases shows the robustness and effectiveness of the proposed approach. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						407	419		10.1016/j.neunet.2019.11.009													
J								Realistic spiking neural network: Non-synaptic mechanisms improve convergence in cell assembly	NEURAL NETWORKS										Spiking neural network; Biophysical model; Synchronism; Burst activity; Convergence	EPILEPTIFORM ACTIVITY; HIPPOCAMPUS; SYNCHRONY; EPILEPSY; MODEL; SEIZURES; NEURONS; MEMORY; ORGANIZATION; INFORMATION	Learning in neural networks inspired by brain tissue has been studied for machine learning applications. However, existing works primarily focused on the concept of synaptic weight modulation, and other aspects of neuronal interactions, such as non-synaptic mechanisms, have been neglected. Non-synaptic interaction mechanisms have been shown to play significant roles in the brain, and four classes of these mechanisms can be highlighted: (i) electrotonic coupling; (ii) ephaptic interactions; (iii) electric field effects; and iv) extracellular ionic fluctuations. In this work, we proposed simple rules for learning inspired by recent findings in machine learning adapted to a realistic spiking neural network. We show that the inclusion of non-synaptic interaction mechanisms improves cell assembly convergence. By including extracellular ionic fluctuation represented by the extracellular electrodiffusion in the network, we showed the importance of these mechanisms to improve cell assembly convergence. Additionally, we observed a variety of electrophysiological patterns of neuronal activity, particularly bursting and synchronism when the convergence is improved. (c) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				FEB	2020	122						420	433		10.1016/j.neunet.2019.09.038													
J								Nonparametric clustering for image segmentation	STATISTICAL ANALYSIS AND DATA MINING										image segmentation; kernel smoothing; mode; nonparametric density estimation	GRADIENT VECTOR FLOW; MEAN SHIFT; OPTIMIZATION; ALGORITHM	Image segmentation aims at identifying regions of interest within an image by grouping pixels according to their properties. This task resembles the statistical one of clustering, yet many standard clustering methods fail to meet the basic requirements of image segmentation since the identified segments are often biased toward predetermined shapes and their number is rarely determined automatically. Nonparametric clustering is, in principle, free from these limitations and particularly suitable for the task of image segmentation. We discuss the application of nonparametric clustering to image segmentation and provide an algorithm specific for this task. Pixel similarity is evaluated in terms of the density of the color representation. The adjacency structure of the pixels is exploited to introduce a simple, yet effective method to identify image segments as disconnected high-density regions. The proposed method answers to the need of both segmenting an image and detecting its boundaries and can be seen as a generalization to color images of the class of thresholding methods.																	1932-1864	1932-1872				FEB	2020	13	1					83	97		10.1002/sam.11444													
J								A new method for performance analysis in nonlinear dimensionality reduction	STATISTICAL ANALYSIS AND DATA MINING										dimension reduction; Isomap; local tangent space alignment; manifold; maximum variance unfolding; principal component analysis; rank correlation	NEIGHBORHOOD PRESERVATION; MANIFOLDS	In this paper, we develop a local rank correlation (LRC) measure which quantifies the performance of dimension reduction methods. The LRC is easily interpretable, and robust against the extreme skewness of nearest neighbor distributions in high dimensions. Some benchmark datasets are studied. We find that the LRC closely corresponds to our visual interpretation of the quality of the output. In addition, we demonstrate that the LRC is useful in estimating the intrinsic dimensionality of the original data, and in selecting a suitable value of tuning parameters used in some algorithms.																	1932-1864	1932-1872				FEB	2020	13	1					98	108		10.1002/sam.11445													
J								Defect identification of wind turbine blades based on defect semantic features with transfer feature extractor	NEUROCOMPUTING										Defect identification; Defect semantic feature; Transfer feature extractor; Wind turbine blades	DAMAGE DETECTION; FUSION; CLASSIFICATION; TEXTURE; RECOGNITION; ENTROPY	The monitoring of the status of the wind turbine blades is significant for the wind generation system and currently mainly dependent on manual visual inspections. The variance of the blade defects and the lack of the blade defect images make the defect identification of the wind turbine blades challenging. This paper proposes a defect identification method of wind turbine blades based on defect semantic features with transfer feature extractor. A deep convolutional neural network (DCNN) is built and is trained on the ImageNet Large Scale Visual Recognition Challenge dataset. The deep hierarchical features of the training blade images are extracted by the trained DCNN and fed into a classifier. By training on the labeled blade images, the first n layers of the trained DCNN is selected as the transfer feature extractor to extract the defect semantic features and the defect classifier is also obtained. The blade images can be diagnosed by the defect classifier based on the defect semantic features. The experiments are conducted on a real dataset of wind turbine blade images. The experimental results demonstrate the high learning ability of the proposed method from the small samples and its effectiveness for the defect identification of wind turbine blades. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						1	9		10.1016/j.neucom.2019.09.071													
J								nu-projection twin support vector machine for pattern classification	NEUROCOMPUTING										Twin support vector machine; Projection twin support vector machine; Nonparallel classifier; Kernel trick; Pattern classification	CLASSIFIERS; DIAGNOSIS; SVM	In this paper, we improve the projection twin support vector machine (PTSVM) to a novel nonparallel classifier, termed as nu-PTSVM. Specifically, our nu-PTSVM aims to seek an optimal projection for each class such that, in each projection direction, instances of their own class are clustered around their class center while keep instances of the other class at least one distance away from such center. Different from PTSVM, our nu-PTSVM enjoys the following characteristics: (i) nu-PTSVM is equipped by a more theoretically sound parameter nu, which can be used to control the bounds of fraction of both support vectors and margin-error instances. (ii) By reformulating the least-square loss of within-class instances in primal problems of nu-PTSVM, its dual problems no longer involve the time-costly matrix inversion. (iii) nu-PTSVM behaves consistent between its linear and nonlinear cases. Namely, the kernel trick can be applied directly to nu-PTSVM for its nonlinear extension. Experimental evaluations on both synthetic and real-world datasets demonstrate the feasibility and effectiveness of the proposed approach. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						10	24		10.1016/j.neucom.2019.09.069													
J								A novel approach inspired by optic nerve characteristics for few-shot occluded face recognition	NEUROCOMPUTING										Sparse representation; Adaptive fusion feature; Few-shot learning; Face recognition; Face occlusion; Dictionary learning	CONSISTENT K-SVD; DISCRIMINATIVE DICTIONARY; SPARSE REPRESENTATION; IMAGE; ROBUST; FEATURES; OCCLUSION; TEXTURE; EXTRACTION; REGRESSION	Although there has been a growing body of work for face recognition, it is still a challenging task for faces under occlusion with limited training samples. In this work, we propose a novel framework to address the problem of few-shot occluded face recognition. In particular, inspired by the human being's optic nerves characteristics that humans recognize the face under occlusion using contextual information rather than paying attention to the facial parts, we propose an effective feature extraction approach to capture the local and contextual information for face recognition. To enhance the robustness, we further introduce an adaptive fusion method to incorporate multiple features, including the proposed structural element feature, connected-granule labeling feature, and Reinforced Centrosymmetric Local Binary Pattern (RCSLBP). Final recognition is derived from the fusion of all classification results according to our proposed novel fusion method. Experimental results on three popular face image datasets of AR, Extended Yale B, and LFW demonstrate that our method performs better than many existing ones for few-shot face recognition in the presence of occlusion. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						25	41		10.1016/j.neucom.2019.09.045													
J								An efficient model-level fusion approach for continuous affect recognition from audiovisual signals	NEUROCOMPUTING										Continuous affect recognition; Multi-modal fusion; Model-level fusion; Adaptive weight network; Side information	EMOTION; NETWORKS	Continuous affect recognition has a huge potential in human computer interaction applications. How to efficiently fuse speech and facial information for inferring the affective state of a person from data captured in real-world conditions is a very important issue for continuous affect recognition. Currently, late fusion is usually used in multi-modal continuous affect recognition to improve system performance. However, late fusion ignores the complementarity and redundancy between multiple streams from the different modalities. In this work, we propose an efficient model-level fusion approach for audiovisual continuous affect recognition. First, we propose a LSTM based model-level fusion approach for audiovisual continuous affect recognition. Our approach considers the complementarity and redundancy between multiple streams from different modalities. In addition, our model can efficiently incorporate side information such as gender using adaptive weight network. At last, we design a deep supervision based effective optimization strategy for training the proposed audiovisual continuous affect recognition model. We demonstrate the effectiveness of our approach on the RECOLA dataset. Our experimental results show that the proposed adaptive weight network improves the performance compared to a plain neural network without adaptive weights. Our approach obtains remarkable improvements on both arousal and valence in terms of concordance correlation coefficient (CCC) compared to state-of-the-art early fusion and model-level fusion approaches. Therefore, we believe that our proposed approach gives a promising direction for further improving the performance of audiovisual continuous affect recognition. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						42	53		10.1016/j.neucom.2019.09.037													
J								A novel geodesic flow kernel based domain adaptation approach for intelligent fault diagnosis under varying working condition	NEUROCOMPUTING										Domain adaptation; GFK; Strengthened feature extraction; Z-score normalization	DEEP NEURAL-NETWORKS	Domain adaptation techniques have drawn much attention for mechanical defect diagnosis in recent years. Nevertheless, the traditional domain adaptation approaches may suffer two shortcomings: (1) Poor performance is obtained for many traditional domain adaptation approaches when the sample is insufficient. (2) The diagnosis results are not stable, that is to say, the traditional domain adaptation approaches may have poor robustness. In order to overcome these deficiencies, we propose a novel domain adaptation model named DAGSZ based on geodesic flow kernel (GFK), strengthened feature extraction and Z-score normalization. Firstly, time domain average and square for the power spectral density (PSD) matrix is applied for preprocessing the original vibration data to learn more representative features. Then, the geodesic flow kernel (GFK), an unsupervised domain adaptation method, is adopted for learning the transferable features. Finally, Z-score normalization is employed to normalize the learned transferable features and softmax regression is utilized to classify the health conditions. The real-world dataset of gears and bearings are employed to validate the effectiveness and robustness of our method. The result shows that DAGSZ obtains fairly high detection accuracies and is superior to the existing methods for mechanical fault detection. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						54	64		10.1016/j.neucom.2019.09.081													
J								Position-aware context attention for session-based recommendation	NEUROCOMPUTING										Session-based recommendation; Attention mechanisms; Sequential behavior		In session-based recommendation scenarios where user profiles are not available, predicting their behaviors is a challenging problem. Previous dominant methods to solve this problem are RNN-based models. Recently, attention mechanisms that allow higher parallelization have shown significant improvement on this issue. However, none of the existing attention-based methods explicitly takes advantage of both the position information and context information in a sequence. We assume that one item usually exhibits different levels of importance when it appears in different positions in a sequence. Therefore, a position-aware context attention (PACA) model is proposed as a remedy, which improves the recommendation performance by taking into account both the position information and the context information of items. PACA introduces positional vectors to model the position information and utilizes a pooling function to generate the context feature vectors. Then the two vectors are combined to generate the attention weight for each item in a session. To further improve the performance, we use a multi-head method to combine several parallel attention modules. Extensive experiments on two real-world datasets show that the proposed attention model is able to achieve very promising performance in comparison with the state-of-the-art methods. Finally, we visualize the positional vectors to explicitly analyze the importance of each position in a sequence. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						65	72		10.1016/j.neucom.2019.09.016													
J								Global mu-stability of neutral-type impulsive complex-valued BAM neural networks with leakage delay and unbounded time-varying delays	NEUROCOMPUTING										Complex-valued BAM neural networks; Neutral-type neural networks; Global stability; Impulses; Linear matrix inequalities	EXPONENTIAL STABILITY	This paper investigates the existence and uniqueness of the equilibrium point and its global mu-stability for neutral-type impulsive complex-valued bidirectional associative memory neural networks with leakage delay and unbounded time-varying delays, by considering two types of Lipschitz conditions to be satisfied by the activation functions. The homeomorphism lemma is used to obtain a sufficient condition for the existence and uniqueness of the equilibrium point. Sufficient delay-dependent conditions both in terms of complex-valued and real-valued linear matrix inequalities which ensure the global mu-stability of the equilibrium point are obtained by constructing appropriate Lyapunov-Krasovskii functionals with simple, double, and triple integral terms, and using the free weighting matrix method, simple and double complex-valued Jensen inequalities, the complex-valued reciprocally convex combination inequality, and the complex-valued Wirtiger-based integral inequality. Lastly, two numerical examples are provided to illustrate the effectiveness of the obtained theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						73	94		10.1016/j.neucom.2019.09.008													
J								Remaining useful life prediction of lithium-ion batteries with adaptive unscented kalman filter and optimized support vector regression	NEUROCOMPUTING										Remaining useful life prediction; Adaptive unscented kalman filter; Genetic algorithm; Support vector regression	HEALTH; STATE; MODEL; PROGNOSTICS	To solve the problem of the inaccurate prediction on remaining useful life (RUL) for lithium-ion battery, we proposed an integrated algorithm which combines adaptive unscented kalman filter (AUKF) and genetic algorithm optimized support vector regression (GA-SVR). Firstly, the state space model with double exponential is established to describe the degradation of lithium battery. Then, the AUKF algorithm is introduced to update adaptively both the process noise covariance and the observation noise covariance. Next, the genetic algorithm is utilized to optimize the key parameters of SVR which realizes multi-step prediction. The effectiveness of the proposed method is verified by simulation experiments with NASA of battery dataset. Simulation results show that the proposed AUKF-GA-SVR achieves better prediction accuracy than existed methods such as unscented kalman filter, extended kalman filter, adaptive extended kalman filter (AEKF), adaptive unscented kalman filter, unscented kalman filter and relevance vector regression and AEKF-GA-SVR. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						95	102		10.1016/j.neucom.2019.09.074													
J								Extreme semi-supervised learning for multiclass classification	NEUROCOMPUTING										Multiclass classification; Semi-supervised support vector machine; Extreme learning machine; Approximate empirical kernel map; Alternating optimization	NYSTROM METHOD; KERNEL; MACHINE; MATRIX	Semi-Supervised Support Vector Machines (S3VMs) provide a powerful framework for Semi-Supervised Learning (SSL) tasks which leverage widely available unlabeled data to improve performance. However, there exist three issues in S3VMs: (i) S3VMs require concurrently training c one-against-all (OAA) classifiers (c is the number of classes) for multiclass classification, which is prohibitive for large c; (ii) S3VMs require huge computational time and large storage (because of the large kernel matrix) in large-scale training and testing; (iii) S3VMs require the balance constraint in the unlabeled data, which not only needs prior knowledge from the unlabeled data (the prior knowledge is unavailable in some applications), but also makes their nonconvex optimization problem more intractable. To address these issues, a novel method called Extreme Semi-Supervised Learning (ESSL) is proposed in this paper. First, the framework of Extreme Learning Machine (ELM) is adopted to handle both binary and multiclass classification problems in a unified model. Second, the hidden layer is encoded by an extremely small approximate empirical kernel map (AEKM) to greatly reduce the computational cost and the memory usage for training and testing. Third, the balance constraint (prior knowledge) in the unlabeled data is removed through the elaborative design of weighting function (which emphasizes the importance of labeled data and the minority pattern in the labeled data).By these three ways, ESSL can be solved effectively and efficiently based on alternating optimization (AO). More specifically, ESSL can be analytically and simply solved by generalized pseudoinverse and oneHotMap function (without any optimization solver and the OAA strategy) in the AO procedure, and consequently, better performance and much faster training speed are always achieved in ESSL. Our empirical study shows that ESSL significantly outperforms existing efficient SSL methods (e.g., meanS3VM and SS-ELM) in terms of accuracy, efficiency and memory, especially for large-scale multiclass problems. As an example, on the 20Newsgroups dataset, ESSL respectively runs 45 and 120 times faster than meanS3VM for training and testing with the improvement in accuracy of 3%, while the memory usage is reduced to 1/14. It is noteworthy that even though all the model parameters are with default values, ESSL already produces very excellent performance without fine-tuning parameters. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						103	118		10.1016/j.neucom.2019.09.039													
J								RBPNET: An asymptotic Residual Back-Projection Network for super-resolution of very low-resolution face image	NEUROCOMPUTING										Super-resolution; Very low-resolution face image; Residual learning; Back projection; Self-supervision		The super-resolution of a very low-resolution face image is a challenge task in single image super-resolution. Most of deep learning methods learn a non-linear mapping of input-to-target space by one-step upsampling. These methods are difficult to reconstruct a high-resolution face image from single very low-resolution face image. In this paper, we propose an asymptotic Residual Back-Projection Network (RBPNet) to gradually learn residual between the reconstructed face image and the ground truth by multi-step residual learning. Firstly, the reconstructed high-resolution feature map is projected to the original low-resolution feature space to generate low-resolution feature map (the projected low-resolution feature map). Secondly, the projected low-resolution feature map is subtracted by original feature map to generate low-resolution residual feature map. And finally, the low-resolution residual feature map is mapped to high-resolution feature space. The network will get a more accurate high-resolution image by iterative residual learning. Meanwhile, we explicitly reconstruct the edge map of face image and embed it into the reconstruction of high-resolution face image to reduce distortion of super-resolution results. Extensive experiments demonstrate the effectiveness and advantages of our proposed RBPNet qualitatively and quantitatively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						119	127		10.1016/j.neucom.2019.09.079													
J								Prior information constrained alternating direction method of multipliers for longitudinal compressive sensing MR imaging	NEUROCOMPUTING										Compressed sensing MRI; Similarity prior information; Longitudinal sparsity; Sparse MRI; MR Image reconstruction method	THRESHOLDING ALGORITHM; INVERSE PROBLEMS; SIGNAL RECOVERY; SPARSE MRI; RECONSTRUCTION; DECOMPOSITION	Sparsity is widely utilized for magnetic resonance imaging (MRI) to reduce k-space sampling. In many clinical MRI scenarios, existing similarity within a series of MRI images and between different contrasts in the same scan can be used to substantially shorten the acquisition time. In this study, the prior information on the pre-acquired reference image is employed in the framework of alternating direction method of multipliers (ADMM) for accurate longitudinal compressed sensing (CS) MRI (LCS-MRI) reconstruction. We propose an efficient algorithm based on the ADMM framework, by using similarity prior information for LCS-MRI. The algorithm minimizes the linear combination of three terms including a least squares data fitting and two l(1) norm regularization terms. The first l(1) norm regularization is utilized for measuring the sparsity of the recovered signal, and the other l(1) norm regularization is employed for measuring the sparsity of the difference between the recovered MR image and the prior known MR scan. The proposed method formulates the reconstruction problem to several unconstrained minimization sub-problems, which can be solved by shrinking operators and alternating minimization algorithms. We compare the proposed algorithm with previous methods in terms of reconstruction accuracy and computation complexity. Numerous experiments demonstrate that the proposed method is more effective and robust and obtained superior performance in reconstructing longitudinal compressed MR image than the other methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						128	140		10.1016/j.neucom.2019.09.057													
J								A 1-norm regularized linear programming nonparallel hyperplane support vector machine for binary classification problems	NEUROCOMPUTING										1-Norm regularization; Nonparallel hyperplane support vector machine; Feature selection; Newton-Armijo algorithm; Pattern classification	ROBUST; MODEL	This research proposes a 1-norm regularized linear programming nonparallel hyperplane support vector machine (LNSVM) model to solve binary classification problems and enhance the robustness performance. Numerous nonparallel support vector machine (SVM) models have been studied with outstanding performance on classification tasks. However, most nonparallel SVM models require two independent models to determine hyperplanes. In addition, due to the involvement of the 2-norm terms, traditional SVM models may suffer from the lack of robustness to outliers and irrelevant features. Therefore, the LNSVM model is proposed by reformulating a typical nonparallel SVM model through the 1-norm regularization. By applying the exterior penalty theory, the proposed LNSVM model is converted to the dual exterior penalty problem, which is solved by the Newton-Armijo algorithm. The essential differences that distinguish the LNSVM model from other nonparallel SVM models are: (1) Different from typical nonparallel SVM models, which solve two quadratic programming (QP) problems, the proposed LNSVM model determines two nonparallel hyperplanes simultaneously by solving a single linear programming (LP) model; (2) The robustness performance of the proposed LNSVM model has been enhanced to tolerate noisy data through the involvement of 1-norm loss function, which can also eliminate redundant features by generating sparse solution during the training procedure. The performance of the proposed LNSVM model is tested through a comparison with state-of-art SVM-based classifiers using a synthetic dataset and 11 practical benchmark datasets. The experimental results show the superiority of the proposed LNSVM model, by achieving better classification performance regarding accuracy, sensitivity, specificity, and removing redundant features synchronously. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						141	152		10.1016/j.neucom.2019.09.068													
J								Triple-adjacent-frame generative network for blind video motion deblurring	NEUROCOMPUTING										Blind motion deblurring; Triple adjacent frames; Temporal feature transfer; Group encoder; Hybrid decoder	DECONVOLUTION	Photos and videos captured by handheld imaging devices are often accompanied by unwanted blur because of hand jitters and fast movement of objects during the exposure time. Most previous studies discussed single image deblurring and video deblurring but neglected detailed analyses of the spatiotemporal continuity between adjacent frames, which limits the deblurring effect. We propose a novel end-to-end blind video motion deblurring network that takes triple adjacent frames as input to deblur a blurry video frame. In our approach, a bidirectional temporal feature transfer between triple adjacent frames is implemented to pass the latent features of the central frame on to a group encoder of its neighbors. Then, a hybrid decoder decodes group features and estimates a sharper video frame relative to the central frame. Experimental results show that our model outperforms previous excellent methods in terms of traditional metrics (PSNR and SSIM) and visual quality within an acceptable time cost. The code is available at https://github.com/BITLIULONGEE/Triple-Adjacent-Frame-Generative-Network. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 1	2020	376						153	165		10.1016/j.neucom.2019.09.031													
J								Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images	NEUROCOMPUTING										Histopathology images; Overlapping nuclei segmentation; Convolutional neural networks; Marker extraction; Watershed transformation	CELL-NUCLEI; CANCER; TRACKING	Nuclei segmentation in histopathology images plays a crucial role in the morphological quantitative analysis of tissue structure and has become a hot research topic. Though numerous efforts have been tried in this research area, the overlapping and touching nuclei segmentation remains a challenging problem. In this paper, we present a novel and effective instance segmentation method for tackling this challenge by integrating Deep Convolutional Neural Networks with Marker-controlled Watershed. Firstly, we design a novel network architecture with multiple segmentation tasks, called Deep Interval-Marker-Aware Network, for learning the foreground, marker, and interval of nuclei, simultaneously. Then the learned interval between overlapping nuclei is used to refine the foreground result of nuclei by using the logical operators. Finally, the learned marker result and the nuclei segmentation result refined by interval are transmitted into the Marker-controlled Watershed for splitting the touching nuclei. The experiments on the standard public datasets demonstrate that our method achieves a substantial improvement compared with state-of-the-art methods. Source codes are available at . https://github.com/appiek/Nuclei_Segmentation_Experiments_Demo. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						166	179		10.1016/j.neucom.2019.09.083													
J								Robust deep auto-encoding Gaussian process regression for unsupervised anomaly detection	NEUROCOMPUTING										Anomaly detection; Deep auto-encoder; Gaussian process; High-dimension data; Data contamination	ONE-CLASS SVM; SUPPORT	Unsupervised anomaly detection (AD) is of great importance in both fundamental machine learning researches and industrial applications. Previous approaches have achieved great advance in improving the performance of unsupervised AD model recently. However, there are still some thorny issues unsolved, especially the problem of efficiency degradation when dealing with high-dimensional data and the inability to maintain robustness when dealing with contaminated data, which have not been addressed simultaneously in the existing models. In our work, we propose a novel hybrid unsupervised AD method, which first integrates convolutional auto-encoder and Gaussian process regression to extract features and to remove anomalies from noisy data as well. Our model behaves more effectively at modeling high-dimension data and more robust to variation of the anomaly rate in dataset. We evaluate its performance on four publicly benchmark datasets and show the state-of-the-art performance against competitive methods. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 1	2020	376						180	190		10.1016/j.neucom.2019.09.078													
J								Further results on mean-square exponential input-to-state stability of time-varying delayed BAM neural networks with Markovian switching	NEUROCOMPUTING										BAM neural networks; Means-square exponential Input-to-state stability; Markovian jump; Lyapounov-Krasovskill functional; Weak infinitesimal operator	DIFFERENTIAL-EQUATIONS; JUMP SYSTEMS; SYNCHRONIZATION; STABILIZATION	This paper mainly discusses the input-to-state stability for BAM neural networks with time-varying delays and Markov jump parameters. Considering the system with Markov jump parameters, we select the improved criterion, namely mean-square exponential input-to-state stability. With the help of stochastic theory, we establish the Markovian switched Lyapunov-Krasovskill functional and obtain its derivation by making use of the weak infinitesimal operator, which is used to obtain the algebraic and linear matrix inequality(LMI) conditions. These conditions can ensure that the system is mean-square exponentially input-to-state stable. In particular, we design a controller to simplify the algebraic conditions. Finally, we provide two numerical examples to show the effectiveness and superiority of the obtained results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						191	201		10.1016/j.neucom.2019.09.033													
J								EPAN: Effective parts attention network for scene text recognition	NEUROCOMPUTING										Scene text recognition; Optical character recognition; Deep neural network		For most previous attention-based scene text recognition methods, images are transformed into highlevel feature vectors that form a feature map with height equal to one. Such vectors may contain unnecessary noise that limits recognition performance. To address this issue, in this paper, we propose the effective parts attention network (EPAN) which can attentively highlight the character region for more precise recognition. EPAN consists of a text image encoder and character effective parts decoder (CEPD), and it is end-to-end trainable. The former separates the high-dimensional feature map into one-dimensional vectors row-by-row, which are connected to a bidirectional long short term memory unit to encode contextual information. Subsequently, the CEPD transforms the vectors using a novel glimpse network at each time step to roughly determine the position of the characters. Then the CEPD uses a refinement network to generate a mask to gradually localize the precise position of important parts of the current character. Experiments were conducted on various benchmarks, including IIIT5K-Words, Street View Text, ICDAR 2003, ICDAR 2013, CUTE80, Street View Text Perspective, and ICDAR 2015, which demonstrated that the proposed EPAN method significantly outperformed or was comparable to existing methods in terms of lexicon-free word accuracy. Additionally, substantial qualitative results further demonstrated the robustness of our method. (C) 2019 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 1	2020	376						202	213		10.1016/j.neucom.2019.10.010													
J								Text classification using capsules	NEUROCOMPUTING										Deep learning; Text classification; Capsule network; Machine learning; Text mining		This paper presents an empirical exploration of the use of capsule networks for text classification. While it has been shown that capsule networks are effective for image classification, the research regarding their validity in the domain of text has been initiated recently. In this paper, we show that capsule networks indeed have the potential for text classification and that they have several advantages over convolutional neural networks. As well, we compare our proposed model to the initial studies regarding capsule network-based text classification. We further suggest a simple routing method that effectively reduces the computational complexity of dynamic routing. We utilized seven benchmark datasets to demonstrate that capsule networks, along with the proposed routing method provide comparable results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						214	221		10.1016/j.neucom.2019.10.033													
J								Multi-block statistics local kernel principal component analysis algorithm and its application in nonlinear process fault detection	NEUROCOMPUTING										Multi-block; Local KPCA; Statistics pattern analysis; Bayesian strategy analysis; Fault detection	PRESERVING PROJECTION; PATTERN-ANALYSIS; DIAGNOSIS; REDUCTION; MODEL; KPCA	It is vital for fault detection technology to extract features of industrial process data effectively. Local kernel principal component analysis (LKPCA) has proved its good performance in preserving global and local structural characteristics. However, it ignored useful high-order statistics of data, so multi-block statistics local kernel principal component analysis (MSLKPCA) algorithm integrating statistics pattern analysis (SPA) into LKPCA is proposed. The correlation coefficient matrix is first calculated and K-means clustering is adopted to divide the original variables into several blocks. Then the weighted SPA, which gives different weights to different samples in each window according to their distributions, is adopted to build statistic spaces containing both low-order and high-order statistics. After that, LKCPA is performed in each statistic space to realize feature extraction. To reduce the noise effect amplified by SPA, PCA is adopted in the residual space to remove noise. Bayesian strategy is used to fuse the results of each block and two monitoring statistics E-T and E-R are proposed to monitor the feature space and the residual space respectively. The Tennessee-Eastman (TE) process simulation shows the effectiveness and superiority of the proposed algorithm for process monitoring. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						222	231		10.1016/j.neucom.2019.09.075													
J								Coarse-to-fine salient object detection with low-rank matrix recovery	NEUROCOMPUTING										Salient object detection; Coarse-to-fine; Low-rank matrix recovery; Learning-based refinement	MODEL	Low-rank matrix recovery (LRMR) has recently been applied to saliency detection by decomposing image features into a low-rank component associated with background and a sparse component associated with visual salient regions. Despite its great potential, existing LRMR-based saliency detection methods seldom consider the inter-relationship among elements within these two components, thus are prone to generating scattered or incomplete saliency maps. In this paper, we introduce a novel and efficient LRMR-based saliency detection model under a coarse-to-fine framework to circumvent this limitation. First, we roughly measure the saliency of image regions with a baseline LRMR model that integrates a l(1)-norm sparsity constraint and a Laplacian regularization smooth term. Given samples from the coarse saliency map, we then learn a projection that maps image features to refined saliency values, to significantly sharpen the object boundaries and to preserve the object entirety. We evaluate our framework against existing LRMR-based methods on three benchmark datasets. Experimental results validate the superiority of our method as well as the effectiveness of our suggested coarse-to-fine framework, especially for images containing multiple objects. MATLAB code for implementing our method is available at https.github.com/qizhust/HLRSaliency. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						232	243		10.1016/j.neucom.2019.08.091													
J								Visual abstraction and exploration of large-scale geographical social media data	NEUROCOMPUTING										Visual abstraction; Social media; Rapid sampling; Feature distribution	COMMUNITY; VISUALIZATIONS; DIFFUSION; ANALYTICS; SEARCH; TOPICS	A great deal of text and geographical information is provided in the geo-tagged social media data, which offers unprecedented opportunities to get insights into the social behaviors across different local areas. With the increasing size of geo-tagged social media data, a large number of visual mapping elements overlap with each other, which makes it difficult to visually capture topics of interest as well as their spatial distribution. In this paper, we propose a visual abstraction framework for the exploration of large scale geo-tagged social media data. Probabilistic topic modeling is firstly utilized to summarize the semantics of texts and extract a set of topic features of interest. Then, a multi-objective sampling model is designed to generate a subset of original dataset, which will not only reduce the visual clutter of large-scale social media data visualization, but also retain the ordering of topic features of interest as well as the geographical distribution of original social media datasets. A rich set of visual designs such as word cloud, text stream and heat map are integrated into the visual abstraction framework, enabling users to evaluate the sampled results from different perspectives including semantic topics, temporal changes and spatial distribution. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in simplifying the geographical visualization of large scale geo-tagged social media data and exploring the social behaviors across different local areas. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 1	2020	376						244	255		10.1016/j.neucom.2019.10.072													
J								Fine-tuning of line and slope based on evolutionary mechanism	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Learning; Evolutionary; Parameter tuning; Point cloud; Denoising	BRAIN STORM OPTIMIZATION; GENETIC ALGORITHM; FEATURE-SELECTION; FUZZY ARTMAP; HYBRID MODEL; CONSTRUCTION; TECHNOLOGY	The line and slope fine-tuning is the process of optimizing the horizontal and longitudinal sections to meet the requirements of building clearance, which is an indispensable step in the building engineering. The traditional line and slope fine-tuning, which is manually completed by designers, depends heavily on the domain knowledge of designers. The more experienced the designer is, the better the effect of line and slope fine-tuning will be. This paper makes a first attempt to apply the evolutionary algorithm to the process of line and slope fine-tuning. The main work includes: a new denoising method for tunnel point cloud data is proposed to remove noisy and redundant data from point cloud; an objective function is given to measure the deviation between the design tunnel and the real tunnel; and a learning model of the line and slope fine-tuning is built based on the point cloud data and evolutionary algorithm. A dataset from a length of the real metro tunnel is used for model testing. The testing results show that, in comparison with the traditional manually-adjusting method, our approach based on the evolutionary algorithm can significantly reduce the deviation between the adjusted tunnel and the real tunnel.																	1868-8071	1868-808X				JUL	2020	11	7					1631	1641		10.1007/s13042-020-01071-0		JAN 2020											
J								Enhanced humanoid assisted human interaction model based on linear structural modeling for knowledge representation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Robotic assistance; Humanoid; Linear linguistic advanced structural modeling (LLASM); Weighted linguistic reasoning (WLR); Human-computer interaction; Linguistic reasoning model	FUZZY PETRI NETS; SYSTEM	Certain rules and regulations are adapted to the linguistic production model to support the decision of human computer interaction using linguistic protection rule method. This paper proposes linear linguistic advanced structural modeling (LLASM) approach to develop weighted linguistic reasoning (WLR) algorithm for the purpose of reasoning and knowledge representation. In this article human computer interaction faces optimal flow and job Characteristics experience using individual computers in the workplace which helps to overcome the issues using LLASM in a variety of organizations with Robotic assistance for verbal and non-verbal sequence using humanoid robot. This model introduces global weight and local knowledge fuzzy rules to determine the optimal flow and job characterization problem faced in linguistic reasoning method. Here, The Weighted linguistic reasoning algorithm allows fuzzy rule-based expert systems with the help of the LLASM method to execute the intelligent and flexible knowledge reasoning model with robotic assistance. Finally, case studies are presented to show the effectiveness of the re-scheduling process which benefits the proposed method using the Linguistic reasoning model.																	1868-5137	1868-5145															10.1007/s12652-020-01735-3		JAN 2020											
J								EE-FMDRP: energy efficient-fast message distribution routing protocol for vehicular ad-hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										VANET; Energy efficient-fast message distribution routing protocol (EE-FMDRP); Distribution delay; Throughput	DATA DISSEMINATION PROTOCOL; MITIGATION; CAST	Vehicular ad-hoc network (VANET) is a promising communication framework that provides connectivity between vehicles via vehicle-to-RSU (road side unit), inter-RSU and inter-vehicle communications and is features by high dynamic topological formation of nodes in the network. In VANET, the message distribution delay and reliable communication are the significant factors in case of emergency based communications. The main contribution of this paper relies on reducing message distribution delay with minimal communication overhead. Moreover, energy efficient-fast message distribution routing protocol (EE-FMDRP) has been proposed with the collaborative features of both time and direction oriented routing models. It has been proposed for broadcasting messages in case of emergencies from the source end to the defined target in fast, reliable and efficient manner. For that, bi-directional evaluation model for moving vehicles and message delivery time derivation model has been framed. This helps in performing fast message broadcasting in emergency cases with maximal throughput and reduced delay. Also, the EE-FMDRP offers consistent and efficient route between source vehicle and the destination with optimal intermediates and reduced complexities. Further, the protocol takes less message distribution time by selecting optimal intermediates, thereby providing energy efficiency and minimized network overhead. For evaluation, NS-2 simulation tool is used and the results prove that the proposed model achieves improved results than the compared existing protocols.																	1868-5137	1868-5145															10.1007/s12652-020-01730-8		JAN 2020											
J								Graph Theory in Coq: Minors, Treewidth, and Isomorphisms	JOURNAL OF AUTOMATED REASONING										Graph theory; Minor; Treewidth; Isomorphisms; Coq; Ssreflect		We present a library for graph theory in Coq/Ssreflect. This library covers various notions on simple graphs, directed graphs, and multigraphs. We use it to formalize several results from the literature: Menger's theorem, the excluded-minor characterization of treewidth-two graphs, and a correspondence between multigraphs of treewidth at most two and terms of certain algebras.																	0168-7433	1573-0670				JUN	2020	64	5			SI		795	825		10.1007/s10817-020-09543-2		JAN 2020											
J								Robust multi-kernelized correlators for UAV tracking with adaptive context analysis and dynamic weighted filters	NEURAL COMPUTING & APPLICATIONS										Visual tracking; Unmanned aerial vehicle (UAV); Multi-kernelized correlators; Adaptive context analysis; Dynamic weighted filters	VISUAL TRACKING	In recent years, the correlation filter (CF)-based method has significantly advanced in the tracking for unmanned aerial vehicles (UAVs). As the core component of most trackers, CF is a discriminative classifier to distinguish the object from the surrounding environment. However, the poor representation of the object and lack of contextual information have restricted the tracker to gain better performance. In this work, a robust framework with multi-kernelized correlators is proposed to improve robustness and accuracy simultaneously. Both convolutional features extracted from the neural network and hand-crafted features are employed to enhance expressions for object appearances. Then, the adaptive context analysis strategy helps filters to effectively learn the surrounding information by introducing context patches with the GMSD index. In the training stage, multiple dynamic filters with time-attenuated factors are introduced to avoid tracking failure caused by dramatic appearance changes. The response maps corresponding to different features are finally fused before the novel resolution enhancement operation to increase distinguishing capability. As a result, the optimization problem is reformulated, and a closed-form solution for the proposed framework can be obtained in the kernel space. Extensive experiments on 100 challenging UAV tracking sequences demonstrate that the proposed tracker outperforms other 23 state-of-the-art trackers and can effectively handle unexpected appearance variations under the complex and constantly changing working conditions.																	0941-0643	1433-3058				AUG	2020	32	16					12591	12607		10.1007/s00521-020-04716-x		JAN 2020											
J								Backward-link computational imaging using batch learning networks	NEURAL COMPUTING & APPLICATIONS										Optical imaging; Point spread function; Full-link imaging	MODULATION TRANSFER-FUNCTION; BLIND DECONVOLUTION; ADAPTIVE-OPTICS; HIGH-RESOLUTION; STAR-TRACKER; PARALLEL ALGORITHMS; NEURAL-NETWORK; QUALITY; COMPENSATION; RADIATION	Optical images are inevitably stained by the multiple aberrations from an optical imaging link itself (OILI, e.g., the camera itself involving an optical system, an electronic system, and a sensor). The development of techniques that enable optical full-link imaging without the OILI aberrations is desirable. In this paper, we demonstrate an aberration-free backward-link imaging method for optical full-link imaging. This method utilizes a batch learning network to reconstruct the invariable point spread function (IPSF) of the imaging processing, which is constant and produced by the design, fabrication, and assembly. The inverted IPSF in conjunction with a digital filter can correct the distorted wavefront aberrations caused by the OILI. This is confirmed experimentally. This method opens the door to aberration-free full-link optical imaging for the involved imaging system applications, such as Coarse Holography Displays.																	0941-0643	1433-3058				AUG	2020	32	16					12895	12907		10.1007/s00521-020-04734-9		JAN 2020											
J								Robust regularized extreme learning machine with asymmetric Huber loss function	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; Asymmetric Huber loss; Robustness; Suspended sediment load	SUSPENDED SEDIMENT LOAD; SUPPORT VECTOR REGRESSION; ANN MODEL; SIMULATION; ALGORITHM; SYSTEM; LEVEL; BASIN	Sediment transport is one of the major challenging fields in hydrology. The tropical atmosphere, complex topography and occasional extreme precipitation are the fundamental explanations behind this challenge. Thus, the rivers in this situation contain a huge quantity of sediment, which may affect the river hydraulics. Hence, it is required to collect various parameters such as discharge, velocity, rainfall and sediment concentration to analyze the impact of sediment for river engineering practices and management. Therefore, the dataset which is collected from the river may contain outliers and noises. For improving the prediction accuracy of sediment load, we present robust regularized extreme learning machine frameworks to reduce the effect of noise by using the asymmetric Huber loss function named as AHELM and epsilon-insensitive Huber loss function named as epsilon-AHELM. Further, the problems are rewritten in the form of strongly convex minimization problems whose solutions are acquired by simple function iterative schemes. To ensure the effectiveness of the proposed approach, we have considered the real-world datasets with two types of noises. Furthermore, the proposed schemes are applied on real sediment load datasets (SLDs) which are collected from the Tawang Chu river of Arunachal Pradesh, India. The results reveal that proposed AHELM and epsilon-AHELM with multiquadric activation function are performed better for real-world datasets, whereas AHELM and epsilon-AHELM with sigmoid activation function perform efficiently and effectively for the sediment load prediction. In overall, the experimental results clearly exhibit the applicability as well as the usability of the proposed extreme learning machine with asymmetric Huber loss functions.																	0941-0643	1433-3058				AUG	2020	32	16					12971	12998		10.1007/s00521-020-04741-w		JAN 2020											
J								Neural networks catching up with finite differences in solving partial differential equations in higher dimensions	NEURAL COMPUTING & APPLICATIONS										Neural networks; Partial differential equations; Nonlinear Poisson equation; 5D boundary value problem	NUMERICAL-SOLUTION; APPROXIMATION	Solving partial differential equations using neural networks is mostly a proof of concept approach. In the case of direct function approximation, a single neural network is constructed to be the solution of a particular boundary value problem. Independent variables are fed into the input layer, and a single output is considered as the solution's value. The network is substituted into the equation, and the residual is then minimized with respect to the weights of the network using a gradient-based method. Our previous work showed that by minimizing all derivatives of the residual up to the third order one can obtain a machine precise solution for 2D boundary value problem using very sparse grids. The goal of this paper is to use this grid complexity advantage in order to obtain a solution faster than finite differences. However, the number of all possible high-order derivatives (and therefore the training time) increases with the number of dimensions and it was unclear whether this goal can be achieved. Here, we demonstrate that this increase can be compensated by using random directional derivatives instead. In 2D case neural networks are slower than finite differences, but for each additional dimension the complexity increases approximately 4 times for neural networks and 125 times for finite differences. This allows neural networks to catch up in 3D case for memory complexity and in 5D case for time complexity. For the first time a machine precise solution was obtained with neural network faster than with finite differences method.																	0941-0643	1433-3058				SEP	2020	32	17					13425	13440		10.1007/s00521-020-04743-8		JAN 2020											
J								Efficient Strategies of Static Features Incorporation into the Recurrent Neural Network	NEURAL PROCESSING LETTERS										Recurrent neural network; Deep learning; Time series; Energy consumption; Static features	TERM; DEMAND	Recurrent neural networks (RNNs) have evolved to become one of the most powerful tools for making predictions on sequenced data, such as time series, textual data, signals, music etc. In many real-life cases, however, sequenced data are additionally characterized by static features which, due to their non-sequential nature, cannot be transferred directly into RNNs. In this paper, we discuss a method which incorporates static features into RNNs in order to influence and generalize the learning process. Furthermore, we will demonstrate that our approach significantly enhances the performance of RNNs, enabling the networks to learn the sequenced data exhibiting varying characteristics and then distinguish between them through the use of static supplementary information. Finally, we will evaluate our model against real energy consumption measurements of energy time series and verify that high-accuracy demand forecasts for different types of customers can be achieved only by way of incorporation of static features.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2301	2316		10.1007/s11063-020-10195-x		JAN 2020											
J								A novel framework for stock trading signals forecasting	SOFT COMPUTING										Piecewise linear representation (PLR); Information gain; Feature weighted SVM (FW-WSVM); Stock trading decision	ARTIFICIAL NEURAL-NETWORKS; SUPPORT VECTOR MACHINE; PIECEWISE-LINEAR REPRESENTATION; COMPONENT ANALYSIS; MODEL; PREDICTION; PRICE; INTEGRATION; SYSTEM; INDEX	This study investigates stock trading points prediction that is an attractive yet challenging research topic in the financial investment area, as the stock market is an unstable and complex system. A small improvement in the predictive performance can make profit. To realize trading points detection, we propose a novel method which integrates piecewise linear representation (PLR) and feature weighted support vector machine (FW-WSVM) to forecast the stock trading points (PLR-FW-WSVM). Firstly, we generate numerous trading points (valley or peak) from the trading data by PLR and formulate the stock trading points prediction as a weighted four-class classification problem. Then, we estimate the importance of each input feature by computing the information gain and apply FW-WSVM to learn the prediction model between the trading points and the input features from the historical data. Afterward, the model is used to forecast the future turning points from the input features. Lastly, we conduct a series of experiments among PLR-FW-WSVM, PLR-WSVM and PLR-ANN over 30 stocks with different investment strategies. The results show that our proposed method generates the highest accuracy and profits in average, which indicates PLR-FW-WSVM is effective and can be applied to forecast the future trading points in the real-world application.																	1432-7643	1433-7479				AUG	2020	24	16					12111	12130		10.1007/s00500-019-04650-8		JAN 2020											
J								A decision support model based on the combined structure of DEMATEL, QFD and fuzzy values	SOFT COMPUTING										Decision-making trial and evaluation laboratory; Quality function deployment; Sustainable supply chain; Fuzzy decision making; Customer values	SUPPLY CHAIN MANAGEMENT; MCDM APPROACH; CUSTOMER SATISFACTION; PROGRAMMING APPROACH; MAKING METHOD; SELECTION; GREEN; ANP; TOPSIS; FRAMEWORK	Uncertainty and risk are inevitable issues in decision making in supply chain systems. In knowledge-based communities, uncertain conditions are recognized, analyzed and eliminated using wide range of concepts and approaches. One intelligent tool to aid in decreasing impreciseness and in delivering an acceptable level of accuracy is fuzzy logic. This study reports a new version of the fuzzy multiple criteria decision-making family. The decision-making trial and evaluation laboratory (DEMATEL) is extended for the first time with interval fuzzy values and then integrated as an input for the quality function deployment (QFD) mechanism. Although the application of decision-making tools in the supply chain and its sustainable development is undeniable, the implementation of fuzzy decision support systems in sustainable supply chains still demands more research work. This study develops an IT2F DEMATEL-QFD model to evaluate and rank sustainable supply chain drivers in a group decision-making environment. The proposed fuzzy decision model is connected to a real research project for eliminating risks in the supply chain related to agricultural production systems. Sensitivity analysis confirms the stability of the model. It is concluded that the outcomes and advantages of the newly developed model will profit academic and non-academic partners.																	1432-7643	1433-7479				AUG	2020	24	16					12449	12468		10.1007/s00500-020-04685-2		JAN 2020											
J								Fuzzy volumetric delineation of brain tumor and survival prediction	SOFT COMPUTING										Multi-thresholding; MRI; Glioblastoma multiforme; Fuzzy connectedness; 3D segmentation; Survival prediction	IMAGE SEGMENTATION; GLIOBLASTOMA-MULTIFORME; CLUSTERING-ALGORITHM; MR-IMAGES; DEFINITION	A novel three-dimensional detailed delineation algorithm is introduced for Glioblastoma multiforme tumors in MRI. It efficiently delineates the whole tumor, enhancing core, edema and necrosis volumes using fuzzy connectivity and multi-thresholding, based on a single seed voxel. While the whole tumor volume delineation uses FLAIR and T2 MRI channels, the outlining of the enhancing core, necrosis and edema volumes employs the T1C channel. Discrete curve evolution is initially applied for multi-thresholding, to determine intervals around significant (visually critical) points, and a threshold is determined in each interval using bi-level Otsu's method or Li and Lee's entropy. This is followed by an interactive whole tumor volume delineation using FLAIR and T2 MRI sequences, requiring a single user-defined seed. An efficient and robust whole tumor extraction is executed using fuzzy connectedness and dynamic thresholding. Finally, the segmented whole tumor volume in T1C MRI channel is again subjected to multi-level segmentation, to delineate its sub-parts, encompassing enhancing core, necrosis and edema. This was followed by survival prediction of patients using the concept of habitats. Qualitative and quantitative evaluation, on FLAIR, T2 and T1C MR sequences of 29 GBM patients, establish its superiority over related methods, visually as well as in terms of Dice scores, Sensitivity and Hausdorff distance.																	1432-7643	1433-7479				SEP	2020	24	17					13115	13134		10.1007/s00500-020-04728-8		JAN 2020											
J								A computational approach for printed document forensics using SURF and ORB features	SOFT COMPUTING										Document forensics; Printer forensics; SURF; ORB; Voting scheme; AdaBoost	CLASSIFIER	Document forgery is quite common nowadays due to the availability of cost-effective scanners and printers. Important documents like certificates, passport, identification cards, etc., are protected using watermarks or signatures. These are made secured with a protective printing mechanism with extrinsic fingerprints. Therefore, it is easy to authenticate such documents. Other documents required a passive approach for their authentication. These approaches look for document inconsistencies for chances of modification. Some of these attempt to detect and fix the source of the printed document. This paper proposes a classifier-based model to identify the source printer and classify the questioned document in one of the printer classes. A novel approach of utilizing Speeded Up Robust Features and Oriented Fast Rotated and BRIEF feature descriptors is proposed for printer attribution. Naive Bayes, k-NN, random forest and different combinations of these classifiers have been experimented for classification. The proposed model can efficiently classify the questioned documents to their respective printer class. An accuracy of 86.5% has been achieved using a combination of Naive Bayes, k-NN, random forest classifiers with a simple majority voting scheme and adaptive boosting methodology.																	1432-7643	1433-7479				SEP	2020	24	17					13197	13208		10.1007/s00500-020-04733-x		JAN 2020											
J								Navigation leads for exploratory search and navigation in digital libraries	KNOWLEDGE AND INFORMATION SYSTEMS										Exploratory search; Navigation leads; Query refinement; Navigational value; Digital libraries; Annota	INFORMATION-RETRIEVAL; ASK	Exploratory search (in contrary to the traditional lookup search) is characterized by the search tasks that have exploration, learning, and investigation as their goals. An example of this task in the domain of digital libraries is exploration of a new domain, a task that is typically performed by a researcher novice, such as a master's or a doctoral student. To support the researcher novices in this task, we proposed an approach of exploratory search and navigation using navigation leads, with which we augment the search results, and which serve as navigation starting points allowing users to follow a specific path by filtering only documents pertinent to the selected lead. In this paper, we present a method of selection of navigation leads considering their navigational value in the form of a corpus relevance. We examined this method by the means of an offline evaluation on the dataset from a bookmarking service Annota. We showed that considering the corpus relevance helps to cover significantly more (relevant) documents when conducting the exploratory search. In addition, our relevance metric combining document and corpus relevance of a lead outperformed the popularity metric based on the frequency of the term in the document corpus.																	0219-1377	0219-3116				JUL	2020	62	7					2739	2764		10.1007/s10115-019-01434-2		JAN 2020											
J								Autonomous ballistic airdrop of objects from a small fixed-wing unmanned aerial vehicle	AUTONOMOUS ROBOTS										Real-time machine vision; Autonomous UAV; Target recognition; Path planning; Guidance and control; Target identification	ICEBERG; TRACKING	Autonomous airdrop is a useful basic operation for a fixed-wing unmanned aerial system. Being able to deliver an object to a known target position extends operational range without risking human lives, but is still limited to known delivery locations. If the fixed-wing unmanned aerial vehicle delivering the object could also recognize its target, the system would take one step further in the direction of autonomy. This paper presents a closed-loop autonomous delivery system that uses machine vision to identify a target marked with a distinct colour, calculates the geographical coordinates of the target location and plans a path to a release point, where it delivers the object. Experimental results present a visual target estimator with a mean error distance of 3.4 m and objects delivered with a mean error distance of 5.5 m.																	0929-5593	1573-7527				MAY	2020	44	5					859	875		10.1007/s10514-020-09902-3		JAN 2020											
J								MasterMovelets: discovering heterogeneous movelets for multiple aspect trajectory classification	DATA MINING AND KNOWLEDGE DISCOVERY										Multiple aspect trajectory; Trajectory classification; Relevant subtrajectories; Multidimensional sequence classification; Movelets; Semantic trajectory classification		In the last few years trajectory classification has been applied to many real problems, basically considering the dimensions of space and time or attributes inferred from these dimensions. However, with the explosion of social media data and the advances in the semantic enrichment of mobility data, a new type of trajectory data has emerged, and the trajectory spatio-temporal points have now multiple and heterogeneous semantic dimensions. By semantic dimensions we mean any type of information that is neither spatial nor temporal. As a consequence, new classification methods are needed to deal with this new type of data. The main challenge is how to automatically select and combine the data dimensions and to discover the subtrajectories that better discriminate the class. In this paper we propose MasterMovelets, a new parameter-free method for trajectory classification which finds the best trajectory partition and dimension combination for robust high dimensional trajectory classification. Experimental results show that our approach outperforms state-of-the-art methods by reducing the classification error up to 63%, indicating that our proposal is very promising for multidimensional sequence data classification.																	1384-5810	1573-756X				MAY	2020	34	3					652	680		10.1007/s10618-020-00676-x		JAN 2020											
J								Neuroevolutionary learning in nonstationary environments	APPLIED INTELLIGENCE										Concept drift; Adaptive learning; Nonstationary environments; Neuroevolutionary ensemble; Quantum-inspired evolution	RULE-BASED CLASSIFIERS; CONCEPT DRIFT; ENSEMBLE; ONLINE	This work presents a new neuro-evolutionary model, called NEVE (Neuroevolutionary Ensemble), based on an ensemble of Multi-Layer Perceptron (MLP) neural networks for learning in nonstationary environments. NEVE makes use of quantum-inspired evolutionary models to automatically configure the ensemble members and combine their output. The quantum-inspired evolutionary models identify the most appropriate topology for each MLP network, select the most relevant input variables, determine the neural network weights and calculate the voting weight of each ensemble member. Four different approaches of NEVE are developed, varying the mechanism for detecting and treating concepts drifts, including proactive drift detection approaches. The proposed models were evaluated in real and artificial datasets, comparing the results obtained with other consolidated models in the literature. The results show that the accuracy of NEVE is higher in most cases and the best configurations are obtained using some mechanism for drift detection. These results reinforce that the neuroevolutionary ensemble approach is a robust choice for situations in which the datasets are subject to sudden changes in behaviour.																	0924-669X	1573-7497				MAY	2020	50	5					1590	1608		10.1007/s10489-019-01591-5		JAN 2020											
J								EA-based resynthesis: an efficient tool for optimization of digital circuits	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Cartesian genetic programming; Evolutionary resynthesis; Logic optimization	HARDWARE	Since the early nineties the lack of scalability of fitness evaluation has been the main bottleneck preventing the adoption of evolutionary algorithms for logic circuits synthesis. Recently, various formal approaches such as SAT and BDD solvers have been introduced to this field to overcome this issue. This made it possible to optimise complex circuits consisting of hundreds of inputs and thousands of gates. Unfortunately, we are facing another problem-scalability of representation. The efficiency of the evolutionary optimization applied at the global level deteriorates with the increasing complexity. To overcome this issue, we propose to apply the concept of local resynthesis in this work. Local resynthesis is an iterative process based on the extraction of smaller sub-circuits from a complex circuit that are optimized locally and implanted back to the original circuit. When applied appropriately, this approach can mitigate the problem of scalability of representation. Two complementary approaches to the extraction of the sub-circuits are presented and evaluated in this work. The evaluation is done on a set of highly optimized complex benchmark problems representing various real-world controllers, logic and arithmetic circuits. The experimental results show that the evolutionary resynthesis provides better results compared to globally operating evolutionary optimization. In more than 85% cases, a substantially higher number of redundant gates was removed while keeping the computational effort at the same level. A huge improvement was achieved especially for the arithmetic circuits. On average, the proposed method was able to remove 25.1% more gates.																	1389-2576	1573-7632				SEP	2020	21	3			SI		287	319		10.1007/s10710-020-09376-3		JAN 2020											
J								On the importance of specialists for lexicase selection	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Lexicase selection; Specialists; Parent selection; Program synthesis; Genetic programming		Lexicase parent selection filters the population by considering one random training case at a time, eliminating any individual with an error for the current case that is worse than the best error of any individual in the selection pool, until a single individual remains. This process often stops before considering all training cases, meaning that it will ignore the error values on any cases that were not yet considered. Lexicase selection can therefore select specialist individuals that have high errors on some training cases, if they have low errors on others and those errors come near the start of the random list of cases used for the parent selection event in question. We hypothesize here that selecting such specialists, which may have high total error, plays an important role in lexicase selection's observed performance advantages over error-aggregating parent selection methods such as tournament selection, which select specialists less frequently. We conduct experiments examining this hypothesis, and find that lexicase selection's performance and diversity maintenance degrade when we deprive it of the ability to select specialists. We also conduct experiments with a form of tournament selection that has been modified to allow for the selection of specialists, and find that it performs better than ordinary tournament selection, but not as well as lexicase selection. These findings, and other data that we present here, help explain the improved performance of lexicase selection compared to tournament selection, and suggest that specialists help drive evolution with lexicase selection toward global solutions.																	1389-2576	1573-7632				SEP	2020	21	3			SI		349	373		10.1007/s10710-020-09377-2		JAN 2020											
J								Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION										Weakly-supervised learning; Semantic segmentation; Affinity		Weakly-supervised semantic segmentation is a challenging task as no pixel-wise label information is provided for training. Recent methods have exploited classification networks to localize objects by selecting regions with strong response. While such response map provides sparse information, however, there exist strong pairwise relations between pixels in natural images, which can be utilized to propagate the sparse map to a much denser one. In this paper, we propose an iterative algorithm to learn such pairwise relations, which consists of two branches, a unary segmentation network which learns the label probabilities for each pixel, and a pairwise affinity network which learns affinity matrix and refines the probability map generated from the unary network. The refined results by the pairwise network are then used as supervision to train the unary network, and the procedures are conducted iteratively to obtain better segmentation progressively. To learn reliable pixel affinity without accurate annotation, we also propose to mine confident regions. We show that iteratively training this framework is equivalent to optimizing an energy function with convergence to a local minimum. Experimental results on the PASCAL VOC 2012 and COCO datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.																	0920-5691	1573-1405				JUN	2020	128	6					1736	1749		10.1007/s11263-020-01293-3		JAN 2020											
J								An efficient medical data classification using oppositional fruit fly optimization and modified kernel ridge regression algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Medical dataset classification; Relevant; Irrelevant; Feature selection; Train and testing; Modified kernel ridge regression(MKRR)	FUZZY RECOMMENDER SYSTEMS; SET BASED CLASSIFICATION; DIAGNOSIS	Medical researches utilize data mining techniques for several years and have been well known to be successful one. In the medical data have certain characteristic that make their analysis very challenging and attractive. In the proposed medical data classification research, it contains relevant and irrelevant features. Here the irrelevant features to be reduced with the aid of oppositional fruit fly optimization algorithm. In the feature selection phase the optimal subset of features are finally divided into training and testing files. The output of training and testing files is given into classifier. This classification is to be performed with the aid of Modified Kernel Ridge Regression (MKRR). KRR gets knowledge about a linear function in the space induced by the respective kernel and the data. For MKRR non-linear kernels, this corresponds to a non-linear function in the original space. The form of the model acquire knowledge by Kernel Ridge is alike to support vector regression.																	1868-5137	1868-5145															10.1007/s12652-020-01733-5		JAN 2020											
J								Shrimp recognition using ShrimpNet based on convolutional neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Shrimp recognition; Convolutional neural network; Deep learning	TRACKING	Shrimp is a world's important trade goods with high economic value and also one of the most important sources of animal protein. Considering the costs of calculation and hardware, this paper presents a convolutional neural network (CNN) architecture (named as ShrimpNet) to obtain shrimp recognition. The proposed ShrimpNet is an important part of the intelligent shrimp aquaculture which is great helpful for the shrimp aquaculture. The proposed ShrimpNet includes two CNN layers and two fully-connected layers. The collected data set includes six different categories of shrimp that are used to train and test the performance of proposed ShrimpNet. Experimental results show that the proposed ShrimpNet has 95.48% accuracy in shrimp recognition. Therefore, the proposed ShrimpNet is a useful tool with good performance for shrimp recognition.																	1868-5137	1868-5145															10.1007/s12652-020-01727-3		JAN 2020											
J								Activation functions selection for BP neural network model of ground surface roughness	JOURNAL OF INTELLIGENT MANUFACTURING										Roughness; Ground surfaces; Grinding process; BP neural network; Activation function	GRINDING PARAMETERS; MATRIX COMPOSITES; CHIP THICKNESS; OPTIMIZATION; PREDICTION; TOPOGRAPHY; WEAR	Roughness prediction of ground surfaces is critical in understanding and optimizing the grinding process. However, it is hitherto difficult to predict accurately the ground surface roughness by theoretical and empirical models due to the complexity of grinding process. BP neural network (BPNN), which can be used to establish the relationship between processing parameters and surface roughness, avoids the difficulty of revealing the complex physical mechanism and thus has unique potential in automatic optimization of grinding process in industrial practice. Activation function is one of the most important factors affecting the efficiency and accuracy of BPNN. Nevertheless, it is often selected arbitrarily or at most by trials or tuning. This paper proposes an activation function selection approach in which virtual data generated from the approximate physical model are employed to evaluate the performance of the BPNN in practice application. The results show that with tansig as the activation function of hidden layer and purelin as the activation function of output layer, the BPNN model can obtain the highest learning efficiency. Moreover, when the activation function of hidden layer is sigmoid, whose shape factor is 1-3, and the output layer activation function is purelin, the model can predict more precisely. Finally, the proposed approach is validated by comparing the performance of BPNN obtained from the virtual data and the experimental data. Obtained results showed that the proposed approach is a simple and effective way to determine the activation function of BPNN.																	0956-5515	1572-8145															10.1007/s10845-020-01538-5		JAN 2020											
J								Improving the epsilon-approximate algorithm for Probabilistic Classifier Chains	KNOWLEDGE AND INFORMATION SYSTEMS										Multi-label; Classifier Chains; Inference; epsilon-approximate algorithm	RANDOM K-LABELSETS; INFERENCE	Probabilistic Classifier Chains are a multi-label classification method which has gained the attention of researchers in recent years. This is because of their ability to optimally estimate the entire joint conditional probability of a label combination through the product rule of probability. Their main drawback is that they require performing an exhaustive search in order to obtain Bayes optimal predictions. This means computing this probability for all possible label combinations before taking a label combination with the highest value of probability. This is the reasonwhyseveralworks have been published in recent years that avoid exploring all combinations, while maintaining optimality. Approaches such as greedy search, beam search and Monte Carlo reduce the computational cost, but at the cost of not ensuring Bayes optimal predictions (although, in general, they provide close to optimal solutions). Methods based on a heuristic search provide optimal predictions, but the computational time has not been as good as expected. In this respect, the -approximate algorithm has been found to be the best inference approach among those that provide Bayes optimal predictions, not only for its optimality, but also for its computational time. However, this paper both theoretically and experimentally shows that it sometimes performs some backtracking during the search for optimal predictions which may prolong the prediction time. The aim of this paper is thus to improve this algorithm by achieving a more direct search. Specifically, it enhances the criterion underwhich the next node to be expanded is chosen by adding heuristic information, although it is only applicable for linear-based models. The experiments carried out confirm that the improved -approximate algorithm explores fewer nodes and reduces the computational time of the original version.																	0219-1377	0219-3116				JUL	2020	62	7					2709	2738		10.1007/s10115-020-01436-5		JAN 2020											
J								Decomposition-based 2-echelon multi-objective evolutionary algorithm with energy-efficient local search strategies for shop floor multi-crane scheduling problems	NEURAL COMPUTING & APPLICATIONS										Multi-objective evolutionary algorithm; Energy-efficient local search strategy; Shop floor multi-crane scheduling; Deep optimization	SINGLE HOIST; OPTIMIZATION; MOEA/D; DESIGN; LINES	Since the environment-friendly production has attracted extensive attention of many manufacturing enterprises, the energy consumption has become one of the core indices to evaluate production processes. Under this circumstance, the main purpose of this paper is to propose an efficient scheduling method for multi-objective shop floor multi-crane scheduling problems. In this research, the objectives of minimizing total weighted tardiness and total energy consumption are considered simultaneously. Owing to the NP-hard nature of the investigated problem, an improved decomposition-based 2-echelon multi-objective evolutionary algorithm with energy-efficient local search strategies (2-echelon iMOEA/D) is developed to solve the problem. The upper echelon optimizer of the proposed algorithm extends the advantages of computing resources allocation and adaptive neighborhood adjustment to accelerate the convergence speed, while the lower echelon optimizer utilizes energy-efficient local search strategies to realize deep optimization of the algorithm. The performance of the proposed method is compared with two other high-efficient multi-objective optimization algorithms. The computation results indicate that the proposed 2-echelon iMOEA/D achieves better performance both on solutions' quality and diversity.																	0941-0643	1433-3058				JUL	2020	32	14					10719	10739		10.1007/s00521-019-04608-9		JAN 2020											
J								On residuation in paraorthomodular lattices	SOFT COMPUTING										Modular lattice; Antitone involution; Regularity; Material implication; Left-residuated l-groupoid; Residuated lattice	LOGIC	Paraorthomodular lattices are quantum structures of prominent importance within the framework of the logico-algebraic approach to (unsharp) quantum theory. However, at the present time it is not clear whether the above algebras may be regarded as the algebraic semantic of a logic in its own right. In this paper, we start the investigation of material implications in paraorthomodular lattices by showing that any bounded modular lattice with antitone involution A can be converted into a left-residuated groupoid if it satisfies a strengthened form of regularity. Moreover, the above condition turns out to be also necessary whenever A is distributive.																	1432-7643	1433-7479				JUL	2020	24	14					10295	10304		10.1007/s00500-020-04699-w		JAN 2020											
J								Complex image recognition algorithm based on immune random forest model	SOFT COMPUTING										Immune algorithm; Random forest; Mathematical model; Complex image; Soft computing	NATIONAL-PARK; CLASSIFICATION	With the continuous advancement of science and technology, the social network based on the Internet has gradually penetrated into people's daily lives. The image data shared on social media using platforms such as mobile phones has exploded, and hundreds of millions of dollars are generated every day with picture information. In the past, the traditional ways of expressing textual information that people are familiar with have begun to be gradually replaced by image information that is not subject to regional culture such as language and script. In modern warfare, with the continuous development and equipment of highly sophisticated weapons and equipment, the amount of information that the entire combat system needs to process will also increase. In particular, the air defense system needs to quickly and accurately identify the aircraft targets that are coming. It mainly uses computer to extract the feature information of the acquired image and converts the content in the image into a feature expression that can be processed by the computer. After the appropriate classification algorithm, the image is the target object is classified by category. In this paper, we propose a complex image recognition algorithm based on immune random forest model. The experimental results show that the proposed algorithm has high recognition efficiency and higher robustness.																	1432-7643	1433-7479				AUG	2020	24	16					12643	12657		10.1007/s00500-020-04706-0		JAN 2020											
J								A binary social spider algorithm for continuous optimization task	SOFT COMPUTING										Binary optimization; Social spider algorithm; Transfer function	PARTICLE SWARM OPTIMIZATION; SELECTION; INTELLIGENCE	The social spider algorithm (SSA) is a new heuristic algorithm created on spider behaviors. The original study of this algorithm was proposed to solve continuous problems. In this paper, the binary version of SSA (binary SSA) is introduced to solve binary problems. Currently, there is insufficient focus on the binary version of SSA in the literature. The main part of the binary version is at the transfer function. The transfer function is responsible for mapping continuous search space to discrete search space. In this study, four of the transfer functions divided into two families, S-shaped and V-shaped, are evaluated. Thus, four different variations of binary SSA are formed as binary SSA-Tanh, binary SSA-Sigm, binary SSA-MSigm and binary SSA-Arctan. Two different techniques (SimSSA and LogicSSA) are developed at the candidate solution production schema in binary SSA. SimSSA is used to measure similarities between two binary solutions. With SimSSA, binary SSA's ability to discover new points in search space has been increased. Thus, binary SSA is able to find global optimum instead of local optimums. LogicSSA which is inspired by the logic gates and a popular method in recent years has been used to avoid local minima traps. By these two techniques, the exploration and exploitation capabilities of binary SSA in the binary search space are improved. Eighteen unimodal and multimodal standard benchmark optimization functions are employed to evaluate variations of binary SSA. To select the best variations of binary SSA, a comparative study is presented. The Wilcoxon signed-rank test has applied to the experimental results of variations of binary SSA. Compared to well-known evolutionary and recently developed methods in the literature, the variations of binary SSA performance is quite good. In particular, binary SSA-Tanh and binary SSA-Arctan variations of binary SSA showed superior performance.																	1432-7643	1433-7479				SEP	2020	24	17					12953	12979		10.1007/s00500-020-04718-w		JAN 2020											
J								Optimal electrical load forecasting for hybrid renewable resources through a hybrid memetic cuckoo search approach	SOFT COMPUTING										Hybrid renewable energy system; Deep recurrent neural network; Memetic; Cuckoo search; RMSE; MAPE; Load forecasting	ENERGY; ALGORITHM	Although renewable energy grows to be progressively trendier in the universal power grid, enhancing the precision or accuracy is a crucial task. Therefore, managing, operating and planning of modern power systems become difficult in case of renewable energy load forecasting. Due to the intermittent and disordered nature of renewable resources, load forecasting becomes a complicated task. The renewable energy system introduces various approaches to enhance load forecasting accuracy. This paper describes the technofeasibility and the optimal design of HRE resources such as photovoltaic, wind turbine, biogasifiers, and battery to satisfy all power demand optimally using a hybrid algorithm. The hybrid algorithm is the grouping of DRNN, memetic and cuckoo search algorithm to form a proposed HMCS-DRNN approach. This proposed approach is employed to provide better optimization performances, and apart from precision and stability in load forecasting, the HMCS-DRNN approach offers the predicted result with better efficiency and minimum error value rate. The efficiency of the proposed approach articulates by calculating the statistical measure regarding RMSE and MAPE, respectively. The simulation results describe that the performances of the HMCS algorithm provide better optimization results on various 30 unconstrained benchmark functions.																	1432-7643	1433-7479				SEP	2020	24	17					13099	13114		10.1007/s00500-020-04727-9		JAN 2020											
J								An analytical study on leader and follower switching in V-shaped Canada Goose flocks for energy management purposes	SWARM INTELLIGENCE										Canada Geese; Drag; Energy; Flock; Leader; Reconfiguration	FORMATION FLIGHT; MIGRATING BIRDS; WING COLOR; GEESE; TEMPERATURE; RECIPROCITY; PREDATION; SELECTION; STRATEGY; SAVINGS	Migrating birds may take advantage of V-shaped flocking to reduce the required energy for their flight. Studies have shown that the birds in different positions in V-shaped flight contend with different drag forces. Lead and follower birds may have to overcome more drag forces than the other birds in a V-shaped flock. Some observations of different kinds of flocking birds repositioning within a flock have been reported. This observation is here interpreted in an energetic context as well as its aerodynamic aspects. This paper presents the repositioning aerodynamics analysis of birds that fly in V-shaped flocks and their energy-saving consequences. This analysis demonstrates how Canada Geese can fly very far in a single day through repositioning. Extensive analysis shows that leader and tail position switching of 14 Canada Geese can improve the flight range and endurance of these migrating birds more than 44.6%. This study gives the guidelines for energy saving and optimization of flocking migrating birds through evolution.																	1935-3812	1935-3820				JUN	2020	14	2					117	141		10.1007/s11721-020-00179-x		JAN 2020											
J								Analysis of watermarking framework for color image through a neural network-based approach	COMPLEX & INTELLIGENT SYSTEMS										Watermarking framework; Neural network-based approach; Contourlet transform; Embedding and de-embedding methods; Subband coefficients	WAVELET TRANSFORM; SCHEME; ROBUST	In the research presented here, the general idea of watermarking framework is analyzed to deal with color image under a set of attacks through a neural network-based approach. It is realized in the area of transformation, especially with a focus on contourlet transform to address the proposed technique, as long as the bands of the suitable coefficients are accurately chosen. In summary, there is the logo information that is embedded in the edge of color image, while the Zenzo edge detector is correspondingly realized to handle the approach. In fact, the edge of the second subband is acquired, and subsequently, the capability of the above-referenced edge is calculated. A number of techniques are discussed to cope with the above-captioned watermarking framework through the new integration of contourlet transform in association with the multilayer perceptron to extract the logo information, appropriately. The approaches of the embedding and the de-embedding in case of learning algorithm of the aforementioned neural network through individual training data set are considered in the present research to carry out a series of experiments with different scenarios for the purpose of verifying the effectiveness of the proposed approach, obviously.																	2199-4536	2198-6053				APR	2020	6	1					213	220		10.1007/s40747-020-00129-4		JAN 2020											
J								Dual sparse learning via data augmentation for robust facial image classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Image classification; Sparse representation; Dictionary learning; l(1) Regularization	FACE RECOGNITION; K-SVD; REPRESENTATION; DICTIONARY; FUSION; SET	Data augmentation has been utilized to improve the accuracy and robustness of face recognition algorithms. However, most of the previous studies focused on using the augmentation techniques to enlarge the feature set, while the diversity produced by the virtual samples lacked sufficient attention. In sparse dictionary learning-based face recognition, l1-based sparse representation (SR) and SVD-based dictionary learning (DL) both have shown promising performance. How to utilize both of them in an enhanced training process by data augmentation is still unclear. This paper proposes a novel method that utilizes the sample diversity generated by data augmentation and integrates sparse representation with dictionary learning, to learn dual sparse features for robust face recognition. An additional feature set is created by applying sample augmentation via simply horizontal flipping of face images. The two sparse models, l1-based SR and SVD-based DL, are integrated together using our new proposed objective function. Under two-level fusion of both data and classifiers, the diversity between two training sets is well learned and utilized, in three implementations, to obtain a robust face recognition. After conducting extensive experiments on some popular facial datasets, we demonstrate the proposed method can produce a higher classification accuracy than many state-of-the-art algorithms, and it can be considered as a promising option for image-based face recognition. Our code is released at GitHub.																	1868-8071	1868-808X				AUG	2020	11	8					1717	1734		10.1007/s13042-020-01067-w		JAN 2020											
J								Decision making methods for formulating the reserve scheme of spare parts based on deep neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Spare; Reserve scheme; Deep neural network; Attribute analysis; Decision making		Determination of the variety and quantity of Spares is the primary step to guarantee the spares supply. Firstly, this paper analyzes the factors affecting the reserve scheme of Spares. Then by analyzing the inherent five attributes of Spares, two methods are proposed to determine the variety and quantity of Spares based on deep neural network. The first method ranks Spares according to their importance. A relatively simple deep neural network is used to analyze every attribute of the Spares in turn. The second method inputs all the attributes of Spares into a relatively complex deep neural network to make the decision. The experimental results show the advantages of the two methods in terms of efficiency and accuracy for formulating the reserve scheme of Spares.																	1868-5137	1868-5145															10.1007/s12652-020-01734-4		JAN 2020											
J								Deep learning in medical image registration: a survey	MACHINE VISION AND APPLICATIONS										Image registration; Deep learning; Medical imaging; Convolutional neural networks	RECONSTRUCTION; MAXIMIZATION; NETWORKS	The establishment of image correspondence through robust image registration is critical to many clinical tasks such as image fusion, organ atlas creation, and tumor growth monitoring and is a very challenging problem. Since the beginning of the recent deep learning renaissance, the medical imaging research community has developed deep learning-based approaches and achieved the state-of-the-art in many applications, including image registration. The rapid adoption of deep learning for image registration applications over the past few years necessitates a comprehensive summary and outlook, which is the main scope of this survey. This requires placing a focus on the different research areas as well as highlighting challenges that practitioners face. This survey, therefore, outlines the evolution of deep learning-based medical image registration in the context of both research challenges and relevant innovations in the past few years. Further, this survey highlights future research directions to show how this field may be possibly moved forward to the next level.																	0932-8092	1432-1769				JAN 29	2020	31	1							8	10.1007/s00138-020-01060-x													
J								A multi-model methodology for forecasting sales and returns of liquefied petroleum gas cylinders	NEURAL COMPUTING & APPLICATIONS										Data analysis; Multivariate analysis; Artificial neural networks; Time series analysis; Forecasting; Ensemble method	ENERGY-CONSUMPTION; DEMAND; ACCURACY; MODEL	In the liquefied petroleum gas (LPG) cylinder business, one of the most important assets is the LPG cylinder. This work addresses the asset acquisition planning for the LPG cylinder business of a company from the energy sector which has recently started this activity. In order to make the acquisition plan, it was necessary to forecast the sales and the LPG cylinder return rate. For that purpose, an ensemble method using time series techniques, multiple linear regression models and artificial neural networks was employed. Sales forecast was obtained using time series techniques, in particular, moving averages and exponential smoothing. Then, forecast of bottled propane gas sales and return rate was also addressed through multiple linear regression and artificial neural networks. A probability density function was defined for each of the different approaches. Afterward, using Monte Carlo simulation, the forecast values are obtained by a linear combination of the probability density functions, thus producing the final forecast. Results show that the company's expectation of growth is larger than that predicted by the proposed methodology, which means the company should reflect on its current asset acquisition strategy. By combining different approaches, the proposed multi-model methodology allowed to obtain an accurate forecasting, without requiring a lot of historical data.																	0941-0643	1433-3058				AUG	2020	32	16					12643	12669		10.1007/s00521-020-04713-0		JAN 2020											
J								A density weighted fuzzy outlier clustering approach for class imbalanced learning	NEURAL COMPUTING & APPLICATIONS										Fuzzy c-means clustering; Density clustering; Imbalanced data; Undersampling	CLASSIFICATION; FRAMEWORK	The class imbalance problem is widely studied in the machine learning community, and it is present in many real-world applications such as spam filtering, anomaly detection and medical diagnosis.In this paper, we propose a density weighted fuzzy outlier clustering approach for class imbalanced learning. The method considers a novel fuzzy neighborhood relation with local density information when assigning the weights to the samples in the clustering process, and it is then hybridized with the fuzzy outlier clustering approach for a novel fuzzy clustering method. In this way, the most representative majority class samples are chosen while the outlier samples are subjected to elimination. The validity of the proposed method is tested with synthetic and real-world datasets which demonstrates superior performance compared to other clustering-based resampling schemes. Thus, the density weighted fuzzy outlier clustering approach can be used for real life imbalanced problems.																	0941-0643	1433-3058				AUG	2020	32	16					13035	13049		10.1007/s00521-020-04747-4		JAN 2020											
J								An outranking method for multicriteria decision making with probabilistic hesitant information	EXPERT SYSTEMS										ELECTRE; Hausdorff distance; multicriteria decision making; outranking relation; probabilistic hesitant fuzzy set	ELECTRE II; FUZZY INFORMATION; SELECTION; WEIGHT; MODEL; DISTANCE; SETS	Defects of hesitant fuzzy set (HFS) manifest in actual decision-making process, so adding probabilities to the values in HFS is necessary. The probabilistic HFS (PHFS) is a useful tool to describe the uncertainty of elements in HFS by introducing occurrence probabilities. However, some important issues in PHFS utilization remain to be addressed. In this study, an outranking method for multicriteria decision making (MCDM) with probabilistic hesitant information is presented. First, the binary relations between two probabilistic hesitant fuzzy elements (PHFEs) are defined on the basis of the elimination and choice translating reality method. Some outranking relations between the alternatives are then introduced. Second, we provide a Hausdorff distance between two PHFEs. The main characteristic of the proposed Hausdorff distance is that it does not require the same length and arrangement of the PHFEs. Third, a maximizing Hausdorff distance deviation method is developed to obtain the evaluation criteria weights under a probabilistic hesitant fuzzy environment. Finally, an illustrative example in conjunction with comparative analysis is used to demonstrate that the proposed method is feasible for practical MCDM problems.																	0266-4720	1468-0394				JUN	2020	37	3			SI				e12513	10.1111/exsy.12513		JAN 2020											
J								A network perspective on genotype-phenotype mapping in genetic programming	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Evolvability; Genotype-phenotype map; Networks; Neutrality; Redundancy; Robustness	ROBUSTNESS; EVOLUTION; EVOLVABILITY	Genotype-phenotype mapping plays an essential role in the design of an evolutionary algorithm. Variation occurs at the genotypic level but fitness is evaluated at the phenotypic level, therefore, this mapping determines if and how variations are effectively translated into quality improvements. In evolutionary algorithms, this mapping has often been observed as highly redundant, i.e., multiple genotypes can map to the same phenotype, as well as heterogeneous, i.e., some phenotypes are represented by a large number of genotypes while some phenotypes only have few. We numerically study the redundant genotype-phenotype mapping of a simple Boolean linear genetic programming system and quantify the mutational connections among phenotypes using tools of complex network analysis. The analysis yields several interesting statistics of the phenotype network. We show the evidence and provide explanations for the observation that some phenotypes are much more difficult to find as the target of a search than others. Our study provides a quantitative analysis framework to better understand the genotype-phenotype map, and the results may be utilized to inspire algorithm design that allows the search of a difficult target to be more effective.																	1389-2576	1573-7632				SEP	2020	21	3			SI		375	397		10.1007/s10710-020-09379-0		JAN 2020											
J								Classification of epileptic electroencephalogram signals using tunable-Q wavelet transform based filter-bank	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Epilepsy; Electroencephalogram signal; Tunable-Q wavelet transform; Cross-information potential; Random forest classifier; Epileptic seizure classification	EEG SIGNALS; SEIZURE DETECTION; AUTOMATED DIAGNOSIS; NEURAL-NETWORKS; IDENTIFICATION; ENTROPY; REPRESENTATION; METHODOLOGY	The epilepsy is a neurological disorder and the seizure events frequently appear in epileptic patients. This disorder can be analysed through electroencephalogram (EEG) signals. In this paper, we propose a novel approach for automated identification of seizure EEG signals. The proposed method in this paper decomposes EEG signal into set of sub-band signals by applying tunable-Q wavelet transform (TQWT) based filter-bank. The sub-bands in TQWT based filter-bank have different value of quality (Q)-factor and have nearly constant bandwidth (BW). The features are computed by applying cross-information potential (CIP) on N s number of sub-band signals, for N s values varying from two to maximum number of sub-band signals obtained from TQWT based filter-bank. The features are computed for various values of N s and fed as input to random forest (RF) classifier. We have observed that, with the increase in the N s, the number of computed features increases and hence the classification accuracy (ACC) depends on N s. In this work, we have obtained ACC of 99% in the classification of normal, seizure-free, and seizure EEG signals using our proposed method. The developed algorithm is ready to be tested using huge database and can be employed to aid the epileptologists to screen the seizure-free and seizure patients accurately.																	1868-5137	1868-5145															10.1007/s12652-020-01722-8		JAN 2020											
J								P4Label: packet forwarding control mechanism based on P4 for software-defined networking	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software-defined networking; P4; Packet forwarding control	MITIGATION	For software-defined networking (SDN), the match fields of the OpenFlow protocol are fixed and limited, packet forwarding lacks fine-grained control and a valid forwarding verification mechanism, and the forwarding device cannot effectively monitor packet forgery; therefore, we propose P4Label, which is an SDN packet forwarding control mechanism based on P4. Because P4 has the feature of allowing the data plane of the network to be programmed, we construct a new P4Label protocol header. For realizing precise control of the forwarding behavior of network packet, the data plane forwarding device verifies the source authenticity and integrity of the forwarded packet, clears abnormal flow such as malicious tampering and forgery of data, and implements fine-grained packet forwarding and verification capabilities based P4Label. Finally, the study constructs a P4Label prototype system based on the open source BMv2 software switch and verifies the effectiveness and performance of the mechanism through experimental analysis. The experiments are conducted in the Mininet simulation environment; the analysis results indicate that P4Label is a valid forwarding control mechanism that can guarantee precise packet forwarding, packet source verification, and packet integrity.																	1868-5137	1868-5145															10.1007/s12652-020-01719-3		JAN 2020											
J								The architecture development of Industry 4.0 compliant smart machine tool system (SMTS)	JOURNAL OF INTELLIGENT MANUFACTURING										Smart machine tool system (SMTS); Cyber-physical manufacturing system (CPMS); Machine tool cyber system (MTCS); Cyber-physical system (CPS) operator; Monitoring; Analysis; Plan; Execution/big data analytics and AI, digital twin (MAPE/BD)	CHATTER	As part of the fourth industrial revolution, the movement to apply various enabling technologies under the name of Industry 4.0 is being promoted worldwide. Because of the wide range of applications and the capacity of manufacturing workpieces flexibly, machine tools are regarded as essential industrial elements. Hence, much research has been concerned with applying various enabling technologies such as cyber-physical systems to machine tools. To realize a machine tool suitable for Industry 4.0, development should be done in a systematic manner rather than the ad-hoc application of enabling technologies. In this paper, we propose a functional architecture for the Industry 4.0 version of machine tools, namely smart machine tool system. To reflect the voices of various stakeholders, stakeholder requirements are identified and transformed into design considerations. The design considerations are incorporated into the conceptual model and functional modeling, both of which are used to derive the functional architecture. The implementation procedure and an illustrative case study are presented for the application of the functional architecture.																	0956-5515	1572-8145															10.1007/s10845-020-01539-4		JAN 2020											
J								FSS-SDD: fuzzy-based semantic search for secure data discovery from outsourced cloud data	SOFT COMPUTING										Cloud computing; Fuzzy logic; Grading; Data discovery; Encryption; Semantic similarities		In current decade, cloud computing has become more prevalent and utilized for managing more sensitive data that are stored over cloud storage. While outsourcing the sensitive and private data on to the cloud, security is the major factor to be concentrated in efficient data search using keywords. As is auspicious, security of selective keyword search can be achieved using the powerful cryptographic technique called encryption. That is, the data that are to be outsourced have to be encrypted before storing it on cloud. Symmetric encryption is used between the data owner and the customer, whereas asymmetric encryption is used between the data owner and the cloud server. In such cases, the data discovery and retrieval have become more stimulating process. However, there are many methodologies developed for searching the encrypted outsourced data from the cloud. In this paper, the efficient data discovery process is focused and enhanced with the developed model called fuzzy-based semantic search for secure data discovery (FSS-SDD). A multi-valued logic called fuzzy logic is used, which has truth values of variables ranging between 0 and 1. It is used where the truth values range between completely true and completely false. The fuzzy logic-based semantic search improves the searching experience of the end user, by finding and retrieving the exact matching files for corresponding search files given by the user. Moreover, the model discovers the closely relevant matches using the semantic similarities, in cases, when the exact matches are not avail. For reducing the false positive rates, grading mechanism is enforced. Using the proposed FSS-SDD model, the processing overhead on new updates is effectively reduced and security in data retrieval is guaranteed.																	1432-7643	1433-7479				AUG	2020	24	16					12633	12642		10.1007/s00500-020-04701-5		JAN 2020											
J								Efficient agricultural yield prediction using metaheuristic optimized artificial neural network using Hadoop framework	SOFT COMPUTING										Big data; Hadoop; Crop yields; Agricultural; Satellite images; Artificial neural network (ANN); Population-based incremental learning (PBIL)	CROP YIELD	The low-resolution imagery of satellite is used extensively for monitoring crops and forecasting of yield which has a major role to play in the operational systems. A combination of high levels of temporal frequency along with an extended coverage was connected with lower costs per each area unit making the images a choice that is convenient at the national level and the regional level scales. There are various quantitative and qualitative approaches for low-resolution satellite imagery to be used for the primary predictor of the final yield of crops. But, very little work is done on the yield prediction that is based on environmental and satellite data. To handle such satellite images may be very challenging owing to large data amounts. Big data analysis is efficient in handling a large amount of data generated for predicting agricultural yield. In this work, a neural network is used for prediction and to enhance its performance; a population-based incremental learning technique is proposed for optimizing the weights. The results of the experiment proved that the method proposed has better results compared to that of the other methods.																	1432-7643	1433-7479				AUG	2020	24	16					12659	12669		10.1007/s00500-020-04707-z		JAN 2020											
J								Image segmentation of nasopharyngeal carcinoma using 3D CNN with long-range skip connection and multi-scale feature pyramid	SOFT COMPUTING										NPC; 3D CNN; Long-range skip connection; Multi-scale feature pyramid; Deep supervision		Nasopharyngeal carcinoma (NPC) is one of the most common cancers of the nasopharynx. A structural analysis of NPC can provide vital insights into methods of treatment. However, manually marking the boundaries of NPC in images is tedious, time-consuming, and prone to error. It has become necessary to use computer-based automatic segmentation algorithms to accurately locate NPC. However, this remains a challenging task owing to the high variation (in shape and size) in the structure of the nasopharynx across subjects. Moreover, the nasopharyngeal area is small, and this causes severe imbalance in the foreground and background categories. In this paper, we propose a 3D convolutional neural network with long-range skip connection and multi-scale feature pyramid (SFP) for the segmentation of images of NPC. Unlike the traditional skip connection in residual blocks, which only considers the feature transfer and feature fusion between the same convolutional layer, long-range skip connection with original features from the first convolution in our network is passed to each down-sampling stage using element-wise sum to effectively increase reuse of low-level features and to solve the problems of gradient disappearance and explosion. The multi-scale feature pyramid with a varying atrous rate adapts to images of different sizes to learn multi-scale features, and hierarchical contextual information regarding NPC. To accelerate the convergence of our network, we use deep supervision to generate three auxiliary segmentation maps and merge the weighted loss into the objective function. And we fuse these auxiliary segmentation maps to refine the final segmentation result. In our experiments, the proposed network was trained and tested on 3D magnetic resonance imaging (MRI) images of 120 clinical patients using 5-fold cross-validation. The average dice similarity coefficient (DSC) and average symmetric surface distance (ASSD), used as evaluation metric, were 0.737 and 1.214 mm, respectively. This shows that in terms of results, our method is superior to five state-of-the-art networks and equivalent to the judgment of an experienced physician.																	1432-7643	1433-7479				AUG	2020	24	16					12671	12680		10.1007/s00500-020-04708-y		JAN 2020											
J								Modified sine cosine algorithm-based fuzzy-aided PID controller for automatic generation control of multiarea power systems	SOFT COMPUTING										Automatic generation control (AGC); Modified sine cosine algorithm (M-SCA); Generation rate constraint (GRC); Boiler dynamics; Governor dead band (GDB); PID controller; Fuzzy logic controller	LOAD-FREQUENCY CONTROL; PI/PID CONTROLLER; AGC; OPTIMIZATION	The research paper presents implementation of a fuzzy rule and membership function-based fuzzy-aided PID controller for automatic generation control (AGC) in multiarea nonlinear power system. At the initial stage of this proposed work, a three-area nine-unit installed interconnected network is considered for developing different dynamic responses in response to AGC analysis. A modified approach named modified sine cosine algorithm (M-SCA) is proposed for tuning the gain parameters of the above-proposed fuzzy controller to produce close optimum gain values. The proposed modified algorithm is developed from its original sine cosine algorithm by improving and updating few equations which is capable of making the balance between exploration and exploitation levels of this algorithm and improving the updating quality of iteration. To impose supremacy of M-SCA technique, it is examined through convergence curves and its performance is compared with host sine cosine algorithm, genetic algorithm, and particle swarm optimization algorithm. For controller supremacy analysis, the performance of the proposed fuzzy-aided PID controller is compared with conventional I, PI, and PID controllers, and it has been revealed that proposed M-SCA-tuned fuzzy-aided PID controller exhibits better performances through different deviated responses for AGC analysis. To demonstrate most standard and supremacy of proposed approaches, finally these are tested through a five-area ten-unit system considering some physical nonlinear constraints like generation rate constraint, governor dead band, boiler dynamics and time delay. At the final observation level, the proposed fuzzy controller has gone through different sensitivity analyses with variation of different system parametric conditions and different load conditions.																	1432-7643	1433-7479				SEP	2020	24	17					12919	12936		10.1007/s00500-020-04716-y		JAN 2020											
J								Adaptive parameter tuning stacked autoencoders for process monitoring	SOFT COMPUTING										Process monitoring; Autoencoder; Stacked autoencoder; Adaptive parameter tuning	K-NEAREST NEIGHBOR; NEURAL-NETWORK; COMPONENT ANALYSIS	In process monitoring based on stacked autoencoders (SAEs), the performance of monitoring models is directly decided by the validity of the structure and parameters, which are primarily determined by time-consuming manual adjustments. This paper presents a novel method that can adaptively select parameters rather than tuning them manually. The proposed method is called adaptive parameter tuning SAE (APT-SAE). Basic SAEs aim to compress the original input data and extract simple and abstract features. Thus, the redundant information of each hidden layer output should be as small as possible. The next layer of nodes can be remarkably reduced if the amount of redundant information is large. During the pre-training stage of APT-SAE, an adaptive parameter tuning strategy is used for rapidly determining the number of layers and nodes in the paper. The cross-covariance of each AE's input data is used to determine the node number of succeeding AE. The pre-training stage ends when the correlation is weak, which is decided by the average value of cross-variance matrix. The proposed method is applied to a benchmark problem, and it outperforms several state-of-the-art methods.																	1432-7643	1433-7479				SEP	2020	24	17					12937	12951		10.1007/s00500-020-04717-x		JAN 2020											
J								Soft computing-based fuzzy integral sliding mode control: a real-time investigation on a conical tank process	SOFT COMPUTING										Conical tank process; Fuzzy integral sliding mode control; Genetic algorithm; Parameter variation; Robustness		In this work, a fuzzy integral sliding mode controller (FISMC) for level control in a conical tank process is demonstrated in real time. In traditional sliding mode controller (SMC) algorithm, the robustness with respect to parameter variations and external disturbances can be achieved only after the reach of sliding phase. However, robustness is not guaranteed during the reaching phase. But integral sliding mode controller (ISMC) hunts to eliminate the reaching phase by imposing sliding mode throughout the system response. ISMC also mitigates chattering caused by discontinuity of controller. Hence, integral of error term is used in the sliding surface. A modified power rate reaching law is proposed to describe the dynamics of the switching function. The fuzzy logic system is integrated to approximate the sliding variable, and control law is formulated so as to alleviate the chattering effect of the control signal. In this paper, Takagi and Sugeno fuzzy logic is integrated with ISMC to achieve smoother sliding surface. Genetic algorithm (GA) is used to tune the membership functions of fuzzy logic and the parameters of the control law. GA-tuned FISMC integrates the features of fuzzy logic control, SMC and soft computing techniques. The effectiveness of algorithm is demonstrated in an experimental setup. The reported results confirm the superiority of GAFISMC compared with proportional integral controller and ISMC algorithm. The real-time implementation ensures the robustness of GAFISMC in terms of operating-level variations, parameter variations and disturbance rejection.																	1432-7643	1433-7479				SEP	2020	24	17					13135	13146		10.1007/s00500-020-04729-7		JAN 2020											
J								The combination of term relations analysis and weighted frequent itemset model for multidocument summarization	COMPUTATIONAL INTELLIGENCE										multidocument summarization; term association; term weighting; weighted pattern	DOCUMENT	Nowadays, it is necessary that users have access to information in a concise form without losing any critical information. Document summarization is an automatic process of generating a short form from a document. In itemset-based document summarization, the weights of all terms are considered the same. In this paper, a new approach is proposed for multidocument summarization based on weighted patterns and term association measures. In the present study, the weights of the terms are not equal in the context and are computed based on weighted frequent itemset mining. Indeed, the proposed method enriches frequent itemset mining by weighting the terms in the corpus. In addition, the relationships among the terms in the corpus have been considered using term association measures. Also, the statistical features such as sentence length and sentence position have been modified and matched to generate a summary based on the greedy method. Based on the results of the DUC 2002 and DUC 2004 datasets obtained by the ROUGE toolkit, the proposed approach can outperform the state-of-the-art approaches significantly.																	0824-7935	1467-8640				MAY	2020	36	2					783	812		10.1111/coin.12270		JAN 2020											
J								Exceptional spatio-temporal behavior mining through Bayesian non-parametric modeling	DATA MINING AND KNOWLEDGE DISCOVERY										Subgroup discovery; Exceptional model mining; Spatio-temporal analytics; Collective social media; Bayesian non-parametric model		Collective social media provides a vast amount of geo-tagged social posts, which contain various records on spatio-temporal behavior. Modeling spatio-temporal behavior on collective social media is an important task for applications like tourism recommendation, location prediction and urban planning. Properly accomplishing this task requires a model that allows for diverse behavioral patterns on each of the three aspects: spatial location, time, and text. In this paper, we address the following question: how to find representative subgroups of social posts, for which the spatio-temporal behavioral patterns are substantially different from the behavioral patterns in the whole dataset? Selection and evaluation are the two challenging problems for finding the exceptional subgroups. To address these problems, we propose BNPM: a Bayesian non-parametric model, to model spatio-temporal behavior and infer the exceptionality of social posts in subgroups. By training BNPM on a large amount of randomly sampled subgroups, we can get the global distribution of behavioral patterns. For each given subgroup of social posts, its posterior distribution can be inferred by BNPM. By comparing the posterior distribution with the global distribution, we can quantify the exceptionality of each given subgroup. The exceptionality scores are used to guide the search process within the exceptional model mining framework to automatically discover the exceptional subgroups. Various experiments are conducted to evaluate the effectiveness and efficiency of our method. On four real-world datasets our method discovers subgroups coinciding with events, subgroups distinguishing professionals from tourists, and subgroups whose consistent exceptionality can only be truly appreciated by combining exceptional spatio-temporal and exceptional textual behavior.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1267	1290		10.1007/s10618-020-00674-z		JAN 2020											
J								Proximity-based cloud resource provisioning for deep learning applications in smart healthcare	EXPERT SYSTEMS										AHP; cloud-hosted deep leaning; mobile cloud; practical deep learning; proximity based; reliable resource provisioning; resource migration; smart healthcare	IOT	Deep learning is a powerful technology that enables intelligent data processing in the smart healthcare domain. Inspired by the tremendous processing power of cloud computing, the training process and the model repository of deep learning are moved to the cloud. Cloud-assisted deep learning applications enable smart mobile users to experience quick predictive results. Health professionals use smart mobile devices to convey recordings of the patient and to receive the best inference results. The mobility of these devices causes severe performance degradation as it increases the distance between its current location and the edge cloud where the virtual machines are provisioned. Therefore, mobility-based resource provisioning to identify a suitable server based on deadline constraints, available resources, and cost metrics is crucial. This paper proposes a proximity-based resource provisioning technique that guarantees minimal delay in obtaining inference results with a local mobile cloud system. The proposed technique comprises two algorithms (a) deadline-based initial resource provisioning and (b) resource migration and provisioning at suitable cloudlet during location change. The proposed technique is implemented in a mobile cloud platform running the inference method of a smart mobile healthcare application. The performance results show that the proposed technique outperforms the state-of-the-art techniques in terms of the response time, deadline meeting percentage, and system utilization.																	0266-4720	1468-0394														e12524	10.1111/exsy.12524		JAN 2020											
J								Learning deep hierarchical and temporal recurrent neural networks with residual learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Deep learning; Recurrent neural networks; Residual learning; Long-short term memory; Sequence modeling		Learning both hierarchical and temporal dependencies can be crucial for recurrent neural networks (RNNs) to deeply understand sequences. To this end, a unified RNN framework is required that can ease the learning of both the deep hierarchical and temporal structures by allowing gradients to propagate back from both ends without being vanished. The residual learning (RL) has appeared as an effective and less-costly method to facilitate backward propagation of gradients. The significance of the RL is exclusively shown for learning deep hierarchical representations and temporal dependencies. Nevertheless, there is lack of efforts to unify these finding into a single framework for learning deep RNNs. In this study, we aim to prove that approximating identity mapping is crucial for optimizing both hierarchical and temporal structures. We propose a framework called hierarchical and temporal residual RNNs, to learn RNNs by approximating identity mappings across hierarchical and temporal structures. To validate the proposed method, we explore the efficacy of employing shortcut connections for training deep RNNs structures for sequence learning problems. Experiments are performed on Penn Treebank, Hutter Prize and IAM-OnDB datasets and results demonstrate the utility of the framework in terms of accuracy and computational complexity. We demonstrate that even for large datasets exploiting parameters for increasing network depth can gain computational benefits with reduced size of the RNN "state".																	1868-8071	1868-808X				APR	2020	11	4					873	882		10.1007/s13042-020-01063-0		JAN 2020											
J								A Closeness- and Priority-Based Logical Study of Social Network Creation	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Social network; Social network creation; Similarity; Distance; Closeness; Priority; Threshold; Dynamic epistemic logic	SIMILARITY; KNOWLEDGE; REVISION	This paper is part of an on-going programme on the study of the logical aspects of social network formation. It recalls the so-called social network model, discussing the properties of a notion of closeness between agents (in terms of the number of traits they have in common); then introduces an extended social network model in which different agents might assign different values to different traits, discussing the properties of the notion of weighted closeness that arises. These notions are used to define social network creation operations by means of a threshold strategy. The paper studies the properties of the social networks the updates create, providing sound and complete axiom systems for formal languages describing these updates' effects.																	0925-8531	1572-9583				MAR	2020	29	1			SI		21	51		10.1007/s10849-019-09311-5		JAN 2020											
J								Faster horn diagnosis-a performance comparison of abductive reasoning algorithms	APPLIED INTELLIGENCE										Abductive reasoning; Model-based diagnosis; Abductive diagnosis	COMPLEXITY	Abductive inference derives explanations for encountered anomalies and thus embodies a natural approach for diagnostic reasoning. Yet its computational complexity, which is inherent to the expressiveness of the underlying theory, remains a disadvantage. Even when restricting the representation to Horn formulae the problem is NP-complete. Hence, finding procedures that can efficiently solve abductive diagnosis problems is of particular interest from a research as well as practical point of view. In this paper, we aim at providing guidance on choosing an algorithm or tool when confronted with the issue of computing explanations in propositional logic-based abduction. Our focus lies on Horn representations, which provide a suitable language to describe most diagnostic scenarios. We illustrate abduction via two contrasting problem formulations: direct proof methods and conflict-driven techniques. While the former is based on determining logical consequences, the later searches for suitable refutations involving possible causes. To reveal runtime performance trends we conducted a case study, in which we compared publicly available general purpose tools, established Horn reasoning engines, as well as new variations of known methods as a means for abduction.																	0924-669X	1573-7497				MAY	2020	50	5					1558	1572		10.1007/s10489-019-01575-5		JAN 2020											
J								Multiple allocation p-hub location problem for content placement in VoD services: a differential evolution based approach	APPLIED INTELLIGENCE										Video-on-demand (VoD) services; Content distribution networks; Database segment location; Hub location; Multiple hub allocation; Differential evolution (DE); IBM ILOG CPLEX	PARTICLE SWARM OPTIMIZATION; ALGORITHM; DESIGN	In video-on-demand (VoD) services, large volumes of digital data are kept at hubs which are spatially distributed over large geographic areas and users are connected to these hubs based on their demands. In this article, we consider a large database of video files, that are pre-partitioned to multiple segments based on the demand patterns of users. These segments are restricted to be located only in hubs. Here, users are allowed to be allocated to multiple hubs and all hubs are assumed to be connected with each other. We jointly decide the location of hubs, the placement of segments to these hubs and then the assignment of users to these hubs as per their demand patterns and finally, we find the optimal paths to route the demands of users for different segments having the objective of minimizing the total routing cost. In this article, a differential evolution (DE) based method is proposed to solve the problem. The proposed DE-based method utilizes an efficient function to evaluate the objective value of a candidate solution to the proposed problem. It also incorporates two problem-specific solution refinement techniques for faster convergence. Instances of the problem are generated from the real world movie database and the proposed method is applied to these instances and the performance is evaluated against the benchmark results obtained from CPLEX.																	0924-669X	1573-7497				MAY	2020	50	5					1573	1589		10.1007/s10489-019-01609-y		JAN 2020											
J								Information extraction framework to build legislation network	ARTIFICIAL INTELLIGENCE AND LAW										Optical character recognition; Information extraction; Named entity recognition; Relation extraction; Approximate string matching; Legislation network; Evaluation	CENTRALITY MEASURES; ROBUSTNESS; ERROR	This paper concerns an information extraction process for building a dynamic legislation network from legal documents. Unlike supervised learning approaches which require additional calculations, the idea here is to apply information extraction methodologies by identifying distinct expressions in legal text in order to extract network information. The study highlights the importance of data accuracy in network analysis and improves approximate string matching techniques to produce reliable network data-sets with more than 98% precision and recall. The applications and the complexity of the created dynamic legislation network are also discussed and challenged.																	0924-8463	1572-8382															10.1007/s10506-020-09263-3		JAN 2020											
J								A Simple and Light-Weight Attention Module for Convolutional Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Attention mechanism; Deep learning; Convolutional Neural Networks; Image Recognition; Self-attention		Many aspects of deep neural networks, such as depth, width, or cardinality, have been studied to strengthen the representational power. In this work, we study the effect of attention in convolutional neural networks and present our idea in a simple self-contained module, called Bottleneck Attention Module (BAM). Given an intermediate feature map, BAM efficiently produces the attention map along two factorized axes, channel and spatial, with negligible overheads. BAM is placed at bottlenecks of various models where the downsampling of feature maps occurs, and is jointly trained in an end-to-end manner. Ablation studies and extensive experiments are conducted in CIFAR-100/ImageNet classification, VOC2007/MS-COCO detection, super resolution and scene parsing with various architectures including mobile-oriented networks. BAM shows consistent improvements over all experiments, demonstrating the wide applicability of BAM. The code and models are available at .																	0920-5691	1573-1405				APR	2020	128	4			SI		783	798		10.1007/s11263-019-01283-0		JAN 2020											
J								EdgeStereo: An Effective Multi-task Learning Network for Stereo Matching and Edge Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION										Stereo matching; Edge detection; Multi-task learning; Edge-aware smoothness loss; Residual pyramid	EFFICIENT; ACCURATE	Recently, leveraging on the development of end-to-end convolutional neural networks, deep stereo matching networks have achieved remarkable performance far exceeding traditional approaches. However, state-of-the-art stereo frameworks still have difficulties at finding correct correspondences in texture-less regions, detailed structures, small objects and near boundaries, which could be alleviated by geometric clues such as edge contours and corresponding constraints. To improve the quality of disparity estimates in these challenging areas, we propose an effective multi-task learning network, EdgeStereo, composed of a disparity estimation branch and an edge detection branch, which enables end-to-end predictions of both disparity map and edge map. To effectively incorporate edge cues, we propose the edge-aware smoothness loss and edge feature embedding for inter-task interactions. It is demonstrated that based on our unified model, edge detection task and stereo matching task can promote each other. In addition, we design a compact module called residual pyramid to replace the commonly-used multi-stage cascaded structures or 3-D convolution based regularization modules in current stereo matching networks. By the time of the paper submission, EdgeStereo achieves state-of-art performance on the FlyingThings3D dataset, KITTI 2012 and KITTI 2015 stereo benchmarks, outperforming other published stereo matching methods by a noteworthy margin. EdgeStereo also achieves comparable generalization performance for disparity estimation because of the incorporation of edge cues.																	0920-5691	1573-1405				APR	2020	128	4			SI		910	930		10.1007/s11263-019-01287-w		JAN 2020											
J								A novel hybrid time series forecasting model based on neutrosophic-PSO approach	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Neutrosophic set; PSO; Time series forecasting; Entropy	TEMPERATURE PREDICTION; FUZZY; ORDER; ENROLLMENTS; ENTROPY; SETS	This article proposed a new time series forecasting model based on neutrosophic set (NS) theory and particle swarm optimization (PSO) algorithm. The proposed model initiated with the representation of time series dataset into NS using three different memberships of NS, i.e., truth-membership, indeterminacy-membership and falsity-membership. This NS representation of time series was referred to as neutrosophic time series (NTS). It was observed that the forecasting accuracy of the proposed model was highly relied on the optimal selection of the universe of discourse of time series dataset. In this study, this problem was resolved by using the PSO algorithm. The proposed model was verified and validated with three different datasets that included the university enrollments dataset of Alabama, TAIFEX index and TSEC weighted index. Experimental results showed that the proposed model outperformed existing benchmark models with average forecasting error rates of 0.80%, 0.015% and 0.90% for the university enrollments, TAIFEX and TSEC, respectively.																	1868-8071	1868-808X				AUG	2020	11	8					1643	1658		10.1007/s13042-020-01064-z		JAN 2020											
J								A self-adaptive exhaustive search optimization-based method for restoration of bridge defects images	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Bridge defects; Computer vision; Image restoration; Elman neural network; Moth-flame optimization; Filtering protocol	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; DAMAGE DETECTION; PEPPER NOISE; GREY WOLF; FILTER; REMOVAL; DESIGN; SALT	Existing bridges are aging and deteriorating. Furthermore, large number of bridges exist in transportation networks meanwhile maintenance budgets are being squeezed. This state of affairs necessities the development of automatic bridge defects evaluation model using computer vision technologies to overcome the limitations of visual inspection. The digital images are prone to degradation by noises during the image acquisition phase. The absence of efficient bridge defects image restoration method results in inaccurate condition assessment models and unreliable bridge management systems. The present study introduces a self-adaptive two-tier method for detection of noises and restoration of bridge defects images. The first model adopts Elman neural network coupled with invasive weed optimization algorithm to identify the type of noise that corrupts images. In the second model, moth-flame optimization algorithm is utilized to design a hybrid image filtering protocol that involves an integration of spatial domain and frequency domain filters. The proposed detection model was assessed through comparisons with other machine learning models as per split validation and tenfold cross validation. It attained the highest classification accuracies, whereas the accuracy, sensitivity, specificity, precision, F-measure and Kappa coefficient are 95.28%, 95.24%, 98.07%, 95.25%, 95.34%. 95.43% and 0.935, respectively in the separate noise recognition module. The capabilities of the proposed restoration model were evaluated against some well-known good-performing optimization algorithms in addition to some conventional restoration models. Moth-flame optimization algorithm outperformed other restoration models, whereas peak signal to noise ratio, mean-squared error, normalized absolute error and image enhancement factor are 25.359, 176.319, 0.0585 and 7.182, respectively.																	1868-8071	1868-808X				AUG	2020	11	8					1659	1716		10.1007/s13042-020-01066-x		JAN 2020											
J								Decision making with MABAC method under probabilistic single-valued neutrosophic hesitant fuzzy environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Probabilistic single valued neutrosophic hesitant fuzzy set; Decision making; MABAC method	AGGREGATION OPERATORS; MULTIPLE CRITERIA; CORRELATION-COEFFICIENT; SIMILARITY MEASURES; SETS; SELECTION; MODEL; WEIGHT	Recently, there has been great interest on single valued neutrosophic hesitant fuzzy set theory. When compared single valued neutrosophic set, it is more convenient for real life situations. But even in this case, there is still missing data for some decision problems. Probabilistic single valued neutrosophic hesitant fuzzy sets (PSVNHFSs) are defined to solve this problem. Even though it contains more information, it needs some improvements. In this paper, the modified PSVNHFS is defined and some improvements in the theory of PSVNHFS are proposed. Also, we improve some algebraic properties of this set theory and define a distance operator for PSVNHFSs. Then we introduce two aggregation operators called probabilistic single valued neutrosophic hesitant fuzzy weighted arithmetic average (PSVNHFWA) operator and probabilistic single valued neutrosophic hesitant fuzzy weighted geometric average (PSVNHFWG) operator related to algebraic properties presented in this paper. Also, we extend the MABAC method under the probabilistic single valued neutrosophic hesitant fuzzy set theory. Finally, we give an illustrative example to demonstrate the stability and reliability of the proposed theory.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4195	4212		10.1007/s12652-020-01699-4		JAN 2020											
J								Energy Efficient Cluster based Multilevel Hierarchical Routing for multi-hop Wireless Sensor Network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster; Multilevel Hierarchical; Clustering; Routing; Data gathering; WSN	HEAD; ALGORITHM	Wireless Sensor Network (WSN) is very effective for research community in recent years in demand and adaptive applications. The nodes of WSN are limited with their energy which requires the energy to be utilized in efficient manner. The factor of energy depletion would be reduced by grouping the nodes to support routing and data collection. Different algorithms recommended towards the energy management in different conditions. This research work proposes an Energy Efficient Cluster based Multilevel Hierarchical Routing (EECMHR) technique to support effective data collection in WSN. The method splits the entire network region into number of regions and clusters where each cluster has single Cluster Head (CH). The nodes of WSN are grouped in hierarchical level which performs routing through the CH identified which is rotated at each timestamp according to energy conditions. EECMHR approach has been simulated by using the Network Simulator 2 under different scenarios, and its performance is compared with existing clustering algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-01700-0		JAN 2020											
J								An efficient hybrid methodology for detection of cancer-causing gene using CSC for micro array data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cancer diagnosis; Cuckoo search; Gene expression data; Genetic algorithm; Classification	SELECTION; CLASSIFICATION; SVM	Cancer is deadly diseases still exist with a lot of subtypes which makes lot of challenges in a biomedical research. The data available of gene expression with relevant gene selection with eliminating redundant genes is challenging for role of classifiers. The availability of multiple scopes of gene expression data is curse, the selection of gene is play vital role for refining gene expression data classification performance. The major role of this article is to derive a heuristic approach to pick the highly relevant genes in gene expression data for the cancer therapy. This article demonstrates a modified bio-inspired algorithm namely cuckoo search with crossover (CSC) for choosing genes from technology of micro array that are able to classify numerous cancer sub-types with extraordinary accuracy. The experiment results are done with five benchmark cancer gene expression datasets. The results depict that CSC is outperforms than CS and other well-known approaches. It returns 99% accuracy in a classification for the dataset namely prostate, lung and lymphoma for top 200 genes. Leukemia and colon dataset CSC is 96.98% and 98.54% respectively.																	1868-5137	1868-5145															10.1007/s12652-020-01731-7		JAN 2020											
J								New filter approaches for feature selection using differential evolution and fuzzy rough set theory	NEURAL COMPUTING & APPLICATIONS										Fuzzy rough set; Differential evolution; Feature selection; Classification	ATTRIBUTE REDUCTION; ALGORITHM; OPTIMIZATION	Nowadays the incredibly advanced developments in information technologies have led to exponential growth in the datasets with respect to both the dimensionality and the sample size. This trend can be easily illustrated in popular online data repositories (e.g., UCI machine learning repository). The more growth in the datasets, the more challenging the data management becomes. This is because such datasets usually comprise a high level of noise as well as the necessary information. Therefore, the elimination of noisy features in the datasets is an important task to discover meaningful knowledge. Although a large number of feature selection approaches have been proposed in the literature to deal with noisy features, the need for the studies based on feature selection has not come to an end. In this paper, we propose differential evolution-based feature selection approaches wrapped around the principles of fuzzy rough set theory. In contrast to well-known feature selection criteria such as standard mutual information, standard rough set and probabilistic rough set, our approaches can also deal with real-valued variables without the requirement of discretization. Moreover, the feature subsets selected by our approaches can profoundly improve the classification performance compared to the recent particle swarm optimization approaches based on probabilistic rough set and the state-of-the-art filter approaches.																	0941-0643	1433-3058				APR	2020	32	7			SI		2929	2944		10.1007/s00521-020-04744-7		JAN 2020											
J								One-dimensional model for the unsteady flow of a generalized third-grade viscoelastic fluid	NEURAL COMPUTING & APPLICATIONS										Third-grade fluids; Shear-thinning fluids; Non-Newtonian fluids; Hierarchical approach	VISCOUS-FLUID; STRESS; RODS; THERMODYNAMICS; STABILITY; MOTION; PIPES	Specific algorithms for non-Newtonian fluids flow depend on the governing constitutive equation. In this work, we present a constitutive equation for a third-grade fluid in which a specific normal stress coefficient depends on the shear rate. This new three-dimensional model is suitable for studies where phenomena like shear-thinning or shear-thickening occur. Using a function of power-law type, we apply the Cosserat theory to fluid dynamics, reducing the exact three-dimensional equations to a one-dimensional system depending only on time and on a single spatial variable. From this reduced system, we solve the unsteady equations for the wall shear stress and mean pressure gradient depending on the volume flow rate, Womersley number, viscoelastic coefficients, and flow index over a finite section of the tube geometry with constant circular cross section. Attention is focused on numerical simulations of unsteady flows regimes by using a Runge-Kutta method.																	0941-0643	1433-3058				AUG	2020	32	16					12881	12894		10.1007/s00521-020-04733-w		JAN 2020											
J								A biologically plausible network model for pattern storage and recall inspired by Dentate Gyrus	NEURAL COMPUTING & APPLICATIONS										CA3; Memory; Hippocampus; Hopfield network; DG	LONG-TERM POTENTIATION; MOSSY-FIBER SYNAPSES; SPARSE; MEMORY; CA3; HIPPOCAMPUS; COMPLETION; INDUCTION; NEURONS; NUMBER	In the race to achieve better performance, artificial intelligence has become more about the end rather than the means, which is general intelligence. This work aims to bridge the gap between the two by finding a complementary midline. The objective of this work is to project the role of Dentate Gyrus in enhancing the performance of an autoassociative network, paving the way to develop a biologically plausible neural network which, in the future, would help in simulating the network present in our brain. The proposed network imbibes biological similarities with respect to connectivity, weight updation, and activation function. Dentate Gyrus uses pre-integration lateral inhibition form of learning, and the autoassociative network is implemented using Hopfield network. The performance of the autoassociative network in the presence and absence of Dentate Gyrus is observed across multiple parameters. The results show an increase of 38% in storage capacity and a decrease of 15% in the error tolerance capability of the network in the presence of Dentate Gyrus.																	0941-0643	1433-3058				SEP	2020	32	17					13289	13299		10.1007/s00521-019-04670-3		JAN 2020											
J								Quintuple Implication Principle on interval-valued intuitionistic fuzzy sets	SOFT COMPUTING										Fuzzy modus ponens; Fuzzy modus tollens; Interval-valued intuitionistic fuzzy sets; Quintuple Implication Principle; Triple Implication Principle	TRIPLE-I METHOD; FULL IMPLICATION ALGORITHMS; UNIFIED FORMS; CLASSIFICATION SYSTEMS; SIMILARITY MEASURE; LOGIC FOUNDATION; T-NORMS; ROBUSTNESS; CONSTRUCTION; PERFORMANCE	This paper mainly aims to introduce Quintuple Implication Principle (QIP) on interval-valued intuitionistic fuzzy sets (IVIFSs). Firstly, some algebraic properties of a class of interval-valued intuitionistic triangular norms are discussed in detail. In particular, a unified expression of residual interval-valued intuitionistic fuzzy implications generated by left-continuous triangular norms is presented. Secondly, Triple Implication Principles (TIPs) of both interval-valued intuitionistic fuzzy modus ponens (IVIFMP) and fuzzy modus tollens (IVIFMT) based on residual interval-valued intuitionistic fuzzy implications are analyzed. It is shown that the TIP solution of IVIFMP is recoverable, and the TIP solution of IVIFMT is only weakly local recoverable. Moreover, it sees by an illustrated example that the TIP method sometimes makes the computed solutions for IVIFMP and IVIFMT meaningless or misleading. To avoid the above shortcoming and enhance the recovery property of TIP solution of IVIFMT, QIP and alpha-QIP for IVIFMP and IVIFMT are investigated and the corresponding expressions of solutions of them are also given, respectively. In addition, the QIP methods for IVIFMP and IVIFMT are recoverable and sound. Finally, QIP solutions of IVIFMP for multiple fuzzy rules are provided. An application example for medical diagnosis is given to illustrate the feasibility and effectiveness of the QIP of IVIFMP.																	1432-7643	1433-7479				AUG	2020	24	16					12091	12109		10.1007/s00500-019-04649-1		JAN 2020											
J								A new soft computing approach for green supplier selection problem with interval type-2 trapezoidal fuzzy statistical group decision and avoidance of information loss	SOFT COMPUTING										Green supplier selection problem (GSSP); Green growth and influential factors; Mean and standard deviation values; Avoiding information loss; Weighting of decision makers; Interval type-2 trapezoidal fuzzy sets (IT2TrFSs)	MULTI CRITERIA APPROACH; CHAIN MANAGEMENT; PERFORMANCE; CONSTRUCTION; NETWORK; INTEGRATION; COMPANIES; RANKING; SYSTEMS	Green supplier selection problem (GSSP) is viewed as multiple attributes group decision-making (MAGDM) issue that includes the green growth and influential factors within subjective and objective natures. Because of the expanding uncertain conditions of social and economic environments, some assessment factors are not sufficiently described by numerical appraisals and classic fuzzy sets. Moreover, supply chain decision makers (DMs) may not provide complete rationality under numerous viable choice circumstances. In this research, a new MAGDM model is proposed by interval type-2 trapezoidal fuzzy numbers (IT2TrFNs) via some matrices of possibilistic mean and standard deviation statistical concepts. A new weighting method of experts within the group decision-making process is developed based on possibilistic statistical information. Also, a new ranking process based on relative-closeness coefficients is presented to rank all green supplier candidates under IT2TrF uncertainty. Finally, this research offers an illustrative example in supply chain networks to appraise green supplier candidates in terms of some factors by the proposed model along with the comparison to a recent decision method.																	1432-7643	1433-7479				AUG	2020	24	16					12313	12327		10.1007/s00500-020-04675-4		JAN 2020											
J								Energy-efficient multipath routing in networking aid of clustering with OGFSO algorithm	SOFT COMPUTING										Mobile ad hoc network (MANET); Clustering of mobile nodes; Energy consumption; Network lifetime and OGFSO algorithm	AD HOC NETWORKS; FUZZY; PROTOCOL; ISSUES	Networking is made out of remote mobile nodes; these nodes progressively structure a network with no centralized controller. Due to this dynamic nature, the nodes act as routers to keep up routing and sending data bundles. Be that as it may, the mobile nodes do not have changeless power supply and need to depend on batteries; in this way, they consume more energy and lessen network lifetime. The proposed work considers the issue of energy consumption in MANET, and it is overwhelmed by the optimization of energy constraint. At first, the mobile nodes in MANET are grouped by the proposed K-medoid clustering algorithm, which can lessen the cost of the routing of data in huge and dense networks. From each cluster, the limited energy consumption-based multipath routing is accomplished by utilizing the opposition genetic-based fish swarm optimization algorithm. The exhibition of the proposed system will be evaluated as far as data transmission, service cost, network lifetime, etc., are concerned. Simulation results revealed that the energy efficiency and network lifetime of our recommended work are improved than those of existing work.																	1432-7643	1433-7479				SEP	2020	24	17					12845	12854		10.1007/s00500-020-04710-4		JAN 2020											
J								Optimum design and analysis of HRES for rural electrification: a case study of Korkadu district	SOFT COMPUTING										HRES; HOMER; Forecasted load demand; PV; WT; Bio-diesel generators; HCRES; SRES; SCS; Renewable fractions	RENEWABLE ENERGY SYSTEM; AGGREGATION OPERATORS; ECONOMIC-ANALYSIS; TECHNOECONOMIC ANALYSIS; POWER-SYSTEMS; HYBRID; MANAGEMENT; VILLAGE; BIOMASS; OPTIMIZATION	This paper demonstrates the optimum design and analysis of hybrid renewable energy system (HRES) for village electrification in Korkadu, Puducherry, India. Renewable energy sources (RES) comprises of photovoltaic, wind turbine and bio-diesel generators. The main target of this work is to design an optimal HRES system that can generate and provide cost-effective electricity to satisfy the electricity need. In the pre-hybrid optimization model for the electric renewable (HOMER), the paper evaluates the load forecasting for the selected district. For reliable electrification, the desired HRES needs to meet the forecasted load demand. HOMER software is used to estimate the different feasible hybrid configurations. The configurations are hybrid conventional (bio-gasifiers) and renewable energy system, standalone renewable energy system with high renewable fractions and standalone conventional (bio-gasifiers) system. From the investigations, it indicates that the Korkadu zone is highly potential area for implementing standalone hybrid electrification system. Furthermore, the proposed work result demonstrates that the HRES-based power generation at off-grid location can be a cost-effective. Additionally, our proposed strategy can conquer the uncertainty found in RES and the over-sizing issues in installed capacity.																	1432-7643	1433-7479				SEP	2020	24	17					13051	13068		10.1007/s00500-020-04724-y		JAN 2020											
J								Enriched Latent Dirichlet Allocation for Sentiment Analysis	EXPERT SYSTEMS										latent Dirichlet allocation (LDA); machine learning; opinion mining; sentiment analysis; topic modelling	FEATURE-SELECTION; TOPIC MODEL	One of the main benefits of unsupervised learning is that there is no need for labelled data. As a method of this category, latent Dirichlet allocation (LDA) estimates the semantic relations between the words of the text effectively and can play an important role in solving various issues, including emotional analysis in combination with other parameters. In this study, three novel topic models called date sentiment LDA (DSLDA), author-date sentiment LDA (ADSLDA), and pack-author-date sentiment LDA (PADSLDA) are proposed. The proposed models extend LDA through some extra parameters such as date, author, helpfulness, sentiment, and subtopic. The proposed models use helpfulness in the Gibbs sampling algorithm. Helpfulness is a part of readers who found the review helpful. The proposed models divide the words into two categories: the words more affected by the distribution of subtopic and the words more affected by the main topic. In this study, a new concept called pack is introduced, and a new model called PADSLDA is proposed for sentiment analysis at pack level. The proposed models outperformed the baseline models because according to evaluations results, the extra parameters can appropriately affect the generating process of words in a review. Sentiment analysis at the document level, perplexity, and topic coherence are the main parameters used in the evaluations.																	0266-4720	1468-0394				AUG	2020	37	4			SI					10.1111/exsy.12527		JAN 2020											
J								On the design of hybrid bio-inspired meta-heuristics for complex multiattribute vehicle routing problems	EXPERT SYSTEMS										ant colony optimization; genetic algorithm; hybrid meta-heuristic; memetic algorithm; vehicle routing problem	ANT COLONY OPTIMIZATION; PARTICLE SWARM OPTIMIZATION; TRAVELING SALESMAN PROBLEM; SEARCH ALGORITHM; BAT ALGORITHM; LOCAL SEARCH; DISCRETE; FLEET	This paper addresses a multiattribute vehicle routing problem, the rich vehicle routing problem, with time constraints, heterogeneous fleet, multiple depots, multiple routes, and incompatibilities of goods. Four different approaches are presented and applied to 15 real datasets. They are based on two meta-heuristics, ant colony optimization (ACO) and genetic algorithm (GA), that are applied in their standard formulation and combined as hybrid meta-heuristics to solve the problem. As such ACO-GA is a hybrid meta-heuristic using ACO as main approach and GA as local search. GA-ACO is a memetic algorithm using GA as main approach and ACO as local search. The results regarding quality and computation time are compared with two commercial tools currently used to solve the problem. Considering the number of customers served, one of the tools and the ACO-GA approach outperforms the others. Considering the cost, ACO, GA, and GA-ACO provide better results. Regarding computation time, GA and GA-ACO have been found the most competitive among the benchmark.																	0266-4720	1468-0394														e12528	10.1111/exsy.12528		JAN 2020											
J								Intelligent document-filling system on mobile devices by document classification and electronization	COMPUTATIONAL INTELLIGENCE										decision tree; document electronization; feature extraction; hypothesis tree; rule-based search		Both of an automatic classification method for original documents based on image feature and a layout analysis method based on rule hypothesis tree are proposed. Then an intelligent document-filling system by electronizing the original documents, which can be applied to cellphones and pads is designed. When users are filling documents online, information can be automatically input to the financial information system merely by taking photos of the original documents. By this means can not only save time but also ensure the accuracy between the data online and that on the original documents. Experiments show that the accuracy of document classification is 88.38%, the accuracy of document-filling is 87.22%, and it takes 5.042 seconds dealing with per document. The system can be applied to financial, government, libraries, electric power, enterprises and many other industries, which has high economic and application value.																	0824-7935	1467-8640															10.1111/coin.12279		JAN 2020											
J								Communicative bottlenecks lead to maximal information transfer	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Signalling games; information transfer; informational bottlenecks; reinforcement learning; emergent communication	EVOLUTION; ROBUSTNESS; PREDICTION; GAMES; LAW	This paper presents new analytic and numerical analysis of signalling games that give rise to informational bottlenecks - that is to say, signalling games with more state/act pairs than available signals to communicate information about the world. I show via simulation that agents learning to coordinate tend to favour partitions of nature which provide maximal information transfer. This is true despite the fact that nothing from an initial analysis of the stability properties of the underlying signalling game suggests that this should be the case. As a first pass to explain this, I note that the underlying structure of our model favours maximal information transfer in regard to the simple combinatorial properties of how the agents might partition nature into kinds. However, I suggest that this does not perfectly capture the empirical results; thus, several open questions remain.																	0952-813X	1362-3079															10.1080/0952813X.2020.1716857		JAN 2020											
J								Automatic alert generation in a surveillance systems for smart city environment using deep learning algorithm	EVOLUTIONARY INTELLIGENCE										Intelligent video surveillance; Smart cities; Early detection; Surveillance analytics; Surveillance video processing; Convolution neural networks	RECOGNITION; PERFORMANCE; FUTURE; MOTION	In Smart cities surveillance is an extremely important feature required for ensuring the safety of citizens and also for deterring the crime. Hence, intelligent video surveillance (IVS) frameworks are by and large increasingly more famous in security applications. The investigation and acknowledgment of anomalous practices in a video succession has step by step attracted the consideration in the field of IVS, as it permits sifting through an enormous number of pointless data, which ensures the high productivity in the security assurance, and spare a great deal of human and material assets. Techniques are proposed in the literature for analyzing the IVS systems. Existing systems for video analysis, suffer with some limitations. The one of the major limitation is lack of real time response from the surveillance systems. In order to overcome this limitation, an IVS system design is proposed using convolution neural networks. In case of emergency like fire, thieves' attacks, Intrusion Detector, the proposed system sends an alert for the corresponding services automatically. Experimentation has done on the number of datasets available for video surveillance testing. The results show that the proposed surveillance system achieves very low false alarm rates.																	1864-5909	1864-5917															10.1007/s12065-020-00353-4		JAN 2020											
J								On solving single elevator-like problems using a learning automata-based paradigm	EVOLVING SYSTEMS										Learning automata (LA); Learning systems; Single elevator problem (SEP); Elevator-like problems (ELPs); Parking problem	OPTIMIZATION; FORMULATION; OPERATION; SYSTEMS	This paper concentrates on a host of problems with characteristics similar to those that are related to moving elevators within a building. These are referred to as Elevator-like problems (ELPs), and their common phenomena will be expanded on in the body of the paper. We shall resolve ELPs using a subfield of AI, namely the field of learning automata (LA). Rather than working with the well-established mathematical formulations of the field, our intention is to use these tools to tackle ELPs, and in particular, those that deal with single "elevators" moving between "floors". ELPs have not been tackled before using AI. In a simplified domain, the ELP involves the problem of optimizing the scheduling of elevators. In particular, we are concerned with determining the elevators' optimal "parking" location. In our case, the objective is to find the optimal parking floors for the single elevator scenario, so as to minimize the passengers' average waiting time (AWT). Apart from proposing benchmark solutions, we have provided two different novel LA-based solutions for the single-elevator scenario as the multi-elevator setting is more complicated. The first solution is based on the well-known LRIscheme, and the second solution incorporates the Pursuit concept to improve the performance and the convergence speed of the first solution, leading to the PLRIscheme. The simulation results presented demonstrate that our solutions performed better than those used in modern-day elevators, and provided results that are near-optimal, yielding a performance increase of up to 90%. The solutions presented for real elevators are directly applicable for the entire family of ELPs.																	1868-6478	1868-6486															10.1007/s12530-020-09325-6		JAN 2020											
J								Improve the translational distance models for knowledge graph embedding	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Knowledge graph embedding; Translational distance model; Positional encoding; Self-attention		Knowledge graph embedding techniques can be roughly divided into two mainstream, translational distance models and semantic matching models. Though intuitive, translational distance models fail to deal with the circle structure and hierarchical structure in knowledge graphs. In this paper, we propose a general learning framework named TransX-pa, which takes various models (TransE, TransR, TransH and TransD) into consideration. From this unified viewpoint, we analyse the learning bottlenecks are: (i) the common assumption that the inverse of a relation r is modelled as its opposite - r; and (ii) the failure to capture the rich interactions between entities and relations. Correspondingly, we introduce position-aware embeddings and self-attention blocks, and show that they can be adapted to various translational distance models. Experiments are conducted on different datasets extracted from real-world knowledge graphs Freebase and WordNet in the tasks of both triplet classification and link prediction. The results show that our approach makes a great improvement, showing a better, or comparable, performance with state-of-the-art methods.																	0925-9902	1573-7675				DEC	2020	55	3					445	467		10.1007/s10844-019-00592-7		JAN 2020											
J								Fast and memory-efficient algorithms for high-order Tucker decomposition	KNOWLEDGE AND INFORMATION SYSTEMS										Tensor; High-order tensor; Tensor decomposition; Tucker decomposition; Out-of-core algorithm		Multi-aspect data appear frequently in web-related applications. For example, product reviews are quadruplets of the form (user, product, keyword, timestamp), and search-engine logs are quadruplets of the form (user, keyword, location, timestamp). How can we analyze such web-scale multi-aspect data on an off-the-shelf workstation with a limited amount of memory? Tucker decomposition has been used widely for discovering patterns in such multi-aspect data, which are naturally expressed as large but sparse tensors. However, existing Tucker decomposition algorithms have limited scalability, failing to decompose large-scale high-order (= 4) tensors, since they explicitly materialize intermediate data, whose size grows exponentially with the order. To address this problem, which we call "Materialization Bottleneck," we propose S- HOT, a scalable algorithm for high-order Tucker decomposition. S- HOT minimizes materialized intermediate data by using an on-the-fly computation, and it is optimized for disk-resident tensors that are too large to fit in memory. We theoretically analyze the amount of memory and the number of data scans required by S- HOT. Moreover, we empirically showthat S- HOT handles tensors with higher order, dimensionality, and rank than baselines. For example, S- HOT successfully decomposes a real-world tensor from the Microsoft Academic Graph on an off-the-shelf workstation, while all baselines fail. Especially, in terms of dimensionality, S- HOT decomposes 1000xlarger tensors than baselines.																	0219-1377	0219-3116				JUL	2020	62	7					2765	2794		10.1007/s10115-019-01435-1		JAN 2020											
J								Hybrid ant colony optimization algorithm applied to the multi-depot vehicle routing problem	NATURAL COMPUTING										Ant colony optimization; Local optimization process; Multi-depot vehicle routing problem; Hybrid algorithms; Bio-inspired algorithms	NEIGHBORHOOD SEARCH ALGORITHM; TIME WINDOWS; DELIVERY; SYSTEM; NUMBER	The article deals with the hybrid Ant Colony Optimization algorithm and its application to the Multi-Depot Vehicle Routing Problem (MDVRP). The algorithm combines both probabilistic and exact techniques. The former implements the bio-inspired approach based on the behaviour of ants in the nature when searching for food together with simulated annealing principles. The latter complements the former. The algorithm explores the search space in a finite number of iterations. In each iteration, the deterministic local optimization process may be used to improve the current solution. Firstly, the key parts and features of the algorithm are presented, especially in connection with the exact optimization process. Next, the article deals with the results of experiments on MDVRP problems conducted to verify the quality of the algorithm; moreover, these results are compared to other state-of-the-art methods. As experiments, Cordreau's benchmark instances were used. The experiments showed that the proposed algorithm overcomes the other methods as it has the smallest average error (the difference between the found solution and the best known solution) on the entire set of benchmark instances.																	1567-7818	1572-9796				JUN	2020	19	2			SI		463	475		10.1007/s11047-020-09783-6		JAN 2020											
J								Robustly detect different types of text in videos	NEURAL COMPUTING & APPLICATIONS										Video text detector; Temporal consistency; Spatial location; Component representation	UNIFIED FRAMEWORK; TRACKING; RECOGNITION; COMPETITION	Text in videos can be categorized into three types: overlaid text, layered text, and scene text. Existing detection methods focus on a specific type of text and cannot obtain a good performance when working on other text types. To our knowledge, few works explore to build a system to simultaneously detect all types of text. In this paper, we present a unified video text detector, which can simultaneously localize all types of text in videos accurately. Our system consists of a spatial text detector and a temporal fusion filter. First, we explore to use three different strategies to learn the spatial text detector based on deep convolutional neural networks, so that it can simultaneously detect various texts without knowing the types of text. Then, a new area-first non-maximum suppression computation combined with multiple constraints is proposed to remove the redundant bounding boxes. Finally, the temporal fusion filter exploits the features of spatial locations and text components to integrate the detection results of consecutive frames to further remove false positives. To validate the proposed approach, comprehensive experiments are carried out on three publicly available datasets, consisting of overlaid text, layered text, and scene text. The experimental results demonstrate that our method consistently achieves the best performance compared with state-of-the-art methods.																	0941-0643	1433-3058				AUG	2020	32	16					12827	12840		10.1007/s00521-020-04729-6		JAN 2020											
J								Hourly day-ahead wind power forecasting with the EEMD-CSO-LSTM-EFG deep learning technique	SOFT COMPUTING										Wind power forecasting (WPF); Deep learning; Long short-term memory network (LSTM); Ensemble empirical mode decomposition (EEMD); Cuckoo search algorithm (CSO); Forecasting accuracy	EMPIRICAL MODE DECOMPOSITION; SPEED; ENSEMBLE; TIME; PREDICTION; HYBRID	Wind power forecasting has gained significant attention due to advances in wind energy generation in power frameworks and the uncertain nature of wind. In this manner, to maintain an affordable, reliable, economical, and dependable power supply, accurately predicting wind power is important. In recent years, several investigations and studies have been conducted in this field. Unfortunately, these examinations disregarded the significance of data preprocessing and the impact of various missing values, thereby resulting in poor performance in forecasting. However, long short-term memory (LSTM) network, a kind of recurrent neural network (RNN), can predict and process the time-series data at moderately long intervals and time delays, thereby producing good forecasting results using time-series data. This article recommends a hybrid forecasting model for forecasting wind power to improve the performance of the prediction. An improved long short-term memory network-enhanced forget-gate network (LSTM-EFG) model, whose appropriate parameters are optimized using cuckoo search optimization algorithm (CSO), is used to forecast the subseries data that is extracted using ensemble empirical mode decomposition (EEMD). The experimental results show that the proposed forecasting model overcomes the limitations of traditional forecasting models and efficiently improves forecasting accuracy. Furthermore, it serves as an operational tool for wind power plants management.																	1432-7643	1433-7479				AUG	2020	24	16					12391	12411		10.1007/s00500-020-04680-7		JAN 2020											
J								Multimodal biometric systems based on different fusion levels of ECG and fingerprint using different classifiers	SOFT COMPUTING										Multimodal biometrics; ECG recognition system; Fingerprint recognition system; Features extraction; Classification algorithms	COMBINING MULTIPLE CLASSIFIERS; RECOGNITION; ELECTROCARDIOGRAM; CLASSIFICATION; METHODOLOGY; COMBINATION; ALGORITHM; FRAMEWORK	Multimodal biometric system can be accomplished at different levels of fusion and achieve higher recognition performance than the unimodal system. This paper concerned to study the performance of different classification techniques and fusion rules in the context of unimodal and multimodal biometric systems based on the electrocardiogram (ECG) and fingerprint. The experiments are conducted on ECG and fingerprint databases to evaluate the performance of the proposed biometric systems. MIT-BIH database is utilized for ECG, FVC2004 database is utilized for the fingerprint, and further experiments are being performed to evaluate the proposed multimodal system with 47 subjects from virtual multimodal database. The performance of the proposed unimodal and multimodal biometric systems is measured using receiver operating characteristic (ROC) curve, AUC (area under the ROC curve), sensitivity, specificity, efficiency, standard error of the mean, and likelihood ratio. The findings indicate AUC up to 0.985 for sequential multimodal system, and up to 0.956 for parallel multimodal system, as compared to the unimodal systems that achieved AUC up to 0.951, and 0.866, for the ECG and fingerprint biometrics, respectively. The overall performance of the proposed multimodal systems is better than that of the unimodal systems based on different classifiers and different fusion levels and rules.																	1432-7643	1433-7479				AUG	2020	24	16					12599	12632		10.1007/s00500-020-04700-6		JAN 2020											
J								Estimating the distribution of enterprise values with quantile neural networks	SOFT COMPUTING										Quantile regression analysis; Neural networks; Enterprise value; Distribution	CASH FLOW; VALUATION; REGRESSION; DIVIDEND; GROWTH	The probability density function of enterprise values may be more precise and useful in the cases of corporate investment, financing, or transactions. Although the quantile regression analysis can generate a set of models for a series of quantiles, it cannot generate the probability density function of the dependent variable. Therefore, this paper proposes a novel method of employing prediction results of the quantile neural networks to build probability density functions with which we can effectively assess enterprise values. Empirical evidence reveals that the estimated cumulative lognormal distribution curves of the price-to-book value ratio (PBR) and the data are well matched. In addition, the corporate market value is equal to the PBR multiplied by the corporate stockholders equity. Thus, the corporate market value is also a lognormal distribution. PBR distributions of building and construction industries are more tilted to the left, implying that enterprise values of building and construction industries are lower than those of other industries with the same stockholders equity and return on equity.																	1432-7643	1433-7479				SEP	2020	24	17					13085	13097		10.1007/s00500-020-04726-w		JAN 2020											
J								Beyond the topics: how deep learning can improve the discriminability of probabilistic topic modelling	PEERJ COMPUTER SCIENCE										Topic modelling; Stacked denoising autoencoders; Text classification; Sentiment analysis	SENTIMENT CLASSIFICATION; ASSOCIATION	The article presents a discriminative approach to complement the unsupervised probabilistic nature of topic modelling. The framework transforms the probabilities of the topics per document into class-dependent deep learning models that extract highly discriminatory features suitable for classification. The framework is then used for sentiment analysis with minimum feature engineering. The approach transforms the sentiment analysis problem from the word/document domain to the topics domain making it more robust to noise and incorporating complex contextual information that are not represented otherwise. A stacked denoising autoencoder (SDA) is then used to model the complex relationship among the topics per sentiment with minimum assumptions. To achieve this, a distinct topic model and SDA per sentiment polarity is built with an additional decision layer for classification. The framework is tested on a comprehensive collection of benchmark datasets that vary in sample size, class bias and classification task. A significant improvement to the state of the art is achieved without the need for a sentiment lexica or over-engineered features. A further analysis is carried out to explain the observed improvement in accuracy.																	2376-5992					JAN 27	2020									e252	10.7717/peerj-cs.252													
J								Composition of cometary particles collected during two periods of the Rosetta mission: multivariate evaluation of mass spectral data	JOURNAL OF CHEMOMETRICS										comet 67P; Churyumov-Gerasimenko; KNN classification; random forest classification; time-of-flight secondary ion mass spectrometry; variable importance	DUST; COSIMA; SPECTROMETER; COMA	The instrument COSIMA (COmetary Secondary Ion Mass Analyzer) onboard of the European Space Agency mission Rosetta collected and analyzed dust particles in the neighborhood of comet 67P/Churyumov-Gerasimenko. The chemical composition of the particle surfaces was characterized by time-of-flight secondary ion mass spectrometry. A set of 2213 spectra has been selected, and relative abundances for CH-containing positive ions as well as positive elemental ions define a set of multivariate data with nine variables. Evaluation by complementary chemometric techniques shows different compositions of sample groups collected during two periods of the mission. The first period was August to November 2014 (far from the Sun); the second period was January 2015 to February 2016 (nearer to the Sun). The applied data evaluation methods consider the compositional nature of the mass spectral data and comprise robust principal component analysis as well as classification with discriminant partial least squares regression, k-nearest neighbor search, and random forest decision trees. The results indicate a high importance of the relative abundances of the secondary ions C+ and Fe+ for the group separation and demonstrate an enhanced content of carbon-containing substances in samples collected in the period with smaller distances to the Sun.																	0886-9383	1099-128X														e3218	10.1002/cem.3218		JAN 2020											
J								O-MedAL: Online active deep learning for medical image analysis	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										active learning; deep learning; medical image analysis; online learning		Active learning (AL) methods create an optimized labeled training set from unlabeled data. We introduce a novel online active deep learning method for medical image analysis. We extend our MedAL AL framework to present new results in this paper. A novel sampling method queries the unlabeled examples that maximize the average distance to all training set examples. Our online method enhances performance of its underlying baseline deep network. These novelties contribute to significant performance improvements, including improving the model's underlying deep network accuracy by 6.30%, using only 25% of the labeled dataset to achieve baseline accuracy, reducing backpropagated images during training by as much as 67%, and demonstrating robustness to class imbalance in binary and multiclass tasks. This article is categorized under: Technologies > Machine Learning Technologies > Classification Application Areas > Health Care																	1942-4787	1942-4795				JUL	2020	10	4							e1350	10.1002/widm.1353		JAN 2020											
J								A parameter-free hybrid instance selection algorithm based on local sets with natural neighbors	APPLIED INTELLIGENCE										Instance selection; K nearest neighbor; Local sets; Natural neighbors; Parameter-free	NEAREST-NEIGHBOR; CLUSTERING-ALGORITHM; REDUCTION ALGORITHM; SIZE	Instance selection aims to search for the best patterns in the training set and main instance selection methods include condensation methods, edition methods and hybrid methods. Hybrid methods combine advantages of both edition methods and condensation methods. Nevertheless, most of existing hybrid approaches heavily rely on parameters and are relatively time-consuming, resulting in the performance instability and application difficulty. Though several relatively fast and (or) parameter-free hybrid methods are proposed, they still have the difficulty in achieving both high accuracy and high reduction. In order to solve these problems, we present a new parameter-free hybrid instance selection algorithm based on local sets with natural neighbors (LSNaNIS). A new parameter-free definition for the local set is first proposed based on the fast search for natural neighbors. The new local set can fast and reasonably describe local characteristics of data. In LSNaNIS, we use the new local set to design an edition method (LSEdit) to remove harmful samples, a border method (LSBorder) to retain representative border samples and a core method (LSCore) to condense internal samples. Comparison experiments show that LSNaNIS is relatively fast and outperforms existing hybrid methods in improving the k-nearest neighbor in terms of both accuracy and reduction.																	0924-669X	1573-7497				MAY	2020	50	5					1527	1541		10.1007/s10489-019-01598-y		JAN 2020											
J								A hybrid multi-objective tour route optimization algorithm based on particle swarm optimization and artificial bee colony optimization	COMPUTATIONAL INTELLIGENCE										artificial bee colony optimization; multi-objective optimization; particle swarm optimization; route optimization; weighted sum	GENETIC ALGORITHM	Computational intelligence techniques have widespread applications in the field of engineering process optimization, which typically comprises of multiple conflicting objectives. An efficient hybrid algorithm for solving multi-objective optimization, based on particle swarm optimization (PSO) and artificial bee colony optimization (ABCO) has been proposed in this paper. The novelty of this algorithm lies in allocating random initial solutions to the scout bees in the ABCO phase which are subsequently optimized in the PSO phase with respect to the velocity vector. The last phase involves loyalty decision-making for the uncommitted bees based on the waggle dance phase of ABCO. This procedure continues for multiple generations yielding optimum results. The algorithm is applied to a real life problem of intercity route optimization comprising of conflicting objectives like minimization of travel cost, maximization of the number of tourist spots visited and minimization of the deviation from desired tour duration. Solutions have been obtained using both pareto optimality and the classical weighted sum technique. The proposed algorithm, when compared analytically and graphically with the existing ABCO algorithm, has displayed consistently better performance for fitness values as well as for standard benchmark functions and performance metrics for convergence and coverage.																	0824-7935	1467-8640				AUG	2020	36	3					884	909		10.1111/coin.12276		JAN 2020											
J								Hybrid dynamic learning mechanism for multivariate time series segmentation	STATISTICAL ANALYSIS AND DATA MINING										dynamic programming; factor analysis; incremental clustering; multivariate time series segmentation	MULTIPLE CHANGE-POINT; MODEL	To improve the efficiency of segmentation methods for multivariate time series, a hybrid dynamic learning mechanism for such series' segmentation is proposed. First, an incremental clustering algorithm is used to automatically cluster variables of multivariate time series. Second, common factors are extracted from every cluster by a dynamic factor model as an ensemble description of the system. Third, this common factor series is segmented by dynamic programming. The proposed method can potentially segment multivariate time series and not only performs segmentation better on multivariate time series with a large number of variables but also improves the running accuracy and efficiency of the algorithm, especially when analyzing complex datasets.																	1932-1864	1932-1872				APR	2020	13	2					165	177		10.1002/sam.11448		JAN 2020											
J								A dissimilarity measure for mixed nominal and ordinal attribute data in k-Modes algorithm	APPLIED INTELLIGENCE										Clustering algorithm; Mixed-type data; k-Modes algorithm; Dissimilarity measure of ordinal attribute	SIMILARITY	Among the existing clustering algorithms, the k-Means algorithm is one of the most commonly used clustering methods. As an extension of the k-Means algorithm, the k-Modes algorithm has been widely applied to categorical data clustering by replacing means with modes. However, there are more mixed-type data containing categorical, ordinal and numerical attributes. Mixed-type data clustering problem has recently attracted much attention from the data mining research community, but most of them fail to notice the ordinal attributes and establish explicit metric similarity of ordinal attributes. In this paper, the limitations of some existing dissimilarity measure of k-Modes algorithm in mixed ordinal and nominal data are analyzed by using some illustrative examples. Based on the idea of mining ordinal information of ordinal attribute, a new dissimilarity measure for the k-Modes algorithm to cluster this type of data is proposed. The distinct characteristic of the new dissimilarity measure is to take account of the ordinal information of ordinal attribute. A convergence study and time complexity of the k-Modes algorithm based on this new dissimilarity measure indicates that it can be effectively used for large data sets. The results of comparative experiments on nine real data sets from UCI show the effectiveness of the new dissimilarity measure.																	0924-669X	1573-7497				MAY	2020	50	5					1498	1509		10.1007/s10489-019-01583-5		JAN 2020											
J								Surgeon-Centered Analysis of Robot-Assisted Needle Driving Under Different Force Feedback Conditions	FRONTIERS IN NEUROROBOTICS										teleoperation; force feedback; needle driving; robot assisted minimally invasive surgery; learning	MINIMALLY INVASIVE SURGERY; HAPTIC FEEDBACK; MOVEMENT; VARIABILITY; PERFORMANCE; TELEOPERATION; TRANSPARENCY; SMOOTHNESS; DYNAMICS; NETWORK	Robotic assisted minimally invasive surgery (RAMIS) systems present many advantages to the surgeon and patient over open and standard laparoscopic surgery. However, haptic feedback, which is crucial for the success of many surgical procedures, is still an open challenge in RAMIS. Understanding the way that haptic feedback affects performance and learning can be useful in the development of haptic feedback algorithms and teleoperation control systems. In this study, we examined the performance and learning of inexperienced participants under different haptic feedback conditions in a task of surgical needle driving via a soft homogeneous deformable object-an artificial tissue. We designed an experimental setup to characterize their movement trajectories and the forces that they applied on the artificial tissue. Participants first performed the task in an open condition, with a standard surgical needle holder, followed by teleoperation in one of three feedback conditions: (1) no haptic feedback, (2) haptic feedback based on position exchange, and (3) haptic feedback based on direct recording from a force sensor, and then again with the open needle holder. To quantify the effect of different force feedback conditions on the quality of needle driving, we developed novel metrics that assess the kinematics of needle driving and the tissue interaction forces, and we combined our novel metrics with classical metrics. We analyzed the final teleoperated performance in each condition, the improvement during teleoperation, and the aftereffect of teleoperation on the performance when using the open needle driver. We found that there is no significant difference in the final performance and in the aftereffect between the 3 conditions. Only the two conditions with force feedback presented statistically significant improvement during teleoperation in several of the metrics, but when we compared directly between the improvements in the three different feedback conditions none of the effects reached statistical significance. We discuss possible explanations for the relative similarity in performance. We conclude that we developed several new metrics for the quality of surgical needle driving, but even with these detailed metrics, the advantage of state of the art force feedback methods to tasks that require interaction with homogeneous soft tissue is questionable.																	1662-5218					JAN 24	2020	13								108	10.3389/fnbot.2019.00108													
J								Semi-Autonomous Robotic Arm Reaching With Hybrid Gaze-Brain Machine Interface	FRONTIERS IN NEUROROBOTICS										brain-machine interface; gaze tracking; human-robot interface; continuous shared control; robotic arm reaching	COMPUTER-INTERFACE; SHARED CONTROL; EYE TRACKING; TELEOPERATION; AUTONOMY; VISION; DRIVEN; EEG	Recent developments in the non-muscular human-robot interface (HRI) and shared control strategies have shown potential for controlling the assistive robotic arm by people with no residual movement or muscular activity in upper limbs. However, most non-muscular HRIs only produce discrete-valued commands, resulting in non-intuitive and less effective control of the dexterous assistive robotic arm. Furthermore, the user commands and the robot autonomy commands usually switch in the shared control strategies of such applications. This characteristic has been found to yield a reduced sense of agency as well as frustration for the user according to previous user studies. In this study, we firstly propose an intuitive and easy-to-learn-and-use hybrid HRI by combing the Brain-machine interface (BMI) and the gaze-tracking interface. For the proposed hybrid gaze-BMI, the continuous modulation of the movement speed via the motor intention occurs seamlessly and simultaneously to the unconstrained movement direction control with the gaze signals. We then propose a shared control paradigm that always combines user input and the autonomy with the dynamic combination regulation. The proposed hybrid gaze-BMI and shared control paradigm were validated for a robotic arm reaching task performed with healthy subjects. All the users were able to employ the hybrid gaze-BMI for moving the end-effector sequentially to reach the target across the horizontal plane while also avoiding collisions with obstacles. The shared control paradigm maintained as much volitional control as possible, while providing the assistance for the most difficult parts of the task. The presented semi-autonomous robotic system yielded continuous, smooth, and collision-free motion trajectories for the end effector approaching the target. Compared to a system without assistances from robot autonomy, it significantly reduces the rate of failure as well as the time and effort spent by the user to complete the tasks.																	1662-5218					JAN 24	2020	13								111	10.3389/fnbot.2019.00111													
J								Optimizing segmentation granularity for neural machine translation	MACHINE TRANSLATION										Neural machine translation; Subword units; Byte-pair encoding; Online optimization; Segmentation		In neural machine translation (NMT), it has become standard to translate using subword units to allow for an open vocabulary and improve accuracy on infrequent words. Byte-pair encoding (BPE) and its variants are the predominant approach to generating these subwords, as they are unsupervised, resource-free, and empirically effective. However, the granularity of these subword units is a hyperparameter to be tuned for each language and task, using methods such as grid search. Tuning may be done inexhaustively or skipped entirely due to resource constraints, leading to sub-optimal performance. In this paper, we propose a method to automatically tune this parameter using only one training pass. We incrementally introduce new BPE vocabulary online based on the held-out validation loss, beginning with smaller, general subwords and adding larger, more specific units over the course of training. Our method matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time. We also show benefits in training efficiency and performance improvements for rare words due to the way embeddings for larger units are incrementally constructed by combining those from smaller units.																	0922-6567	1573-0573				APR	2020	34	1					41	59		10.1007/s10590-019-09243-8		JAN 2020											
J								Storing and querying fuzzy RDF(S) in HBase databases	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										fuzzy HBase databases; fuzzy RDF(S); fuzzy sets; querying; storing	OBJECT-ORIENTED DATABASES; RELATIONAL DATABASE; GRAPH; INFORMATION; FRAMEWORK	The Resource Description Framework (RDF) and RDF Schema (RDFS) recommended by World Wide Web Consortium (W3C) have been used for meta-data management on the Web. With the rapid development of Web-based applications, the growth of RDF(S) data is increasing dramatically. In the context of open Web, uncertainty naturally arises in RDF and RDFS (RDF(S) in short). To manage large-scale fuzzy RDF(S) data efficiently and effectively, we propose, in this paper, a fuzzy RDF(S) storage schema with fuzzy HBase databases (FHDBs). On the basis, we investigate how to query in FHDBs. We propose a set of query algorithms. We implement a prototype system to demonstrate the feasibility of our approach.																	0884-8173	1098-111X				APR	2020	35	4					751	780		10.1002/int.22224		JAN 2020											
J								Informing the design of a LoRa (TM)-based digital bracelet for the timely assistance of indigents	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Indigents; Wearable devices; Dynamic coverage; LPWAN		We conducted a qualitative study to get a preliminary understanding of indigents needs. The results helped us identify system requirements that led to the design of the HELPi system, a digital bracelet to enable local agencies to provide support to indigents who used roam the city. The paper aims to present the studies conducted to inform the design of our technological approach to addressing challenges related to acceptance, coverage of users, and battery lifespan. We built a prototype using LoRa (TM), which is a Low Power Wide Area Network (LPWAN) technology that offers several features that make it suitable for our application domain; e.g., it enables Machine-to-Machine (M2M) communications. Thus, we have taken advantage of this feature and propose a communication protocol to allow a dynamic coverage of users. We conducted assessments to determine users' perception about its usefulness, and to measure the performance of the LoRa's signal propagation and the battery energy consumption for the deployment of the HELPi system. According to our results, the proposed communication protocol should work with a LoRa (TM) Gateway in the outside setting condition and with an SNR threshold higher than - 2 dB to guarantee the successful reception of messages transmitted.																	1868-5137	1868-5145															10.1007/s12652-020-01732-6		JAN 2020											
J								Identification and role of opinion leaders in information diffusion for online discussion network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social networks; Complex networks; Opinion leaders; Centrality; Twitter	CENTRALITY	Social networks are playing a vibrant role in spreading information among its users. With the ongoing boom in technology, social networks are becoming very popular nowadays and are made up of a multitude of users. Some of these users may have a strong influence on the other network users depending on their uncommon elevated values of betweenness centrality (BC). In the online discussion network such as Twitter, the extremely important users are called Opinion Leaders, who play a vital role in the spread of information in an efficient and fast way and keep the isolates interested in the online discussion network. One of the most significant problems in the associated sector is the identification of opinion leaders. In this paper, opinion leaders are chosen based on various centrality measures. The central users are identified based on their in-degree and out-degree links and are ranked within the network by their BC values. Furthermore, we analyze community evolution by using the standard Louvain algorithm. The experiment is performed on publicly available Higgs Boson data from Twitter. The conversation starter and influencer have been observed as an opinion leader for each network. These users have been observed to play a crucial part in the dissemination of information in an online discussion network.																	1868-5137	1868-5145															10.1007/s12652-019-01623-5		JAN 2020											
J								Enhanced histogram equalization based nodule enhancement and neural network based detection for chest x-ray radiographs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computer-aided detection; X-ray radiograph; Lung nodule detection; Solitary-feature; Neural network; Continuous wavelet transform; Enhanced histogram equalization	LUNG-CANCER MORTALITY	Lung cancer is the malignant tumors, which maximizes the speed of incidence and death rate and it will in the shape of spherical nodules in a conventional radiograph. Moreover, few lung nodules can't be identified, since it overlaps with the normal anatomic structures like ribs and clavicles. Earlier, Rib suppression is executed to rectify this issue, which works according to the Linear Discriminant Analysis (LDA) and it is brought-into maximize the visibility of lung nodules. With this motivation, initially, it is constructed through LDA; then, the pixel intensity of lung nodule is measure through the usage of the Enhanced Histogram Equalization and it is included to the subtracted image of original image and rib model to improve the original brightness; at last, the border of ribs is recognized and smoothed, is done through mean function is measure through the use of pixel intensity. The white nodule-likeness extraction is the second step. Texture feature extraction is done for Continuous Wavelet Transform and the multi-scale Convergence-index filter. In order to create the white nodule-likeness map (WNLM), Neural Network classifier is utilized. Candidate extraction on WNLM using Laplace of Gaussian blob detection method is the third step. To estimate the rib suppression method, the JSRT database and dual-energy images were employed. The experiment represents when the method is enforced on the test images, the ribs can be suppressed significantly.																	1868-5137	1868-5145															10.1007/s12652-020-01701-z		JAN 2020											
J								A knowledge-based system for quality analysis in model-based design	JOURNAL OF INTELLIGENT MANUFACTURING										Model quality analysis; Model definition unit; Knowledge representation; Object chain; Parameter table	PRODUCT DATA QUALITY; CAD; LIFE	The quality of model-based definition (MBD) models is one of the significant challenges to achieving model-based enterprise (MBE). Different stakeholders in MBE put forward different quality requirements for MBD models, while any quality defect may hinder the production. Therefore, it is essential to analyze the quality of MBD models adequately before the usage. However, existing quality tools are difficult to apply in MBE. A knowledge-based quality analysis system for MBD part models is presented in this paper. The system utilizes model quality knowledge to analyze model quality from the perspective of various stakeholders in MBE. And it can be extended by adding new MBD part model quality knowledge. In order to judge model quality, a concept of the model definition unit is introduced for converting model information into the description towards model semantics quality, and a quality analysis method based on this description is proposed. For the ease of model quality knowledge representation and maintenance, a knowledge representation framework based on the object chain and parameter table is developed. Finally, a prototype system is implemented to verify the effectiveness and practicality. With 46 knowledge items derived from eight model-used stages, in total 24 potential quality defects are found in two case models. And additional two defects are discovered by extending the system with new model quality knowledge. In this way, the model quality is improved from the perspective of corresponding users.																	0956-5515	1572-8145				AUG	2020	31	6					1579	1606		10.1007/s10845-020-01535-8		JAN 2020											
J								A Smooth and Safe Path Planning for an Active Lower Limb Exoskeleton	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Path planning; Probabilistic foam; Path smoothing; Bubble of free space; Active exoskeleton	VORONOI DIAGRAM; RRT; AVOIDANCE; VEHICLE	The Probabilistic Foam method (PFM) is a path planner that ensures a volumetric region for safe maneuverability called bubble. This method generates paths bounded by a set of overlapped bubbles, called rosary. In this paper, we present an approach to obtain safe and smooth collision-free paths from PFM for a lower limb active exoskeleton, which is based on two main processes: The rosary adjustment and the path smoothing. The first process keeps the size of the bubbles more regular, while the second process guarantees a smooth and short path, satisfying the safe constraints imposed by the rosary. To evaluate these proposed approaches, we presented a simulation of the exoskeleton leg performing the swing phase movement in three different scenarios: overcoming an obstacle, walking up and down a step. The resulting planned paths were evaluated and compared, considering the path length and the path smoothness.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		535	553		10.1007/s10846-019-01134-7		JAN 2020											
J								Application of improved adaptive Kalman filter in China's interest rate market	NEURAL COMPUTING & APPLICATIONS										Improved adaptive Kalman filter; No-arbitrage NS model; Exponential decay factor	TERM STRUCTURE; ARBITRAGE	With the increasing number of data in China's interest rate market, the model is increasingly complex. For most interest rate models at this stage, its parameter estimation has become the focus of many scholars in recent years. At present, for the parameter estimation of the interest rate model, there is often a problem that the parameter estimation accuracy is not high and the algorithm stability is poor. Therefore, this research work investigates the parameter estimation method of the no-arbitrage Nelson-Siegel model for the classical interest rate model and points out the shortcomings of the estimation method. In the iterative process, the mean error and the estimated error covariance will expand continuously. The phenomenon eventually leads to the consequences of poor estimation. In this study, an improved adaptive Kalman filtering method is proposed. In the process of algorithm updating, an exponential decay factor is added to improve the accuracy of parameter estimation and the stability of the algorithm. Finally, based on China's national debt data, the simulation experiment is carried out.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5315	5327		10.1007/s00521-020-04706-z		JAN 2020											
J								Image retrieval based on gradient-structures histogram	NEURAL COMPUTING & APPLICATIONS										Image retrieval; Edge detection; Orientation selection; Gradient-structures histogram	INVARIANT; SCALE; DESCRIPTOR; PATTERNS; FEATURES	Color perception and orientation selection are very important mechanisms of the human brain that have close relationships with feature extraction and representation. However, extracting low-level features by mimicking these mechanisms remains challenging. To address this problem, we present the gradient-structures histogram as a novel method of content-based image retrieval (CBIR). Its main highlights are: (1) a novel and easy-to-calculate local structure detector, the gradient-structures, which simulates the orientation selection mechanism based on the opponent-color space and connects it with low-level features, (2) a novel discriminative representation method that describes color, intensity and orientation features. It is convenient, as it does not require weight coefficients for color, intensity and orientation. (3) The proposed representation method has the advantages of being histogram-based and having the power to discriminate spatial layout, color and edge cues. The proposed method provides efficient CBIR performance, as demonstrated by comparative experiments in which it significantly outperformed some state-of-the-art methods, including the Bow method, local binary pattern histogram, perceptual uniform descriptor, color volume histogram, color difference histogram, some improved LBP methods and the Tree2Vector method in terms of precision/recall and AUC metrics.																	0941-0643	1433-3058				AUG	2020	32	15					11717	11727		10.1007/s00521-019-04657-0		JAN 2020											
J								Lidstone-type problems on the whole real line and homoclinic solutions applied to infinite beams	NEURAL COMPUTING & APPLICATIONS										Problems in the whole real line; Fixed-point theory; Green's functions; Beams simply supported on nonuniform elastic foundations	BOUNDARY-VALUE-PROBLEMS; POSITIVE SOLUTIONS; NONLINEAR-ANALYSIS; EXISTENCE; EQUATIONS	This work provides sufficient conditions for the existence of solutions to fourth-order nonlinear ordinary differential equations with Lidstone-type boundary conditions on the real line. Using Green's functions, we formulate a modified integral equation and correspondent integral operators, in which fixed points are the solutions of the initial problem. Moreover, it is proved that every solution of the Lidstone problem on the whole real line is an homoclinic solution.																	0941-0643	1433-3058				AUG	2020	32	16					12873	12879		10.1007/s00521-020-04732-x		JAN 2020											
J								Deep-recursive residual network for image semantic segmentation	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks; Semantic segmentation; Recursive residual network; Deep learning		A good semantic segmentation method for visual scene understanding should consider both accuracy and efficiency. However, the existing networks tend to concentrate only on segmentation results but not on simplifying the network. As a result, a heavy network will be made and it is difficult to deploy such heavy network on some hardware with limited memory. To address this problem, we in this paper develop a novel architecture by involving the recursive block to reduce parameters and improve prediction, as recursive block can improve performance without introducing new parameters for additional convolutions. In detail, for the purpose of mitigating the difficulty of training recursive block, we have adopted a residual unit to give the data more choices to flow through and utilize concatenation layer to combine the output maps of the recursive convolution layers with same resolution but different field-of-views. As a result, richer semantic information can be included in the feature maps, which is good to achieve satisfying pixel-wise prediction. Meriting from the above strategy, we also extend it to enhance Mask-RCNN for instance segmentation. Extensive simulations based on different benchmark datasets, such as DeepFashion, Cityscapes and PASCAL VOC 2012, show that our method can improve segmentation results as well as reduce the parameters.																	0941-0643	1433-3058				AUG	2020	32	16					12935	12947		10.1007/s00521-020-04738-5		JAN 2020											
J								Deep Dual-Stream Network with Scale Context Selection Attention Module for Semantic Segmentation	NEURAL PROCESSING LETTERS										Semantic segmentation; Dual-stream network; Multi-scale fusion; Scale context selection attention		The fusion of multi-scale features has been an effective method to get state-of-the-art performance in semantic segmentation. In this work, we concentrate on two tricky problems-the intra-class inconsistency and the blur on the localization of object boundaries and tackle them by combining two separate multi-scale context features respectively. Specifically, we propose a dual-stream structure with the scale context selection attention module to enhance the capabilities for multi-scale processing, where one stream collects global-scale context and the other captures local-scale information. Meanwhile, the embedded scale context selection attention module in each stream can adaptively focus on different scale context information to get optimal scale features. Based on our dual-stream structure with attention modules, our network can efficiently make use of multi-scale context to generate more comprehensive and powerful features. Our experiments show that our dual-stream network with scale context selection attention module achieves promising performance on the PASCAL VOC 2012 and PASCAL-Person-Part datasets.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2281	2299		10.1007/s11063-019-10148-z		JAN 2020											
J								A Novel Chaotic Interior Search Algorithm for Global Optimization and Feature Selection	APPLIED ARTIFICIAL INTELLIGENCE											ARTIFICIAL BEE COLONY; FLOWER POLLINATION ALGORITHM; PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZATION; BUTTERFLY OPTIMIZATION; FIREFLY ALGORITHM; DESIGN; CLASSIFICATION; MAPS; ISA	Interior Search Algorithm (ISA) is a recently proposed metaheuristic inspired by the beautification of objects and mirrors. However, similar to most of the metaheuristic algorithms, ISA also encounters two problems, i.e., entrapment in local optima and slow convergence speed. In the past, chaos theory has been successfully employed to solve such problems. In this study, 10 chaotic maps are embedded to improve the convergence rate as well as the resulting accuracy of the ISA algorithms. The proposed Chaotic Interior Search Algorithm (CISA) is validated on a diverse subset of 13 benchmark functions having unimodal and multimodal properties. The simulation results demonstrate that the chaotic maps (especially tent map) are able to significantly boost the performance of ISA. Furthermore, CISA is employed as a feature selection technique in which the aim is to remove features which may comprise irrelevant or redundant information in order to minimize the classification error rate. The performance of the proposed approaches is compared with five state-of-the-art algorithms over 21 data sets and the results proved the potential of the proposed binary approaches in searching the optimal feature subsets.																	0883-9514	1087-6545				MAR 20	2020	34	4					292	328		10.1080/08839514.2020.1712788		JAN 2020											
J								Improved fuzzy weighted-iterative association rule based ontology postprocessing in data mining for query recommendation applications	COMPUTATIONAL INTELLIGENCE										association rules; ontology; postprocessing; query; recommendation	HEALTH-CARE; ARCHITECTURES; PREDICTION; INTERNET; IOT	The usage of association rules is playing a vital role in the field of knowledge data discovery. Numerous rules have to be processed and plot based on the ranges on the schema. The step in this process depends on the user's queries. Previously, several projects have been proposed to reduce work and improve filtration processes. However, they have some limitations in preprocessing time and filtration rate. In this article, an improved fuzzy weighted-iterative concept is introduced to overcome the limitation based on the user request and visualization of discovering rules. The initial step includes the mix of client learning with posthandling to use the semantics. The above advance was trailed by surrounding rule schemas to fulfill and anticipate unpredictable guidelines dependent on client desires. Preparing the above developments can be imagined by the use of yet another clever method of study. Standards on guidelines are recognized by the average learning professionals.																	0824-7935	1467-8640				MAY	2020	36	2					773	782		10.1111/coin.12269		JAN 2020											
J								Use a sequential gradient-enhanced-Kriging optimal experimental design method to build high-precision approximate model for complex simulation problem	EVOLUTIONARY INTELLIGENCE										Surrogate model; Kriging; Gradient enhanced Kriging; Optimal experimental design; Cycloid gear pump design	CONSTRUCTION; OPTIMIZATION	The surrogate model based on Kriging has been widely used to approximate simulation problems of expensive computing. Although the accuracy of the gradient enhanced Kriging (GEK) is often higher than that of ordinary Kriging, designers cannot avoid more time consuming during gradient calculation of GEK. To this end, a sequential gradient-enhanced-Kriging optimal experimental design method with the Gaussian correlation function (GCF) is investigated to approximate complex black-box simulation problems by introducing gradient information of Kriging parameters. Due to the differentiable GCF, the gradient information can be simply evaluated. This characteristic make the proposed method effectively improve the modeling accuracy and efficiency of GEK. As expected, the test results from benchmark functions and the cycloid gear pump simulation show the feasibility, stability and applicability of the proposed method.																	1864-5909	1864-5917															10.1007/s12065-019-00345-z		JAN 2020											
J								StarTroper, a film trope rating optimizer using machine learning and evolutionary algorithms	EXPERT SYSTEMS										computational narrative; content generation; genetic algorithms; machine learning; tropes	NETWORKS	Designing a story is widely considered a crafty yet critical task that requires deep specific human knowledge in order to reach a minimum quality and originality. This includes designing at a high level different elements of the film; these high-level elements are called tropes when they become patterns. The present paper proposes and evaluates a methodology to automatically synthesize sets of tropes in a way that they maximize the potential rating of a film that conforms to them. We use machine learning to create a surrogate model that maps film ratings from tropes, trained with the data extracted and processed from huge film databases in Internet, and then we use a genetic algorithm that uses that surrogate model as evaluator to optimize the combination of tropes in a film. In order to evaluate the methodology, we analyse the nature of the tropes and their distributions in existing films, the performance of the models and the quality of the sets of tropes synthesized. The results of this proof of concept show that the methodology works and is able to build sets of tropes that maximize the rating and that these sets are genuine. The work has revealed that the methodology and tools developed are directly suitable for assisting in the plots generation as an authoring tool and, ultimately, for supporting the automatic generation of stories, for example, in massively populated videogames.																	0266-4720	1468-0394														e12525	10.1111/exsy.12525		JAN 2020											
J								Multiple attribute group decision-making based on order-alpha divergence and entropy measures under q-rung orthopair fuzzy environment	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										divergence measure; entropy measure; ERP system selection; MAGDM; q-rung orthopair fuzzy set	AGGREGATION OPERATORS; MEAN OPERATORS; INFORMATION MEASURES; SETS; MEMBERSHIP	The q-rung orthopair fuzzy set ((ROPFS)-R-q), proposed by Yager, is a more effective and proficient tool to represent uncertain or vague information in real-life situations. Divergence and entropy are two important measures, which have been extensively studied in different information environments, including fuzzy, intuitionistic fuzzy, interval-valued fuzzy, and Pythagorean fuzzy. In the present communication, we study the divergence and entropy measures under the q-rung orthopair fuzzy environment. First, the work defines two new order-alpha divergence measures for (q)ROPFSs to quantify the information of discrimination between two (q)ROPFSs. We also examine several mathematical properties associated with order-alpha (ROPF)-R-q divergence measures in detail. Second, the paper introduces two new parametric entropy functions called "order-alpha (ROPF)-R-q entropy measures" to measure the degree of fuzziness associated with a (ROPFS)-R-q. We show that the proposed order-alpha divergence and entropy measures include several existing divergence and entropy measures as their particular cases. Further, the paper develops a new decision-making approach to solve multiple attribute group decision-making problems under the (ROPF)-R-q environment where the information about the attribute weights is completely unknown or partially known. Finally, an example of selecting the best enterprise resource planning system is provided to illustrate the decision-making steps and effectiveness of the proposed approach.																	0884-8173	1098-111X				APR	2020	35	4					718	750		10.1002/int.22223		JAN 2020											
J								A study on deep learning spatiotemporal models and feature extraction techniques for video understanding	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Spatiotemporal; Deep learning; Video understanding; Computer vision; Survey	ACTION RECOGNITION; OBJECT DETECTION; REPRESENTATION; CHALLENGES; FRAMEWORK; CUES; CNN	Video understanding requires abundant semantic information. Substantial progress has been made on deep learning models in the image, text, and audio domains, and notable efforts have been recently dedicated to the design of deep networks in the video domain. We discuss the state-of-the-art convolutional neural network (CNN) and its pipelines for the exploration of video features, various fusion strategies, and their performances; we also discuss the limitations of CNN for long-term motion cues and the use of sequential learning models such as long short-term memory to overcome these limitations. In addition, we address various multi-model approaches for extracting important cues and score fusion techniques from hybrid deep learning frameworks. Then, we highlight future plans in this domain, recent trends, and substantial challenges for video understanding. This survey's objectives are to study the plethora of approaches that have been developed for solving video understanding problems, to comprehensively study spatiotemporal cues, to explore the various models that are available for solving these problems and to identify the most promising approaches.																	2192-6611	2192-662X				JUN	2020	9	2			SI		81	101		10.1007/s13735-019-00190-x		JAN 2020											
J								Fast method for GA-PLS with simultaneous feature selection and identification of optimal preprocessing technique for datasets with many observations	JOURNAL OF CHEMOMETRICS										GA-PLS; Genetic algorithms; Partial least squares; Feature selection; Preprocessing	LEAST-SQUARES REGRESSION; VARIABLE SELECTION; ALGORITHM	A fast and memory-efficient new method for performing genetic algorithm partial least squares (GA-PLS) on spectroscopic data preprocessed in multiple different ways is presented. The method, which is primarily intended for datasets containing many observations, involves preprocessing a spectral dataset with several different techniques and concatenating the different versions of the data horizontally into a design matrix X which is both tall and wide. The large matrix is then condensed into a substantially smaller covariance matrix (XX)-X-T whose resulting size is unrelated to the number of observations in the dataset, i.e. the height of X. It is demonstrated that the smaller covariance matrix can be used to efficiently calibrate partial least squares (PLS) models containing feature selections from any of the involved preprocessing techniques. The method is incorporated into GA-PLS and used to evolve variable selections for a set of different preprocessing techniques concurrently within a single algorithm. This allows a single instance of GA-PLS to determine which preprocessing technique, within the set of considered methods, is best suited for the spectroscopic dataset. Additionally, the method allows feature selections to be evolved containing variables from a mixture of different preprocessing techniques. The benefits of the introduced GA-PLS technique can be summarized as threefold: (1) for datasets with many observations, the proposed method is substantially faster compared to conventional GA-PLS implementations based on NIPALS, SIMPLS, etc. (2) using a single GA-PLS automatically reveals which of the considered preprocessing techniques results in the lowest model error. (3) it allows the exploration of highly complex solutions composed of features preprocessed using various techniques.																	0886-9383	1099-128X				MAR	2020	34	3			SI				e3195	10.1002/cem.3195		JAN 2020											
J								Sparse hierarchical regression with polynomials	MACHINE LEARNING										Nonlinear regression; Sparse regression; Integer optimization; Polynomial learning	SELECTION; KERNEL; MATRIX	We present a novel method for sparse polynomial regression. We are interested in that degree r polynomial which depends on at most k inputs, counting at most monomial terms, and minimizes the sum of the squares of its prediction errors. Such highly structured sparse regression was denoted by Bach (Advances in neural information processing systems, pp 105-112, 2009) as sparse hierarchical regression in the context of kernel learning. Hierarchical sparse specification aligns well with modern big data settings where many inputs are not relevant for prediction purposes and the functional complexity of the regressor needs to be controlled as to avoid overfitting. We propose an efficient two-step approach to this hierarchical sparse regression problem. First, we discard irrelevant inputs using an extremely fast input ranking heuristic. Secondly, we take advantage of modern cutting plane methods for integer optimization to solve the remaining reduced hierarchical (k,l-sparse problem exactly. The ability of our method to identify all k relevant inputs and all l monomial terms is shown empirically to experience a phase transition. Crucially, the same transition also presents itself in our ability to reject all irrelevant features and monomials as well. In the regime where our method is statistically powerful, its computational complexity is interestingly on par with Lasso based heuristics. Hierarchical sparsity can retain the flexibility of general nonparametric methods such as nearest neighbors or regression trees (CART), without sacrificing much statistical power. The presented work hence fills a void in terms of a lack of powerful disciplined nonlinear sparse regression methods in high-dimensional settings. Our method is shown empirically to scale to regression problems with n approximate to 10,000 observations for input dimension p approximate to 1000.																	0885-6125	1573-0565				MAY	2020	109	5					973	997		10.1007/s10994-020-05868-6		JAN 2020											
J								Self-adaptive Multi-population Rao Algorithms for Engineering Design Optimization	APPLIED ARTIFICIAL INTELLIGENCE											LEARNING-BASED OPTIMIZATION; SEARCH ALGORITHM; SWARM ALGORITHM; EVOLUTIONARY	The performance of various population-based advanced optimization algorithms has been significantly improved by using the multi-population search scheme. The multi-population search process improves the diversity of solutions by dividing the total population into a number of sub-populations groups to search for the best solution in different areas of a search space. This paper proposes improved optimization algorithms based on self-adaptive multi-population for solving engineering design optimization problems. These proposed algorithms are based on Rao algorithms which are recently proposed simple and algorithm-specific parameter-less advanced optimization algorithms. In this work, Rao algorithms are upgraded with the multi-population search process to enhance the diversity of search. The number of sub-populations is changed adaptively considering the strength of solutions to control the exploration and exploitation of the search process. The performance of proposed algorithms is investigated on 25 unconstrained benchmark functions and 14 complex constrained engineering design optimization problems. The results obtained using proposed algorithms are compared with the various advanced optimization algorithms. The comparison of results shows the effectiveness of proposed algorithms for solving engineering design optimization problems. The significance of the proposed methods has proved using a well-known statistical test known as "Friedman test." Furthermore, the convergence plots are illustrated to show the convergence speed of the proposed algorithms.																	0883-9514	1087-6545															10.1080/08839514.2020.1712789		JAN 2020											
J								Dynamic modeling and control of five phase SVPWM inverter fed induction motor drive with intelligent speed controller	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wavelet-fuzzy logic controller; Direct torque control; SVPWM; 5-phi voltage source inverter 5-phi induction motor	DIRECT TORQUE CONTROL; WAVELET TRANSFORM; MACHINES	Multi-phase induction motor (IM) drive is an emerging field of electric drive applications. This enables us to use the multi-phase AC machine than a three-phase machine. Precision speed control is important for industrial applications. Hence, direct torque control (DTC) with PI or PID regulators used, and these regulators are not flexible because of a constant gain in all working conditions. Therefore, robust speed control is essential for the precision speed control in industrial applications. The major objective of this study is to investigate a novel speed control for DTC of 5-phi VSI with space vector PWM (SVPWM) technique fed IM drive using wavelet fuzzy logic interface system. In precise, wavelet-fuzzy controller is an alternative for the PID controller used for DTC IM drive. The speed error signal comprises many short and extraordinary-frequency components. The scaled gains are generated for a DTC IM drive by the decomposition of discrete wavelet transform and the fuzzy logic controller (FLC) from high and low-frequency signals. The investigation of 2 hp 5-phi VSI fed IMD is carried out using the simulation model developed in MATLAB2016a software and examined under various working conditions. The simulation results are shown in this paper to confirm the suitability for industrial applications.																	1868-5137	1868-5145															10.1007/s12652-020-01717-5		JAN 2020											
J								A tutorial on elementary cellular automata with fully asynchronous updating General properties and convergence dynamics	NATURAL COMPUTING										Dynamical systems; Asynchronous updating; Convergence; Markov chains	BEHAVIOR; EXAMPLES	We present a panorama of the convergence properties of the 256 Elementary Cellular Automata under fully asynchronous updating, that is, when only one cell is updated at each time step. We regroup here various results which have been presented in different articles and expose a full analysis of the behaviour of finite systems with periodic boundary conditions. Our classification relies on the scaling properties of the average convergence time to a fixed point. We observe that different scaling laws can be found, which fall in one of the following classes: logarithmic, linear, quadratic, exponential and non-converging. The techniques for quantifying this behaviour rely mainly on Markov chain theory and martingales. Most behaviours can be studied analytically but there are still many rules for which obtaining a formal characterisation of their convergence properties is still an open problem.																	1567-7818	1572-9796				MAR	2020	19	1			SI		179	197		10.1007/s11047-020-09782-7		JAN 2020											
J								Fish Detection Using Deep Learning	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING												Recently, human being's curiosity has been expanded from the land to the sky and the sea. Besides sending people to explore the ocean and outer space, robots are designed for some tasks dangerous for living creatures. Take the ocean exploration for an example. There are many projects or competitions on the design of Autonomous Underwater Vehicle (AUV) which attracted many interests. Authors of this article have learned the necessity of platform upgrade from a previous AUV design project, and would like to share the experience of one task extension in the area of fish detection. Because most of the embedded systems have been improved by fast growing computing and sensing technologies, which makes them possible to incorporate more and more complicated algorithms. In an AUV, after acquiring surrounding information from sensors, how to perceive and analyse corresponding information for better judgement is one of the challenges. The processing procedure can mimic human being's learning routines. An advanced system with more computing power can facilitate deep learning feature, which exploit many neural network algorithms to simulate human brains. In this paper, a convolutional neural network (CNN) based fish detection method was proposed. The training data set was collected from the Gulf of Mexico by a digital camera. To fit into this unique need, three optimization approaches were applied to the CNN: data augmentation, network simplification, and training process speed up. Data augmentation transformation provided more learning samples; the network was simplified to accommodate the artificial neural network; the training process speed up is introduced to make the training process more time efficient. Experimental results showed that the proposed model is promising, and has the potential to be extended to other underwear objects.																	1687-9724	1687-9732				JAN 23	2020	2020								3738108	10.1155/2020/3738108													
J								Automatic speaker recognition from speech signal using bidirectional long-short-term memory recurrent neural network	COMPUTATIONAL INTELLIGENCE										Mel-frequency cepstral coefficient; probabilistic principal component analysis; recurrent neural network-bidirectional long short term memory; Wiener filter algorithm	IDENTIFICATION; VERIFICATION	Speaker recognition is a major challenge in various languages for researchers. For programmed speaker recognition structure prepared by utilizing ordinary speech, shouting creates a confusion between the enlistment and test, henceforth minimizing the identification execution as extreme vocal exertion is required during shouting. Speaker recognition requires more time for classification of data, accuracy is optimized, and the low root-mean-square error rate is the major problem. The objective of this work is to develop an efficient system of speaker recognition. In this work, an improved method of Wiener filter algorithm is applied for better noise reduction. To obtain the essential feature vector values, Mel-frequency cepstral coefficient feature extraction method is used on the noise-removed signals. Furthermore, input samples are created by using these extracted features after the dimensions have been reduced using probabilistic principal component analysis. Finally, recurrent neural network-bidirectional long-short-term memory is used for the classification to improve the prediction accuracy. For checking the effectiveness, the proposed work is compared with the existing methods based on accuracy, sensitivity, and error rate. The results obtained with the proposed method demonstrate an accuracy of 95.77%.																	0824-7935	1467-8640															10.1111/coin.12278		JAN 2020											
J								Anthropometric clothing measurements from 3D body scans	MACHINE VISION AND APPLICATIONS										Anthropometric measurement; 3D body model; Non-rigid ICP		We propose a full processing pipeline to acquire anthropometric measurements from 3D measurements. The first stage of our pipeline is a commercial point cloud scanner. In the second stage, a pre-defined body model is fitted to the captured point cloud. We have generated one male and one female model from the SMPL library. The fitting process is based on non-rigid iterative closest point algorithm that minimizes overall energy of point distance and local stiffness energy terms. In the third stage, we measure multiple circumference paths on the fitted model surface and use a nonlinear regressor to provide the final estimates of anthropometric measurements. We scanned 194 male and 181 female subjects, and the proposed pipeline provides mean absolute errors from 2.5 to 16.0 mm depending on the anthropometric measurement.																	0932-8092	1432-1769				JAN 22	2020	31	1-2							7	10.1007/s00138-019-01054-4													
J								Semigroup of fuzzy automata and its application for fast accurate fault diagnosis on machine and anti-fatigue control	APPLIED INTELLIGENCE										Semigroup of fuzzy automata; Homomorphism of semigroup; Fuzzy inference system; Fault diagnosis	DETERMINIZATION	In order to carry out machine fault diagnosis earlier and more accurately such as the automatic detection for the ship's level scale, and the existing literatures didn't discuss these until now. However, for solving the problem, this paper presents a semigroup of fuzzy automata and its properties, constructs a fuzzy inference system on the semigroup of fuzzy automata, and discusses its application on machine fault diagnosis and the anti-fatigue driving reminder device. At the same time, the comparison between this inference model and the existing diagnosis methods is discussed. The experimental results show that the diagnosis speed and the average precision of the proposed inference model are faster and higher than those of traditional methods, which their maximum diagnosis precision is 95.98%.																	0924-669X	1573-7497				MAY	2020	50	5					1542	1557		10.1007/s10489-019-01611-4		JAN 2020											
J								Detecting community structure in complex networks using genetic algorithm based on object migrating automata	COMPUTATIONAL INTELLIGENCE										community structure; community structure detection; complex networks; modularity optimization	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; OPTIMIZATION	Community structure is an important topological feature of complex networks. Detecting community structure is a highly challenging problem in analyzing complex networks and has great importance in understanding the function and organization of networks. Up until now, numerous algorithms have been proposed for detecting community structure in complex networks. A wide range of these algorithms use the maximization of a quality function called modularity. In this article, three different algorithms, namely, MEM-net, OMA-net, and GAOMA-net, have been proposed for detecting community structure in complex networks. In GAOMA-net algorithm, which is the main proposed algorithm of this article, the combination of genetic algorithm (GA) and object migrating automata (OMA) has been used. In GAOMA-net algorithm, the MEM-net algorithm has been used as a heuristic to generate a portion of the initial population. The experiments on both real-world and synthetic benchmark networks indicate that GAOMA-net algorithm is efficient for detecting community structure in complex networks.																	0824-7935	1467-8640				MAY	2020	36	2					824	860		10.1111/coin.12273		JAN 2020											
J								Discriminative Training of Conditional Random Fields with Probably Submodular Constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION										Structured prediction; Submodularity; Graphical models; Segmentation	SEGMENTATION	Problems of segmentation, denoising, registration and 3D reconstruction are often addressed with the graph cut algorithm. However, solving an unconstrained graph cut problem is NP-hard. For tractable optimization, pairwise potentials have to fulfill the submodularity inequality. In our learning paradigm, pairwise potentials are created as the dot product of a learned vector w with positive feature vectors. In order to constrain such a model to remain tractable, previous approaches have enforced the weight vector to be positive for pairwise potentials in which the labels differ, and set pairwise potentials to zero in the case that the label remains the same. Such constraints are sufficient to guarantee that the resulting pairwise potentials satisfy the submodularity inequality. However, we show that such an approach unnecessarily restricts the capacity of the learned models. Guaranteeing submodularity for all possible inputs, no matter how improbable, reduces inference error to effectively zero, but increases model error. In contrast, we relax the requirement of guaranteed submodularity to solutions that are probably approximately submodular. We show that the conceptually simple strategy of enforcing submodularity on the training examples guarantees with low sample complexity that test images will also yield submodular pairwise potentials. Results are presented in the binary and muticlass settings, showing substantial improvement from the resulting increased model capacity.																	0920-5691	1573-1405				JUN	2020	128	6					1722	1735		10.1007/s11263-019-01277-y		JAN 2020											
J								Characterization and classification of semantic image-text relations	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Image-text class; Multimodality; Data augmentation; Semantic gap		The beneficial, complementary nature of visual and textual information to convey information is widely known, for example, in entertainment, news, advertisements, science, or education. While the complex interplay of image and text to form semantic meaning has been thoroughly studied in linguistics and communication sciences for several decades, computer vision and multimedia research remained on the surface of the problem more or less. An exception is previous work that introduced the two metrics Cross-Modal Mutual Information and Semantic Correlation in order to model complex image-text relations. In this paper, we motivate the necessity of an additional metric called Status in order to cover complex image-text relations more completely. This set of metrics enables us to derive a novel categorization of eight semantic image-text classes based on three dimensions. In addition, we demonstrate how to automatically gather and augment a dataset for these classes from the Web. Further, we present a deep learning system to automatically predict either of the three metrics, as well as a system to directly predict the eight image-text classes. Experimental results show the feasibility of the approach, whereby the predict-all approach outperforms the cascaded approach of the metric classifiers.																	2192-6611	2192-662X				MAR	2020	9	1			SI		31	45		10.1007/s13735-019-00187-6		JAN 2020											
J								Hypergraph learning with collaborative representation for image search reranking	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Image search; Hypergraph; Reranking; Regression; Collaborative representation	RETRIEVAL; RANKING; DIVERSIFICATION; TREE	Image search reranking has received considerable attention in recent years. It aims at refining the text-based image search results by boosting the rank of relevant images. Hypergraph has been widely used for relevance estimation, where textual results are taken as vertices and the hypergraph ranking is performed to learn their relevance scores. Rather than using the K-nearest neighbor method, recent works have adopted the sparse representation to effectively construct an informative hypergraph. The sparse representation is insensitive to noise and can capture the real neighborhood structure. However, it suffers from a heavy computational cost. Motivated by this observation, in this paper, we leveraged the ridge regression for hypergraph construction. By imposing an l(2)-regularizer on the size of their regression coefficients, the ridge regression enforces the training samples to collaborate to represent one query. The so-called collaborative representation exhibits more discriminative power and robustness while being computationally efficient. Thereafter, based on the obtained collaborative representation vectors, we measured the pairwise similarities among samples and generated hyperedges. Extensive experiments on the public MediaEval benchmarks demonstrated the effectiveness and superiority of our method over the state-of-the-art reranking methods.																	2192-6611	2192-662X				SEP	2020	9	3					205	214		10.1007/s13735-019-00191-w		JAN 2020											
J								Efficient algorithms to minimize the end-to-end latency of edge network function virtualization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network function virtualization; Hosting device; Latency; Stable matching; Local search	PLACEMENT; IOT	In future wireless networks, network function virtualization will lay the foundation for establishing a new dynamic resource management framework to efficiently utilize network resources. The main problem discussed in this paper is the minimization of total latency for an edge network and how to solve it efficiently. A model of users, virtual network functions and hosting devices has been taken, and is used to find the minimum latency using integer linear programming. The problem is NP-hard and takes exponential time to return the optimal solution. We apply the stable matching based algorithm to solve the problem in polynomial time and then utilize local search to improve its efficiency further. From extensive performance evaluation, it is found that our proposed algorithm is very close to the optimal scheme in terms of latency and better in terms of time complexity.																	1868-5137	1868-5145				OCT	2020	11	10			SI		3963	3974		10.1007/s12652-019-01630-6		JAN 2020											
J								A novel energy-aware bio-inspired clustering scheme for IoT communication	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; WSN; PSO; Gini coefficient; Connectivity and coverage	WIRELESS SENSOR NETWORKS; EFFICIENT INTERNET; ARCHITECTURE; COVERAGE; EXTEND; WSN	Nowadays, the internet of thing (IoT) is a novel paradigm that is rapidly gaining ground in the scenario of modern wireless telecommunications. Wireless sensor network (WSN) is an important part of IoT, and it is mainly responsible for acquiring and reporting data. As lifetime and coverage area of WSN directly determine IoT performance, how to design a method to conserve nodes energy and reduce nodes death rates become important issues. Sensor network clustering is one efficient method to solve these problems. It divides nodes into clusters and selects one to be cluster head (CH). The data transmission and communication within one cluster are managed by its CH. Many traditional strategies have been designed out, but because of network dynamic feature, machine learning methods become more attractive and many literature are working on them. Particle swarm optimization (PSO) is one evolutionary algorithm. Inspired by this algorithm, we propose a novel energy-aware bio-inspired clustering scheme (PSO-WZ). We firstly initialize CHs combination randomly and assign non-CHs based on division rules. Then, using the fitness function to guide the selection process until the maximum time is reached. Since the division rule is directly related with the network topology and node energy consumption distribution, we design it from two angles: non-CHs and the whole network, to save the energy of each node as much as possible. Meanwhile, in order to balance energy load among nodes, which contributes to lowering nodes reduction and preserving network coverage range, we introduce the Gini coefficient into the objective function. From the results obtained, we conclude that the proposed algorithm is able to keep more nodes alive over time, prolong the network life cycle, and improve the overall performance of IoT further.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4239	4248		10.1007/s12652-020-01704-w		JAN 2020											
J								Fine-grained data-locality aware MapReduce job scheduler in a virtualized environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MapReduce job scheduling; Combiner; Bandwidth minimization	DATA PLACEMENT; HADOOP; PERFORMANCE	Big data overwhelmed industries and research sectors. Reliable decision making is always a challenging task, which requires cost-effective big data processing tools. Hadoop MapReduce is being used to store and process huge volume of data in a distributed environment. However, due to huge capital investment and lack of expertise to set up an on-premise Hadoop cluster, big data users seek cloud-based MapReduce service over the Internet. Mostly, MapReduce on a cluster of virtual machines is offered as a service for a pay-per-use basis. Virtual machines in MapReduce virtual cluster reside in different physical machines and co-locate with other non-MapReduce VMs. This causes to share IO resources such as disk and network bandwidth, leading to congestion as most of the MapReduce jobs are disk and network intensive. Especially, the shuffle phase in MapReduce execution sequence consumes huge network bandwidth in a multi-tenant environment. This results in increased job latency and bandwidth consumption cost. Therefore, it is essential to minimize the amount of intermediate data in the shuffle phase rather than supplying more network bandwidth that results in increased service cost. Considering this objective, we extended multi-level per node combiner for a batch of MapReduce jobs to improve makespan. We observed that makespan is improved up to 32.4% by minimizing the number of intermediate data in shuffle phase when compared to classical schedulers with default combiners.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4261	4272		10.1007/s12652-020-01707-7		JAN 2020											
J								A study on harmonious chromatic number of total graph of central graph of generalized Petersen graph	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Harmonious coloring; Generalized Petersen graph; Central graph; Total graph		For a given graph G, chi(h)(G) is the least integer of colors such that no two adjacent nodes receive the same color and each combination of color seems together on at most one line. The least number of compartments into which a warehouse should be partitioned to store chemicals certain pairs of which are incompatible is again the chromatic number of the conflict graph. In this article we have found the harmonious coloring of central graph of generalized Petersen graph and have characterized the harmonious chromatic number with the maximum matching number and further we have found the total graph of central graph of generalized Petersen graph using the clique. Clique is a complete graph where every vertex is adjacent to every other. In computational biology we use cliques as a technique of abstracting pair wise relationships consisting of protein-protein interaction or gene similarity. In the latter case we would need to set up a side among the vertices representing two genes if the ones genes have say comparable expression profiles over several time factors of an experiments to study the health care information of patients.																	1868-5137	1868-5145															10.1007/s12652-020-01697-6		JAN 2020											
J								Robustness control in bilinear modeling based on maximum correntropy	JOURNAL OF CHEMOMETRICS										maximum correntropy; multivariate calibration; normal distribution; outliers; robust models	REGRESSION	We present the development of a bilinear regression model for multivariate calibration on the basis of maximum correntropy criteria (MCC) whose robustness can be easily controlled. MCC regression methods can be more effective when the assumption of normality does not hold or when data are contaminated with outliers. These methods are competitive when the degree of robustness against outliers should be controlled. By controlling the robustness, information from candidate outliers can be partially retained rather than completely included or discarded during calibration. Within the context of bilinear regression models, an MCC approach using statistically inspired modification of the partial least squares (SIMPLS) is proposed, which is named maximum correntropy-weighted partial least squares (MCW-PLS). Thanks to the controllable robustness of MCC models, observations are upweighted or downweighted during the calibration process, rendering robust models with soft discrimination of samples. Such a weighting represents an important advantage, especially for cases when samples are not drawn from a normal distribution. Applications to three real case studies are presented. These applications uncovered three main features of MCW-PLS: robustness control between SIMPLS and robust SIMPLS (RSIMPLS), improvements in prediction performance of bilinear calibration models, and the possibility to detect the most informative samples in a calibration set.																	0886-9383	1099-128X														e3215	10.1002/cem.3215		JAN 2020											
J								Sentiment analysis using rule-based and case-based reasoning	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Sentiment analysis; Opinion mining; Machine learning; Expert systems	CLASSIFICATION	Sentiment analysis becomes increasingly popular with the rapid growth of various reviews, survey responses, tweets or posts available from social media like Facebook or Twitter. Sentiment analysis can be turned into the question of whether a piece of text is expressing positive, negative or neutral sentiment towards the discussed topic and can be thus understood as a knowledge-based classification problem. A variety of knowledge-based techniques can be used to solve this problem. The paper focuses on two complementary approaches that originate in the area of AI (artificial intelligence), rule-based reasoning and case-based reasoning. We describe basic principles of both approaches, their strengths and limitations and, based on a review of literature, show how these approaches can be used for sentiment analysis.																	0925-9902	1573-7675				AUG	2020	55	1					51	66		10.1007/s10844-019-00591-8		JAN 2020											
J								A novel learning-based feature recognition method using multiple sectional view representation	JOURNAL OF INTELLIGENT MANUFACTURING										Feature recognition; Deep learning; Multiple sectional views; Transfer learning; Data augmentation	MANUFACTURING FEATURE RECOGNITION; AUTOMATIC FEATURE RECOGNITION; SHEET-METAL COMPONENTS; NEURAL-NETWORK; SOLID MODELS; CAD; DESIGN; SYSTEM; PARTS; INTEGRATION	In computer-aided design (CAD) and process planning (CAPP), feature recognition is an essential task which identifies the feature type of a 3D model for computer-aided manufacturing (CAM). In general, traditional rule-based feature recognition methods are computationally expensive, and dependent on surface or feature types. In addition, it is quite challenging to design proper rules to recognise intersecting features. Recently, a learning-based method, named FeatureNet, has been proposed for both single and multi-feature recognition. This is a general purpose algorithm which is capable of dealing with any type of features and surfaces. However, thousands of annotated training samples for each feature are required for training to achieve a high single feature recognition accuracy, which makes this technique difficult to use in practice. In addition, experimental results suggest that multi-feature recognition part in this approach works very well on intersecting features with small overlapping areas, but may fail when recognising highly intersecting features. To address the above issues, a deep learning framework based on multiple sectional view (MSV) representation named MsvNet is proposed for feature recognition. In the MsvNet, MSVs of a 3D model are collected as the input of the deep network, and the information achieved from different views are combined via the neural network for recognition. In addition to MSV representation, some advanced learning strategies (e.g. transfer learning, data augmentation) are also employed to minimise the number of training samples and training time. For multi-feature recognition, a novel view-based feature segmentation and recognition algorithm is presented. Experimental results demonstrate that the proposed approach can achieve the state-of-the-art single feature performance on the FeatureNet dataset with only a very small number of training samples (e.g. 8-32 samples for each feature), and outperforms the state-of-the-art learning-based multi-feature recognition method in terms of recognition performances.																	0956-5515	1572-8145				JUN	2020	31	5					1291	1309		10.1007/s10845-020-01533-w		JAN 2020											
J								A study on the relationship between the rank of input data and the performance of random weight neural network	NEURAL COMPUTING & APPLICATIONS										Random weight neural network; Random vector functional link network; Extreme learning machine	EXTREME LEARNING-MACHINE; SINGULAR VALUE DECOMPOSITION	Random feature mapping (RFM) is the core operation in the random weight neural network (RWNN). Its quality has a significant impact on the performance of a RWNN model. However, there has been no good way to evaluate the quality of RFM. In this paper, we introduce a new concept called dispersion degree of matrix information distribution (DDMID), which can be used to measure the quality of RFM. We used DDMID in our experiments to explain the relationship between the rank of input data and the performance of the RWNN model and got some interesting results. We found that: (1) when the rank of input data reaches a certain threshold, the model's performance increases with the increase in the rank; (2) the impact of the rank on the model performance is insensitive to the type of activation functions and the number of hidden nodes; (3) if the DDMID of an RFM matrix is very small, it implies that the first k singular values in the singular value matrix of the RFM matrix contain too much information, which usually has a negative impact on the final closed-form solution of the RWNN model. Besides, we verified the improvement effect of intrinsic plasticity (IP) algorithm on RFM by using DDMID. The experimental results showed that DDMID allows researchers evaluate the mapping quality of data features before model training, so as to predict the effect of data preprocessing or network initialization without model training. We believe that our findings could provide useful guidance when constructing and analyzing a RWNN model.																	0941-0643	1433-3058				AUG	2020	32	16					12685	12696		10.1007/s00521-020-04719-8		JAN 2020											
J								K-OpenAnswer: a simulation environment to analyze the dynamics of massive open online courses in smart cities	SOFT COMPUTING										Smart cities; Peer assessment; Machine learning; E-learning	PEER ASSESSMENT; SUPPORT	The smartness of a city is given by the technologies it put to use, and more than that, by the people empowered by such technologies; it is worth thinking about how people can be trained to be empowered by smart technologies, and how cities can become "educational." So, while sustainability and technology solutions for smart cities are strategic challenges, one of these is surely distance education and training. In this field, the Web offers many opportunities, such as the e-learning platforms where students can learn, according to their own needs and pace. The massive open online courses (MOOCs) are particular distance learning platforms, generally offering, so far, free courses on a huge amount of topics, and characterized by a (potentially) very high number of enrollments. In a MOOC, a teacher, or tutor, has a hard life when trying to follow and manage with the learning processes of thousands of students. In particular, assessment can be managed almost exclusively by letting the student answer questions in closed answers tests. This strategy has some didactic limits, while a valid alternative is to use peer assessment (PA) over more articulated assessment activities (e.g., open-ended questions). PA makes students grade their peers' answers, and provides learners with significant advantages, such as refining their knowledge of the subject matter, and developing their meta-cognitive skills. In this work, we present a software platform called K-OpenAnswer, which helps teachers to simulate the dynamic of a MOOC where PA is used. The system uses a machine learning technique, based on a modified version of the K-NN algorithm, and provides teachers with a statistical environment by which they can monitor the evolving dynamic of a simulated MOOC, according to the techniques we use to implement PA. An experimental evaluation is presented that highlights the advantages of using the system as a valid tool for the study of real MOOCs.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11121	11134		10.1007/s00500-020-04696-z		JAN 2020											
J								An artificial neural network model for the unary description of pure substances and its application on the thermodynamic modelling of pure iron	SOFT COMPUTING										Neural networks; Function approximation; Derivative approximation; Computational thermodynamics	HEAT-CAPACITIES; ATOMIC HEAT; 0 K; ALLOYS; LIQUID; PHASES; NICKEL	The aim of this work is to introduce a novel approach for the universal description of the thermodynamic functions of pure substances on the basis of artificial neural networks. The proposed approximation method is able to describe the thermodynamic functions (Cp(T),S(T),H(T)-H(Tref),G(T)) of the different phases of unary material systems in a wide temperature range (between 0 and 6000 K). Phase transition temperatures and the respective enthalpies of transformation, which are computationally determined by the minimization of the Gibbs free energy, are also approximated. This is achieved by using artificial neural networks as models for the thermodynamic functions of the individual phases and by expressing the thermodynamic quantities in terms of the free network parameters. The resulting expressions are then optimized with machine learning algorithms on the basis of measurement data. A physical basis for the resulting approximation is given by the use of, among others, Planck-Einstein functions as activation function of the neurons of the network. This article provides a description of the method and as an example of a specific application the approximation of the thermodynamic functions of the different phases of pure iron. The article focuses on the problem of the representation of thermodynamic data and their practical application.																	1432-7643	1433-7479				AUG	2020	24	16					12227	12239		10.1007/s00500-019-04663-3		JAN 2020											
J								A real-time physiological signal acquisition and analyzing method based on fractional calculus and stream computing	SOFT COMPUTING										Physiological signal; Stream computing; Fractal calculus; Apache storm; Distributed parallel computing		The physiological signal acquisition and analyzing are important for intelligent health services, human-computer interaction and other applications. Due to the computing power limitation of terminal devices, many analyzing methods of physiological signals are in offline mode. However, in many applications, physiological signal should be analyzed in real time. To overcome this problem, a real-time physiological signal acquisition and analysis method based on fractional calculus and stream computing is proposed. Mobile terminals read the physiological data from sensors and upload them to the stream computing platform. A fractal index is used to estimate the physiological status. Based on the stream computing platform, this index is calculated by distributed parallel computing. The experiment results show this method can distinguish the heart health status and reflect driver mental status to some extent.																	1432-7643	1433-7479															10.1007/s00500-020-04703-3		JAN 2020											
J								Lane departure warning algorithm based on probability statistics of driving habits	SOFT COMPUTING										Image processing; Lane departure warning; Kalman filter; Probability statistics	ROAD CURVATURE ESTIMATION; SYSTEM	For the different degrees of danger caused by different driving habits, a lane departure warning algorithm based on probability statistics of driving habits is proposed in this paper. According to the different driving habits of different drivers, the early warning mechanism can be adaptively adjusted through the method of probability statistics to make lane departure warning more targeted and accurate. Firstly, each frame of image is preprocessed, including gray treatment, edge detection and binarization. Then, Canny operator is used to detect the edge, and Hough transform is applied to detect the lines. And the lane median line equation for the detection and identification of lane also can be calculated. After that, the image coordinate system is transformed into the world coordinate system by means of the formula and matrix of coordinate conversion. According to the theory of Kalman filter, the statistics of lateral acceleration and lateral velocity are updated continuously, and the position of the vehicle in the next moment is predicted by the state transition equation and the forecast equation. From the results of experiments and the comparison with exhaustive algorithms, the advantages of using Kalman filter to predict the location of vehicles and the improved time-to-lane-crossing combined with probabilistic statistics to warning are illustrated clearly.																	1432-7643	1433-7479															10.1007/s00500-020-04704-2		JAN 2020											
J								Reduced-gate convolutional long short-term memory using predictive coding for spatiotemporal prediction	COMPUTATIONAL INTELLIGENCE										convolutional long short-term memory; convLSTM; predictive coding; rgcLSTM; spatiotemporal prediction; video prediction	TIME-SERIES ANALYSIS	Spatiotemporal sequence prediction is an important problem in deep learning. We study next-frame(s) video prediction using a deep-learning-based predictive coding framework that uses convolutional LSTM (convLSTM) modules. We introduce a novel rgcLSTM architecture that requires a significantly lower parameter budget than a comparable convLSTM. By using a single multifunction gate, our reduced-gate model achieves equal or better next-frame(s) prediction accuracy than the original convolutional LSTM while using a smaller parameter budget, thereby reducing training time and memory requirements. We tested our reduced gate modules within a predictive coding architecture on the moving MNIST and KITTI datasets. We found that our reduced-gate model has a significant reduction of approximately 40% of the total number of training parameters and a 25% reduction in elapsed training time in comparison with the standard convolutional LSTM model. The performance accuracy of the new model was also improved. This makes our model more attractive for hardware implementation, especially on small devices. We also explored a space of 20 different gated architectures to get insight into how our rgcLSTM fits into that space.																	0824-7935	1467-8640				AUG	2020	36	3					910	939		10.1111/coin.12277		JAN 2020											
J								A Control Software Framework for Wearable Mechatronic Devices	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Software; Framework; Open-source; Control systems; Wearable devices	ROBOT; THERAPY; SYSTEMS	The rapid growth of wearable mechatronic devices for motion assistance applications has created a demand for tools that assist with control software development. The wearable mechatronics-enabled control software (WearMECS) framework is proposed as a software development tool for the control systems of these devices. The WearMECS framework was developed to support both software design and implementation. Control software has been developed using the framework that supports various motion tasks, control system models, and wearable mechatronic devices. In this research, the control systems developed with the framework have resulted in some of the lowest elbow motion tracking errors in the literature. The framework has also helped to increase the efficiency of experimental evaluations and the comparison of control components. The versatility of the WearMECS framework provides control system developers with a foundation to build upon, while maintaining their creative freedom. This framework provides a platform that supports the continued growth and improvement of control systems for wearable assistive devices.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		757	771		10.1007/s10846-019-01144-5		JAN 2020											
J								Detection of difficult airway using deep learning	MACHINE VISION AND APPLICATIONS										Difficult airway; Deep learning; Convolutional neural networks	MALLAMPATI TEST; PREDICT	Whenever a patient needs to enter the operating room, in case the surgery requires general anesthesia, he/she must be intubated, and an anesthesiologist has to make a previous check to the patient in order to evaluate his/her airway. This process should be done to the patient to anticipate any problem, such as a difficult airway at the time of being anesthetized. In fact, the inadequate detection of a difficult airway can cause serious complications, even death. This research work proposes a mobile app that uses a convolutional neural network to detect a difficult airway. This model classifies two classes of the Mallampati score, namely Mallampati 1-2 (with low risk of difficult airway) and Mallampati 3-4 (with higher risk of difficult airway). The average accuracy of the predictive model is 88.5% for classifying pictures. A total of 240 pictures were used for training the model. The results of sensitivity and specificity were 90% in average.																	0932-8092	1432-1769				JAN 21	2020	31	1							4	10.1007/s00138-019-01055-3													
J								Convolutional networks for appearance-based recommendation and visualisation of mascara products	MACHINE VISION AND APPLICATIONS										Deep learning; Generative adversarial networks; Siamese networks; Recommender systems; Cosmetics		In this work, we explore the problems of recommending and visualising makeup products based on images of customers. Focusing on mascara, we propose a two-stage approach that first recommends products to a new customer based on the preferences of other customers with similar visual appearance and then visualises how the recommended products might look on the customer. For the initial product recommendation, we train a Siamese convolutional neural network, using our own dataset of cropped eye regions from images of 91 female subjects, such that it learns to output feature vectors that place images of the same subject close together in high-dimensional space. We evaluate the trained network based on its ability to correctly identify existing subjects from unseen images, and then assess its capability to identify visually similar subjects when an image of a new subject is used as input. For product visualisation, we train per-product generative adversarial networks to map the appearance of a specific product onto an image of a customer with no makeup. We train models to generate images of two mascara formulations and assess their capability to generate realistic mascara lashes while changing as little as possible within non-lash image regions and simulating the different effects of the two products used.																	0932-8092	1432-1769				JAN 21	2020	31	1							5	10.1007/s00138-019-01053-5													
J								Rosette plant segmentation with leaf count using orthogonal transform and deep convolutional neural network	MACHINE VISION AND APPLICATIONS										Deep convolutional neural network; Leaf count; Orthogonal transform coefficients; Plant segmentation	IMAGE-ANALYSIS; TRACKING; GROWTH	Plant image analysis plays an important role in agriculture. It is used to record the morphological plant traits regularly and accurately. The plant growth is one of the key traits to be analyzed, which relies on leaf area (i.e., leaf region or plant region) and leaf count. One of the ways to find the leaf count is counting the leaves using segmented plant region. In this paper, a new plant region segmentation scheme is proposed in the orthogonal transform domain based on orthogonal transform coefficients. Initially, an analysis of orthogonal transform coefficients is carried out in terms of the response of orthogonal basis vectors to extract the plant region. After extracting the plant region, the L*a*b and CMYK color spaces are used for noise removal in the segmentation scheme. Finally, the leaves are counted using fine-tuned deep convolutional neural network models. The proposed scheme is experimented on CVPPP benchmark datasets and also tested with the images taken from mobile phone to ensure its reliability and cross-platform applicability. The experiment results on CVPPP benchmark datasets are promising.																	0932-8092	1432-1769				JAN 21	2020	31	1							6	10.1007/s00138-019-01056-2													
J								Droplet-Transmitted Infection Risk Ranking Based on Close Proximity Interaction	FRONTIERS IN NEUROROBOTICS										influenza-like infection; person re-identification; multi-person pose estimation; infection risk ranking; multi-tasking	PERSON REIDENTIFICATION; CONTACT PATTERNS; INFLUENZA; NETWORK	We propose an automatic method to identify people who are potentially-infected by droplet-transmitted diseases. This high-risk group of infection was previously identified by conducting large-scale visits/interviews, or manually screening among tons of recorded surveillance videos. Both are time-intensive and most likely to delay the control of communicable diseases like influenza. In this paper, we address this challenge by solving a multi-tasking problem from the captured surveillance videos. This multi-tasking framework aims to model the principle of Close Proximity Interaction and thus infer the infection risk of individuals. The complete workflow includes three essential sub-tasks: (1) person re-identification (REID), to identify the diagnosed patient and infected individuals across different cameras, (2) depth estimation, to provide a spatial knowledge of the captured environment, (3) pose estimation, to evaluate the distance between the diagnosed and potentially-infected subjects. Our method significantly reduces the time and labor costs. We demonstrate the advantages of high accuracy and efficiency of our method. Our method is expected to be effective in accelerating the process of identifying the potentially infected group and ultimately contribute to the well-being of public health.																	1662-5218					JAN 21	2020	13								113	10.3389/fnbot.2019.00113													
J								Multi-level feature optimization and multimodal contextual fusion for sentiment analysis and emotion classification	COMPUTATIONAL INTELLIGENCE										bidirectional LSTM; contextual information; evolutionary computing; feature selection; multimodal fusion		The availability of the humongous amount of multimodal content on the internet, the multimodal sentiment classification, and emotion detection has become the most researched topic. The feature selection, context extraction, and multi-modal fusion are the most important challenges in multimodal sentiment classification and affective computing. To address these challenges this paper presents multilevel feature optimization and multimodal contextual fusion technique. The evolutionary computing based feature selection models extract a subset of features from multiple modalities. The contextual information between the neighboring utterances is extracted using bidirectional long-short-term-memory at multiple levels. Initially, bimodal fusion is performed by fusing a combination of two unimodal modalities at a time and finally, trimodal fusion is performed by fusing all three modalities. The result of the proposed method is demonstrated using two publically available datasets such as CMU-MOSI for sentiment classification and IEMOCAP for affective computing. Incorporating a subset of features and contextual information, the proposed model obtains better classification accuracy than the two standard baselines by over 3% and 6% in sentiment and emotion classification, respectively.																	0824-7935	1467-8640				MAY	2020	36	2					861	881		10.1111/coin.12274		JAN 2020											
J								Class of Monotone Kernelized Classifiers on the basis of the Choquet Integral	EXPERT SYSTEMS										Choquet Integral; Choquet kernels; isotonic regression; kernel machines; monotone classification	PRIOR KNOWLEDGE; FUZZY MEASURES; CLASSIFICATION; ALGORITHMS; REGRESSION; NETWORKS; MODEL	The key property of monotone classifiers is that increasing (decreasing) input values lead to increasing (decreasing) the output value. Preserving monotonicity for a classifier typically requires many constraints to be respected by modeling approaches such as artificial intelligence techniques. The type of constraints strongly depends on the modeling assumptions. Of course, for sophisticated models such conditions might be very complex. In this study we present a new family of kernels that we call it Choquet kernels. Henceforth it allows for employing popular kernel-based methods such as support vector machines. Instead of a naive approach with exponential computational complexity we propose an equivalent formulation with quadratic time in the number of attributes. Furthermore, since coefficients derived from kernel solutions are not necessarily monotone in the dual form, different approaches are proposed to monotonize coefficients. Finally experiments illustrate beneficial properties of the Choquet kernels.																	0266-4720	1468-0394				JUN	2020	37	3			SI				e12506	10.1111/exsy.12506		JAN 2020											
J								Designing Proportional Integral Controller for a Solar Powered DC-DC Converter Using PIPSO Algorithm and Deep Learning approach in Intelligent Bio-Medical Applications	EXPERT SYSTEMS										integral squared error; parameter improved particle swarm optimization; proportional integral controller; proportional integral controller; solar panel	SLIDING-MODE CONTROL; ENERGY; SYSTEM; MPPT	Numerous engineering complexities are simplified using optimization algorithms. In a solar power system, the necessity of the voltage regulator is obvious. To control the regulator existent research works used PI, PID controllers that might have an unwanted transient response. To overcome such drawbacks here, a fresh scheme is proposed for the designing of the adaptive sliding mode (SM) controller of a solar powered LUO converter using optimization algorithms. The PSO ('Particle Swarm Optimization') is proved to expedite the convergence characteristic for many applications. Here, an ameliorated PSO version is developed. This algorithm is termed the Parameter Improved-PSO (PIPSO) algorithm. In this algorithm, the parameters, say, inertia weight, social along with cognitive agents is updated in every generation. The Proportional Integrator (PI) controller is used. The gain of this controller is tuned using the PIPSO. This algorithm's objective function is to lessen ISE ('Integral Squared Error') of the converter's output voltage. This parameter is picked as the objective function of the optimization algorithm. The proposed PIPSO is established to show better outcomes when contrasted to the traditional PSO concerning tuning a collection of parameters. An analysis is also made to evaluate the effect of usage of the solar panel () in the proposed work.																	0266-4720	1468-0394														e12488	10.1111/exsy.12488		JAN 2020											
J								RIdeogram: drawing SVG graphics to visualize and map genome-wide data on the idiograms	PEERJ COMPUTER SCIENCE										Genome; Chromosome; Idiogram; R package; Data visualization		Background. Owing to the rapid advances in DNA sequencing technologies, whole genome from more and more species are becoming available at increasing pace. For whole-genome analysis, idiograms provide a very popular, intuitive and effective way to map and visualize the genome-wide information, such as GC content, gene and repeat density, DNA methylation distribution, genomic synteny, etc. However, most available software programs and web servers are available only for a few model species, such as human, mouse and fly, or have limited application scenarios. As more and more non-model species are sequenced with chromosome-level assembly being available, tools that can generate idiograms for a broad range of species and be capable of visualizing more data types are needed to help better understanding fundamental genome characteristics. Results. The R package RIdeogram allows users to build high-quality idiograms of any species of interest. It can map continuous and discrete genome-wide data on the idiograms and visualize them in a heat map and track labels, respectively. Conclusion. The visualization of genome-wide data mapping and comparison allow users to quickly establish a clear impression of the chromosomal distribution pattern, thus making RIdeogram a useful tool for any researchers working with omics.																	2376-5992					JAN 20	2020									e251	10.7717/peerj-cs.251													
J								A norm enforcement mechanism for a time-constrained conditional normative framework	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Conditional norms; Time constraints; Norm enforcement; Contrary-to-duty norms; Logic programming	SPECIFICATION	This paper presents the formalization for a system that monitors and enforces regulative time-constrained conditional norms through sanctioning, for agent societies. The representation here has the advantage of allowing for qualitative and quantitative interval-based temporal constraints between a norm's condition and its effect. The system possesses mechanisms for monitoring both norm compliance and violation of reified norms. Each norm has an identity, which enables the identification of norms violated or complied with. The formalism seamlessly treats violation- or conformance-handling norms (such as violation notification, sanctioning and rewarding norms) as regular norms. The implementation formalizes norms as logic program clauses, the head of which specifies what normative position (i.e. the norm's effect) an agent must observe within some time constraint of some arising situation; and the body of which describes a situation defined as the norm's pre-condition and the role that the implicated agent plays within it. For the purpose of imposing sanctions, the only violations deemed liable are those violations that the agent fails to mitigate by compliance with all relevant contrary-to-duty norms. An agent may ultimately be sanctioned for a liable norm while norm compliance is rewarded in the system. Sanctioning itself takes place in two norm-guided phases: the obligation of an enforcer agent to notify an erring agent of reparative actions it is obliged to take and the time constraint within which it should be taken, and a norm for monitoring the agent's compliance with the notification. The violation of the latter norm leads to the meting out of sanctions.																	1387-2532	1573-7454				JAN 20	2020	34	1							20	10.1007/s10458-020-09441-2													
J								Semantic Smart World Framework	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING											DATA ANALYTICS FRAMEWORK; CITIES	This paper presents a general Semantic Smart World framework (SSWF), to cover the Migratory birds' paths. This framework combines semantic and big data technologies to support meaning for big data. In order to build the proposed smart world framework, technologies such as cloud computing, semantic technology, big data, data visualization, and the Internet of Things are hybrid. We demonstrate the proposed framework through a case study of automatic prediction of air quality index and different weather phenomena in the different locations in the world. We discover the association between air pollution and increasing weather conditions. The experimental results indicate that the framework performance is suitable for heterogeneous big data.																	1687-9724	1687-9732				JAN 20	2020	2020								8081578	10.1155/2020/8081578													
J								Majority rule dynamics between a double coalition and a third opinion: coalition profit models and majority coalition ties	ADAPTIVE BEHAVIOR										Opinion dynamics; majority rule models; coalition dynamics; majority ties; social complexity; adaptive behavior; critical mass; logistic functions; Monte Carlo simulation	SOCIAL-INFLUENCE; HETEROGENEITY; SOCIOPHYSICS; BEHAVIOR	This article explores the opinion dynamics of a double coalition opinion against a third opinion under majority rule updates on odd fixed size connected groups. For this purpose, coalition benefit criteria and three opinion formation models which extend the 2-state majority rule model on lattices are introduced. The proposed models focus on the coalition profit of its constituent coalition opinions and cover the possible final scenarios from coalition alliance perspective: either minor opinion or major opinion is favored, or dynamics do not favor to any coalition opinion. Opinion exchanges take place on a torus embedded lattice network of a 3-state system having in consideration tie configurations and two rules to break them: either by random choice or leaving ties unaltered. Models were analyzed in the statistical mechanics spirit through Monte Carlo simulations without node replacement. Estimations for coalition benefits, the growth of coalition ties, and consensus probabilities are reported. The loss of coalition strengths due to coalition ties and its indecision is indicated. In particular, the logistic decay of consensus probability is due to the logistic adaptive growth of coalition ties. Scaling behaviors for consensus time and coalition ties in terms of network size are suggested. The results of numerical simulations are discussed in the context of social influence and social dynamics.																	1059-7123	1741-2633														UNSP 1059712319895486	10.1177/1059712319895486		JAN 2020											
J								Development of body-based spatial knowledge through mental imagery in an artificial agent	ADAPTIVE BEHAVIOR										Mental imagery; spatial affordances; cognitive robotics; prediction; internal models	EGOCENTRIC DISTANCE; EMBODIED COGNITION; INTERNAL-MODELS; VISUAL CONTROL; PERCEPTION; AFFORDANCE; ROBOTICS; SPACE; INFORMATION; SIMULATION	Distance perception for mobile agents is of great importance for safe navigation in unknown environments. Traditional methods make use of analytical solutions. Yet, according to some research hypothesis, distance perception is not the result of mathematical calculations, but an emergent consequence of an association process, where visual and tactile information acquire a central role. Designing models closer to natural cognition poses paramount challenges to artificial intelligence (AI), which call for a review of some of the foundations of current methods. Our work is framed in the embodied cognition paradigm, which highlights the importance of the body for the development of cognitive processes. We provide theoretical grounds and empirical evidence for an artificial account of distance perception through a multimodal association process. By learning multimodal sensorimotor schemes, an agent is capable of perceiving affordances related to distance perception without any non-body-based geometric knowledge. We let an agent interact with an environment cluttered with objects, while learning multimodal sensorimotor associations. The learned spatial relations are thoroughly characterized to show how the model depends on the agent's specific sensorimotor capabilities. The system is tested in a passability experiment and a navigation task, showing the agent anticipates undesired situations using the learned model predictions.																	1059-7123	1741-2633														UNSP 1059712319895604	10.1177/1059712319895604		JAN 2020											
J								Single image deraining via deep pyramid network with spatial contextual information aggregation	APPLIED INTELLIGENCE										Single image deraining; Pyramid network; Spatial contextual information aggregation; Residual learning	RAIN; REMOVAL	Rain streaks usually give rise to visual degradation and cause many computer vision algorithms to fail. So it is necessary to develop an effective deraining algorithm as preprocess of high-level vision tasks. In this paper, we propose a novel deep learning based deraining method. Specifically, the multi-scale kernels and feature maps are both important for single image deraining. However, the previous works ignore the two multi-scale information or only consider the multi-scale kernels information. Instead, our method learns multi-scale information both from the perspectives of kernels and feature maps, respectively, by designing spatial contextual information aggregation module and pyramid network module. The former module can capture the rain streaks with different sizes and the latter module can extract rain streaks from different scales further. Moreover, we also employ squeeze-and-excitation and skip connections to enhance the correlation between channels and transmit the information from low-level to high-level, respectively. The experimental results show that the proposed method achieves significant improvements over the recent state-of-the-art methods in Rain100H, Rain100L, Rain1200 and Rain1400 datasets.																	0924-669X	1573-7497				MAY	2020	50	5					1437	1447		10.1007/s10489-019-01567-5		JAN 2020											
J								BlockU: Extended usage control in and for Blockchain	EXPERT SYSTEMS										fabric; Hyperledger Composer; permissioned Blockchain; UCON		An electronic business transaction among untrusted bodies without consulting a mutually trusted party has remained widely accepted problem. Blockchain resolves this problem by introducing peer-to-peer network with a consensus algorithm and trusted ledger. Blockchain originally introduced for cryptocurrency that came with proof-of-work consensus algorithm. Due to some performance issues, scientists brought concept of permissioned Blockchain. Hyperledger Fabric is a permissioned Blockchain targeting business-oriented problems for industry. It is designed for efficient transaction execution over Blockchain with pluggable consensus model; however, there is limitation of rapid application development. Hyperledger introduced a new layer called Hyperledger Composer on top of the Fabric layer, which provides an abstract layer to model the business application readily and quickly. Composer provides a smart contract to extend the functionality and flexibility of Fabric layer and provides a way of communication with other systems to meet business requirements. Hyperledger Composer uses role-based access control (RBAC) model to secure access to its valuable assets. However, RBAC is not enough because many business deals require continuous assets monitoring. Our proposed model, BlockU, covers all possible access control models required by a business. BlockU can monitor assets continuously during transactions and updates attributes accordingly. Moreover, we incorporate hooks in Hyperledger Composer to implement extended permission model that provides extensive permission management capability on an asset. Subsequently, our proposed enhanced access control model is implemented with a minimal change to existing Composer code base and is backward compatible with the current security mechanism.																	0266-4720	1468-0394				JUN	2020	37	3			SI					10.1111/exsy.12507		JAN 2020											
J								CNN-VSR: A Deep Learning Architecture with Validation-Based Stopping Rule for Time Series Classication	APPLIED ARTIFICIAL INTELLIGENCE											CONVOLUTIONAL NEURAL-NETWORKS; MODEL; REPRESENTATION; SELECTION; FEATURES	Deep learning methods for univariate time series classification (TSC) are recently gaining attention. Especially, convolutional neural network (CNN) is utilized to solve the problem of predicting class labels of time series obtained through various important applications, such as engineering, biomedical, and finance. In this work, a novel CNN model is proposed with validation-based stopping rule (VSR) named as CNN-VSR, for univariate TSC using 2-D convolution operation, inspired by image processing properties. For this, first, we develop a novel 2-D transformation approach to convert 1-D time series of any length to 2-D matrix automatically without any manual preprocessing. The transformed time series will be given as an input to the proposed architecture. Further, the implicit and explicit regularization is applied, as time series signal is highly chaotic and prone to over-fitting with learning. Specifically, we define a VSR, which provides a set of parameters associated with a low validation set loss. Moreover, we also conduct a comparative empirical performance evaluation of the proposed CNN-VSR with the best available methods for individual benchmark datasets whose information are provided in a repository maintained by UCR and UEA. Our results reveal that proposed CNN-VSR advances the baseline methods by achieving higher performance accuracy. In addition, we demonstrate that the stopping rule considerably contributes to the classifying performance of the proposed CNN-VSR architecture. Furthermore, we also discuss the optimal model selection and study the effects of different factors on the performance of the proposed CNN-VSR.																	0883-9514	1087-6545				JAN 28	2020	34	2					101	124		10.1080/08839514.2020.1713454		JAN 2020											
J								eXplainable Cooperative Machine Learning with NOVA	KUNSTLICHE INTELLIGENZ										Annotation; Cooperative machine learning; Explainable AI	EMOTIONAL SPEECH; COEFFICIENT	In the following article, we introduce a novel workflow, which we subsume under the term "explainable cooperative machine learning" and show its practical application in a data annotation and model training tool called NOVA. The main idea of our approach is to interactively incorporate the 'human in the loop' when training classification models from annotated data. In particular, NOVA offers a collaborative annotation backend where multiple annotators join their workforce. A main aspect is the possibility of applying semi-supervised active learning techniques already during the annotation process by giving the possibility to pre-label data automatically, resulting in a drastic acceleration of the annotation process. Furthermore, the user-interface implements recent eXplainable AI techniques to provide users with both, a confidence value of the automatically predicted annotations, as well as visual explanation. We show in an use-case evaluation that our workflow is able to speed up the annotation process, and further argue that by providing additional visual explanations annotators get to understand the decision making process as well as the trustworthiness of their trained machine learning models.																	0933-1875	1610-1987				JUN	2020	34	2			SI		143	164		10.1007/s13218-020-00632-3		JAN 2020											
J								Real-time defect detection network for polarizer based on deep learning	JOURNAL OF INTELLIGENT MANUFACTURING										Automatic testing; Image classification; Defect detection; Global average pooling; Parallel module	CONVOLUTIONAL NEURAL-NETWORK; POLYMERIC POLARIZER; AESTHETIC DEFECTS; INSPECTION; LIGHT	Quality analysis of the polarizer of a production line can be performed using image processing technology. The existing method of detecting defective images based on deep learning can ensure accurate classification; however, its detection speed is low, the model requires a large amount of memory, and it is difficult to meet the real-time requirements of online detection systems when hardware resources are limited. Therefore, in this study a lightweight polarizer defect detection network, called DDN, was developed based on deep learning. First, a parallel module was designed to build the network. This module has two main advantages. First, it mixes different convolution template sizes, and can fuse the features of different scales and extract more defect features than the traditional convolution layer. Second, depthwise separable convolution is used to replace full convolution in this module, which significantly reduces the number of parameters and the multiply-accumulate operations. Finally, a global average pooling (GAP) layer is used instead of a fully connected layer. The GAP layer has no parameters to optimize, which substantially reduces the number of network parameters. Experimental results show that the proposed method is better than existing methods in terms of classification speed, precision, and memory consumption for polarizer detection, and can satisfy real-time requirements.																	0956-5515	1572-8145															10.1007/s10845-020-01536-7		JAN 2020											
J								Explainable AI under contract and tort law: legal incentives and technical challenges	ARTIFICIAL INTELLIGENCE AND LAW										Explainability; Explainable AI; Interpretable machine learning; Contract law; Tort law; Explainability-accuracy trade-off; Medical malpractice; Corporate takeovers	AUTOMATED DECISION-MAKING; MACHINE; CLASSIFICATION; MEDICINE; CANCER	This paper shows that the law, in subtle ways, may set hitherto unrecognized incentives for the adoption of explainable machine learning applications. In doing so, we make two novel contributions. First, on the legal side, we show that to avoid liability, professional actors, such as doctors and managers, may soon be legally compelled to use explainable ML models. We argue that the importance of explainability reaches far beyond data protection law, and crucially influences questions of contractual and tort liability for the use of ML models. To this effect, we conduct two legal case studies, in medical and corporate merger applications of ML. As a second contribution, we discuss the (legally required) trade-off between accuracy and explainability and demonstrate the effect in a technical case study in the context of spam classification.																	0924-8463	1572-8382				DEC	2020	28	4					415	439		10.1007/s10506-020-09260-6		JAN 2020											
J								Multi-scale feature fusion based on swarm intelligence collaborative learning for full-stage anti-interference object tracking	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Object tracking; Feature fusion; Swarm intelligence; Collaborative learning; Anti-interference; Locality-constrained		Highly maneuverable object locating and tracking under anti-stealth and anti-interference environment is a challenging research subject. Since the locality-constrained linear coding and the cooperative representation are intrinsically linear model, this will cause the discriminant information is insufficient in object tracking. Therefore, a multi-scale feature fusion based on swarm intelligence collaborative learning for full-stage anti-interference object tracking is proposed in paper. This paper combines multiple features to describe the object so as to improve the representation performance of the single feature, and uses local constrained linear coding to obtain better classification performance; then use the swarm intelligence kernel function to extend the cooperative learning of local constraints to the kernel space, and derive the kernel sparse representation. Simulation results show that the improved algorithm has obvious advantages in real-time, stability and quantitative indexes, and is suitable for high-performance, low-cost video surveillance.																	1868-5137	1868-5145															10.1007/s12652-019-01671-x		JAN 2020											
J								Comparison of sensory evaluation techniques for Hungarian wines	JOURNAL OF CHEMOMETRICS										ANOVA; PLS-DA; sensometrics; sensory analysis; wine	POLYPHENOLS; OXYGENATION; ASTRINGENCY; PERCEPTION; COLOR; TASTE	The aim of this study was to compare different Hungarian Kadarka, Kekfrankos, and Cabernet franc wines produced and aged by the same methods and to compare two types of sensory analysis methods as well: the 100-point OIV system and quantitative descriptive analysis (QDA). Both tests were conducted by 12 assessors of the University of Pecs, Institute for Regional Development, Faculty of Horticulture and Oenology. This study provides conclusions about the use of sensory analysis methods, highlighting the advantages and disadvantages of QDA and the OIV system. Principal component analysis, analysis of variance (ANOVA), multiple factor analysis, and partial least squares dicriminant analysis were used for the evaluation of the data. Our results showed that the sensory panel was able to discriminate the samples by both sensory methods; however, the information provided by them was significantly different. ANOVA clearly showed that the two methods have different sensitivity when comparing wines (commercial and produced wine samples) and QDA proved to be the more sensitive, as well as more detailed, method. Partial least squares discriminant analysis augmented the findings in the classification part of the different type of wine samples. In general, OIV is able to show the general quality of the wines, while QDA coupled with proper chemometric methods is able to describe why the given samples received good or bad OIV scores.																	0886-9383	1099-128X														e3219	10.1002/cem.3219		JAN 2020											
J								An efficient XGBoost-DNN-based classification model for network intrusion detection system	NEURAL COMPUTING & APPLICATIONS										IDS; Deep neural network (DNN); XGBoost; NSL-KDD	RANDOM FOREST	There is a steep rise in the trend of the utility of Internet technology day by day. This tremendous increase ushers in a massive amount of data generated and handled. For apparent reasons, undivided attention is due for ensuring network security. An intrusion detection system plays a vital role in the field of the stated security. The proposed XGBoost-DNN model utilizes XGBoost technique for feature selection followed by deep neural network (DNN) for classification of network intrusion. The XGBoost-DNN model has three steps: normalization, feature selection, and classification. Adam optimizer is used for learning rate optimization during DNN training, and softmax classifier is applied for classification of network intrusions. The experiments were duly conducted on the benchmark NSL-KDD dataset and implemented using Tensor flow and python. The proposed model is validated using cross-validation and compared with existing shallow machine learning algorithms like logistic regression, SVM, and naive Bayes. The classification evaluation metrics such as accuracy, precision, recall, and F1-score are calculated and compared with the existing shallow methods. The proposed method outperformed over the existing shallow methods used for the dataset.																	0941-0643	1433-3058				AUG	2020	32	16					12499	12514		10.1007/s00521-020-04708-x		JAN 2020											
J								Human action recognition using short-time motion energy template images and PCANet features	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks (CNN); Principal component analysis network (PCANet); Short-time motion energy image (ST-MEI) templates; Deep learning; Action recognition	GOING DEEPER; SHAPE	Human action recognition has received significant attention because of its wide applications in human-machine interaction, visual surveillance, and video indexing. Recent progress in deep learning algorithms and convolutional neural networks (CNNs) significantly affects the performance of many action recognition systems. However, CNNs are designed to learn features from 2D spatial images, while action videos are considered as 3D spatiotemporal signals. In addition, the complex structure of most deep networks and their dependency on backpropagation learning algorithm have a negative impact on the efficiency of real-time human action recognition systems. To avoid these limitations, we propose a new human action recognition method based on principal component analysis network (PCANet) which is a simple architecture of CNN exploiting unsupervised learning instead of the commonly used supervised algorithms. However, PCANet is originally designed to solve 2D image classification problems. In order to make it suitable for solving action recognition problem in videos, the temporal information of the input video is appropriately represented using motion energy templates. Multiple short-time motion energy image (ST-MEI) templates are computed to capture human motion information. The deep structure of PCANet helps to learn hierarchical local motion features from the input ST-MEI templates. The dimension of the feature vector learned from PCANet is reduced using whitening principal component analysis algorithm and finally fed to linear support vector machines classifier for classification. The proposed method is evaluated using three different benchmark datasets, namely KTH, Weizmann, and UCF sports action. Experimental results using leave-one-out strategy demonstrate the effectiveness of the proposed method compared with other state-of-the-art methods.																	0941-0643	1433-3058				AUG	2020	32	16					12561	12574		10.1007/s00521-020-04712-1		JAN 2020											
J								Estimation of photovoltaic module model's parameters using an improved electromagnetic-like algorithm	NEURAL COMPUTING & APPLICATIONS										PV modelling; Parameter extraction; Single-diode model; Electromagnetic-like; I-V characteristics	SOLAR-CELLS; DIODE MODEL; PV CELLS; EXTRACTION; MECHANISM; CONVERGENCE; PERFORMANCE; OPTIMIZATION; ENERGY	This paper offers an electromagnetism-like (IEM) algorithm to estimate the five parameters of a single-diode PV module's model. IEM uses local search and improves movement step to increase the convergence to optimal solutions. The key of improvement is performed by adding a nonlinear equation to adjust the length of the particle in each iteration. Moreover, the total force formula is simplified to speed up the exploration for an optimal solution. Analyses are carried out by experimental data points at various operational conditions to show the stability and reliability of the proposed methods. The results of the proposed IEM algorithm show a better convergence speed and high accuracy compared with other models in the literature, which involves various statistical errors. The values of average root mean square error, mean bias error, standard deviation, average absolute error, and average test statistic of the proposed method are 589%, 0.51%, 0.19%, 46%, and 0.53, respectively. As a conclusion, the IEM algorithm presents better performance than other methods in the literature in terms of accuracy and convergence.																	0941-0643	1433-3058				AUG	2020	32	16					12627	12642		10.1007/s00521-020-04714-z		JAN 2020											
J								Machine learning ensemble for neurological disorders	NEURAL COMPUTING & APPLICATIONS										Machine learning; Soft-computing; Ensemble; Neurological disorders; Parkinson disease	PARKINSONS-DISEASE; REGRESSION; CLASSIFICATION; PROGRESSION; FREQUENCY; SPEECH; CART	Parkinson disease is a neurodegenerative disorder of the central nerve system which affects body movements. The proposed technique selects best five machine learning models competitively, out of 25 state-of-the-art regression models to generate a robust ensemble. Data from 42 patients having early stage of Parkinson disease were collected which contains a total of 5875 voice recordings. Numerous state-of-the-art machine learning models have been explored to predict the motor Unified Parkinson's Disease Rating Score (UPDRS) for the collected voice measures. Evaluation parameters such as correlation, R-Square, RMSE, and accuracy have been calculated for comparative analysis. Results from the ensemble model consisting of best five models have been recalculated to analyze the prediction. K-fold validation has been incorporated to measure the robustness of ensembled model. The proposed ensemble yields UPDRS with higher accuracy of 99.6% making it well suitable to assist the diagnose for Parkinson disease.																	0941-0643	1433-3058				AUG	2020	32	16					12697	12714		10.1007/s00521-020-04720-1		JAN 2020											
J								High-performance attribute reduction on graphics processing unit	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Parallel attribute reduction; rough sets; GPU; big data mining	ROUGH-SET; DECISION; APPROXIMATIONS; ALGORITHMS	Recently, graphics processing unit (GPU) gained lots of attention from academia and industry for its applicability in high-performance computing. It has been successfully applied to many fields, such as image processing, machine learning, object detection, etc. In our previous work, GPU was adopted to accelerate the computation of rough set approximation (RSA), which is the core step in most of the rough sets based tasks, e.g. attribute reduction. The method is essentially a CPU-GPU cooperative paradigm. That is to say, there are lots of data exchanged between host memory and GPU memory, which greatly degrades the performance of the system. This paper introduces a unified GPU framework for parallel attribute reduction, in which two critical steps in attribute reduction, i.e. computation of equivalence class and attributes significance, are both executed on GPU. Moreover, the algorithm is well designed by exploiting the architectural characteristics of the modern GPU architecture. Experiments were carried out on data sets with different sizes. The results show that the proposed algorithm can outperform the CPU-GPU cooperative algorithm on large data sets.																	0952-813X	1362-3079															10.1080/0952813X.2019.1710577		JAN 2020											
J								Reduction methods of type-2 fuzzy variables and their applications to Stackelberg game	APPLIED INTELLIGENCE										Bi-level programming; Stackelberg game; Type-2 fuzzy set; Critical value; Genetic algorithm	SOLID TRANSPORTATION PROBLEM; INFERENCE SYSTEMS; OPTIMIZATION; SETS; DEFUZZIFICATION; PRODUCT	This paper is designed based on the mathematical models for bi-level programming in Stackelberg game under type-2 fuzzy environment. The parameters of the objective functions on both levels are considered as type-2 fuzzy numbers in the first case whereas the parameters of the objective functions and the constraints are chosen as type-2 fuzzy numbers in the second case. Critical value based reduction methods are applied to reduce type-2 fuzzy numbers to type-1 fuzzy numbers in the first case. After that, centroid method is used for completely defuzzifying type-2 fuzzy numbers. Besides this, the obtained results are compared with the help of LINGO iterative scheme and genetic algorithm. Coming to the second case, a chance constraint programming with the help of generalized credibility measure is utilized to convert the fuzzy problem to its equivalent crisp form. LINGO iterative scheme is used to solve the deterministic problem using fuzzy programming. The sensitivity analysis is shown to different credibility levels of right hand side of the constraints to find the value of objective function in each level. Finally, real-life based numerical problems are presented to show the performance of the proposed models and techniques. At last, conclusion about the findings and outlook are described.																	0924-669X	1573-7497				MAY	2020	50	5					1398	1415		10.1007/s10489-019-01578-2		JAN 2020											
J								The AI Settlement Generation Challenge in Minecraft First Year Report	KUNSTLICHE INTELLIGENZ										Competition; Generative design; Procedural content generation; Minecraft		This article outlines what we learned from the first year of the AI Settlement Generation Competition in Minecraft, a competition about producing AI programs that can generate interesting settlements in Minecraft for an unseen map. This challenge seeks to focus research into adaptive and holistic procedural content generation. Generating Minecraft towns and villages given existing maps is a suitable task for this, as it requires the generated content to be adaptive, functional, evocative and aesthetic at the same time. Here, we present the results from the first iteration of the competition. We discuss the evaluation methodology, present the different technical approaches by the competitors, and outline the open problems.																	0933-1875	1610-1987				MAR	2020	34	1			SI		19	31		10.1007/s13218-020-00635-0		JAN 2020											
J								Active and Incremental Learning with Weak Supervision	KUNSTLICHE INTELLIGENZ										Active learning; Wildlife surveillance; Weak supervision; Object detection; Incremental learning	SEGMENTATION; IMAGES	Large amounts of labeled training data are one of the main contributors to the great success that deep models have achieved in the past. Label acquisition for tasks other than benchmarks can pose a challenge due to requirements of both funding and expertise. By selecting unlabeled examples that are promising in terms of model improvement and only asking for respective labels, active learning can increase the efficiency of the labeling process in terms of time and cost. In this work, we describe combinations of an incremental learning scheme and methods of active learning. These allow for continuous exploration of newly observed unlabeled data. We describe selection criteria based on model uncertainty as well as expected model output change (EMOC). An object detection task is evaluated in a continuous exploration context on the PASCAL VOC dataset. We also validate a weakly supervised system based on active and incremental learning in a real-world biodiversity application where images from camera traps are analyzed. Labeling only 32 images by accepting or rejecting proposals generated by our method yields an increase in accuracy from 25.4 to 42.6%.																	0933-1875	1610-1987				JUN	2020	34	2			SI		165	180		10.1007/s13218-020-00631-4		JAN 2020											
J								Interval 2-tuple Pythagorean fuzzy linguistic MULTIMOORA method with CIA and their application to MCGDM	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										comprehensive integration approach; hamming distance; interval 2-tuple Pythagorean fuzzy linguistic set; multicriteria group decision making; MULTIMOORA method	GROUP DECISION-MAKING; AGGREGATION OPERATORS; TOPSIS METHOD; TODIM METHOD; VIKOR METHOD; MODEL	The reliable evaluation of financial performance of universities plays an important role in sustainable development of universities, and it could be regarded as a multicriteria group decision making problem. Based on this, in this paper, with respect to the attribute evaluation values are expressed by interval 2-tuple Pythagorean fuzzy linguistic variables, we develop an extended MULTIMOORA method. First, we propose the interval 2-tuple Pythagorean fuzzy linguistic set based on Pythagorean fuzzy set and interval-valued 2-tuple linguistic variables, and further introduce the score and accuracy functions and Hamming distance of it. Then, considering that there are some deficiencies in the traditional MULTIMOORA method, such as exist the case of circular reasoning, and so forth. We improve it by proposing a comprehensive integration approach, which can not only consider both the utility values and sort results, but also reflect the preference attitude of decision makings simultaneously. Based on these research findings, an interval 2-tuple Pythagorean fuzzy linguistic MULTIMOORA method is presented and the calculation process of this method is described in detail. Finally, we give the numerical example concerning the evaluation of financial management performance in universities to illustrate practicability and reliability of the proposed approach and compare the proposed method with different methods to perform its flexibility.																	0884-8173	1098-111X				APR	2020	35	4					650	681		10.1002/int.22221		JAN 2020											
J								A Piezoresistive Array Armband With Reduced Number of Sensors for Hand Gesture Recognition	FRONTIERS IN NEUROROBOTICS										muscle sensors array; piezoresistive sensor; human-machine interface; hand gesture recognition; support vector machine; exergaming	FORCE MYOGRAPHY; LOW-COST	Human machine interfaces (HMIs) are employed in a broad range of applications, spanning from assistive devices for disability to remote manipulation and gaming controllers. In this study, a new piezoresistive sensors array armband is proposed for hand gesture recognition. The armband encloses only three sensors targeting specific forearm muscles, with the aim to discriminate eight hand movements. Each sensor is made by a force-sensitive resistor (FSR) with a dedicated mechanical coupler and is designed to sense muscle swelling during contraction. The armband is designed to be easily wearable and adjustable for any user and was tested on 10 volunteers. Hand gestures are classified by means of different machine learning algorithms, and classification performances are assessed applying both, the 10-fold and leave-one-out cross-validations. A linear support vector machine provided 96% mean accuracy across all participants. Ultimately, this classifier was implemented on an Arduino platform and allowed successful control for videogames in real-time. The low power consumption together with the high level of accuracy suggests the potential of this device for exergames commonly employed for neuromotor rehabilitation. The reduced number of sensors makes this HMI also suitable for hand-prosthesis control.																	1662-5218					JAN 17	2020	13								114	10.3389/fnbot.2019.00114													
J								A Fireworks Algorithm Based on Transfer Spark for Evolutionary Multitasking	FRONTIERS IN NEUROROBOTICS										evolutionary multitasking; multitask optimization; fireworks algorithm; transfer spark; evolutionary algorithm	MULTIFACTORIAL INHERITANCE; CULTURAL TRANSMISSION; MODEL	In recent years, lots of multifactorial optimization evolutionary algorithms have been developed to optimize multiple tasks simultaneously, which improves the overall efficiency using implicit genetic complementarity between different tasks. In this paper, a novel multitask fireworks algorithm is proposed with novel transfer sparks to solve multitask optimization problems. For each task, some transfer sparks would be generated with adaptive length and promising direction vector, which are very helpful to transfer useful genetic information between different tasks. Finally, the proposed algorithm is compared against some chosen state-of-the-art evolutionary multitasking algorithms. The experimental results show that the proposed algorithm provides better performance on several single objectives and multiobjective MTO test suites.																	1662-5218					JAN 17	2020	13								109	10.3389/fnbot.2019.00109													
J								Joint dictionary and graph learning for unsupervised feature selection	APPLIED INTELLIGENCE										Unsupervised feature selection; Dictionary learning; Similarity graph learning; Local structure preservation	CLASSIFICATION; ALGORITHM	With the explosion of unlabelled and high-dimensional data, unsupervised feature selection has become an critical and challenging problem in machine learning. Recently, data representation based model has been successfully deployed for unsupervised feature selection, which defines feature importance as the capability to represent original data via a reconstruction function. However, most existing algorithms conduct feature selection on original feature space, which will be affected by the noisy and redundant features of original feature space. In this paper, we investigate how to conduct feature selection on the dictionary basis space of the data, which can capture higher level and more abstract representation than original low-level representation. In addition, a similarity graph is learned simultaneously to preserve the local geometrical data structure which has been confirmed critical for unsupervised feature selection. In summary, we propose a model (referred to as DGL-UFS briefly) to integrate dictionary learning, similarity graph learning and feature selection into a uniform framework. Experiments on various types of real world datasets demonstrate the effectiveness of the proposed framework DGL-UFS.																	0924-669X	1573-7497				MAY	2020	50	5					1379	1397		10.1007/s10489-019-01561-x		JAN 2020											
J								Improved spotted hyena optimizer with space transformational search for training pi-sigma higher order neural network	COMPUTATIONAL INTELLIGENCE										classification; higher order neural network; metaheuristic algorithm; optimization; space transformation technique; spotted hyena optimizer; swarm intelligence	SINE COSINE ALGORITHM; MODEL	Spotted hyena optimizer (SHO) is a recently developed swarm-based algorithm in the field of metaheuristic research, for solving realistic engineering design constraint and unconstrained difficulties. To resolve complicated nonlinear physical world tasks, at times, SHO reveals deprived performance concerning to explorative strength. So, to enhance the explorative strength along with exploitation in the search region, an attempt has been made by proposing the enhanced version of classical SHO. The suggested method is designated as space transformation search (STS)-SHO. In STS-SHO, a new evolutionary technique named as STS technique has been incorporated with original SHO. The suggested method has been assessed by IEEE CEC 2017 benchmark problems. The efficacy of the said method has been proven by using standard measures such as given performance metrics in CEC 2017, complexity analysis, convergence analysis, and statistical implications. Further as real-world application, the said algorithm has been applied to train pi-sigma neural network by means of 13 benchmark datasets considered from UCI depository. From the article it can be concluded that the suggested method STS-SHO is an effective and trustworthy algorithm, which has the ability to resolve real-life optimization complications.																	0824-7935	1467-8640				FEB	2020	36	1					320	350		10.1111/coin.12272		JAN 2020											
J								A slice-based decentralized NFV framework for an end-to-end QoS-based dynamic resource allocation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network function virtualization; Resource allocation; Chaining; Mapping/dynamic scheduling; Slices; Pay as you go	OPTIMIZATION; CLOUD	Network function virtualization concept has recently merged to solve network operators and service provider's problems related to the non-flexibility of the traditional network and the increase of capital and operational expenditures (CAPEX and OPEX). ETSI has standardized an architectural framework to serve as a springboard for reflection to the application and the setup of the concept. However, that architecture presents many shortcomings and several challenges that have to be addressed. In this paper, we propose a new original framework that aims at satisfying the subscriber's Service Level Agreement and optimizes Telecom Service Provider fees under the scope of pay as you go concept. Based on the proposed framework, the end to end service execution process is presented. We afterward address the dynamic scheduling problem to minimize service execution time in the network by meant of the makespan. The dynamic scheduling problem is modeled as a mixed-integer linear programming one, and an Event-Driven Upward Ranking-Based Dynamic Scheduling Heuristic (EDURBaDySH) is proposed to solve large instances of the problem. Simulation results show that our approach performs better than Round Robin, Min Min and HEFT approaches in terms of makespan for different workflow configurations, considering a small, middle and large number of VNFs.																	1868-5137	1868-5145															10.1007/s12652-020-01709-5		JAN 2020											
J								Hybrid system for video game recommendation based on implicit ratings and social networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender systems; Colaborative filtering; Content-based filtering; Rating estimation; Graph-based methods; Video games	CONTEXT	The digital entertainment sector is one of the fastest growing in recent years. In the case of video games, the productions of some of the most popular titles are on a par with film productions. The sale of video games is in the millions, and yet there are few works on the recommendation of video games. In this work a hybrid system of video game recommendation is presented, through the use of collaborative filtering and content-based filtering, and the construction of relationship graphs. In order to improve the recommendations, a new method for estimating implicit ratings is proposed that takes into account the hours of play. The proposed recommender system improves the results of other techniques presented in the state of the art.																	1868-5137	1868-5145															10.1007/s12652-020-01681-0		JAN 2020											
J								Perception, acceptance and willingness of older adults in Malaysia towards online shopping: a study using the UTAUT and IRT models	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										UTAUT; IRT; Mobile computing; Technology management; Online shopping; e-Commerce	INFORMATION-TECHNOLOGY; UNIFIED THEORY; RESISTANCE; BANKING; AGE	With the confluence of information technology era and the progressively aging society, the internet usage rate by older adults in Malaysia is growing at a substantial rate. Therefore, older adults are becoming an increasingly significant potential target market for electronic commerce. However, previous researchers have focused mainly on the youth market and paid less attention to the online behaviours of older adults. To bridge the gap, the objective of this research is to increase a better understanding of how the factors affecting the perception, acceptance and willingness of older adults in Malaysia towards online shopping. To this end, this study is developed by integrating the unified theory of acceptance and use of technology (UTAUT) and innovation resistance theory (IRT). They are applied to an original dataset of 200 responses from respondents that were collected through a survey.																	1868-5137	1868-5145															10.1007/s12652-020-01718-4		JAN 2020											
J								Drones Are Flying outside of Segregated Airspace in Poland New Rules for BVLOS UAV Operations	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										drones; UAV; UAS; Polish regulations; EU regulations; air law		Poland was one of the first European countries to adopt a national regulatory framework for the operation of drones. During its first years (2013-2016), the national regulator was more focused on VLOS operations, and BVLOS operations were possible only in segregated airspace. Since 2019, a new law has allowed for certain types of BVLOS operations to be conducted outside of segregated airspace, at very low levels. This paper will analyze how national legislation is dealing with this new technology, focusing on the new law on BVLOS operations. As the national regulation will be replaced in June 2020 by common European rules that have been adopted 2019, this paper also aims to highlight the most important provisions of the EU regulation.																	0921-0296	1573-0409				NOV	2020	100	2					483	491		10.1007/s10846-019-01145-4		JAN 2020											
J								Some new correlation coefficients of picture fuzzy sets with applications	NEURAL COMPUTING & APPLICATIONS										Intuitionistic fuzzy set; Correlation coefficient; Pattern recognition; Clustering; Picture fuzzy set	SIMILARITY MEASURES; AGGREGATION OPERATORS; SOFT SET; CONVERGENCE; OPERATIONS; SEQUENCES	Picture fuzzy set (PFS) is an important tool for handling uncertainty and vagueness, particularly in situations that require more answers of the type "yes," "no," "abstain" and "refusal." Correlation coefficient of picture fuzzy sets (PFSs) is an essential measure in picture fuzzy set theory and has a lot of applications in many areas, such as "decision-making," "medical diagnosis," "pattern recognition" and "clustering analysis". In this article, two correlation coefficients of PFSs are introduced along with some of their properties. These correlation coefficients of PFSs are better than existing ones and effective in expressing the nature of correlation (positive or negative correlation). We also show the applications and advantages of the proposed picture fuzzy correlation coefficients over some existing methods in pattern recognition, medical diagnosis and clustering with the help of illustrative examples.																	0941-0643	1433-3058				AUG	2020	32	16					12609	12625		10.1007/s00521-020-04715-y		JAN 2020											
J								Neural network modeling for groundwater-level forecasting in coastal aquifers	NEURAL COMPUTING & APPLICATIONS										Emotional artificial neural network; Genetic algorithm; Feedforward neural network; Groundwater-level forecasting; Coastal aquifer	GENETIC ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; COMPUTER-SIMULATION; SEAWATER INTRUSION; GAS GENERATION; REGRESSION; FLUCTUATIONS; INTELLIGENCE; PREDICTION; INTERFACE	Advances in the artificial intelligence-based models can act as robust tools for modeling hydrological processes. Neural network architectures coupled with learning algorithms are considered as useful modeling tools for groundwater-level fluctuations. Emotional artificial neural network coupled with genetic algorithm (EANN-GA) is one such novel hybrid neural network which has been used in the present study for the forecasting of groundwater levels at three sites (Site H3, Site H4.5, and Site H9) in a coastal aquifer system. This study was conceived to address and investigate the efficiency of the ensemble model (EANN-GA) for forecasting one-month ahead groundwater level and to compare its performance with emotional artificial neural network (EANN), generalized regression neural network (GRNN), and the conventional feedforward neural network (FFNN). Variations in the rainfall, tidal levels, and groundwater levels are selected as inputs for the development of EANN-GA, EANN, GRNN, and FFNN models. Suitable goodness-of-fit criteria such as Nash-Sutcliffe efficiency (NSE), bias, root mean squared error (RMSE), and graphical indicators are used for assessing the efficiency of the developed models. The improvement in the performance of the EANN-GA model over the developed EANN, GRNN, and FFNN models in terms of NSE is 0.81, 6.02, and 9.56% at Site H3; 4.35, 5.50, and 22.68% at Site H4.5; and 1.05, 7.18, and 21.75% at Site H9. Thus, it can be inferred that the EANN-GA model outperforms the developed EANN model, GRNN model, and FFNN model. Further, this paper examines the predictive capability of extreme events by the EANN-GA, EANN, GRNN, and FFNN models. The RMSE values of the EANN-GA model at all peak points are found as 0.27, 0.23, and 0.10 m at sites H3, H4.5, and H9, respectively, and the results indicate superior performance of EANN-GA model. To check the generalization ability of the developed EANN-GA models, they are validated with the data of another site (Site I2) located in the same coastal aquifer. Superior prediction capability and generalization ability make the EANN-GA model a better alternative for predicting groundwater levels. Overall, this study demonstrates the effectiveness of EANN-GA in modeling spatio-temporal fluctuations of groundwater levels. It is also concluded that the EANN-GA model yields remarkably better predictions of extreme events, and hence, it could be a promising technique for developing alarm systems for real-world water problems.																	0941-0643	1433-3058				AUG	2020	32	16					12737	12754		10.1007/s00521-020-04722-z		JAN 2020											
J								Bio-inspired predictive models for shear strength of reinforced concrete beams having steel stirrups	SOFT COMPUTING										Neural network; Reinforced concrete beam; Shear capacity; Shear failure	FUZZY INFERENCE SYSTEM; INTELLIGENCE MODELS; NEURAL-NETWORKS; FLY-ASH; BEHAVIOR; FIBER; OPTIMIZATION; ANN; ALGORITHM; LOAD	In this article, three bio-inspired predictive models are proposed with the aim of estimating the shear capacity of reinforced concrete (RC) beams having steel stirrups. For this purpose, 194 experimental tests of this type of RC beams were gathered from the literature. Then, the structures of the artificial neural network models are trained and validated using seven parameters including concrete compressive strength, width of the member, effective depth of the member, the yielding strength of transverse reinforcement, area of the reinforcement as a proportion of the beam area, the yielding strength of longitudinal reinforcement and also the transverse reinforcement ratio to determine the observed shear capacity in the experimental tests. It was concluded that the proposed mathematical frameworks could determine the shear capacity with a satisfactory level of precision in comparison with the obtained results of ACI-318.																	1432-7643	1433-7479				AUG	2020	24	16					12587	12597		10.1007/s00500-020-04698-x		JAN 2020											
J								Relaxing the strong triadic closure problem for edge strength inference	DATA MINING AND KNOWLEDGE DISCOVERY										Strong triadic closure; Strength of social ties; Linear programming; Convex relaxations; Half-integrality	VERTEX COVER; ALGORITHMS; TIES; SET	Social networks often provide only a binary perspective on social ties: two individuals are either connected or not. While sometimes external information can be used to infer the strength of social ties, access to such information may be restricted or impractical to obtain. Sintos and Tsaparas (KDD 2014) first suggested to infer the strength of social ties from the topology of the network alone, by leveraging the Strong Triadic Closure (STC) property. The STC property states that if person A has strong social ties with persons B and C, B and C must be connected to each other as well (whether with a weak or strong tie). They exploited this property to formulate the inference of the strength of social ties as a NP-hard maximization problem, and proposed two approximation algorithms. We refine and improve this line of work, by developing a sequence of linear relaxations of the problem, which can be solved exactly in polynomial time. Usefully, these relaxations infer more fine-grained levels of tie strength (beyond strong and weak), which also allows one to avoid making arbitrary strong/weak strength assignments when the network topology provides inconclusive evidence. Moreover, these relaxations allow us to easily change the objective function to more sensible alternatives, instead of simply maximizing the number of strong edges. An extensive theoretical analysis leads to two efficient algorithmic approaches. Finally, our experimental results elucidate the strengths of the proposed approach, while at the same time questioning the validity of leveraging the STC property for edge strength inference in practice.																	1384-5810	1573-756X				MAY	2020	34	3					611	651		10.1007/s10618-020-00673-0		JAN 2020											
J								q-Rung orthopair fuzzy soft average aggregation operators and their application in multicriteria decision-making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										MCDM; Pythagorean fuzzy set; q-ROFS(f)OWA operator; q-ROFS(f)WA operator; q-ROFS; q-ROFS(f)HA operator	PYTHAGOREAN MEMBERSHIP GRADES; SET; HYBRID; VIEW	Molodtsov investigated the pioneer notion of soft set (SfS) which provides a general framework for mathematical problems by affix parameterization tools during the analysis as compared to fuzzy set and q-rung orthopair fuzzy set (q-ROFS). The aim of this manuscript is to investigate the notion of q-rung orthopair fuzzy soft set (q-ROFSfS), which provide a lay of foundation for those difficulties and complexities which the contemporary theories face during the study of uncertainty. Therefore, our main contribution in this manuscript is to investigate the q-rung orthopair fuzzy soft weighted averaging, q-rung orthopair fuzzy soft ordered weighted averaging and q-rung orthopair fuzzy soft hybrid averaging operators in q-ROF soft (q-ROFSf) environment. Further, the fundamental properties of these aggregation operators are studied. On the base of developed approach an algorithm for multicriteria decision making method is being presented. An application of medical diagnosis problems is solved on the proposed algorithm under the q-ROFSf environment. Finally, comparison between the developed operators with some existing operators are being presented showing the superiority and efficiency of the developed approach than the existing literature.																	0884-8173	1098-111X				APR	2020	35	4					571	599		10.1002/int.22217		JAN 2020											
J								Landmark-based homologous multi-point warping approach to 3D facial recognition using multiple datasets	PEERJ COMPUTER SCIENCE										3D facial landmark; Landmark algorithm; Homologous facial points; TPS warping; 3D morphology; Multiple datasets	GEOMETRIC MORPHOMETRICS; MORPHOLOGICAL INTEGRATION; FLUCTUATING ASYMMETRY; MEASUREMENT ERROR; FACE RECOGNITION; SHAPE; EVOLUTION; SELECTION; ANCESTRY; SKULL	Over the years, neuroscientists and psychophysicists have been asking whether data acquisition for facial analysis should be performed holistically or with local feature analysis. This has led to various advanced methods of face recognition being proposed, and especially techniques using facial landmarks. The current facial landmark methods in 3D involve a mathematically complex and time-consuming workflow involving semi-landmark sliding tasks. This paper proposes a homologous multi-point warping for 3D facial landmarking, which is verified experimentally on each of the target objects in a given dataset using 500 landmarks (16 anatomical fixed points and 484 sliding semi-landmarks). This is achieved by building a template mesh as a reference object and applying this template to each of the targets in three datasets using an artificial deformation approach. The semi-landmarks are subjected to sliding along tangents to the curves or surfaces until the bending energy between a template and a target form is minimal. The results indicate that our method can be used to investigate shape variation for multiple datasets when implemented on three databases (Stirling, FRGC and Bosphorus).																	2376-5992					JAN 16	2020									e249	10.7717/peerj-cs.249													
J								Device-free passive wireless localization system with transfer deep learning method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Device-free indoor localization; Channel state information (CSI); Deep learning; Transfer learning	INDOOR LOCALIZATION; KERNEL	Recently, device-free passive wireless indoor localization (DFPWL) has attracted great interest due to the widespread deployment of Wi-Fi devices and the rapid growth in demand for location-based services (LBS). The DFPWL fingerprinting approach based on channel state information (CSI) has become the mainstream method since its simple deployment and localization accuracy. It determines the location of the target from the new measurement CSI by collecting a training database that measures the CSI and using a machine learning classifier. However, we have found through experiments that even if the indoor environment does not change, the CSI fingerprint will be different from the CSI fingerprint in the database over time, and most of the CSI-based DFPWL fingerprinting method ignores this. To cope with the reduction in localization accuracy caused by the time-varying characteristic of CSI, we propose a novel transfer deep learning-based DFPWL system in this paper. It uses the CSI extracted from a single link to estimate the location of the target, neither requiring the target to wear any electronic equipment nor deploying a large number of APs and Monitor Devices. Unlike the other traditional CSI-based DFPWL fingerprinting approaches using the pre-processed CSI samples as fingerprints, our system utilizes the transfer deep learning (TDL) method to learn new feature representations from the CSI samples as fingerprints, which can simultaneously minimize the intra-class differences, maximize inter-class differences, and minimize the distribution differences between fingerprint database and test samples. Finally, the KNN algorithm is utilized to compare the test samples and the fingerprint database under the new feature representation to obtain the estimation location of the target. Experiment results show that our system can effectively improve localization accuracy compared to the other state-of-art, and can maintain stable localization accuracy for a long time without reacquiring the CSI fingerprint database.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4055	4071		10.1007/s12652-019-01662-y		JAN 2020											
J								Channel congestion control model based on improved asynchronous back-pressure routing algorithm in wireless distributed networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Channel congestion control; Flow scheduling; Wireless distributed networks; Back-pressure routing; Network utility; Energy utility; Queue backlog		Due to the continuous increase of the network bandwidth, the traditional coarse-grained congestion control mechanisms and flow scheduling schemes are difficult to provide satisfying performance. Therefore, a distributed algorithm based on an improved asynchronous back-pressure routing algorithm is proposed for joint channel congestion control, routing and power allocation in this paper. Considering the application scenario of wireless distributed networks with node power constraints and independent buffers for traffic flow, this paper studies joint congestion control, routing and power allocation when channel state information is known. In order to improve the defect of Newton method, an algorithm with second-order convergence performance is designed and matrix decomposition method is used to realize the distributed updating of traffic source rate, link rate and link power in network nodes and links so as to maximize network utility. Compared with the known existing algorithms, our proposed algorithm has faster convergence speed, which improves the network utility and energy utility by optimizing power allocation, and it can control the queue backlog at a very low level.																	1868-5137	1868-5145															10.1007/s12652-020-01685-w		JAN 2020											
J								A two-stage approach for automatic liver segmentation with Faster R-CNN and DeepLab	NEURAL COMPUTING & APPLICATIONS										Liver segmentation; DeepLab; Faster R-CNN		Proper liver segmentation is a key step in many clinical applications, including computer-assisted diagnosis, radiation therapy and volume measurement. However, liver segmentation is still challenging due to fuzzy boundary, complex liver anatomy, present of pathologies, and diversified shape. This paper presents a novel two-stage liver detection and segmentation model DSL. The first stage uses improved Faster Regions with CNN features (Faster R-CNN) to detect approximate position of liver. The obtained images are processed and input into DeepLab to obtain the contour of liver. The proposed approach is validated on two datasets MICCAI-Sliver07 and 3Dircadb. Experimental results reveal that the proposed method outperforms the state-of-the-art solutions in terms of volume overlap error, average surface distance, relative volume difference, and total score.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6769	6778		10.1007/s00521-019-04700-0		JAN 2020											
J								Novel multi-objective, multi-item and four-dimensional transportation problem with vehicle speed in LR-type intuitionistic fuzzy environment	NEURAL COMPUTING & APPLICATIONS										Four-dimensional transportation problem; Intuitionistic fuzzy number; LR-type intuitionistic fuzzy number; Accuracy function	NUMBER	In this paper, we present some novel multi-objective, multi-item and four-dimensional transportation problems in LR-type intuitionistic fuzzy environment.Here, for the first time, the speed of different vehicles and rate of disturbance of speed due to the road condition of different routes for the time minimization objective are introduced. Furthermore, three models are presented under three different conditions.The reduced deterministic models are obtained on implementation of a defuzzification approach by using the accuracy function. Moreover, a new method for converting multi-objective problem into single-objective one is proposed and also we use convex combination method. The models are illustrated by some numerical examples and optimal results are presented.																	0941-0643	1433-3058				AUG	2020	32	15					11937	11955		10.1007/s00521-019-04675-y		JAN 2020											
J								Design of meta-heuristic computing paradigms for Hammerstein identification systems in electrically stimulated muscle models	NEURAL COMPUTING & APPLICATIONS										Parameter estimation; Electrically stimulated muscle models; Differential evolution; Genetic algorithms; Nonlinear Hammerstein systems; Evolutionary computing	FRACTIONAL ADAPTIVE STRATEGY; ARTIFICIAL NEURAL-NETWORK; REACTIVE POWER DISPATCH; SPINAL-CORD-INJURY; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; FEATURE-SELECTION; PARAMETER-ESTIMATION; MATHEMATICAL-MODELS; NUMERICAL TREATMENT	In this study, a novel application of differential evolution (DE)-based computational heuristics is proposed for the identification of Hammerstein structures representing the electrically stimulated muscle (ESM) models as a part of rehabilitation interventions for the stock patient to prevent the post-spinal cord injury atrophy. The strength of approximation theory is incorporated for defining the fitness function for ESM system based on mean square deviation between actual and estimated responses. DE, genetic algorithms (GAs), particle swarm optimization (PSO), pattern search (PS), and simulated annealing (SA) are used as optimization mechanisms to identify the ESM models with input nonlinearities of sigmoidal, polynomial, and spline kernels for noiseless and noisy environments. Comparative studies based on detailed statistics establish the worth of DE-based heuristics over its counterparts GAs, PSO, PS, and SA in terms of accuracy, convergence, robustness, and efficiency for the identification of ESM models arising in rehabilitation of the stock patients.																	0941-0643	1433-3058				AUG	2020	32	16					12469	12497		10.1007/s00521-020-04701-4		JAN 2020											
J								Heartbeat classification by using a convolutional neural network trained with Walsh functions	NEURAL COMPUTING & APPLICATIONS										Heartbeat classification; Convolutional neural networks; Deep learning; Walsh function	ECG ARRHYTHMIA; MORPHOLOGY; MIXTURE; SYSTEM	From recent studies, it is observed that convolutional neural networks are proved to be extremely successful in classification problems. Accurate and fast classification of electrocardiogram (ECG) beats is a crucial step in the implementation of real-time arrhythmia diagnosis systems. In this study, convolutional neural networks are employed to classify eleven different ECG beat types in the MIT-BIH arrhythmia database. We aimed to implement a computer-aided mobile diagnosis system equipped with artificial intelligence that detects the classes of heartbeats by visual inspection of the ECG records in the manner as the cardiologists do. Since doctors make their decisions basing heavily on the 2D visual appearances of the ECG signals without doing numerical calculations on 1D time samples, 2D images of 1D ECG records were given to the classifier as the input data. It would not be surprising that the structure of the network classifying 2D ECG data has to be larger than the one used to classify 1D ECG signals. The small size of a neural network is an important property for real-time use of the system. In this study, smaller network structures that provide high performances using the Walsh functions (WF), and drawbacks of converting 1D signals to 2D images have been investigated. The network structures using the WF during the training stage have been applied to different databases and successful results have been obtained. Classification results and sizes of the network structures are compared for the ECG beats in one dimension and in the form of 2D visuals. The training of ECG signals in the form of 2D visuals takes much longer time than that of the 1D signals. However, it is observed that training and testing times of both networks were quite fast. Moreover, average success rates of 99% were achieved for all beat types by using small-size networks.																	0941-0643	1433-3058				AUG	2020	32	16					12515	12534		10.1007/s00521-020-04709-w		JAN 2020											
J								A multi-objective algorithm for U-shaped disassembly line balancing with partial destructive mode	NEURAL COMPUTING & APPLICATIONS										Disassembly line balancing; Partial destructive disassembly; Multi-objective optimization; Flower pollination algorithm	FLOWER POLLINATION ALGORITHM; ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; SEARCH ALGORITHM; PROFIT	The disassembly line is the best way to deal with large-scale waste electrical and electronic equipment. Balancing of disassembly line is a hot and challenging problem in recent years. Given the uncertainty factors including corrosion and deformation of parts and components of waste products, this paper introduces the destructive mode and uncertainty disassembly time into the disassembly line and establishes a multi-objective disassembly line balancing model, considering partial destructive mode and U-shaped layout. The model aims to reduce the number of stations, balance the workload and reduce energy consumption while increasing the disassembly profit. A new multi-objective discrete flower pollination algorithm is proposed to solve the problem. Both task assignment and disassembly modes are considered in the encoding and decoding strategies of the flowers. Combining the discrete characteristics of the problem, the cross-pollination and self-pollination behaviors of the algorithm are redefined. The performance of the proposed algorithm is verified by solving two classical examples and by comparing with seven meta-heuristic algorithms. Then the proposed model and method are applied to a television disassembly line of a disassembly enterprise in China. The disassembly schemes of the proposed algorithm are superior to that of the five classical multi-objective algorithms. The results show that the proposed method can improve the performance of the disassembly line.																	0941-0643	1433-3058				AUG	2020	32	16					12715	12736		10.1007/s00521-020-04721-0		JAN 2020											
J								Development of operation multi-objective model of dam reservoir under conditions of temperature variation and loading using NSGA-II and DANN models: a case study of Karaj/Amir Kabir dam	SOFT COMPUTING										Optimal operation policy; ABAQUS simulator model; Dynamic artificial neural network; Non-dominated sorting genetic algorithm; Multi-criteria decision-making methods; Karaj dam	ARTIFICIAL NEURAL-NETWORK; CONJUNCTIVE USE MANAGEMENT; KRILL HERD ALGORITHM; OPTIMIZATION ALGORITHM; SIMULATION-MODEL; PREDICTION; RIVER; ANN	For determining optimal operation policies, it is of vital importance to the structural stability of dams, besides meeting the downstream water demands during the operation period. Since water level variations in upstream dam cause changes in loadings of the dam body, ignoring factors, which are effective in the structural stability of dams, might result in cracks and instability of dams in the long term. Therefore, in the present study, a multi-objective model was developed by integrating the reservoir structure simulation model and optimization approach in order to meet water supply demands and maintain the dam structural stability. Linear static analysis of Amir Kabir dam was performed using ABAQUS 6.14.3 software to extract the structural parameters, indicating the dam structural stability. For this purpose, a dynamic artificial neural network was also used as an interfaced model to relate the results of the simulator model to those of the proposed approach. The best solution was extracted from the optimal trade-off curve, which was developed using non-dominated sorting genetic algorithm (NSGA-II) and multi-criteria decision-making methods. Results showed that the allocation to the downstream water demands was applied in the best possible way based on the stability of the dam. A comparison between the existing and optimal conditions indicated that the value of reliability and vulnerability coefficients was improved in optimal conditions in comparison with the existing conditions. According to the results, there was a 7% rise in the fuzzy stability index in optimal conditions compared to the existing conditions that indicate better performance in the optimal model. Therefore, as to consider the downstream water demands, summer was the most deficient and spring was the least deficient with the water supply of 30% and 72%, respectively. During summer, the average optimal allocation and the average demand were 20.51 MCM and 67.50 MCM, respectively. However, this season showed the best performance in preserving the dam stability with having the lowest parameters of the dam structural stability compared to other seasons. Finally, the support vector regression method was used to develop an optimal operational policy based on optimal allocation values obtained from the proposed model structure. The optimal developed policy can be used to determine optimal allocation based on hydrological parameters of the dam and its water demands. Meanwhile, it can maintain dam stability without implementing the proposed structure.																	1432-7643	1433-7479				AUG	2020	24	16					12469	12499		10.1007/s00500-020-04686-1		JAN 2020											
J								Sketch discriminatively regularized online gradient descent classification	APPLIED INTELLIGENCE										Machine learning; Online learning; Classification; Sketch technique	PERCEPTRON	Online learning represents an important family of efficient and scalable algorithms for large-scale classification problems. Many of them are linear with fast computational speed, but when faced with complex classification, they more likely have low accuracies. In order to improve accuracies, kernel trick is applied, however, it often brings high computational cost. In fact, discriminative information is vital in classification which is still not fully utilized in these algorithms. In this paper, we proposed a novel online linear method, called Sketch Discriminatively Regularized Online Gradient Descent Classification (SDROGD). In order to exploit inter-class separability and intra-class compactness, SDROGD utilizes a matrix to characterize the discriminative information and embeds it directly into a new regularization term. This matrix can be updated by the sketch technique in an online manner. After applying a simple but effective optimization, we show that SDROGD has a good time complexity bound, which is linear with the feature dimension or the number of samples. Experimental results on both toy and real-world datasets demonstrate that SDROGD has not only faster computational speed but also much better classification accuracies than some related kernelized algorithms.																	0924-669X	1573-7497				MAY	2020	50	5					1367	1378		10.1007/s10489-019-01590-6		JAN 2020											
J								Merging Similar Neurons for Deep Networks Compression	COGNITIVE COMPUTATION										Machine learning; Deep neural networks; Structure compression; Neurons; Clustering		Deep neural networks have achieved outstanding progress in many fields, such as computer vision, speech recognition and natural language processing. However, large deep neural networks often need huge storage space and long training time, making them difficult to apply to resource restricted devices. In this paper, we propose a method for compressing the structure of deep neural networks. Specifically, we apply clustering analysis to find similar neurons in each layer of the original network, and merge them and the corresponding connections. After the compression of the network, the number of parameters in the deep neural network is significantly reduced, and the required storage space and computational time is greatly reduced as well. We test our method on deep belief network (DBN) and two convolutional neural networks. The experimental results demonstrate that our proposed method can greatly reduce the number of parameters of the deep networks, while keeping their classification accuracy. Especially, on the CIFAR-10 dataset, we have compressed VGGNet with compression ratio 92.96%, and the final model after fine-tuning obtains even higher accuracy than the original model.																	1866-9956	1866-9964				MAY	2020	12	3					577	588		10.1007/s12559-019-09703-6		JAN 2020											
J								The 2D Dependency Pair Framework for Conditional Rewrite Systems-Part II: Advanced Processors and Implementation Techniques	JOURNAL OF AUTOMATED REASONING										Conditional term rewriting; Dependency pairs; Program analysis; Operational termination	PROVING TERMINATION PROPERTIES; LOGICAL MODELS; PROOFS	Proving termination of programs in 'real-life' rewriting-based languages like CafeOBJ, Haskell, Maude, etc., is an important subject of research. To advance this goal, faithfully capturing the impact in the termination behavior of the main language features (e.g., conditions in program rules) is essential. In Part I of this work, we have introduced a 2D Dependency Pair Framework for automatically proving termination properties of Conditional Term Rewriting Systems. Our framework relies on the notion of processor as the main practical device to deal with proofs of termination properties of conditional rewrite systems. Processors are used to decompose and simplify the proofs in a divide and conquer approach. With the basic proof framework defined in Part I, here we introduce new processors to further improve the ability of the 2D Dependency Pair Framework to deal with proofs of termination properties of conditional rewrite systems. We also discuss relevant implementation techniques to use such processors in practice.																	0168-7433	1573-0670				DEC	2020	64	8					1611	1662		10.1007/s10817-020-09542-3		JAN 2020											
J								Design and Tension Modeling of a Novel Cable-Driven Rigid Snake-Like Manipulator	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Snake-like manipulator; Cable-driven; Kinematics; Tension model; Neural network; Reinforcement learning		In this study, a cable-driven snake-like manipulator with high load capacity and end-positioning accuracy is designed for applications in complex and narrow environments. The Hooke joint-like two degree-of-freedom joint design and the full actuation mode enhanced the rigidity of the robot. The modular link design increased the local flexibility of the robot. Because the cable tension cannot be ignored under high load on the basis of the kinematics model, a cable tension model is established based on rigid body static equilibrium to describe the relationship between posture and cable tension. This provided a foundation for follow-up studies on obstacle avoidance path planning with optimized tension. At the same time, in order to improve the response speed of the motor position controller to the tension change, this study introduces both the tension as the reference model input and the system state variable into the adaptive control method based on model identification and reinforcement learning. The kinematics model and the cable tension model were validated by experiments on the prototype. The practical results of the two adaptive control methods were compared and the result shows that the method based on model identification has a better effect.																	0921-0296	1573-0409				AUG	2020	99	2					211	228		10.1007/s10846-019-01115-w		JAN 2020											
J								Some picture fuzzy Bonferroni mean operators with their application to multicriteria decision making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										aggregation operators; Bonferroni mean; multicriteria decision making; picture fuzzy numbers; picture fuzzy sets	AGGREGATION OPERATORS; SELECTION; MODEL	In this paper, we extend the Bonferroni mean (BM) operator with the picture fuzzy numbers (PFNs) to propose novel picture fuzzy aggregation operators and demonstrate their application to multicriteria decision making (MCDM). On the basis of the algebraic operational rules of PFNs and BM, we introduce some aggregation operators: the picture fuzzy Bonferroni mean, the picture fuzzy normalized weighted Bonferroni mean, and the picture fuzzy ordered weighted Bonferroni mean. Then, a new picture fuzzy MCDM method is proposed with the help of the proposed operators. Lastly, a practical application of proposed model is given to verify the developed model and related results of the proposed model is compared with the results of the existing models to indicate its applicability.																	0884-8173	1098-111X				APR	2020	35	4					625	649		10.1002/int.22220		JAN 2020											
J								On some correlation coefficients in Pythagorean fuzzy environment with applications	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										clustering; correlation coefficient; medical diagnosis; pattern recognition; Pythagorean fuzzy set	AGGREGATION OPERATORS; DECISION-MAKING; SIMILARITY MEASURES; OPERATIONAL LAWS; SETS; DISTANCE; NETWORKS	A Pythagorean fuzzy set (PFS) is an extension of an intuitionistic FS that can be extended by relaxing the restriction on the grades of satisfaction and dissatisfaction. PFS is a powerful tool for dealing with uncertainty and vagueness. Correlation analysis of PFSs is a hot research topic in Pythagorean fuzzy (PF) theory and has practical applications in many areas, such as decision-making, pattern recognition, medical diagnosis, engineering, and so forth. In this communication, we introduce some novel correlation coefficients in the PF-environment satisfying the condition that the correlation coefficient of two PFSs is one if and only if the two sets are equal. We discuss the properties and applications of the proposed measures in pattern recognition, medical diagnosis, multicriteria decision-making, and clustering analysis. Furthermore, the superiority of our proposed correlation coefficients over some existing ones is also established. We also extend the correlation coefficients to interval-valued PFSs.																	0884-8173	1098-111X				APR	2020	35	4					682	717		10.1002/int.22222		JAN 2020											
J								Comparison of locally weighted PLS strategies for regression and discrimination on agronomic NIR data	JOURNAL OF CHEMOMETRICS										discrimination; locally weighted calibration; near-infrared spectroscopy; partial least squares; regression	PARTIAL-LEAST-SQUARES; NEAR-INFRARED SPECTROSCOPY; LW-PLS; PREDICTIONS; OPTIMIZATION; CALIBRATION; COMPONENTS; REDUCTION	In multivariate calibrations, locally weighted partial least squared regression (LWPLSR) is an efficient prediction method when heterogeneity of data generates nonlinear relations (curvatures and clustering) between the response and the explicative variables. This is frequent in agronomic data sets that gather materials of different natures or origins. LWPLSR is a particular case of weighted PLSR (WPLSR; ie, a statistical weight different from the standard 1/n is given to each of the n calibration observations for calculating the PLS scores/loadings and the predictions). In LWPLSR, the weights depend from the dissimilarity (which has to be defined and calculated) to the new observation to predict. This article compares two strategies of LWPLSR: (a) "LW": the usual strategy where, for each new observation to predict, a WPLSR is applied to the n calibration observations (ie, entire calibration set) vs (b) "KNN-LW": a number of k nearest neighbors to the observation to predict are preliminary selected in the training set and WPLSR is applied only to this selected KNN set. On three illustrating agronomic data sets (quantitative and discrimination predictions), both strategies overpassed the standard PLSR. LW and KNN-LW had close prediction performances, but KNN-LW was much faster in computation time. KNN-LW strategy is therefore recommended for large data sets. The article also presents a new algorithm for WPLSR, on the basis of the "improved kernel #1" algorithm, which is competitor and in general faster to the already published weighted PLS nonlinear iterative partial least squares (NIPALS).																	0886-9383	1099-128X				MAY	2020	34	5							e3209	10.1002/cem.3209		JAN 2020											
J								An interactive nonparametric evidential regression algorithm with instance selection	SOFT COMPUTING										Nonparametric evidential regression; Belief functions; Instance selection	OPERATORS; IMPRECISE; UNCERTAIN	The nonparametric evidential regression (EVREG) method provides flexible forms of prediction regarding the value of output, allowing the output of training instances to be partially unknown. However, the superfluous training instances still have negative effects on the parameter learning in EVREG. To relax this limitation, this paper introduces an interactive nonparametric evidential regression (IEVREG) algorithm with instance selection. More specifically, the significance of an instance is firstly measured by defining the evaluation functions, taking into account both the prediction accuracy of regression model and the spatial information between that instance with other ones. According to a search strategy, the instances with high degree of significance are then selected to maximize an objective function. Different from existing instance selection methods, the selection of training instances is synchronously accomplished with the parameter learning in IEVREG, rather than just a separated data preprocessing operation as traditional methods do. Furthermore, the noise and redundant instances can be simultaneously removed and the performance of IEVREG is robust to the order of presentation of instances in raw data set. Experimental results show that the proposed IEVREG algorithm has appropriate prediction accuracy, while performing well selection of the representative training instances from the raw data set. Simulations on synthetic and UCI real-world data sets validate our conclusions.																	1432-7643	1433-7479															10.1007/s00500-020-04667-4		JAN 2020											
J								Improved linear profiling methods under classical and Bayesian setups: An application to chemical gas sensors	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Chemical gas sensors; Control charts; Efficiency; Profiling monitoring; Metal oxide; Neoteric ranked set sampling; Posterior analysis	AVERAGE CONTROL CHARTS; PHASE; DISTRIBUTIONS; DIAGNOSIS; SELECTION	A profile is a functional relationship, between two or more variables, used to monitor the process performance and its quality. The relationship may be linear or nonlinear depending upon the situation. Linear profiling methods with a fixed-effect model are commonly used under simple random sampling (SRS). In this article, we propose linear profiles monitoring methods under a new ranked set sampling (RSS) scheme named as Neoteric RSS (NRSS). The new profiling methods are proposed under all the three popular structures, namely Shewhart, cumulative sum (CUSUM) and exponentially weighted moving average (EWMA). The study proposal considers both classical and Bayesian setups. We have investigated the detection ability of newly proposed classical charts (i.e., Shewhart_NRSS(C), CUSUM_NRSS(C), EWMA_NRSS(C) charts) and Bayesian charts (i.e., ShewhartNRSS(B), CUSUMNRSS(B) and EWMA_NRSS(B) charts). An extensive simulation study showed that the proposed charts have better detection ability for perfect NRSS scheme, while Bayesian control charts showed superiority over its classical counterpart under both perfect and imperfect NRSS. The significance of the proposed study is further highlighted using the real data study of chemical gas sensors from the chemical industry.																	0169-7439	1873-3239				JAN 15	2020	196								103908	10.1016/j.chemolab.2019.103908													
J								cACP: Classifying anticancer peptides using discriminative intelligent model via Chou's 5-step rules and general pseudo components	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Anticancer peptides; QSO; SVM; RF; PCA	AMINO-ACID-COMPOSITION; SEQUENCE-BASED PREDICTOR; PROTEIN SUBCELLULAR-LOCALIZATION; WEB SERVER; DIPEPTIDE COMPOSITION; IDENTIFICATION; CANCER; PSEAAC; SITES; RNA	World widely, cancer is considered a fatal disease and remains the major cause of death. Conventional medication approaches using therapies and anticancer drugs are deemed ineffective due to its high cost and harmful impacts on the normal cells. However, the innovation of anticancer peptides (ACPs) provides an effective way how to deals with cancer affected cells. Due to the rapid increases in peptide sequences, truly characterization of ACPs has become a challenging task for investigators. In this paper, an effort has been carried out to develop a reliable and intelligent computational method for the accurate discrimination of anticancer peptides. Three statistical feature representation schemes namely: Quasisequence order (QSO), conjoint triad feature, and Geary autocorrelation descriptor are applied to express motif of the target class. In order to eradicate irrelevant and noisy features, while select salient, profound and high variated features, principal component analysis is employed. Furthermore, the diverse nature of learning algorithms is utilized in order to select the best operational engine for the proposed model. After examining the empirical outcomes, support vector machine obtained quite encouraging results in combination with QSO feature space. It has achieved an accuracy of 96.91% and 89.54% using the main dataset and alternative dataset, respectively. It is observed that our proposed model shows an outstanding improvement compared to literature methods. It is expected that the developed model may be played a useful role in research academia as well as proteomics and drug development.																	0169-7439	1873-3239				JAN 15	2020	196								103912	10.1016/j.chemolab.2019.103912													
J								Multi-block SO-PLS approach based on infrared spectroscopy for anaerobic digestion process monitoring	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Anaerobic digestion monitoring; Near InfraRed spectroscopy; Polarization light spectroscopy; SO-PLS	VOLATILE FATTY-ACIDS; CO-DIGESTION; REFLECTANCE; CHEMOMETRICS; INFORMATION; PREDICTION; PARAMETERS; EXTENSION; MODELS; SILAGE	Near infrared spectroscopy combined with multivariate calibration such as partial least squares regression is a promising technique for on-line monitoring of anaerobic digesters. Different substrates are used in digesters, depending on their availability and their methanogen potential, to optimize the process. In Europe, the feedstock for anaerobic digesters is dominated by slurry and food waste which are respectively highly biodegradable and fat-containing substrates. The monitoring of the anaerobic digestion process based on digestates coming from these substrates presents some difficulties. The digestion of highly biodegradable substrates comes with the presence of water, which hinders spectroscopic calibration. And fat-containing substrates could lead to the accumulation of long chain fatty acids which are quite difficult to detect in the infrared region. While all existing studies have explored adapted spectroscopic measurements to improve the process monitoring, this study investigated the use of NIRS combined with multi-block analysis to track important anaerobic digestion stability parameters. Infrared measurements can come from several sources in the process monitoring. In addition, sequential and orthogonalized partial least squares have proven their ability of exploiting the underlying relation between several data blocks. These multi-block methods are powerful chemometric tools which can be applied in the monitoring of anaerobic digestion. Polarization light spectroscopy which is also known to improve the comprehension of scattering media like the digestate was also studied.																	0169-7439	1873-3239				JAN 15	2020	196								103905	10.1016/j.chemolab.2019.103905													
J								Determination of the effect of red blood cell parameters in the discrimination of iron deficiency anemia and beta thalassemia via Neighborhood Component Analysis Feature Selection-Based machine learning	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										IDA; beta-thalassemia; Erythrocyte index; Machine learning; Feature selection	CLASSIFICATION	Differential diagnosis of iron deficiency anemia (IDA) and beta-thalassemia is a time-taking and costly procedure. Complete blood count (CBC) is a quick, inexpensive, and easily accessible test which is used as the primary test for the diagnosis of anemia. However, as CBC cannot successfully discriminate between IDA and beta-thalassemia, advanced techniques are needed. To date, numerous red blood cell (RBC) indices have been investigated and various parameters have been proposed for each index. In the present study, a differential diagnosis of IDA and beta-thalassemia was performed by using RBC indices and machine learning techniques including Support Vector Machine (SVM) and K-Nearest Neighbor (KNN). The RBC indices were used as input parameters for the classifier and the performances of SVM and KNN were evaluated separately, in order to determine the effectivity of both techniques. Fewer parameters were given as an inputs to machine learning algorithms, and higher performance was achieved. On the other hand, a feature selection technique, the Neighborhood Component Analysis Feature Selection (NCA) algorithm, was used for selecting features from the datasets, and the parameters selected via NCA provided high performance (97% Area Under the ROC curve [AUC]. Taken together, the results indicated that the RBC indices used in the study showed higher performance compared to those reported in the literature. By using these indices, not only the individual effect of each index parameter on the machine learning model was investigated but also a different subset of features from those employed in the literature was established. In addition, as distinct from the literature, the study revealed that different CBC parameters were efficient in distinguishing between IDA and beta-thalassemia in male and female patients. Accordingly, the RBC indices employed in the study can be easily and inexpensively used in clinical and daily practice for the discrimination of IDA and beta-thalassemia.																	0169-7439	1873-3239				JAN 15	2020	196								103886	10.1016/j.chemolab.2019.103886													
J								Colourgrams GUI: A graphical user-friendly interface for the analysis of large datasets of RGB images	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Colourgrams; RGB images; Multivariate image analysis; Data exploration; Multivariate calibration; MATLAB graphical user interface	COMPUTER VISION; ELECTRONIC EYE; DATA FUSION; FOOD; COLOR; QUALITY; CLASSIFICATION; QUANTIFICATION; IDENTIFICATION; PREDICTION	Colourgrams GUI is a graphical user-friendly interface developed in order to facilitate the analysis of large datasets of RGB images through the colourgrams approach. Briefly, the colourgrams approach consists in converting a dataset of RGB images into a matrix of one-dimensional signals, the colourgrams, each one codifying the colour content of the corresponding original image. This matrix of signals can be in turn analysed by means of common multivariate statistical methods, such as Principal Component Analysis (PCA) for exploratory analysis of the image dataset, or Partial Least Squares (PLS) regression for the quantification of colour-related properties of interest. Colourgrams GUI allows to easily convert the dataset of RGB images into the colourgrams matrix, to interactively visualize the signals coloured according to qualitative and/or quantitative properties of the corresponding samples and to visualize the colour features corresponding to selected colourgram regions into the image domain. In addition, the software also allows to analyse the colourgrams matrix by means of PCA and PLS.																	0169-7439	1873-3239				JAN 15	2020	196								103915	10.1016/j.chemolab.2019.103915													
J								All sparse PCA models are wrong, but some are useful. Part I: Computation of scores, residuals and explained variance	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Sparse principal component analysis; Explained variance; Scores; Residuals; Exploratory data analysis	COMPONENT; SELECTION	Sparse Principal Component Analysis (sPCA) is a popular matrix factorization approach based on Principal Component Analysis (PCA) that combines variance maximization and sparsity with the ultimate goal of improving data interpretation. When moving from PCA to sPCA, there are a number of implications that the practitioner needs to be aware of. A relevant one is that scores and loadings in sPCA may not be orthogonal. For this reason, the traditional way of computing scores, residuals and variance explained that is used in the classical PCA can lead to unexpected properties and therefore incorrect interpretations in sPCA. This also affects how sPCA components should be visualized. In this paper we illustrate this problem both theoretically and numerically using simulations for several state-of-the-art sPCA algorithms, and provide proper computation of the different elements mentioned. We show that sPCA approaches present disparate and limited performance when modeling noise-free, sparse data. In a follow-up paper, we discuss the theoretical properties that lead to this undesired behavior. We title this series of papers after the famous phrase of George Box "All models are wrong, but some are useful" with the same original meaning: sPCA models are only approximations of reality and have structural limitations that should be taken into account by the practitioner, but properly applied they can be useful tools to understand data.																	0169-7439	1873-3239				JAN 15	2020	196								103907	10.1016/j.chemolab.2019.103907													
J								Linear programming applied to polarized Raman spectroscopy for elucidating molecular structure at surfaces	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Linear programming; Molecular structure; Vibrational spectroscopy	ORIENTATION; MONOLAYERS; SPECTRA; ABSORPTION; INTERFACE; DYE	We present a framework for using linear programming to solve a challenging problem in surface science, the elucidation of the structure and composition of adsorbed molecules from a mixture, using simulated data from polarized Raman experiments. In the past, methods applied in order to interpret such spectroscopic information were combinatorial approaches that are limited in scalability or accuracy. Quantum mechanical electronic structure calculations yield the optical response of a single molecule, from which spectra of a mixture can be determined by appropriate weighting. Furthermore, spectral obtained in different beam polarizations provide projections of the signal in the laboratory frame. We demonstrate that linear programming is an ideal tool for utilizing all of this information in order to provide the sought structural picture.																	0169-7439	1873-3239				JAN 15	2020	196								103898	10.1016/j.chemolab.2019.103898													
J								Feature selection and classification by minimizing overlap degree for class-imbalanced data in metabolomics	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Class-imbalance; Overlapping; Feature selection; Metabolomics	VARIABLE SELECTION; PRECISION-RECALL; REGULARIZATION; REGRESSION; LASSO; SMOTE	Learning from class-imbalanced data has gained increasing attention in recent years due to the massive growth of skewed data across many scientific fields such as metabolomics. Some researches show that it is not the imbalance itself which hinders the classification performance, but class overlapping do play an important role in the performance degradation when associated to class-imbalance. So alleviating the overlapping of the imbalanced data might be an effective way to improve the performance in class-imbalance learning. In this study, we propose two feature selection algorithms that aim to minimize the overlap degree between the majority and the minority, which is based on a simple assumption that decreasing overlap degree of a data set makes it more separable. The proposed MOSNS and MOSS methods are built via sparse regularization techniques. Simulation results indicate that our algorithms is effective in recognizing key features and control false discoveries for class-imbalance learning. Four class-imbalanced metabolomics data sets are also employed to test the performance of our algorithm, and a comparison with accuracy (ACC)-based and ROC-based selection procedures is performed. The result shows that our algorithms are highly competitive and can be an alternative feature selection strategy in class-imbalance learning.																	0169-7439	1873-3239				JAN 15	2020	196								103906	10.1016/j.chemolab.2019.103906													
J								Continuous statistical modelling in characterisation of complex hydrocolloid mixtures using near infrared spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Food hydrocolloids; Characterisation; NIR spectroscopy; CLPP; PLS	MCR-ALS; FOOD; DIFFERENTIATION; CARRAGEENANS; OILS	Hydrocolloids such as natural gums and carrageenans are used extensively in the food industry in various mixtures that are difficult to be characterised due to their similar chemical structure. The aim of this study was to develop an analytical framework for the identification and quantification of these compounds in complex mixtures using Near-infrared (NIR) spectroscopy and chemometrics. Partial Least Squares (PLS) regression accompanied by Continuous Locality Preserving Projections (CLPP) dimensionality reduction technique is proposed as chemometric framework. Four different analytical models based on this framework are developed and compared for the analysis of spectral fingerprints of food hydrocolloids mixtures. Classification results showed that this method allowed the discrimination of hydrocolloids in blends with a 100% of correct classification. The same scheme also allows the quantitative determination of the different types of food hydrocolloids (3 types) and/or their individual compounds (8 different compounds) with a relative low root mean square error of prediction (RMSEP) of 0.028 and 0.038 respectively.																	0169-7439	1873-3239				JAN 15	2020	196								103910	10.1016/j.chemolab.2019.103910													
J								Identification and visualization of cell subgroups in uncompensated flow cytometry data	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Automated FCM analysis; Annealing-based model-free expectation maximization; Automated gating; FCM data visualization; Quasi-supervised learning	JOINT DIAGONALIZATION; COMPENSATION	We propose a new method for identification and visualization of cell-sub groups in uncompensated multi-color flow cytometry data. The method combines annealing-based model-free expectation-maximization to identify cell sub-groups and joint diagonalization on clustered data for better visualization. The proposed method was evaluated on a real, publicly available 8-color flow cytometry dataset manually gated beforehand for lymphocytes. The results obtained in three separable scenarios indicate that the method accurately identifies cell subgroups while properly adjusting visualization of identified cell groups by reducing the spectral overlap between the different fluorochrome channels.																	0169-7439	1873-3239				JAN 15	2020	196								103892	10.1016/j.chemolab.2019.103892													
J								Weighted incremental minimax probability machine-based method for quality prediction in gasoline blending process	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Non-Gaussian; Near-infrared spectroscopy; Biased estimation; Minimax probability machine; Gasoline blending	NEAR-INFRARED SPECTROSCOPY; SUPPORT VECTOR MACHINES; NIR SPECTROSCOPY; REGRESSION; CLASSIFICATION; CALIBRATION; SPECTRA; SENSOR	Near-infrared (NIR) spectroscopy is frequently used to predict quality-relevant variables that are difficult to measure online. This technology can be applied by developing the NIR model in advance. Obtaining a high-accuracy NIR model is difficult using traditional modeling methods because process data inherently contain uncertainties and present strong non-Gaussian characteristics. Considering the difficulty in obtaining precise prediction results, biased estimation is important in producing qualified products when NIR spectroscopy is used in a feedback quality control system. The present work proposes a biased estimation model based on probabilistic representation to address the aforementioned issues. Additionally, a novel weighted incremental strategy with "just-in-time" learning is proposed to improve model adaptiveness. In this way, the NIR model could be established and maintained without imposing any distribution hypothesis on process data, and biased estimation could be obtained in the form of probability. The performance of the proposed method is demonstrated on an actual data set from a gasoline blending process.																	0169-7439	1873-3239				JAN 15	2020	196								103909	10.1016/j.chemolab.2019.103909													
J								Wavelet functional principal component analysis for batch process monitoring	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Functional principal component analysis; Wavelet function; Uneven-length problem; Nonlinear problem; Within-batch fault detection; Active learning strategy	FAULT-DETECTION; MODEL; DECOMPOSITION; PCA	To facilitate the understanding and analysis of process conditions, a novel wavelet functional principal component analysis is proposed for monitoring batch processes from the functional perspective. In the proposed method, the variables' trajectories are taken as smooth functions instead of discrete vectors. To this end, the original discrete variables are transferred into continuous functions using wavelet basis functions in an active way. This can not only highlight the subtle shape differences between the normal and faulty variables trajectories but also easily address the uneven-length issue in practical batch processes. Additionally, without unfolding the operation, the 3D matrix is transferred into the functional matrix directly. The functional principal component analysis method is then performed on the functional space to establish monitoring models. Thanks to the compact-support characteristics of the wavelet functions, the proposed method can be directly applied to within-batch detection without data pre-treatment. A numerical case, a case of the simulated penicillin fermentation process, and a case of the laboratorial injection molding process are given to demonstrate the effectiveness of the proposed method.																	0169-7439	1873-3239				JAN 15	2020	196								103897	10.1016/j.chemolab.2019.103897													
J								Data mining assisted prediction of liquidus temperature for primary crystallization of different electrolyte systems	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Aluminium electrolysis industry; Electrolyte; Liquidus temperature; Support vector machine; Back-propagation artificial neural network; Random forest regression; Gradient boosting regression	SUPPORT VECTOR MACHINE; REGRESSION; CRYOLITE	Liquidus temperature for primary crystallization is an important physical and chemical property for electrolyte system. It plays a crucial role on the stability of the electric cell in electrolysis production process. So how to accurately predict the liquidus temperature for primary crystallization of electrolyte based on the composition of electrolyte is a meaningful research subject. In this work, data mining assisted prediction of liquidus temperature for primary crystallization of electrolyte systems was proposed. The essential differences between the complex industrial electrolyte system and electrolyte system prepared in laboratory were revealed by means of comparing the micro-morphology, phase composition and thermal analysis. To some extent, it was verified that the empirical formula has no versatility in the two different electrolyte systems. The prediction model of liquidus temperature for primary crystallization of different electrolyte systems was constructed by using SVM(support vector machine), BPANN(back-propagation artifical neural networks), RFR(random forest regression) and GBR(gradient boosting regression) algorithm, respectively. The electroyte system inculdes Na3AlF6(CR)-Al2O3-AlF3-CaF2, Na3AlF6(CR)-Al2O3-MgF2-CaF2-LiF, Na3AlF6(CR)Al2O3-MgF2-CaF2-KF-LiF, and Na3AlF6(CR)-Al2O3-AlF3-CaF2-MgF2-LiF-KF-NaF. For different electrolyte systems, ANN, SVM, RFR and other models all have good performances, they can effectively predict the liquidus temperature for primary crystallization of each electrolyte systems. For some electrolyte systems, ANN, SVM, RFR models are obviously superior to the prediction level of empirical formula described in the literature. It can be seen that data mining has a good application prospect in the prediction of the liquidus temperature for primary crystallization of electrolyte systems. We provide a new method for predicting the liquidus temperature for primary crystallization of different electrolyte systems based on the electrolyte composition dataset in this work.																	0169-7439	1873-3239				JAN 15	2020	196								103885	10.1016/j.chemolab.2019.103885													
J								Does the signal contribution function attain its extrema on the boundary of the area of feasible solutions?	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										signal contribution function; Multivariate curve resolution; Lawton-Sylvestre plot; Borgen plot; Area of feasible solutions; MCR-Bands; FACPACK	MULTIVARIATE CURVE RESOLUTION; POLYGON INFLATION ALGORITHM; ROTATION AMBIGUITIES; BAND BOUNDARIES; COMPUTATION; RANGE	The signal contribution function (SCF) was introduced by Gemperline in 1999 and Tauler in 2001 in order to study band boundaries of multivariate curve resolution (MCR) methods. In 2010 Rajko pointed out that the extremal profiles of the SCF reproduce the limiting profiles of the Lawton-Sylvestre plots for the case of noise-free two-component systems. This paper mathematically investigates two-component systems and includes a self-contained proof of the SCF-boundary property for two-component systems. It also answers the question if a comparable behavior of the SCF still holds for chemical systems with three components or even more components with respect to their area of feasible solutions. A negative answer is given by presenting a noise-free three-component system for which one of the profiles maximizing the SCF is represented by a point in the interior of the associated area of feasible solutions.																	0169-7439	1873-3239				JAN 15	2020	196								103887	10.1016/j.chemolab.2019.103887													
J								Variable contribution identification and visualization in multivariate statistical process monitoring	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Biplots; Process monitoring; Variable contribution	PRINCIPAL COMPONENT	Multivariate statistical process monitoring (MSPM) has received book-length treatments and wide spread application in industry. In MSPM, multivariate data analysis techniques such as principal component analysis (PCA) are commonly employed to project the (possibly many) process variables onto a lower dimensional space where they are jointly monitored given a historical or specified reference set that is within statistical control. In this paper, PCA and biplots are employed together in an innovative way to develop an efficient multivariate process monitoring methodology for variable contribution identification and visualization. The methodology is applied to a commercial coal gasification production facility with multiple parallel production processes. More specifically, it is shown how the methodology is used to specify the optimal principal component combinations and biplot axes for visualization and interpretation of process performance, and for the identification of the critical variables responsible for performance deviations, which yielded direct benefits for the commercial production facility.																	0169-7439	1873-3239				JAN 15	2020	196								103894	10.1016/j.chemolab.2019.103894													
J								Probabilistic just-in-time approach for nonlinear modeling with Bayesian nonlinear feature extraction	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Nonlinear dimensionality reduction; Soft sensor; Probabilistic models; Missing values	PRINCIPAL COMPONENT ANALYSIS; DATA ANALYTICS; SOFT SENSORS	In this work, we propose a probabilistic just-in-time (PJIT) modeling methodology with nonlinear feature extraction for estimating quality variables of interest. In literature, deterministic nonlinear feature extraction methods have been employed to deal with high dimensional input data. However, these methods require prespecifying the latent dimensions, which often results in overfitting. To circumvent this issue, we employ the Bayesian Gaussian process latent variable model (BGPLVM) to extract nonlinear latent variables and determine their dimensions automatically. Owing to the probabilistic framework, the proposed approach involves computing the variational distribution of latent variables for the query sample as well as historical samples, and selecting relevant samples based on a distribution measure for building a local Gaussian process model to predict the quality variable. Furthermore, the applicability of the proposed approach to missing data and multi-rate data is discussed. Two case studies are presented to demonstrate the efficacy of the proposed PJIT model.																	0169-7439	1873-3239				JAN 15	2020	196								103895	10.1016/j.chemolab.2019.103895													
J								Quantitative structure-activity relationship (QSAR) models and their applicability domain analysis on HIV-1 protease inhibitors by machine learning methods	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										HIV-1 protease inhibitors (HIV-1 PIs); Quantitative structure-activity relationship (QSAR); Support vector machine (SVM); Random forest (RF); Deep neural networks (DNN); Applicability domain	STRUCTURE-BASED DESIGN; X-RAY; BIOLOGICAL EVALUATION; ANTIVIRAL ACTIVITY; CYCLIC UREAS; TERTIARY-ALCOHOL; SELECTION; PREDICTION; BIOAVAILABILITY; CLASSIFICATION	HIV-1 protease inhibitors (PIs) make a vital contribution on highly active antiretroviral therapy (HAART) of human immunodeficiency virus (HIV). In this study, 14 quantitative structure-activity relationship (QSAR) models on 1238 PIs were built by four machine learning methods, including multiple linear regression (MLR), support vector machine (SVM), random forest (RF) and deep neural networks (DNlN). For the best model Model2G constructed by DNN algorithm, the coefficient of determination (R-2) of 0.88 and 0.79, the root mean squared error (RMSE) of 0.39 and 0.51 were obtained on training set and test set, respectively. For model Model2G, the applicability domain threshold (ADT) of 1.765 was obtained for training set, a compound that has a similarity distance (d) less than the ADT is considered to be inside the applicability domain, could be predicted accurately, and thus 65.37% compounds in test set performed reliable. In addition, the 1238 PIs were manually divided into eight subsets containing different scaffolds. It was found that hydroxylamine derivatives and sevenmember cyclic urea derivatives showed highly inhibitory activity comparing with other subsets. We also built QSAR models with SVM, RF and DNN methods on two subsets of 299 hydroxylamine derivatives inhibitors (Dataset2) and 377 seven-member cyclic urea derivatives inhibitors (Dataset3). For the best model Model3A on Dataset2, R-2 of 0.71 and RMSE of 0.53 were obtained for test set. For the best model Model4B on Dataset3, R-2 of 0.82 and RMSE of 0.51 were obtained for test set. At last, we analyzed the descriptors which make significant contributions on the bioactivity of inhibitors among these two subsets. It was found that highly active inhibitors of seven-member cyclic urea derivatives usually contained several aromatic nitrogen heterocyclic ring substituents such as the inidazole and the pyrazole. The oxazolidinone group and sulfanilamide mainly appeared in highly active inhibitors of hydroxylamine derivatives. These observations may be utilized further in designing promising HIV-1 protease inhibitors.																	0169-7439	1873-3239				JAN 15	2020	196								103888	10.1016/j.chemolab.2019.103888													
J								Integrated integer programming and decision diagram search tree with an application to the maximum independent set problem	CONSTRAINTS										Decision diagrams; Integrated methods; Integer linear programming; Hybrid optimization; Supervised learning	BRANCH-AND-PRICE; OPTIMIZATION; ALGORITHM	We propose an optimization framework which integrates decision diagrams (DDs) and integer linear programming (ILP) to solve combinatorial optimization problems. The hybrid DD-ILP approach explores the solution space based on a recursive compilation of relaxed DDs and incorporates ILP calls to solve subproblems associated with DD nodes. The selection of DD nodes to be explored by ILP technology is a significant component of the approach. We show how supervised machine learning can be useful to detect, on-the-fly, a subproblem structure for ILP technology. We use the maximum independent set problem as a case study. Computational experiments show that, in presence of suitable problem structure, the integrated DD-ILP approach can exploit complementary strengths and improve upon the performance of both a stand-alone DD solver and an ILP solver in terms of solution time and number of solved instances.																	1383-7133	1572-9354				APR	2020	25	1-2					23	46		10.1007/s10601-019-09306-w		JAN 2020											
J								ArA*summarizer: An Arabic text summarization system based on subtopic segmentation and using an A* algorithm for reduction	EXPERT SYSTEMS										data-driven; graph theory; information extraction; mathematics; method; natural language processing; text analysis; text mining; topic identification		Automatic text summarization is a field situated at the intersection of natural language processing and information retrieval. Its main objective is to automatically produce a condensed representative form of documents. This paper presents ArA*summarizer, an automatic system for Arabic single-document summarization. The system is based on an unsupervised hybrid approach that combines statistical, cluster-based, and graph-based techniques. The main idea is to divide text into subtopics then select the most relevant sentences in the most relevant subtopics. The selection process is done by an A* algorithm executed on a graph representing the different lexical-semantic relationships between sentences. Experimentation is conducted on Essex Arabic Summaries Corpus and using recall-oriented understudy for gisting evaluation, automatic summarization engineering, merged model graphs, and n-gram graph powered evaluation via regression evaluation metrics. The evaluation results showed the good performance of our system compared with existing works.																	0266-4720	1468-0394				APR	2020	37	2							e12476	10.1111/exsy.12476		JAN 2020											
J								Gated multimodal networks	NEURAL COMPUTING & APPLICATIONS										Multimodal learning; Representation learning; Information fusion; GMU	FUSION; TESTS	This paper considers the problem of leveraging multiple sources of information or data modalities (e.g., images and text) in neural networks. We define a novel model called gated multimodal unit (GMU), designed as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities.The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates.The GMU can be used as a building block for different kinds of neural networks and can be seen as a form of intermediate fusion. The model was evaluated on two multimodal learning tasks in conjunction with fully connected and convolutional neural networks. We compare the GMU with other early- and late-fusion methods, outperforming classification scores in two benchmark datasets: MM-IMDb and DeepScene.																	0941-0643	1433-3058				JUL	2020	32	14					10209	10228		10.1007/s00521-019-04559-1		JAN 2020											
J								A Privacy-Preserving Multi-Task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition	FRONTIERS IN NEUROROBOTICS										multi-task learning; privacy preserving; differential private stochastic gradient descent; balance different learning tasks; differential privacy guarantees	NEURAL-NETWORK; NOISE	Recently, multi-task learning (MTL) has been extensively studied for various face processing tasks, including face detection, landmark localization, pose estimation, and gender recognition. This approach endeavors to train a better model by exploiting the synergy among the related tasks. However, the raw face dataset used for training often contains sensitive and private information, which can be maliciously recovered by carefully analyzing the model and outputs. To address this problem, we propose a novel privacy-preserving multi-task learning approach that utilizes the differential private stochastic gradient descent algorithm to optimize the end-to-end multi-task model and weighs the loss functions of multiple tasks to improve learning efficiency and prediction accuracy. Specifically, calibrated noise is added to the gradient of loss functions to preserve the privacy of the training data during model training. Furthermore, we exploit the homoscedastic uncertainty to balance different learning tasks. The experiments demonstrate that the proposed approach yields differential privacy guarantees without decreasing the accuracy of HyperFace under a desirable privacy budget.																	1662-5218					JAN 14	2020	13								112	10.3389/fnbot.2019.00112													
J								Comparison of supervised learning statistical methods for classifying commercial beers and identifying patterns	JOURNAL OF CHEMOMETRICS										beer; error estimation; fruit beer; learning algorithms; multiple factor analysis (MFA)	CHEMOMETRICS; PERFORMANCE; PREDICTION	In this study, 13 properties (alcohol-, real extract-, flavonoid-, anthocyanin, glucose, fructose, maltose, sucrose content, EBC [European Brewery Convention] and L*a*b* color, bitterness) of 21 beers (alcohol-free pale lagers, alcohol-free beer-based mixed drinks, beer-based mixed drinks, international lagers, wheat beers, stouts, fruit beers) were determined. In the first step, multiple factor analysis (MFA) was performed for the whole data and five clusters (target classes) were determined; then, a bootstrapping was applied to establish a balanced data so as every cluster should contain 100 samples and the total sample size is 500. In the second step, 12 supervised learning algorithms (random trees [RND], Quinlan's C4.5 decision tree algorithm [C4.5], Iterative Dichotomiser 3 algorithm [ID3], cost-sensitive decision tree algorithm [CSMC4], cost-sensitive classification tree [CSCRT], k-nearest neighbors algorithm [KNN], radial basis function [RBF], multilayer perceptron neural network [MLP], prototype nearest neighbor [PNN], linear discriminant analysis [LDA], naive Bayes with continuous variables [NBC], partial least squares discriminant analysis [PLS-DA]) were applied to classify each brand into the target classes. Furthermore, several error rates were calculated: re-substitution error rate (RER), cross-validated error rate (CV), bootsrap error (BOOT), leave-one-out (LOO), and train-test error rate (TRAIN). The MFA could discriminate five groups, which can be characterized by some analytical parameters, and the other multivariate methods performed similarly. The methods can be discriminated best based on the BOOT, CV, and LOO. The best estimation methods are the C4.5, CSMC4, and CSCRT; these performed best along the flavonoid content and EBC color. It identified that the methods most sensitive to the properties are the NBC. The classification ability fluctuated greatly in the case of three properties (glucose, maltose, sucrose). A remarkable fluctuation has been experienced in the case of L*a*b* color parameters, flavonoid content, EBC color, and bitterness by NBC method.																	0886-9383	1099-128X														e3216	10.1002/cem.3216		JAN 2020											
J								DL-VSM based document indexing approach for information retrieval	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Document indexing; Vector space model; Description logics; Partial matching; Stemming; Biomedical vocabulary	CONCEPT EXTRACTION; MODEL; STEMMER; SYSTEM; LABELS; TF	Textual information is constantly increasing. With this accumulation of documents, the satisfaction of user needs becomes more and more complex. For that, several information retrieval systems have been designed in order to respond to user requests. Document indexing is considered as a crucial phase in the information retrieval field. The main contribution of the current work resides in the suggestion of a novel hybrid approach for biomedical document indexing. We improve the estimation of the correspondence between a document and a given concept using two methods: vector space model (VSM) and description logics (DL). VSM performs partial matching between documents and external resource terms. DL allows representing knowledge in a relevant manner for better matching. The proposed contribution reduces the limitation of exact matching. It serves to index documents by exploiting medical subject headings (MeSH) thesaurus services with approximate matching. The latter partially matches document terms with biomedical vocabularies to extract other morphological variants in that resource. It also generates irrelevant concepts. The filtering step solves this problem and grants the selection of the most important concepts by exploiting the knowledge provided by MeSH. The experiments, carried out on different corpora, show encouraging results (+ 25% improvement in average accuracy compared to other approaches in the literature).																	1868-5137	1868-5145															10.1007/s12652-020-01684-x		JAN 2020											
J								Images data practices for Semantic Segmentation of Breast Cancer using Deep Neural Network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computer aided detection; Deep learning; Feature extraction; Neural network; Breast cancer; Cancer prediction	MAMMOGRAPHY; CLASSIFICATION; MASSES; CAD	Image data in healthcare is playing a vital role. Medical data records are increasing rapidly, which is beneficial and detrimental at the same time. Large Image dataset are difficult to handle, extracting information, and machine learning. The mammograms data used in this research are low range x-ray images of the breast region, which contains abnormalities. Breast cancer is the most frequently diagnosed cancer and ranked 9th worldwide in breast cancer-related deaths. In Pakistan 1 in 9 women expected to have breast cancer at some stage in life. Screening mammography is the most effective means for its early detection. This high rate of oversampling is responsible for billions in excess health care cost and unnecessary patient anxiety. This research mainly focuses on the development of deep learning based computer-aided system to detect, classify and segment the cancerous region in mammograms. Moreover, the preprocessing mechanism is proposed that remove noise, artifacts and muscle region that can cause a high false positive rate. In order to increase the efficiency of the system and counter the large resource requirement, the pre-processed image is converted to 512 x 512 patches. The two publicly available breast cancer dataset are employed i.e. Mammographic Image Analysis Society (MIAS) digital mammogram dataset and Curated Breast Imaging Subset of (Digital Database for Screening Mammography) (CBIS-DDSM). The two states of art deep learning-based instance segmentation frameworks are used, i.e. DeepLab and Mask RCNN. The pre-processing algorithm helps to increase the area under the receiver operating curve for each transfer learning method. The fine tuning is performed for better performance, the area under the curve was equal to 0.98 and 0.95 for mask RCNN and deep lab respectively on a test set of 150 cases. However, mean average precision for the segmentation task is 0.80 and 0.75. The radiologists accuracy ranged from 0.80 to 0.88. The proposed research has the potential to help radiologists with breast mass classification as well as segmentation of the cancerous region.																	1868-5137	1868-5145															10.1007/s12652-020-01680-1		JAN 2020											
J								Improving parallel executions by increasing task granularity in task-based runtime systems using acyclic DAG clustering	PEERJ COMPUTER SCIENCE										Task-based; Graph; DAG; Clustering; Partitioning	ALGORITHM; OPENMP	The task-based approach is a parallelization paradigm in which an algorithm is transformed into a direct acyclic graph of tasks: the vertices are computational elements extracted from the original algorithm and the edges are dependencies between those. During the execution, the management of the dependencies adds an overhead that can become significant when the computational cost of the tasks is low. A possibility to reduce the makespan is to aggregate the tasks to make them heavier, while having fewer of them, with the objective of mitigating the importance of the overhead. In this paper, we study an existing clustering/partitioning strategy to speed up the parallel execution of a task-based application. We provide two additional heuristics to this algorithm and perform an in-depth study on a large graph set. In addition, we propose a new model to estimate the execution duration and use it to choose the proper granularity. We show that this strategy allows speeding up a real numerical application by a factor of 7 on a multi-core system.																	2376-5992					JAN 13	2020									e247	10.7717/peerj-cs.247													
J								Artificial bee colony algorithm for content-based image retrieval	COMPUTATIONAL INTELLIGENCE										artificial bee colony; content-based image retrieval; gray-level cooccurrence matrix; material classification; texture feature extraction	TEXTURE; CLASSIFICATION; FEATURES; ABC	Image retrieval is the process of searching for digital images from a large database. There exist two distinctive research groups, which employ the content-based and description-based approaches, respectively. However, research in the content-based domain is currently dominant in the field, while the other approaches are not as widely utilized. Although there are a number of different techniques that are available for image retrieval, the development of more effective methods is still necessary. In recent years, previous research has shown that biologically inspired metaheuristic algorithms have great potential for use in solving problems in many science and engineering domains. The artificial bee colony (ABC) algorithm is one of the more promising biologically inspired metaheuristic approaches used to find optimal solutions as it has the advantages of convenient implementation and efficient performance. In this article, a new efficient method based on a combination of the gray-level cooccurrence matrix (GLCM) with the ABC, referred to as "GLCM-ABC," is proposed for use in content-based image retrieval (CBIR). The experimental results demonstrate that the proposed approach works well for CBIR and can classify specific types of material surfaces in images with a reasonably high level of accuracy as well as outperform other existing algorithms.																	0824-7935	1467-8640				FEB	2020	36	1					351	367		10.1111/coin.12275		JAN 2020											
J								Variable importance: Comparison of selectivity ratio and significance multivariate correlation for interpretation of latent-variable regression models	JOURNAL OF CHEMOMETRICS										biomarkers; selectivity ratio; significance multivariate correlation; variable importance; variable selection	PARTIAL LEAST-SQUARES; MASS-SPECTRAL PROFILES; PROJECTION VIP; BIOCHEMOMETRICS; CLASSIFICATION; COMPONENTS	This work examines the performance of significance multivariate correlation (sMC) and selectivity ratio (SR) for ranking variables according to their importance in latent-variable regressions (LVRs) models. Both indices are based on target projection (TP) of a validated LVR model obtained by partial least squares (PLS). The matrix of explanatory x-variables is projected on the normalized regression vector to obtain a score vector that is proportional to the vector of predicted values for the response variable y. sMC for each x-variable is calculated by dividing the squared variance explained by the decomposition obtained from these two vectors on the squared residuals. This is similar to how SR is calculated except that for SR, the regression vector is replaced by the loading matrix obtained by projecting the data matrix of x-variables onto the score matrix obtained by TP. The two indices for variable importance are compared for three different applications with data representing instrumental profiles from liquid chromatography, infrared spectroscopy, and proton nuclear magnetic spectroscopy. Results show that SR outperforms sMC for interpretation and biomarker selection. The main drawback of sMC appears to be the mixing of predictive and orthogonal variation resulting from the direct use of the normalized regression vector in the calculation. SR uses a loading vector that is proportional to the covariances between x-variables and the predicted response variable.																	0886-9383	1099-128X														e3211	10.1002/cem.3211		JAN 2020											
J								Assessing stationarity in web analytics: A study of bounce rates	EXPERT SYSTEMS										bounce rate; DTW; numerical analysis; stationary progress; time series	SERIES; TIME	Evidence-based methods for evaluating marketing interventions such as A/B testing have become standard practice. However, the pitfalls associated with the misuse of this decision-making instrument are not well understood by managers and analytics professionals. In this study, we assess the impact of stationarity on the validity of samples from conditioned time series, which are abundant in web metrics. Such a prominent metric is the bounce rate, which is prevalent in assessing engagement with web content as well as the performance of marketing touchpoints. In this study, we show how to control for stationarity using an algorithmic transformation to calculate the optimum sampling period. This distance is based on a novel stationary ergodic process that considers that a stationary series presents reversible symmetric features and is calculated using a dynamic time warping algorithm in a self-correlation procedure. This study contributes to the expert and intelligent systems literature by demonstrating a robust method for sub-sampling time-series data, which are critical in decision making.																	0266-4720	1468-0394				JUN	2020	37	3			SI				e12502	10.1111/exsy.12502		JAN 2020											
J								An epsilon-constraint method for fully fuzzy multiobjective linear programming	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										epsilon-constraint method; fully fuzzy multiobjective linear programming; lexicographic ranking criterion; LR fuzzy number; project crashing	INEQUALITY CONSTRAINTS; NUMBER	Linear ranking functions are often used to transform fuzzy multiobjective linear programming (MOLP) problems into crisp ones. The crisp MOLP problems are then solved by using classical methods (eg, weighted sum, epsilon-constraint, etc), or fuzzy ones based on Bellman and Zadeh's decision-making model. In this paper, we show that this transformation does not guarantee Pareto optimal fuzzy solutions for the original fuzzy problems. By using lexicographic ranking (LR) criteria, we propose a fuzzy epsilon-constraint method that yields Pareto optimal fuzzy solutions of fuzzy variable and fully fuzzy MOLP problems, in which all parameters and decision variables take on LR fuzzy numbers. The proposed method is illustrated by means of three numerical examples, including a fully fuzzy multiobjective project crashing problem.																	0884-8173	1098-111X				APR	2020	35	4					600	624		10.1002/int.22219		JAN 2020											
J								A Smart Environments Architecture (Search)	APPLIED ARTIFICIAL INTELLIGENCE											PEOPLE; USERS	We report on a system architecture, SEArch, and its associated methods and tools we have been developing, testing, and extending for several years through a number of innovation processes in the field of Smart Environments. We have developed these infrastructure in a bottom-up fashion directed by the needs of the different projects as opposed to an ideal one which projects have to conform to. In this sense is practical and although necessarily incomplete, it has significant versatility and reasonable efficiency. Projects developed using this architecture have been funded by different companies and funding bodies in Europe. The different components of the architecture are explained through the software supporting those aspects of the system and through the functionality they exhibit in different practical scenarios, extracted from some of the projects implemented with SEArch.																	0883-9514	1087-6545				JAN 28	2020	34	2					155	186		10.1080/08839514.2020.1712778		JAN 2020											
J								Multi feature behavior approximation model based efficient botnet detection to mitigate financial frauds	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Botnet attack; Money laundering; Financial frauds; Behavior analysis; MFTS; BTS		Money laundering and other financial frauds are increasing day by day and the financial industries face various challenges from them. They construct botnets to generate such fraudulent attacks towards financial sectors. To mitigate such threats and detect the presence of botnet, different solutions have been arrived earlier. But they struggle to achieve higher performance in detecting such botnet and restrict them from fraudulent transactions. To improve the performance, a novel multi feature behavior approximation algorithm has been presented in this article. The multi feature behavior approximation algorithm monitors each transaction performed by different users, their behavior in accessing service, the status of service access and so on. This botnet detection scheme monitors the behaviors of users and intermediate nodes involved in each transaction. Using the trace, the method performs behavior approximation in two ways like source orient and intermediate orient. In both the scheme, the method considers the frequency of transactions, their status, completion, the intermediate nodes involved and their reputation. Using all these, multi feature trust measure (MFTS) is estimated. Based on the value of MFTS, the method detects the presence of botnet and mitigates them by eliminating the node according to the backward trust score. The transaction has been accepted only when the backward trust score is high enough. The proposed algorithm improves the performance of botnet detection and reduces the frequency of money laundering.																	1868-5137	1868-5145															10.1007/s12652-020-01677-w		JAN 2020											
J								LRBC: a lightweight block cipher design for resource constrained IoT devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Lightweight encryption; IoT; Block cipher; FPGA; ASIC chip	INTERNET; CRYPTANALYSIS; THINGS	The internet of things (IoT) is now an in-demand technology that has been adopted in various applications and includes various embedded devices, sensors and other objects connected to the Internet. Due to the rapid development of this technology, it covers a significant portion of the research interests nowadays. IoT devices are typically designed for collecting different types of data from various sources and transmitting them in digitized form. However, data security is the burning issue in the IoT technology, which can broadly impact the privacy of crucial data. In this regard, a new lightweight encryption method called LRBC has been proposed in this work for resource constraint IoT devices which can provide data security at the sensing level. The LRBC has used the structural advantages of both substitution-permutation network (SPN) and Feistel structure together to achieve better security. Furthermore, the proposed method has been tested on NEXYS 4 DDR FPGA (Artix-7) trainer kit and implemented for application specific integrated circuit (ASIC) chip on TSMC 65 nm technology. The proposed algorithm consumes very less power of 11.40 mu W and occupies a 258.9 GE (Gate Equivalent) area. Besides, a thorough security analysis shows that the proposed scheme ensures high security against various attacks with robustness. Moreover, the average avalanche effect of LRBC is found to be 58% and 55.75% concerning plaintext and key, respectively.																	1868-5137	1868-5145															10.1007/s12652-020-01694-9		JAN 2020											
J								Development and experimental validation of algorithms for human-robot interaction in simulated and real scenarios	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human-robot interaction; Human-robot cooperation; Robotic manipulators	GESTURE; RECOGNITION; SYSTEM	The development of robots, which can safely and effectively interact with people and assist them in structured environments, is an open research problem whose importance has been growing rapidly in the last years. Indeed working in shared environments with human beings, these robots require new ways to achieve human-robot interaction and cooperation. This work presents an approach for performing human-robot interaction by means of robotic manipulators. The interaction is composed by three main steps, namely the selection, the recognition and the grasping of an object. The object selection is recorded on the base of a gesture execution, realized by the user in front of a RGB-D camera and associated to each particular object. The object recognition is achieved by means of the RGB cameras mounted on the two manipulator arms, which send the workspace information to a specific classifier. With the aim of realizing the grasping step, the object position and orientation are extracted in order to correctly rotate the gripper according to the object on the desk in front of the robot. The final goal is to release the grasped object on the hand of the user standing in front of the desk. This system could support people with limited motor skills who are not able to get an object on their own, playing an important role in structured assistive and smart environments, thus promoting the human-robot interaction in activity of daily living.																	1868-5137	1868-5145															10.1007/s12652-019-01676-6		JAN 2020											
J								A new whale optimizer for workflow scheduling in cloud computing environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless communication; Service and semantic computing; Autonomic computing; Whale optimizer; Makespan; Bubble-net search mechanism	ALGORITHMS	Cloud computing environments enable real time applications on virtualized resources that can be provisioned dynamically. It is one of the efficient platform service which permits to enable the various applications based on cloud infrastructure. Nowadays workflow systems become an easy and efficient task for the development of scientific applications. Efficient workflow scheduling algorithms are employed to improve the resource utilization by enhancing the cloud computing performance and to meet the users' requirements. Many scheduling algorithms have been proposed but they are not optimal to incorporate benefits of cloud computing. In this paper a new framework are introduced as whale optimizer algorithm (WOA) which mimics the social behaviour of humpback whales and aims to maximize the work completion for meeting QoS constraints such as deadline and budget. This proposed method outperforms well when compared with other techniques and measured in terms of makespan, deadline and it is applicable for real time applications.																	1868-5137	1868-5145															10.1007/s12652-020-01678-9		JAN 2020											
J								Brain tumor detection based on extreme learning	NEURAL COMPUTING & APPLICATIONS										Fuzzy rules; Erosion; MRI; Dilation; Gabor filter; Gliomas	CONVOLUTIONAL NEURAL-NETWORKS; SEGMENTATION; CLASSIFICATION; FRAMEWORK; TISSUES; FUSION; IMAGES; VOLUME; EDEMA; CRF	Gliomas are dreadful and common type of brain tumor. Therefore, treatment planning is significant to increase the survival rate of gliomas patients. The large structural and spatial variation between tumors makes an automated detection more challenging. Brain magnetic resonance imaging is utilized for tumor evaluation on the basis of automated segmentation and classification methods. In this work, triangular fuzzy median filtering is applied for image enhancement that helps in accurate segmentation based on unsupervised fuzzy set method. Gabor features are extracted across each candidate's lesions, and similar texture (ST) features are calculated. These ST features are supplied to extreme learning machine (ELM), and regression ELM leaves one out for tumor classification. The technique is evaluated on BRATS 2012, 2013, 2014 and 2015 challenging datasets as well as on 2013 Leader board. The proposed approach shows better results and less computational time.																	0941-0643	1433-3058				OCT	2020	32	20			SI		15975	15987		10.1007/s00521-019-04679-8		JAN 2020											
J								A Safe Semi-supervised Classification Algorithm Using Multiple Classifiers Ensemble	NEURAL PROCESSING LETTERS										Semi-supervised learning; Safety; Multiple classifiers; Ensemble; Filter	INTERNET	In order to improve the performance of semi-supervised learning, a safe semi-supervised classification algorithm using multiple classifiers ensemble (S3C-MC) is proposed. First, unlabeled samples are filtered and unlabeled samples with small ambiguity are selected for semi-supervised learning. Next, the labeled training set is sampled to multiple subsets and they generate multiple classifiers to predict the filtered unlabeled sample respectively. The predicted label is formed by multiple classifiers with weighted voting mechanism, and the weight of classifier is changing constantly according to the correctness of the prediction for unlabeled samples by classifier. Then, security verification is carried out to ensure that the classifier evolves in the direction of error reduction when the new sample is added. Only the label making classifiers error lower and having the same predictive value with the three classifiers in security verification is added into the labeled set to expand the number of labeled sets. Finally, the algorithm iterates until the unlabeled sample set is empty. The experiment is carried out on the UCI data set and the result shows that the proposed S3C-MC has good safety and has a higher classification rate.																	1370-4621	1573-773X															10.1007/s11063-020-10191-1		JAN 2020											
J								Algorithms and values in justice and security	AI & SOCIETY										Values; Value sensitive design; Responsibility; Ethics; Algorithms; Justice; Security; AI	BIG DATA; GROUP PRIVACY; RISK; RESPONSIBILITY; DATAVEILLANCE; LOOKING; DESIGN; WAR	This article presents a conceptual investigation into the value impacts and relations of algorithms in the domain of justice and security. As a conceptual investigation, it represents one step in a value sensitive design based methodology (not incorporated here are empirical and technical investigations). Here, we explicate and analyse the expression of values of accuracy, privacy, fairness and equality, property and ownership, and accountability and transparency in this context. We find that values are sensitive to disvalue if algorithms are designed, implemented or deployed inappropriately or without sufficient consideration for their value impacts, potentially resulting in problems including discrimination and constrained autonomy. Furthermore, we outline a framework of conceptual relations of values indicated by our analysis, and potential value tensions in their implementation and deployment with a view towards supporting future research, and supporting the value sensitive design of algorithms in justice and security.																	0951-5666	1435-5655				SEP	2020	35	3					533	555		10.1007/s00146-019-00932-9		JAN 2020											
J								Bio-inspired dual cluster heads optimized routing algorithm for wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Energy efficiency; Routing; Power consumption; Krill Herd Optimization; Clustering; Data aggregation		The optimal usage of scarce and inadequate resources is the current need of wireless sensor networks (WSNs). Reducing power consumption and increasing WSN's lifetime can contribute to this. Energy-efficient algorithms should be proposed for effective routing, as a lot of energy is consumed during the communication between sensor nodes. A principal practice followed to bring down energy consumption is, combining together in-network data aggregation and standardized routing. Numerous conventional data aggregation algorithms exist, and each of these have their own disadvantages like delayed delivery of packets, time constraints, or high costs. The most suitable solution can therefore be achieved via optimization alone. The current research seeks to address these challenges faced in the wireless network by using the dual-cluster heads technique based on Krill Herd Optimisation (DC-KHO) Routing algorithm. Since the existing wireless routing approaches choose a random path for data transmission, the end- to- end delay, which is directly proportional to energy consumption, is high. So, the current study chooses an optimized path by calculating its path trust value using the devised krill herd maximization algorithm. The proposed DC-KHO algorithm is energy- efficient, and invariably amplifies the network's lifetime. The study's results reveal the ability of the proposed method in overcoming the challenges faced in transmission time, residual energy and computational time. Further, the method increases the life span of the network by 64.58%, when a 9 V battery is used for the nodes.																	1868-5137	1868-5145															10.1007/s12652-019-01657-9		JAN 2020											
J								Privacy sensitive environment re-decomposition for junction tree agent organization construction	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Coordination and control models for multiagent systems; Organisations and institutions; Self-organization; Privacy in MAS	DISTRIBUTED CONSTRAINT OPTIMIZATION; FRAMEWORK; NETWORKS	A number of frameworks for decentralized probabilistic reasoning, constraint reasoning, and decision theoretic reasoning assume a junction tree agent organization (JT-org). A natural decomposition of agent environment may not admit a JT-org. Hence, JT-org construction involves three related tasks: (1) Recognize whether a JT-org exists for a given environment decomposition. (2) When JT-orgs exist, construct one. (3) When no JT-org exists, revise the environment decomposition so that one exists and then construct it. Task 3 requires re-decomposition of the environment. However, re-decomposition incurs loss of JT-org linked privacy, including agent privacy, topology privacy, privacy on private variables, and privacy on shared variables. We propose a novel algorithm suite Distributed Agent Environment Re-decomposition (DAER) that accomplishes all three tasks distributively. For Tasks 1 and 2, DAER incurs no loss of JT-org linked privacy. For Task 3, it incurs significantly less privacy loss than existing JT-org construction methods. Performance of DAER is formally analyzed and empirically evaluated.																	1387-2532	1573-7454				JAN 10	2020	34	1							15	10.1007/s10458-019-09438-6													
J								Image-independent optimal non-negative integer bit allocation technique for the DCT-based image transform coders	IET IMAGE PROCESSING										wavelet transforms; adaptive signal processing; discrete cosine transforms; regression analysis; image coding; image denoising; image reconstruction; computational complexity; data compression; quantisation (signal); nonnegative integer bit allocation technique; DCT-based image; coders; optimum nonnegative integer bit allocation; optimal quantisation; transform coefficients; existing ONIBA algorithms; image-dependent nature; additional side information requirements; image-independent ONIBA technique; efficient quantisation; DCT-ITCs; IIONIBA technique; image-dependent ONIBA; desired image-independent solution; prepared combined image; step size mapping technique; quantisation tables; nonlinear regression analysis; reconstructed images; image quality indexes peak signal; recent quantisation techniques	COMPRESSION	The optimum non-negative integer bit allocation (ONIBA) is an important technique, which provides optimal quantisation of transform coefficients for the image transform coders (ITCs). However, the existing ONIBA algorithms are still not popular for the discrete cosine transform (DCT)-based ITCs, due to their image-dependent nature and additional side information requirements. Therefore, this study presents a novel image-independent ONIBA (IIONIBA) technique to achieve efficient quantisation for the DCT-ITCs. For the development of the proposed IIONIBA technique, initially, an image-dependent ONIBA algorithm is proposed, which is then mapped into desired image-independent solution via utilisation of a prepared combined image and proposed modified step size mapping technique. Thereafter, a new lookup table for the elements of quantisation tables, obtained from the proposed IIONIBA technique, is established using non-linear regression analysis, to reduce the problem of additional side information requirements. Several experiments are performed to evaluate the performance of the proposed IIONIBA technique based on the visual quality assessment of reconstructed images and the image quality indexes peak signal to noise ratio (PSNR) and mean structural similarity index (MSSIM). The results show that the proposed IIONIBA technique delivers better quantisation and provides significant gains in the image quality indexes as compared to the recent quantisation techniques.																	1751-9659	1751-9667				JAN 10	2020	14	1					11	24		10.1049/iet-ipr.2018.6302													
J								Binary tomography on the isometric tessellation involving pixel shape orientation	IET IMAGE PROCESSING										gradient methods; image reconstruction; minimisation; computerised tomography; reconstruction method; natural directions; lane directions; triangular tessellation; rectangular grids; projection ray; grid lane; middle line; triangle shape pixels; exact information; delta shape triangle pixels; reconstruction process; energy-minimisation problem; test images; quality reconstructions; binary tomography; isometric tessellation; pixel shape orientation; tomography reconstruction problem; binary images; isometric grid	DISCRETE TOMOGRAPHY; RECONSTRUCTION	In this study, a tomography reconstruction problem of binary images is considered on the isometric grid. On this grid, the triangle pixels have two types of orientations, accordingly, the authors call them delta or nabla shape pixels. The proposed reconstruction method uses data of projections of three natural directions. They are the lane directions of the triangular tessellation (these directions are somewhat analogous to row/column directions on the rectangular grids). The projection ray, penetrating through a grid lane, now not passing through the middle of pixels (i.e. through the middle line of triangle shape pixels), as usually taken, but little bit shifted from the middle parallel to the lane. This method provides the exact information about the number of nabla and delta shape triangle pixels in each lane of the image. This additional information is included in the reconstruction process to improve the quality of reconstruction. They formulate the suggested model into an energy-minimisation problem and apply a gradient-based approach for its minimisation. They show and analyse various experimental results on test images. The presented approach shows both better quality reconstructions and shorter running time than the earlier approaches.																	1751-9659	1751-9667				JAN 10	2020	14	1					25	30		10.1049/iet-ipr.2019.0099													
J								Image sorting via a reduction in travelling salesman problem	IET IMAGE PROCESSING										pattern clustering; content-based retrieval; image retrieval; travelling salesman problems; image sequences; sorting; genetic algorithms; content-based image clustering; content-based image sorting; similar content; travelling salesman problem; TSP problem; image sequences; unsupervised image sorting; GHIM-10K dataset	OPTIMIZATION; ALGORITHMS	The authors define and approximately solve the problem of unsupervised image sorting that is considered as a kind of content-based image clustering. The content-based image sorting is the creation of a route that passes through all the images once, in such an order that the next one from the previous image has similar content. In the end, an image ordering (e.g. slideshow) is automatically produced, so that the images with similar content should be close to each other. This problem resembles the problem known in the literature as 'travelling salesman problem' (TSP). In this work, the authors have proposed two classes of methods (the nearest-neighbour and genetic methods) that have also been applied on the TSP problem. Their benefits on computational efficiency and accuracy are discussed over six datasets that have been created from the GHIM-10K dataset. The experimental results demonstrate that the proposed methods efficiently solve the image sorting problem, producing image sequences that almost agree with human intuition.																	1751-9659	1751-9667				JAN 10	2020	14	1					31	39		10.1049/iet-ipr.2018.5880													
J								Colour image encryption scheme based on enhanced quadratic chaotic map	IET IMAGE PROCESSING										statistical analysis; cryptography; image colour analysis; chaos; image processing; chaotic communication; colour image encryption scheme; enhanced quadratic chaotic map; enhanced quadratic map; EQM; larger chaotic ranges; classical quadratic map; excellent confusion; diffusion properties; encryption structure; plaintext image; encrypted components; USC-SIPI image dataset; real-life image dataset; image encryption schemes	ALGORITHM; PERMUTATION; DIFFUSION; SYSTEM; CRYPTANALYSIS; CRYPTOSYSTEM; COMBINATION	In this study, an enhanced quadratic map (EQM) is proposed and has been applied in a new colour image encryption scheme. The performance evaluations show that the EQM has excellent performances such as better Lyapunov exponent and larger chaotic ranges when compared with the classical quadratic map. The sequences generated from this EQM are successfully used in a new proposed colour image encryption scheme with excellent confusion and diffusion properties. The encryption structure is based on the permutation-diffusion process, and then adopted on the classical permutation, it is characterised by a high speed of diffusion, which enables the encryption of the three components of the plaintext image at the same time, and these encrypted components are simultaneously related to each other. The proposed scheme is tested on the USC-SIPI image dataset and on the real-life image dataset; its effectiveness is also compared with five latterly proposed image encryption schemes. The simulation results indicate that the proposed scheme has the properties of large key space, a weaker correlation between neighbouring pixels, higher sensitivity towards key, greater randomness of pixels and the capacity to withstand statistical analysis, plaintext/chosen-plaintext attacks, and differential attacks, thus that it has higher security and can be appropriate for image encryption.																	1751-9659	1751-9667				JAN 10	2020	14	1					40	52		10.1049/iet-ipr.2019.0123													
J								Real-time least-squares ensemble visual tracking	IET IMAGE PROCESSING										regression analysis; least squares approximations; learning (artificial intelligence); image classification; object tracking; image sampling; strong classifier; Fourier domain; diverse weak classifiers; historical targets; training process; visual tracking; novel ensemble tracking system; tracking task; linear regression; least-squares problem; Moore-Penrose inverse	OBJECT TRACKING; ROBUST	In this study, the authors present a novel ensemble tracking system by formulating the tracking task in terms of a linear regression which is a least-squares problem. A set of weak classifiers are trained using least squares which are solved efficiently using the Moore-Penrose inverse. Then, these weak classifiers are combined into a strong classifier using bagging. The strong classifier is used to recognise the target and locate its position, which is obtained efficiently in the Fourier domain. For obtaining a good ensemble, a novel sampling strategy is proposed to train accurate and diverse weak classifiers. By exploiting historical targets to monitor the training process, pose change and occlusion are well-handled. The proposed method is extensively evaluated using a variety of evaluation protocols on the recent standard datasets including OTB50, OTB100 and VOT2016. Experimental results show that the proposed methodology performs favourably against state-of-the-art methods in terms of efficiency, accuracy and robustness.																	1751-9659	1751-9667				JAN 10	2020	14	1					53	61		10.1049/iet-ipr.2018.6037													
J								Automatic optical-to-SAR image registration using a structural descriptor	IET IMAGE PROCESSING										radar imaging; image registration; feature extraction; medical image processing; synthetic aperture radar; geometric differences; automatic optical-to-SAR image registration; structural descriptor; optical-to-synthetic aperture radar image registration; nonlinear intensity variations; optical images; optical-to-SAR image registration methods; novel optical-to-SAR image registration algorithm; optical SAR images	EDGE DETECTOR; ALGORITHM; SIFT	Optical-to-synthetic aperture radar (SAR) image registration is a challenging task in remote sensing as the images have significant non-linear intensity variations as well as large geometric differences. Moreover, the influence of speckle noise in the SAR image further affects the registration result. The structural descriptors are very effective to handle the non-linear intensity variations between optical and SAR images. Although a number of optical-to-SAR image registration methods have been proposed in the past few years based on the structural information of image, most of them are ineffective for the images having large geometric differences. To address these problems, a novel optical-to-SAR image registration algorithm is proposed by using a new structural descriptor. Initially, the corner features are extracted from the optical and SAR images. Then, the proposed structural descriptor is constructed for the extracted features. Finally, feature matching is performed between the optical and SAR images and the correct match are identified. The proposed method is very effective to register the optical and SAR images having significant intensity variations and large geometric differences. It can increase the number of correct match between the images. Experiments on six sets of optical and SAR image pairs demonstrate the effectiveness of the proposed method.																	1751-9659	1751-9667				JAN 10	2020	14	1					62	73		10.1049/iet-ipr.2019.0389													
J								Automatic rectification of warped Bangla document images	IET IMAGE PROCESSING										document image processing; cameras; text analysis; natural language processing; automatic rectification; robust algorithm; camera-captured document images; Bangla script; document surfaces; font type; font size; font style; camera view angle; text line; document image; headline position; distorted text; dewarping; shape improvement; warped Bangla document images	SEGMENTATION; RESTORATION	In this study, a robust algorithm for dewarping of camera-captured document images, mainly in Bangla script, is proposed. The algorithm can handle various types of warped document images and they are generated due to different types of document surfaces (convex, concave or multi-folded). The proposed algorithm is independent of font type, font size, font style and camera view angle. After initial preprocessing, the method first demarcates the text lines present in the document image. Then, the headline (shirorekha) position of each text line is estimated. Based on the headline position and shape, each text line is dewarped. If the document is highly warped, distorted text (e.g. thinner and shorter characters) is generated after dewarping. Special care has been taken to minimise this distortion based on most undistorted character information. Exhaustive testing shows the robustness and shape improvement of the proposed algorithm. Finally, for shape quality evaluation, some new measures are defined.																	1751-9659	1751-9667				JAN 10	2020	14	1					74	83		10.1049/iet-ipr.2019.0831													
J								Gradient-based kernel selection technique for tumour detection and extraction of medical images using graph cut	IET IMAGE PROCESSING										biomedical MRI; gradient methods; image segmentation; biological tissues; brain; tumours; medical image processing; GC-based; kernel selection technique; tumour detection; medical images; magnetic resonance imaging; powerful imaging technique; ubiquitous imaging technique; high-contrast images; soft tissues; low radio-frequency bias field; quantitative analyses; qualitative analyses; segmentation aids; bias effect; graph-cut segmentation; supervised computer-assisted diagnosis; GC's interactive nature; inaccurate extraction; fallacious extraction; shrinkage problem; locates tumour; human interaction; bias field images; intensity scales; high-grade glioma; low-grade glioma images; average performance metrics; tumour images	SEGMENTATION; CLASSIFICATION; METRICS; MODEL	Magnetic resonance imaging is a powerful, ubiquitous imaging technique that provides detailed high-contrast images differentiating soft tissues. The low radio-frequency bias field creates intensity inhomogeneity generating low contrast that often creates difficulty for quantitative and qualitative analyses. Segmentation aids in analysis of changes occurring in brain, where bias effect severely affects performance. The graph-cut (GC) segmentation provides supervised computer-assisted diagnosis and treatment. GC's interactive nature requires manual selection of kernels for initialisation. The shrinkage behaviour of GC creates inaccurate and fallacious extraction. On the basis of these problems, this study proposes gradient-based kernel selection GC method that simultaneously removes shrinkage problem and locates tumour in image, eliminating human interaction with accurate segmentation for even bias field images. The proposed method addresses these problems by emphasising on directive inclination of intensity scales of symmetrical halves of images. The proposed method is evaluated for high-grade glioma and low-grade glioma images with and without bias field. The average performance metrics evaluated for these images depict remarkable improvement in comparison with existing techniques. The proposed technique is validated by applying on real-time dataset of tumour images obtained from State Government Hospital, Shimla, India.																	1751-9659	1751-9667				JAN 10	2020	14	1					84	93		10.1049/iet-ipr.2018.6615													
J								Regularised IHS-based pan-sharpening approach using spectral consistency constraint and total variation	IET IMAGE PROCESSING										optimisation; geophysical image processing; image colour analysis; image resolution; image fusion; image reconstruction; remote sensing; artificial satellites; regularised IHS-based pan-sharpening approach; spectral consistency constraint; low-resolution multispectral image; corresponding high-resolution panchromatic image; high-resolution multispectral; pan sharpening; intensity-hue-saturation-based pan-sharpening methods; high-spatial quality; inevitable spectral distortion; IHS-based pan-sharpening approaches; fusion results; pan-sharpening problem; HRM image; total variation regularisation term; nonquadratic optimisation problem; variable splitting method; local spectral; patch-based strategy; different pan-sharpening methods	RESOLUTION IMAGES; FUSION; MULTIRESOLUTION; ENHANCEMENT; TRANSFORM; MS	In this study, the authors address the fusion of low-resolution multi-spectral image with the corresponding high-resolution panchromatic image to provide high-resolution multi-spectral (HRM) one, i.e. pan sharpening. The intensity-hue-saturation (IHS)-based pan-sharpening methods are popular because they are simple, efficient, and of high-spatial quality. However, their frameworks are unavoidably subject to spectral distortion. To reduce the inevitable spectral distortion of IHS-based pan-sharpening approaches, the spectral consistency constraint is used in the proposed method. Moreover, to stabilise fusion results obtained from the ill-posed pan-sharpening problem and to keep the smoothness of the HRM image, a total variation regularisation term is considered. These considerations are formulated in a non-quadratic optimisation problem. To solve this problem, a kind of variable splitting method, known as half-quadratic approximation is utilised, and also an alternating optimisation procedure is used to reconstruct HRM image. To gain convenient control on the local spectral and the spatial information, and also to reduce the required memory, in the optimisation stage, the patch-based strategy is employed. The proposed method was tested on two datasets acquired by GeoEye-1 and Pleiades satellites. To evaluate the proposed method, visual assessment, as well as quantitative comparison with different pan-sharpening methods, was carried out.																	1751-9659	1751-9667				JAN 10	2020	14	1					94	104		10.1049/iet-ipr.2019.0283													
J								XNORCONV: CNNs accelerator implemented on FPGA using a hybrid CNNs structure and an inter-layer pipeline method	IET IMAGE PROCESSING										field programmable gate arrays; logic gates; convolutional neural nets; neural chips; multiplying circuits; computational complexity; pipeline processing; logic design; XNORCONV; CNNs accelerator; FPGA; hybrid CNNs structure; convolutional neural networks; computer vision; pattern recognition; energy consumption; graphic processing units-based CNNs; field-programmable gate array; convolutional layer; XNOR operations; multiplier; computational complexity; inter-layer pipeline design; Xilinx Zynq-7000 xc7z020clg400-1; clock frequency; MNIST dataset		Nowadays, convolutional neural networks (CNNs) have become a research hotspot because of their high performance in computer vision and pattern recognition. However, as the high energy consumption of traditional graphic processing units-based CNNs, it is difficult to deploy them into portable devices. To deal with this problem, a hybrid CNN structure (XNORCONV) was proposed and implemented on field-programmable gate array (FPGA) in this study. Two improvements have been applied in XNORCONV. Firstly, the multiplications in the convolutional layer (CONV) were replaced by XNOR operations to save the multiplier and reduce computational complexity. Secondly, an inter-layer pipeline was designed to further accelerate the calculation. XNORCONV was implemented on Xilinx Zynq-7000 xc7z020clg400-1 under the clock frequency of 150 MHz and tested with MNIST dataset. The results of the experiment show that XNORCONV can classify each picture from MNIST in $ 18 .97\, {\rm \mu s}$18.97 mu s, and achieve 98.4% recognition accuracy. Compared with traditional Lenet-5 on different platforms, XNORCONV reduced multiplication by 85.6% with only 0.4% accuracy loss.																	1751-9659	1751-9667				JAN 10	2020	14	1					105	113		10.1049/iet-ipr.2019.0385													
J								Low-rank tensor completion for visual data recovery via the tensor train rank-1 decomposition	IET IMAGE PROCESSING										singular value decomposition; convex programming; approximation theory; tensors; data analysis; Tucker rank; tensor entries; proximal operator; tensor nuclear norms; tensor-train rank-1 decomposition; singular value decomposition; tensor singular values; low-rank constraint; low-rank tensor completion; visual data recovery; tensor train rank-1 decomposition; primal-dual splitting; soft-thresholding operation	IMAGE; FACTORIZATION; ALGORITHM	In this study, the authors study the problem of tensor completion, in particular for three-dimensional arrays such as visual data. Previous works have shown that the low-rank constraint can produce impressive performances for tensor completion. These works are often solved by means of Tucker rank. However, Tucker rank does not capture the intrinsic correlation of the tensor entries. Therefore, the authors propose a new proximal operator for the approximation of tensor nuclear norms based on tensor-train rank-1 decomposition via the singular value decomposition. The proximal operator will perform a soft-thresholding operation on tensor singular values. In addition, the low-rank constraint can capture the global structure of data well, but it does not exploit local smooth of visual data. Therefore, they integrate total variation as a regularisation term into low-rank tensor completion. Finally, they use a primal-dual splitting to achieve optimisation. Experimental results have shown that the proposed method, can preserve the multi-dimensional nature inherent in the data, and thus provide superior results over many state-of-the-art tensor completion techniques.																	1751-9659	1751-9667				JAN 10	2020	14	1					114	124		10.1049/iet-ipr.2018.6594													
J								Higher precision range estimation for context-based adaptive binary arithmetic coding	IET IMAGE PROCESSING										video coding; binary codes; arithmetic codes; finite state machines; rate distortion theory; optimisation; probability; adaptive codes; range estimation; Lagrangian rate distortion optimisation; modern video encoders; high-efficiency video coding; accurate context-based adaptive binary arithmetic coding look-up table; HEVC Test Model standard table; finite-state machine; context modelling; BD-RATE gain; probability states	COMPLEXITY ANALYSIS; HEVC	The Lagrangian rate distortion optimisation is widely employed in modern video encoders, such as high-efficiency video coding (H.265/HEVC). In this work, the authors propose a more accurate context-based adaptive binary arithmetic coding look-up table that can enhance compression quality and provide substantially better accuracy of range estimation, by employing one-more bit with 64 probability states. For the hardware implementation, they propose a higher precision look-up table instead of the HEVC Test Model (HM) standard table. The authors also define a new finite-state machine to handle the probability changing in real-time. The significant BD-RATE gain of the proposed context modelling is up to 6.0% for all-intra mode and 13.0% for inter mode. This finite state machine offers no divergence from the H.265/HEVC standards and can be used in the current systems.																	1751-9659	1751-9667				JAN 10	2020	14	1					125	131		10.1049/iet-ipr.2018.6602													
J								Automatic detection of acute lymphoblastic leukaemia based on extending the multifractal features	IET IMAGE PROCESSING										cancer; image colour analysis; blood; pattern classification; medical image processing; image denoising; feature extraction; principal component analysis; fractals; image segmentation; image enhancement; image classification; support vector machines; automatic detection; acute lymphoblastic leukaemia; multifractal features; diagnosis efficiency; microscopic images; watershed algorithms; chaotic features; nuclei images; fractal dimension; binary types; multiclass types; support vector machine algorithm; principal component analysis; feature space	CLASSIFICATION; SYSTEM	The main purpose of this study is to introduce a new species of features to improve the diagnosis efficiency of acute lymphoblastic leukaemia from microscopic images. First, the authors segmented nuclei by the k-means and watershed algorithms. They extracted three sets of geometrical, statistical, and chaotic features from nuclei images. Six chaotic features were extracted by calculating the fractal dimension from five sub-images driven from the nuclei images, with their grey levels being modified. The authors classified the images into binary and multiclass types via the support vector machine algorithm. They conducted principal component analysis for dimensional reduction of feature space and then evaluated the proposed algorithm for the overfitting problem. The obtained overall results represent 99% accuracy, 99% specificity, and 97% sensitivity values in the classification of six-cell groups. The difference between the train and test errors was <3%, which proves that the classification performance had improved by using the multifractal features.																	1751-9659	1751-9667				JAN 10	2020	14	1					132	137		10.1049/iet-ipr.2018.5910													
J								Image retrieval based on ASIFT features in a Hadoop clustered system	IET IMAGE PROCESSING										image matching; affine transforms; parallel processing; feature extraction; support vector machines; computational complexity; image retrieval; stereo image processing; ASIFT features; image matching; scale invariant feature transform algorithm; image rotation; scale zooming; 3D camera viewpoint; affine SIFT; higher computational complexity; Hadoop-based image retrieval system; Hadoop clustered system; MapReduce technology; bag-of-words method; support vector machine	OPTIMIZATION; CONTOURS	For image matching, the scale invariant feature transform (SIFT) algorithm is a commonly used one. They are invariant to image rotation, scale zooming, and partially invariant to change in illumination and 3D camera viewpoint. Affine SIFT (ASIFT) is an extension of SIFT, which solves the problem when images are captured at different angles. However, ASIFT has higher computational complexity than SIFT, due to a huge amount of features in the images. Therefore, in this study, a Hadoop-based image retrieval system is proposed to solve the ASIFT shortcomings of high computation by the MapReduce technology. The system uses a combination of the Bag-of-Words method and support vector machine. Finally, the experimental results verify that the proposed method is more effective than the other state-of-the-art methods for a variety of datasets.																	1751-9659	1751-9667				JAN 10	2020	14	1					138	146		10.1049/iet-ipr.2019.0229													
J								Integrated system for automatic detection of representative video frames in wireless capsule endoscopy using adaptive sliding window singular value decomposition	IET IMAGE PROCESSING										biomedical optical imaging; endoscopes; image segmentation; bioacoustics; singular value decomposition; medical image processing; patient diagnosis; video signal processing; feature extraction; integrated system; automatic detection; representative video frames; wireless capsule endoscopy; window singular value decomposition; noninvasive diagnosis method; capsule travels; gastrointestinal tract; practical drawback; long clinical video; review process; automated summarisation methods; evaluation time; adaptive contrast diffusion; WCE frames; novel knowledge-based method; segment video frames; GI tract; review time	EXTRACTION; REDUCTION; SPACE	Wireless capsule endoscopy (WCE) is a non-invasive diagnosis method that allows recording a video as the capsule travels through the gastrointestinal (GI) tract. The practical drawback is producing a long clinical video in which the review process by an experienced specialist is tedious. Automated summarisation methods can reduce the evaluation time by experts as well as errors in manual interpretation. The proposed approach consists of three main steps as follows: First, an adaptive sliding window singular value decomposition is employed to extract representative video frames. Then, adaptive contrast diffusion is utilised to increase the visibility of WCE frames. At the end stage, a novel knowledge-based method is developed to segment video frames into four topographic zones of GI tract, which are oesophagus, stomach, small intestine and large intestine. The authors have evaluated the proposed framework in the presence of 30 local datasets as well as publicly available KID database. The average recall and precision were estimated by 0.86 and 0.83, and by 0.82 and 0.83 for KID database, respectively. Their results reveal that significant reduction in the review time is feasible using the proposed technique. Quantitative results of summarisation show that the proposed method is more effective than three methods in the literature.																	1751-9659	1751-9667				JAN 10	2020	14	1					147	153		10.1049/iet-ipr.2019.0251													
J								Correction of complex purple fringing by green-channel compensation and local luminance adaptation	IET IMAGE PROCESSING										brightness; image colour analysis; image texture; cameras; aberrations; green-channel compensation algorithm; local luminance adaptation; natural photography; camera imaging pipeline; camera parameters; scene dependent variables; local object colour reflectivity profiles; purple fringing aberration; scene dependent noise; high contrast zones; semitransparent purple haze; C-PFA; PFA-correction solutions; IS-PFA; colour information; luminance profile; colour restoration; localised luminance adaptation procedure; isolated PFA; colour-channel correlation algorithm		In natural photography, defects in camera imaging pipeline often result in some form of colour noise or distortion. Nature of this distortion is generally intertwined with scene dependent variables such as positioning, intensity and composition of light source and local object colour reflectivity. One such defect is called purple fringing aberration (PFA). PFA problems are of two types, one of which corresponds to a localised fringing effect near high contrast zones (termed as Isolated PFA or IS-PFA) and the second which corresponds to a widespread semi-transparent purple haze over a large part of natural scene (termed as complex PFA or C-PFA). Much of the PFA-correction solutions have been driven towards IS-PFA and very little towards C-PFA. Based on a premise that in C-PFA, green channel is heavily suppressed and noisy, while colour information in red and blue channels are largely conserved, authors propose a green-channel compensation algorithm for restoring true natural colours in fringe affected region. To correct white-tuft produced by proposed compensation algorithm, they also devise a suitable localised luminance adaptation procedure to equalise perceived changes in luminance profile. Comparisons with state-of-the-art methods devised to combat this purple haze effect yield promising results for a majority of test cases.																	1751-9659	1751-9667				JAN 10	2020	14	1					154	167		10.1049/iet-ipr.2019.0732													
J								Novel image restoration method based on multi-frame super-resolution for atmospherically distorted images	IET IMAGE PROCESSING										image resolution; image restoration; image enhancement; image reconstruction; image fusion; multiple fusion scheme; image super-resolution methods; image restoration method; atmospherically distorted images; image-quality enhancement; fixed camera; spatial deformations; atmospheric turbulence; adaptive frame selection method; relatively clear edges; multiframe super-resolution method		In this study, the authors propose a novel multi-frame super-resolution method using frame selection and multiple fusions for atmospherically distorted, zoomed-in, image-quality enhancement. When a small part of the image captured by placing a target several kilometres away from the fixed camera is enlarged, the quality of the part becomes poor owing to low resolution, spatial deformations and noise that are mainly caused by long distance and atmospheric turbulence. Thus, the authors propose an adaptive frame selection method that selects only a few frames with small blur based on the corresponding images with relatively clear edges. Further, they propose multiple fusion schemes to reconstruct the selected frames, thereby suppressing the influence of deformation. By converting all the frames into high-resolution based on each frame and integrating them, deformation and noise are effectively removed without high computation cost using the multiple fusion scheme. The proposed method, which enhances the quality of atmospherically distorted zoomed-in images, exhibits superior performance than the state-of-the-art image super-resolution methods with regard to high accuracy, efficiency and ease of implementation, ensuring that the proposed method is suitable for enhancing the quality of an image captured using a general digital camera or a smartphone.																	1751-9659	1751-9667				JAN 10	2020	14	1					168	175		10.1049/iet-ipr.2019.0319													
J								Fusing HOG and convolutional neural network spatial-temporal features for video-based facial expression recognition	IET IMAGE PROCESSING										computer vision; face recognition; feature extraction; support vector machines; emotion recognition; convolutional neural nets; video signal processing; convolutional neural network spatial-temporal features; video-based facial expression recognition; VFER; fundamental feature; visual features; comprehensive feature; video frame; HOG features; facial expressions; CNN shallow features		Video-based facial expression recognition (VFER) is the fundamental feature of various computer vision applications. Visual features are the key factors for facial expression recognition. However, the gap between the visual features and the emotions is large. In order to bridge the gap, the proposed method utilises convolutional neural networks (CNNs) and histogram of oriented gradient (HOG) to obtain the more comprehensive feature for VFER. Firstly, it extracts shallow features from the video frame through a number of convolutional kernels in CNNs, which has the characteristics of displacement, scale and deformation invariance. Then, the HOG is employed to extract HOG features from CNN's shallow features, which are strongly correlated with facial expressions. Finally, the support vector machine (SVM) is employed to conduct the task of facial expression recognition. The extensive experiments on RML, CK+ and AFEW5.0 database show that this framework takes on the promising performance and outperforming the state of the arts.																	1751-9659	1751-9667				JAN 10	2020	14	1					176	182		10.1049/iet-ipr.2019.0293													
J								Level set based shape prior and deep learning for image segmentation	IET IMAGE PROCESSING										learning (artificial intelligence); affine transforms; image segmentation; convolutional neural nets; realistic image priors; training set; fully convolutional networks; FCN; image segmentation; deep prior method; high-level semantic patterns; high-level semantic information; intrinsic prior shape; specific image; improved level set method; corrected prior shape; segmented images; deep learning; convolutional neural network; portrait data set	EVOLUTION; ENERGY; MODEL	Deep convolutional neural network can effectively extract hidden patterns in images and learn realistic image priors from the training set. And fully convolutional networks (FCNs) have achieved state-of-the-art performance in the image segmentation. However, these methods have the disadvantages of noise, boundary roughness and no prior shape. Therefore, this study proposes a level set with the deep prior method for the image segmentation based on the priors learned by FCNs. The FCNs can learn high-level semantic patterns from the training set. Also, the output of the FCNs represents the high-level semantic information as a probability map and the global affine transformation can obtain the optimal affine transformation of the intrinsic prior shape. Moreover, the improved level set method integrates the information of the original image, the probability map and the corrected prior shape to achieve the image segmentation. Compared with the traditional level set method of simple scenes, the proposed method solves the disadvantage of FCNs by using the high-level semantic information to segment images of complex scenes. Finally, Portrait data set are used to verify the effectiveness of the proposed method. The experimental results show that the proposed method can obtain more accurate segmentation results than the traditional FCNs.																	1751-9659	1751-9667				JAN 10	2020	14	1					183	191		10.1049/iet-ipr.2018.6622													
J								DRU-net: a novel U-net for biomedical image segmentation	IET IMAGE PROCESSING										medical image processing; learning (artificial intelligence); image segmentation; image restoration; image resolution; image sampling; convolutional neural nets; image fusion; DRU-net; biomedical images; medical field; medical intervention; full convolutional neural networks; regular geometric structure; standard convolutions; geometric transformations; biomedical objects; deformable encoder; reshaping upsampling convolution decoder; deformable convolutional networks; biomedical image segmentation tasks; novel upsampling method; U-net; clinical diagnosis; pathological analysis; fusion features; Drosophila electron microscopy dataset; Warwick-QU dataset		With the wide applications of biomedical images in the medical field, the segmentation of biomedical images plays an important role in clinical diagnosis, pathological analysis, and medical intervention. Full convolutional neural networks, especially U-net, have improved the performance of segmentation greatly in recent years. However, due to their regular geometric structure, the standard convolutions that they use are inherently limited in dealing with geometric transformations while biomedical objects have huge variations in shape and size. In this study, the authors propose the DRU-net, which is a novel U-net with deformable encoder and reshaping upsampling convolution decoder, for biomedical image segmentation. First, deformable convolutional networks are applied and improved to enhance the learning ability of the encoder for geometric transformations. Second, a novel upsampling method named reshape upsampling convolution is proposed for better-restoring resolution and fusion features. Furthermore, focal loss is used to address class imbalance and model overwhelmed problems in biomedical image segmentation tasks. Theoretic analysis and experimental results have shown that the proposed algorithm not only reduces the number of parameters of U-Net, but also achieves produces competitive results compared with the state-of-the-art algorithms in terms of various quantitative measures on Drosophila electron microscopy dataset and Warwick-QU dataset.																	1751-9659	1751-9667				JAN 10	2020	14	1					192	200		10.1049/iet-ipr.2019.0025													
J								Optimisation of linear dependence energy for object co-segmentation in a set of images with heterogeneous contents	IET IMAGE PROCESSING										Markov processes; image segmentation; object detection; iterative methods; optimisation; categorised images; foreground objects; joint segmentation; category-independent objects; foreground histograms; input images; Markov random field-based energy function; iterative optimisation; full-object segmentation; uncategorised objects; linear dependence energy; object co-segmentation; heterogeneous contents; multiple images; segmented similar objects		This work proposes a framework for simultaneously segmenting foreground objects in a collection of images having heterogeneous contents. Rather than resorting to image co-segmentation to segment similar objects in multiple images, which requires the use of categorised images, the authors' idea disseminates segmentation information within images. In this way, it becomes easier to detect foreground objects in all of them simultaneously, mainly under the hypothesis of using similar or different images. General information is aggregated, on foregrounds as well as on backgrounds, from a set of images for joint segmentation of category-independent objects. The key idea is to estimate the linear dependence of the foreground histograms of the input images to optimise a Markov random field-based energy function. Iterative optimisation of each image permits after that the enhancement of the final segmentation results. Extensive experiments demonstrate that the proposed method (PM) enables full-object segmentation of foreground objects within a collection of images composed of different classes. Indeed, the validation of the accuracy on five challenging datasets (iCoseg, Oxford Flowers, MicroSoft Research Cambridge (MSRC), Caltech101 and Berkeley) shows that the PM achieves satisfactory results as compared with state-of-the-art methods. Besides, it has the challenging ability to efficiently deal with uncategorised objects.																	1751-9659	1751-9667				JAN 10	2020	14	1					201	210		10.1049/iet-ipr.2018.5176													
J								An evolutionary lion optimization algorithm-based image compression technique for biomedical applications	EXPERT SYSTEMS										evolutionary algorithms; Lion optimization algorithm; medical data; LZMA; vector quantization; biomedical applications	VECTOR QUANTIZATION; SEARCH	Recently, medical image compression becomes essential to effectively handle large amounts of medical data for storage and communication purposes. Vector quantization (VQ) is a popular image compression technique, and the commonly used VQ model is Linde-Buzo-Gray (LBG) that constructs a local optimal codebook to compress images. The codebook construction was considered as an optimization problem, and a bioinspired algorithm was employed to solve it. This article proposed a VQ codebook construction approach called the L2-LBG method utilizing the Lion optimization algorithm (LOA) and Lempel Ziv Markov chain Algorithm (LZMA). Once LOA constructed the codebook, LZMA was applied to compress the index table and further increase the compression performance of the LOA. A set of experimentation has been carried out using the benchmark medical images, and a comparative analysis was conducted with Cuckoo Search-based LBG (CS-LBG), Firefly-based LBG (FF-LBG) and JPEG2000. The compression efficiency of the presented model was validated in terms of compression ratio (CR), compression factor (CF), bit rate, and peak signal to noise ratio (PSNR). The proposed L2-LBG method obtained a higher CR of 0.3425375 and PSNR value of 52.62459 compared to CS-LBG, FA-LBG, and JPEG2000 methods. The experimental values revealed that the L2-LBG process yielded effective compression performance with a better-quality reconstructed image.																	0266-4720	1468-0394														e12508	10.1111/exsy.12508		JAN 2020											
J								A flexible procedure for mixture proportion estimation in positive-unlabeled learning	STATISTICAL ANALYSIS AND DATA MINING										classification; empirical processes; local false discovery rate; mixture proportion estimation; multiple testing; PU learning	FALSE DISCOVERY RATE; TRUE NULL HYPOTHESES	Positive-unlabeled (PU) learning considers two samples, a positive set P with observations from only one class and an unlabeled set U with observations from two classes. The goal is to classify observations in U. Class mixture proportion estimation (MPE) in U is a key step in PU learning. Blanchard et al. showed that MPE in PU learning is a generalization of the problem of estimating the proportion of true null hypotheses in multiple testing problems. Motivated by this idea, we propose reducing the problem to one-dimension via construction of a probabilistic classifier trained on the P and U data sets followed by application of a one-dimensional mixture proportion method from the multiple testing literature to the observation class probabilities. The flexibility of this framework lies in the freedom to choose the classifier and the one-dimensional MPE method. We prove consistency of two mixture proportion estimators using bounds from empirical process theory, develop tuning parameter free implementations, and demonstrate that they have competitive performance on simulated waveform data and a protein signaling problem.																	1932-1864	1932-1872				APR	2020	13	2					178	187		10.1002/sam.11447		JAN 2020											
J								The application of artificial neural network in watch modeling design with network community media	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network community media; Neural network; Watch modeling design	SOCIAL MEDIA; PREDICTION; ALGORITHM; AREA	In order to study the design direction of watch modelling in the future, the needs of consumer groups were collected by using network social media. Enterprises and designers carried out preliminary design according to the needs description of consumers. Then, these design data were summarized and analyzed by computer neural network, and then revised according to the feedback of consumers, so as to get the poduct modeling as close as possible to the needs of consumers. The results show that, as an economic model that can mobilize the social activity and communication power of consumers, the network community media can obtain more accurate consumer demand. In the process of integration analysis, the data are vague, so the integration is difficult, but it can provide more creative materials and ideas for designers. The watch eventually designed can meet the needs of consumers to the greatest extent. It can be seen that the use of network social media and computer neural network has a certain significance for watch modeling design and appearance design of other products.																	1868-5137	1868-5145															10.1007/s12652-020-01689-6		JAN 2020											
J								Fast key-frame image retrieval of intelligent city security video based on deep feature coding in high concurrent network environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent city; Security video; Deep feature; Deep learning; Key-frame selection; Retraining model	EXTRACTION; QUANTIZATION	Aiming at the problems faced by the connection management module in single packet processing in intelligent city security video retrieval, we firstly propose the traffic locality quantization index based on the traffic characteristics of the backbone link to quantitatively analyze the traffic locality characteristics in the backbone link. And then, a key-frame abstraction and retrieval of videos based on deep learning is proposed to improve the efficiency and accuracy of video retrieval, where an adaptive key-frame selection algorithm is designed and the existing convolutional neural network framework is used to extract the features of key-frames, and unsupervised, semi-supervised and supervised retraining models are designed to improve the effectiveness of the feature extraction of the convolutional neural network and the accuracy of the video retrieval. Experimental results based on the public video datasets show that our proposed key-frame image retrieval model realizes a good precision for key-frame representation, and achieves high accuracy and efficiency for video retrieval.																	1868-5137	1868-5145															10.1007/s12652-020-01679-8		JAN 2020											
J								A secure image encryption scheme based on a novel 2D sine-cosine cross-chaotic (SC3) map	JOURNAL OF REAL-TIME IMAGE PROCESSING										Confusion and diffusion; Sine-cosine cross-chaotic map; Lyapunov exponent; Differential attack	CELLULAR-AUTOMATA; PERMUTATION; ALGORITHM; DIFFUSION; CIPHER; SYSTEM; BOX	In this paper, we propose a new 2D sine-cosine cross-chaotic (SC3) map to design an image encryption scheme with high confusion and diffusion capability. We evaluate the maximum Lyapunov exponent (MLE) of the proposed SC3 map to measure its degree of sensitivity to initial conditions and perform bifurcation analysis to find the chaotic region. The proposed chaotic map generates two pseudo-random sequence R-1 and R-2, which are used in confusion (permutation) and diffusion phase, respectively. The confusion layer is designed by shuffling the image pixels, and the diffusion layer is designed by bitwise XOR operation. The strength of the proposed image encryption scheme is evaluated against resistance to the statistical attack (information entropy, correlation coefficient, and histogram analysis), differential attack (NPCR and UACI), and sensitivity to the secret key. The experimental results of both security and performance analysis show that the proposed image encryption scheme is secure enough to resist all the existing cryptanalytic attack and efficient in terms of encryption time.																	1861-8200	1861-8219															10.1007/s11554-019-00940-4		JAN 2020											
J								The hybrid ant colony optimization and ensemble method for solving the data stream e-mail foldering problem	NEURAL COMPUTING & APPLICATIONS										Ant colony optimization; Data stream; Ensemble methods; Decision trees; Enron e-mail; ACDF	DECISION TREE; ALGORITHMS; RULES	The e-mail foldering problem is a special classification problem. It concerns a situation where e-mail users create new folders and, at the same time, stop using some of the folders created in the past. Additionally, messages arrive in the system at different time stamps. This article proposes a novel approach to ant colony optimization adapted to data stream analysis. The article is related to the revision of the ant colony optimization algorithm in the e-mail foldering problem and the proposition of a new solution adapted to the data stream. The goal of this work is to allow the classification of messages arriving at the system as data packages; however, due to the large number of decision classes (folders in the inbox), successive packages lead to a large concept drift. To assure the stability of the algorithm, an approach based on the memory being represented as a pheromone trail is introduced. This concept is known from the ant colony optimization methods. At the same time, multiple numbers of classifiers (similar to an ensemble method) are included. The proposed approach was tested on real-world data from the Enron e-mail dataset. An analysis of the two proposed methods related to the data stream was proposed. Both methods were compared with the methods used in the literature. The results achieved, in terms of the accuracy as well as the stability, confirm that (according to a statistical analysis) the proposed solutions are capable of better classifying e-mail messages derived from the system as data packages.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15429	15443		10.1007/s00521-019-04672-1		JAN 2020											
J								A Wiener Causality Defined by Divergence	NEURAL PROCESSING LETTERS										Granger causality; Time series; Relative entropy causality; Transfer entropy; Bregman divergence	ECONOMETRIC-MODELS; MAXIMUM-LIKELIHOOD; INFORMATION; INFERENCE	Discovering causal relationships is a fundamental task in investigating the dynamics of complex systems (Pearl in Stat Surv 3:96-146, 2009). Traditional approaches like Granger causality or transfer entropy fail to capture all the interdependence of the statistic moments, which might lead to wrong causal conclusions. In the previous papers (Chen et al. in 25th international conference, ICONIP 2018, Siem Reap, Cambodia, proceedings, Part II, 2018), the authors proposed a novel definition of Wiener causality for measuring the intervene between time series based on relative entropy, providing an integrated description of statistic causal intervene. In this work, we show that relative entropy is a special case of an existing more general divergence estimation. We argue that any Bregman divergences can be used for detecting the causal relations and in theory remedies the information dropout problem. We discuss the benefits of various choices of divergence functions on causal inferring and the quality of the obtained causal models. As a byproduct, we also obtain the robustness analysis and elucidate that RE causality achieves a faster convergence rate among BD causalities. To substantiate our claims, we provide experimental evidence on how BD causalities improve detection accuracy.																	1370-4621	1573-773X															10.1007/s11063-019-10187-6		JAN 2020											
J								Twitter sentiment analysis using hybrid Spider Monkey optimization method	EVOLUTIONARY INTELLIGENCE										Sentiment-Analysis (SA); Twitter; Nature-Inspired-Algorithm; Machine learning techniques; Optimization; K-means clustering; Spider-Monkey optimizer		The use of social media, over the past few years, has escalated enormously. Social media has formed a platform for the availability of abundant data. Thousands of people express their perceptions through social media. Sentiment Analysis (SA) of such views and perceptions is very substantial to measure public notion on a peculiar/specific subject matter of concern. SA is a remarkable field of data mining concerned with identification and translation of sentiments accessible on social media. Twitter is a microblogging site in which users can post updates (tweets) to friends (followers). This paper proposes a mechanism for extracting the sentiments from the tweets posted on Twitter. Tweets can be classified as positive, neutral or negative. The metaheuristic-based clustering techniques are superior to conventional techniques due to the subjective behaviour of tweets. A hybrid strategy, named as Hybrid Spider Monkey optimization with k-means clustering, is introduced to obtain the optimal cluster-heads of the dataset. The accuracy of the proposed method is determined on two datasets, namely, sender2 and twitter. To analyse the authenticity of the proposed method, a comparative analysis is performed with a few significant Nature-Inspired Algorithms such as Spider-Monkey optimization, Particle-Swarm algorithm, Genetic-Algorithm and Differential Evolution.																	1864-5909	1864-5917															10.1007/s12065-019-00334-2		JAN 2020											
J								Process capability indices when two sources of variability present, a tolerance interval approach	JOURNAL OF CHEMOMETRICS										process capability indices; process performance; tolerance interval; balanced one-way ANOVA; proportion of non-conforming parts	LIMITS; MODELS	The sound tolerance interval-based method and two P-p-based approximations are compared on the proportion of nonconforming parts. As output distribution of the process, one possible model is examined here: two sources of variations are in a one-way structure. It was found that the uncertainty of variance components estimates plays the major role in the goodness of the three calculation methods. Statistical indices - like process capability (C-P) or process performance (P-P) index - make the relationship between the width of the specification interval and the extent of the process variability illustrative. The latter is characterized by the tolerance interval, which contains the major part of the population with high confidence. In the original concept this tolerance interval is calculated using oversimplified models. In practice this model is not conform with reality. As improvement two sources of variation is assumed in a one-way structure. The proportion of non-conforming parts of the population is the quantity of interest. Non-conforming means that the characteristic is beyond the specification limits. According to this, the quantile of the distribution shall be determined that is equal to the specification limits. Thus, the task is to calculate the tolerance interval for the N<mml:mfenced close=)open=(separators=mu sigma A2+sigma e2 distribution. In practical cases the variance components are unknown and are to be estimated. To estimate the ratio of non-conforming parts, two approximate calculation methods which are coherent with the definition of P-P are investigated, as well. The aim of this work is to compare the results of the two P-P based approximate methods with the tolerance interval based (theoretically sound) calculation method. Two situations from the practice are considered - datasets from process validation and process monitoring environment are evaluated during which the effect of the number of experiments is given. Our study indicated that the main difference of the goodness of the approximations is from the uncertainty of parameter estimates.																	0886-9383	1099-128X														e3213	10.1002/cem.3213		JAN 2020											
J								Selection of optimized features for fusion of palm print and finger knuckle-based person authentication	EXPERT SYSTEMS										2D(2)LDA; back tracking search; line ordinal pattern; quaternion wavelet; random forest	PALMPRINT VERIFICATION; ORIENTATION; INFORMATION; ROBUST; TREE	The impact of digital technology in biometrics is much more efficient at interpreting data than humans, which results in completely replacement of manual identification procedures in forensic science. Because the single modality-based biometric frameworks limit performance in terms of accuracy and anti-spoofing capabilities due to the presence of low quality data, therefore, information fusion of more than one biometric characteristic in pursuit of high recognition results can be beneficial. In this article, we present a multimodal biometric system based on information fusion of palm print and finger knuckle traits, which are least associated to any criminal investigation as evidence yet. The proposed multimodal biometric system might be useful to identify the suspects in case of physical beating or kidnapping and establish supportive scientific evidences, when no fingerprint or face information is present in photographs. The first step in our work is data preprocessing, in which region of interest of palm and finger knuckle images have been extracted. To minimize nonuniform illumination effects, we first normalize the detected circular palm or finger knuckle and then apply line ordinal pattern (LOP)-based encoding scheme for texture enrichment. The nondecimated quaternion wavelet provides denser feature representation at multiple scales and orientations when extracted over proposed LOP encoding and increases the discrimination power of line and ridge features. To best of our knowledge, this first attempt is a combination of backtracking search algorithm and 2D(2)LDA has been employed to select the dominant palm and knuckle features for classification. The classifiers output for two modalities are combined at unsupervised rank level fusion rule through Borda count method, which shows an increase in performance in terms of recognition and verification, that is, 100% (correct recognition rate), 0.26% (equal error rate), 3.52 (discriminative index), and 1,262 m (speed).																	0266-4720	1468-0394														e12523	10.1111/exsy.12523		JAN 2020											
J								Adaptive PID control of multi-DOF industrial robot based on neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multimodal nonlinear control system; Fuzzy neural network; PID feedforward control algorithms; Parallel industrial robot; Joint simulation technology	DESIGN	The control system of parallel robot, especially industrial robot, is a very complex multi-modal nonlinear system, which has the characteristics of time-varying, strong coupling and strong nonlinearity. Trajectory tracking control algorithm is a very important part of industrial robot control system. It is required that the algorithm can realize the continuous tracking of each joint of the robot and the processing and tracking of the desired trajectory. However, due to the strong influence of acceleration and speed on the trajectory tracking of industrial robots, the corresponding control difficulty and control accuracy are seriously affected. Based on the core idea of fuzzy neural network algorithm, the functional relationship between control error and arrival degree is established to improve the control quality of industrial robots. At the same time, combining with the PID feedforward control algorithm, the self-adaptive adjustment of PID parameters is realized, and the accuracy of the tracking algorithm is improved. In order to verify the superiority of the proposed trajectory tracking control algorithm over the traditional PID algorithm, the model of industrial robot is established by using the virtual simulation system Adams. At the same time, the model is simulated by joint experiment. Experimental results show that the trajectory control algorithm based on the proposed trajectory control algorithm is effective. This method has good control accuracy and stability.																	1868-5137	1868-5145															10.1007/s12652-020-01693-w		JAN 2020											
J								Automated conversion from natural language query to SPARQL query	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										SPARQL generation; Domain ontology; Natural language query; Korean language		Nowadays, domain ontologies are widely used as background knowledge bases. However, end users of ontology-based question answering (QA) systems are unaware of major concepts of ontology or the structure of domain ontology schema. Thus, it has been essential to provide an efficient method to reduce this gap. Namely, the critical issue for ontology-based QA systems is how to generate a SPARQL query from a user's natural language query (NLQ). Therefore, we proposed a method to generate SPARQL queries from Korean natural language queries. When an input query comes in, we split it into a set of tokens and map each token to certain resources in the ontology. Subsequently, a graph generation process creates multiple "query graphs" by arranging the resources and identifying relationships between them. To identify relations between resources, we applied a path search algorithm based on the structure of domain ontology schema. We score query graphs by measuring the degree to which the graph reflected the general user's intent, and the best-rated query graph is converted into a SPARQL query. We implemented a prototype system to evaluate the proposed method for the music domain ontology and conclude that our query conversion process can convert Korean natural language queries into semantically equivalent SPARQL queries. We anticipate that, after appropriate modification, the process can be applied to other languages.																	0925-9902	1573-7675				DEC	2020	55	3					501	520		10.1007/s10844-019-00589-2		JAN 2020											
J								Lung Cancer Prediction Using Stochastic Diffusion Search (SDS) Based Feature Selection and Machine Learning Methods	NEURAL PROCESSING LETTERS										Lung cancer; Small cell lung cancer (SCLC); Non-small cell lung cancer (NSCLC); Radiomic features; Gray level co-occurrence matrix (GLCM); Gabor filter; Stochastic diffusion search (SDS); Neural network (NN); Naive Bayes and decision tree	NODULES	The symptoms of cancer normally appear only in the advanced stages, so it is very hard to detect resulting in a high mortality rate among the other types of cancers. Thus, there is a need for early prediction of lung cancer for the purpose of diagnosing and this can result in better chances of it being able to be treated successfully. Histopathology images of lung scan can be used for classification of lung cancer using image processing methods. The features from lung images are extracted and employed in the system for prediction. Grey level co-occurrence matrix along with the methods of Gabor filter feature extraction are employed in this investigation. Another important step in enhancing the classification is feature selection that tends to provide significant features that helps differentiating between various classes in an accurate and efficient manner. Thus, optimal feature subsets can significantly improve the performance of the classifiers. In this work, a novel algorithm of feature selection that is wrapper-based is proposed by employing the modified stochastic diffusion search (SDS) algorithm. The SDS, will benefit from the direct communication of agents in order to identify optimal feature subsets. The neural network, Naive Bayes and the decision tree have been used for classification. The results of the experiment prove that the proposed method is capable of achieving better levels of performance compared to existing methods like minimum redundancy maximum relevance, and correlation-based feature selection.																	1370-4621	1573-773X															10.1007/s11063-020-10192-0		JAN 2020											
J								On ensemble techniques of weight-constrained neural networks	EVOLVING SYSTEMS										Weight-constrained neural networks; Ensemble learning; Bagging; Boosting-AdaBoost	CLASSIFICATION; DROPOUT	Ensemble learning constitutes one of the most fundamental and reliable strategies for building powerful and accurate predictive models, aiming to exploit the predictions of a number of multiple learners. In this paper, we propose two ensemble prediction models which exploit the classification performance of weight-constrained neural networks (WCNNs). The proposed models are based on Bagging and Boosting, which constitute two of the most popular strategies, to efficiently combine the predictions of WCNN classifiers. We conducted a series of experiments using a variety of benchmarks from UCI repository in order to evaluate the performance of the two proposed models against other state-of-the-art ensemble classifiers. The reported experimental results illustrate the prediction accuracy of the proposed models providing empirical evidence that the hybridization of ensemble learning and WCNNs can build efficient and powerful classification models.																	1868-6478	1868-6486															10.1007/s12530-019-09324-2		JAN 2020											
J								Indoor Simulated Training Environment for Brain-Controlled Wheelchair Based on Steady-State Visual Evoked Potentials	FRONTIERS IN NEUROROBOTICS										simulated environment; brain-controlled wheelchair; indoor training; steady-state visual evoked potentials; path recommendation	COMPUTER-INTERFACE; NEUROPHYSIOLOGICAL PROTOCOL; VIRTUAL ENVIRONMENTS; ACTUATED WHEELCHAIR; BCI; REHABILITATION; MOTIVATION; DESIGN; STROKE	Brain-controlled wheelchair (BCW) has the potential to improve the quality of life for people with motor disabilities. A lot of training is necessary for users to learn and improve BCW control ability and the performances of BCW control are crucial for patients in daily use. In consideration of safety and efficiency, an indoor simulated training environment is built up in this paper to improve the performance of BCW control. The indoor simulated environment mainly realizes BCW implementation, simulated training scenario setup, path planning and recommendation, simulated operation, and scoring. And the BCW is based on steady-state visual evoked potentials (SSVEP) and the filter bank canonical correlation analysis (FBCCA) is used to analyze the electroencephalography (EEG). Five tasks include individual accuracy, simple linear path, obstacles avoidance, comprehensive steering scenarios, and evaluation task are designed, 10 healthy subjects were recruited and carried out the 7-days training experiment to assess the performance of the training environment. Scoring and command-consuming are conducted to evaluate the improvement before and after training. The results indicate that the average accuracy is 93.55% and improves from 91.05% in the first stage to 96.05% in the second stage (p = 0.001). Meanwhile, the average score increases from 79.88 in the first session to 96.66 in the last session and tend to be stable (p < 0.001). The average number of commands and collisions to complete the tasks decreases significantly with or without the approximate shortest path (p < 0.001). These results show that the performance of subjects in BCW control achieves improvement and verify the feasibility and effectiveness of the proposed simulated training environment.																	1662-5218					JAN 8	2020	13								101	10.3389/fnbot.2019.00101													
J								On the utility of dreaming: A general model for how learning in artificial agents can benefit from data hallucination	ADAPTIVE BEHAVIOR										Artificial dream mechanisms; data simulation; machine learning; reinforcement learning	SIMULATION THEORY	We consider the benefits of dream mechanisms - that is, the ability to simulate new experiences based on past ones - in a machine learning context. Specifically, we are interested in learning for artificial agents that act in the world, and operationalize "dreaming" as a mechanism by which such an agent can use its own model of the learning environment to generate new hypotheses and training data. We first show that it is not necessarily a given that such a data-hallucination process is useful, since it can easily lead to a training set dominated by spurious imagined data until an ill-defined convergence point is reached. We then analyse a notably successful implementation of a machine learning-based dreaming mechanism by Ha and Schmidhuber (Ha, D., & Schmidhuber, J. (2018). World models. arXiv e-prints, arXiv:1803.10122). On that basis, we then develop a general framework by which an agent can generate simulated data to learn from in a manner that is beneficial to the agent. This, we argue, then forms a general method for an operationalized dream-like mechanism. We finish by demonstrating the general conditions under which such mechanisms can be useful in machine learning, wherein the implicit simulator inference and extrapolation involved in dreaming act without reinforcing inference error even when inference is incomplete.																	1059-7123	1741-2633														UNSP 1059712319896489	10.1177/1059712319896489		JAN 2020											
J								Large-scale dynamic process monitoring based on performance-driven distributed canonical variate analysis	JOURNAL OF CHEMOMETRICS										canonical variate analysis; distributed monitoring; genetic algorithm; large-scale dynamic process; performance-driven	PLS; DIAGNOSIS; SELECTION; SYSTEMS; DESIGN	As a typical process monitoring method for the large-scale industrial process, the distributed principal components analysis (DPCA) needs to be improved because of its rough selection for the variables in each subblock. Moreover, for DPCA, the process dynamic property is ignored and invalid fault diagnosis may occur. Therefore, a performance-driven distributed canonical variate analysis (DCVA) is proposed. Firstly, with historical fault information, the genetic algorithm is utilized to select appropriate variables for each subblock; secondly, canonical variate analysis is introduced to capture the dynamic information for performance improvement; finally, a novel fault diagnosis method is developed for the DCVA model. Case studies on a numerical example and the Tennessee Eastman benchmark process demonstrate the effectiveness of the proposed model. Highlights A novel fault diagnosis approach based on the distributed CVA model is presented. The dynamic property of process data for each sub-block is firstly captured by CVA. With the genetic algorithm, the historical fault information is utilized to select appropriate variables in each sub-block. The superiority of the developed method is validated on the numerical example and the TE process.																	0886-9383	1099-128X				MAR	2020	34	3			SI				e3192	10.1002/cem.3192		JAN 2020											
J								On accurate localization of sensor nodes in underwater sensor networks: a Doppler shift and modified genetic algorithm based localization technique	EVOLUTIONARY INTELLIGENCE										UWSNs; Localization; Doppler shift; Modified genetic algorithm	SYNCHRONIZATION; GPS	The problem of localization in under water sensor nodes has led to proposal of many techniques over the past few decades that depend primarily on Time of Arrival and Time Difference of Arrival. While these techniques are intuitively very appealing and easy to deploy, accurate node localization in dynamic under water environment has remained elusive. Sensor nodes deployed underwater tend to move from their original positions due to water currents and hence their exact positions at a given moment of time are not known with precision. Due to inherent drawbacks of radio signal propagation in underwater environment, localization of sensor nodes depends on acoustic signals. In this paper, we propose a Doppler shift based localization followed by a genetic algorithm based optimization technique that improves accuracy in localizing unknown nodes in underwater sensor networks. The proposed technique envisages sink nodes playing a pivotal role in taking over a bulk of the computational load on account of being comparatively more accessible and serviceable as compared to any other nodes in the network that are deployed underwater. The algorithm relies on observed frequency shifts (Doppler shift) of sound waves compared to actual, that happen when source and observer are mobile as they do in a marine environment. While Doppler shift determines the approximate location of an unknown sensor node, genetic algorithm minimizes the error in localization. Our proposed methodology has much lower localization error as compared to existing protocols.																	1864-5909	1864-5917															10.1007/s12065-019-00343-1		JAN 2020											
J								Blood vessel extraction in fundus images using hessian eigenvalues and adaptive thresholding	EVOLUTIONARY INTELLIGENCE										Image enhancement; Filters; Unsupervised classification; Retinal fundus images; Blood vessel extraction	SEGMENTATION; ENHANCEMENT	Retinal image blood vessels are having significant role in different eye related diseases such as diabetic retinopathy, glaucoma, cataract, age-related macular degeneration and many more. Vasculature retinal feature extraction is an important factor to different doctors for treatment and diagnosis of different diseases. For automatic extraction of retinal blood vessels, different types of clustering related approaches (i.e. k-means/fuzzy c-means) are introduced to explore blood vessels from real time retinal images. Novel blood vessel extraction approach is introduced to explore retinal blood vessels with unsupervised clustering procedures like fuzzy c-means followed with Gabor filter. In this paper, we propose Enhanced blood vessel exploration approach (EBVEA) to improve the segmentation and visualization of vasculature retinal images or fundus images in blood vessel extraction with a combination of hessian based center-to boundary (BB) filters. These filters are used to indicate elongated boundary structures by enhancing the functions based on hessian Eigen values represented in (nxn) matrix. Performance of proposed enhanced approach with traditional approaches in terms of true positive rate (tpr), accuracy etc. are tested. Experimental results carried out and tested on bench mark data sets like DRIVE and STARE datasets produced better results.																	1864-5909	1864-5917															10.1007/s12065-019-00329-z		JAN 2020											
J								Aerial-DEM geolocalization for GPS-denied UAS navigation	MACHINE VISION AND APPLICATIONS										GPS-denied UAS navigation; Aerial-DEM matching; Cross-domain representation; Triplet-ranking network	IMAGES	Accelerated by the proliferation of small, affordable, and lightweight electronically scanning radar systems as well as advances in Unmanned Aircraft System (UAS) technology, Geo-Registered Radar Returns data are becoming an incredible source for geolocalization in GPS-denied UAS navigation. Most existing approaches match aerial images to pre-stored Digital Elevation Models (DEMs) through 3D terrain reconstruction or GPU-based terrain rendering techniques. However, these reconstruction or rendering processes are themselves error-prone and time-consuming, which further decrease UAS navigation accuracy. In this work, we propose a novel geolocalization approach by directly matching aerial images to DEMs. Inspired by success of deep learning in face recognition/verification, we develop a triplet-ranking network to embed aerial images and DEMs into the same low-dimensional feature space, where matching Aerial-DEM are near one another and mismatched Aerial-DEM are far apart. To create large-scale training dataset, we design an efficient terrain generation approach using per-pixel displacement mapping technique. This approach augments aerial datasets by simulating visual appearances of terrain under different lighting conditions. Experiments are conducted to show the effectiveness of our deep network in finding matches between aerial images and DEMs.																	0932-8092	1432-1769				JAN 7	2020	31	1							3	10.1007/s00138-019-01052-6													
J								Hand gesture recognition using multimodal data fusion and multiscale parallel convolutional neural network for human-robot interaction	EXPERT SYSTEMS										hand gesture recognition; multimodal data fusion; parallel CNN; sEMG signal		Hand gesture recognition plays an important role in human-robot interaction. The accuracy and reliability of hand gesture recognition are the keys to gesture-based human-robot interaction tasks. To solve this problem, a method based on multimodal data fusion and multiscale parallel convolutional neural network (CNN) is proposed in this paper to improve the accuracy and reliability of hand gesture recognition. First of all, data fusion is conducted on the sEMG signal, the RGB image, and the depth image of hand gestures. Then, the fused images are generated to two different scale images by downsampling, which are respectively input into two subnetworks of the parallel CNN to obtain two hand gesture recognition results. After that, hand gesture recognition results of the parallel CNN are combined to obtain the final hand gesture recognition result. Finally, experiments are carried out on a self-made database containing 10 common hand gestures, which verify the effectiveness and superiority of the proposed method for hand gesture recognition. In addition, the proposed method is applied to a seven-degree-of-freedom bionic manipulator to achieve robotic manipulation with hand gestures.																	0266-4720	1468-0394														e12490	10.1111/exsy.12490		JAN 2020											
J								Routing using reinforcement learning in vehicular ad hoc networks	COMPUTATIONAL INTELLIGENCE										DRL; packet transmission; RSU; VANETs; vehicle movement	HEALTH-CARE; V2V; COMMUNICATION; ARCHITECTURES; PREDICTION; PROTOCOLS; IOT	In vehicular ad hoc networks (VANETs), the frequent change in vehicle mobility creates dynamic changes in communication link and topology of the network. Hence, the key challenge is to address and resolve longer transmission delays and reduced transmission stability. During the establishment of routing path, the focus of entire research is on traffic detection and road selection with high traffic density for increased packet transmission. This reduces the transmission delays and avoids carry-and-forward scenarios; however, these techniques fail in obtaining accurate traffic density in real-time scenario due to rapid change in traffic density. Thus, it is necessary to create a model that efficiently monitors the traffic density and assist VANETs in route selection in an automated way with increased accuracy. In this article, a novel machine learning architecture using deep reinforcement learning (DRL) model is proposed to monitor and estimate the data essential for the routing protocol. In this model, the roadside unit maintains the traffic information on roads using DRL. The DRL predicts the movement of the vehicle and makes a suitable routing path for transmitting the packets with improved transmission capacity. It further uses predicted transmission delays and the destination location to choose the forwarding directions between two road safety units (RSUs). The application of DRL over VANETs yields increased network performance, which provides on-demand routing information. The simulation results show that the DRL-based routing is effective in routing the data packets between the source and destination vehicles than other existing method.																	0824-7935	1467-8640				MAY	2020	36	2					682	697		10.1111/coin.12261		JAN 2020											
J								Digital Confucius? Exploring the implications of artificial intelligence in spiritual education	CONNECTION SCIENCE										Artificial intelligence; Confucianism; spiritual education; teaching	ROBOT; COMPETENCE; RELIGION	This paper explores the prospect of artificial intelligence (AI) supplementing or supplanting the human teacher in spiritual education using the example of Confucianism. The article compares the role and capabilities of two Confucian teachers: Confucius and an imagined robot teacher ("Digital Confucius"). It is argued in this article that Digital Confucius, if developed, would be useful as an "AI partner" in transmitting the knowledge and skills. As "AI mind", however, digital Confucius, would be weak in catering its teaching to suit the learner's profiles and contextual needs as well as serving as an ethical-spiritual guide and role-model to its students. Two major implications are discussed in this paper. First, the study highlights the potential complementary function of AI in the transfer of information and the irreplaceable role of the human teacher in moral education. Secondly, rather than making spirituality redundant, the limitation of AI in Confucian teaching speaks of the continuous relevance and centrality of spiritual development for human beings.																	0954-0091	1360-0494				JUL 2	2020	32	3					280	291		10.1080/09540091.2019.1709045		JAN 2020											
J								The convex hull heuristic for nonlinear integer programming problems with linear constraints and application to quadratic 0-1 problems	JOURNAL OF HEURISTICS										Nonlinear 0-1 integer programming; Simplicial decomposition; Quadratic 0-1 programs with linear constraints; Primal relaxation; Convex hull relaxation; Convex hull heuristic		The convex hull heuristic is a heuristic for mixed-integer programming problems with a nonlinear objective function and linear constraints. It is a matheuristic in two ways: it is based on the mathematical programming algorithm called simplicial decomposition, or SD (von Hohenbalken in Math Program 13:49-68, 1977), and at each iteration, one solves a mixed-integer programming problem with a linear objective function and the original constraints, and a continuous problem with a nonlinear objective function and a single linear constraint. Its purpose is to produce quickly feasible and often near optimal or optimal solutions for convex and nonconvex problems. It is usually multi-start. We have tested it on a number of hard quadratic 0-1 optimization problems and present numerical results for generalized quadratic assignment problems, cross-dock door assignment problems, quadratic assignment problems and quadratic knapsack problems. We compare solution quality and solution times with results from the literature, when possible.																	1381-1231	1572-9397															10.1007/s10732-019-09433-w		JAN 2020											
J								A novel stacked sparse denoising autoencoder for mammography restoration to visual interpretation of breast lesion	EVOLUTIONARY INTELLIGENCE										Deep learning; Mammogram restoration; Convolutional autoencoder; Sparse autoencoder; KL divergence	LOW-DOSE CT; IMAGE; NOISE; NETWORK; VIEW	This paper proposes a deep unsupervised learning based denoising autoencoder model for the restoration of degraded mammogram with visual interpretation of breast lumps or lesion in mammography images (called SSDAE). The proposed model attempts to intensify the underexposed and abnormal structural regions through noise elimination in mammography image. A deep stacked convolutional autoencoder is designed by combining the autoencoder and the deconvolution network which conjointly reduces noisy artifacts and improves image details in mammogram. The proposed SSDAE model takes large noisy mammogram image patches as input and extracts relevant features from target batches. The suggested model can extract relevant features and reduce the dimensionality through sparsity property of the image data while preserving the key features that have been applied to restore image data in feature space. In order to reconstruct a deafening mammogram, the proposed model is carried out through a patched base training on samples to suppress noise thereby preserving structural details in mammography imaging. Experimental results authenticate that the suggested SSDAE model outplays a number of state-of-the-art methods for both X-ray mammogram and ultrasonographic mammogram. The execution speed for target noisy images increases with fine tuning of the network when compared to other algorithms.																	1864-5909	1864-5917															10.1007/s12065-019-00344-0		JAN 2020											
J								Emotion recognition of speech signal using Taylor series and deep belief network based classification	EVOLUTIONARY INTELLIGENCE										Emotion recognition; Taylor series; Deep Belief Network; Berlin database; Real database	FEATURES; PARAMETERS; FRAMEWORK	In the recent years, one of the multidisciplinary research areas attracting the researchers is emotion recognition. It is an important and challenging process to be achieved in emotional interaction. Accordingly, this work introduces the emotion recognition system by proposing the Taylor series based Deep Belief Network (Taylor-DBN). The noise present in the speech signal is removed through the speech enhancement process and then, subjected to the feature extraction. The features, such as tonal power ratio, Multiple Kernel Mel Frequency Cepstral Coefficients (MKMFCC) parameters, and the spectral flux are extracted and provided as the training input to the proposed Taylor-DBN classifier for identifying the emotions present in the signal. The experimentation is done with the help of the Berlin database, real database 1, and the real database 2. The experimental datasets contain the speech signals from different domain and language, and the performance of the proposed Taylor-DBN has shown minimal variations in each domain, and thus, the proposed model is suitable for various domains. The proposed Taylor-DBN outclassed other comparative models with Accuracy, False Acceptance Rate (FAR), and False Rejection Rate (FRR) values of 0.97, 0.0135, and 0.0165, respectively.																	1864-5909	1864-5917															10.1007/s12065-019-00333-3		JAN 2020											
J								Convolutional neural networks approach for multimodal biometric identification system using the fusion of fingerprint, finger-vein and face images	PEERJ COMPUTER SCIENCE										CNN; Multimodal biometrics; Fingerprint recognition; Finger-vein recognition; Face recognition; Fusion; Random forest	FEATURE-LEVEL FUSION; HISTOGRAM EQUALIZATION	In recent years, the need for security of personal data is becoming progressively important. In this regard, the identification system based on fusion of multibiometric is most recommended for significantly improving and achieving the high performance accuracy. The main purpose of this paper is to propose a hybrid system of combining the effect of tree efficient models: Convolutional neural network (CNN), Softmax and Random forest (RF) classifier based on multi-biometric fingerprint, finger-vein and face identification system. In conventional fingerprint system, image pre-processed is applied to separate the foreground and background region based on K-means and DBSCAN algorithm. Furthermore, the features are extracted using CNNs and dropout approach, after that, the Softmax performs as a recognizer. In conventional fingervein system, the region of interest image contrast enhancement using exposure fusion framework is input into the CNNs model. Moreover, the RF classifier is proposed for classification. In conventional face system, the CNNs architecture and Softmax are required to generate face feature vectors and classify personal recognition. The score provided by these systems is combined for improving Human identification. The proposed algorithm is evaluated on publicly available SDUMLA-HMT real multimodal biometric database using a GPU based implementation. Experimental results on the datasets has shown significant capability for identification biometric system. The proposed work can offer an accurate and efficient matching compared with other system based on unimodal, bimodal, multimodal characteristics.																	2376-5992					JAN 6	2020									e248	10.7717/peerj-cs.248													
J								Linking emotions to behaviors through deep transfer learning	PEERJ COMPUTER SCIENCE										Behavior quantification; Emotion; Affective computing; Neural networks; Couples therapy	RISK-ASSESSMENT; SPEECH; RECOGNITION; DEPRESSION; FEATURES; CONTEXT	Human behavior refers to the way humans act and interact. Understanding human behavior is a cornerstone of observational practice, especially in psychotherapy. An important cue of behavior analysis is the dynamical changes of emotions during the conversation. Domain experts integrate emotional information in a highly nonlinear manner; thus, it is challenging to explicitly quantify the relationship between emotions and behaviors. In this work, we employ deep transfer learning to analyze their inferential capacity and contextual importance. We first train a network to quantify emotions from acoustic signals and then use information from the emotion recognition network as features for behavior recognition. We treat this emotion-related information as behavioral primitives and further train higher level layers towards behavior quantification. Through our analysis, we find that emotion-related information is an important cue for behavior recognition. Further, we investigate the importance of emotional-context in the expression of behavior by constraining (or not) the neural networks' contextual view of the data. This demonstrates that the sequence of emotions is critical in behavior expression. To achieve these frameworks we employ hybrid architectures of convolutional networks and recurrent networks to extract emotion-related behavior primitives and facilitate automatic behavior recognition from speech.																	2376-5992					JAN 6	2020									e246	10.7717/peerj-cs.246													
J								HACSim: an R package to estimate intraspecific sample sizes for genetic diversity assessment using haplotype accumulation curves	PEERJ COMPUTER SCIENCE										Algorithm; DNA barcoding; Extrapolation; Iterative method; Sampling sufficiency; Species	SPECIES DELIMITATION; DNA; BARCODE; IDENTIFICATION; MODELS; NUMBER; WILL	Assessing levels of standing genetic variation within species requires a robust sampling for the purpose of accurate specimen identification using molecular techniques such as DNA barcoding; however, statistical estimators for what constitutes a robust sample are currently lacking. Moreover, such estimates are needed because most species are currently represented by only one or a few sequences in existing databases, which can safely be assumed to be undersampled. Unfortunately, sample sizes of 5 10 specimens per species typically seen in DNA barcoding studies are often insufficient to adequately capture within-species genetic diversity. Here, we introduce a novel iterative extrapolation simulation algorithm of haplotype accumulation curves, called HACSim (Haplotype Accumulation Curve Simulator) that can be employed to calculate likely sample sizes needed to observe the full range of DNA barcode haplotype variation that exists for a species. Using uniform haplotype and non-uniform haplotype frequency distributions, the notion of sampling sufficiency (the sample size at which sampling accuracy is maximized and above which no new sampling information is likely to be gained) can be gleaned. HACSim can be employed in two primary ways to estimate specimen sample sizes: (1) to simulate haplotype sampling in hypothetical species, and (2) to simulate haplotype sampling in real species mined from public reference sequence databases like the Barcode of Life Data Systems (BOLD) or GenBank for any genomic marker of interest. While our algorithm is globally convergent, runtime is heavily dependent on initial sample sizes and skewness of the corresponding haplotype frequency distribution.																	2376-5992					JAN 6	2020									e243	10.7717/peerj-cs.243													
J								Simulating and modelling the DAX index and the USO Etf financial time series by using a simple agent-based learning architecture	EXPERT SYSTEMS										agent-based modelling; financial markets; simulated annealing; USO Etf and DAX index time series	OPTIMIZATION; MARKETS	This work presents an extensive case study on modelling the DAX (Deutscher Aktienindex) index and United States Oil Fund (USO) exchange-traded fund (Etf) time series with the financial agent-based system learning financial agent-based simulator (L-FABS) that exploits simulated annealing as a learning method. The USO Etf time series is highly correlated with oil price behaviour, and the DAX index is based on the weighted and accumulated behaviour of the share prices of some of the largest companies traded on the Frankfurt Stock Exchange. These two time series are driven by completely different economic factors and thus provide two diverse empirical settings to evaluate the effectiveness of our methodology. Our experimentation shows that a relatively simple computational representation of real financial markets is effective in capturing the overall behaviour of the time series with varying approximation levels while the prediction target is moved into the future. The reported experimental investigation of L-FABS shows that it is robust notwithstanding the learning method used and the data sets exploited. L-FABS indeed produced a relatively low approximation error in several settings even when evaluated with respect to other modelling approaches, for example, 0.88% and 1.61% errors on average for 1 day ahead experiments in, respectively, DAX index and USO Etf.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12516	10.1111/exsy.12516		JAN 2020											
J								Consensus reaching process for fuzzy behavioral TOPSIS method with probabilistic linguistic q-rung orthopair fuzzy set based on correlation measure	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										consensus reaching process; fuzzy behavioral TOPSIS method; linguistic scale function; probabilistic linguistic q-rung orthopair fuzzy set	GROUP DECISION-MAKING; DISTANCE MEASURE; TERM SETS; OPERATORS; MODELS	The paper proposes a consensus reaching process for fuzzy behavioral TOPSIS method with probabilistic linguistic q-rung orthopair fuzzy sets (PLq-ROFSs) based on correlation measure. First, the operational laws of adjusted PLq-ROFSs based on linguistic scale function (LSF) for semantics of linguistic terms are introduced, where the PLq-ROFSs have same probability space. In addition, we define the score function and accuracy function of PLq-ROFS based on the proposed operational laws to compare the PLq-ROFSs. Furthermore, we propose the probabilistic linguistic q-rung orthopair fuzzy weighted averaging (PLq-ROFWA) operator and the probabilistic linguistic q-rung orthopair fuzzy order weighted averaging (PLq-ROFOWA) operator to aggregate the linguistic decision information. Considering the inconsistency between the individual information and aggregated information in decision-making process and the demiddle of given linguistic sets tocision makers' behavioral factors, we define a new correlation measure based on LSF to develop a consensus reaching process for fuzzy behavioral TOPSIS method with PLq-ROFSs. Finally, a numerical example concerning the selection of optimal green enterprise is given to illustrate the feasibility of the proposed method and some comparative analyses with the existing methods are given to show its effectiveness. The sensitivity analysis and stability analysis of the proposed method on the ranking results are also discussed.																	0884-8173	1098-111X				MAR	2020	35	3					494	528		10.1002/int.22215		JAN 2020											
J								H-1 NMR spectroscopy coupled with multivariate analysis was applied to investigate Italian cherry tomatoes metabolic profile	JOURNAL OF CHEMOMETRICS										H-1 NMR profiling; cherry tomatoes geographical origin; kNN algorithm; PCA and PLS-DA analysis; tomato lipophilic metabolites	NUCLEAR-MAGNETIC-RESONANCE; OLIVE OILS COMPOSITION; GEOGRAPHICAL ORIGIN; PATTERN-RECOGNITION; NMR-SPECTROSCOPY; DISCRIMINATION; H-1; CLASSIFICATION; CAROTENOIDS; SPECTRA	Nuclear magnetic resonance (NMR) spectroscopy, in combination with different chemometric methods, was widely used for metabolomic profiling in the geographical determination of food origin. In the present study, spectra data of cherry tomatoes, collected from Pachino (Sicily) and Sabaudia (Latium), were analyzed by principal component analysis (PCA), k nearest neighbors (kNN), and partial least-squares discriminant analysis (PLS-DA) in order to discriminate the samples according to their geographical provenance. The PCA analysis of H-1 NMR spectra of Sabaudia cherry tomatoes showed significant differences linked to the production year: phospholipids had higher levels in 2004, but less amounts of polyunsaturated acids and lycopene were observed with respect to the year 2005. Despite the annual differences in H-1 NMR metabolic profile of Sabaudia cherry tomatoes, using unsupervised (PCA) and supervised (PLS-DA, kNN) approaches, the geographical origin differentiation was obtained. In fact, the kNN algorithm correctly classified approximately 84% to 87% of Pachino samples and 77% of Sabaudia ones with recognition ability varied from 82% to 84.4% and prediction ability (CV) of 76.2% and 94.7%. The PC1 component, with 53% of total variance, greatly separated Pachino cherry tomatoes from Sabaudia ones and PLS-DA model showed a good degree of separation with recognition ability of 100% and prediction ability (CV) of 93% to 100%. PCA and PLS-DA combined analysis highlighted the most prominent spectral areas that well separated the two groups of samples. So, phytosterols were found discriminating compounds according to PCA and PLS-DA and differences in aroma components were observed mainly in PCA analysis.																	0886-9383	1099-128X				JAN	2020	34	1							e3191	10.1002/cem.3191		JAN 2020											
J								Some realizations and instances of Yager prioritized preference frame with application in evaluation and decision making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										aggregation operators; decision making; induced OWA operators; information fusion; ordered weighted averaging operators; prioritized OWA operators; Yager prioritized preference frame	OWA OPERATORS; AGGREGATION	This study first revamps Yager prioritized ordered weighted averaging operators, and condenses them into a conceptual frame with putting aside one realization from Yager's original proposal. Then, based on elicited conceptual frame called Yager prioritized preference conceptual frame, this article proposes three distinct realizations to the conceptual frame with corresponding different instances, in which some evaluation models with weights determination methods are provided. Numerical examples are also presented immediately after every instance. Lastly, this study proposes the concepts of outer monotonic, inner monotonic, and global monotonic weights functions, and discusses some related properties, which are often embodied in preferences of decision makers.																	0884-8173	1098-111X				MAR	2020	35	3					557	568		10.1002/int.22218		JAN 2020											
J								Generic metadata representation framework for social-based event detection, description, and linkage	KNOWLEDGE-BASED SYSTEMS										Social media; Metadata; Semantics; Similarity evaluation; Event detection; Event relationships; Collective knowledge	KNOWLEDGE; SIMILARITY; CLASSIFICATION; INFORMATION; QUALITY; NUMBER; MODEL	Various methods have been put forward to perform automatic social-based event detection and description. Yet, most of them do not capture the semantic meaning embedded in online social media data, which are usually highly heterogeneous and unstructured, and do not identify event relationships (e.g., car accident temporally occurs after storm, and geographically occurs near soccer match). To address this problem, we introduce a generic Social-based Event Detection, Description, and Linkage framework titled SEDDaL, taking as input: a collection of social media objects from heterogeneous sources (e.g., Flickr, YouTube, and Twitter), and producing as output a collection of semantically meaningful events interconnected with spatial, temporal, and semantic relationships. The latter are required as the building blocks for event-based Collective Knowledge (CK) organization, where CK underlines the combination of all known data, information, and metadata concerning a given concept or event. SEDDaL consists of four main modules for: i) describing social media objects in a generic Metadata Representation Space Model (MRSM) consisting of three composite dimensions: temporal, spatial, and semantic, ii) evaluating the similarity between social media objects' descriptions following MRSM, iii) detecting events from similar social media objects using an adapted unsupervised learning algorithm, where events are represented as clusters of objects in MRSM, and iv) identifying directional, metric, and topological relationships between events following MRSM's dimensions. We believe this is the first study to provide a generic model for describing semantic-aware events and their relationships extracted from social metadata on the Web. Experimental results confirm the quality and potential of our approach. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104817	10.1016/j.knosys.2019.06.025													
J								Multi-target QSAR modelling of chemo-genomic data analysis based on Extreme Learning Machine	KNOWLEDGE-BASED SYSTEMS										Machine learning; Whale Optimization Algorithm; Genetic algorithm; Extreme learning machine; QSAR; Molecular docking; ExCAPE chemo-genomics database	DRUG DESIGN; ALGORITHM; OPTIMIZATION; CLASSIFICATION; STRATEGIES; EFFICIENT; DOCKING	This paper presents a new Quantitative Structure-Activity Relationship (QSAR) model based on Extreme Learning Machine (ELM) to predict the biological activity of the benchmark Escape-Data sets compounds in order to provide an effective learning solution for regression analysis. The pre-processing phase of this model has been performed for the chemo-genomics datasets using the k-Nearest Neighbours (k-NN) algorithm to predict missing values of the dataset. In the second phase, the Genetic algorithm hybrid with Binary Whale Optimization algorithm (GBWOA) is adapted to determine the significance and the optimized features in feature selection phase. The minmax method is used in the third phase to transform all features to binary form in order to increases the efficiency of the proposed model by smoothing the data points and reducing fluctuation among features. ELM is used in the final phase as regression algorithm to predict chemo-genomics chemical compound. Different experiments have been performed in this paper on datasetwhich has been collected from ExCAPE chemo-genomics database project composed of 43509 compounds, 1134 targets besides biological activity and 40 chemical descriptors. The experimental results show that the proposed model is efficient in improving the level of prediction based on some statistical measurements. Also, ELM produced satisfactory results when the number of hidden nodes is greater than or equal to 1000 L. Moreover, the proposed model achieved high accuracy using R-2 measure (approximate to 0.971) which outperforms the other algorithms in literature such as (WOA, ALO, BAT and CSA) with accuracies (approximate to 0.673, approximate to 0.753, approximate to 0.680, and approximate to 0.897) respectively. In addition, the docking results succeeded in validating the current QSAR model. In the current research, 41686 (95.81%) compounds are lead compound and 36965 (84.95%) compounds are a candidate for multi-target genes. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104977	10.1016/j.knosys.2019.104977													
J								Fireworks explosion based artificial bee colony for numerical optimization	KNOWLEDGE-BASED SYSTEMS										Artificial bee colony; Fireworks explosion search; Exploitation; Meta-heuristic algorithm; Numerical optimization	ECONOMIC-DISPATCH PROBLEMS; SEARCH ALGORITHM; PARTICLE SWARM; GLOBAL OPTIMIZATION; STRATEGY; MECHANISM	Artificial bee colony (ABC) is a swarm optimization algorithm that shows competitive performance for many optimization problems. However, ABC often suffers from poor exploitation and slow convergence. To overcome this deficiency, this paper introduces the explosion search mechanism of fireworks algorithm into ABC, and proposes a fireworks explosion based artificial bee colony (FW-ABC) framework. More specifically, the proposed FW-ABC framework consists of two search stages, namely bee search stage and fireworks explosion search stage. After three bee search operators (i.e., employed, onlooker and scout), a fireworks explosion search is implemented, which is used to exploit the promising regions for finding better solutions. The proposed FW-ABC framework is applied to six ABC algorithms, and the experimental results show that FW-ABC can significantly improve the performances of various existing ABC algorithms on CEC2014 benchmark functions. Moreover, FW-ABC algorithm also exhibits better performance compared with state-of-the-art fireworks algorithms and other meta-heuristic algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105002	10.1016/j.knosys.2019.105002													
J								A new sparse representation framework for compressed sensing MRI	KNOWLEDGE-BASED SYSTEMS										Compressed sensing (CS); Double tight frame (DTF); Magnetic resonance imaging (MRI); Robust L-1,L-a-norm; Sparse representation (SR)	IMAGE-RECONSTRUCTION; EFFICIENT ALGORITHM; REGULARIZATION; MINIMIZATION	Compressed sensing based Magnetic Resonance imaging (MRI) via sparse representation (or transform) has recently attracted broad interest. The tight frame (TF)-based sparse representation is a promising approach in compressed sensing MRI. However, the conventional TF-based sparse representation is difficult to utilize the sparsity of the whole image. Since the whole image usually has different structure textures and a kind of tight frame can only represent a particular kind of ground object, how to reconstruct high-quality of magnetic resonance (MR) image is a challenge. In this work, we propose a new sparse representation framework, which fuses the double tight frame (DTF) into the mixed norm regularization for MR image reconstruction from undersampled k-space data. In this framework, MR image is decomposed into smooth and nonsmooth regions. For the smooth regions, the wavelet TF-based weighted L-1-norm regularization is developed to reconstruct piecewise-smooth information of image. For nonsmooth regions, we introduce the curvelet TF-based robust L-1,L-a-norm regularization with the parameter to preserve the edge structural details and texture. To estimate the reasonable parameter, an adaptive parameter selection scheme is designed in robust L-1,L-a-norm regularization. Experimental results demonstrate that the proposed method can achieve the best image reconstruction results when compared with other existing methods in terms of quantitative metrics and visual effect. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104969	10.1016/j.knosys.2019.104969													
J								Automatic detection of breathing disorder from ballistocardiography signals	KNOWLEDGE-BASED SYSTEMS										Disordered breathing; Ballistocardiography; Cartan curvature; Convolutional neural networks; Tensometers	SYSTEM; IDENTIFICATION; DIAGNOSIS; MODEL	Ballistocardiography (BCG) is a common method, wherein sensory information is used to identify blood-flow cardiac activity by measuring the mechanical micromovements of the human body generated by heart movements and blood eviction to the large arteries. BCG signals can be used to detect non-standard vital functions or predict likely health problems. However, the analysis of BCG signal is challenging because it contains various mechanical noises made by human body movements. This study is aimed at extracting information regarding the pulse arrival time from BCG signals and then establishing a connection with changes in breathing disorders, such as simulated apnoea, using convolutional neural networks. We present a novel approach toward recognizing the form of breathing which is independent of the body position while data are being collected from tensometers measuring the mechanical micromovements (motion) of the individual. The mechanical motions are caused by cardiac activity with multivariate time series output, which is processed to obtain the source data for breath detection. The signals are first processed by Cartan curvature. This is a differential geometric invariant, which enables the detection of marginal variations in the signals. Conditional dependency and short-term fluctuations are eliminated in longer measuring-periods. By these means, the breathing anomalies of individuals are subsequently detected between heartbeats using the time delay between the R-wave from the electrocardiogram (ECG) and the pulse arrival times. Moreover, ECG signals are included in the system for data sampling. In addition, the values of the time delay are used as the inputs to train a convolutional neural network classifier with two outputs (regular and disordered breathing) to validate the experiment. We achieved an average accuracy of 89.35%, sensitivity of 86.35%, and specificity of 91.22% on 828 regular and 1332 disordered breathing states from eight human subjects. The conclusion is that our novel method can detect disordered breathing from processed BCG signal, i.e. from the pulse arrival time, in a manner not previously used elsewhere. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104973	10.1016/j.knosys.2019.104973													
J								Group inference method of attribution theory based on Dempster-Shafer theory of evidence	KNOWLEDGE-BASED SYSTEMS										Attribution theory; Group inference method; Shafer's discounting; Dempster's rule; Group decision making; Consensus reaching	GROUP DECISION-MAKING; CAUSAL ATTRIBUTION; CONSENSUS; MODEL; COVARIATION; COMBINATION; LEADERSHIP; LOCUS	Kelley's attribution theory has been widely popular in recent years. Lots of efforts have been spent on improving it with the assumption that there is only one expert to make attributions and the expert is assumed to be omniscient and omnipotent. However, such an assumption hardly exists in reality for the reason that the knowledge of each expert to make judgments is always limited. In order to solve this problem, this paper proposes a group inference method under the framework of Kelley's attribution theory based on Dempster-Shafer theory of evidence. An information extraction mechanism is introduced to ensure that the real judgments of each expert can be well described, Then Shafer's discounting is used to generate the basic probability assignment (BPA) functions by integrating the weights of experts on different criteria into the judgments of experts. The Dempster's rule is employed to make fusion for the BPA functions, and a consensus reaching model which can increase the satisfaction degrees of group decision as much as possible is established to determine the probabilities of external and internal causes for the evaluated behavior. Finally, an algorithm is summarized, and illustrative example and discussion are provided to demonstrate its applicability. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104985	10.1016/j.knosys.2019.104985													
J								Partial reduction algorithms for fuzzy relation systems	KNOWLEDGE-BASED SYSTEMS										Discernibility matrix; Fuzzy relation; Fuzzy relation system; Reduction algorithm; Rough set	ATTRIBUTE REDUCTION; KNOWLEDGE REDUCTION; DECISION TABLES	Relationships among elements of sets appear in many contexts and can be represented using fuzzy relations. A fuzzy relation system is an extensive information system. Attribute reduction is widely used to discover knowledge hidden in databases. Many approaches to reduction based on the rough set theory have been proposed in recent years to deal with information systems, and partial reductions have been proposed in relation to decision systems to extract partial decision rules. This paper considers an approach to fuzzy relation systems based on significant partial attribute reduction by means of discernibility matrices. We propose the concept of X-lower and -upper approximation reductions, and develop their corresponding reduction algorithms. These methods of reduction unify results that have been reported in the literature. We provide examples from the UCI datasets to verify our algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105047	10.1016/j.knosys.2019.105047													
J								Gaussian Mixture Descriptors Learner	KNOWLEDGE-BASED SYSTEMS										Minimum description length; Classification; Machine learning	MINIMUM DESCRIPTION LENGTH; ONLINE; CLASSIFICATION; ALGORITHMS; EFFICIENT	In recent decades, various machine learning methods have been proposed to address classification problems. However, most of them do not support incremental (or online) learning and therefore are neither scalable nor robust to dynamic problems that change over time. In this study, a classification method was introduced based on the minimum description length principle, which offered a very good trade-off between model complexity and predictive power. The proposed method is lightweight, multiclass, and online. Moreover, despite its probabilistic nature, it can handle continuous features. Experiments conducted on real-world datasets with different characteristics demonstrated that the proposed method outperforms established online classification methods and is robust to overfitting, which is a desired characteristic for large, dynamic, and real-world classification problems. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105039	10.1016/j.knosys.2019.105039													
J								User intimacy model for question recommendation in community question answering	KNOWLEDGE-BASED SYSTEMS										Question recommendation; User intimacy model; Community question answering; Recommender systems	TOPIC MODELS; RANKING	In this paper, we address the problem of automatic recommendation of new questions to suitable users in community question answering (CQA). The major challenge is the accurate selection of suitable users to answer a given question. Most approaches seek suitable users for a question by estimating their capability, interests or a blend of both. However, this ignores intimacy between the user and the asker of a question over different topics. Intimacy between askers and answerers is an important factor in question recommendation. For example, a user is likely to post an answer if interested in a question and intimate with its asker. We propose to model and learn intimacy between users over topics with social interaction in CQA for question recommendation using a novel topic model. We believe this paper is the first to estimate the intimacy between users over different topics and investigate influences on the performance of question recommendation in CQA. We propose a user intimacy model (UIM), an LDA-style model that incorporates social interaction in the generative process of a question-answer (QA) pair to model and learn intimacy between users over topics. Experiments using real-world data from Stack Overflow show that our UIM-based approach consistently and significantly improves the performance of question recommendation, demonstrating that our approach can increase question recommendation accuracy in CQA by utilizing the intimacy between users over topics and that this is an important factor in question recommendation. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104844	10.1016/j.knosys.2019.07.015													
J								Sparse attention based separable dilated convolutional neural network for targeted sentiment analysis	KNOWLEDGE-BASED SYSTEMS										Targeted sentiment analysis; Sparse attention; Separable dilated CNN; Multichannel embedding		Long short-term memory networks (LSTM) and classical convolutional neural networks (CNN) are two critical methods for the task of targeted sentiment analysis, but LSTM are difficult to parallelize and time-inefficient, and classical CNN can only capture local semantic features. To this end, this paper first proposes a sparse attention based separable dilated convolutional neural network (SA-SDCCN), which consists of multichannel embedding layer, separable dilated convolution module, sparse attention layer, and output layer. Specifically, our work is mainly concentrated on the first three parts. In multichannel embedding layer, semantic and sentiment embeddings are incorporated into an embedding tensor, which builds richer representations over the input sequence. In separable dilated convolution module, long-range contextual semantic information is explored and multi-scale contextual semantic dependencies are aggregated simultaneously through diverse dilation rates. Moreover, the separable structure further reduces the model parameters. In sparse attention layer, sentiment-oriented components are noticed according to the features of specific target entity. Finally, some experiments on three benchmark datasets demonstrate that SA-SDCCN achieves comparable or even better performance than state-of-the-art methods in terms of higher parallelism and lower computational cost. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104827	10.1016/j.knosys.2019.06.035													
J								Unmanned aerial vehicle swarm distributed cooperation method based on situation awareness consensus and its information processing mechanism	KNOWLEDGE-BASED SYSTEMS										Unmanned aerial vehicle swarm; Cooperation method; Situation awareness; Situation awareness consensus; Group decision making; Consensus	GROUP DECISION-MAKING; LINGUISTIC PREFERENCE RELATIONS; THREAT ASSESSMENT METHOD; OPERATORS; NETWORKS; MODEL; GDM	Unmanned aerial vehicle (UAV) swarm autonomous cooperation method is the basis of UAV swarm intelligent decision-making and cooperative engagement. Due to the complexity and antagonism of the mission environment, usually UAV swarm works in a distributed architecture. However, the process of situation awareness (SA) and the uncertainty of situation information have not been considered carefully by existing distributed cooperation methods. This paper proposes UAV swarm distributed cooperation method based on SA consensus and its information processing mechanism. Firstly, combined with the characteristics of swarm cooperative engagement, we analyze the essence of swarm distributed cooperation methods and propose swarm distributed cooperation method based on SA consensus, which can be further classified into situation perception consensus and situation comprehension consensus. Then, based on different types of consensus model of group decision making (GDM), we regard the UAVs which have a certain autonomous intelligence as intelligent decision-makers and establish the corresponding information processing mechanisms of proposed distributed cooperation methods. Finally, theoretical analysis and case study are used to demonstrate the practicality and effectiveness of the proposed method, which is more suitable for complex and antagonistic mission environment. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105034	10.1016/j.knosys.2019.105034													
J								Optimizing Hearthstone agents using an evolutionary algorithm	KNOWLEDGE-BASED SYSTEMS										Evolutionary algorithms; Hearthstone; Videogames; Evolution strategy; Artificial intelligence; Games; Card games; Collectible card games	CARLO TREE-SEARCH; INFORMATION; GAMES	Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105032	10.1016/j.knosys.2019.105032													
J								A new similarity measure for collaborative filtering based recommender systems	KNOWLEDGE-BASED SYSTEMS										Recommendation systems; Collaborative filtering; Neighborhood based CF; Similarity measure	ACCURACY	The objective of a recommender system is to provide customers with personalized recommendations while selecting an item among a set of products (movies, books, etc.). The collaborative filtering is the most used technique for recommender systems. One of the main components of a recommender system based on the collaborative filtering technique, is the similarity measure used to determine the set of users having the same behavior with regard to the selected items. Several similarity functions have been proposed, with different performances in terms of accuracy and quality of recommendations. In this paper, we propose a new simple and efficient similarity measure. Its mathematical expression is determined through the following paper contributions: 1) transforming some intuitive and qualitative conditions, that should be satisfied by the similarity measure, into relevant mathematical equations namely: the integral equation, the linear system of differential equations and a non-linear system and 2) resolving the equations to achieve the kernel function of the similarity measure. The extensive experimental study driven on a benchmark datasets shows that the proposed similarity measure is very competitive, especially in terms of accuracy, with regards to some representative similarity measures of the literature. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105058	10.1016/j.knosys.2019.105058													
J								Distributed multi-label feature selection using individual mutual information measures	KNOWLEDGE-BASED SYSTEMS										Multi-label learning; Feature selection; Mutual information; Distributed computing; Apache spark	CLASSIFICATION; TRANSFORMATION; ALGORITHM; SPARK; KNN	Multi-label learning generalizes traditional learning by allowing an instance to belong to multiple labels simultaneously. This causes multi-label data to be characterized by its large label space dimensionality and the dependencies among labels. These challenges have been addressed by feature selection techniques which improve the final model accuracy. However, the large number of features along with a large number of labels call for new approaches to manage data effectively and efficiently in distributed computing environments. This paper proposes a distributed model to compute a score that measures the quality of each feature with respect to multiple labels on Apache Spark. We propose two different approaches that study how to aggregate the mutual information of multiple labels: Euclidean Norm Maximization (ENM) and Geometric Mean Maximization (GMM). The former selects the features with the largest L-2-norm whereas the latter selects the features with the largest geometric mean. Experiments compare 9 distributed multi-label feature selection methods on 12 datasets and 12 metrics. Results validated through statistical analysis indicate that ENM is able to outperform the reference methods by maximizing the relevance while minimizing the redundancy of the selected features in constant selection time. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105052	10.1016/j.knosys.2019.105052													
J								MSC+: Language pattern learning for word sense induction and disambiguation	KNOWLEDGE-BASED SYSTEMS										Lexical semantics; Information extraction; Linguistic pattern mining; Word sense induction; Word sense disambiguation	INFORMATION	Identifying the correct meaning of words in context or discovering new word senses is particularly useful for several tasks such as question answering, information extraction, information retrieval, and text summarization. However, specially in the context of user-generated contents and on-line communication (e.g. Twitter), new meanings are continuously crafted by speakers as the result of existing words being used in novel contexts. Consequently, lexical semantics inventories and systems have difficulties to cope with semantic drifting problems. In this work, we propose an approach to induce and disambiguate word senses of some target words in collections of short texts, such as tweets, through the use of fuzzy lexico-semantic patterns that we define as sequences of Morpho-semantic Components (MSC). We learn these patterns, that we call MSC+ patterns, from text data automatically. Experimental results show that instances of some MSC+ patterns arise in a number of tweets, but sometimes using different words to convey the sense of the respective MSC in some tweets where pattern instances appear. The exploitation of MSC+ patterns when they induce semantics on target words enable effective word sense disambiguation mechanisms leading to improvements in the state of the art. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105017	10.1016/j.knosys.2019.105017													
J								A heterogeneous online learning ensemble for non-stationary environments	KNOWLEDGE-BASED SYSTEMS										Heterogeneous ensemble classifier; Majority Algorithm; Concept drift; Data stream mining	WEIGHTED MAJORITY	Learning in non-stationary environments is a challenging task which requires the updating of predictive models to deal with changes in the underlying probability distribution of the problem, i.e., dealing with concept drift. Most work in this area is concerned with updating the learning system so that it can quickly recover from concept drift, while little work has been dedicated to investigating what type of predictive model is most suitable at any given time. This paper aims to investigate the benefits of online model selection for predictive modelling in non-stationary environments. A novel heterogeneous ensemble approach is proposed to intelligently switch between different types of base models in an ensemble to increase the predictive performance of online learning in non-stationary environments. This approach is Heterogeneous Dynamic Weighted Majority (HDWM). It makes use of "seed" learners of different types to maintain ensemble diversity, overcoming problems of existing dynamic ensembles that may undergo loss of diversity due to the exclusion of base learners. The algorithm has been evaluated on artificial and real-world data streams against existing well-known approaches such as a heterogeneous Weighted Majority Algorithm (WMA) and a homogeneous Dynamic Weighted Majority (DWM). The results show that HDWM performed significantly better than WMA in non-stationary environments. Also, when recurring concept drifts were present, the predictive performance of HDWM showed an improvement over DWM. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104983	10.1016/j.knosys.2019.104983													
J								DTW-NN: A novel neural network for time series recognition using dynamic alignment between inputs and weights	KNOWLEDGE-BASED SYSTEMS										Neural networks; Dynamic time warping; Temporal kernel; Time series; Dynamic programming	SPIKING NEURONS; SHAPE; KERNEL; ALGORITHM; FEATURES; TEXTURE; SYSTEMS	This paper describes a novel model for time series recognition called a Dynamic Time Warping Neural Network (DTW-NN). DTW-NN is a feedforward neural network that exploits the elastic matching ability of DTW to dynamically align the inputs of a layer to the weights. This weight alignment replaces the standard dot product within a neuron with DTW. In this way, the DTW-NN is able to tackle difficulties with time series recognition such as temporal distortions and variable pattern length within a feedforward architecture. We demonstrate the effectiveness of DTW-NNs on four distinct datasets: online handwritten characters, accelerometer-based active daily life activities, spoken Arabic numeral Mel-Frequency Cepstrum Coefficients (MFCC), and one-dimensional centroid-radii sequences from leaf shapes. We show that the proposed method is an effective general approach to temporal pattern learning by achieving state-of-the-art results on these datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104971	10.1016/j.knosys.2019.104971													
J								Gated Mixture Variational Autoencoders for Value Added Tax audit case selection	KNOWLEDGE-BASED SYSTEMS										Value Added Tax; Audit selection; Variational autoencoder; Finite mixture model	FRAUD	In this work, we address the problem of targeted Value Added Tax (VAT) audit case selection by means of machine learning. This is a challenging problem that has remained rather elusive for EU-based Tax Departments, due to the inadequate quantity of tax audits that can be used for conventional supervised model training. To this end, we devise a novel Gated Mixture Variational Autoencoder deep network, that can be effectively trained with data from a limited number of audited taxpayers, combined with a large corpus of filed VAT returns. This gives rise to a semi-supervised learning framework that leverages the latest advances in deep learning and robust regularization using variational inference. We developed our approach in collaboration with the Cyprus Tax Department and experimentally deployed it to facilitate its audit selection process; to this end, we used actual VAT data from Cyprus-based taxpayers. This way, we obtained strong empirical evidence that our approach can greatly facilitate the VAT audit case selection process. Specifically, we obtained up to 76% out-of-sample accuracy in detecting whether a significant tax yield will be generated from a specific prospective VAT audit. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105048	10.1016/j.knosys.2019.105048													
J								Subspace clustering based on alignment and graph embedding	KNOWLEDGE-BASED SYSTEMS										Subspace clustering; Face recognition; Image alignment; Graph embedding	LOW-RANK REPRESENTATION; SPARSE; SEGMENTATION; RECOGNITION; ALGORITHM; MIXTURES	In this paper, we propose a new subspace clustering method based on alignment and graph embedding (SCAGE). In SCAGE, we unify the image alignment process and clustering subspace learning process based on low rank and sparse representation. Besides, we use the label prediction information, error information and coefficients to conduct the graph embedding. In addition, the prior knowledge is used to initialize the label prediction matrix which not only speeds up the converge of the clustering process but also achieves a better result. Various experiments show that SCAGE achieves better performance than state-of-the-art algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105029	10.1016/j.knosys.2019.105029													
J								Time-dependent vehicle routing problem with time windows of city logistics with a congestion avoidance approach	KNOWLEDGE-BASED SYSTEMS										Time-dependent vehicle routing problem with time windows; Road travel time across time periods; Congestion avoidance approach; Improved ant colony algorithm	TRAVEL-TIMES; GREEN; EMISSIONS; ALGORITHM; QUALITY; SEARCH	The time-dependent vehicle routing problem with time windows (TDVRPTW) is examined in this study. A mathematical model for the TDVRPTW is formulated by considering the time-dependent vehicle speeds, capacity, travel time, wait time, customer demand, service time, time window, impact of dynamic loads, and travel speed effect on vehicle carbon emissions. The objective of the TDVRPTW model is to minimize the sum of the fixed costs of the vehicle used, as well as the costs of drivers, fuel consumption, and carbon emissions. A calculation method for the road travel time across time periods with time-dependent speeds is presented. A congestion avoidance approach is proposed to avoid peak hour traffic congestion and temporal traffic congestion. An improved ant colony algorithm with a congestion avoiding approach (IACACAA) is elucidated according to the characteristics of the TDVRPTW model. The effectiveness of the proposed approaches was demonstrated in several computational experiments. The results show that the proposed IACACAA can scientifically plan the departure time and driving route for each vehicle, effectively avoid traffic congestion, and reduce the total distribution costs. The congestion avoidance approach effectively reduces vehicle fuel consumption and carbon emissions and protects the environment. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104813	10.1016/j.knosys.2019.06.021													
J								Evolutionary game dynamics in multiagent systems with prosocial and antisocial exclusion strategies	KNOWLEDGE-BASED SYSTEMS										Cooperation; Social exclusion; Costly punishment; Second-order sanctioning	REPLICATOR DYNAMICS; COOPERATION; PUNISHMENT; EMERGENCE; PROMOTES	Cooperation is very important in human society and identified as an essential principle of evolution, but how to promote cooperation among rational individuals remains a huge challenge. Recent works have found that prosocial exclusion can work as a powerful control strategy to promote cooperation effectively. However, it remains unclear whether prosocial exclusion can still favor cooperation when antisocial exclusion is introduced. And does prosocial exclusion have evolutionary advantages when comparing with prosocial and antisocial punishment strategies? To address these issues, we first introduce prosocial and antisocial pool exclusion strategies into the public goods game and study the stationary distribution of each strategy in finite well-mixed populations. We find that the introduction of antisocial exclusion inhibits cooperation, but it does not reduce the evolutionary advantage of prosocial exclusion. We then investigate the competition between the full set of pool exclusion and pool punishment strategies, and reveal that prosocial pool excluders can do better than other strategists no matter whether the second-order sanctioning is considered or not. Our results suggest that social exclusion is a better way for restraining defection than costly punishment, even when antisocial behavior is allowed. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								104835	10.1016/j.knosys.2019.07.006													
J								Aspect-based sentiment analysis with gated alternate neural network	KNOWLEDGE-BASED SYSTEMS										Aspect-based sentiment analysis; Natural language processing; Text classification; Deep learning; CNN; RNN; Attention	CLASSIFICATION	Aspect-based sentiment analysis (ABSA) is a type of fine-grained sentiment analysis. Previous work in ABSA is mostly based on recurrent neural networks (RNNs). However, RNNs employed in ABSA have some weaknesses, such as lacking position invariance and lacking sensitivity to local key patterns. Meanwhile, a convolutional neural network (CNN) addresses the limitations in RNN, but itself is weak at capturing long-distance dependency and modeling sequence information. Moreover, the attention mechanism employed in ABSA may introduce some noise that is detrimental to capturing important sentiment expressions. In this paper, we assume that a sentence consists of some sentiment clues, and a sentence clue consists of multiple words. Based on this, we propose a novel neural network structure, named the Gated Alternate Neural Network (GANN), to address the limitations mentioned above. In GANN, a specially designed module, named the Gate Truncation RNN (GTR), is used to learn informative aspect-dependent sentiment clue representations. In these representations, the relative distance between each context word and aspect target, the sequence information, and semantic dependency within a sentiment clue are concurrently encoded. To filter out noise, a gating mechanism is designed to control information flow to obtain more precise representations. Convolution and pooling mechanisms are employed to capture key local sentiment clue features and acquire the position invariance of features. To verify the effect and generalization of GANN, we conducted abundant experiments on four Chinese and three English datasets. The experimental results show that GANN achieves state-of-the-art results and indicate that our proposed model is language-independent. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105010	10.1016/j.knosys.2019.105010													
J								Redundancy reduction based node classification with attribute augmentation	KNOWLEDGE-BASED SYSTEMS										Attribute graph; Node classification; Community detection	COMMUNITY DETECTION; NETWORKS	Node classification for attributed graphs has attracted more and more researchers, it is very useful when labeled data are expensive and hard to obtain. However, most existing methods either only focus on node attribute data or are designed for working in single mode, resulting in omitting extra information and impairing classification performance. In view of this, in this work, we propose a semi-supervised method that can effectively incorporate various available prior information to augment the attribute matrix. This approach adds a two-level learning strategy to select and find those discriminative attributes for classification, and an augmentation step to combining various information, thereby reducing the redundancy exists between attributes and strengthening the community structure. It provides a unified way that can preserve both the node and edge information of the network. Finally, we use the classical classifier SVM to partition the induced augmented graph, we compare our method with 5 state-of-the-art methods on 4 data sets, the results confirm the effectiveness of our method. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105080	10.1016/j.knosys.2019.105080													
J								Non-ferrous metals price forecasting based on variational mode decomposition and LSTM network	KNOWLEDGE-BASED SYSTEMS										Non-ferrous metals; Price forecasting; Variational mode decomposition; Long short-term memory network; London Metal Exchange	NEURAL-NETWORKS; VOLATILITY; COPPER; PREDICTION; HYBRID	Non-ferrous metals are indispensable industrial materials and strategic supports of national economic development. The price forecasting of non-ferrous metals is critical for investors, policymakers, and researchers. Nevertheless, an accurate and robust non-ferrous metals price forecasting is a difficult yet challenging problem due to severe fluctuations and irregular cycles in the metal price evolution. Motivated by the "Divide-and-Conquer" principle, we present a novel hybrid deep learning model, which combines the VMD (variational mode decomposition) method and the LSTM (long short-term memory) network to construct a forecasting model in this paper. Here, the VMD method is firstly employed to disassemble the original price series into several components. The LSTM network is used to forecast for each component. Lastly, the forecasting results of each component are aggregated to formulate an ultimate forecasting output for the original price series. To investigate the forecasting performance of the proposed model, extensive experiments have been executed using the LME (London Metal Exchange) daily future prices of Zinc, Copper and Aluminum, and other six state-of-the-art methods are included for comparison. The experiment results demonstrate that the proposed model has superior performance for non-ferrous metals price forecasting. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105006	10.1016/j.knosys.2019.105006													
J								Spiking neural P systems with inhibitory rules	KNOWLEDGE-BASED SYSTEMS										Membrane computing; Spiking neural P systems; Spiking neural P systems with inhibitory rules; Inhibitory synapse	MEMBRANE CONTROLLERS; FAULT-DIAGNOSIS; POWER; ALGORITHM; IMAGES	Motivated by the mechanism of inhibitory synapses, a new kind of spiking neural P (SNP) system rules, called inhibitory rules, is introduced in this paper. Based on this, a new variant of SNP systems is proposed, called spiking neural P systems with inhibitory rules (SNP-IR systems). Different from the usual firing rules in SNP systems, the firing condition of an inhibitory rule not only depends on the state of the neuron associated with the rule but also is related to the states of other neurons. Moreover, from the perspective of topological structure, the new variant is shown as a directed graph with inhibitory arcs, and therefore seems to have more powerful control. The computational completeness of SNP-IR systems is discussed. In particular, it is proved that SNP-IR systems are Turing universal number accepting/generating devices. Moreover, we obtain a small universal function-computing device for SNP-IR systems consisting of 100 neurons. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105064	10.1016/j.knosys.2019.105064													
J								On Computing Entity Relatedness in Wikipedia, with Applications	KNOWLEDGE-BASED SYSTEMS										Entity relatedness; Wikipedia; Knowledge graph		Many text mining tasks, such as clustering, classification, retrieval, and named entity linking, benefit from a measure of relatedness between entities in a knowledge graph. We present a thorough study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph. To facilitate this study, we introduce a new dataset with human judgments of entity relatedness. No clear dominance is seen between measures based on textual similarity and graph proximity. Some of the better measures involve expensive global graph computations. We propose a new, space-efficient, computationally lightweight, two-stage framework for relatedness computation. In the first stage, a small weighted subgraph is dynamically grown around the two query entities; in the second stage, relatedness is derived based on computations on this subgraph. Our system shows better agreement with human judgment than existing proposals both on the new dataset and on an established one. Our framework also shows improvements with respect to the state-of-the-art on three different extrinsic evaluations in the domains of ranking entity pairs, entity linking, and synonym extraction. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105051	10.1016/j.knosys.2019.105051													
J								Multiple kernel subspace clustering with local structural graph and low-rank consensus kernel learning	KNOWLEDGE-BASED SYSTEMS										Multiple kernel learning; Subspace clustering; Self-expressiveness; Structure learning; Low-rank kernel	ROBUST; REPRESENTATION	Multiple kernel learning (MKL) methods are generally believed to perform better than single kernel learning (SKL) methods in handling nonlinear subspace clustering problem, largely thanks to MKL avoids selecting and tuning a pre-defined kernel. However, previous MKL methods mainly focused on how to define a kernel weighting strategy, but ignored the structural characteristics of the input data in both the original space and the kernel space. In this paper, we first propose a novel graph based MKL method for subspace clustering, namely, Local Structural Graph and Low-Rank Consensus Multiple Kernel Learning (LLMKL). It jointly learns an optimal affinity graph and a suitable consensus kernel for clustering purpose by elegantly integrating the MKL technology, the global structure in the kernel space, the local structure in the original space, and the Hilbert space self-expressiveness property in a unified optimization model. In particular, to capture the data global structure, we employ a substitute of the desired consensus kernel, and then introduce a low-rank constraint on the substitute to encourage that the structure of linear subspaces is present in the feature space. Moreover, the data local structure is explored by building a complete graph, where each sample is treated as a node, and an edge codes the pairwise affinity between two samples. By such, the consensus kernel learning and the affinity graph learning can promote each other such that the data in resulting Hilbert space are both self-expressive and low-rank. Experiments on both image and text clustering well demonstrate that LLMKL outperforms the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105040	10.1016/j.knosys.2019.105040													
J								Enhanced deep gated recurrent unit and complex wavelet packet energy moment entropy for early fault prognosis of bearing	KNOWLEDGE-BASED SYSTEMS										Enhanced deep gated recurrent unit; Bearing; Early fault prognosis; Energy moment entropy; Modified training algorithm	CONVOLUTIONAL NEURAL-NETWORK; BELIEF NETWORK; DIAGNOSIS; MANIFOLD; ENCODER	Early fault prognosis of bearing is a very meaningful yet challenging task to improve the security of rotating machinery. For this purpose, a novel method based on enhanced deep gated recurrent unit and complex wavelet packet energy moment entropy is proposed in this paper. First, complex wavelet packet energy moment entropy is defined as a new monitoring index to characterize bearing performance degradation. Second, deep gated recurrent unit network is constructed to capture the nonlinear mapping relationship hidden in the defined monitoring index. Finally, a modified training algorithm based on learning rate decay strategy is developed to enhance the prognosis capability of the constructed deep model. The proposed method is applied to analyze the simulated and experimental signals of bearing. The results demonstrate that the proposed method is more superior in sensibility and accuracy to the existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105022	10.1016/j.knosys.2019.105022													
J								Dual memory network model for sentiment analysis of review text	KNOWLEDGE-BASED SYSTEMS										Network embedding; Heterogeneous network; Attention mechanism; Text processing		In sentiment analysis of product reviews, both user and product information are proven to be useful. Current works handle user profile and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product information for reviews classification using separate memory networks. Then, the two representations are used jointly for sentiment analysis. The use of separate models aims to capture user profiles and product information more effectively. Comparing with state-of-the-art unified prediction models, evaluations on three benchmark datasets (IMDB, Yelp13, and Yelp14) show that our dual learning model gives performance gain of 0.6%, 1.2%, and 0.9%, respectively. The improvements are also deemed very significant measured by p-values. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105004	10.1016/j.knosys.2019.105004													
J								Automated heartbeat classification based on deep neural network with multiple input layers	KNOWLEDGE-BASED SYSTEMS										Electrocardiogram; Heartbeat classification; Convolutional neural network; Long short-term memory; Multiple input layers	ECG CLASSIFICATION; ATRIAL-FIBRILLATION; LEARNING APPROACH; DYNAMIC FEATURES; RECOGNITION; SIGNALS; MODEL	The arrhythmia is an important group of cardiovascular disease. Electrocardiogram (ECG) is commonly used for detecting arrhythmias. Computer-aided diagnosis system can diagnose ECG automatically without the limitations of visual inspection. In order to improve the performance of ECG heartbeat classification, this paper proposes a novel automatic classification system. Based on convolutional neural network (CNN) and long short-term memory (LSTM) network, a deep structure with multiple input layers is proposed. Four input layers are constructed based on different regions of a heartbeat and RR interval features. The first three inputs are convolved using different strides. The three outputs of CNN are then concatenated and go through an LSTM network. Two fully-connected layers follow and the output is concatenated with the fourth input. Eventually, the last fully-connected layer outputs the predicted label. The proposed system was evaluated by two division schemes of the MIT-BIH arrhythmia database. Class-oriented scheme achieved an overall accuracy of 99.26% and subject-oriented scheme obtained an accuracy of 94.20%. The comparison with previous works showed the excellent performance of the novel network. The combination of automatic features and handcraft features was demonstrated to be helpful in heartbeat classification. Hence, the system can be used for clinical application. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105036	10.1016/j.knosys.2019.105036													
J								Memetic search for the equitable coloring problem	KNOWLEDGE-BASED SYSTEMS										Memetic search; Heuristics; Equitable coloring; Graph coloring	TABU SEARCH; ALGORITHM	Given an undirected graph G = (V, E) and a positive integer k, an equitable legal k-coloring of G is a partition of the vertex set V into k disjoint independent sets such that the cardinalities of any two independent sets differ by one at most. The equitable coloring problem is to find the smallest k for which an equitable legal k-coloring exists. The problem has a number of applications. However, it is known to be NP-hard and thus computationally challenging. In this work, we present the first population-based memetic algorithm for solving the problem. The proposed algorithm combines a backbone-based crossover operator (to generate promising offspring solutions), a 2-phase tabu search procedure (to seek high-quality local optima) as well as a quality-and-distance based pool updating strategy (to maintain a healthy population). The computational results on 73 benchmark instances demonstrate that the proposed algorithm competes favorably with the state-of-the-art algorithms in the literature. Specifically, our algorithm attains the optimal results for all 41 instances with known optima and discovers improved upper bounds for 9 out of the 32 instances whose optimal solutions are still unknown. We investigate the benefits of the 2-phase tabu search procedure and the crossover operator with the memetic framework. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105000	10.1016/j.knosys.2019.105000													
J								Performance-driven adaptive differential evolution with neighborhood topology for numerical optimization	KNOWLEDGE-BASED SYSTEMS										Differential evolution; Mutation strategy; Parameter setting; Neighborhood topology; Numerical optimization	PARTICLE SWARM OPTIMIZER; DIRECTION INFORMATION; GLOBAL OPTIMIZATION; ALGORITHM; MUTATION; PARAMETERS; ENSEMBLE; METHODOLOGY; MECHANISM; DESIGN	This paper presents a novel differential evolution algorithm for numerical optimization by making full use of the neighborhood information to balance exploration and exploitation. To effectively meet the search requirement of each individual, a neighborhood-based adaptive mutation strategy is developed by using the ring topology to construct an elite individual set and adaptively choosing a suitable elite individual to guide its search according to its neighborhood performance. Then, a neighborhood-based adaptive parameter setting is designed to improve the suitability of parameters for each individual by utilizing the feedback information of population and its neighbors simultaneously. Furthermore, a restart mechanism is proposed to further enhance the performance of algorithm by adaptively strengthening the search abilities of unpromising individuals, removing the worse individuals and randomly replacing some individuals with Gaussian Walks. Differing from the existing DE variants, the proposed algorithm adaptively guides the search and suitably adjusts the parameters for each individual by using its neighborhood performance, and strengthens the exploitation and exploration by removing the worse individuals and randomly replacing some individuals. Then it could properly adjust the search ability of each individual, and effectively balance diversity and convergence. Compared with 16 typical algorithms, the numerical results on 30 IEEE CEC2014 benchmark functions show that the proposed algorithm has better performance. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105008	10.1016/j.knosys.2019.105008													
J								Ensemble extreme learning machines for compound-fault diagnosis of rotating machinery	KNOWLEDGE-BASED SYSTEMS										Extreme learning machines; Compound-fault diagnosis; Rotating machinery; Signal processing; Feature extraction	EMPIRICAL MODE DECOMPOSITION; PERMUTATION ENTROPY; NEURAL-NETWORKS; OPTIMIZATION	The data-driven fault indicator for rotating machinery is designed to reveal the possible fault scenarios from the observed statistical vibration signals. This study develops a novel ensemble extreme learning machine (EELM) network to replace the conventional layout by combining binary classifiers (e.g., binary relevance) for compound-fault diagnosis of rotating machinery. The proposed EELMs consist of two sub-networks, namely, the first extreme learning machine (ELM) for clustering, and the second for multi-label classification. The first network generates the Euclidean distance representations from each point to every centroid with unsupervised clustering, and the second identifies potential output tags through multiple-output-node multi-label learning. Compared to the existing multi-label classifiers (e.g., multi-label radial basis function, rank support vector machine, back-propagation multi-label learning, and binary classifiers with binary relevance), the theoretical verification reveals EELMs perform the best in hamming loss, one-error, training time, and achieves the best overall evaluation for the two real-world databases (e.g., Yeast and Image). Regarding the real test for the compound-fault diagnosis of rotating machinery, this paper applies the particle swarm optimization-based variational mode decomposition to decompose the raw vibration signals into a series of intrinsic modes, and selects ten time-domain indicators and five frequency-domain statistical characteristics for feature extraction. The experimental results illustrate that the EELM-based fault diagnosis method achieves the best overall performance. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105012	10.1016/j.knosys.2019.105012													
J								Allocation of fresh water recourses in China with nested probabilistic-numerical linguistic information in multi-objective optimization	KNOWLEDGE-BASED SYSTEMS										Fresh water resources; Conventional water resources; Unconventional water resources; Nested probabilistic-numerical linguistic term information; Multi-objective optimization model	GROUP DECISION-MAKING; TERM SETS; RESOURCES; MODEL; EARTHQUAKE; MANAGEMENT	Fresh water resources are made of conventional water resources (CWRs) and unconventional water resources (UWRs), and they are one of the important natural resources that cannot be replaced. The purpose of this paper is to predict and allocate China's fresh water resources in 2025 under complex environment. According to historical data, we first conduct data reprocessing including data collection, data prediction and discussion. In order to achieve an appropriate tradeoff between ecological environment and economic development, a multi-objective optimization model is constructed based on market mechanism. Specifically, we establish two objective functions: one is to minimize the total cost, the other is to minimize the whole amount of CWRs, and then, we optimize the parameters in the model based on nested probabilistic-numerical linguistic information. After that, the solution and the strategy of fresh water resources are obtained, and the sustainable development and risk response by adjusting and adding the parameters are further analyzed. The results show that the model is effective, feasible and applicable. Finally, we make some discussions about the strengths and weakness of the model, and the suggestions for the fresh water resources in China. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105014	10.1016/j.knosys.2019.105014													
J								Dynamic opposite learning enhanced teaching-learning-based optimization	KNOWLEDGE-BASED SYSTEMS										Teaching-learning-based optimization; Dynamic-opposite learning; Opposition-based learning; Global optimization	DIFFERENTIAL EVOLUTION; PARTICLE SWARM; ALGORITHM; DISPATCH; QUALITY	The teaching-learning-based optimization (TLBO) algorithm has been one of most popular bio-inspired meta-heuristic algorithms due to the competitive converging speed and high accuracy. A batch of TLBO variants has been proposed to enhance the exploitation ability and accelerate the exploration process. However, they still suffer from premature convergence in solving complex non-linear problems. In the study, a novel TLBO variant named dynamic-opposite learning TLBO (DOLTLBO) is proposed, which employs a new dynamic-opposite learning (DOL) strategy to overcome premature convergence. The search space of DOL has the characteristics of asymmetry and dynamically adjusting along with a random opposite number. The asymmetric search space significantly increase the probability for the population in obtaining the global optimum, which holistically improves the exploitation capability of DOLTLBO. Meanwhile, the dynamically changing characteristic enriches the diversity of the search space, thus enhancing the exploration ability. To validate the proposed DOL operator and DOLTLBO algorithm, shifted and rotated benchmark functions from CEC 2014, multiextremal functions and constrained engineering problems have been experimented upon. Comprehensive numerical results with the comparisons with the state-of-the-art counterparts show that DOLTLBO has significant advantages of converging to the global optimum on most benchmarks and engineering problems, which also validates the superiority of the novel DOL operator. (C) 2019 The Authors. Published by Elsevier B.V.																	0950-7051	1872-7409				JAN 5	2020	188								104966	10.1016/j.knosys.2019.104966													
J								Improving tree-based neural machine translation with dynamic lexicalized dependency encoding	KNOWLEDGE-BASED SYSTEMS										Syntactic modeling; Dynamic parameters; Tree-RNN; Neural machine translation (NMT)		Tree-to-sequence neural machine translation models have proven to be effective in learning the semantic representations from the exploited syntactic structure. Despite their success, tree-to-sequence models have two major issues: (1) the embeddings of constituents at the higher tree levels tend to contribute less in translation; and (2) using a single set of model parameters is difficult to fully capture the syntactic and semantic richness of linguistic phrases. To address the first problem, we proposed a lexicalized dependency model, in which the source-side lexical representations are learned in a head-dependent fashion following a dependency graph. Since the number of dependents is variable, we proposed a variant recurrent neural network (RNN) to jointly consider the long-distance dependencies and the sequential information of words. Concerning the second problem, we adopt a latent vector to dynamically condition the parameters for the composition of each node representation. Experimental results reveal that the proposed model significantly outperforms the recently proposed tree-based methods in English-Chinese and English-German translation tasks with even far fewer parameters. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105042	10.1016/j.knosys.2019.105042													
J								Robust Neighborhood Covering Reduction with Determinantal Point Process sampling	KNOWLEDGE-BASED SYSTEMS										Neighborhood covering reduction; Determinantal point process	SYSTEMS	Neighborhood Covering Reduction (NCR) methods are too rigid to filter out redundant neighborhoods and sensitive to noise. To tackle this problem, we propose a flexible neighborhood reduction method based on Determinantal Point Process (DPP) sampling in this paper. Sampling from a DPP, the probabilities of subset selection are computed from the correlation matrix of items and the subsets of diverse items will be assigned high probabilities. We model the process of neighborhood selection as a DPP and thereby implement a NCR algorithm with DPP neighborhood sampling (NCRDPP). NCRDPP selects significant neighborhoods of both high quality and diversity to form concise coverings as the approximations of data distributions, which facilitate the model generalization and robustness of neighborhood-based learning. Experimental results verify the superiority of DPP sampling for neighborhood selection and the robustness of NCRDPP method for noisy data classification. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105063	10.1016/j.knosys.2019.105063													
J								Promoting active learning with mixtures of Gaussian processes	KNOWLEDGE-BASED SYSTEMS										Active learning; Mixtures of Gaussian processes	STATISTICAL COMPARISONS; CLASSIFIERS	Active learning is an effective methodology to relieve the tedious and expensive work of manual annotation for many supervised learning applications. The active learning framework with good performance usually contains powerful learning models and delicate active learning strategies. Gaussian process (GP)-based active learning was proposed to be one of the most effective methods. However, the single GP suffers from the limitation of not modeling multimodal data well enough, and thus existing active learning strategies based on GPs only make use of limited information from data. In this paper, we propose three novel active learning methods, in which the existing mixture of GP model (MGP) is adjusted as the learning model and three active learning strategies are designed based on the adjusted MGP. Through experiments on multiple data sets, we analyze the performance and characteristics of the three proposed active learning methods, and further compare with popular GP-based methods and some other state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JAN 5	2020	188								105044	10.1016/j.knosys.2019.105044													
J								Evolutionary multi-objective automatic clustering enhanced with quality metrics and ensemble strategy	KNOWLEDGE-BASED SYSTEMS										Clustering; Evolutionary multi-objective optimization; Cluster validity index; Ensemble method	GENETIC ALGORITHM; INDEX	Automatic clustering problem, which needs to detect the appropriate clustering without a pre-defined number of clusters (k), is difficult and challenging in unsupervised learning owing to the lack of prior domain knowledge. Despite a rising tendency with the application of evolutionary multi-objective optimization (EMO) techniques for automatic clustering, there still exist some obvious under-explored issues. In this paper, we resort to quality metrics and ensemble strategy for the sake of explicit/implicit knowledge discovery to guide the optimization process. The quality and diversity of solutions defined in terms of cluster validities, as similar to performance indicator for multi-objective optimization, are applied to assist in addressing automatic clustering problems and decreasing unnecessary computational overhead. To be specific, the main components like initialization, reproduction operations, and environmental selection which involved during EMO based automatic clustering are discussed and refined. For the determination of the final partitioning, quality metrics and cluster ensemble strategy are both considered to improve the retrieve system in the unsupervised way. Experiments are conducted from several different aspects and the corresponding analyses are provided, which confirm that the proposals are more efficient and effective for automatic clustering. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				JAN 5	2020	188								105018	10.1016/j.knosys.2019.105018													
J								Ensemble cluster pruning via convex-concave programming	COMPUTATIONAL INTELLIGENCE										clustering; DCCP; ensemble learning; ensemble pruning; k-means	WEIGHTED ENSEMBLE; SELECTION; DIVERSITY; QUALITY	Ensemble learning is the process of aggregating the decisions of different learners/models. Fundamentally, the performance of the ensemble relies on the degree of accuracy in individual learner predictions and the degree of diversity among the learners. The trade-off between accuracy and diversity within the ensemble needs to be optimized to provide the best grouping of learners as it relates to their performance. In this optimization theory article, we propose a novel ensemble selection algorithm which, focusing specifically on clustering problems, selects the optimal subset of the ensemble that has both accurate and diverse models. Those ensemble selection algorithms work for a given number of the best learners within the subset prior to their selection. The cardinality of a subset of the ensemble changes the prediction accuracy. The proposed algorithm in this study determines both the number of best learners and also the best ones. We compared our prediction results to recent ensemble clustering selection algorithms by the number of cardinalities and best predictions, finding better and approximated results to the optimum solutions.																	0824-7935	1467-8640				FEB	2020	36	1					297	319		10.1111/coin.12267		JAN 2020											
J								Estimation of gasoline properties by H-1 NMR spectroscopy with repeated double cross-validated partial least squares models	JOURNAL OF CHEMOMETRICS										chemometrics; gasoline; H-1 NMR spectroscopy; quality control; partial least squares; repeated double cross validation	NUCLEAR-MAGNETIC-RESONANCE; CRUDE-OIL	Commercial gasoline must satisfy several product specifications before trading. In the present work, repeated double cross validation using partial least squares regression was applied to create reliable prediction models for 13 physicochemical parameters (eg, density, vapour pressure, evaporate at 70 degrees C, evaporate at 100 degrees C, evaporate at 150 degrees C, final boiling point, research octane number, motor octane number, aromatic content, olefinic content, benzene content, oxygen content, and methyl tert-butyl ether content) of gasoline produced in Matosinhos' refinery. The input variables for the regression are the H-1 NMR spectral intensities of a total of 448 samples, which were recorded using a picoSpin NMR spectrometer operating at 80 MHz. The output variables are the corresponding property values, which were also measured according to ISO standard methods. A spectral feature elimination before multivariate analysis was done to remove noise and speed up the chemometric analysis. The optimum complexity of each model was achieved by repeated double cross-validation strategy, consisting of 100 repetitions of two nested cross-validation loops. Quantitative partial least squares yielded accurate predictions of 11 of 13 properties within the reproducibility of ISO standards. The methodology presented in this work has been proven effective in property estimation and enables a significant reduction in the total time of gasoline quality control.																	0886-9383	1099-128X				MAY	2020	34	5							e3212	10.1002/cem.3212		JAN 2020											
J								COMBIMA: truthful, budget maintaining, dynamic combinatorial market	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Strategic agents; Electronic commerce; Combinatorial exchanges	VICKREY AUCTIONS	Current interest in two-sided markets is motivated by examples of successful practical applications of market mechanisms in supply chain markets, online advertising exchanges, and pollution-rights markets. Many of these examples require markets where agents arrive dynamically and can trade multiple commodities. However, the known literature largely focuses on settings with single-commodity unit demand. We present, prove and evaluate a general solution that matches agents in a dynamic, two-sided combinatorial market. Multiple commodities, each with multiple units, are bought and sold in different bundles by agents that arrive over time. Our mechanism, COMBIMA, provides the first dynamic two-sided combinatorial market that allows truthful and individually-rational behavior for all agents, keeps the market budget balanced and approximates social welfare efficiency. We experimentally examine and compare the allocative efficiency of COMBIMA with respect to our proven theoretical bounds and with respect to all known (dynamic and non-dynamic) social-welfare maximizing two-sided markets under variety of distributions of bids, market demands and market size. COMBIMA performs well by all benchmarks and in many cases improves on previous mechanisms.																	1387-2532	1573-7454				JAN 4	2020	34	1							14	10.1007/s10458-019-09437-7													
J								A cognitive mechanism for mitigating DDoS attacks using the artificial immune system in a cloud environment	EVOLUTIONARY INTELLIGENCE										Cloud computing; Distributed Denial of service attack (DDoS); Artificial immune system; Anomalous behaviour	PRACTICAL APPROACH; DEFENSE	Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks can largely damage the availability of the cloud services and can be effectively initiated by utilizing different tools, prompting financial harm or influencing the reputation. Consequently, there is a requirement for a more grounded and general approach to block these attacks. This paper proposes the use of artificial immune systems to alleviate DDoS attacks in cloud computing by identifying the most potential features of the attack. This methodology is capable of detecting threats and responding according to the behavior of the biological resistance mechanism in human beings. It is carried out by emulating the various immune reactions and the construction of the intrusion detection system. For the assessment, experiments with public domain datasets (KDD cup 99) were implemented. Based on broad theoretical and performance analysis, the proposed system is capable to identify the anomalous entries with high detection accuracy and low false alarm rate.																	1864-5909	1864-5917															10.1007/s12065-019-00340-4		JAN 2020											
J								Manipulating an election in social networks through link addition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											LONG-RUN; EQUILIBRIA; EVOLUTION	We investigate the effects of the social influence in determining the behavior of agents in a social network in the context of an election. In particular, we concentrate our attention on how the structure of a social network can be manipulated in order to determine the outcome of an election. We consider an election with m candidates and n voters, each one with her own ranking on the candidates. Voters are part of a social network and the information that each voter has about the election is limited to what her friends are voting. We consider an iterative elective process where, at each round, each voter decides her vote strategically, based on what her neighbors voted in the previous round and her ranking. Thus, a voter may decide to vote for a candidate different from her favorite to avoid the election of a candidate she dislikes. Following Sina et al. (Adapting the social network to affect elections. In: Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp 705-713, 2015) we investigate how a central organization that knows rankings of all the voters and can manipulate the structure of the social network can determine the outcome of the election by creating new connections among voters. Our main result is an algorithm that, under mild conditions on the social network topology and on the voters' rankings, is able to compute a limited number of links to be added to the social network in order to make our designed candidate the winner of the election. Our results can be seen as another indication that who controls social media can have a great influence on our lives by strategically determining what information we are exposed to.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4073	4088		10.1007/s12652-019-01669-5		JAN 2020											
J								GroupShop: monitoring group shopping behavior in real world using mobile devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Group shopping; Potential buyer; Group influence; Mobile sensing		In brick-and-mortar stores, customers often shop in small groups, which may be more common compared with shopping alone. Retailers in these physical stores, however, do not have effective methods to recognize who are potential buyers and how group members influence each other in purchase choices. This work aims to use mobile devices to help characterize these behaviors. Specifically, by utilizing the motion and acoustic sensors on mobile devices, we propose three features to describe different stages of group shopping and then identify potential buyers. Further, the influence behavior among group members is characterized based on the movements and interactions of customers. We have conducted real-life experiments with the recruited volunteers shopping in three environments. And the experimental results validate the effectiveness of our approaches, showing a high accuracy in group shopping behavior recognition.																	1868-5137	1868-5145															10.1007/s12652-019-01673-9		JAN 2020											
J								Computational intelligence based control of cascaded H-bridge multilevel inverter for shunt active power filter application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Active and reactive power method (PQ); Alternate phase opposition disposition (APOD); Cascade H-bridge (CHB); Phase disposition (PD); Power factor (PF); Phase opposition disposition (POD); Reactive power compensation (RPC); Total harmonic distortion (THD)		This work proposes computational intelligence based control of five-level cascaded H-bridge inverter (5LCHB) for an application of shunt active power filter (SAPF) to reduce total harmonic distortion (THD) in supply current under nonlinear loads. Active and reactive power (PQ) method is used to extract reference currents. The proposed controller is effectively verified for three different carrier based pulse width modulation methods such as phase disposition (PD), phase opposition disposition (POD) and alternate phase opposition disposition (APOD). The required rules for fuzzy logic controller (FLC) are set by human brain computation. The performance of computational intelligence based FLC is implemented and is verified for reduction of THD under nonlinear loads. The DC voltage balance is achieved using the proposed controller. The proposed work is carried out in the environment of Matlab Simulink. The comparative performance analysis of THD in source current, individual harmonics (5th, 7th, 11th, and 13th), power factor (PF), real power, and reactive power compensation (RPC) for APOD, POD and PD using both PI and FLC are presented.																	1868-5137	1868-5145															10.1007/s12652-019-01660-0		JAN 2020											
J								Intelligent computing hardware for collision avoidance and warning in high speed rail networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Agents; IoT; Server; Warning system; Tracking; Collision avoidance; Database	OPTIMIZATION; SYSTEM; INTERNET; MODEL; IOT	One of the very important issue is safety of transport vehicle, particularly in increasing number of trains on the same tracks and collision avoidance warning system. We have successfully developed Linux based hardware, modules and software code for IoT enabled Train Tracking and Collision avoidance warning system. A strong tracking of the vehicle an entire system and their updates of data in database and analysis of the database from multi-angles may reduce the occurrence of an accident even in the critical failure situation. In view of this we have made two types of train tracking system using Linux based robust hardware, which simultaneously monitors the movement of train by two different mechanism and the data collected are instantaneously stored in cloud server. Based on the live data analysis, the signalling of trains to different tracks or halt can be estimated. The estimated computational results will be highly useful in providing collision avoidance alerts. Also, we have developed user interactive GUI screen for onboard train module as well as on various analytics utilizing server module data. This paper strongly recommends a secure & safety process purely depend on tracking of any device simultaneously by two different mechanism. The system developed in the present work is cost effective, hence this system can be added as an additional safety layer to the existing safety systems in the train.																	1868-5137	1868-5145															10.1007/s12652-019-01661-z		JAN 2020											
J								High definition wireless multimedia transmission model based on bit-stream control and differential fault tolerance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Stereoscopic; High definition; Multimedia transmission; Bit-stream control; Differential fault tolerance; Video distortion; Channel bandwidth; Cross-layer optimization	COMMUNICATION; PROTOCOL; AWARE; QOS	Compared with the wired channel, wireless channel transmission has more challenges, such as channel decay, noise and interference and other factors, resulting in stereoscopic video transmission in the wireless channel has higher latency, lower bandwidth and higher error rate and so on. This paper proposes a cross-layer code-rate adaptation and cross-layer error control transmission optimization method. The cross-layer optimization method tunes the coding rate at the application layer, and modulation and channel coding scheme at the physical layer by minimizing the end-to-end stereoscopic video distortion. Thus, the proposed method can reach the goal of adapting source rate to the channel bandwidth as well as leveraging the error control strengths between the application layer and physical layer. The experimental results show that compared with the cross-layer optimization and non-cross-layer optimization methods, our proposed optimization method can improve the peak signal-to-noise ratio (PSNR) of the virtual view-point video.																	1868-5137	1868-5145															10.1007/s12652-019-01620-8		JAN 2020											
J								A streaming architecture for Convolutional Neural Networks based on layer operations chaining	JOURNAL OF REAL-TIME IMAGE PROCESSING										Convolutional Neural Networks; Streaming architecture; Layer operation chaining	ACCELERATOR	Convolutional Neural Networks (CNN) have become one of the best algorithms in machine learning for content classification of digital images. The CNN computational complexity is much larger than traditional algorithms, that is why the use of Graphical Processor Units (GPU) and online servers to achieve operations acceleration is a common solution. However, there is a growing demand for real-time processing solutions in the object recognition field mainly implemented on embedded systems, which are limited both in resources and energy consumption. Recently, reported works are focused on minimizing the required resources through two design strategies. The first one is by implementing one accelerator that can be adapted to the operations of the whole CNN. The CNN architecture proposals with one accelerator for each convolution layer belong to the second design strategy, where higher performance is achieved in multiple image processing. A new design strategy is proposed in this paper, which is based on multiple accelerators using a layer operation chaining scheme for computing in parallel the operations corresponding to multiple CNN layers. Three types of parallel data processing are adopted in the proposed architecture, where the parallelism level for convolution layers is determined by defined cost-function-based algorithms. The proposed design strategy is shown by implementing three naive CNNs on a De2i-150 board, in which a peak acceleration of 18.04x was achieved in contrast with state-of-the-art design methods without layer operation chaining. Furthermore, the design results of one modified Alexnet CNN were obtained. According to the obtained results, the proposed design strategy allows to achieve a smaller processing time than that obtained by reported works using the other two design strategies. In addition, a competitive result in resources utilization is obtained for naive CNNs.																	1861-8200	1861-8219				OCT	2020	17	5					1715	1733		10.1007/s11554-019-00938-y		JAN 2020											
J								Exploring high-order adder compressors for power reduction in sum of absolute differences architectures for real-time UHD video encoding	JOURNAL OF REAL-TIME IMAGE PROCESSING										Video coding; Hardware architecture; HEVC; SAD; Low power; Adder compressors	MOTION ESTIMATION ALGORITHM; LOW-COST; DESIGN; HEVC; SAD; OPTIMIZATION	The sum of absolute difference (SAD) calculation is one of the most computing-intensive operations in video encoders compatible with recent standards, such as high-efficiency video coding (HEVC). SAD hardware architectures employ an adder tree to accumulate the coefficients from the absolute difference between two video blocks. This paper employs high-order adder compressors (HOAC) structures into SAD hardware architectures to achieve ultra-high definition (UHD) encoding in real time, using block sizes compatible with HEVC. The proposed HOAC architectures are power-efficient and enable low-power SAD hardware accelerators. Our throughput analysis shows that the HOAC-based SAD hardware architecture is capable of encoding UHD 4K (3840 x 2160) videos in real-time at 60 frames per second. The architectures were entirely designed as dedicated ASIC blocks and were synthesized to ST 65 nm CMOS standard cells. Synthesis results show that SAD architectures using 64-2, 32-2, 16-2 and 8-2 compressors built from 4-2 compressors are significantly more efficient in terms of circuit area and total power dissipation when compared with SAD architectures using conventional adders selected by a commercial logic synthesis tool.																	1861-8200	1861-8219				OCT	2020	17	5					1735	1754		10.1007/s11554-019-00939-x		JAN 2020											
J								Multi-objective cluster analysis using a gradient evolution algorithm	SOFT COMPUTING										Cluster analysis; Multi-objective problem; Metaheuristics; Gradient evolution	GENETIC ALGORITHM; OPTIMIZATION ALGORITHM; HYBRID	Data analysis becomes more important since rapid technology developments. In data analysis, data clustering is one of the very useful approaches. It can reveal important information hiding inside the dataset by organizing the instances based on their similarity. The objectives of data clustering are maximizing dissimilarity between clusters and minimizing dissimilarity within clusters. In order to construct a good clustering results, many clustering algorithms have been proposed, including the metaheuristic-based clustering algorithms. Recently, a new metaheuristic algorithm named gradient evolution has been proposed. This algorithm shows a good performance on solving the optimization problems. Therefore, this paper employs this GE algorithm for solving the clustering problem. In order to obtain a better clustering result, this paper considers multi-objective clustering instead of single-objective clustering. In this paper, the original GE algorithm is improved so then it is suitable for the multi-objective problem. The proposed modification includes the procedure for vector updating and jumping which involves Pareto rank assignment. In addition, it also employs K-means algorithm to provide the final clustering result. The proposed algorithm is verified using some benchmark datasets. It is also compared with some other multi-objective metaheuristic-based clustering algorithms. The experimental results show that the proposed algorithm can obtain better results than other metaheuristic-based algorithms.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11545	11559		10.1007/s00500-019-04620-0		JAN 2020											
J								Soft computing approaches for character credential and word prophecy analysis with stone encryptions	SOFT COMPUTING										Image segmentation; Feature extraction; SVM; CNN; Character recognition	RECOGNITION	The uniform corpus of untranslated script is a preliminary stage for computational epigraphy. Mechanizing this process through deep learning algorithms will be an essential support to the epigraphical research. Our proposed system based on soft computing techniques focuses on the progression of recognizing the eleventh-century ancient Tamil character and converting them into current-century word form. Initially, the system is implemented by performing preprocessing steps followed by image segmentation. The decomposed image undergoes a hybrid feature extraction technique along with Chi-square test to check whether entire pixel in image of Zernike is bounded inside the unit circle or not, whereas ANOVA method is used for testing the significant difference between HOG feature and zoning feature. These functions are subjected to image classification and proceeded with character recognition using convolutional neural networks. Finally, the identified character is progressed into word form with the help of boggle algorithm. The hybrid feature extraction along with convolutional neural networks is achieved with 92.78% of recognition rate accurately. Our experiment shows a large perspective of deep learning algorithms in automatic epigraphy.																	1432-7643	1433-7479				AUG	2020	24	16					12013	12026		10.1007/s00500-019-04643-7		JAN 2020											
J								Classification of bearing vibration speeds under 1D-LBP based on eight local directional filters	SOFT COMPUTING										F-1D-LBP; Feature extraction; Bearing fault diagnosis; Machine learning	FEATURE-EXTRACTION; FAULT-DIAGNOSIS; BINARY PATTERNS; WAVELET TRANSFORM; EPILEPTIC EEG; IDENTIFICATION; RECOGNITION; MOTORS	Bearings are the most commonly used machine element in order to reduce rotational friction in machines and to compensate radial and axial loads. It is very important to determine the faults in the bearings in terms of the machine health. In order to accurately diagnose bearing-related faults with traditional machine learning methods, it is necessary to identify the features that characterize bearing fault most accurately. Therefore, a new feature extraction procedure has been proposed to determine the vibration signal velocities of different fault sizes and types in this study. The new approach has been employed to obtain features from the vibration signals for different scenarios. After different filtering based on 1D-LBP method, the F-1D-LBP method was used to construct feature vectors. The filters reduce the noise in the signals and provide different feature groups. In other words, it is aimed to generate filters in order to extract different patterns that can separate signals. For each filter applied, different patterns can be obtained for the same local point on signals. Thus, the signals can be represented by different feature vectors. Then, by using these feature groups with various machine learning methods, vibration velocities were separated from each other. As a result, it was observed that the obtained feature had promising results for classification of bearing vibrations.																	1432-7643	1433-7479				AUG	2020	24	16					12175	12186		10.1007/s00500-019-04656-2		JAN 2020											
J								Multidimensional agro-economic model with soft-IoT framework	SOFT COMPUTING										Agricultural economic modeling; Internet of things (IoT); Big data; Soft computing	BIG DATA; INTERNET; CLOUD	In recent times, there has been a great motivation toward research in the field of big data analysis and their incorporation into Internet of Things (IoT). The basic idea behind IoT is to ensure provision of services to clients all over the globe at any point of time from a pool of resources. With increasing volumes of data being handled, processed and stored in recent times, an efficient processing mechanism for these huge volumes of data is available in the form of big data handlers which ensure speedy provision of services without any delay overhead. Hence, big data and IoT put together prove to be the most effective tool and need of the hour for smooth handling and provision of services demanded by clients thus improving the overall quality of service. These two concepts have been effectively applied to developing an intelligent agricultural economic model which is quite heterogeneous in nature thus posing to be a great research challenge. Agro-economic models are quite essential and critical as agriculture forms the backbone of many developing nations across the globe. ANFIS model introduces the necessary intelligence for the big data analytic system to handle the heterogeneous nature of agro-economic input data and provide a suitable prediction. A sample data of five hundred details from five different subsets have been used in the experimental model and prediction of yield computed and compared against recent techniques.																	1432-7643	1433-7479				AUG	2020	24	16					12187	12196		10.1007/s00500-019-04657-1		JAN 2020											
J								Towards the use of genetic programming in the ecological modelling of mosquito population dynamics	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Ecological modelling; Genetic programming; Machine learning; Regression	WEST NILE-VIRUS; ABUNDANCE	Predictive algorithms are powerful tools to support infection surveillance plans based on the monitoring of vector abundance. In this article, we explore the use of genetic programming (GP) to build a predictive model of mosquito abundance based on environmental and climatic variables. We claim, in fact, that the heterogeneity and complexity of this kind of dataset demands algorithms capable of discovering complex relationships among variables. For this reason, we benchmarked GP performance with state of the art machine learning predictive algorithms. In order to provide a real exploitable model of mosquito abundance, we trained GP and the other algorithms on mosquito collections from 2002 to 2005 and we tested the predictive ability in 2006 collections. Results reveal that, among the studied methods, GP has the best performance in terms of accuracy and generalization ability. Moreover, the intrinsic feature selection and readability of the solution provided by GP offer the possibility of a biological interpretation of the model which highlights known or new behaviours responsible for mosquito abundance. GP, therefore, reveals to be a promising tool in the field of ecological modelling, opening the way to the use of a vector based GP approach (VE-GP) which may be more appropriate and beneficial for the problems in analysis.																	1389-2576	1573-7632				DEC	2020	21	4					629	642		10.1007/s10710-019-09374-0		JAN 2020											
J								Performance exploration of on-body WBAN using CM3A-IEEE 802.15.6 channel model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cooperative communication; IEEE 802; 15; 6; Path loss; Shadowing; WBAN; WSN		Wireless Body Area Network (WBAN) is an upcoming technology for comprehending effective healthcare. In order to enhance the enactment of the WBAN system, performance evaluation under diverse working scenarios is important. This paper proposes a novel approach to explore the performance of a WBAN operating at the IEEE 802.15.6 standard. The investigation has been performed for non-cooperative and cooperative scenarios. IEEE 802.15.6 CM3A path loss modelling has been implemented to formulate and further evaluate link quality metrics like: Signal to Noise Ratio (SNR) and Bit Error Rate (BER). Energy consumption and throughput have been analyzed. Critical comparative analysis has been performed for three IEEE 802.15.6 narrow bands: W-ISM (Worldwide Industrial, Scientific and Medical band)-(2.4 GHz), U-ISM (US-Industrial, Scientific and Medical band)-(902-928MHz) and WMTS (Wireless Mobile Telemetry Services band)-(420-450 MHz). From the scrutiny of simulation results, it is observed that comparative energy consumptionis lowest in WMTS and U-ISM band wrt distance and antenna gain respectively in non-cooperative as well as cooperative scenarios. Link quality evaluation reveals that the U-ISM band exhibits better performance as it offers highest SNR and lowest BER for the considered scenarios. W-ISM exhibits the highest throughput compared to U-ISM and WMTS. Cooperative communication offers better link quality and throughput compared to non-cooperative scenario. The evaluation results aid a WBAN implementer to choose a preferable and permissible frequency band as per design requirements.																	1868-5137	1868-5145															10.1007/s12652-019-01663-x		JAN 2020											
J								Natural Density and the Quantifier "Most"	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Logic of natural languages; Natural density; Asymptotic density; Arithmetic progression; Syllogistic; Most; Semantics; Quantifiers; Cardinality	LOGICS	This paper proposes a formalization of the class of sentences quantified by most, which is also interpreted as proportion of or majority of depending on the domain of discourse. We consider sentences of the form "Most A are B", where A and B are plural nouns and the interpretations of A and B are infinite subsets of N. There are two widely used semantics for MostAareB: (i) C(A boolean AND B) > C(A\ B) and (ii) C(A boolean AND B) > C(A)/2, where C(X) denotes the cardinality of a given finite set X. Although (i) is more descriptive than (ii), it also produces a considerable amount of insensitivity for certain sets. Since the quantifier most has a solid cardinal behaviour under the interpretation majority and has a slightly more statistical behaviour under the interpretation proportional of, we consider an alternative approach in deciding quantity-related statements regarding infinite sets. For this we introduce a new semantics using natural density for sentences in which interpretations of their nouns are infinite subsets of N, along with a list of the axiomatization of the concept of natural density. In other words, we take the standard definition of the semantics of most but define it as applying to finite approximations of infinite sets computed to the limit.																	0925-8531	1572-9583				DEC	2020	29	4					511	523		10.1007/s10849-019-09312-4		JAN 2020											
J								Multi-objective bacterial colony optimization algorithm for integrated container terminal scheduling problem	NATURAL COMPUTING										Bacterial colony optimization algorithm; Container terminals; Integrated scheduling optimization; Multi-objective optimization	BERTH-ALLOCATION; QUAY CRANE; EVOLUTIONARY ALGORITHMS	This paper proposes a multi-objective integrated container terminal scheduling problem considering three key components: berth allocation, quay cranes assignment and containers transportation in port operation process. In the suggested problem, one of the objectives is to shorten service time of ships with by coordinating of quay cranes, and the other is to reduce operating costs of quay cranes and yard trucks. Then, a Multi-objective Bacterial Colony Optimization algorithm (MOBCO) incorporating concepts of multi-swarm, topology, personal best and global best, named Multi-objective BCO with ring topology (MORBCO), is designed to handle the resulting problem. The extension of standard MOBCO to the MORBCO involves the addition of three specialized strategies: global chemotaxis operation, elite reproduction strategy and personal best archive with neighborhood communication mechanism. In order to test the performance of the MORBCO, benchmark tests are performed and compared with traditional MOBCO and three other well-known multi-objective algorithms first. The computational results indicate that the proposed algorithm can outperform other rivals and efficiently solve a variety of multi-objective problems in most of cases. Subsequently, MORBCO and two best performing algorithms from the previous test are applied to three instances generated by the proposed model. Judging by quality and diversity of obtained non-dominant solutions, we find that MORBCO has superior performance, especially for large instances of the container terminal problem.																	1567-7818	1572-9796															10.1007/s11047-019-09781-3		JAN 2020											
J								Study on the influence of different color temperature and illumination environment on cognitive processing depth	EVOLUTIONARY INTELLIGENCE										Color temperature; Illumination; Cognitive processing; Signal detection task; Visual search task		Indoor lighting environment has an impact on people's physiology, psychology, cognition, and other aspects, thus affecting people's work tasks and work efficiency. This paper studies the effects of different color temperature and illumination conditions on cognitive processing depth. The experimental results show that different screen brightness has significant influence on information processing ability under three light environments: dark, white and yellow. The performance of tasks with different processing depth is different under different light environmental conditions. High brightness display and color matching style of night vision interface is recommended in dark environment. High brightness display is recommended in yellow light environment; In white light, medium brightness display and white interface matching style are recommended.																	1864-5909	1864-5917															10.1007/s12065-019-00338-y		JAN 2020											
J								A normal wiggly hesitant fuzzy linguistic projection-based multiattributive border approximation area comparison method	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										linguistic scale functions; MABAC method; multiattribute decision-making; normal wiggly hesitant fuzzy linguistic term set; projection measure	MULTICRITERIA DECISION-MAKING; AGGREGATION OPERATORS; TERM SETS; MODEL	As a useful information representation tool, hesitant fuzzy linguistic term set (HFLTS) allows decision makers (DMs) to express their cognitive preferences in terms of several ordered and continuous linguistic terms. Considering the fact that much valuable information related to the cognitive behavior of DMs is hidden in the original evaluation information, this paper studies how to comprehensively mine uncertain information from original hesitant fuzzy linguistic evaluation information given by DMs. To address this objective, we present a new representation tool, normal wiggly hesitant fuzzy linguistic term set (NWHFLTS), which not only retains the original evaluation information, but also delivers and quantifies potential uncertain information, and can also help DMs express their evaluation information in a more complete manner. First, we develop the basic operations, score function, and comparison rule of NWHFLTS based on linguistic scale functions (LSFs), and propose the projection measure, the normal projection measure, and the normalized projection-based distance measure to describe the degree of deviation between two NWHFLTSs. Furthermore, for the case when the attribute weight is completely unknown, we combine the multiattributive border approximation area comparison (MABAC) method and develop a new method called as normal wiggly hesitant fuzzy linguistic projection-based MABAC to solve the multiattribute decision-making problems where attribute values are expressed in the form of NWHFLTS. Finally, through a practical example of marine ecological security situation, the specific calculation steps of this method are exemplified, the feasibility and advancement of the proposed method are demonstrated via a comprehensive comparative study.																	0884-8173	1098-111X				MAR	2020	35	3					432	469		10.1002/int.22213		JAN 2020											
J								Enhanced crow search algorithm for AVR optimization	SOFT COMPUTING										Crow search algorithm (CSA); Enhanced crow search algorithm (ECSA); Proportional-integral-derivative (PID) controller; Automatic voltage regulator (AVR)	PID CONTROLLER; DESIGN	This paper proposes an enhanced crow search algorithm (ECSA) for solving numerical and real-life engineering problems. Novelties of the proposed method are fourfold: (1) addition of an archive component in the standard crow search algorithm (CSA) to incorporate past experience of finding solution (2) formulation of non-hideout position so that crow will remain near its hideout position, (3) Rechenberg's 1/5th rule is exploited to change the flight length (instead of fixed) to speed up optimization process and (4) awareness probability is regulated to set a trade-off between local and global exploration. The performance of proposed technique is investigated on 23 benchmark functions such as unimodal, multimodal and fixed-dimension multimodal benchmark functions. The results of ECSA are compared to other state-of-the-art metaheuristic algorithms, in which ECSA outperformed other algorithms in majority of the benchmark functions. Further, to validate the effectiveness of the proposed method, ECSA has been used for optimization of proportional-integral-derivative (PID) controller. Results of ECSA-PID have been compared with conventional CSA as well as with other state-of-the-art techniques like Ziegler-Nichols (Z-N), Kitamori, ACO, multi-objective ACO, multi-objective GA and fuzzy and space gravitational optimization algorithm. The proposed algorithm is implemented on the AVR system and tested under various conditions for robustness. Consistency in the results on benchmark systems as well as on their variants and AVR system and its variants prove the robustness of the proposed method. Also, the performance of the proposed algorithm is found to be better than the existing techniques.																	1432-7643	1433-7479				AUG	2020	24	16					11957	11987		10.1007/s00500-019-04640-w		JAN 2020											
J								Homo-ELM: fully homomorphic extreme learning machine	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Encrypted machine learning; Fully homomorphic encryption; Privacy preservation; Extreme learning machine; Encrypted image classification	NEURAL-NETWORKS; ENCRYPTION	Extreme learning machine (ELM) as a machine learning method has been successfully applied to many classification problems. However, when applying ELM to classification tasks on the encrypted data in cloud, the classification performance is extremely low. Due to the data encryption, ELM is hard to extract informative features from the encrypted data for correct classification. Moreover, the trained neural network is un-protected on the cloud environments, that makes cloud service highly risky to the attackers. In this paper, we propose a novel fully homomorphic ELM (Homo-ELM), which makes cloud searching tasks under a fully protected environment without compromising the privacy of users. To demonstrate the effectiveness of our approach, we conduct a comprehensive experiment on both cloud and local environments. The experiment results show that Homo-ELM can achieve high accuracy on the local environments as well as cloud environments than other machine learning methods.																	1868-8071	1868-808X				JUL	2020	11	7					1531	1540		10.1007/s13042-019-01054-w		JAN 2020											
J								Impacts of computer game concept formulation on project development processes: a FIOWA explorative case study	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computer game development; Shared concept formulation; Coordination; Fuzzy teamwork mental model; Task mental model; Fuzzy OWA	MENTAL MODELS; COORDINATION; PERFORMANCE	The development of computer game projects is different from that of traditional software development projects, which often require clear system specifications for to achieve effective team coordination and cooperation. This unique context requires particular attention for effective project management and project success. Nevertheless, computer game development projects involve different expert groups working on different parts of the computer game design who need to have a shared understanding of the game vision/concept. Arriving at a shared understanding of the application domain among these diversified groups within the project team can be difficult and challenging. Coordination and communication within this diverse talent team is critical to conveying the intent and details of a game's design. In this study, we will explore how coordination practices across different fuzzy OWA expert groups can be enhanced by shared concept formulation during the computer game development process. Based upon an exploratory case study, the result indicates that the shared game vision serves as an important part of the team task mental model, which, in turn, provides the cognitive foundation for the coordination and cooperation needed among team members.																	1868-5137	1868-5145															10.1007/s12652-019-01670-y		JAN 2020											
J								Routing technique for data transmission under jammer in multi-hop wireless network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network; Ad hoc on demand distance vector (AODV); Gravitational search algorithm (GSA); Optimized link state routing protocol (OLSR)		The value and general execution of jammers create them an indispensable hazard for wireless sensor networks. In the multi-hop wireless network, jammers be able to obstruct the broadcast of consumer messages in the transitional nodes alongside the way, one may utilize jamming insensible routing after that occupy first layer mechanisms (e.g. spread spectrum) to restrain the jamming. The novel technique is introduced hybrid ERS-RIP (efficient route selection-routing information protocol) for data transmission under jammer in multi hop wireless sensor networks. Efficient route selection is to maintain minimum cost to identify the routes. For route selection the energy cost and trustable path must be measured. RIP is a distance vector routing protocol presence in the application of OSI layer and is to measure the optimal path between sources to destination. RIP gives updating occasionally regarding network exchange. When the jammers are found, then the network area nodes are choose alternate path with secure and monitor the energy of these path and the packets are send safely with the help of routing protocols to destination node. This algorithm explains in which provides the route based on the jammer detection path. This ERS-RIP algorithm works based on the quality of the optimal link. The performance of the proposed methodology is estimated by its end-to-end delay, throughput, packet delivery rate, packet loss rate and node life time. Results show that the performance of hybrid ERS-RIP is considerably improved than the existing algorithms ERS, ESA, and DSR. The Performance analysis of existing techniques is 80.5, 81.6, 83.4 and hybrid ERS-RIP is 85.5.																	1868-5137	1868-5145															10.1007/s12652-019-01651-1		JAN 2020											
J								The usage of artificial intelligence in the commodity house price evaluation model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial intelligence; Price evaluation; BP neural network; Real estate	LEVEL SHIFTS; VOLATILITY; IMPACTS	The current standardization and legalization of commercial housing price assessment is low. In addition, computer technology is less used in this field. Therefore, the evaluation of commercial housing prices can no longer meet the requirements of today's economy and society. Traditional real estate assessment methods such as market comparison method, cost estimation method, income reduction method, etc., pay attention to market information, but tend to emphasize the total value of real estate in evaluation practice, and they ignore the impact of various constraints on real estate prices, so it is impossible to scientifically understand the influencing factors of real estate prices and their mechanisms of action, resulting in a lack of scientific basis for decision-making by governments, developers or property users. Therefore, this paper introduces the improved algorithm of BP neural network to establish the housing price evaluation model, which reduces the subjectivity and randomness in the evaluation process. This has certain reference for the development of China's real estate assessment method. The evaluation results show that the real estate evaluation price of the network output is close to the actual price, with maximum error of 3.04%. This shows that the application of improved BP neural network model in real estate price evaluation is not only technically feasible, but also credible.																	1868-5137	1868-5145															10.1007/s12652-019-01616-4		JAN 2020											
J								On conflicts between ethical and logical principles in artificial intelligence	AI & SOCIETY										Artificial intelligence ethics; Formal logic constraints; Machine incompleteness; Value alignment; Algorithm transparency vs; explainability		Artificial intelligence is nowadays a reality. Setting rules on the potential outcomes of intelligent machines, so that no surprise can be expected by humans from the behavior of those machines, is becoming a priority for policy makers. In its recent Communication "Artificial Intelligence for Europe" (EU Commission2018), for instance, the European Commission identifies the distinguishing trait of an intelligent machine in the presence of "a certain degree of autonomy" in decision making, in the light of the context. The crucial issue to be addressed is, therefore, whether it is possible to identify a set of rules for data use by intelligent machines so that the decision-making autonomy of machines can allow for humans' traditional informational self-determination (humans provide machines only with the data they decide to), as enshrined in many existing legal frameworks (including, for personal data protection, the EU's General Data Protection Regulation) (EU Parliament and Council2016) and can actually turn out to be further beneficial to individuals. Governing the autonomy of machines can be a very ambitious goal for humans since machines are geared first to the principles of formal logic and then-possibly-to ethical or legal principles. This introduces an unprecedented degree of complexity in how a norm should be engineered, which requires, in turn, an in-depth reflection in order to prevent conflicts between the legal and ethical principles underlying humans' civil coexistence and the rules of formal logic upon which the functioning of machines is based (EU Parliament2017).																	0951-5666	1435-5655															10.1007/s00146-019-00927-6		JAN 2020											
J								Novel deep learning framework for broadcasting abnormal events obtained from surveillance applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Convolution neural network; Object detection and classification; Wireless sensor network; Surveillance monitoring applications	TRACKING	Nowadays, wireless sensor networks based applications is drastically increasing. One of the emerging and public safety applications is surveillance monitoring. This paper focused on monitoring a forest environment, is considered as the sample surveillance application. The existing deep learning application has been used only for object detection and classification alone. It does not explain about data transmission or the base platform where the application deployed. But this paper describes the surveillance application and routing in WSN more clearly. Surveillance monitoring is mainly used for identifying and detecting abnormal activities to tighten the security and provide prevention in a specific field. This paper proposed a Novel Deep Learning Framework (NDLF) for integrating two different fields such as image processing and routing in wireless sensor network. NDLF involves a deep Convolution Neural Network for identifying the abnormal events and the data streaming by creating a Multipath routing in WSN. The multipath routing immediately creates an optimal path and transmit the data as quick as possible to the admin, which leads to provide security. NDLF is implemented and experimented using MATLAB software, verify the results and evaluate the performance. From the comparison, it is identified that the proposed NDLF method obtained 99.53% of accuracy in classification, which is better than the existing approaches such as Time-efficient Object Recovery Scheme and a Communication-efficient Object Recovery Scheme. Also, the proposed NDLF performs well than the existing CNN method in terms of precision and recall.																	1868-5137	1868-5145															10.1007/s12652-019-01668-6		JAN 2020											
J								Fractional power series neural network for solving delay fractional optimal control problems	CONNECTION SCIENCE										Delay optimal control problems; fractional-order; Pade approximation; Hamiltonian conditions; transfer functions; fractional power series; neural network; error function; optimisation scheme	PARTIAL-DIFFERENTIAL-EQUATIONS; VARIATIONAL ITERATION METHOD; BOUNDARY-VALUE-PROBLEMS; BLOCK-PULSE FUNCTIONS; NUMERICAL-SOLUTION; COLLOCATION METHOD; OPTIMIZATION PROBLEMS; PROGRAMMING PROBLEMS; MODEL; SYSTEMS	In this paper, we develop a numerical method for solving the delay optimal control problems of fractional-order. The fractional derivatives are considered in the Caputo sense. The process begins with the assumption that the problem is first transformed into an equivalent problem with a fractional dynamical system without delay, using a Pade approximation. We then try to approximate the solution of the Hamiltonian conditions based on the Pontryagin minimum principle. The main feature is to implement nonlinear polynomial expansions in a neural network adaptive structure. The transfer functions of the employed neural network follow a fractional power series. The proposed technique does not use sigmoid or hyperbolic tangent nonlinear transfer functions commonly adopted in conventional neural networks at the output. Instead, linear transfer functions are employed which lead to explicit fractional power series formulae for the fractional optimal control problem. To do this, we use trial solutions for the states, Lagrange multipliers and control functions where these trial solutions are constructed by fractional power series neural network model. We then minimise the error function using an unconstrained optimisation scheme where weight parameters (or coefficients of the series) and biases associated with all neurons are unknown. Some numerical examples are given to illustrate the effectiveness of the proposed scheme.																	0954-0091	1360-0494				JAN 2	2020	32	1					53	80		10.1080/09540091.2019.1605498													
J								Method of intelligent support of decision-making at dispatching the geospatial processes	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Geospatial data; multilevel information processing; intelligent geoinformation system; spatial processes dispatching intelligent		State-of-the-art means of intelligent support of decision-making at dispatching the geospatial processes that use the digital cartographic data sets do not allow for an operative and adequate enough consideration of the rapidly varying natural environment's characteristics influencing upon the dispatching decisions being made when controlling the complexly distributed geospatial processes' totalities. At that dispatching is an overall manifestation of a versatile process of geospatial processes' management. As a rule, the task of managing the objects realising geospatial processes is solved in isolation from the current state of their natural environment, and does not consider the environment radio, physical and other characteristics' variability. The issues of changes in navigational and meteorological conditions, as well as the issues related to ensuring spatial security, are exclusively subjected to the consideration. Under current conditions becomes meaningful a more complex version of the spatial processes dispatching problem, when the environment variability determines the unique conditions for dispatching at each point of the coordinated geographical space. This statement of a problem is especially relevant under an introduction of modern adaptive means intended for monitoring and sensing the natural environment, like subsystems for monitoring the corresponding systems of spatial processes dispatching. The novelty both of this formulation and the method of this version problem-solving consists in the data fusion from the representation of spatial processes subject domain and those corresponding to the standard models of digital data sets, the integration of software mechanisms for working with information on geospatial processes and programmes for working with cartographic data sets. Development of a reasonable problem statement and a method for this problem's version solving is the essence of this article.																	0952-813X	1362-3079				JAN 2	2020	32	1					1	15		10.1080/0952813X.2019.1592237													
J								Indoor Topological Localization Based on a Novel Deep Learning Technique	COGNITIVE COMPUTATION										Convolutional neural networks; Localization; Semantic map; Visually impaired people	VISION	Millions of people in the world suffer from vision impairment or vision loss. Traditionally, they rely on guide sticks or dogs to move around and avoid potential obstacles. However, both guide sticks and dogs are passive. They are unable to provide conceptual knowledge or semantic contents of an environment. To address this issue, this paper presents a vision-based cognitive system to support the independence of visually impaired people. More specifically, a 3D indoor semantic map is firstly constructed with a hand-held RGB-D sensor. The constructed map is then deployed for indoor topological localization. Convolutional neural networks are used for both semantic information extraction and location inference. Semantic information is used to further verify localization results and eliminate errors. The topological localization performance can be effectively improved despite significant appearance changes within an environment. Experiments have been conducted to demonstrate that the proposed method can increase both localization accuracy and recall rates. The proposed system can be potentially deployed by visually impaired people to move around safely and have independent life.																	1866-9956	1866-9964				MAY	2020	12	3					528	541		10.1007/s12559-019-09693-5		JAN 2020											
J								A Novel Group Decision-Making Method Based on Linguistic Neutrosophic Maclaurin Symmetric Mean (Revision IV)	COGNITIVE COMPUTATION										Maclaurin symmetric mean; Linguistic neutrosophic set; Linguistic neutrosophic Maclaurin symmetric mean; Multiple attribute group decision-making	AGGREGATION OPERATORS; BONFERRONI MEANS; NUMBERS; SETS	Linguistic neutrosophic number (LNN) is a specific form of neutrosophic number whose elements are expressed by linguistic terms. Maclaurin symmetric mean (MSM) operator is one of the basic collection operators in the modern knowledge fusion theory. Its most important feature is to consider the interrelationships among multiple input arguments. Multiple attribute group decision-making (MAGDM) with linguistic neutrosophic information is considered. First, we present some basic concepts, then we combine the MSM operator with linguistic neutrosophic environment and develop a sequence of linguistic neutrosophic MSM operators which are the linguistic neutrosophic Maclaurin symmetric mean (LNMSM) operator, the weighted linguistic neutrosophic Maclaurin symmetric mean (WLNMSM) operator, linguistic neutrosophic dual Maclaurin symmetric mean (LNDMSM) operator, and the weighted linguistic neutrosophic dual Maclaurin symmetric mean (WLNDMSM) operator. We look into some features of them such as monotonicity, boundedness, and idempotency and then discuss some special situations of these operators. A new idea based on the WLNMSM operator is proposed to solve an MAGDM problem where evaluation information is composed of LNNs. It is worth mentioning that the weight information of the decision-makers (DMs) and the attributes are completely unknown. In conclusion, a comparison analysis is performed with the existing methods. The developed method is based on both the WLNMSM operator which considers the interrelationships among any number of input arguments and LNNs which is a combination of the neutrosophic numbers, linguistic variables. At the same time, it also has the advantages of mentioned components. So, it enables preventing the loss or distortion of the original decision information in the decision-making process.																	1866-9956	1866-9964				MAY	2020	12	3					699	717		10.1007/s12559-019-09709-0		JAN 2020											
J								Learning Multi-human Optical Flow	INTERNATIONAL JOURNAL OF COMPUTER VISION										Optical; Flow; Deep; Learning; Human; Bodies; Synthetic; Dataset; Humanflow	MOTION	The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single- and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.																	0920-5691	1573-1405				APR	2020	128	4			SI		873	890		10.1007/s11263-019-01279-w		JAN 2020											
J								Text2Sign: Towards Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative adversarial networks; Neural machine translation; Sign language production		We present a novel approach to automatic Sign Language Production using recent developments in Neural Machine Translation (NMT), Generative Adversarial Networks, and motion generation. Our system is capable of producing sign videos from spoken language sentences. Contrary to current approaches that are dependent on heavily annotated data, our approach requires minimal gloss and skeletal level annotations for training. We achieve this by breaking down the task into dedicated sub-processes. We first translate spoken language sentences into sign pose sequences by combining an NMT network with a Motion Graph. The resulting pose information is then used to condition a generative model that produces photo realistic sign language video sequences. This is the first approach to continuous sign video generation that does not use a classical graphical avatar. We evaluate the translation abilities of our approach on the PHOENIX14T Sign Language Translation dataset. We set a baseline for text-to-gloss translation, reporting a BLEU-4 score of 16.34/15.26 on dev/test sets. We further demonstrate the video generation capabilities of our approach for both multi-signer and high-definition settings qualitatively and quantitatively using broadcast quality assessment metrics.																	0920-5691	1573-1405				APR	2020	128	4			SI		891	908		10.1007/s11263-019-01281-2		JAN 2020											
J								Spatially-Adaptive Filter Units for Compact and Efficient Deep Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Compact ConvNets; Efficient ConvNets; Displacement units; Adjustable receptive fields		Convolutional neural networks excel in a number of computer vision tasks. One of their most crucial architectural elements is the effective receptive field size, which has to be manually set to accommodate a specific task. Standard solutions involve large kernels, down/up-sampling and dilated convolutions. These require testing a variety of dilation and down/up-sampling factors and result in non-compact networks and large number of parameters. We address this issue by proposing a new convolution filter composed of displaced aggregation units (DAU). DAUs learn spatial displacements and adapt the receptive field sizes of individual convolution filters to a given problem, thus reducing the need for hand-crafted modifications. DAUs provide a seamless substitution of convolutional filters in existing state-of-the-art architectures, which we demonstrate on AlexNet, ResNet50, ResNet101, DeepLab and SRN-DeblurNet. The benefits of this design are demonstrated on a variety of computer vision tasks and datasets, such as image classification (ILSVRC 2012), semantic segmentation (PASCAL VOC 2011, Cityscape) and blind image de-blurring (GOPRO). Results show that DAUs efficiently allocate parameters resulting in up to 4x more compact networks in terms of the number of parameters at similar or better performance.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2049	2067		10.1007/s11263-019-01282-1		JAN 2020											
J								State dependent queueing models under admission control F-policy: a survey	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Admission control; State dependent queue; F-policy; Finite queue; Literature survey	CONTROLLING ARRIVALS; MACHINING SYSTEM; M/G/1 QUEUE; N-POLICY; INTERRELATIONSHIP; SERVICE; REPAIR	The optimal F-policy can be used to control the admission of customers in queueing scenarios having limited waiting space. According to F-policy, as soon as the capacity of the system becomes full, the arriving customers are restricted to join the system until there is a drop in the number of customers in the system to a threshold level 'F' due to departure of the served customers. In spite of enormous applications of F-policy models in several queueing systems, a limited number of papers has appeared in the queueing literature. The main objective of this article is to provide a state-of-the-art and survey of literature on the state dependent queueing models operating under F-policy. In this article, we describe the mathematical formulation of queueing models operating under F-policy in two categories namely, finite capacity and finite population models. The methodological aspect of non-Markov models with F-policy is presented. The exhaustive list of work done on F-policy is also provided in tabular form.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3873	3891		10.1007/s12652-019-01638-y		JAN 2020											
J								Gait identification using a new time-warped similarity metric based on smartphone inertial signals	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gait identification; Dynamic time warping; Person identification; Smartphone inertial signal	EVENT DETECTION; AUTHENTICATION; ACCELEROMETER; RECOGNITION; PERFORMANCE; WALKING	The evolution of smart devices and ubiquitous computing have paved the way for more intelligent user verification options. Gait pattern recognition has gained prominence due to their noninvasive and seamless verification capabilities without requiring dedicated attention from the user. This paper proposes a new gait identification approach by proposing a new time-warped similarity metric for memory-based gait pattern analysis. The proposed metric is incorporated into a computationally-efficient template matching framework for gait identification. The proposed approach outperforms the conventional approaches by achieving a user recognition error rate of 7.7% and equal error rate of 11.2% based on samples from 50 subjects in two benchmark datasets.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4041	4053		10.1007/s12652-019-01659-7		JAN 2020											
J								Predictive analysis of student academic performance and employability chances using HLVQ algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Student placement; Academic performance; Data mining; EDM; LVQ; Classification		Educational Data Mining (EDM) uses various data mining tools and methods for different applications in the field of education. EDM applications and techniques follow both pure and practical research objectives to enhance the learning process and to improve and develop learning quality. Educational data mining helps in forecasting the future patterns to make the organizations or institutions provide quality based education to the learners. Educational institutions still struggle with graduation and employment toll. It is an appropriate demonstration of a "surplus of information but a starvation of knowledge". It is essential to identify the prospective ability of student by predicting the present performance by means of earlier period performance and awareness to ensure the student start the career and move ahead in the right path for better quality. This paper presents a novel hybrid algorithm, HLVQ to predict student academic performance and employability chances.																	1868-5137	1868-5145															10.1007/s12652-019-01674-8		JAN 2020											
J								Cross-domain transfer person re-identification via topology properties preserved local fisher discriminant analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Transfer learning; Person re-identification; Topology properties preserve; Local Fisher discriminant analysis	KISS	Person re-identification (Re-ID) systems aim to identify a person appeared in non-overlapping cameras. However, a sufficient amount of pairwise cross-camera-view person images are often not available in a new scenario. Transfer learning can assist the new Re-ID system through leveraging knowledge from other related scenarios. Since the images from existed scenarios may not be the exact representative samples, how to learn a robust transfer Re-ID model with limited labeled person images is still a challenge so far. To solve this problem, a novel cross-domain transfer person Re-ID via topology properties preserved local Fisher discriminant analysis (TPPLFDA) method is proposed in this paper. Making an assumption that all person images in the new and related scenarios share common manifold, TPPLFDA projects all cross-domain images into a low dimensional linear subspace, while preserves the topology properties according to the geodesic distances on manifold. Then, multiple cross-domain datasets as source domains are considered and kernel TPPLFDA for multi-source domain transfer is proposed, so that TPPLFDA can handle more complex cross-domain transfer Re-ID tasks. Extensive experiments on several transfer Re-ID datasets show that TPPLFDA is effective.																	1868-5137	1868-5145															10.1007/s12652-019-01665-9		JAN 2020											
J								Multi-disease prediction model using improved SVM-radial bias technique in healthcare monitoring system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										SVM; Random forest; Decision tree; Data analytics; Chronic kidney disease; Diabetes; Heart disease; Clinical data analytics; Healthcare analytics	SUPPORT VECTOR MACHINES; DECISION-SUPPORT; IDENTIFICATION; DIAGNOSIS	In this digital world, data is an asset, and enormous data was generating in all the fields. Data in the healthcare industry consists of patient information and disease-related information. This medical data and machine learning techniques will help us to analyse a large amount of data to find out the hidden patterns in the disease, to provide personalised treatment for the patient and also used to predict the disease. In this work, a general architecture has proposed for predicting the disease in the healthcare industry. This system was experimented using with reduced set features of Chronic Kidney Disease, Diabetes and Heart Disease dataset using improved SVM-Radial bias kernel method, and also this system has compared with other machine learning techniques such as SVM-Linear, SVM-Polynomial, Random forest and Decision tree in R studio. The performance of all these machine learning algorithms has evaluated with accuracy, misclassification rate, precision, sensitivity and specificity. From the experiment results, improved SVM-Radial bias kernel technique produces accuracy as 98.3%, 98.7% and 89.9% in Chronic Kidney Disease, Diabetes and Heart Disease dataset respectively.																	1868-5137	1868-5145															10.1007/s12652-019-01652-0		JAN 2020											
J								Effective feature selection technique in an integrated environment using enhanced principal component analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; Feature classification; Supervised learning; Unsupervised learning; Enhanced Principal Component Analysis (EPCA)	FEATURE-EXTRACTION; ALGORITHMS	Dataset have various number of features. Feature extraction plays a crucial job in recognition and extraction of most useful data from the dataset. Appropriate mining method must be performed in order to remove the required information from the dataset. Feature selection helps to train the machine learning algorithm faster as well as reduce the complexity of the model. The hidden patterns are learnt and perceived the unlabeled data in unsupervised learning. Supervised learning alludes to the ability of learning and organizing the signal. This research focus the feature selection by the Enhanced Principal Component Analysis (EPCA) algorithm. This is appropriate for supervised, unsupervised environment. Double measures are associated with EPCA technique as feature selection and the feature extraction. These two measures are used for removing the unnecessary features in the data using dissimilarity matrix calculation. At that point this calculation was contrasted with different calculations by ensuring that the important features were never neglected by EPCA. This EPCA gives better results in supervised, unsupervised environment and it has been tested various numerical and text data in various learning environment.																	1868-5137	1868-5145															10.1007/s12652-019-01647-x		JAN 2020											
J								Designing a clinical decision support system to predict readmissions for patients admitted with all-cause conditions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										All-cause admission; Imbalanced dataset; National health insurance research database (NHIRD); Integrated genetic algorithm and support vector machine (IGS)	HOSPITAL READMISSION; RISK; MODELS; PNEUMONIA; MORTALITY; INDEX	Readmission of patients is generally caused by inappropriate inpatient care, unexpected disease deterioration, inadequate health care plans for transition to community care, or premature discharge, and may be manifested by a degradation of healthcare quality and an increase of healthcare costs. In this study, we aimed to design a clinical decision support system (CDSS) based on patient data collected in Taiwan National Health Insurance Research Database in the prediction of 30-day all-cause readmissions for patients admitted with all-cause conditions. In order to select significant variables and adjust appropriate model parameters, we applied a method integrating genetic algorithm and support vector machine to design CDSSs based on 3 different cost-sensitive objective functions. The imbalanced data of patients admitted with all-cause conditions, including 6864 cases of admission and 48,374 cases of non-admission in 2011 as well as 6923 admissions and 49,521 non-admissions in 2012 were retrieved and used for training and testing the CDSSs, respectively. Each patient data consist of 31 candidate variables, including age, gender, number of comorbidities, Charlson comorbidity index (CCI), presence of comorbidities for calculating CCI, events within 1 year before admission, inpatient intervention, category of admitted hospital, length of admission, and discharge status. In the training phase, cluster-based under-sampling method (kNN) was used to prepare the balanced dataset for training and validating the CDSS. In contrast, the imbalanced testing dataset was applied to test the predictive performance of the trained CDSS model. The accuracy, sensitivity, specificity, and area under ROC curve (AUC) of the CDSSs designed with tenfold cross validation are 69.0-69.4%, 69.3.9-69.7%, 68.8-69.4%, and 0.74-0.76, respectively, in the training phase, and 62.3-64.3%, 68.0-69.5%, 61.3-63.8%, and 0.70-0.72, respectively, in the testing phase. Compared with the model in Maali et al. (BMC Med Inform Decis Making 18(1):1, 2018) (training phase: 0.74; testing phase: 0.71), all three models exhibit higher sensitivity and greater AUC. Among the 31 variables, 10, 18, and 17 salient variables, respectively, were selected for designing the models with three different objective functions. An online web service system designed based on Model 2 is provided for assisting physicians to detect potential patients having higher risk of all-cause readmissions after discharge and, if necessary, give them appropriate interventions to reduce their morbidities and mortalities as well as to reduce their healthcare costs.																	1868-5137	1868-5145															10.1007/s12652-019-01579-6		JAN 2020											
J								An integrated approach towards automated software requirements elicitation from unstructured documents	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Requirements elicitation; Ontology; Unstructured documents; Entity extraction; Relation extraction; Semantic analysis		Enormous amount of text documents that contains requirements expressed in form of unstructured text are obtained in big software development projects and they have to be transformed into structured documents. Collecting requirements from such huge volume of informal text is still a key mission to be achieved. The majority of the software requirements data that are available to software engineers are articulated in form of Natural Language text and there is no formal grammar or specified syntax are followed by the customer for writing requirements i.e. they are highly unstructured. Manual requirements elicitation from these unstructured documents takes lot of time, it is also subject to errors and it is tedious to perform. Any deficiency in the process of gathering the requirements will lead to undesirable consequences that affects the software development. Requirements that are confusing and not complete in any aspect can collapse the software development project itself. We focus on developing an integrated framework towards automated software requirements elicitation from unstructured documents. A formal grammar based technique is developed to tackle the complex sentence structure, regular expression grammar rules that analyzes the linguistic elements in unstructured text are developed and applied to extract the core concepts/entities and relations between entities, the building blocks of any information system development. Another important task that contributes for automation is to build a domain ontology consisting of concepts, their attributes and relation between concepts and axioms, it extends guidance in the semantic labelling of the extracted information constructs. Since it is expensive to have a domain expert to be present all through the stages of software requirements elicitation, domain ontology serves as a background knowledge source. The extracted requirements after semantic association are then presented to the user in form of Object Role Model diagrams to obtain their consent. The framework provides an overall accuracy of 91% in terms of semantic mapping of the extracted requirements.																	1868-5137	1868-5145															10.1007/s12652-019-01667-7		JAN 2020											
J								A survey on feature selection approaches for clustering	ARTIFICIAL INTELLIGENCE REVIEW										Clustering; Feature selection; Data mining; Evolutionary computation	UNSUPERVISED FEATURE-SELECTION; K-MEANS; DIFFERENTIAL EVOLUTION; VARIABLE SELECTION; SEARCH ALGORITHM; CLASSIFICATION; OPTIMIZATION	The massive growth of data in recent years has led challenges in data mining and machine learning tasks. One of the major challenges is the selection of relevant features from the original set of available features that maximally improves the learning performance over that of the original feature set. This issue attracts researchers' attention resulting in a variety of successful feature selection approaches in the literature. Although there exist several surveys on unsupervised learning (e.g., clustering), lots of works concerning unsupervised feature selection are missing in these surveys (e.g., evolutionary computation based feature selection for clustering) for identifying the strengths and weakness of those approaches. In this paper, we introduce a comprehensive survey on feature selection approaches for clustering by reflecting the advantages/disadvantages of current approaches from different perspectives and identifying promising trends for future research.																	0269-2821	1573-7462				AUG	2020	53	6					4519	4545		10.1007/s10462-019-09800-w		JAN 2020											
J								Probabilistic linguistic information fusion: A survey on aggregation operators in terms of principles, definitions, classifications, applications, and challenges	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										aggregation operator; cognitive complex information; information fusion; probabilistic linguistic term set	GROUP DECISION-MAKING; POWER-GEOMETRIC OPERATORS; MEAN AGGREGATION; SETS; ART	The probabilistic linguistic term set is a flexible and efficient tool to represent the cognitive complex information of experts. It has attracted many scholars' attention since it was proposed. Information fusion over the cognitive complex information is a significant issue for decision-making problems. Over the past years, more than 40 aggregation operators have been proposed to fuse the probabilistic linguistic term sets. The aim of this paper is to survey the existing probabilistic linguistic aggregation operators from the perspectives of principles, definitions, classifications, and applications. To do so, first, we summarize the present normalization techniques and operations of probabilistic linguistic term sets. Afterward, this study classifies the existing probabilistic linguistic aggregation operators into 12 kinds. Then, the application areas of these probabilistic linguistic aggregation operators are outlined. Future research directions with interests are proposed to tackle present challenges.																	0884-8173	1098-111X				MAR	2020	35	3					529	556		10.1002/int.22216		JAN 2020											
J								Using single axioms to characterize L-rough approximate operators with respect to various types of L-relations	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Fuzzy rough set; Fuzzy approximate operator; Fuzzy set; Fuzzy relation	SETS; SYSTEMS	Considering L being a complete Heyting algebra, this paper mainly proposes a general framework of L-rough approximate operators in which constructive and axiomatic approaches are used. In the constructive approach, upper and lower L-rough approximate operators are introduced and their connections with L-relations are investigated. In the axiomatic approach, various types of set-theoretic L-operators are defined. It is shown that each type of L-rough approximate operators corresponding to special kind of L-relations, including serial, reflexive, symmetric, transitive, mediate, Euclidean and adjoint L-relations as well as their compositions, can be characterized by single axioms.																	1868-8071	1868-808X				MAY	2020	11	5					1061	1082		10.1007/s13042-019-01051-z		JAN 2020											
J								The Focus-Aspect-Value model for predicting subjective visual attributes	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Visual interpretation; Subjectivity; Neural networks; Information fusion; Attribute prediction; Tensor fusion		Predicting subjective visual interpretation is important for several prominent tasks in computer vision, including multimedia retrieval. Many approaches reduce this problem to the prediction of adjective or attribute labels from images while neglecting attribute semantics and only processing the image in a holistic manner. Furthermore, there is a lack of relevant datasets with fine-grained subjective labels and sufficient scale for machine learning. In this paper, we explain the Focus-Aspect-Value (FAV) model to break down the process of subjective image interpretation into three steps and describe a dataset following this way of modeling. We train and evaluate several deep learning methods on this dataset, while we extend the experiments of the paper originally introducing FAV by adding a new evaluation metric, improving the concatenation approach and adding Multiplicative Fusion as another method. In our experiments, Tensor Fusion is among the best performing methods across all measures and outperforms the default way of information fusion (concatenation). In addition, we find that the way of combining information in neural networks not only affects prediction performance but can drastically change other properties of the model as well.																	2192-6611	2192-662X				MAR	2020	9	1			SI		47	60		10.1007/s13735-019-00188-5		JAN 2020											
J								Compliance Control for Robot Manipulation in Contact with a Varied Environment Based on a New Joint Torque Controller	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Joint torque control; Force impedance control; Active disturbance rejection control; Varied environment	CARTESIAN IMPEDANCE CONTROLLER; STATE-FEEDBACK; IDENTIFICATION; BANDWIDTH; OBSERVER; DESIGN	Compliance control is required in the applications of robots for assembling, grinding, polishing and human-robot interface, which needs both position control and force control of robots. The impedance control based on joint torque servo is a promising and practical method to realize compliance control in industrial applications. The performance of a joint torque servo controller is thus crucial to the success of compliance control. However, both nonlinear friction torque of the joint motor and varied dynamics of environmental contact during manipulation increase control difficulty greatly. This paper focuses on the compliance control problem and presents a new joint torque servo controller which is a cascading structure including an inner velocity feedback loop and an outer torque control loop. Due to the high signal-to-noise ratio of the motor velocity, an extended state observer is designed to effectively estimate and compensate for the motor friction torque, varied dynamics of environmental contact and other unknown disturbance. And benefiting from the introduction of the efficient and powerful velocity inner loop, the new torque controller performs very well not only in the rigid but also elastic contact environment. Then a standard PD controller is designed in the outer torque loop to produce the control law. Based on the satisfactory joint torque controller, a simplified impedance control algorithm is designed to achieve the force control. Experiments with three other joint torque controllers on a robot manipulator are conducted to demonstrate the proposed joint torque method and the overall force control framework. These results show the proposed joint torque controller can reduce the steady-state error from 3.07 Nm to 0.21 Nm which indicates the joint actuator friction can be eliminated by more than 99%. And a substantial improvement can also be observed in the experiments of force control based on the new joint torque controller.																	0921-0296	1573-0409				JUL	2020	99	1					79	90		10.1007/s10846-019-01109-8		JAN 2020											
J								2D Underwater Obstacle Avoidance Control Algorithm Based on IB-LBM and APF Method for a Multi-Joint Snake-Like Robot	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Snake-like robot; IB-LBM; APF; Obstacle avoidance		In order to study the rapidity and stability of multi-joint snake-like robots in underwater obstacle avoidance, a new underwater obstacle avoidance control algorithm for multi-joint snake-like robots based on the combination of IB-LBM (Immersed Boundary-Lattice Boltzmann Method) and APF (Artificial Potential Field) is proposed in this paper. Firstly, the IB-LBM is used to establish the non-linear flow field model and the fluid-structure coupling model of a multi-joint snake-like robot. Secondly, the 2-D Serpentine curve equation of motions of a multi-joint snake-like robot is improved, and APF method is added to each joint of the robot to control its motion direction, so as to achieve effective obstacle avoidance of the robot in the flow field. Then by MATLAB simulation experiment, influences of different control parameters on obstacle avoidance performance of a multi-joint snake-like robot are analyzed, and the best parameters suitable for obstacle avoidance of a robot are found to improve the rapidity and stability of the whole system. Finally, an underwater obstacle avoidance experiment of a multi-joint snake-like robot is carried out to verify the effectiveness of the proposed control algorithm in real environment.																	0921-0296	1573-0409				JUN	2020	98	3-4					771	790		10.1007/s10846-019-01097-9		JAN 2020											
J								Group behavior recognition based on deep hierarchical network	NEURAL COMPUTING & APPLICATIONS										Deep hierarchical network; Group; Behavior recognition; Efficiency; Extraction	CONVOLUTIONAL NEURAL-NETWORKS	In order to achieve accurate judgment and identification of group behaviors, the hierarchical deep network model is constructed to judge the group behaviors. Through the construction of the hierarchical deep network model, the group behaviors are judged; the stability, accuracy, expression movement, orientation, error, and work efficiency of the model, as well as the support vector machine model and the convolution network model, are compared and analyzed. The hierarchical depth network model has distinct advantages in comparison with the support vector machine model and the convolution network model. The standard deviation of the hierarchical depth network model is 0.013, while the standard deviations of the other two models are larger than the hierarchical depth network model. The proposed algorithm has an excellent performance in detection accuracy and error. Compared with the other two models, it has certain advantages. In addition, the hierarchical depth network is used for recognizing human behaviors and orientations, as well as extracting and recovering the expressions in group behaviors. Compared with the other two models, the proposed model is also more efficient. The model used in this study has little influence on group behavior recognition by the regional environment and other factors, and there is no significant difference in the judgment results. The operation of the model is studied by identifying the group behaviors based on the hierarchical depth network. The research results show that the model proposed in this study has a comprehensive and excellent result, which also indicates that group behavior recognition is an overall result. It is necessary to have an accurate identification of multiple layer parameters. The research in this study has greatly improved the understanding of hierarchical deep network and group behavior recognition.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5389	5398		10.1007/s00521-019-04699-4		JAN 2020											
J								CRHASum: extractive text summarization with contextualized-representation hierarchical-attention summarization network	NEURAL COMPUTING & APPLICATIONS										Extractive text summarization; Contextualized representation; Hierarchical attention mechanism; Bi-GRU		The requirements for automatic document summarization that can be applied to practical applications are increasing rapidly. As a general sentence regression architecture, extractive text summarization captures sentences from a document by leveraging externally related information. However, existing sentence regression approaches have not employed features that mine the contextual information and relations among sentences. To alleviate this problem, we present a neural network model, namely the Contextualized-Representation Hierarchical-Attention Summarization (CRHASum), that uses the contextual information and relations among sentences to improve the sentence regression performance for extractive text summarization. This framework makes the most of their advantages. One advantage is that the contextual representation is allowed to vary across linguistic context information, and the other advantage is that the hierarchical attention mechanism is able to capture the contextual relations from the word-level and sentence-level by using the Bi-GRU. With this design, the CRHASum model is capable of paying attention to the important context in the surrounding context of a given sentence for extractive text summarization. We carry out extensive experiments on three benchmark datasets. CRHASum alone can achieve comparable performance to the state-of-the-art approach. Meanwhile, our method significantly outperforms the state-of-the-art baselines in terms of multiple ROUNG metrics and includes a few basic useful features.																	0941-0643	1433-3058				AUG	2020	32	15					11491	11503		10.1007/s00521-019-04638-3		JAN 2020											
J								Full-reference image quality metric for blurry images and compressed images using hybrid dictionary learning	NEURAL COMPUTING & APPLICATIONS										Image quality assessment; Dictionary learning; Sparse coding; Image blur; Image compression	SPARSE REPRESENTATION; SHARPNESS ASSESSMENT; ENERGY; SEGMENTATION; ALGORITHM	The image quality degradation due to the loss of high-frequency components of images is often seen in real scenarios, such as artifacts caused by image compression and image blur caused by camera shake or out of focus. Quantifying such degradation is very useful for many tasks that are related to image quality. In this paper, an effective approach is proposed for the image quality assessment on images with blur as well as images with compression artifacts. Based on the relation between the dictionaries of the degraded image and the reference image, we build up a hybrid dictionary learning model to characterize the space of patches of the reference image as well as that of the degraded image. The image quality is then measured by the difference between the two resulting dictionaries. Combined with a simple sparse-coding-based metric, the proposed method shows competitive performance on five benchmark datasets, which demonstrates its effectiveness.																	0941-0643	1433-3058				AUG	2020	32	16					12403	12415		10.1007/s00521-019-04694-9		JAN 2020											
J								A new hybrid algorithm to solve bound-constrained nonlinear optimization problems	NEURAL COMPUTING & APPLICATIONS										Global optimization; Genetic algorithm; Self-organizing migrating algorithm; Performance index; Multimodal continuous function	ORGANIZING MIGRATION ALGORITHM; PARTICLE SWARM OPTIMIZER; GENETIC ALGORITHM; GLOBAL OPTIMIZATION; METAHEURISTIC ALGORITHM; DIFFERENTIAL EVOLUTION; INTELLIGENCE; INTEGER; DESIGN; TESTS	The goal of this work is to propose a hybrid algorithm called real-coded self-organizing migrating genetic algorithm by combining real-coded genetic algorithm (RCGA) and self-organizing migrating algorithm (SOMA) for solving bound-constrained nonlinear optimization problems having multimodal continuous functions. In RCGA, exponential ranking selection, whole-arithmetic crossover and non-uniform mutation operations have been used as different operators where as in SOMA, a modification has been done. The performance of the proposed hybrid algorithm has been tested by solving a set of benchmark optimization problems taken from the existing literature. Then, the simulated results have been compared numerically and graphically with existing algorithms. In the graphical comparison, a modified performance index has been proposed. Finally, the proposed algorithm has been applied to solve two real-life problems.																	0941-0643	1433-3058				AUG	2020	32	16					12427	12452		10.1007/s00521-019-04696-7		JAN 2020											
J								LMM: latency-aware micro-service mashup in mobile edge computing environment	NEURAL COMPUTING & APPLICATIONS										Micro-service; Mobile edge computing; Network resource consumption; Latency; Mashup	RESOURCE-ALLOCATION; SMART CITY; CLOUD	Internet of Things (IoT) applications introduce a set of stringent requirements (e.g., low latency, high bandwidth) to network and computing paradigm. 5G networks are faced with great challenges for supporting IoT services. The centralized cloud computing paradigm also becomes inefficient for those stringent requirements. Only extending spectrum resources cannot solve the problem effectively. Mobile edge computing offers an IT service environment at the Radio Access Network edge and presents great opportunities for the development of IoT applications. With the capability to reduce latency and offer an improved user experience, mobile edge computing becomes a key technology toward 5G. To achieve abundant sharing, complex IoT applications have been implemented as a set of lightweight micro-services that are distributed among containers over the mobile edge network. How to produce the optimal collocation of suitable micro-service for an application in mobile edge computing environment is an important issue that should be addressed. To address this issue, we propose a latency-aware micro-service mashup approach in this paper. Firstly, the problem is formulated into an integer nonlinear programming. Then, we prove the NP-hardness of the problem by reducing it into the delay constrained least cost problem. Finally, we propose an approximation latency-aware micro-service mashup approach to solve the problem. Experiment results show that the proposed approach achieves a substantial reduction in network resource consumption while still ensuring the latency constraint.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15411	15425		10.1007/s00521-019-04693-w		JAN 2020											
J								Supply chain pricing and effort decisions with the participants' belief under the uncertain demand	SOFT COMPUTING										Supply chain management; Supply chain's pricing; Game theory; Uncertainty theory; Confidence level	COORDINATION; MANUFACTURER; CONTRACTS; RETAILER; STRATEGY	This paper addresses the pricing and effort decisions of a supply chain consisting of a manufacturer and a retailer. All the parties make optimal decisions to maximize their profits with uncertainty of demand under their confidence levels. Taking this into account, Stackelberg models are formulated to study the impact of the confidence levels on pricing and effort decisions for the decentralized and centralized supply chains. We obtain that the confidence levels of participants have a significant impact on the pricing and effort decisions. Specifically, when the retailer's confidence level is increasing, the retail price, the wholesales price, the sales effort, the profit of each member and the total profit of supply chain are all increasing. However, the manufacturer's confidence level is not independent of the power structure, i.e., there are different characteristics under the different power structures. The power structure has an outstanding effect on the profit of each member in the supply chain. The leader's profit is always more than that of the follower, and the profit of upstream is more than that of the downstream when they have the same power. We use numerical experiments to verify the validity of the model.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6483	6497		10.1007/s00500-019-04633-9		JAN 2020											
J								Image recognition method of building wall cracks based on feature distribution	SOFT COMPUTING										Wall cracks; Characteristic distribution; SAR image segmentation; Crack identification rate	ALGORITHM	In order to solve the building damage caused by cracks in the wall surface, based on feature distribution and SAR image segmentation technology, the cracks in the building wall were identified by extracting feature data images, de-noising and enhancing image edges, and segmenting target images. The validity and feasibility of the method are verified by the actual concrete wall image. The results show that the recognition accuracy of cracks in non-cracked walls is 100%, and that of longitudinal cracks is 78.2%. Compared with the color feature discrimination, this method has a good processing effect on the image, clear crack line, good coincidence degree with the original image, and crack width is close to the width of the original image. After processing the image with rough set, the recognition rate of the image is 98.1%, the false reject rate is 1.9%, the recognition time is 12 min, and the execution time of the algorithm is 126 s. After the processing of gray histogram, the feature distribution of image set has a certain distribution transfer, but the transfer effect is not particularly obvious. It can be found that this method has advantages of high recognition accuracy, short time, and practical application value, significantly enhancing pretreatment effect.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8285	8294		10.1007/s00500-019-04644-6		JAN 2020											
J								A new initialization and performance measure for the rough k-means clustering	SOFT COMPUTING										Rough k-means; Zeta values; Davies-Bouldin index; Root-mean-square standard deviation	FUZZY C-MEANS; ALGORITHM; STABILITY	A new initialization algorithm is proposed in this study to address the issue of random initialization in the rough k-means clustering algorithm refined by Peters. A new means to choose appropriate zeta values in Peters algorithm is proposed. Also, a new performance measure S/O [within-variance (S)/total-variance (O)] index has been introduced for the rough clustering algorithm. The performance criteria such as root-mean-square standard deviation, S/O index, and running time complexity are used to validate the performance of the proposed and random initialization with that of Peters. In addition, other popular initialization algorithms like k-means(++), Peters pi, Bradley, and Ioannis are also herein compared. It is found that our proposed initialization algorithm has performed better than the existing initialization algorithms with Peters refined rough k-means clustering algorithm on different datasets with varying zeta values.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11605	11619		10.1007/s00500-019-04625-9		JAN 2020											
J								A novel entropy and divergence measures with multi-criteria service quality assessment using interval-valued intuitionistic fuzzy TODIM method	SOFT COMPUTING										Divergence measure; Entropy; Interval-valued intuitionistic fuzzy sets; MCDM; TODIM	GROUP DECISION-MAKING; INFORMATION MEASURES; SIMILARITY MEASURE; SUPPLIER SELECTION; SETS; PERSPECTIVE; SUBSETHOOD; OPERATORS; DISTANCE; WEIGHTS	Interval-valued intuitionistic fuzzy sets (IVIFSs) are proven to be the fastest growing research area and are more flexible way to handle the uncertainty. Information measures play vital role in the study of uncertain information; therefore, number of new interval-valued intuitionistic fuzzy divergence and entropy measures have been proposed in the literature and applied for different purposes. Recently, multi-criteria decision-making (MCDM) methods with IVIFSs have broadly studied by researchers and practitioners in various fields. In this paper, firstly surveys of IVIF-divergence and entropy measures are conducted and then demonstrated some counter-intuitive cases. Then, novel divergence and entropy measures are originated for IVIFSs to avoid the shortcomings of previous measures. Later on, systematic reviews of Portuguese for Interactive Multi-criteria Decision Making (TODIM) method are presented with recent fuzzy developments. Based on classical TODIM method, a new approach for MCDM is introduced under IVIF environment which considers the bounded rationality of decision makers. In the present method, the proposed entropy measure is utilized to compute the weight vector of the criteria, and the proposed divergence measure is applied in the calculation of dominance degrees. To illustrate the effectiveness of the present approach, a decision-making problem of vehicle insurance companies is presented where the evaluation values of the alternatives are given in terms of IVIF numbers. Comparison with some existing methods shows the applicability and consistency of the present method.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11641	11661		10.1007/s00500-019-04627-7		JAN 2020											
J								Reliability of ranking-based decision methods: a new perspective from the alternatives' supremacy	SOFT COMPUTING										Ranking-based methods; Alternatives' predominance; Multi-criteria decision analysis; AHP; Pairwise reconstruction methods	PAIRWISE COMPARISONS; SUPPORT-SYSTEMS; CONSISTENCY; PREFERENCE; MATRICES; INFORMATION; POPULATIONS; IMPROVEMENT; DOMINANCE; DISTANCE	Due to the immediacy of communications, the interconnection of different data sources and the large volume of information available in digital environments, rankings have become one of the most used tools in the decision making (DM) process. When choosing an option, the decision maker not only considers the positions of the different alternatives into the ranking, but also usually checks the intensity values associated with them. Therefore, it is very important that methods used to build rankings adequately represent the preferences of users. These issues, known as order and intensity preservation conditions, have been studied for the well-known multi-criteria decision analysis (MCDA) called analytical hierarchical process (AHP) and extended to reconstruction methods of AHP matrices by defining a measure that considers only the order preservation condition. In this article, a measure, complementary to the latter, that allows establishing the difference between the predominance of the alternatives between two rankings is defined. To do this, the relations of predominance between the alternatives for each ranking are analyzed, and then the comparison between these relations is made by defining a bounded supremacy-based measure called supremacy difference index sdi(m). The sdi(m) behavior is compared to conventional distance measures that are not bounded, and it is used to compare three reconstruction methods of AHP from the supremacy point of view and, finally, how the sdi(m) usage can be extended for the evaluation of any ranking-based method is discussed.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11769	11790		10.1007/s00500-019-04637-5		JAN 2020											
J								A new randomness approach based on sine waves to improve performance in metaheuristic algorithms	SOFT COMPUTING										Randomness; Stochastic process; Performance; GWO; FA; FPA; Benchmark functions; Metaheuristics; Sine waves	OPTIMIZATION; STRATEGY	The main goal of this paper is to outline a new approach to represent the randomness that we can find in different metaheuristics as a stochastic process which helps in the performance of the analyzed metaheuristic. This new way of viewing randomness is based on the behavior of sine waves that we can find in many situations in nature or in physics laws. In this paper, we evaluate this proposed randomness with three metaheuristics: the grey wolf optimizer, firefly algorithm and flower pollination algorithm, with the goal of studying the performance of the proposed randomness method in different types of metaheuristics. A set of standard benchmark functions were used to test the proposed randomness method, which are classified as unimodal and multimodal benchmark functions. In addition, the benchmark functions of the CEC 2015 Competition are used. Finally, we present tests with functions that were presented in the CEC 2017 competition for constrained real-parameter optimization. We also present a comparative study of the analyzed metaheuristics, and this comparison is between their original randomness method and the proposed randomness method for each algorithm. Finally, we present the performance and results of the methods with different number of dimensions to complete the study.																	1432-7643	1433-7479				AUG	2020	24	16					11989	12011		10.1007/s00500-019-04641-9		JAN 2020											
J								Color harmony algorithm: an art-inspired metaheuristic for mathematical function optimization	SOFT COMPUTING										Color harmony algorithm; Metaheuristic; Population-based optimization; Global optimization	KRILL HERD	In the last 3 decades, metaheuristic algorithms have received more popularity because of their superior performance to solve large and complex optimization problems. Most of these algorithms are inspired by biological phenomena, social behavior of animals, science and art. Among these four sources, the last one is utilized only by one algorithm. In this paper, we propose another novel art-inspired population-based metaheuristic, called color harmony algorithm (CHA), for solving the global optimization problems. The proposed method models its search behavior through combining harmonic colors based on their relative positions around the hue circle in the Munsell color system and harmonic templates. We utilize simultaneously four different fitness information to construct the hue groups, which improve search ability of the algorithm. CHA has two different phases including the concentration phase and the dispersion phase which are employed to explore and exploit the search space. The performance of the proposed method has been examined using several benchmark test functions commonly used in the literature. To show the effectiveness and robustness of the proposed method, the results are compared with those obtained using ten well-known metaheuristic algorithms. Also, the Wilcoxon Signed-Rank test is conducted to measure the pair-wise statistical performances of the algorithms. The results indicate that besides the simplicity of the proposed algorithm, CHA can outperform the other considered algorithms in terms of the convergence speed and the number of function evaluations.																	1432-7643	1433-7479				AUG	2020	24	16					12027	12066		10.1007/s00500-019-04646-4		JAN 2020											
J								Culture codes of scientific concepts in global scientific online discourse	AI & SOCIETY										Culture code; Scientific online discourse; Researchers; Image; Electronic communications; Scientific concept; Globalization	BODY-IMAGE; SEGMENTATION; FACEBOOK; NUMBER; IMPACT; SELF	This paper utilizes Rapaille's (2006) concept of culture codes and Hall's (2001) encoding and decoding model of communication to identify the culture codes of scientific concepts in global scientific online discourse. As an example, we attempted to identify the culture codes of the concept of "image", because this concept can be interpreted in different ways in Russian and international scientific discourse. To identify these codes, we analyzed the interpretations of the concept of "image" in scientific online discourse in Russia and abroad. We studied the titles, key words, and abstracts of papers published in 2014-2018 that appeared in the Russian Science Citation Index (RSCI) and the Scopus abstract and citation database. As a result, we identified the culture codes of the concept of "image" in Russian and international scientific online discourse and compared the culture codes of RSCI-indexed and Scopus-indexed papers. The method we utilized may be used for revealing the culture codes of any scientific concept (using any citation database), which can contribute to revealing and understanding the interpretations of these concepts by researchers from different countries.																	0951-5666	1435-5655				SEP	2020	35	3					699	714		10.1007/s00146-019-00934-7		JAN 2020											
J								A CTR prediction model based on user interest via attention mechanism	APPLIED INTELLIGENCE										Click-through rate; Advertising; Attention mechanism; Bidirectional gated recurrent unit		Recently, click-through rate (CTR) prediction is a challenge problem in the aspect of online advertising. Some researchers have proposed deep learning-based models that follow a similar embedding and MLP paradigm. However, the corresponding approaches generally ignore the importance of capturing the latent user interest behind user behaviour data. In this paper, we present a novel attentive deep interest-based network model called ADIN. Specifically, we capture the interest sequence in the interest extractor layer, and the auxiliary losses are employed to produce the interest state with the deep supervision. First, we model the dependency between behaviours by using a bidirectional gated recurrent unit (Bi-GRU). Next, we extract the interest evolving process that is related to the target and propose an interest evolving layer. At the same time, attention mechanism is embedded into the sequential structure. Then, the model learns highly non-linear interactions of features based on stack autoencoders. An experiment has been done using four real-world datasets, the proposed model achieves superior performance than the existing state-of-the-art models.																	0924-669X	1573-7497				APR	2020	50	4					1192	1203		10.1007/s10489-019-01571-9		JAN 2020											
J								A highly stable and efficient spherical underwater robot with hybrid propulsion devices	AUTONOMOUS ROBOTS										Spherical underwater robot; Hybrid thrusters; Hybrid propulsion devices; Computational fluid dynamics simulation; Multi-reference frame method	DESIGN	Underwater robots have been promoted a significant interest in monitoring the marine environment. In some complex situation, robots sometimes need to keep moving fast, sometimes need to keep low speed and low noise. To address this issue, a novel spherical underwater robot (SUR IV) with hybrid propulsion devices including vectored water-jet and propeller thrusters is proposed in this paper. The diversity of the movement modes is also proposed for the different targets as remote or hover and general or silent. To analyze the hydrodynamic characteristics of the hybrid thruster, the computational fluid dynamics simulation is calculated in ANSYS CFX by using the multi-reference frame method. The simulation results show the interaction between the propeller and water-jet thruster. The thrust experiment to evaluate the performance of the improved hybrid thruster is also conducted. The maximum thrust of the hybrid thruster is increased 2.27 times than before. In addition, a noise comparison experiment is conducted to verify the low noise of the water-jet thruster. Finally, the 3 DoF motions which include the surge, heave and yaw for the SUR IV were carried out in the swimming pool. The improvement of the overall robot is assessed by the experimental results.																	0929-5593	1573-7527				MAY	2020	44	5					759	771		10.1007/s10514-019-09895-8		JAN 2020											
J								Parameterized low-rank binary matrix approximation	DATA MINING AND KNOWLEDGE DISCOVERY										Binary matrices; Clustering; Low-rank approximation; Fixed-parameter tractability	INDEPENDENT COMPONENT ANALYSIS; COMPLEXITY; FIELDS	Low-rank binary matrix approximation is a generic problem where one seeks a good approximation of a binary matrix by another binary matrix with some specific properties. A good approximation means that the difference between the two matrices in some matrix norm is small. The properties of the approximation binary matrix could be: a small number of different columns, a small binary rank or a small Boolean rank. Unfortunately, most variants of these problems are NP-hard. Due to this, we initiate the systematic algorithmic study of low-rank binary matrix approximation from the perspective of parameterized complexity. We show in which cases and under what conditions the problem is fixed-parameter tractable, admits a polynomial kernel and can be solved in parameterized subexponential time.																	1384-5810	1573-756X				MAR	2020	34	2					478	532		10.1007/s10618-019-00669-5		JAN 2020											
J								Integer programming ensemble of temporal relations classifiers	DATA MINING AND KNOWLEDGE DISCOVERY										Natural language processing; Temporal reasoning; Ensemble methods; Integer programming	KNOWLEDGE; NEWS	The extraction of temporal events from text and the classification of temporal relations among both temporal events and time expressions are major challenges for the interface of data mining and natural language processing. We present an ensemble method, which reconciles the outputs of multiple heterogenous classifiers of temporal expressions. We use integer programming, a constrained optimisation technique, to improve on the best result of any individual classifier by choosing consistent temporal relations from among those recommended by multiple classifiers. Our ensemble method is conceptually simple and empirically powerful. It allows us to encode knowledge about the structure of valid temporal expressions as a set of constraints. It obtains new state-of-the-art results on two recent natural language processing challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval), with F1 scores of 0.3915 and 0.595 respectively.																	1384-5810	1573-756X				MAR	2020	34	2					533	562		10.1007/s10618-019-00671-x		JAN 2020											
J								Metaheuristic anopheles search algorithm	EVOLUTIONARY INTELLIGENCE										Anopheles; Optimization; Informed search; Metaheuristic search; Continuous decision variable	OPTIMIZATION ALGORITHM; EVOLUTION	Today, various optimization problems have been solved using different optimization techniques such as linear programming, nonlinear programming and dynamic programming. These methods mainly try to find optimal solution in the proximity of starting point. Due to increase in complexity of optimization problems and number of optimal points, efficiency of these methods in finding global optima has decreased. The mentioned challenges encouraged researchers to introduce novel methods inspired by natural phenomena. In this study, Anopheles metaheuristic search algorithm which is inspired by transmission of Malaria disease by Anopheles mosquito is proposed. Our proposed method might be utilized in engineering optimization problems with continuous design variables. The algorithm explores search space using random search in order to eliminate unnecessary information. The efficiency of Anopheles optimization algorithm is evaluated using famous optimization problems. Also, the performance of the proposed method is compared with other optimization methods.																	1864-5909	1864-5917				SEP	2020	13	3					511	523		10.1007/s12065-019-00348-w		JAN 2020											
J								Anti-windup TS Fuzzy PI-like Control for Discrete-Time Nonlinear Systems with Saturated Actuators	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										TS fuzzy discrete-time systems; Anti-windup action; Local stabilization; PI-like controller design; Actuators saturation; Real-time experiments	TRACKING CONTROL; SPEED CONTROL; IMPLEMENTATION; STABILIZATION; DESIGN; MODEL	This paper deals with piecewise constant set-points tracking control of nonlinear discrete-time systems represented by Takagi-Sugeno models under actuators' saturation. To this end, a fuzzy Proportional Integral-like (PI-like) discrete-time control scheme is considered, which consists of a proportional state feedback, an integral action over the tracking error, and an anti-windup action. All the control gains are obtained through a convex optimization procedure formulated in term of Linear Matrix Inequalities (LMIs). The proposed method yields a Parameter Distributed Compensation (PDC) PI-like control and a non-PDC anti-windup action structure. Due to the actuators' saturation, a local approach is considered with a fuzzy Lyapunov function to ensure the local closed-loop stability, to provide an estimate of the region of attraction, and to compute the amplitude bounds of set-points changes. This latter issue allows delivering operational security by providing a bounded range for the set-points variation. To validate and illustrate the performance of the proposed tracking control approach, real-time experiments has been performed on an industrial oriented process consisting on the nonlinear level control of two interactive tanks.																	1562-2479	2199-3211				FEB	2020	22	1					46	61		10.1007/s40815-019-00781-0		JAN 2020											
J								Production scheduling problem in a factory of automobile component primer painting	JOURNAL OF INTELLIGENT MANUFACTURING										Flow shop scheduling; Automobile component primer painting; Mixed-integer programming; Heuristic algorithms	BATCH-PROCESSING MACHINES; NO-WAIT FLOWSHOP; DEPENDENT SETUP TIMES; TOTAL COMPLETION-TIME; MAKESPAN MINIMIZATION; 2-MACHINE FLOWSHOP; SHOP PROBLEM; TABU SEARCH; OPTIMIZATION; ALGORITHMS	In a factory of automobile component primer painting, various automobile parts are attached to overhead hangers in a conveyor line and undergo a series of coating processes. Thereafter, the components are wrapped at a packaging station. The packaging process should be fully balanced by an appropriate sequence of components to prevent the bottleneck effect because each component requires different packaging times and materials. An overhead hanger has a capacity limit and can hold varying numbers of components depending on the component type. Capacity loss can occur if the hanger capacity is not fully utilized. To increase hanger utilization, companies sometimes mix two or more component types on the same hangers, and these hangers are called mixed hangers. However, mixed hangers generally cause heavy workload because different items require additional setup times during hanging and packing processes. Hence, having many mixed hangers is not recommended. A good production schedule requires a small number of mixed hangers and maximizes hanger utilization and packaging workload balance. We show that the scheduling problem is NP-hard and develop a mathematical programming model and efficient solution approaches for the problem. When applying the methods to solve real problems, we also use an initial solution-generating method that minimizes the mixing cost, set a rule for hanging the items on hangers considering eligibility constraint, and decrease the size of tabu list in proportion to the remaining computational time for assuring intensification in the final iterations of the search. Experimental results demonstrate the effectiveness of the proposed approaches.																	0956-5515	1572-8145				AUG	2020	31	6					1483	1496		10.1007/s10845-019-01524-6		JAN 2020											
J								Path-Based Analysis for Structure-Preserving Image Filtering	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Structure-preserving image filtering; Distance transform; Gestalt principles of grouping	SCALE-SPACE; ALGORITHMS; CONNECTIVITY; TRANSFORM	Structure-preserving image filtering is an image smoothing technique that aims to preserve prominent structures while removing unwanted details in natural images. However, relevant studies mainly focus on small variances/fluctuations suppression and are vulnerable to separate pixels connected by some low-contrast edges or cluster pixels which exhibit strong differences between neighbors in highly textured region.Inspired by the fact that the human visual system significantly outperforms manually designed operators in extracting meaningful structures from natural scenes, we present an efficient structure-preserving filtering method which integrates similarity, proximity and continuation principles of human perception to accomplish high-contrast details (textures/noises) smoothing. Additionally, a Liebig's law of minimum-based distance transform is presented to seamlessly incorporate the three properties for the description of the filter kernel. Experiments demonstrate that our distance transform keeps a clustering-like manner of separating different image pixels and grouping similar ones with the awareness of structure. When integrating this affinity measure into the bilateral-filter-like framework, our method can efficiently remove high-contrast textures/noises while preserving major structures.																	0924-9907	1573-7683				FEB	2020	62	2					253	271		10.1007/s10851-019-00941-9		JAN 2020											
J								Model-based kernel sum rule: kernel Bayesian inference with probabilistic models	MACHINE LEARNING										Kernel methods; Probabilistic models; Kernel mean embedding; Kernel Bayesian inference; Reproducing kernel Hilbert spaces; Filtering; State space models	EMBEDDINGS	Kernel Bayesian inference is a principled approach to nonparametric inference in probabilistic graphical models, where probabilistic relationships between variables are learned from data in a nonparametric manner. Various algorithms of kernel Bayesian inference have been developed by combining kernelized basic probabilistic operations such as the kernel sum rule and kernel Bayes' rule. However, the current framework is fully nonparametric, and it does not allow a user to flexibly combine nonparametric and model-based inferences. This is inefficient when there are good probabilistic models (or simulation models) available for some parts of a graphical model; this is in particular true in scientific fields where "models" are the central topic of study. Our contribution in this paper is to introduce a novel approach, termed the model-based kernel sum rule (Mb-KSR), to combine a probabilistic model and kernel Bayesian inference. By combining the Mb-KSR with the existing kernelized probabilistic rules, one can develop various algorithms for hybrid (i.e., nonparametric and model-based) inferences. As an illustrative example, we consider Bayesian filtering in a state space model, where typically there exists an accurate probabilistic model for the state transition process. We propose a novel filtering method that combines model-based inference for the state transition process and data-driven, nonparametric inference for the observation generating process. We empirically validate our approach with synthetic and real-data experiments, the latter being the problem of vision-based mobile robot localization in robotics, which illustrates the effectiveness of the proposed hybrid approach.																	0885-6125	1573-0565				MAY	2020	109	5					939	972		10.1007/s10994-019-05852-9		JAN 2020											
J								Exponential Lag Synchronization and Global Dissipativity for Delayed Fuzzy Cohen-Grossberg Neural Networks with Discontinuous Activations	NEURAL PROCESSING LETTERS										Fuzzy Cohen-Grossberg neural networks; Discontinuous neuron activations; Exponential lag synchronization; Global dissipativity; Globally attractive set	LIMIT-CYCLES; PERIODIC SYNCHRONIZATION; STABILITY; SYSTEMS; CONVERGENCE; EXISTENCE	This paper investigates the qualitative behavior of a new class of fuzzy Cohen-Grossberg neural networks with discontinuous neuron activations and mixed delays. Roughly speaking, some novel sufficient conditions are established in order to demonstrate the global dissipativity and the exponential lag synchronization of the considered model by using the theory of Filippov systems and Lyapunov method. Finally, two examples are presented to show the effectiveness of the obtained results.																	1370-4621	1573-773X				APR	2020	51	2					1653	1676		10.1007/s11063-019-10169-8		JAN 2020											
J								Generating Text Sequence Images for Recognition	NEURAL PROCESSING LETTERS										Image generation; Text sequence images; Training data; Text recognition		Recently, methods based on deep learning have dominated the field of text recognition. With a large number of training data, most of them can achieve the state-of-the-art performances. However, it is hard to harvest and label sufficient text sequence images from the real scenes. To mitigate this issue, several methods to synthesize text sequence images were proposed, yet they usually need complicated preceding or follow-up steps. In this work, we present a method which is able to generate infinite training data without any auxiliary pre/post-process. We tackle the generation task as an image-to-image translation one and utilize conditional adversarial networks to produce realistic text sequence images in the light of the semantic ones. Some evaluation metrics are involved to assess our method and the results demonstrate that the caliber of the data is satisfactory. The code and dataset will be publicly available soon.																	1370-4621	1573-773X				APR	2020	51	2					1677	1688		10.1007/s11063-019-10166-x		JAN 2020											
J								Tree aggregation for random forest class probability estimation	STATISTICAL ANALYSIS AND DATA MINING										aggregation; class probability estimation; random forest	REGRESSION; ERROR	In random forest methodology, an overall prediction or estimate is made by aggregating predictions made by individual decision trees. Popular implementations of random forests rely on different methods for aggregating predictions. In this study, we provide an empirical analysis of the performance of aggregation approaches available for classification and regression problems. We show that while the choice of aggregation scheme usually has little impact in regression, it can have a profound effect on probability estimation in classification problems. Our study illustrates the causes of calibration issues that arise from two popular aggregation approaches and highlights the important role that terminal nodesize plays in the aggregation of tree predictions. We show that optimal choices for random forest tuning parameters depend heavily on the manner in which tree predictions are aggregated.																	1932-1864	1932-1872				APR	2020	13	2					134	150		10.1002/sam.11446		JAN 2020											
J								Optimum reference distance based path loss exponent determination for vehicle-to-vehicle communication	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Vehicle-to-vehicle communication; path loss exponent; reference distance; log-distance path loss model; dedicated short range communication	CHANNEL	Vehicle-to-vehicle (V2V) communication environment differs from classical wireless communication with respect to low antenna heights and high mobility. Therefore, V2V channel modeling based on real measurements is still crucial to get the channel parameters for the various road environments. One of the most extracted parameters from measurements is path loss exponent and selecting a fixed reference distance value in obtaining this parameter may also cause remarkable fitting errors. Thus, in this study, least square method-based approach for the best-fitted path loss exponent calculation was proposed by determining the optimum reference distance value from the V2V channel measurements. First, V2V channel measurements were carried out with commercially available DSRC OBU devices for six different highway scenarios in Gumushane, Turkey. Then, path loss exponents both for some fixed reference distance values in the literature and for the optimum reference distance values from our proposed approach were calculated and compared. Best-fitted path loss exponent values were obtained as 1.39-2.60 for the optimum reference distances determined using the proposed approach. The findings clearly show that the path loss exponents obtained with our proposed approach fit the measured data better than the results of fixed reference distance values.																	1300-0632	1303-6203					2020	28	5					2956	2967		10.3906/elk-1910-128													
J								Dual-Path Attention Network for Compressed Sensing Image Reconstruction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reconstruction; Optimization; Compressed sensing; Machine learning; Iterative methods; Periodic structures; Image coding; Compressed sensing; structure-texture decomposition; dual-path network; texture attention	ROBUST UNCERTAINTY PRINCIPLES; DECOMPOSITION; MINIMIZATION; ALGORITHMS; COMPONENT	Although deep neural network methods achieved much success in compressed sensing image reconstruction in recent years, they still have some issues, especially in preserving texture details. In this article, we propose a new dual-path attention network for compressed sensing image reconstruction, which is composed of a structure path, a texture path and a texture attention module. Motivated by the classical paradigm of image structure-texture decomposition, the structure path aims to reconstruct the dominant structure component of the original image, and the texture path targets at recovering the remaining texture details. To better bridge the information between two paths, the texture attention module is designed to deliver the useful structure information to the texture path and predict the texture region, thereby facilitating the recovery of texture details. Two paths are optimized with a unified loss function. In the testing phase, given the measurement vector of a new image, it can be well reconstructed by carrying out the well trained dual-path attention network and integrating the outputs of the structure path and the texture path. Experimental results on the SET5, SET11 and BSD68 testing datasets demonstrate that the proposed method achieves comparable or better results compared with some state-of-the-art deep learning based methods and conventional iterative optimization based methods in terms of reconstruction quality and robustness to noise.																	1057-7149	1941-0042					2020	29						9482	9495		10.1109/TIP.2020.3023629													
J								Boundary-Aware RGBD Salient Object Detection With Cross-Modal Feature Sampling	IEEE TRANSACTIONS ON IMAGE PROCESSING										Salient object detection; cross-modal; boundary-aware estimation	NEURAL-NETWORK	Mobile devices usually mount a depth sensor to resolve ill-posed problems, like salient object detection on cluttered background. The main barrier of exploring RGBD data is to handle the information from two different modalities. To cope with this problem, in this paper, we propose a boundary-aware cross-modal fusion network for RGBD salient object detection. In particular, to enhance the fusion of color and depth features, we present a cross-modal feature sampling module to balance the contribution of the RGB and depth features based on the statistics of their channel values. In addition, in our multi-scale dense fusion network architecture, we not only incorporate edge-sensitive losses to preserve the boundary of the detected salient region, but also refine its structure by merging the estimated saliency maps of different scales. We accomplish the multi-scale saliency map merging using two alternative methods which produce refined saliency maps via per-pixel weighted combination and an encoder-decoder network. Extensive experimental evaluations demonstrate that our proposed framework can achieve the state-of-the-art performance on several public RGBD-based datasets.																	1057-7149	1941-0042					2020	29						9496	9507		10.1109/TIP.2020.3028170													
J								Joint Raindrop and Haze Removal From a Single Image	IEEE TRANSACTIONS ON IMAGE PROCESSING										Rain; Shape; Artificial neural networks; Generative adversarial networks; Gallium nitride; Visualization; Atmospheric modeling; Raindrop removal; haze removal; generative adversarial network; visual attention	QUALITY ASSESSMENT	In a recent study, it was shown that, with adversarial training of an attentive generative network, it is possible to convert a raindrop degraded image into a relatively clean one. However, in real world, raindrop appearance is not only formed by individual raindrops, but also by the distant raindrops accumulation and the atmospheric veiling, namely haze. Current methods are limited in extracting accurate features from a raindrop degraded image with background scene, the blurred raindrop regions, and the haze. In this paper, we propose a new model for an image corrupted by the raindrops and the haze, and introduce an integrated multi-task algorithm to address the joint raindrop and haze removal (JRHR) problem by combining an improved estimate of the atmospheric light, a modified transmission map, a generative adversarial network (GAN) and an optimized visual attention network. The proposed algorithm can extract more accurate features for both sky and non-sky regions. Experimental evaluation has been conducted to show that the proposed algorithm significantly outperforms state-of-the-art algorithms on both synthetic and real-world images in terms of both qualitative and quantitative measures.																	1057-7149	1941-0042					2020	29						9508	9519		10.1109/TIP.2020.3029438													
J								A Novel Active Contour Model for Noisy Image Segmentation Based on Adaptive Fractional Order Differentiation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Computational modeling; Adaptation models; Active contours; Level set; Numerical models; Mathematical model; Image segmentation; active contour model; fractional order differentiation; level set; variational method	LEVEL SET EVOLUTION; CLASSIFICATION; SNAKES; DRIVEN	The images used in various practices are often disturbed by noise, such as Gaussian noise, speckled noise, and salt and pepper noise. Images with noise are one of the challenges for segmentation, since the noise may cause inaccurate segmented results. To cope with the effect of noise on images during segmentation, a novel active contour model is proposed in this paper. The newly proposed model consists of fitting term, regularization term and penalty term. The fitting term is designed using a Gaussian kernel function and fractional order differentiation with an adaptively defined fractional order, which applies different orders to different pixels. The regularization term is applied to maintain the smoothness of curves. In order to ensure stable evolution of curves, a penalty term is added into the proposed model. Comparison experiments are conducted to show the effectiveness and efficiency of the proposed model.																	1057-7149	1941-0042					2020	29						9520	9531		10.1109/TIP.2020.3029443													
J								From traceability to provenance of agricultural products through blockchain	WEB INTELLIGENCE										Internet plus; agricultural products supply chain; traceability; blockchain; provenance; food safety	SUPPLY CHAIN MANAGEMENT; SYSTEM; DESIGN; SAFETY; ISSUES	As China's agricultural output has improved, the national and local monitoring system of agricultural product safety has become much better, and monitoring standards have become increasingly strict. Despite this, there are agricultural product safety incidents which have caused consumer panic. One way to address this is by properly establishing tracking systems so that agricultural product logistics in China can be tracked and monitored. We explored this research objective with agricultural traceability and security in mind. One option that could be considered is the blockchain technology. Blockchain could also be used to ascertain the provenance of agricultural products to increase the quality and safety of the Chinese agricultural supply chain. In this context, this research converged on big data and technology, platforms and other means for product quality and safety of agricultural products traceability. In order to verify the accuracy of these three convergence, regression analysis were used to construct five models for verification of three hypothesis. The results show that based on "Internet+", using big data, big technology and big platform can significantly increase the accuracy of agricultural products traceability system hence improve consumer acceptance of the safety of agricultural products.																	2405-6456	2405-6464					2020	18	3					181	189		10.3233/WEB-200440													
J								Exploring propagation factors of social media moods for stock prices prediction	WEB INTELLIGENCE										Weibo; sentiment analysis; propagation effect; stock prediction		Weibo, the most widely-used social media in China, makes researchers highly regard its profound impact in public and gather moods for social computing and analysis, such as financial prediction. Most existing literatures concern excessively on text semantic or sentiment mining techniques, but neglect the procedure of moods dissemination and its factors. This paper proposes an integrated framework of social media moods mining, which creatively focuses on information transmission and propagating factors analysis, to predict stock prices more accurately. For the part of propagating factors on social media, several essential factors are distinguished in the dissemination process, such as emotional absorption of forwarding, influence of content and poster, user categories, release time, etc. to optimize the fitting effect of original model. And the count of forwarding also matters on predicting stock prices. Searching a given finance-related keyword, from Weibo we collected over 500,000 micro-blogs and their user information. Then we adopt the proposed integrated framework to predict stock price fluctuation, as well as the simple neural network method. Experiments demonstrate that the former outperformed the latter. The results also show that user categories and the count of forwarding differ on the lag phase of influence. And more, this paper studies the fitting effect of prediction models for different periods of the stock curve. The results indicate that the model works the best in the rising periods of stock prices curves, relatively well in the declining and the worst in the random fluctuating.																	2405-6456	2405-6464					2020	18	3					191	204		10.3233/WEB-200441													
J								A survey on text classification and its applications	WEB INTELLIGENCE										Text classification; text mining; feature selection; machine learning	FEATURE-SELECTION; FEATURES; SUPPORT	Text classification (a.k.a text categorisation) is an effective and efficient technology for information organisation and management. With the explosion of information resources on the Web and corporate intranets continues to increase, it has being become more and more important and has attracted wide attention from many different research fields. In the literature, many feature selection methods and classification algorithms have been proposed. It also has important applications in the real world. However, the dramatic increase in the availability of massive text data from various sources is creating a number of issues and challenges for text classification such as scalability issues. The purpose of this report is to give an overview of existing text classification technologies for building more reliable text classification applications, to propose a research direction for addressing the challenging problems in text mining.																	2405-6456	2405-6464					2020	18	3					205	216		10.3233/WEB-200442													
J								Image inspired Chinese couplet generation	WEB INTELLIGENCE										Convolution neural network; machine translation; text summary generation; encoder-decoder; couplet		Chinese couplets, as one of the traditional Chinese culture, is the treasure of Chinese civilization and the inheritance of Chinese history. Given a sentence (namely an antecedent clause), people reply with another sentence (namely a subsequent clause) equal in length. Because of the complexity of the semantic and grammatical rules of couplet, it is not easy to create a suitable couplet that meets the requirements of sentence pattern, context, and flatness. With the development of neural models and natural language processing, automatic generation of Chinese couplets has drawn significant attention due to its artistic and cultural value, most of these works mainly focus on generating couplet by given text information, while visual inspirations for couplet generation have been rarely explored. In this paper, we design a Chinese couplet generation model based on NIC (Neural Image Caption), which can compose a piece of couplet suitable to the artistic conception in an image. At first, we use the improved VGG16 model to predict the input image. The content of the image can be automatically recognized and the corresponding description are generated and translated into Chinese keywords. Then, the encoder-decoder framework is used repeatedly to process these keywords, and finally the couplet can be generated. Moreover, to satisfy special characteristics of couplets, we incorporate the attention mechanism into the encoding-decoding process, which greatly improves the accuracy of couplets generated automatically.																	2405-6456	2405-6464					2020	18	3					217	227		10.3233/WEB-200443													
J								ITIL process management to mitigate operations risk in cloud architecture infrastructure for banking and financial services industry	WEB INTELLIGENCE										Incident management; change management; operational control; information technology infrastructure library (ITIL); emergency change		Banking and Financial Services Corporations need to update information systems with logic and data of various appli-cations on everyday basis to remain consistent with change in economy and business activity. This helps to work with latest information included in information system available in current economic and business scenario. This enables information systems and empowers work force to complete tasks in rapidly changing flow of monetary resources. With several employees in Banking and Financial Services Corporations using cloud infrastructure and reporting incidents arising in using cloud infrastructure, it is of prime importance to fix incidents reported within specific timelines. IT change management process is followed in order to adhere to IT governance & compliance framework and reduce risk of failure while performing changes in cloud infrastructure. With incident management and change management processes are aligned to keep cloud infrastructure available and secure, they become integral part of IT operations everyday activity. To make ITIL processes efficient, further organization specific policies are developed. With global standards and organization level controls in place, there are failures in IT incident and change management processes and implementation. In this paper, we have identified the risk arising due to incident management and change management processes that lead to emergency changes being implemented on cloud infrastructure architecture and discussed the steps to mitigate risks to bring greater responsibility and accountability for cloud services providers.																	2405-6456	2405-6464					2020	18	3					229	238		10.3233/WEB-200444													
J								Document classification using convolutional neural networks with small window sizes and latent semantic analysis	WEB INTELLIGENCE										Convolutional neural networks; document classification; latent semantic analysis; word embedding; word vectors		A parsimonious convolutional neural network (CNN) for text document classification that replicates the ease of use and high classification performance of linear methods is presented. This new CNN architecture can leverage locally trained latent semantic analysis (LSA) word vectors. The architecture is based on parallel 1D convolutional layers with small window sizes, ranging from 1 to 5 words. To test the efficacy of the new CNN architecture, three balanced text datasets that are known to perform exceedingly well with linear classifiers were evaluated. Also, three additional imbalanced datasets were evaluated to gauge the robustness of the LSA vectors and small window sizes. The new CNN architecture consisting of 1 to 4-grams, coupled with LSA word vectors, exceeded the accuracy of all linear classifiers on balanced datasets with an average improvement of 0.73%. In four out of the total six datasets, the LSA word vectors provided a maximum classification performance on par with or better than word2vec vectors in CNNs. Furthermore, in four out of the six datasets, the new CNN architecture provided the highest classification performance. Thus, the new CNN architecture and LSA word vectors could be used as a baseline method for text classification tasks.																	2405-6456	2405-6464					2020	18	3					239	248		10.3233/WEB-200445													
J								Fuzzy wavelet neural networks applied as inferential sensors of neonatal incubator dynamics	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy wavelet neural networks; inferential sensors; neonatal incubators; system identification	IDENTIFICATION; PREDICTION; MODEL	Newborns with health complications have great difficulty in regulating the body temperature due to distinct factors, which include the high metabolism rate and low weight. In this context, neonatal incubators help maintaining good health conditions because they provide a thermally-neutral environment, which is adequate to ensure the least energy expenditure by the newborn. In the last decades, artificial neural networks (ANNs) have been established as one of the main tools for the identification of nonlinear systems. Among the various approaches used in the identification process, the fuzzy wavelet neural network (FWNN) can be regarded as a prominent technique, consisting of the combination of wavelet neural network (WNN) and adaptive network-based fuzzy inference system (ANFIS). This work proposes the use of FWNN to infer the temperature and humidity values inside the incubator in order to certify the equipment operation. Results obtained with the analyzed neural system have shown the generalization and inference capacities of FWNNs, thus allowing their application to practical tasks aiming to increase the efficiency of incubators.																	1064-1246	1875-8967					2020	39	3					2567	2579		10.3233/JIFS-190129													
J								A VIKOR-based framework to optimize the location of fast-charging stations with proportional hesitant fuzzy information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-criteria group decision making; proportional hesitant fuzzy set; distance measure; VIKOR; maximizing; deviation method	GROUP DECISION-MAKING; LINGUISTIC TERM SETS; SELECTION; MODEL; OPERATORS; STRATEGY	In this study, a multi-criteria group decision making (MCGDM) framework is constructed for electric vehicle fast-charging station (EVFCS) selection using a proportional hesitant fuzzy set (PHFS) that can describe two aspects of information: the possible membership degrees in the hesitant fuzzy elements and associated proportion representing sta-tistical information from different groups. A newly extended distance measure for PHFSs is introduced and an extended maximizing deviation method is constructed to obtain criteria weights objectively. Accordingly, an integrated PHFS-VIKOR (VlseKriterijum-ska Optimizacija I Kompromisno Resenje) method embedded with a new distance measure and extended maximizing deviation method is presented. With increasing concerns about range anxiety, it is essential to seek an optimal location for EVFCS considering efficient utilization of resources and long-term development of socio-economy under proportional hesitant fuzzy environment. Lastly, an illustration with sensitivity analysis and comparative analyses is provided to demonstrate the validity and robustness of our proposal.																	1064-1246	1875-8967					2020	39	3					2581	2596		10.3233/JIFS-190156													
J								Stability and monotony dependence of second-order fuzzy functional systems in partially ordered metric spaces	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy functional DEs; gH-derivatives; ordered partial metric spaces	DIFFERENTIAL-EQUATIONS	In this paper, we consider Cauchy problems for second order fuzzy functional differential equations (DEs) with generalized Hukuhara (gH) derivatives. We study the solvability of the problem by using Perov fixed point theorem in ordered partial metric spaces. The data monotony, continuity, diferentiability dependence of mild solutions with respect to parameters are investigated via weak Picard operators. Moreover, the stability of mild solutions is addressed in sense of Ulam-Hyers stability related to the technique of coefficient matrix converges to zero. Some examples are presented to demonstrate for theoretical results.																	1064-1246	1875-8967					2020	39	3					2597	2610		10.3233/JIFS-190222													
J								An efficient method for determining the optimal convolutional neural network structure based on Taguchi method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network; hyperparameter combination; optimization algorithm; Taguchi method	OPTIMIZATION; CLASSIFICATION; PARAMETERS; DESIGN; ART	Structure of convolutional neural network (CNN) applied for image recognition requires large numbers of tuning for designated datasets in practice. It is a time-consuming process to finally come up with a feasible structure for specific requirement. This paper proposes a method based on Taguchi method which can efficiently determine the optimal structure of hyperparameters combination. Five hyperparameters with four levels are defined as control factors and two indicators are chosen to measure the performance of CNN structure. L16(45) orthogonal array is used to arrange the experiment. S/N ratio and main effect plot are used to identify the optimal structure (hyperparameter combination) of CNN. The classic case of MNIST is employed to verify the practicability of the proposed method. Results show that the proposed method can identify the optimal CNN structure efficiently and also rank the significance priority of hyperparameters.																	1064-1246	1875-8967					2020	39	3					2611	2625		10.3233/JIFS-190275													
J								Interval probability hesitant fuzzy linguistic analytic hierarchy process and its application in talent selection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval probability hesitant fuzzy linguistic variable; interval probability hesitant fuzzy linguistic analytic hierarchy process; general geometric consistency index; possibility degree; reliability	DECISION-MAKING; TERM SETS; MULTIPLICATIVE CONSISTENCY; AHP; IMPROVEMENT	Hesitant fuzzy linguistic term set (HFLTS) can handle the qualitative and hesitant information in multiple attribute decision making (MADM) problems which are widely used in various fields. However, the experts' evaluation of information is not completely reliable in the situation where their own knowledge background is insufficient. In order to deal with deviations due to incomplete reliability of the evaluation, this paper first proposes the interval probability hesitant fuzzy linguistic variable (IPHFLV), which takes the HFLTS as the evaluation part and adds a novel element-reliability of evaluation, thus can describe the different credibility of information evaluation due to the familiarity of experts with schemes and the differences in knowledge cognition. The operation rules and comparison methods are also illustrated. Particularly, under the inspiration of probability theory, we propose the possibility degree of the IPHFLVs. Then we propose IPHFL-AHP based on the AHP and interval probability hesitant fuzzy linguistic variable. Especially, the general geometric consistency index (G-GCI) based on the unbiased estimator of the variance is presented to measure the consistency and the iterative algorithm is constructed to improve the consistency. We use the possibility degree to calculate the priority vector to acquire the total ranking and introduce the process of IPHFL-AHP. Finally, case study of talent selection is given to illustrate the effectiveness and feasibility of the proposed method.																	1064-1246	1875-8967					2020	39	3					2627	2645		10.3233/JIFS-190427													
J								A new rough set based Bayesian classifier prior assumption	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Rough set theory; prior assumption; Bayesian classifier; approximation quality; probability theory	3-WAY DECISIONS; SELECTION; RULES	Aiming at the imprecise and uncertain data and knowledge, this paper proposes a novel prior assumption by the rough set theory. The performance of the classical Bayesian classifier is improved through this study. We applied the operations of approximations to represent the imprecise knowledge accurately, and the concept of approximation quality is first applied in this method. Thus, this paper provides a novel rough set theory based prior probability in classical Bayesian classifier and the corresponding rough set prior Bayesian classifier. And we chose 18 public datasets to evaluate the performance of the proposed model compared with the classical Bayesian classifier and Bayesian classifier with Dirichlet prior assumption. Sufficient experimental results verified the effectiveness of the proposed method. The mainly impacts of our proposed method are: firstly, it provides a novel methodology which combines the rough set theory with the classical probability theory; secondly, it improves the accuracy of prior assumptions; thirdly, it provides an appropriate prior probability to the classical Bayesian classifier which can improve its performance only by improving the accuracy of prior assumption and without any effect to the likelihood probability; fourthly, the proposed method provides a novel and effective method to deal with the imprecise and uncertain data; last but not least, this methodology can be extended and applied to other concepts of classical probability theory, which providing a novel methodology to the probability theory.																	1064-1246	1875-8967					2020	39	3					2647	2655		10.3233/JIFS-190517													
J								Sensorless speed control of SPIM using BS_PCH novel control structure and NNSM_SC MRAS speed observer	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Neural networks; sensorless vector control; six phase induction motor drive; stator current MRAS based on speed observer; backstepping control; port controlled hamiltonian	SLIDING-MODE CONTROL; STRICT-FEEDBACK SYSTEMS; FIELD-ORIENTED CONTROL; NONLINEAR-SYSTEMS; INDUCTION-MOTORS; VECTOR CONTROL; IMPLEMENTATION; MACHINES	This paper propose a novel Port Controlled Hamiltonian Backstepping (PCH BS) control structure with online tuned parameters, in combination with the modified Stator Current Model Reference Adaptive Syatem (SC MRAS) based on speed and flux estimator using Neural Networks(NN) and sliding mode (SM) for sensorless vector control of the six phase induction motor (SPIM). The control design is based on combination PCH and BS techniques to improve its performance and robustness. The combination of BS PCH controller with speed estimator can compensate for the uncertainties caused by the machine parameter variations, measurement errors, and external load disturbances, enables very good static and dynamic performance of the sensorless drive system (perfect tuning of the speed reference values, fast response of the motor current and torque, high accuracy of speed regulation) in a wide speed range, and robust for the disturbances of the load, the speed variation and low speed. The proposed sensorless speed control scheme is validated through Matlab-Simulink. The simulation results verify the effectiveness of the proposed control and observer.																	1064-1246	1875-8967					2020	39	3					2657	2677		10.3233/JIFS-190540													
J								Parallel CNN based big data visualization for traffic monitoring	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network; deep learning; traffic data visualization; traffic prediction	SYSTEM; PREDICTION; MODEL; NETWORKS	In a real-time application such as traffic monitoring, it is required to process the enormous amount of data. Traffic prediction is essential for intelligent transportation systems (ITSs), traffic management authorities, and travelers. Traffic prediction has become a challenging task due to various non-linear temporal dynamics at different locations, complicated underlying spatial dependencies, and more extended step forecasting. To accommodate these instances, efficient visualization and data mining techniques are required to predict and analyze the massive amount of traffic big data. This paper presents a deep learning-based parallel convolutional neural network (Parallel-CNN) methodology to predict the traffic conditions of a specific region. The methodology of deep learning contains multiple processing layers and performs various computational strategies, which is used to learn representations of data with multilevel abstraction. The data has captured from the department of transportation; thus, the size of data is vast, and it can be analyzed to get the behavior of the traffic condition. The purpose of this paper is to monitor traffic behavior, which enables the user to make decisions to build the traffic-free cities. Experimental results show that the proposed methodology outperforms other existing methods such as KNN, CNN, and FIMT-DD.																	1064-1246	1875-8967					2020	39	3					2679	2691		10.3233/JIFS-190601													
J								A lightweight vehicle detection and tracking technique for advanced driving assistance systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Computer vision; self-driving car; autonomous driving; ADAS; vehicle detection; vehicle tracking	TRAFFIC SIGNS	In this paper, an advanced-and-reliable vehicle detection-and-tracking technique is proposed and implemented. The Real-TimeVehicle Detection-and-Tracking (RT_VDT) technique is well suited for Advanced Driving Assistance Systems (ADAS) applications or Self-Driving Cars (SDC). The RT_VDT is mainly a pipeline of reliable computer vision and machine learning algorithms that augment each other and take in raw RGB images to produce the required boundary boxes of the vehicles that appear in the front driving space of the car. The main contribution of this paper is the careful fusion of the employed algorithms where some of them work in parallel to strengthen each other in order to produce a precise and sophisticated real-time output. In addition, the RT_VDT provides fast enough computation to be embedded in CPUs that are currently employed by ADAS systems. The particulars of the employed algorithms together with their implementation are described in detail. Additionally, these algorithms and their various integration combinations are tested and their performance is evaluated using actual road images, and videos captured by the front-mounted camera of the car as well as on the KITTI benchmark with 87% average precision achieved. The evaluation of the RT_VDT shows that it reliably detects and tracks vehicle boundaries under various conditions.																	1064-1246	1875-8967					2020	39	3					2693	2710		10.3233/JIFS-190634													
J								Scale prediction based MDNet for infrared target tracking	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial intelligence; infrared target tracking; convolutional neural network; scale prediction; Engineering		Infrared target tracking is increasingly becoming important for various applications in recent years. However, it still a challenging task as limited information can be obtained from the infrared image. Inspired by the excellent performance of deep tracker, a novel tracker based on MDNet is proposed. As the prior information has great value for target tracking, modified Back-Propagation network is used for predicting the scale of target during tracking. The result of the prediction used for generating candidate windows for online learning, which can improve the performance of tracker. To evaluate the proposed tracking algorithm, we performed experiments on the VOT-TIR2016 and AMCOM infrared data. The experimental results demonstrate that our algorithm provides a 1.94% relative gain in accuracy and 21.4% in robustness on VOT-TIR2016 when compared with MDNet.																	1064-1246	1875-8967					2020	39	3					2711	2723		10.3233/JIFS-190787													
J								A comparative analysis of information provision strategies for parking variable message sign display problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intelligent transportation system; parking VMS; display problem; provision strategies; agent-based simulation	RESERVATION; SIMULATION; GUIDANCE; SYSTEMS; SEARCH; MODEL; POLICY	Parking spaces are insufficient and are plagued by over-consumption in hot areas. To assist drivers easily in identifying available parking spaces, parking variable message signs are commonly adopted to display information on space availability. This paper analyzes the performance of various information provision strategies. To achieve this objective, we first present the mechanisms of the information provision strategies. Then, the information provision strategies are classified into three categories: regular, symmetric, and discriminative. The regular strategies provide the collected parking information directly to drivers; the symmetric schemes employ the equal threshold values for all parking lots; and the discriminative schedules adopt an independent threshold value for each parking lot. The threshold value provides an upper limit for the Space Occupancy Percentage (SOP): when the SOP is larger than the threshold value, the parking lot status becomes FULL; otherwise, it is displayed having available spaces. Finally, an agent-based simulation model is introduced to describe the parking and traffic conditions. The results indicate that both the symmetric and discriminative strategies significantly decrease the highest failure rate and average travel time, whereas the latter performs better. The results of this comparative analysis can assist in the configuration and operation of an urban parking guidance and information system.																	1064-1246	1875-8967					2020	39	3					2725	2735		10.3233/JIFS-190962													
J								Existence of an equilibrium for pure exchange economy with fuzzy preferences	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pure exchange economy; fuzzy preference; fuzzy utility function; fuzzy competitive equilibrium; quasi-variational; inequality	SYSTEMS; GAMES	This paper focuses on a new model to reach the existence of equilibrium in a pure exchange economy with fuzzy preferences (PXE-FP). The proposed model integrates exchange, consumption and the agent's fuzzy preference in the consumption set. We set up a new fuzzy binary relation on the consumption set to evaluate the fuzzy preferences. Also, we prove that there exists a continuous fuzzy order-preserving function in the consumption set under certain conditions. The existence of a fuzzy competitive equilibrium for the PXE-FP is confirmed through a new result on the existence of fuzzy Nash equilibrium for fuzzy non-cooperative games. The payoffs of all strategy profiles for any agent are fuzzy numbers in fuzzy non-cooperative games. Finally, we show that the fuzzy competitive equilibrium could be characterized as a solution to an associated-variational rise to an solution.																	1064-1246	1875-8967					2020	39	3					2737	2752		10.3233/JIFS-191011													
J								A novel ranking function-based triangular intuitionistic fuzzy fault tree analysis method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fault tree analysis; triangular intuitionistic numbers; ranking function; bottom event	INTEGRATION; NUMBERS; MODEL	In reliability field, the probabilities of basic events are often treated as exact values in conventional fault tree analysis. However, for many practical systems, because the concept of events may be ambiguous, the factors affecting the occurrence of events are complex and changeable, so it is difficult to obtain accurate values of the occurrence probability of events. Fuzzy sets can well deal with these situations. Thus this paper will develop a novel fault tree analysis method in the assumption of the values of probability of basic events expressed with triangular intuitionistic fuzzy numbers. First, a new ranking function of triangular intuitionistic numbers is established, which can reflect the behavior factors of the decision maker. Then a novel fault tree analysis method is put forward on the basis of operational laws and the proposed ranking function of triangular intuitionistic numbers. Finally, an example of weapon system "automatic gun" is employed to show that the proposed fault tree analysis method is feasible and effective.																	1064-1246	1875-8967					2020	39	3					2753	2761		10.3233/JIFS-191018													
J								Multi-task learning model for aspect term extraction and aspect polarity classification based on dual-labels	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-task learning; aspect term extraction; aspect polarity classification; sentiment classification		Aspect-based sentiment analysis (ABSA) is a hot and significant task of natural language processing, which is composed of two subtasks, the aspect term extraction (ATE) and aspect polarity classification (APC). Previous researches generally studied two subtasks independently and designed neural network models for ATE and APC respectively. However, it integrates various manual features into the model, which will consume plenty of computing resources and labor. Moreover, the quality of the ATE results will affect the performance of APC. This paper proposes a multi-task learning model based on dual auxiliary labels for ATE and APC. In this paper, general IOB labels, and sentimental IOB labels are equipped to efficiently solve both ATE and APC tasks without manual features adopted. Experiments are conducted on two general ABSA benchmark datasets of SemEval-2014. The experimental results reveal that the proposed model is of great performance and efficient for both ATE and APC tasks compared to the main baseline models.																	1064-1246	1875-8967					2020	39	3					2763	2774		10.3233/JIFS-191047													
J								Certain fuzzy hyperstructures from a fuzzy set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hypergroup; fuzzy sets; fuzzy hypergroup		Fuzzy set theory and also the hypergroups in the sense of Marty are both generalizations of some existing mathematical concepts which are used for modeling many real life situations. The main purpose of this paper is the study of the link between fuzzy sets and fuzzy hypergroups and fuzzy semihypergroups. As a matter of fact, some commutative fuzzy hypergroups and fuzzy semihypergroups have been constructed from fuzzy set and some of their properties were investigated.																	1064-1246	1875-8967					2020	39	3					2775	2782		10.3233/JIFS-191054													
J								The construction of multi-granularity concept lattices	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-granularity; formal concept analysis; formal concept; formal concept lattice		The construction of concept lattices is an important research topic in formal concept analysis. Inspired by multigranularity rough sets, multi-granularity formal concept analysis has become a new hot research issue. This paper mainly studies the construction methods of concept lattices in multi-granularity formal context. The relationships between concept forming operators under different granularity are discussed. The mutual transformation methods of formal concepts under different granularity are presented. In addition, the approaches of obtaining coarse-granularity concept lattice by fine-granularity concept lattice and fine-granularity concept lattice by coarse-granularity concept lattice are examined. The related algorithms for generating concept lattices are proposed. The practicability of the method is illustrated by an example.																	1064-1246	1875-8967					2020	39	3					2783	2790		10.3233/JIFS-191090													
J								Research on speech emotion recognition algorithm for unbalanced data set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Speech emotion recognition; spectrograms; CRNN; focal loss		In speech emotion recognition, most emotional corpora generally have problems such as inconsistent sample length and imbalance of sample categories. Considering these problems, in this paper, a variable length input CRNN deep learning model based on Focal Loss is proposed for speech emotion recognition of anger, happiness, neutrality and sadness in IEMOCAP emotional corpus. In this model, Firstly, a variable-length strategy is introduced to input the speech spectra of the filled speech samples into CNN. Then the effective part of the input sequence is preserved and output by masking matrix and convolution layer. Thirdly, the effective output of input sequence is input into BiGRU network for learning. Finally, the focal loss is used for network training to control and adjust the contribution of various samples to the total loss. Compared with the traditional speech emotion recognition model, simulations show that our method can effectively improve the accuracy and performance of emotion recognition.																	1064-1246	1875-8967					2020	39	3					2791	2796		10.3233/JIFS-191129													
J								Certain models of granular computing based on rough fuzzy approximations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Rough fuzzy approximations; rough fuzzy digraphs; information granulation; algorithms	INFORMATION GRANULATION; SOFT SETS; HYPERGRAPHS; GRAPHS	An extraction of granular structures using graphs is a powerful mathematical framework in human reasoning and problem solving. The visual representation of a graph and the merits of multilevel or multiview of granular structures suggest the more effective and advantageous techniques of problem solving. In this research study, we apply the combinative theories of rough fuzzy sets and rough fuzzy digraphs to extract granular structures. We discuss the accuracy measures of rough fuzzy approximations and measure the distance between lower and upper approximations. Moreover, we consider the adjacency matrix of a rough fuzzy digraph as an information table and determine certain indiscernible relations. We also discuss some general geometric properties of these indiscernible relations. Further, we discuss the granulation of certain social network models using rough fuzzy digraphs. Finally, we develop and implement some algorithms of our proposed models to granulate these social networks.																	1064-1246	1875-8967					2020	39	3					2797	2816		10.3233/JIFS-191165													
J								A learning automata-based algorithm to solve imbalanced k-coverage in visual sensor networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Visual sensor networks; balanced coverage; k-coverage; learning automata	TARGET COVERAGE; LIFETIME; ISSUES	One of the most important problems in directional sensor networks is k-coverage in which the orientation of a minimum number of directional sensors is determined in such a way that each target can be monitored at least k times. This problem has been already considered in two different environments: over provisioned where the number of sensors is enough to cover all targets, and under provisioned where there are not enough sensors to do the coverage task (known as imbalanced k-coverage problem). Due to the significance of solving the imbalanced k-coverage problem, this paper proposes a learning automata (LA)-based algorithm capable of selecting a minimum number of sensors in a way to provide k-coverage for all targets in a balanced way. To evaluate the efficiency of the proposed algorithm performance, several experiments were conducted and the obtained results were compared to those of two greedy-based algorithms. The results confirmed the efficiency of the proposed algorithm in terms of solving the problem.																	1064-1246	1875-8967					2020	39	3					2817	2829		10.3233/JIFS-191170													
J								Congrunce lattices of rectangular lattices	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Distributive lattice; Congruence lattice; Rectangular lattice	CONGRUENCE LATTICES; REPRESENTATION	Let D be a finite distributive lattice with n join-irreducible elements. It is well-known that D can be represented as the congruence lattice of a rectangular lattice L which is a special planer semimodular lattice. In this paper, we shall give a better upper bound for the size of L by a function of n, improving a 2009 result of G. Gra center dot tzer and E. Knapp.																	1064-1246	1875-8967					2020	39	3					2831	2843		10.3233/JIFS-191220													
J								Switching function based on hypergraphs with algorithm and python programming	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Switching function; hypergraphable Boolean function; Boolean functionable hypergraph; Boolean function-based hypergraph; Unitor; TBT	GRAPHS	According to Boolean logic, a disjunctive normal form (DNF) is a canonical normal form of a logical formula consisting of a disjunction of conjunctions (it can also be described as an OR of AND's). For each table an arbitrary T.B.T is given (total binary truth table) Boolean expression can be written as a disjunctive normal form. This paper considers a notation of a T.B.T, introduces a new concept of the hypergraphable Boolean functions and the Boolean functionable hypergraphs with respect to any given T.B.T. This study defines a notation of unitors set on switching functions and proves that every T.B.T corresponds to a minimum Boolean expression via unitors set and presents some conditions on a T.B.T to obtain a minimum irreducible Boolean expression from switching functions. Indeed, we generate a switching function in different way via the concept of hypergraphs in terms of Boolean expression in such a way that it has a minimum irreducible Boolean expression, for every given T.B.T. Finally, an algorithm is presented. Therefore, a Python programming(with complete and original codes) such that for any given T.B.T, introduces a minimum irreducible switching expression.																	1064-1246	1875-8967					2020	39	3					2845	2859		10.3233/JIFS-191230													
J								Workload prediction of cloud computing based on SVM and BP neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Back propagation neural network; support vector machine; cloud computing; workload prediction	MODEL	In this study, support vector machine (SVM) and back-propagation (BP) neural networks were combined to predict the workload of cloud computing physical machine, so as to improve the work efficiency of physical machine and service quality of cloud computing. Then, the SVM and BP neural network was simulated and analyzed in MATLAB software and compared with SVM, BP and radial basis function (RBF) prediction models. The results showed that the average error of the SVM and BP based model was 0.670%, and the average error of SVM, BP and RBF was 0.781%, 0.759% and 0.708%, respectively; in the multi-step prediction, the prediction accuracy of SVM, BP, RBF and SVM + BP in the first step was 89.3%, 94.6%, 96.3% and 98.5%, respectively, the second step was 87.4%, 93.1%, 95.2% and 97.8%, respectively, the third step was 83.5%, 90.3%, 93.1% and 95.7%, the fourth step was 79.1%, 87.4%, 90.5% and 93.2%, respectively, the fifth step was 75.3%, 81.3%, 85.9% and 91.1% respectively, and the sixth step was 71.1%, 76.6%, 82.1% and 89.4%, respectively.																	1064-1246	1875-8967					2020	39	3					2861	2867		10.3233/JIFS-191266													
J								Using FAHP and CBR to evaluate the intention of adoption of internet banking service: The example of web ATM	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy analytic hierarchy process; case-based reasoning; web ATM; innovation adoption	ANALYTIC HIERARCHY PROCESS; FUZZY-AHP; MODEL; SYSTEM	This research combines the Fuzzy Analytic Hierarchy Process (FAHP) with Case-Based Reasoning (CBR) to evaluate the intention of adoption of web ATM services. Compared with physical ATM service, web ATM allows users to perform financial transactions over the internet conveniently. Based on literature and considering the characteristics of web ATM, this study constructs a model for web ATM adoption that comprises three dimensions: The knowledge, the potential value, and the security. 222 valid user questionnaires are collected, and factor analysis is used to verify the factor structure of the decision hierarchy. FAHP is then used to calculate the weights of criteria with six experts through pairwise comparisons. Finally, FAHP weights are integrated into a CBR prediction mechanism for evaluating a user's adoption intention toward web ATM. The results are helpful for financial institutions to understand and to evaluate the user behavior toward internet banking service adoption.																	1064-1246	1875-8967					2020	39	3					2869	2879		10.3233/JIFS-191408													
J								Training robust support vector regression machines for more general noise	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Support vector regression; training in the primal; robustness; asymmetric Huber loss; concave-convex procedure	QUANTILE REGRESSION	Symmetric loss functions are widely used in regression algorithms to focus on estimating the means. Huber loss, a symmetric smooth loss function, has been proved that it can be optimized with high efficiency and certain robustness. However, mean estimators may be poor when the noise distribution is asymmetric (even outliers caused heavy-tailed distribution noise) and estimators beyond the means are necessary. Under the circumstances, quantile regression is a natural choice which estimates quantiles instead of means through asymmetric loss functions. In this paper, an asymmetric Huber loss function is proposed to implement different penalty for overestimation and underestimation so as to deal with more general noise. Moreover, a smooth truncated version of the proposed loss is introduced to enhance stronger robustness to outliers. Concave -convex procedure is developed in the primal space with the proof of convergence to handle the non-convexity of the involved truncated objective. Experiments are carried out on both artificial and benchmark datasets and robustness of the proposed methods are verified.																	1064-1246	1875-8967					2020	39	3					2881	2892		10.3233/JIFS-191429													
J								Predict pneumonia with chest X-ray images based on convolutional deep neural learning networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Chest X-ray; CNN; adaptive median filter; RF; image classification	DIAGNOSIS	The chest X-ray examination is one of the most important methods for screening and diagnosing of many lung diseases. Diagnosis of pneumonia by chest X-ray is one of the common methods used by medical experts. However, the image quality of chest X-Ray has some defects, such as low contrast, overlapping organs and blurred boundary, which seriously affects detecting pneumonia in chest X-rays. Therefore, it has important medical value and application significance to construct a stable and accurate automatic detection model of pneumonia through a large number of chest X-ray images. In this paper, we propose a novel hybrid system for detecting pneumonia from chest X-Ray image: ACNN-RF, which is an adaptive median filter Convolutional Neural Network (CNN) recognition model based on Random forest (RF). Firstly, the improved adaptive median filtering is employed to remove noise in the chest X-ray image, which makes the image more easily recognized. Secondly, we establish the CNN architecture based on Dropout to extract deep activation features from each chest X-ray image. Finally, we employ the RF classifier based on GridSearchCV class as a classifier for deep activation features in CNN model. It not only avoids the phenomenon of over-fitting in data training, but also improves the accuracy of image classification. During our experiment, the public chest X-ray image dataset used in the experiment contains 5863 images, which comprises 4265 frontal-view X-ray images of 1574 unique patients. The average recognition rate of pneumonia is up to 97% by the proposed ACNN-RF. The experimental results show that the ACNN-RF identification system is more effective than the previous traditional image identification system.																	1064-1246	1875-8967					2020	39	3					2893	2907		10.3233/JIFS-191438													
J								GRA method for waste incineration plants location problem with probabilistic linguistic multiple attribute group decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										multiple attribute group decision making (MAGDM); probabilistic linguistic term sets (PLTSs); GRA method; incomplete weight information; waste incineration plants	GREY RELATIONAL ANALYSIS; TERM SETS; AGGREGATION OPERATORS; TODIM METHOD; 2-TUPLE; INFORMATION; ENTERPRISES; SELECTION; INTEGRATION; PERFORMANCE	In this paper, we provide the probabilistic linguistic multiple attribute group decision making (PL-MAGDM) with incomplete weight information. In such method, the linguistic information firstly is shifted into probabilistic linguistic information. For obtaining the weight information of the attribute, two optimization models are built on the basis of the basic idea of grey relational analysis (GRA), by which the attribute weights can be obtained. Then, the optimal alternative is obtained through calculating largest relative relational degree from the probabilistic linguistic positive ideal solution (PLPIS) which considers both the largest grey relational coefficient (GRC) from the PLPIS and the smallest GRC form probabilistic linguistic negative ideal solution (PLNIS). Finally, a case study for waste incineration plants location problem is given to demonstrate the advantages of the developed methods.																	1064-1246	1875-8967					2020	39	3					2909	2920		10.3233/JIFS-191443													
