PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Automatic Segmentation of Liver from CT Scans with CCP-TSPM Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Liver segmentation; automatic segmentation; deformable model; CCP-TSPM	CONVOLUTIONAL NEURAL-NETWORK	With the increase in the morbidity of liver cancer and its high mortality rate, liver segmentation in abdominal computed tomography (CT) scan images has received extensive attention. Segmentation results play an important role in computer-assisted diagnosis and therapy. However, it remains a challenging task because of the complexity of the liver's anatomy, low contrast between the liver and its adjacent organs, and presence of lesions. This study presents an automatic method for liver segmentation from CT scan images based on the convex-concave point for tree structured part model (CCP-TSPM). First, TSPM is utilized as a coarse segmentation tool for capturing the topological shape variation. Then, the proposed CCP is implemented to adjust the position between adjacent points dynamically. As a result, the CCP-TSPM can locate the liver boundary adaptively. Furthermore, color space data provide abundant feature information, which can further improve the method's effectiveness and efficiency. Finally, the curve is evolved by an iteration level set function to obtain the fine segmentation results. The experimental results show that the proposed method can extract the liver boundary successfully. Furthermore, a comparison of the results with those of the state-of-the-art methods demonstrates the superior performance of the proposed method.																	0218-0014	1793-6381				DEC 31	2019	33	14							1957005	10.1142/S0218001419570052													
J								Spatial Error Concealment by Jointing Gauss Bayes Model and SVD for High Efficiency Video Coding	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Gaussian process regression; error concealment; SVD; confidence	SHAPE INFORMATION; IMAGE	This paper proposes a novel sparsity-based error concealment (EC) algorithm which integrates the Gauss Bayes model and singular value decomposition for high efficiency video coding (HEVC). Under the sequential recovery framework, pixels in missing blocks are successively reconstructed in Gauss Bayes mode. We find that the estimation error follows the Gaussian distribution in HEVC, so the error pixel estimation problem can be transferred to a Bayesian estimation. We utilize the singular value decomposition (SVD) technique to select sample pixels, which yields high estimation accuracy and reduces estimation error. A new recovery order based on confidence is established to resolve the error propagation problem. Compared to other state-of-the-art EC algorithms, experimental results show that the proposed method gives better reconstruction performance in terms of objective and subjective evaluations. It also has significantly lower complexity.																	0218-0014	1793-6381				DEC 31	2019	33	14							1954037	10.1142/S0218001419540375													
J								Support Vector Machine Optimized Using the Improved Fish Swarm Optimization Algorithm and Its Application to Face Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Support vector machine; fish swarm optimization algorithm; face recognition; feature extraction	KERNEL; SELECTION	Support vector machine (SVM) is always used for face recognition. However, kernel function selection is a key problem for SVM. This paper tries to make some contributions to this problem with focus on optimizing the parameters in the selected kernel function to improve the accuracy of classification and recognition of SVM. Firstly, an improved artificial fish swarm optimization algorithm (IAFSA) is proposed to optimize the parameters in SVM. In the improved version of artificial fish swarm optimization algorithm, the visual distance and the step size of artificial fish are adjusted adaptively. In the early stage of convergence, artificial fish are widely distributed, and the visual distance and step size take larger values to accelerate the convergence of the algorithm. In the later stage of convergence, artificial fish gathered gradually, and the visual distance and the step size were given small values to prevent oscillation. Then the optimized SVM is used to recognize face images. Simultaneously, in order to improve the accuracy rate of face recognition, an improved local binary pattern (ILBP) is proposed to extract features of face images. Numerical results show the advantage of our new algorithm over a range of existing algorithms.																	0218-0014	1793-6381				DEC 31	2019	33	14							1956010	10.1142/S021800141956010X													
J								Technoperformances: using metaphors from the performance arts for a postphenomenology and posthermeneutics of technology use	AI & SOCIETY										Technoperformance; Postphenomenology; Posthermeneutics; Performance; Movement; Sociality; Temporality; Metaphor; Technology; Philosophy of technology; Dance; Theatre; Music	PHENOMENOLOGY	Postphenomenology and posthermeneutics as initiated by Ihde have made important contributions to conceptualizing understanding human-technology relations. However, their focus on individual perception, artifacts, and static embodiment has its limitations when it comes to understanding the embodied use of technology as (1) involving bodily movement, (2) social, and (3) taking place within, and configuring, a temporal horizon. To account for these dimensions of experience, action, and existence with technology, this paper proposes to use a conceptual framework based on performance metaphors. Drawing on metaphors from three performance arts-dance, theatre, and music-and giving examples from social media and other technologies, it is shown that we can helpfully describe technology use and experience as performance involving movement, sociality, and temporality. Moreover, it is argued that these metaphors can also be used to reformulate the idea that in such uses and experiences, now understood as "technoperformances", technology is not merely a tool but also takes on a stronger, often non-intended role: not so much as "mediator" but as choreographer, director, and conductor of what we experience and do. Performance metaphors thus allow us to recast the phenomenology and hermeneutics of technology use as moving, social, and temporal-indeed historical-affair in which technologies take on the role of organizer and structurer of our performances, and in which humans are not necessarily the ones who are fully in control of the meanings, experiences, and actions that emerge from our engagement with the world, with technology, and with each other. This promises to give us a more comprehensive view of what it means to live with technology and how our lives are increasingly organized by technology-especially by smart technologies. Finally, it is argued that this has normative implications for an ethics and politics of technology, now understood as an ethics and politics of technoperformances.																	0951-5666	1435-5655				SEP	2020	35	3					557	568		10.1007/s00146-019-00926-7		DEC 2019											
J								Constrained representation learning for recurrent policy optimisation under uncertainty	ADAPTIVE BEHAVIOR										Policy optimisation; auto-encoder; representation learning; partially observable Markov decision process		Learning to make decisions in partially observable environments is a notorious problem that requires a complex representation of controllers. In most work, the controllers are designed as a non-linear mapping from a sequence of temporal observations to actions. These problems can, in principle, be formulated as a partially observable Markov decision process whose policy can be parameterised through the use of recurrent neural networks. In this paper, we will propose an alternative framework that (a) uses the Long-Short-Term-Memory (LSTM) Encoder-Decoder framework to learn an internal state representation for historical observations and then (b) integrates it into existing recurrent policy models to improve the task performance. The LSTM Encoder encodes a history of observations as input into a representation of internal states. The LSTM Decoder can perform two alternative decoding tasks: predicting the same input observation sequence or predicting future observation sequences. The first proposed decoder acts like an auto-encoder that will guide and constrain the learning of a useful internal state for the policy optimisation task. The second proposed decoder decodes the learnt internal state by the encoder to predict future observation sequences. This idea makes the network act like a non-linear predictive state representation model. Both these decoding parts, which introduce constraints to policy representation, will help guide both the policy optimisation problem and latent state representation learning. The integration of representation learning and policy optimisation aims to help learn more complex policies and improve the performance of policy learning tasks.																	1059-7123	1741-2633														1059712319891641	10.1177/1059712319891641		DEC 2019											
J								The minimum cost consensus model considering the implicit trust of opinions similarities in social network group decision-making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										consensus opinions; social network group decision-making; the minimum cost consensus model; trust relationships	SOFT CONSENSUS; FEEDBACK; DYNAMICS	The social network group decision-making is popular due to the advantages of social relationships in the consensus reaching process, especially the trust relationships. To explore the effects of trust on consensus, some minimum cost consensus models are proposed based on implicit trust between individuals and the moderator. The implicit trust is computed based on the similarity of opinions and it is implied into the traditional minimum cost consensus model to obtain a new quadratic programming problem and the related dual problem. The weights of individuals can be determined based on implicit trust and can be used to modify the possible deviations among individuals' adjustment costs. A numerical example and the comparative analysis are given to analyze the effectiveness of the proposed models, which suggests that individuals are willing to give up some benefit to reach consensus due to their implicit trust to the moderator and make minor revisions to their adjustment costs due to their implicit trust to each other.																	0884-8173	1098-111X				MAR	2020	35	3					470	493		10.1002/int.22214		DEC 2019											
J								Trend following deep Q-Learning strategy for stock trading	EXPERT SYSTEMS										deep learning; reinforcement learning; trend following		Computers and algorithms are widely used to help in stock market decision making. A few questions with regards to the profitability of algorithms for stock trading are can computers be trained to beat the markets? Can an algorithm take decisions for optimal profits? And so forth. In this research work, our objective is to answer some of these questions. We propose an algorithm using deep Q-Reinforcement Learning techniques to make trading decisions. Trading in stock markets involves potential risk because the price is affected by various uncertain events ranging from political influences to economic constraints. Models that trade using predictions may not always be profitable mainly due to the influence of various unknown factors in predicting the future stock price. Trend Following is a trading idea in which, trading decisions, like buying and selling, are taken purely according to the observed market trend. A stock trend can be up, down, or sideways. Trend Following does not predict the stock price but follows the reversals in the trend direction. A trend reversal can be used to trigger a buy or a sell of a certain stock. In this research paper, we describe a deep Q-Reinforcement Learning agent able to learn the Trend Following trading by getting rewarded for its trading decisions. Our results are based on experiments performed on the actual stock market data of the American and the Indian stock markets. The results indicate that the proposed model outperforms forecasting-based methods in terms of profitability. We also limit risk by confirming trading actions with the trend before actual trading.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12514	10.1111/exsy.12514		DEC 2019											
J								Electronic Nose and Its Applications: A Survey	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Artificial intelligence; machine learning; pattern recognition; electronic nose (EN); sensors technology	QUARTZ-CRYSTAL-MICROBALANCE; ARTIFICIAL NEURAL-NETWORKS; PARTIAL LEAST-SQUARES; GAS SENSOR ARRAYS; BIOELECTRONIC NOSE; MYCOBACTERIUM-TUBERCULOSIS; OLFACTORY SYSTEM; ROASTING DEGREE; LEARNING-METHOD; QUALITY-CONTROL	In the last two decades, improvements in materials, sensors and machine learning technologies have led to a rapid extension of electronic nose (EN) related research topics with diverse applications. The food and beverage industry, agriculture and forestry, medicine and health-care, indoor and outdoor monitoring, military and civilian security systems are the leading fields which take great advantage from the rapidity, stability, portability and compactness of ENs. Although the EN technology provides numerous benefits, further enhancements in both hardware and software components are necessary for utilizing ENs in practice. This paper provides an extensive survey of the EN technology and its wide range of application fields, through a comprehensive analysis of algorithms proposed in the literature, while exploiting related domains with possible future suggestions for this research topic.																	1476-8186	1751-8520				APR	2020	17	2					179	209		10.1007/s11633-019-1212-9		DEC 2019											
J								The Swing Control of Knee Exoskeleton Based on Admittance Model and Nonlinear Oscillator	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Exoskeleton; Rehabilitation robot; Adaptive oscillator; Electromyography (EMG); Admittance control	ASSISTANCE	It is important to control exoskeleton to act synchronously with the user for movement assistance. A knee exoskeleton was constructed for gait rehabilitation training, and a control method based on adaptive frequency oscillator (AFO) was presented for swing motion of knee exoskeleton. Firstly, during the learning mode, the user flexed and extended with the exoskeleton leg together, and the exoskeleton leg was not actuated. The phase of the exoskeleton angle extracted, and the envelope curve of the user's EMG is generated based on AFO. Then, during power-assisted mode, the assistive torque is generated according to the angle phase on-line and the restructuring muscle torque. At last, the assistive torque as a feedback signal was input to the admittance model for the swing control of the knee exoskeleton. The experiments of swing control showed that knee exoskeleton can follow the user's motion well.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		747	756		10.1007/s10846-019-01133-8		DEC 2019											
J								Cat Swarm Fractional Calculus optimization-based deep learning for artifact removal from EEG signal	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										EEG signal; artefacts; wavelet transform; deep learner; LSTM network	OCULAR ARTIFACTS; BAT ALGORITHM; CLASSIFICATION	Electroencephalogram (EEG) signals are commonly used in analysing the brain activity. The EEG signals have small amplitude and hence, they are often affected by the artefacts. For the efficient processing, it is necessary to remove the artefacts from the EEG signals. This paper develops a technique through deep learning scheme for removing the artefacts present in the EEG signal. Initially, the EEG signals are pre-processed and provided to the feature extraction process, where the wavelet features are extracted from the signal by applying the wavelet transform. The extracted features are provided to the proposed classifier, namely deep-ConvLSTM, for removing the artefacts from the EEG signal. Here, the deep learner is trained based on the proposed Cat Swarm Fractional Calculus Optimisation (CSFCO) algorithm, which is the integration of Cat Swarm Optimisation (CSO) and Fractional Calculus (FC). Experimentation of the proposed technique is carried out by introducing artefacts, such as ECG, EMG, EOG and random noise on the EEG signal. Simulation results carried out on the proposed deep-ConvLSTM depict that the proposed framework has better performance than the comparative techniques with the values of 3888.362, 62.356, and 69.939 dB, for the MSE, RMSE, and SNR, respectively.																	0952-813X	1362-3079															10.1080/0952813X.2019.1704438		DEC 2019											
J								Active materials: minimal models of cognition?	ADAPTIVE BEHAVIOR										Minimal cognition; active matter; minimal models	BEHAVIOR; MOTION	Work on minimal cognition raises a variety of questions concerning the boundaries of cognition. Many discussions of minimal cognition assume that the domain of minimal cognition is a subset of the domain of the living. In this article, I consider whether non-living 'active materials' ought to be included as instances of minimal cognition. I argue that seeing such cases as 'minimal models' of (minimal) cognition requires recognising them as members of a class of systems sharing the same basic features and exhibiting the same general patterns of behaviour. Minimal cognition in this sense is a very inclusive concept: rather than specifying some threshold level of cognition or a type of cognition found only in very simple systems, it is a concept of cognition associated with very minimal criteria that pick out only the most essential requirements for a system to exhibit cognitive behaviour.																	1059-7123	1741-2633														1059712319891742	10.1177/1059712319891742		DEC 2019											
J								A simplified neutrosophic multiplicative set-based TODIM using water-filling algorithm for the determination of weights	EXPERT SYSTEMS										aggregation operators; decision making; decision support systems; distance measure; simplified neutrosophic multiplicative number; simplified neutrosophic multiplicative set; TODIM	INTUITIONISTIC FUZZY-SETS; GROUP DECISION-MAKING; AGGREGATION OPERATORS; PREFERENCE RELATIONS; SUPPORT-SYSTEM; NUMBERS; MODELS	Due to the technological developments around the world, the amount of information that researchers work on increases continuously. This information density contains incomplete and uncertain data that cannot be fully expressed with crisp numbers. Fuzzy sets, intuitionistic fuzzy sets, and neutrosophic sets are useful tools to manage such information, but these concepts use a symmetrical and uniform scale to express data, whereas real-life problems contain nonsymmetrical and non-uniform information. Intuitionistic multiplicative sets (IMSs) are effective tools for dealing with these real-life problems. However, IMSs cannot handle real-life problems completely because indeterminate information depends on membership and non-membership information of IMSs, which is a restriction for decision makers and also for decision problems. To overcome this limitation, this paper generalizes the IMSs by using simplified neutrosophic set and introduces a novel approach that is called simplified neutrosophic multiplicative sets (SNMSs). Firstly, we define SNMS, show their set-based operations, and then give a description of simplified neutrosophic multiplicative numbers (SNMNs). Based on SNMNs, we develop two simplified neutrosophic multiplicative aggregation operators on SNMNs that are called simplified neutrosophic multiplicative weighted arithmetic average operator and simplified neutrosophic multiplicative weighted geometric average operator. Furthermore, we define some simplified neutrosophic multiplicative distance measures. Finally, using a model based on water-filling algorithm for determining criteria weights, we give a numerical example to demonstrate the effectiveness of the introduced concept with the proposed simplified neutrosophic multiplicative simplified neutrosophic multiplicative-TODIM method.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12515	10.1111/exsy.12515		DEC 2019											
J								Air pollution forecasting based on attention-based LSTM neural network and ensemble learning	EXPERT SYSTEMS										air pollution forecasting; attention mechanism; ensemble learning; LSTM; XGBoost	SHORT-TERM-MEMORY; PM2.5 CONCENTRATIONS; MODEL; PREDICTION; QUALITY; SYSTEM; WATER	With air pollution having become a global concern, scientists are committed to working on its amelioration. In the field of air pollution prediction, there have been good results in experimental research so far, but few studies have integrated weather forecast information and the properties of air pollution drift. In this work, we propose a novel wind-sensitive attention mechanism with a long short-term memory (LSTM) neural network model to predict the air pollution - PM2.5 concentrations by considering the influence of wind direction and speed on the changes of spatial-temporal PM2.5 concentrations in neighbouring areas. Preliminary predictions for PM2.5 are then made by an LSTM neural network regarding neighbouring pollution; these predictions are "paid attention to" and we finally apply an ensemble learning method based on eXtreme Gradient Boosting (XGBoost) to combine the preliminary predictions with weather forecasting to make second phase predictions of PM2.5. The experiment is conducted using PM2.5 data and weather forecast data. Our results illustrate that the proposed method is superior to other methods in predicting PM2.5 concentrations, including multi-layer perceptron, support vector regression, LSTM neural network, and extreme gradient boosting algorithm.																	0266-4720	1468-0394				JUN	2020	37	3			SI					10.1111/exsy.12511		DEC 2019											
J								Generalized Multiscale RBF Networks and the DCT for Breast Cancer Detection	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Nonlinear system identification; image processing; discrete cosine transform; radial basis functions; computer-aided diagnosis; neural networks	COMPUTER-AIDED DIAGNOSIS; FACE RECOGNITION; CLASSIFICATION; SYSTEMS	The use of the multiscale generalized radial basis function (MSRBF) neural networks for image feature extraction and medical image analysis and classification is proposed for the first time in this work. The MSRBF networks hold a simple and flexible architecture that has been successfully used in forecasting and model structure detection of input-output nonlinear systems. In this work instead, MSRBF networks are part of an integrated computer-aided diagnosis (CAD) framework for breast cancer detection, which holds three stages: an input-output model is obtained from the image, followed by a high-level image feature extraction from the model and a classification module aimed at predicting breast cancer. In the first stage, the image data is rendered into a multiple-input-single-output (MISO) system. In order to improve the characterisation, the nonlinear autoregressive with exogenous inputs (NARX) model is introduced to rearrange the available input-output data in a nonlinear way. The forward regression orthogonal least squares (FROLS) algorithm is then used to take advantage of the previous arrangement by solving the system as a model structure detection problem and finding the output layer weights of the NARX-MSRBF network. In the second stage, once the network model is available, the feature extraction takes place by stimulating the input to produce output signals to be compressed by the discrete cosine transform (DCT). In the third stage, we leverage the extracted features by using a clustering algorithm for classification to integrate a CAD system for breast cancer detection. To test the method performance, three different and well-known public image repositories were used: the mini-MIAS and the MMSD for mammography, and the BreaKHis for histopathology images. A comparison exercise was also made between different database partitions to understand the mammogram breast density effect in the performance since there are few remarks in the literature on this factor. Classification results show that the new CAD method reached an accuracy of 93.5% in mini-Mammo graphic image analysis society (mini-MIAS), 93.99% in digital database for screening mammography (DDSM) and 86.7% in the BreaKHis. We found that the MSRBF networks are able to build tailored and precise image models and, combined with the DCT, to extract high-quality features from both black and white and coloured images.																	1476-8186	1751-8520				FEB	2020	17	1			SI		55	70		10.1007/s11633-019-1210-y		DEC 2019											
J								Smooth-optimal Adaptive Trajectory Tracking Using an Uncalibrated Fish-eye Camera	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Smooth motion; adaptive control; trajectory tracking; wheeled mobile robots; visual servoing	NONHOLONOMIC MOBILE ROBOTS; POSITION	This paper presents a two-stage smooth-optimal trajectory tracking strategy. Different from existing methods, the optimal trajectory tracked point can be directly determined in an uncalibrated fish-eye image. In the first stage, an adaptive trajectory tracking controller is employed to drive the tracking error and the estimated error to an arbitrarily small neighborhood of zero. Afterwards, an online smooth-optimal trajectory tracking planner is proposed, which determines the tracked point that can be used to realize smooth motion control of the mobile robot. The tracked point in the uncalibrated image can be determined by minimizing a utility function that consists of both the velocity change and the sum of cross-track errors. The performance of our planner is compared with other tracked point determining methods in experiments by tracking a circular trajectory and an irregular trajectory. Experimental results show that our method has a good performance in both tracking accuracy and motion smoothness.																	1476-8186	1751-8520				APR	2020	17	2					267	278		10.1007/s11633-019-1209-4		DEC 2019											
J								Ground Vehicle Driving by Full Sized Humanoid	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Humanoids; Vehicle driving; Darpa robotics challenge; DRC finals; DRC-Hubo plus	ROBOT; SYSTEM	This paper presents a technical overview of the Team DRC-Hubo@UNLV's approach to the driving task in DARPA Robotics Challenge Finals (DRC-Finals). First, hardware updates in the main body and the perception head of the contestant robot, DRC-Hubo+, were presented by comparison with the previous platform (DRC-Hubo in DRC-Trials). Then, the control system which enabled the full-sized humanoid to drive the off-the-shelf utility vehicle in the competition was provided. For this, the sensor data fusion process and the advanced driving assistant techniques were emphasized. Next, the driving pattern analysis of other type operators (such as human-drivers or pure tele-operation systems) was provided to demonstrate the performance of the developed system. Lastly, test and evaluation of the built robot and control system was provided via experimentation which DRC-Hubo+ drove the vehicle in a real-world setting. The presented approach was also verified in DRC-Finals as the author's team was placed in the top record at the driving task in the competition.																	0921-0296	1573-0409				AUG	2020	99	2					407	425		10.1007/s10846-019-01130-x		DEC 2019											
J								Fault tolerant control for nonlinear systems using sliding mode and adaptive neural network estimator	SOFT COMPUTING										Fault tolerant control; Intelligent method; Sliding mode control; Adaptive neural estimator; Data analysis	DISTURBANCE OBSERVER; DIAGNOSIS	This paper proposes a new fault tolerant control scheme for a class of nonlinear systems including robotic systems and aeronautical systems. In this method, a sliding mode control is applied to maintain system stability under the post-fault dynamics. A neural network is used as on-line estimator to reconstruct the change rate of the fault and compensate for the impact of the fault on the system performance. The control law and the neural network learning algorithms are derived using the Lyapunov method, so that the neural estimator is guaranteed to converge to the fault change rate, while the entire closed-loop system stability and tracking control is guaranteed. Compared with the existing methods, the proposed method achieved fault tolerant control for time-varying fault, rather than just constant fault. This greatly expands the industrial applications of the developed method to enhance system reliability. The main contribution and novelty of the developed method is that the system stability is guaranteed and the fault estimation is also guaranteed for convergence when the system subject to a time-varying fault. A simulation example is used to demonstrate the design procedure and the effectiveness of the method. The simulation results demonstrated that the post-fault is stable and the performance is maintained.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11535	11544		10.1007/s00500-019-04618-8		DEC 2019											
J								Evolutionary dataset optimisation: learning algorithm quality through evolution	APPLIED INTELLIGENCE										Evolutionary algorithm; Optimisation; Algorithm design; Artificial data generation		In this paper we propose a novel method for learning how algorithms perform. Classically, algorithms are compared on a finite number of existing (or newly simulated) benchmark datasets based on some fixed metrics. The algorithm(s) with the smallest value of this metric are chosen to be the 'best performing'. We offer a new approach to flip this paradigm. We instead aim to gain a richer picture of the performance of an algorithm by generating artificial data through genetic evolution, the purpose of which is to create populations of datasets for which a particular algorithm performs well on a given metric. These datasets can be studied so as to learn what attributes lead to a particular progression of a given algorithm. Following a detailed description of the algorithm as well as a brief description of an open source implementation, a case study in clustering is presented. This case study demonstrates the performance and nuances of the method which we call Evolutionary Dataset Optimisation. In this study, a number of known properties about preferable datasets for the clustering algorithms known as k-means and DBSCAN are realised in the generated datasets.																	0924-669X	1573-7497				APR	2020	50	4					1172	1191		10.1007/s10489-019-01592-4		DEC 2019											
J								Leveraging active learning to reduce human effort in the generation of ground-truth for entity resolution	COMPUTATIONAL INTELLIGENCE										active learning; classification; deduplication; ground-truth; machine learning; record linkage		Several methods of entity resolution (ER) have been developed in academia and industry over the years, with the intention to identify duplicate entities (eg, records) in datasets. To evaluate the efficacy of such methods, it is necessary to compare their results with a ground-truth, which consists of a document containing all known duplicate record pairs in a dataset. In general, the generation of ground-truths for real datasets is performed manually from the inspection of all combinations of pairs of records in a dataset. This is subject to error and presents quadratic complexity, with respect to the size(s) of the dataset(s), requiring a long time to be performed. In this context, some works present (semi)automatic approaches for the generation of ground-truths for the ER task. However, such approaches are either not applicable to several domains or still present a considerable manual effort. In this work, we propose GTGenERAL, a semiautomatic approach that combines results from multiple algorithms of ER together with active learning to generate accurate ground-truths employing reduced manual effort. Experiments using real datasets show that, with great manual effort reduction, GTGenERAL is able to generate ground-truths close to those generated by the state-of-the-art approach.																	0824-7935	1467-8640				MAY	2020	36	2					743	772		10.1111/coin.12268		DEC 2019											
J								High-dimensional spectral data classification with nonparametric feature screening	JOURNAL OF CHEMOMETRICS										classification; high-dimensional spectral datasets; nonparametric feature screening; random forest	PARTIAL LEAST-SQUARES; FEATURE-SELECTION; REGRESSION; LASSO	Two nonparametric feature screening methods, namely, the Kolmogorov filter and model free, marginally measure the relationship between categorical response and predictor variables without the parametrical assumption. And they can select important variables in the high-dimensional classification data. Random forest, as a classical nonparametric method, can solve various classification problems. In this paper, we combine the two nonparametric feature screening methods with random forest to handle with spectral data classification. And then other conventional classification methods are compared with ours on three spectral datasets. The comparison results illustrated that our methods have more desirable ability about classification performance and variable selection than other methods.																	0886-9383	1099-128X				MAR	2020	34	3			SI				e3199	10.1002/cem.3199		DEC 2019											
J								Improved graph-based SFA: information preservation complements the slowness principle	MACHINE LEARNING										Supervised dimensionality reduction; Similarity-based learning; Information preservation; Deep neural networks; Age estimation	AGE ESTIMATION; RECOGNITION; EXTENSION	Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a multi-dimensional time series. SFA has been extended to supervised learning (classification and regression) by an algorithm called graph-based SFA (GSFA). GSFA relies on a particular graph structure to extract features that preserve label similarities. Processing of high dimensional input data (e.g., images) is feasible via hierarchical GSFA (HGSFA), resulting in a multi-layer neural network. Although HGSFA has useful properties, in this work we identify a shortcoming, namely, that HGSFA networks prematurely discard quickly varying but useful features before they reach higher layers, resulting in suboptimal global slowness and an under-exploited feature space. To counteract this shortcoming, which we call unnecessary information loss, we propose an extension called hierarchical information-preserving GSFA (HiGSFA), where some features fulfill a slowness objective and other features fulfill an information preservation objective. The efficacy of the extension is verified in three experiments: (1) an unsupervised setup where the input data is the visual stimuli of a simulated rat, (2) the localization of faces in image patches, and (3) the estimation of human age from facial photographs of the MORPH-II database. Both HiGSFA and HGSFA can learn multiple labels and offer a rich feature space, feed-forward training, and linear complexity in the number of samples and dimensions. However, the proposed algorithm, HiGSFA, outperforms HGSFA in terms of feature slowness, estimation accuracy, and input reconstruction, giving rise to a promising hierarchical supervised-learning approach. Moreover, for age estimation, HiGSFA achieves a mean absolute error of 3.41 years, which is a competitive performance for this challenging problem.																	0885-6125	1573-0565				MAY	2020	109	5					999	1037		10.1007/s10994-019-05860-9		DEC 2019											
J								Electromagnetic optimization-based clustering algorithm	EXPERT SYSTEMS										data clustering; electromagnetic field; optimization; meta-heuristic		This paper introduces the electromagnetic clustering algorithm (ELMC), an enhanced variant of electromagnetic field optimization (EFO), for clustering. The motivation behind ELMC is to overcome the shortcomings of traditional k-means clustering algorithm. The performance of k-means primarily depends upon the initial choice of centroids, which can lead the algorithm towards an undesirable local optimum, if chosen incorrectly or inefficiently. The ELMC utilizes the attraction-repulsion concept of the EFO algorithm to maintain the diversity of the population, making it less vulnerable towards the initial choice of centroids. The performance of ELMC is validated on a set of benchmark problems, and the results are compared with other state-of-the-art algorithms. Numerical and graphical results indicate the competence of the proposed ELMC algorithm.																	0266-4720	1468-0394														e12491	10.1111/exsy.12491		DEC 2019											
J								Digital bandstop filtering in the quantitative analysis of glucose from near-infrared and midinfrared spectra	JOURNAL OF CHEMOMETRICS										Near Infrared; Mid Infrared; Principal Component Regression; Partial Least Squares Regression; Root Mean Square Error of Prediction	LEAST-SQUARES REGRESSION; PHYSIOLOGICAL LEVELS; MODELS	This work proposes the use of bandstop filtering (BSF) as a pretreatment method in the quantitative analysis of glucose from both near-infrared (NIR) and midinfrared (MIR) spectra. The proposed method is investigated and evaluated against the traditional bandpass filtering (BPF) and implemented with the linear calibration models principal component regression (PCR) and partial least squares regression (PLSR) to predict the glucose from an aqueous mixture consisting of glucose and human serum albumin dissolved in a phosphate buffer solution. The results obtained show that BSF pretreatment achieves better prediction performance than BPF in both the NIR and MIR spectral regions. For detailed analysis, the BPF and BSF were implemented under both the Butterworth and Chebyshev filter configurations in both bands; in the NIR region, the Butterworth BSF combined with the PLSR model provides the best glucose prediction by reducing the root mean square error of prediction (RMSEP) from 100 mg/dL without filtering to 34 mg/dL with a coefficient of determination R-2 of .982. In the MIR region, the Chebyshev BSF combined with either PLSR or PCR improves the glucose prediction by reducing the RMSEP by 54% compared with 45% when using BPF and with R-2 of.995.																	0886-9383	1099-128X				MAR	2020	34	3			SI				e3206	10.1002/cem.3206		DEC 2019											
J								Machine learning methods to predict solubilities of rock samples	JOURNAL OF CHEMOMETRICS										acid solubility; artificial neural network (ANN) TensorFlow; Extreme Gradient Boosting (XGBoost); mid-infrared spectra; multivariate data analysis (MVDA)		Interests in the use of chemometric and data science methods for laboratory techniques have grown rapidly over the last 10 years, for the reason that they are cheaper and faster than traditional analytical methods of material testing. This study uses 888 rock samples collected from the exploration and production (E&P) sector of the oil industry. Based on the Fourier-transform infrared (FT-IR) spectra of these rock samples their solubility predictions have been developed and investigated with nine methods including both linear and non-linear ones. Two of these methods such as Partial Least Squares Regression (PLSR) and Support Vector Regression (SVR) are available in a commercial software package and the other seven methods, Extreme Gradient Boosting (XGBoost), Ridge Regression (RR), k-nearest neighbours (k-NN), Decision Tree (DT), Multilayer Perceptron (MLP), Support Vector Regression (SVR), Artificial Neural Network (ANN) with TensorFlow (TF), were coded by the authors based either on commercial applications or open source libraries. The investigation starts with spectral data pre-processing carried out by standard normal variate (SNV), baseline correction and feature selection methods creating the feature set for all machine learning (ML) applications. The accuracy of predictions has been evaluated with mean squared error as a performance metric for each investigated method. The comparisons of predicted values to real data of test samples have shown that mineral solubility in acids can be well predicted in the range of the uncertainties of real laboratory measurements, therefore it can be used to improve the response time of these investigations and reduce the risk in industrial applications. In those cases, where the unknown samples have got some out of the range features, the limitations in the accuracy of predictions have become clear. We have also identified the limitations in the methodology and planned steps to further improve the prediction capabilities. The identified constraint of samples' multitude further emphasizes the need for database building efforts, so that the real potential in big data and machine learning can be realized.																	0886-9383	1099-128X				FEB	2020	34	2			SI				e3198	10.1002/cem.3198		DEC 2019											
J								Diagnostics via partial residual plots in inverse Gaussian regression	JOURNAL OF CHEMOMETRICS										diagnostics; inverse Gaussian regression; predictor transformations; visual impression	LOGISTIC-REGRESSION	Regression diagnostics is the basic requirement to apply regression analysis to reach reliable conclusions. Generalized linear models also required diagnostics for its implementation. The construction of partial residuals using response residuals for the inverse Gaussian regression model is carried out to explore the structure and usefulness for visualizing diagnostics and curvature as a function of selected predictors. The current study established the performance of partial residual plots over conventional diagnostic methods. The comparison has been made using aerial biomass data and with the help of simulation study. It has been observed that partial residual plots provide much better diagnosis than do conventional methods. Moreover, multiple diagnostics in a single display provide better perceptive towards lack of fit, specification, and data anomalies.																	0886-9383	1099-128X				JAN	2020	34	1							e3203	10.1002/cem.3203		DEC 2019											
J								A data-driven predictive system using Case-Based Reasoning for the configuration of device-assisted back pain therapy	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Case Based Reasoning; Visual Explanations; Personalisation; Device-assisted back pain therapy	PREVALENCE; MANAGEMENT; EXTENSION; FRAMEWORK	Lower back Pain (LBP) is pathological and occurs in about 80% of the population at least once in their life. Physiotherapists personalise manual treatments to heal or relieve pain according to the patient characteristics. The contribution of this methodological paper is the description and evaluation of the configuration software associated to a therapy machine that executes back segment mobilisations. The configuration software uses Case-Based Reasoning (CBR), a successful Machine Learning technique, based on mimicking the human decision making process by reusing previously applied configuration episodes on similar individuals. This paper demonstrates its feasibility and cost-effectiveness for the configuration of treatments as it reuses expert knowledge and maximises effectiveness by taking into account the patient's personal medical record and similar patterns among different patients. Having a baseline of 31% success rate using a standard solution based on interpolation, the CBR engine can achieve, on average, up to 70% success rate when proposing a machine configuration to the physiotherapist. Regarding clinical results, we run a longitudinal observational study that achieves an average improvement of 31.63% using the pain Visual Analogue Scale (VAS), a 7% according to the Oswestry Disability Index (ODI), and 13% in the 36-Item Short Form Health Survey (SF-36).																	0952-813X	1362-3079															10.1080/0952813X.2019.1704441		DEC 2019											
J								Unsupervised discriminative feature representation via adversarial auto-encoder	APPLIED INTELLIGENCE										Unsupervised learning; Image clustering; Feature representation; Adversarial auto-encoder; Generative adversarial networks	FACE RECOGNITION; DIMENSIONALITY	Feature representation is generally applied to reducing the dimensions of high-dimensional data to accelerate the process of data handling and enhance the performance of pattern recognition. However, the dimensionality of data nowadays appears to be a rapidly increasing trend. Existing unsupervised feature representation methods are susceptible to the rapidly increasing dimensionality of data, which may result in learning a meaningless feature that in turn affect their performance in other applications. In this paper, an unsupervised adversarial auto-encoder network is studied. This network is a probability model that combines generative adversarial networks and variational auto-encoder to perform variational inference and aims to generate reconstructed data similar to original data as much as possible. Due to its adversarial training, this model is relatively robust in feature learning compared with other methods. First, the architecture and training strategy of adversarial auto-encoder are presented. We attempt to learn a discriminative feature representation for high-dimensional image data via adversarial auto-encoder and take its advantage into image clustering, which has become a difficult computer vision task recently. Then amounts of comparative experiments are carried out. The comparison contains eight feature representation methods and two recently proposed deep clustering methods performed on eight different publicly available image data sets. Finally, to evaluate their performance, we utilize a K-means clustering on the low-dimensional feature learned from each feature representation algorithm, and select three evaluation metrics including clustering accuracy, adjusted rand index and normalized mutual information, to provide a comparison. Comprehensive experiments prove the usefulness of the learned discriminative feature via adversarial auto-encoder in the tested data sets.																	0924-669X	1573-7497				APR	2020	50	4					1155	1171		10.1007/s10489-019-01581-7		DEC 2019											
J								Detecting Gas Turbine Combustor Anomalies Using Semi-supervised Anomaly Detection with Deep Representation Learning	COGNITIVE COMPUTATION										Anomaly detection; Combustor; Deep learning; Extreme learning machine; Gas turbine; Prognostics and health management; Semi-supervised learning	ONE-CLASS CLASSIFICATION; NOVELTY DETECTION; MACHINE; NETWORK; ALGORITHMS	Deep learning (DL), regarded as a breakthrough machine learning technique, has proven to be effective for a variety of real-world applications. However, DL has not been actively applied to condition monitoring of industrial assets, such as gas turbine combustors. We propose a deep semi-supervised anomaly detection (deepSSAD) that has two key components: (1) using DL to learn representations or features from multivariate, time-series sensor measurements; and (2) using one-class classification to model normality in the learned feature space, thus performing anomaly detection. Both steps use normal data only; thus our anomaly detection falls into the semi-supervised anomaly detection category, which is advantageous for industrial asset condition monitoring where abnormal or faulty data is rare. Using the data collected from a real-world gas turbine combustion system, we demonstrate that our proposed approach achieved a good detection performance (AUC) of 0.9706 +/- 0.0029. Furthermore, we compare the detection performance of the proposed approach against that of other different designs, including different features (i.e., the deep learned, handcrafted and PCA features) and different detection models (i.e., one-class ELM, one-class SVM, isolation forest, and Gaussian mixture model). The proposed approach significantly outperforms others. The proposed combustor anomaly detection approach is effective in detecting combustor anomalies or faults.																	1866-9956	1866-9964				MAR	2020	12	2			SI		398	411		10.1007/s12559-019-09710-7		DEC 2019											
J								Scalable Person Re-Identification by Harmonious Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION										Person re-identification; Scalable search; Compact model; Attention learning; Local and global representation learning	GLOBAL FEATURES	Existing person re-identification (re-id) deep learning methods rely heavily on the utilisation of large and computationally expensive convolutional neural networks. They are therefore not scalable to large scale re-id deployment scenarios with the need of processing a large amount of surveillance video data, due to the lengthy inference process with high computing costs. In this work, we address this limitation via jointly learning re-id attention selection. Specifically, we formulate a novel harmonious attention network (HAN) framework to jointly learn soft pixel attention and hard region attention alongside simultaneous deep feature representation learning, particularly enabling more discriminative re-id matching by efficient networks with more scalable model inference and feature matching. Extensive evaluations validate the cost-effectiveness superiority of the proposed HAN approach for person re-id against a wide variety of state-of-the-art methods on four large benchmark datasets: CUHK03, Market-1501, DukeMTMC, and MSMT17.																	0920-5691	1573-1405				JUN	2020	128	6					1635	1653		10.1007/s11263-019-01274-1		DEC 2019											
J								A survey of semantic relatedness evaluation datasets and procedures	ARTIFICIAL INTELLIGENCE REVIEW										Semantic relatedness; Semantic similarity; Evaluation dataset; Evaluation metric; Evaluation procedure	INFORMATION-RETRIEVAL; SIMILARITY MEASURES; WORDNET; DISAMBIGUATION; FRAMEWORK; DISTANCE; MODELS; CORPUS	Semantic relatedness between words is a core concept in natural language processing. While countless approaches have been proposed, measuring which one works best is still a challenging task. Thus, in this article, we give a comprehensive overview of the evaluation protocols and datasets for semantic relatedness covering both intrinsic and extrinsic approaches. One the intrinsic side, we give an overview of evaluation datasets covering more than 100 datasets in 20 different languages from a wide range of domains. To provide researchers with better guidance for selecting suitable dataset or even building new and better ones, we describe also the construction and annotation process of the datasets. We also shortly describe the evaluation metrics most frequently used for intrinsic evaluation. As for the extrinsic side, several applications involving semantic relatedness measures are detailed through recent research works and by explaining the benefit brought by the measures.																	0269-2821	1573-7462				AUG	2020	53	6					4407	4448		10.1007/s10462-019-09796-3		DEC 2019											
J								Mining relaxed functional dependencies from data	DATA MINING AND KNOWLEDGE DISCOVERY										Functional dependency; Discovery algorithm; Approximate match; Constraints Mining	EFFICIENT ALGORITHM; RELATIONAL DATA; DISCOVERY	Relaxed functional dependencies (rfds) are properties expressing important relationships among data. Thanks to the introduction of approximations in data comparison and/or validity, they can capture constraints useful for several purposes, such as the identification of data inconsistencies or patterns of semantically related data. Nevertheless, rfds can provide benefits only if they can be automatically discovered from data. In this paper we present an rfd discovery algorithm relying on a lattice structured search space, previously used for fd discovery, new pruning strategies, and a new candidate rfd validation method. An experimental evaluation demonstrates the discovery performances of the proposed algorithm on real datasets, also providing a comparison with other algorithms.																	1384-5810	1573-756X				MAR	2020	34	2					443	477		10.1007/s10618-019-00667-7		DEC 2019											
J								A Novel Fuzzy Inference Approach: Neuro-fuzzy Cognitive Map	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Neuro-fuzzy inference system; Autoimmune hepatitis; Fuzzy cognitive map; Diagnosis	SYSTEM-IDENTIFICATION; ALGORITHM; FUZZINESS	In this study, a new approach based on fuzzy cognitive map (FCM) and neuro-fuzzy inference system (NFIS), called the neuro-fuzzy cognitive map (NFCM), is proposed. Here, the NFCM is used for diagnosis of autoimmune hepatitis (AIH). AIH is a chronic inflammatory liver disease. AIH primarily affects women and typically responds to immunosuppressive therapy with clinical, biochemical, and histological remission. An untreated AIH can lead to scarring of the liver and ultimately to liver failure. If rapidly diagnosed, AIH can often be controlled by medication. NFCM is a new extension of FCM, which employs a NFIS to determine the causal relationships between concepts. In the proposed approach, weights are calculated using the knowledge and experience of experts as well as the advantages of NFIS. This makes the presented model more accurate. Having a high convergence speed, the proposed NFCM model performs well by achieving an AIH diagnosis accuracy of 89.81%. The superiority of the proposed NFCM model over the conventional FCM is that, it uses the NFIS to determine the link weights which train system parameters.																	1562-2479	2199-3211				APR	2020	22	3					859	872		10.1007/s40815-019-00762-3		DEC 2019											
J								A survey of robust optimization based machine learning with special reference to support vector machines	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Robust optimization; Classification; Regression; Unsupervised learning; Uncertainty; Optimization techniques	ONE-CLASS CLASSIFICATION; POWER ALLOCATION; ROUTING PROBLEM; BEAMFORMING DESIGN; UNCERTAINTY; ENERGY; REGULARIZATION; REGRESSION; NETWORK; TRANSMISSION	This paper gives an overview of developmentsin the field of robust optimization in machine learning (ML) in general and Support Vector Machine (SVM)/Support Vector Regression (SVR) models in particular. This survey comprises of researches in which robustness is sought against uncertainty. This uncertainty is in the values of parameters of the given model or it can be in the data. In this work, we have discussed how robust optimization has entered in the field of machine learning. Here, we investigate the contribution of various researchers in dealing with different types of uncertainties arising in the problem of maximizing or minimizing the objective function. We deal with the variants of SVM/SVR in more detail, although we have also covered supervised, unsupervised ML and various other aspects of ML. Also, we present an extensive study of research carried out in the applications of robust optimization in other areas like energy and power systems, networking, transportation etc. as well.																	1868-8071	1868-808X				JUL	2020	11	7					1359	1385		10.1007/s13042-019-01044-y		DEC 2019											
J								Multiobjective hybrid monarch butterfly optimization for imbalanced disease classification problem	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-objective optimization; SVM; Evolutionary algorithm; Totally uni-modular matrix; Limit-points	NEURAL-NETWORK; ENSEMBLE; PERFORMANCE; PREDICTION; ALGORITHM; INSIGHT; NOISY	Datasets obtained from the real world are far from balanced, particularly for disease datasets, since such datasets are usually highly skewed having a few minority classes apart from one or more prominent majority classes. In this research, we put forward the novel hybrid architecture to handle imbalanced binary disease datasets that arrives upon the efficient combination of Support vector machine (SVM) classifier's sensitive parameter values for improved performance of SVM by means of an Evolutionary algorithm (EA), namely monarch butterfly optimization (MBO). In this paper, MBO is used to enumerate three objectives, namely prediction accuracy (PAC), sensitivity (SEN), specificity (SPE). Additionally, we propose a Totally uni-modular matrix (TUM) and limit points based non-dominated solutions selection for deciding local and global search and to generate an efficient initial population respectively. Since these two greatly affect the performance of EAs, the performance of the proposed hybrid architecture is tested on 18 disease datasets having binary class labels and the results obtained demonstrate improvements using the proposed method. For the majority of the datasets, either 100% sensitivity and/or specificity were attained. Moreover, pertinent statistical tests were carried out to ascertain the performances obtained.																	1868-8071	1868-808X				JUL	2020	11	7					1423	1451		10.1007/s13042-019-01047-9		DEC 2019											
J								KYP modeling architecture for cardiovascular diseases and treatments in healthcare institutions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Decision support systems in healthcare; Business intelligence; Automated systems; Feature extraction; Heart disease; Healthcare informatics	SELECTION	Healthcare industry is a platform which is introduced with immense technological advancements every day. Taking a populous country like India in picture and limited manpower in inevitable medical institutions, healthcare industries need alternate and foolproof techniques to ensure that quality and round the clock attention in provided to deserving people. Since automation and intelligent systems are developed in every domain known to mankind, hospitals and medical records, diagnosis, online pharmacies, surgeries operated by robots are coming into practice. This proposal is one of the promising solutions to limited manpower and decision support system to the medical practitioners. With the ideal knowledge of background information, literature and proposed algorithm, the proposal delivers architecture for supporting automated decisions to healthcare institutions. Electronic records are continuously collected and organized to provide a detailed history of patients, their diseases and diagnosis plans. The institution maintains a sufficient Datawarehouse for holding these diverse medical records and provides access to registered and trusted practitioners. The Datawarehouse regularly update the records and maintained by independent organization. From the obtained information, virtual doctoring engine (VDE) attempts to analyze the observable characteristics from the datasets using Known-Yet-Predict algorithm to propose an optimal diagnosis plan. This treatment plan will later be officiated by a doctor for treating the patients. The performance of VDE system is tested against patients suffering from cardiovascular diseases. The characteristics taken into consideration vary accordingly among individual patients and thus demands heterogeneous features to be extracted from patients' electronic medical records. This approach has been investigated against various feature extraction algorithms and found to be 18.2% more accurate in predicting the optimal treatment plan. Steps to overcome missing information are also proposed in this model for ensuring an efficient decision support system.																	1868-5137	1868-5145															10.1007/s12652-019-01653-z		DEC 2019											
J								Diagnosis model of pancreatic cancer based on fusion of distribution estimation algorithm and genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Distribution estimation algorithm; Genetic algorithm; Pancreatic cancer diagnosis; Model evaluation		Since the beginning of the twenty-first century, people's living standards have been continuously improved, followed by changes in diet structure and living habits. These changes have affected the body's endocrine system, causing lesions in the pancreatic tissue. Among these pancreatic tissue diseases, pancreatic cancer is the most harmful to human health because of its inability to find and high mortality within 1 year. At present, in the diagnosis of pancreatic cancer, medical imaging and pathological puncture are the main methods of diagnosis. These methods have a high diagnostic rate for patients with advanced pancreatic cancer, but it is difficult to apply to the diagnosis of early pancreatic cancer. In response to these problems, this paper proposes a pancreatic cancer diagnosis model based on the fusion of distribution estimation algorithm and genetic algorithm. By collecting pathological data of patients with pancreatic cancer from a hospital oncology, pathological data include clinical manifestations of pancreatic cancer patients, serum tumor markers, etc., after data preprocessing, input models, and then use different machine learning classification algorithms to make pancreatic cancer for diagnosis. By evaluating the diagnosis results of each classification algorithm, an optimal classification algorithm is obtained and applied to the diagnosis model of pancreatic cancer. The results show that compared with other classification algorithms, the model using classification algorithm has the highest accuracy, recall rate and harmonic mean, and the diagnostic performance is the best. The results show that the diagnostic model constructed in this paper has a very high application value in the early auxiliary pre-diagnosis of pancreatic cancer.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5425	5434		10.1007/s00521-019-04684-x		DEC 2019											
J								Learning inverse dynamics for human locomotion analysis	NEURAL COMPUTING & APPLICATIONS										Inverse dynamics; Deep neural networks; Human locomotion; Gait phase detection	GROUND REACTION FORCES; NEURAL-NETWORK; HUMAN MOTION; KINEMATICS; OPTIMIZATION; PREDICTION; POSE; GAIT	In this work, learning-based inverse dynamics algorithms are proposed for the analysis of human motion. Immeasurable joint torques and exterior contact forces are directly estimated from motions by machine learning techniques including deep neural networks, random forests and Ridge regression. A multistage subclass approach is introduced. The method recovers occluded motion data and generates meaningful features, as well as gait phase labels to restrict and facilitate the regression of forces and moments. In contrast to the state-of-the-art inverse dynamics optimization, the learning-based methods are independent of ground reaction force measurements and the global position and orientation of the human body. These properties make the application to reconstructed poses from videos or inertial measurements possible, creating fast and simple access to the underlying dynamics of recorded human motions. The performance of the proposed methods is evaluated on a self-recorded data set including walking and running motions and on a publicly available gait data set by Fukuchi et al. (PeerJ 6:e4640, 2018). Furthermore, the applicability to reconstructed gait sequences taken from the well-known CMU database (Human motion capture database, 2014. ) is investigated. Finally, the method is tested as a tool to detect abnormal torque distributions in gait, based on a reconstructed 3D motion of a limping subject.																	0941-0643	1433-3058				AUG	2020	32	15					11729	11743		10.1007/s00521-019-04658-z		DEC 2019											
J								Wajsberg algebras of order n(n <= 9)	NEURAL COMPUTING & APPLICATIONS										MV-algebras; Wajsberg algebras; BCK-commutative algebras		Knowing the applications of logical algebras in various fields, such as artificial intelligence or coding theory, in this paper, we study some properties of a special class of such algebras, namely finite Wajsberg algebras. For this purpose, we give a representation theorem for finite Wajsberg algebras and give a formula for the number of non-isomorphic Wajsberg algebras of order n; also we give the total number of finite Wajsberg algebras of order n. Since a big value of n involves a lot of computations, as examples, we present and describe all finite Wajsberg algebras of order n <= 9.																	0941-0643	1433-3058				SEP	2020	32	17					13301	13312		10.1007/s00521-019-04676-x		DEC 2019											
J								A novel topic model for documents by incorporating semantic relations between words	SOFT COMPUTING										Topic model; Must-links; Cannot-links; Word embeddings; Gibbs sampling		Topic models have been widely used to infer latent topics in text documents. However, the unsupervised topic models often result in incoherent topics, which always confused users in applications. Incorporating prior domain knowledge into topic models is an effective strategy to extract coherent and meaningful topics. In this paper, we go one step further to explore how different forms of prior semantic relations of words can be encoded into models to improve the performance of topic modeling process. We develop a novel topic model-called Mixed Word Correlation Knowledge-based Latent Dirichlet Allocation-to infer latent topics from text corpus. Specifically, the proposed model mines two forms of lexical semantic knowledge based on recent progress in word embedding, which can represent semantic information of words in a continuous vector space. To incorporate generated prior knowledge, a Mixed Markov Random Field is constructed over the latent topic layer to regularize the topic assignment of each word during the topic sampling process. Experimental results on two public benchmark datasets illustrate the superior performance of the proposed approach over several state-of-the-art baseline models.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11407	11423		10.1007/s00500-019-04604-0		DEC 2019											
J								Engaging soft computing in material and modeling uncertainty quantification of dam engineering problems	SOFT COMPUTING										Soft computing; Uncertainty quantification; Dam class; Machine learning; Response prediction; Big data	CONCRETE GRAVITY DAMS; RELIABILITY-ANALYSIS; SEISMIC CRACKING; REGRESSION; IDENTIFICATION; RISK	Due to complex nature of nearly all infrastructures (and more specifically concrete dams), the uncertainty quantification is an inseparable part of risk assessment. Uncertainties might be propagated in different aspects depending on their relative importance such as epistemic and aleatory, or spatial and temporal. The objective of this paper is to focus on the material and modeling uncertainties, and to couple them with soft computing techniques aiming to reduce the computational burden of the conventional Monte Carlo-based finite element simulations. Several scenarios are considered in which the concrete and foundation material properties, the water level, and the dam geometry are assumed as random variables. Five soft computing techniques (i.e., random forest, boosted regression trees, multi-adaptive regression splines, artificial neural networks, and support vector machines) are employed to predict various quantities of interest based on different training sizes. It is argued that the artificial neural network is the most accurate algorithm in majority of cases, with enough accuracy as to be useful in reliability analysis as a complement to numerical models. The results with 200 samples in the training set are enough for reaching useful accuracy in most cases. For the simple prediction tasks, the results were predicted with less than 1% error. It is observed that increasing the number of input parameters increases the prediction error. The partial dependence plots provided most sensitive variables in dam design, which were consistent with the physics of the problem. Finally, several practical recommendations are provided for future applications.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11583	11604		10.1007/s00500-019-04623-x		DEC 2019											
J								Multivector particle swarm optimization algorithm	SOFT COMPUTING										Optimization; Meta-heuristic; Particle swarm; Particle position; Swarm intelligence	SEARCH ALGORITHM	This paper proposes an improved meta-heuristic algorithm called multivector particle swarm optimization (MVPSO) for solving single-objective optimization problems. MVPSO improves particle swarm optimization (PSO) algorithm by creating more possible solutions for each particle during the optimization process. It proposes a mathematical model and new position vectors for each particle that enhance the particle movement toward the global best value. This improvement emphasizes the exploration and exploitation of the particles in the search space during the optimization process. To test the performance of MVPSO, the algorithm is then benchmarked on 23 well-known test functions including unimodal, multimodal and fixed multimodal functions at different dimensions. These benchmark functions test the exploration, exploitation, local optima avoidance and convergence features of MVPSO. MVPSO has been compared to the state-of-the-art swarm optimization algorithms as well as PSO algorithm. Experimental results indicate that in terms of robustness, stability and quality of the solution obtained, MVPSO is better than original PSO algorithm, especially as the dimension increases. Further, it shows that a MVPSO based on the multivector mathematical model is competitive with the state-of-the-art swarm optimization algorithms. Moreover, the results of the tested benchmark functions, statistical analysis and performance metrics prove that the proposed algorithm is able to explore more solutions and regions in the search space, avoiding local optima points.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11695	11713		10.1007/s00500-019-04631-x		DEC 2019											
J								A novel systematic approach to diagnose brain tumor using integrated type-II fuzzy logic and ANFIS (adaptive neuro-fuzzy inference system) model	SOFT COMPUTING										Brain tumor; Clustering; Feature extraction; Classification; Type-II fuzzy inference system; Adaptive neuro-fuzzy inference system	SEGMENTATION; ALGORITHM; NETWORK	Brain tumor is an alarming threat among children and adults worldwide. Early detection and proper diagnosis of the tumor can enhance the chance of accurate survival among the individuals. Segmentation and classification of the detected tumor are based on its grade, i.e., criticality intensifies the survival rate and accurate treatment planning. However, manual segmentation of gliomas is time-consuming and results in an inaccurate diagnosis. Prompted by these facts, a multi-module automated framework has been developed to segment the brain multi-resonance images and classify it into two major classes, namely benign (low-grade) and malignant (high-grade). The present work is divided into four distinct modules: pre-processing, segmentation (clustering), feature extraction and classification. An efficient segmentation technique of the glioma images is proposed, which thereby provides a novel approach for the detection algorithm. Subsequently, prominent features characterizing mass effect, contrast, midline shift and irregularity of the edges of the tumor that are necessary for the physicians to detect tumor, are extracted. Using an ensemble of type-II fuzzy inference system and adaptive neuro-fuzzy inference system, a novel classifying technique has been developed to classify the detected tumor incorporating the extracted features. Finally, the research is tested and validated to show its consistency and accuracy using the images of patients of the BRATS dataset where the ground truth is made available. The detailed implementation of the proposed hybrid model is accomplished to establish its superiority in recognizing the grade of the tumor over other models mentioned in the literature survey.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11731	11754		10.1007/s00500-019-04635-7		DEC 2019											
J								A robust and efficient convolutional deep learning framework for age-invariant face recognition	EXPERT SYSTEMS										age-invariant features; face recognition; deep learning; facial ageing; data augmentation	VERIFICATION	Extensive research has been carried out in the past on face recognition, face detection, and age estimation. However, age-invariant face recognition (AIFR) has not been explored that thoroughly. The facial appearance of a person changes considerably over time that results in introducing significant intraclass variations, which makes AIFR a very challenging task. Most of the face recognition studies that have addressed the ageing problem in the past have employed complex models and handcrafted features with strong parametric assumptions. In this work, we propose a novel deep learning framework that extracts age-invariant and generalized features from facial images of the subjects. The proposed model trained on facial images from a minor part (20-30%) of lifespan of subjects correctly identifies them throughout their lifespan. A variety of pretrained 2D convolutional neural networks are compared in terms of accuracy, time, and computational complexity to select the most suitable network for AIFR. Extensive experimental results are carried out on the popular and challenging face and gesture recognition network ageing dataset. The proposed method achieves promising results and outperforms the state-of-the-art AIFR models by achieving an accuracy of 99%, which proves the effectiveness of deep learning in facial ageing research.																	0266-4720	1468-0394				JUN	2020	37	3			SI				e12503	10.1111/exsy.12503		DEC 2019											
J								Reducing the number of function evaluations in derivative-free algorithm for bound constrained optimization	EVOLUTIONARY INTELLIGENCE										Derivative-free algorithm; Bound constrained optimization; Number of function evaluations; Modified minimal positive base; Technique of search direction rotation	WORST-CASE COMPLEXITY; SEARCH ALGORITHMS	In this paper, we present a derivative-free algorithm based on modified minimal positive base for bound constrained optimization problems. Compared with the derivative-free algorithms based on the maximal 2n positive base, the algorithms based on the minimal n + 1 positive base only need at most n + 1 function evaluations at every iteration, where n is the number of variables. Therefore, we can reduce the number of function evaluations from 2n to n + 1 at each iteration. But the minimal positive base can cause undesirable large angles between some positive base directions and large unexplored feasible domain. In order to overcome this defect, we propose a modified set of feasible directions based on the minimal positive base and the technique of search direction rotation to investigate the unexplored domain at the next iteration. Accordingly, convergence to stationary points is proved. Moreover, the numerical experiments show that the method based on modified minimal positive base can reduce the number of function evaluations and is beneficial in a derivative-free context.																	1864-5909	1864-5917															10.1007/s12065-019-00324-4		DEC 2019											
J								Hand rehabilitation assessment system using leap motion controller	AI & SOCIETY										Hand rehabilitation; K-nearest neighbor; Leap motion controller	STROKE REHABILITATION; GESTURE RECOGNITION; VIRTUAL-REALITY	This paper presents an approach for monitoring exercises of hand rehabilitation for post stroke patients. The developed solution uses a leap motion controller as hand-tracking device and embeds a supervised machine learning. The K-nearest neighbor methodology is adopted for automatically characterizing the physiotherapist or helper hand movement resulting a unique movement pattern that constitutes the basis of the rehabilitation process. In the second stage, an evaluation of the patients rehabilitation exercises results is compared to the movement pattern of the patient and results are presented, saved and statistically analyzed. Physicians and physiotherapists monitor and assess patients' rehabilitation improvements through a web application, furthermore, offer medical assisted rehabilitation processes through low cost technology, which can be easily exploited at home. Recorded tracked motion data and results can be used for further medical study and evaluating rehabilitation trends according to patient's rehabilitation practice and improvement.																	0951-5666	1435-5655				SEP	2020	35	3					581	594		10.1007/s00146-019-00925-8		DEC 2019											
J								A novel hardware-oriented ultra-high-speed object detection algorithm based on convolutional neural network	JOURNAL OF REAL-TIME IMAGE PROCESSING										FPGA implementation; High-speed vision; Fast-object detection; Convolutional neural network		This paper describes a hardware-oriented two-stage algorithm that can be deployed in a resource-limited field-programmable gate array (FPGA) for fast-object detection and recognition with out external memory. The first stage is the bounding boxes proposal with a conventional object detection method, and the second is convolutional neural network (CNN)-based classification for accuracy improvement. Frequently accessing external memories significantly affects the execution efficiency of object classification. Unfortunately, the existing CNN models with a large number of parameters are difficult to deploy in FPGAs with limited on-chip memory resources. In this study, we designed a compact CNN model and performed the hardware-oriented quantization for parameters and intermediate results. As a result, CNN-based ultra-fast-object classification was realized with all parameters and intermediate results stored on chip. Several evaluations were performed to demonstrate the performance of the proposed algorithm. The object classification module consumes only 163.67 Kbits of on-chip memories for ten regions of interest (ROIs), this is suitable for low-end FPGA devices. In the aspect of accuracy, our method provides a correctness rate of 98.01% in open-source data set MNIST and over 96.5% in other three self-built data sets, which is distinctly better than conventional ultra-high-speed object detection algorithms.																	1861-8200	1861-8219				OCT	2020	17	5					1703	1714		10.1007/s11554-019-00931-5		DEC 2019											
J								On hesitant neutrosophic rough set over two universes and its application	ARTIFICIAL INTELLIGENCE REVIEW										Single valued neutrosophic sets; Hesitant fuzzy sets; Hesitant neutrosophic sets; Hesitant neutrosophic rough set; Decision making	DECISION-MAKING METHOD; CORRELATION-COEFFICIENTS; SIMILARITY MEASURES; FUZZY-SETS; AGGREGATION; MODEL	As a further generalization of the concepts of fuzzy set, intuitionistic fuzzy set, single valued neutrosophic refined set, hesitant fuzzy set, and dual hesitant fuzzy set, Ye (J Intell Syst 24(1):23-36, 2015) proposed the concept of hesitant neutrosophic sets (also called single valued neutrosophic hesitant fuzzy sets). Following the idea of hesitant neutrosophic sets as introduced by Ye, in this paper, the model of hesitant neutrosophic rough sets is proposed, then the join semi-lattice structure of lower and upper hesitant neutrosophic rough approximation operators over two universes is given. In addition, an algorithm to handle decision making problem in medical diagnosis based on hesitant neutrosophic rough sets over two universes is provided. Finally, a numerical example is employed to demonstrate the validness of the proposed hesitant neutrosophic rough sets.																	0269-2821	1573-7462				AUG	2020	53	6					4387	4406		10.1007/s10462-019-09795-4		DEC 2019											
J								Whale Optimization Controller for Load Frequency Control of a Two-Area Multi-source Deregulated Power System	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Automatic generation control; Capacitive energy storage; Deregulation; Load frequency control; Multi-source power system; Sine-cosine algorithm; Whale optimization algorithm	AUTOMATIC-GENERATION CONTROL; ENERGY-STORAGE; CONTROL STRATEGY; PID CONTROLLER; AGC; DESIGN; ALGORITHM; IMPACT; MODEL; SSSC	This article discusses a Whale Optimization (WO) controller for design analysis, stability management, application, and performance analysis of a deregulated two-area multi-source energy system. Within the power grid, the Automatic Generation Control (AGC) maintains the power balance and strengthens the sudden frequency disturbance of the grid. Various researchers are targeted many kinds of fast-acting energy-storing devices to dampen space frequency oscillations and diversion of the tie line because of unexpected load variation. To regulate the power flow between multi-areas, load frequency control (LFC) is used by keeping frequency constant. But, to design a controller for minimizing the issues of LFC remains a challenge within the deregulated atmosphere. The main objective of the proposed methodology is to design a controller for minimizing the issues in LFC within the deregulated atmosphere. A two-area deregulated hydrothermal power system is employed to analyze many LFC problems in the deregulated power grid. During this modeling, the Thyristor-Controlled Phase Shifters (TCPS) and Capacitive Energy Storage unit (CES) are placed in the tie line and its control space. To mitigate the various issues of LFC in a two-area deregulated hydrothermal power system, the CES-TCPS combination is placed between the two control areas and the transient performance of constant is evaluated. The WO controller will increase the system power size to focus on the better performance of low-frequency management tasks. In addition, the effectiveness of the WO algorithm in tuning the proportional integral controller is also compared in a different contract (unilateral, bilateral and contract violation cases) scenarios of the proposed deregulated power system. Finally, the simulation is carried out in MATLAB/Simulink platform and the results obtained for the WO controller is compared with the normal and fuzzy PI controllers.																	1562-2479	2199-3211				FEB	2020	22	1					122	137		10.1007/s40815-019-00761-4		DEC 2019											
J								An updated dashboard of complete search FSM implementations in centralized graph transaction databases	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Graph mining; Graph transaction databases; Centralized environment; Frequent subgraph mining (FSM); FSM algorithms; Complete search	FREQUENT SUBGRAPH; ALGORITHM; PATTERNS	Frequent subgraph mining algorithms are widely used in various areas for information analysis. As yet, a handful of algorithms have been proposed and defined in the literature. While several experimental studies were reported, these experiments lack critical information which are important for selecting an implementation of an algorithm for a specific case of use. In this paper, we report on experiments that we carried out on available implementations of complete search Frequent Subgraph Mining (FSM) algorithms. These experiments are conducted in order to choose a suitable FSM solution (i.e., implementation). We identified 32 algorithms in the literature, six of them were selected for our experiments, through a filtering process relying on a set of criteria. Thirteen working implementations of these 6 algorithms are experimented. In this paper, we provide details of the experiments in terms of performance metrics and input variation effect. We propose a preliminary selection of the most efficient FSM solutions for end users, based on the most tested centralized graph-transaction datasets of the literature.																	0925-9902	1573-7675				AUG	2020	55	1					149	182		10.1007/s10844-019-00579-4		DEC 2019											
J								Error-Related Neural Responses Recorded by Electroencephalography During Post-stroke Rehabilitation Movements	FRONTIERS IN NEUROROBOTICS										assist-as-needed; brain-computer interface (BCI); electroencephalography (EEG); error-related potential (ErrP); stroke rehabilitation	BRAIN ACTIVITY; EEG; POTENTIALS; COMPONENT; IMPROVES; STROKE; N400; ERN	Error-related potential (ErrP) based assist-as-needed robot-therapy can be an effective rehabilitation method. To date, several studies have shown the presence of ErrP under various task situations. However, in the context of assist-as-needed methods, the existence of ErrP is unexplored. Therefore, the principal objective of this study is to determine if an ErrP can be evoked when a subject is unable to complete a physical exercise in a given time. Fifteen stroke patients participated in an experiment that involved performing a physical rehabilitation exercise. Results showed that the electroencephalographic (EEG) response of the trials, where patients failed to complete the exercise, against the trials, where patients successfully completed the exercise, significantly differ from each other, and the resulting difference of event-related potentials resembles the previously reported ErrP signals as well as has some unique features. Along with the highly statistically significant difference, the trials differ in time-frequency patterns and scalp distribution maps. In summary, the results of the study provide a novel basis for the detection of the failure against the success events while executing rehabilitation exercises that can be used to improve the state-of-the-art robot-assisted rehabilitation methods.																	1662-5218					DEC 20	2019	13								107	10.3389/fnbot.2019.00107													
J								Constraints for generating graphs with imposed and forbidden patterns: an application to molecular graphs	CONSTRAINTS										Graph theory; Forbidden subgraph patterns; Constraint satisfaction problem; Constraint programming; Molecular generation	STRUCTURE ELUCIDATION; SYSTEM	Although graphs are widely used to encode and solve various computational problems, little research exists on constrained graph construction. The current research was carried out to shed light on the problem of generating graphs, where the construction process is guided by various structural restrictions, like vertex degrees, proximity among vertices, and imposed and forbidden patterns. The main contribution of this paper is an encoding of the constrained graph generation problem in terms of a constraint satisfaction problem (CSP). This approach is motivated by the flurry of efficient solution algorithms available within the constraint programming (CP) framework. The obtained encoding has given rise to the CP-MolGen program, a new open source program dedicated to the generation of molecular graphs with imposed and forbidden fragments. Experimental results on several real-world molecular graph generation instances have shown the effectiveness and efficiency of the proposed program, especially the benefits of forbidding cyclic patterns as induced subgraphs.																	1383-7133	1572-9354				APR	2020	25	1-2					1	22		10.1007/s10601-019-09305-x		DEC 2019											
J								Hybrid ACO algorithm for edge detection	EVOLVING SYSTEMS										Algorithms; Ant colony optimization; Edge detection methods; Image processing		Ant colony optimization is a metaheuristic where a colony of artificial ants cooperate to find good solutions to different optimization problems. Edge detection plays an important role in image processing. It consists in detecting edges or contours in images that allow to extract relevant information. Here, an algorithm based on the ACO metaheuristic for edge detection is proposed. Using heuristic and knowledge information in the construction phase and a repair operator in the improvement phase, a binary image containing detected edges is reached. Our proposal was tested with several images in and without presence of noise. Results are competitive in terms of output images, effectiveness and CPU time.																	1868-6478	1868-6486															10.1007/s12530-019-09321-5		DEC 2019											
J								Parameter self-tuning schemes for the two phase test sample sparse representation classifier	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Sparse representation; Two phase test sample sparse representation; Parameter self-tuning; Clusters; Classification; Performance evaluation	ROBUST FACE RECOGNITION; SELECTION; MACHINE	Sparse Representation Classifier (SRC) and its variants were considered as powerful classifiers in the domains of computer vision and pattern recognition. However, classifying test samples is computationally expensive due to the l1norm minimization problem that should be solved in order to get the sparse code. Therefore, these classifiers could not be the right choice for scenarios requiring fast classification. In order to overcome the expensive computational cost of SRC, a two-phase coding classifier based on classic Regularized Least Square was proposed. This classifier is more efficient than SRC. A significant limitation of this classifier is the fact that the number of the samples that should be handed over to the next coding phase should be specified a priori. This paper overcomes this main limitation and proposes five data-driven schemes allowing an automatic estimation of the optimal size of the local samples. These schemes handle the three cases that are encountered in any learning system: supervised, unsupervised, and semi-supervised. Experiments are conducted on five image datasets. These experiments show that the introduced learning schemes can improve the performance of the two-phase linear coding classifier adopting ad-hoc choices for the number of local samples.																	1868-8071	1868-808X				JUL	2020	11	7					1387	1403		10.1007/s13042-019-01045-x		DEC 2019											
J								An online PLA algorithm with maximum error bound for generating optimal mixed-segments	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Time series; Piecewise linear approximation; Maximum error bound	TIME-SERIES	Piecewise Linear Approximation (PLA) is an effective method used to represent and compress a time series. It divides a time series into a number of segments, each of which is approximated by a straight line. This division and approximation is done under a metric enforcing optimized storage and compressed data quality criteria. In this article, we propose a new optimal linear-time PLA algorithm (SemiMixedAlg) for generating a set of mixed-connected (continue and disconnected segments) with guaranteed maximum error and minimized storage. An efficient "k-length" strategy is designed to determine the location of mixed segments in order to minimize the storage of mixed-connected segments. Our experiments on 43 real-world data sets show that SemiMixedAlg achieves exactly the same results as that of PipeMixedAlg (Luo et al. in Piecewise linear approximation of streaming time series data with max-error guarantees. In: IEEE international conference on data engineering, pp 173-184); the only state of the art algorithm, but with much lower time and memory costs.																	1868-8071	1868-808X				JUL	2020	11	7					1483	1499		10.1007/s13042-019-01052-y		DEC 2019											
J								Fall detection based on posture classification for smart home environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fall detection; Posture recognition; Normal activity; Machine learning; Classification	DETECTION SYSTEM; SENSOR	Automatic human fall detection is one of the major components in the elderly monitoring system. Detection of human fall in a smart home environment can be utilized as a means to improve the quality of care offered to elderly people thus reducing the risk factor when they are alone. Recently various fall detection approaches have been proposed, among which computer vision based approaches offer promising and effective solutions. In this paper, an analysis of fall detection is carrier out based on the automatic, feature learning in a hybrid approach. Initially, a model is generated using the training dataset that contains samples of both fall and normal active events. Then key frames are extracted from the video sequence that is subjected to two stream classification. The classification results are approved if both the streams project the same results, failing so, additional information are used to classify the fall from the normal activity. The selection of key frames depends on the displacement in the centroid of the detected object have threshold greater than the predefined value. Experiments show that the proposed approach achieves reliable results compared with other methods, and a better result is achieved in our method even when training with fewer training samples.																	1868-5137	1868-5145															10.1007/s12652-019-01600-y		DEC 2019											
J								What is of interest for tourists in an alpine destination: personalized recommendations for daily activities based on view data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender system; Personalized; Collaborative filtering; Tourism; Implicit feedback	HYBRID RECOMMENDATION; INFORMATION OVERLOAD; SYSTEMS; STATE; MANAGEMENT; DECISION; INTERNET; ONLINE; SEARCH; WEB	Smartphones are nowadays important tools for tourists. For instance, while on the go in a destination, tourists can use smartphones to find places of interest and to identify activities that might be of interest, as well as a wealth of related information and even special offers in real-time. However, it is time consuming and not easy for tourists in an unknown destination to choose among the numerous options available. Recommendations for instance from other tourists with similar interests would help immensely. Indeed, places and activities are often reviewed and rated by other tourists, however, this information is typically not personalized. This article proposes a recommender system as part of an evolving mobile destination app. Our recommender app is capable of providing personalized recommendations to tourists thereby facilitating and enriching tourists' experience and stay. This work is based on two qualitative studies towards exploring the information needs of tourists in an alpine destination. These studies were conducted using the mobile ethnography approach and semi-structured interviews. A hybrid recommender system is proposed that uses implicit user feedback in the form of view duration. The proposed system was tested using real data derived from tourists using the mobile app in a Swiss alpine destination. The results of these experiments demonstrate that the system is capable of providing high-quality and diverse recommendations. The core contribution of this work lies in the transformation of the viewing durations to a set of preference values and in learning the optimized weights of the parameters of a hybrid system utilizing an energy minimization framework.																	1868-5137	1868-5145															10.1007/s12652-019-01619-1		DEC 2019											
J								Time/sequence-dependent scheduling: the design and evaluation of a general purpose tabu-based adaptive large neighbourhood search algorithm	JOURNAL OF INTELLIGENT MANUFACTURING										Time; sequence-dependent scheduling; Time windows; Hybrid algorithms; Adaptive large neighbourhood search; Tabu search; Factor analysis	MACHINE ORDER ACCEPTANCE; SINGLE-MACHINE; HYBRID	In intelligent manufacturing, it is important to schedule orders from customers efficiently. Make-to-order companies may have to reject or postpone orders when the production capacity does not meet the demand. Many such real-world scheduling problems are characterised by processing times being dependent on the start time (time dependency) or on the preceding orders (sequence dependency), and typically have an earliest and latest possible start time. We introduce and analyze four algorithmic ideas for this class of time/sequence-dependent over-subscribed scheduling problems with time windows: a novel hybridization of adaptive large neighbourhood search (ALNS) and tabu search (TS), a new randomization strategy for neighbourhood operators, a partial sequence dominance heuristic, and a fast insertion strategy. Through factor analysis, we demonstrate the performance of these new algorithmic features on problem domains with varying properties. Evaluation of the resulting general purpose algorithm on three domains-an order acceptance and scheduling problem, a real-world multi-orbit agile Earth observation satellite scheduling problem, and a time-dependent orienteering problem with time windows-shows that our hybrid algorithm robustly outperforms general algorithms including a mixed integer programming method, a constraint programming method, recent state-of-the-art problem-dependent meta-heuristic methods, and a two-stage hybridization of ALNS and TS.																	0956-5515	1572-8145				APR	2020	31	4					1051	1078		10.1007/s10845-019-01518-4		DEC 2019											
J								A Dataset Schema for Cooperative Learning from Demonstration in Multi-robot Systems	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-Agent system; Learning from demonstration; Dataset; Coordination; Multi-robot plan; Clustering	COORDINATION; SELECTION	Multi-Agent Systems (MASs) have been used to solve complex problems that demand intelligent agents working together to reach the desired goals. These Agents should effectively synchronize their individual behaviors so that they can act as a team in a coordinated manner to achieve the common goal of the whole system. One of the main issues in MASs is the agents' coordination; it is common for domain experts observing MASs execution to disapprove agents' decisions. Even if the MAS was designed using the best methods and tools for agents' coordination, this difference in decisions between experts and MAS is confirmed. Therefore, this paper proposes a new dataset schema to support learning the coordinated behavior in MASs from demonstration. The results of the proposed solution are validated in a Multi-Robot System (MRS) organizing a collection of new cooperative plan recommendations from the demonstration by domain experts.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		589	608		10.1007/s10846-019-01123-w		DEC 2019											
J								Incorporating monotonic domain knowledge in support vector learning for data mining regression problems	NEURAL COMPUTING & APPLICATIONS										Data mining; Monotonicity constraint; Support vector regression; Prior domain knowledge	APPROXIMATION; ALGORITHM; MACHINES	A common problem of data-driven data mining methods is that they might lack considering domain knowledge, despite possibly having high accuracy with respect to the data. As such, prior knowledge plays an important role in many data mining applications. Incorporating prior knowledge into data mining techniques is not trivial and remains a partially open issue drawing much attention. In this paper, we propose a new support vector regression (SVR) model that takes into account the prior knowledge of domain experts in the form of inequalities, which reflect the monotonic relationship between the output and some of the attributes of the input. A dual quadratic programming problem corresponding to the SVR model is derived, along with algorithms for solving it and creating constraints, respectively. The experiment results, which were conducted on two artificial and two practical datasets, show that the proposed model, which considers the monotonicity defined by domain experts, performs better than the original SVR. Moreover, the proposed method is also suitable for prior domain knowledge of piecewise monotonicity.																	0941-0643	1433-3058				AUG	2020	32	15					11791	11805		10.1007/s00521-019-04661-4		DEC 2019											
J								Approaches to multiple attribute group decision making based on triangular cubic linguistic uncertain fuzzy aggregation operators	SOFT COMPUTING										Triangular cubic linguistic uncertain fuzzy sets; Aggregation operators on triangular cubic linguistic uncertain fuzzy information; Multiple attribute group decision making; Numerical application	INTERVAL; ALGORITHM; MODEL; SCALE	In this paper, triangular cubic linguistic uncertain fuzzy averaging (geometric) operator, triangular cubic linguistic uncertain fuzzy weighted averaging operator, triangular cubic linguistic uncertain fuzzy weighted geometric operator, triangular cubic linguistic uncertain fuzzy ordered weighted averaging operator, triangular cubic linguistic uncertain fuzzy ordered weighted geometric operator, triangular cubic linguistic uncertain fuzzy hybrid averaging operator, and triangular cubic linguistic uncertain fuzzy hybrid geometric operator for triangular cubic linguistic uncertain fuzzy numbers have been introduced. Furthermore, by using these aggregation operators an approach to multiple attribute group decision making with triangular cubic linguistic uncertain fuzzy information has been developed. Finally, a numerical example is constructed to validate the established approach.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11511	11533		10.1007/s00500-019-04614-y		DEC 2019											
J								Streaming generalized cross entropy	SOFT COMPUTING										Generalized Cross Entropy; Generalized Maximum Entropy; Streaming data; Adaptive process	MAXIMUM-ENTROPY; GME ESTIMATION; WORK	We propose a new method to combine adaptive processes with a class of entropy estimators for the case of streams of data. Starting from a first estimation obtained from a batch of initial data, model parameters are estimated at each step by combining the prior knowledge with the new observation (or a block of observations). This allows to extend the maximum entropy technique to a dynamical setting, also distinguishing between entropic contributions of the signal and the error. Furthermore, it provides a suitable approximation of standard GME problems when the exacted solutions are hard to evaluate. We test this method by performing numerical simulations at various sample sizes and batch dimensions. Moreover, we extend this analysis exploring intermediate cases between streaming GCE and standard GCE, i.e., considering blocks of observations of different sizes to update the estimates, and incorporating collinearity effects as well. The role of time in the balance between entropic contributions of signal and errors is further explored considering a variation of the Streaming GCE algorithm, namely Weighted Streaming GCE. Finally, we discuss the results: In particular, we highlight the main characteristics of this method, the range of application, and future perspectives.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13837	13851		10.1007/s00500-019-04632-w		DEC 2019											
J								An enhanced feature-based sentiment analysis approach	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										emoticons detection; machine learning; negation handling; opinion mining and sentiment analysis; spam reviews detection		In the last few years, online reviews where individuals express their thoughts, interests, experiences, and opinions have broadly spread over the internet. Sentiment analysis has evolved to analyze these online reviews and provide valuable insights for both individuals and organizations that may help them in making decisions. Unfortunately the performance of sentiment analysis process is affected by the nature of online reviews' content that may contain emoticons and negation words. Moreover, spam reviews have been written for the purpose of deceiving others. Therefore, there is a need to develop an approach that considers these issues. In this paper, an enhanced approach for sentiment analysis is proposed which aims to enhance the performance of classifying reviews based on their features and assigning accurate sentiment score to features. This enhanced approach is achieved by handling negation, detecting emoticons, and detecting spam reviews using a combination of different types of properties which leads to achieving better predictive performance. The proposed approach has been verified against three datasets of different sizes. The results indicate that the proposed approach achieves a maximum accuracy of about 99.06% in detecting spam reviews and a maximum accuracy of about 97.13% in classifying reviews. This article is categorized under: Algorithmic Development > Text Mining Technologies > Classification Technologies > Machine Learning																	1942-4787	1942-4795				MAR	2020	10	2							e1347	10.1002/widm.1347		DEC 2019											
J								Diffusion tensor imaging denoising based on Riemann nonlocal similarity	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diffusion tensor imaging; Riemannian manifold; Similarity measure; Bayesian inference	FRAMEWORK; MANIFOLDS	Diffusion tensor imaging (DTI) is a non-invasive magnetic resonance imaging technique and a special type of magnetic resonance imaging, which has been widely used to study the diffusion process in the brain. The signal-to-noise ratio of DTI data is relatively low, the shape and direction of the noisy tensor data are destroyed. This limits the development of DTI in clinical applications. In order to remove the Rician noise and preserve the diffusion tensor geometry of DTI, we propose a DTI denoising algorithm based on Riemann nonlocal similarity. Firstly, DTI tensor is mapped to the Riemannian manifold to preserve the structural properties of the tensor. The Riemann similarity measure is used to search for non-local similar blocks to form similar patch groups. Then the Gaussian mixture model is used to learn the prior distribution of patch groups. Finally, the noisy patch group is denoised by Bayesian inference and the denoised patch group is reconstructed to obtain the final denoised image. The denoising experiments of real and simulated DTI data are carried out to verify the feasibility and effectiveness of the proposed algorithm. The experimental results show that our algorithm not only effectively removes the Rician noise in the DTI image, but also preserves the nonlinear structure of the DTI image. Comparing to the existing denoising algorithms, our algorithm has better improvement of the principal diffusion direction, lower absolute error of fractional anisotropy and higher peak signal-to-noise ratio.																	1868-5137	1868-5145															10.1007/s12652-019-01642-2		DEC 2019											
J								Exudate characterization to diagnose diabetic retinopathy using generalized method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Exudate; ANFIS; Morphological component; Retinal	DECISION-SUPPORT-SYSTEM; RETINAL IMAGES; AUTOMATED DETECTION; ALGORITHM; TRANSFORM	Globally in recent days, the potential risk for patients with diabetes mellitus is the prevalence of diabetic retinopathy, which is a silent disease with no early symptoms and is the imperative cause of vision loss. An early diagnosis can be used to prevent for visual loss and blindness. In the regular screening process, assistance of computerized diagnosis can considerably minimize an ophthalmologists work and improve inter and intra viewer variability. A generalized method of semi automatic exudates characterization to diagnose diabetic retinopathy with exudates screening system of retinal image is presented in this paper. This system uses morphological processing based retinal blood vessel suppression, Semi automatic masking of optic disc structure and morphological component analysis based texture enhancement followed by segmentation and Adaptive Neuro-Fuzzy Inference System (ANFIS) based classification method to discriminate between normal and pathological retinal structures. The novelty of this system relies on the appropriate sequential application of exclusive image processing techniques in synergy with ANFIS classifier to improve the accuracy of exudate lesions characterization. The performance of the system has been evaluated by comparing it with various state of the art existing methods in terms of several performance metrics such as Accuracy, Average error rate, F-Score and Kappa value. The obtained numerical results prove that the proposed system with ANFIS classifier demonstrated superior performance in identification of exudate lesions.																	1868-5137	1868-5145															10.1007/s12652-019-01617-3		DEC 2019											
J								A new flexible pricing mechanism considering price-quality relation for cloud resource allocation	EVOLVING SYSTEMS										Cloud resource allocation; Quality of service (QoS); Negotiation; Price-quality relation; Fuzzy system	NEGOTIATION	Determining the true value of a bid received from a trading opponent with respect to different conditions of the negotiation market is an important challenge in negotiation-based cloud resource allocation. This means that, in some market conditions, the received bid is more valuable than it seems (and vice versa). The image constructed from the price bid of a trading opponent determined with respect to the market conditions is called TVB (True Value of Bid). Moreover, the pricing mechanism that supports the calculation of TVB by a resource provider is called FPM (Flexible Pricing Mechanism). In well-known cloud markets, where resource type instances are supplied with various QoS levels, the mentioned challenge becomes more complicated. This is because picking negotiation strategy based on the calculated TVB should not lead to the rental of resource type instances with a lower QoS at a price higher than that of the same resource type instances with a higher level of QoS. Therefore, the aim of this research is designing a flexible pricing mechanism that supports price-quality relation in the cloud markets where resource type instances with various QoS levels are supplied. The simulation results indicate that the proposed negotiators outperform two other negotiators in names EMDA (Enhanced Market Driven Agent) and FNSSA (Fuzzy Negotiation Strategy Selection Agent).																	1868-6478	1868-6486															10.1007/s12530-019-09315-3		DEC 2019											
J								Epileptic seizures identification with autoregressive model and firefly optimization based classification	EVOLVING SYSTEMS										Epileptic seizures classification; AR model; Firefly algorithm; SVM; Akaike information criterion	ARTIFICIAL NEURAL-NETWORK; EEG SIGNALS; SUPPORT	Identifying epilepsy cases and epileptic seizures from electroencephalogram (EEG) signals is a challenging issue, which usually needs high level of skilled neurophysiologists. Numerous works have attempted to develop tools that can provide an assistant to neurophysiologist in analyzing the EEG for epileptic seizures detection. This paper proposes a new automatic framework to identify and classify the epileptic seizure from EEG using a machine learning method. In particular, the feature extraction process of the proposed scheme utilizes autoregressive model (AR) and firefly optimization (FA) to procure an optimal model order (P). Namely, the main aim of FA is to find the best model order (P) with minimum residual variance using Akaike information criterion (AIC) as an objective function of FA algorithm. A support vector machine (SVM) classifier is employed for the classification of the epileptic seizures signals. The presented scheme is also effective for short segment of EEG signals owing to use of AR model in features extraction stage. Experiments with the publicly available Bonn database that is composed of healthy (nonepileptic), interictal and ictal EEG samples show promising results with high accuracy.																	1868-6478	1868-6486															10.1007/s12530-019-09319-z		DEC 2019											
J								Multi-area economic dispatching using improved grasshopper optimization algorithm	EVOLVING SYSTEMS										Multi-area economic dispatch; Grasshopper optimization algorithm; Logistic map; Tie line constraints	PARTICLE SWARM OPTIMIZATION; CHAOS OPTIMIZATION; MANAGEMENT	In this study a new optimization algorithm is presented to solve the multi-area economic dispatching (MAED) problem. The studied system includes the tie-line constraints including transmission losses, prohibited operating zones, multiple fuels, and valve-point loading. For optimized solving the MAED algorithm, grasshopper optimization algorithm (GOA) is utilized. GOA is a newly introduced swarm-based optimization algorithm that is inspired by the swarming behavior of the grasshopper insects. For validating the proposed method, it is compared with three different case studies and the results have been compared with some different meta-heuristics such as particle swarm optimization, artificial bee colony, evolutionary programming, and differential evolution to demonstrate the capability of the presented system to solve the MAED problems.																	1868-6478	1868-6486															10.1007/s12530-019-09320-6		DEC 2019											
J								Real-time watermark reconstruction for the identification of source information based on deep neural network	JOURNAL OF REAL-TIME IMAGE PROCESSING										Real-time source detection; Watermark reconstruction; Forwarded message identification; App source detection; Deep neural network	MACHINE LEARNING-METHODS; DIRECT-SEQUENCE CDMA; IMAGE WATERMARKING; SOCIAL MEDIA; DWT-SVD; ROBUST; PREDICTION; PRIVACY; SYSTEM; SCHEME	A novel deep neural network-based image watermarking method is presented to identify the source of digital data that is shared/forwarded on the internet using various messenger apps. The app that is used to share/communicate the image at the very first time is also identified in the proposed method. The ten-digit mobile number of the source (user) and identification data of particular messenger app (i.e. WhatsApp, Snapchat, Kik, Facebook messenger, etc.) is combined to get the text watermark signal. The part of the watermark signal representing specific mobile-based messenger application is obtained by randomizing the Walsh orthogonal codes using secret keys. To embed the watermark, the host image (shared/forwarded) is divided into blocks of equal size and then, slantlet transform is applied on each block. To get high reliability, three copies of the source information (user and app) are embedded during watermark embedding. Watermark extraction is performed using trained multilayer deep neural network. Furthermore, an optimal block selection logic is used to get improved results for real-time applications. The method is examined against various signal-processing attacks and high robustness with significant imperceptibility is attained. The method is also found to be fast enough for real-time applications. The prime objective of identifying the first user (source) and the shared/forwarded status (app detection) is successfully accomplished.																	1861-8200	1861-8219															10.1007/s11554-019-00937-z		DEC 2019											
J								Toward a Gecko-Inspired, Climbing Soft Robot	FRONTIERS IN NEUROROBOTICS										mobile soft robots; fast pneu-nets; apriltags; gecko-inspired robot; climbing robot		In this paper, we present a gecko-inspired soft robot that is able to climb inclined, flat surfaces. By changing the design of the previous version, the energy consumption of the robot could be reduced, and at the same time, its ability to climb and its speed of movement could be increased. As a result, the new prototype consumes only about a third of the energy of the previous version and manages to climb slopes of up to 84 degrees. In the horizontal plane, its velocity could be increased from 2 to 6 cm/s. We also provide a detailed analysis of the robot's straight gait.																	1662-5218					DEC 19	2019	13								106	10.3389/fnbot.2019.00106													
J								Rethinkingk-means clustering in the age of massive datasets: a constant-time approach	NEURAL COMPUTING & APPLICATIONS										k-means; Clustering; Efficiency; Scalable; Large datasets	K-MEANS ALGORITHM; APPROXIMATION; VERSION	We introduce a highly efficientk-means clustering approach. We show that the classical central limit theorem addresses a special case (k = 1) of thek-means problem and then extend it to the general case. Instead of using the full dataset, our algorithm namedk-means-lite applies the standardk-means to the combinationC(sizenk) of all sample centroids obtained fromnindependent small samples. Unlike ordinary uniform sampling, the approach asymptotically preserves the performance of the original algorithm. In our experiments with a wide range of synthetic and real-world datasets,k-means-lite matches the performance ofk-means whenCis constructed using 30 samples of size 40 + 2k. Although the 30-sample choice proves to be a generally reliable rule, when the proposed approach is used to scalek-means++ (we call this scaled versionk-means-lite++),k-means++' performance is matched in several cases, using only five samples. These two new algorithms are presented to demonstrate the proposed approach, but the approach can be applied to create a constant-time version of any otherk-means clustering algorithm, since it does not modify the internal workings of the base algorithm.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15445	15467		10.1007/s00521-019-04673-0		DEC 2019											
J								Real time prefix matching based IP lookup and update mechanism for efficient routing in networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IPv4 and IPv6; Addressing scheme; Lookup problem; Mapping	SCHEME; HARDWARE	IPv6 is another form of the inter networking convention intended to address the inadequacies of the present standard, IPv4. The deliver space is restricted to just 32-bits in IPv4 though IPv6 has 128-bits deliver not withstanding versatility requirement and system layer security. The principle issue is that IPv4 and IPv6 are not good at directly. Along with comprising lines, plans and structures to one standard can't express with those meant for the other. The lookup issue of IPv6 change enables the clients to service their hosts to IPv6, and the system administrators to send IPv6 in switches, with next to no coordination between the two. In the meantime, it has to rely on address translation mechanisms which facilitate communication between IPv4/IPv6 networks since migration from IPv4 to IPv6 cannot take place all of a sudden. Various lookup problem change tools have been produced to address interoperability of IPv4 and IPv6 systems and frameworks. Be that as it may, none of the current devices address basic issues like the noninvasive movement of basic heritage IPv4-just frameworks to IPv6, and operation of IPv4-just frameworks on IPv6-just center systems. This work exhibits an answer for these IP lookup issues. This paper address the problem of IPv6 address lookup problem and present a routing algorithm for both IPv4 and IPv6 machines which uses address prefix matching algorithm. The proposed method improves the performance of address lookup and reduces the latency as well.																	1868-5137	1868-5145															10.1007/s12652-019-01646-y		DEC 2019											
J								A secured multiplicative Diffie Hellman key exchange routing approach for mobile ad hoc network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MANET; Multiplicative Diffie Hellman key exchange; Advanced encryption standard; Access control; Authentication; Communication system security	MANAGEMENT; PROTOCOL	As related to environments which possess non-malicious nodes contained in a network, the well-known protocols that are provides by the existing researchers for routing are designed. There are several security threats which are prevented the mobile network development which essentially is an ad hoc network and this is because of the network's susceptible nature. Hence in order to secure the nodes from anonymous behaviours, an efficient routing algorithm is needed to provide as secure network. Secure routing based multiplicative Diffie Hellman key exchange (MDKE) algorithm is suggested in order to enhance MANET's security in this research paper and MDKESR is the framework being proposed. In order to enhance the packet delivery ratio, advanced encryption standard (AES) algorithm based encrypted data is utilized in this proposed scheme. By comparing the results security simulation by deployment of the already existing Routing otherwise known as SUPERMAN with IPSec, secure ad hoc on-demand distance vector (SAODV) and secure optimized link state routing (SOLSR) are taken into account in order to show that the proposed frameworks of MDKESR with respect to security which is a wireless communication.																	1868-5137	1868-5145															10.1007/s12652-019-01612-8		DEC 2019											
J								Analytics on real time security attacks in healthcare, retail and banking applications in the cloud	EVOLUTIONARY INTELLIGENCE										Cloud security; Data analytics; Security attacks; Security attack complexity; Security attack complexity; Attack mitigation time		Cloud computing is used by large enterprises to provide services to customers. The major advantage of adapting cloud systems is that the clients need not have to manage the maintenance and operations of the cloud systems along with the maintenance of hardware but they will have full control over the data. The downsides of the cloud systems are the security threats that includes confidentiality and security issues. Software as a service, platform as a service, and infrastructure as a service; all comes with its own security issues of different complexities. The security challenges posed by cloud computing systems are overshadowing the economic edge it has. Based on the analysis of the research performed in the area of security issues in the cloud it is evident that a detailed analysis on the same will throw light on root causes and vulnerabilities. The research primarily throws light on the statistics and detailed analysis on the security attacks in the cloud systems in different domains. The paper focuses on the issues and threats in healthcare, retail and banking domains. Analytics on the attacks in these domains will give insights on the loss, mitigation time etc. The real time data on security threats is acquired from IT cloud service providers. The statistics on the same will open up different perspectives of cloud security issues and can throw light on the possible research directions and that is the motivation behind this preliminary research.																	1864-5909	1864-5917															10.1007/s12065-019-00337-z		DEC 2019											
J								Behavioural artificial intelligence: an agenda for systematic empirical studies of artificial inference	AI & SOCIETY										Artificial intelligence; Artificial inference; Behavioral artificial intelligence; Artificial intelligent behaviour; Bias; Transparency; Accountability; Ethics	BIG DATA; JUDGMENT	Artificial intelligence (AI) receives attention in media as well as in academe and business. In media coverage and reporting, AI is predominantly described in contrasted terms, either as the ultimate solution to all human problems or the ultimate threat to all human existence. In academe, the focus of computer scientists is on developing systems that function, whereas philosophy scholars theorize about the implications of this functionality for human life. In the interface between technology and philosophy there is, however, one imperative aspect of AI yet to be articulated:how do intelligent systems make inferences?We use the overarching concept "Artificial Intelligent Behaviour" which would include both cognition/processing and judgment/behaviour. We argue that due to the complexity and opacity ofartificial inference,one needs to initiate systematic empirical studies of artificial intelligent behavior similar to what has previously been done to study human cognition, judgment and decision making. This will provide valid knowledge, outside of what current computer science methods can offer, about the judgments and decisions made by intelligent systems. Moreover, outside academe-in the public as well as the private sector-expertise in epistemology, critical thinking and reasoning are crucial to ensure human oversight of the artificial intelligent judgments and decisions that are made, because only competent human insight into AI-inference processes will ensure accountability. Such insights require systematic studies of AI-behaviour founded on the natural sciences and philosophy, as well as the employment of methodologies from the cognitive and behavioral sciences.																	0951-5666	1435-5655				SEP	2020	35	3					519	532		10.1007/s00146-019-00928-5		DEC 2019											
J								Walking Human Detection Using Stereo Camera Based on Feature Classification Algorithm of Second Re-projection Error	FRONTIERS IN NEUROROBOTICS										feature target; classification; dynamic environments; SLAM; re-projection error	DYNAMIC ENVIRONMENTS; SLAM; MODEL	This paper presents a feature classification method based on vision sensor in dynamic environment. Aiming at the detected targets, a double-projection error based on orb and surf is proposed, which combines texture constraints and region constraints to achieve accurate feature classification in four different environments. For dynamic targets with different velocities, the proposed classification framework can effectively reduce the impact of large-area moving targets. The algorithm can classify static and dynamic feature objects and optimize the conversion relationship between frames only through visual sensors. The experimental results show that the proposed algorithm is superior to other algorithms in both static and dynamic environments.																	1662-5218					DEC 18	2019	13								105	10.3389/fnbot.2019.00105													
J								Approaching minimal cognition: introduction to the special issue	ADAPTIVE BEHAVIOR										Minimal cognition; minimal models; cognitive behavior	DYNAMICS; AUTONOMY	This special issue highlights the growing interdisciplinary interest in minimal cognition, bringing together a number of philosophers and scientists interested in investigating where, how, and why cognition arises. In what follows, we introduce the topic of minimal cognition by giving a brief look at debates and discussions about the lower bounds of cognition, minimally cognitive behaviors, and the possibility of life-mind continuity. Afterwards, we offer a short summary of each of the contributions to this issue. In the spirit of the Minimal Cognition conferences at the University of Wollongong at which the contributors participated, we hope this special issue will enrich the current state of minimal cognition research by putting a number of different disciplines and approaches into conversation.																	1059-7123	1741-2633														1059712319891620	10.1177/1059712319891620		DEC 2019											
J								A structured support vector machine for hyperspectral satellite image segmentation and classification based on modified swarm optimization approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hyperspectral image; Structured support vector machine; Otsu's binary threshold method; Modified particle swarm optimization; Image segmentation; Classification		The hyperspectral image analysis improved by the most powerful and fastest growing technologies in the field of remote sensing in recent years. The hyperspectral image classification involves the identification and recognition by capturing spectral information over the region and consequently analysis by the pixel value. The conventional method uses the wiener filter for pre-processing and GLCM approach to extract the second order statistical features with dragonfly optimization technique for image extraction. The machine learning techniques used in the conventional technique is extreme learning machine and relevance vector machine. Here the high-resolution hyperspectral remote sensing datasets are taken from hyperspectral remote sensing scenes. This scene acquired by the AVIRIS sensor during a flight campaign over the Indian pines test site in Northwestern Indian. The hyperspectral images are filtered by a modified swarm optimization approach and these images are extracted by threshold-based segmentation process with the use of OTSU's binary threshold method. The structured support vector machine is proposed for the classification of the satellite image. By the use of the optimization process, the structured support vector machine is improved its performance. Since overall sensitivity, specificity, and accuracy is improved. The simulation part carried out the data set for Indian pines and Salinas's scene and the overall design is done with MATLAB.																	1868-5137	1868-5145															10.1007/s12652-019-01643-1		DEC 2019											
J								Supporting asymmetric interaction in the age of social media	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Asymmetric social interaction; Asymmetry of social media preferences; Ubiquitous social interaction; Ad hoc social applications	OLDER-ADULTS	Supporting asymmetric social interaction is a key requirement for developers of ad hoc social applications, who need to keep up with the pace of the evolution of social media technology without leaving digital orphans on the road. This asymmetry becomes more evident in intergenerational social communication and it has been recognized as an open problem given the lack of interoperability among the social media platforms. The quick evolution and diversification of these platforms help increase this asymmetry. On the one hand, this situation impacts the developers of ad hoc social applications, who have to keep updating their systems. On the other hand, the end-users of these applications are affected as well, given that they depend on the developers to continue interacting with such platforms. This manuscript presents social message translator (SMT), a software system that allows addressing the interaction asymmetry between ad hoc social applications and regular social media platforms. The message translation mechanisms of SMT, its security, and the usability and extensibility of its services were evaluated through three empirical studies. The obtained results are highly positive, showing that SMT goes one step forward in the development of socio-domestic computing systems to support asymmetric social interaction.																	1868-5137	1868-5145															10.1007/s12652-019-01641-3		DEC 2019											
J								Multi-feature fusion and selection method for an improved particle swarm optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Binary particle swarm optimization; Feature selection; Immune operator; Chaos optimization	FEATURE SUBSET-SELECTION; BINARY PSO; ALGORITHM; BPSO	In data mining, feature selection is an important part in data preprocessing, therefore selecting the optimal feature subset can effectively reduce the data dimension and computing cost of the learning algorithm. In this paper, Binary Particle Swarm Optimization is used to optimize the feature selection process. Firstly, a population initialization strategy based on chaos theory is proposed to reduce information redundancy and improve the quality of the initial population. Secondly, a chaos clonal operator, chaos mutation operator, and immune selection operator are proposed to improve the convergence speed and population diversity of the algorithm. Finally, three representative optimization algorithms were selected and compared in eight UCI datasets. The experimental results showed that the improved IBPSOCC algorithm is superior to other contrast algorithms in reducing the number of features and improving the classification accuracy.																	1868-5137	1868-5145															10.1007/s12652-019-01624-4		DEC 2019											
J								A stochastic analysis of the impact of fluctuations in the environment on pre-exposure prophylaxis for HIV infection	SOFT COMPUTING										HIV; AIDS epidemic model; Pre-exposure prophylaxis; Stochastic differential equations; Brownian motion; Extinction; Persistence; Numerical simulations	SIS EPIDEMIC MODEL; MATHEMATICAL-ANALYSIS; COST-EFFECTIVENESS; PREVENTION; HIV/AIDS; AIDS; COINFECTION; THRESHOLD; MEN; SEX	We propose a stochastic model for HIV/AIDS transmission where pre-exposure prophylaxis is considered as a prevention measure for new HIV infections. A white noise is introduced into the model, representing fluctuations in the environment that manifest themselves on the transmission coefficient rate. We prove the existence and uniqueness of a global positive solution of the stochastic model and establish conditions under which extinction and persistence in mean hold. Numerical simulations are provided which illustrate the theoretical results, and conclusions are derived on the impact of the fluctuations in the environment on the number of the susceptible individuals that are under pre-exposure prophylaxis.																	1432-7643	1433-7479															10.1007/s00500-019-04611-1		DEC 2019											
J								Information Theoretic Model to Simulate Agent-Signage Interaction for Wayfinding	COGNITIVE COMPUTATION										Cognitive agent; Information theory; Entropy; Artificial intelligence; Agent-based model; Signage system	EVACUATION BEHAVIOR; EMERGENCY SIGNAGE; ROBOTS; LEGIBILITY; PERCEPTION	Signage systems are critical for communicating spatial information during wayfinding among a plethora of noise in the environment. A proper signage system can improve wayfinding performance and user experience by reducing the perceived complexity of the environment. However, previous models of sign-based wayfinding do not incorporate realistic noise or quantify the reduction in perceived complexity from the use of signage. Drawing upon concepts from information theory, we propose and validate a new agent-signage interaction model that quantifies available wayfinding information from signs for wayfinding. We conducted two online crowd-sourcing experiments to compute the distribution of a sign's visibility and an agent's decision-making confidence as a function of observation angle and viewing distance. We then validated this model using a virtual reality (VR) experiment with trajectories from human participants. The crowd-sourcing experiments provided a distribution of decision-making entropy (conditioned on visibility) that can be applied to any sign/environment. From the VR experiment, a training dataset of 30 trajectories was used to refine our model, and the remaining test dataset of 10 trajectories was compared with agent behavior using dynamic time warping (DTW) distance. The results revealed a reduction of 38.76% in DTW distance between the average trajectories before and after refinement. Our refined agent-signage interaction model provides realistic predictions of human wayfinding behavior using signs. These findings represent a first step towards modeling human wayfinding behavior in complex real environments in a manner that can incorporate several additional random variables (e.g., environment layout).																	1866-9956	1866-9964															10.1007/s12559-019-09689-1		DEC 2019											
J								Incremental document clustering using fuzzy-based optimization strategy	EVOLUTIONARY INTELLIGENCE										Incremental document; Stop word removal; Boundary degree; Stemming; Clustering		The technical advances in the information systems contribute towards the massive availability of the documents stored in the electronic database, such as e-mails, internet and web pages. Thus, it becomes a complex task for arranging and browsing the required document. This paper proposes an incremental document clustering method for performing effective document clustering. The proposed model undergoes three steps for document clustering, namely pre-processing, feature extraction and Incremental document categorization. The pre-processing step is carried out for removing the artifacts and redundant data from the documents by undergoing stop word removal process and stemming process. Then, the next step is the feature extraction based on Term Frequency-Inverse Document Frequency (TF-IDF) and Wordnet features. Here, the feature is selected using support measure named ModSupport, and then, the incremental document clustering is performed based on the hybrid fuzzy bounding degree and Rider-Moth Flame optimization algorithm (RMFO) using the boundary degree. Here, the RMFO aims at the selection of the optimal weights for the boundary degree model and is designed by integrating Rider Optimization Algorithm (ROA) with Moth Flame optimization (MFO). The performance of the proposed RMFO outperformed the existing techniques using accuracy, F-measure, precision, and recall with maximal values 93.98%, 94.876%, 93.958% and 93.964% respectively.																	1864-5909	1864-5917															10.1007/s12065-019-00335-1		DEC 2019											
J								PARASOL: a hybrid approximation approach for scalable frequent itemset mining in streaming data	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Streaming data mining; Online approximation algorithm	CLOSED ITEMSETS; ALGORITHM	Here, we present a novel algorithm for frequent itemset mining in streaming data (FIM-SD). For the past decade, various FIM-SD methods in one-pass approximation settings that allow to approximate the support of each itemset have been proposed. They can be categorized into two approximation types:parameter-constrained(PC) mining andresource-constrained(RC) mining. PC methods control the maximum error that can be included in the approximate support based on a pre-defined parameter. In contrast, RC methods limit the maximum memory consumption based on resource constraints. However, the existing PC methods can exponentially increase the memory consumption, while the existing RC methods can rapidly increase the maximum error. In this study, we address this problem by introducing a hybrid approach of PC-RC approximations, calledPARASOL. For any streaming data, PARASOL ensures to provide a condensed representation, called a Delta-covered set, which is regarded as an extension of the closedness compression; when Delta = 0, the solution corresponds to the ordinary closed itemsets. PARASOL searches for such approximate closed itemsets that can restore the frequent itemsets and their supports while the maximum error is bounded by an integer, Delta. Then, we empirically demonstrate that the proposed algorithm significantly outperforms the state-of-the-art PC and RC methods for FIM-SD.																	0925-9902	1573-7675				AUG	2020	55	1					119	147		10.1007/s10844-019-00590-9		DEC 2019											
J								Unimodal optimization using a genetic-programming-based method with periodic boundary conditions	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Optimization; Evolutionary computation; Genetic programming; Niching methods; Periodic boundary conditions; Parameter mapping approach	NEURAL-NETWORKS; IMPROVEMENT; EVOLUTIONARY	This article describes a new genetic-programming-based optimization method using a multi-gene approach along with a niching strategy and periodic domain constraints. The method is referred to as Niching MG-PMA, where MG refers to multi-gene and PMA to parameter mapping approach. Although it was designed to be a multimodal optimization method, recent tests have revealed its suitability for unimodal optimization. The definition of Niching MG-PMA is provided in a detailed fashion, along with an in-depth explanation of two novelties in our implementation: the feedback of initial parameters and the domain constraints using periodic boundary conditions. These ideas can be potentially useful for other optimization techniques. The method is tested on the basis of the CEC'2015 benchmark functions. Statistical analysis shows that Niching MG-PMA performs similarly to the winners of the competition even without any parametrization towards the benchmark, indicating that the method is robust and applicable to a wide range of problems.																	1389-2576	1573-7632				SEP	2020	21	3			SI		503	523		10.1007/s10710-019-09373-1		DEC 2019											
J								Towards intrinsic autonomy through evolutionary computation	ARTIFICIAL INTELLIGENCE REVIEW										Autonomous agents; Embodied open-ended evolution; Reinforcement learning; Philosophical aspects of evolutionary computing; Enactive artificial intelligence; Artificial life	EMBODIED EVOLUTION; POPULATION; SYSTEMS	This paper presents an embodied open-ended environment driven evolutionary algorithm capable of evolving behaviors of autonomous agents without any explicit description of objectives, evaluation metrics or cooperative dynamics. The main novelty of our technique is obtaining intrinsically motivated autonomy of a virtual robot in continuous activity, by internalizing evolutionary dynamics in order to achieve adaptation of a neural controller, and with no need to frequently restart the agent's initial conditions as in traditional training methods. Our work is grounded on ideas from the enactive artificial intelligence field and the biological concept of enaction, from which it is argued that what makes a living being "intentional" is the ability to autonomously, adaptively rearrange their microstructure to suit some function in order to maintain its own constitution. We bring an alternative understanding of intrinsic motivation than that traditionally approached by intrinsic motivated reinforcement learning, and so we also make a brief discussion of the relationship between both paradigms and the autonomy of an agent. We show the autonomous development of foraging and navigation behaviors of a virtual robot.																	0269-2821	1573-7462				AUG	2020	53	6					4449	4473		10.1007/s10462-019-09798-1		DEC 2019											
J								Towards infield, live plant phenotyping using a reduced-parameter CNN	MACHINE VISION AND APPLICATIONS										Pixel-wise segmentation for plant phenotyping; Lightweight deep convolutional neural networks; Separable convolutions; Singular value decomposition	DENSITY	There is an increase in consumption of agricultural produce as a result of the rapidly growing human population, particularly in developing nations. This has triggered high-quality plant phenotyping research to help with the breeding of high-yielding plants that can adapt to our continuously changing climate. Novel, low-cost, fully automated plant phenotyping systems, capable of infield deployment, are required to help identify quantitative plant phenotypes. The identification of quantitative plant phenotypes is a key challenge which relies heavily on the precise segmentation of plant images. Recently, the plant phenotyping community has started to use very deep convolutional neural networks (CNNs) to help tackle this fundamental problem. However, these very deep CNNs rely on some millions of model parameters and generate very large weight matrices, thus making them difficult to deploy infield on low-cost, resource-limited devices. We explore how to compress existing very deep CNNs for plant image segmentation, thus making them easily deployable infield and on mobile devices. In particular, we focus on applying these models to the pixel-wise segmentation of plants into multiple classes including background, a challenging problem in the plant phenotyping community. We combined two approaches (separable convolutions and SVD) to reduce model parameter numbers and weight matrices of these very deep CNN-based models. Using our combined method (separable convolution and SVD) reduced the weight matrix by up to 95% without affecting pixel-wise accuracy. These methods have been evaluated on two public plant datasets and one non-plant dataset to illustrate generality. We have successfully tested our models on a mobile device.																	0932-8092	1432-1769				DEC 17	2019	31	1							2	10.1007/s00138-019-01051-7													
J								Absolute time encoding for temporal super-resolution using de Bruijn coded exposures	MACHINE VISION AND APPLICATIONS										Computational imaging; Object tracking; Coded exposures; Camera systems; Temporal super-resolution; Trajectory analysis	METEORITE; RECOVERY; MOTION; RADAR	Many target tracking tasks require high spatial and temporal precision. High frame rate imaging at high spatial resolution is commonly used in these applications, but this approach is expensive and generates large amounts of data which can complicate implementation. When tracking a single object in motion, almost all of this information is unused. A technique has been developed to exploit this sparsity and track motion with a long exposure where absolute timing is encoded by modulating the exposure over time according to a de Bruijn sequence. This technique has been implemented in the Desert Fireball Network to track bright meteors entering the Earth's atmosphere for orbit determination and successful meteorite recovery. An alternate proof of concept implementation was also developed demonstrating tracking at 36 megapixels and 1000 Hz using a consumer camera with an inexpensive modulated light source and retroreflective target. The technique could be applied to other tracking problems requiring high temporal and spatial precision such as particle image velocimetry and space surveillance and tracking.																	0932-8092	1432-1769				DEC 17	2019	31	1							1	10.1007/s00138-019-01050-8													
J								Discriminative Region Proposal Adversarial Network for High-Quality Image-to-Image Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image-to-image translation; GAN; Pix2pix; Pix2pixHD; CycleGAN; DRPAN		Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photo-reality. In this work, we present Discriminative Region Proposal Adversarial Network (DRPAN) for high-quality image-to-image translation. We decompose the image-to-image translation procedure into three iterated steps: the first is to generate an image with global structure but some local artifacts (via GAN), the second is to use our Discriminative Region Proposal network (DRPnet) for proposing the most fake region from the generated image, and the third is to implement "image inpainting" on the most fake region for yielding more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. We explore patch-based GAN to construct DRPnet for proposing the discriminative region to produce masked fake samples, further, we propose a reviser for GANs to distinguish real from masked fake for providing constructive revisions to the generator for producing realistic details, and serve as auxiliaries of the generator to synthesize high-quality results. In addition, we combine pix2pixHD with DRPAN to synthesize high-resolution results with much finer details. Moreover, we improve CycleGAN by DRPAN to address unpaired image-to-image translation with better semantic alignment. Experiments on a variety of paired and unpaired image-to-image translation tasks validate that our method outperforms the state of the art for synthesizing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures. Our code is available at .																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2366	2385		10.1007/s11263-019-01273-2		DEC 2019											
J								Muscle Synergy Constraints Do Not Improve Estimates of Muscle Activity From Static Optimization During Gait for Unimpaired Children or Children With Cerebral Palsy	FRONTIERS IN NEUROROBOTICS										electromyography; muscle synergies; musculoskeletal modeling; cerebral palsy; static optimization	TYPICALLY DEVELOPING-CHILDREN; MUSCULOSKELETAL MODELS; DYNAMIC SIMULATIONS; MODULAR CONTROL; HUMAN WALKING; TENDON PROPERTIES; FORCE; PREDICTION; COMPLEXITY; SUPPORT	Neuromusculoskeletal simulation provides a promising platform to inform the design of assistive devices or inform rehabilitation. For these applications, a simulation must be able to accurately represent the person of interest, such as an individual with a neurologic injury. If a simulation fails to predict how an individual recruits and coordinates their muscles during movement, it will have limited utility for informing design or rehabilitation. While inverse dynamic simulations have previously been used to evaluate anticipated responses from interventions, like orthopedic surgery or orthoses, they frequently struggle to accurately estimate muscle activations, even for tasks like walking. The simulated muscle activity often fails to represent experimentally measured muscle activity from electromyographic (EMG) recordings. Research has theorized that the nervous system may simplify the range of possible activations used during dynamic tasks, by constraining activations to weighted groups of muscles, referred to as muscle synergies. Synergies are altered after neurological injury, such as stroke or cerebral palsy (CP), and may provide a method for improving subject-specific models of neuromuscular control. The aim of this study was to test whether constraining simulation to synergies could improve estimated muscle activations compared to EMG data. We evaluated modeled muscle activations during gait for six typically developing (TD) children and six children with CP. Muscle activations were estimated with: (1) static optimization (SO), minimizing muscle activations squared, and (2) synergy SO (SynSO), minimizing synergy activations squared using the weights identified from EMG data for two to five synergies. While SynSO caused changes in estimated activations compared to SO, the correlation to EMG data was not higher in SynSO than SO for either TD or CP groups. The correlations to EMG were higher in CP than TD for both SO (CP: 0.48, TD: 0.36) and SynSO (CP: 0.46, TD: 0.26 for five synergies). Constraining activations to SynSO caused the simulated muscle stress to increase compared to SO for all individuals, causing a 157% increase with two synergies. These results suggest that constraining simulated activations in inverse dynamic simulations to subject-specific synergies alone may not improve estimation of muscle activations during gait for generic musculoskeletal models.																	1662-5218					DEC 17	2019	13								102	10.3389/fnbot.2019.00102													
J								Prediction of pre-term groups from EHG signals using optimal multi-kernel SVM	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electrohysterogram signals; Multi-kernel support vector machine; Opposition based ant lion optimization; Wiener filter	ALGORITHM; DELIVERY	Pre-term birth is the birth that carries out before the baby's expected date. Sometimes, it augments the possibility of health problems or death. In this research, the abdominal EHG signal incorporates both the maternal heart beat signal and Fetal ECG signal. The removal of fetal ECG signal from the heart beat signal is difficult in pre-term detection. In this work, the EHG signal is pre-processed using wiener filter that is applied to enhance the signal quality. Then, the attributes are removed from the pre-processed signal to find the distinctive class. In addition, Opposition based Ant Lion Optimization is used to select the multi-kernel Support Vector Machine and training algorithms for predicting the pre-term birth. The proposed methodology is simulated by using MATLAB software and the results are investigated to verify the classification accuracy. From the experimental study, the proposed work enhanced the classification accuracy upto 3-19% related to the existing works.																	1868-5137	1868-5145															10.1007/s12652-019-01648-w		DEC 2019											
J								Machine learning for quality control system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Quality control; Incremental learning; Image classification; Defect detection system	FABRIC DEFECT DETECTION	In this work, we propose and develop a classification model to be used in a quality control system for clothing manufacturing using machine learning algorithms. The system consists of using pictures taken through mobile devices to detect defects on production objects. In this work, a defect can be a missing component or a wrong component in a production object. Therefore, the function of the system is to classify the components that compose a production object through the use of a classification model. As a manufacturing business progresses, new objects are created, thus, the classification model must be able to learn the new classes without losing previous knowledge. However, most classification algorithms do not support an increase of classes, these need to be trained from scratch with all . Thus. In this work, we make use of an incremental learning algorithm to tackle this problem. This algorithm classifies features extracted from pictures of the production objects using a convolutional neural network (CNN), which have proven to be very successful in image classification problems. We apply the current developed approach to a process in clothing manufacturing. Therefore, the production objects correspond to clothing items																	1868-5137	1868-5145															10.1007/s12652-019-01640-4		DEC 2019											
J								An anomaly prediction framework for financial IT systems using hybrid machine learning methods	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Anomaly prediction; Time series prediction; Hierarchical classifier; Imbalanced learning		In financial field, a robust IT system is of vital importance to ensure the smooth operation of financial transactions. However, many financial corporations still depend on operators to identify and eliminate the system failures when financial IT systems break down. This traditional operation method is time consuming and extremely inefficient. To improve the efficiency and accuracy of system failure detection and thereby reduce the impact of system failures on financial services, we propose a novel machine learning-based framework to predict the occurrence of system exceptions and failures in a financial IT system. In particular, we first extract rich information from system logs and eliminate noises in the data. Then the cleaned data is leveraged as the input of our proposed anomaly prediction framework which consists of three modules: key performance indicator data prediction module, anomaly identification module and severity classification module. Notably, we design a hierarchical architecture of alarm classifiers and try to alleviate the influence of class-imbalance problem on the overall performance. Empirically, the experimental results demonstrate the superior performance of our proposed method on a real-world financial IT system log data set.																	1868-5137	1868-5145															10.1007/s12652-019-01645-z		DEC 2019											
J								A distributed computation of the shortest path in large-scale road network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										The shortest path; MapReduce model; Parallel computing; Road network	ENCRYPTION; MAPREDUCE	The shortest path solution in large-scale road network has become a hot topic. How to find a shortest path efficiently and quickly in a road network is very difficult. With the development of parallel computing and the improvement of Hadoop ecology, it makes it easier to solve the problem by using the MapReduce model in the Hadoop ecosystem. In this paper, we propose a improved shortest path algorithm based on cloud computing in large data architecture. The principle of this algorithm is to divide the whole large-scale road network into small-scale road network according to a certain distance, calculate the shortest path of small-scale road network, and finally combine these shortest paths into the shortest path of large-scale road network. The experimental results show that the algorithm has good results, especially with the increase of computing nodes, and the efficiency of calculation is higher.																	1868-5137	1868-5145															10.1007/s12652-019-01615-5		DEC 2019											
J								Machine learning of symbolic compositional rules with genetic programming: dissonance treatment in Palestrina	PEERJ COMPUTER SCIENCE										Counterpoint; Rule learning; Palestrina; Genetic programming; Clustering; Algorithmic composition; Dissonance detection; Computer music	MUSIC	We describe a method for automatically extracting symbolic compositional rules from music corpora. Resulting rules are expressed by a combination of logic and numeric relations, and they can therefore be studied by humans. These rules can also be used for algorithmic composition, where they can be combined with each other and with manually programmed rules. We chose genetic programming (GP) as our machine learning technique, because it is capable of learning formulas consisting of both logic and numeric relations. GP was never used for this purpose to our knowledge. We therefore investigate a well understood case in this study: dissonance treatment in Palestrina's music. We label dissonances with a custom algorithm, automatically cluster melodic fragments with labelled dissonances into different dissonance categories (passing tone, suspension etc.) with the DBSCAN algorithm, and then learn rules describing the dissonance treatment of each category with GP. Learning is based on the requirement that rules must be broad enough to cover positive examples, but narrow enough to exclude negative examples. Dissonances from a given category are used as positive examples, while dissonances from other categories, melodic fragments without dissonances, purely random melodic fragments, and slight random transformations of positive examples, are used as negative examples.																	2376-5992					DEC 16	2019									e244	10.7717/peerj-cs.244													
J								Role clarity deficiencies can wreck agile teams	PEERJ COMPUTER SCIENCE										Agile; Scrum; conflicts; coach; Scrum Master; Social skills; Technical skills; Reflection; Team dynamics; Self-organization	SOFTWARE-DEVELOPMENT	Background. One of the twelve agile principles is to build projects around motivated individuals and trust them to get the job done. Such agile teams must self-organize, but this involves conflict, making self-organization difficult. One area of difficulty is agreeing on everybody's role. Background. What dynamics arise in a self-organizing team from the negotiation of everybody's role? Method. We conceptualize observations from five agile teams (work observations, interviews) by Charmazian Grounded Theory Methodology. Results. We define role as something transient and implicit, not fixed and named. The roles are characterized by the responsibilities and expectations of each team member. Every team member must understand and accept their own roles (Local role clarity) and everbody else's roles (Team-wide role clarity). Role clarity allows a team to work smoothly and effectively and to develop its members' skills fast. Lack of role clarity creates friction that not only hampers the day-to-day work, but also appears to lead to high employee turnover. Agile coaches are critical to create and maintain role clarity. Conclusions. Agile teams should pay dose attention to the levels of Local role clarity of each member and Team-wide role clarity overall, because role clarity deficits are highly detrimental.																	2376-5992					DEC 16	2019									e241	10.7717/peerj-cs.241													
J								Studying the impact of CI on pull request delivery time in open source projects-a conceptual replication	PEERJ COMPUTER SCIENCE										Continuous integration; Mining software repositories; Replication; Pull-request based development		Nowadays, continuous integration (CI) is indispensable in the software development process. A central promise of adopting CI is that new features or bug fixes can be delivered more quickly. A recent repository mining study by Bernardo, da Costa & Kulesza (2018) found that only about half of the investigated open source projects actually deliver pull requests (PR) faster after adopting CI, with small effect sizes. However, there are some concerns regarding the methodology used by Bernardo et al., which may potentially limit the trustworthiness of this finding. Particularly, they do not explicitly control for normal changes in the pull request delivery time during a project's lifetime (independently of CI introduction). Hence, in our work, we conduct a conceptual replication of this study. In a first step, we replicate their study results using the same subjects and methodology. In a second step, we address the same core research question using an adapted methodology. We use a different statistical method (regression discontinuity design, RDD) that is more robust towards the confounding factor of projects potentially getting faster in delivering PRs over time naturally, and we introduce a control group of comparable projects that never applied CI. Finally, we also evaluate the generalizability of the original findings on a set of new open source projects sampled using the same methodology. We find that the results of the study by Bernardo et al. largely hold in our replication. Using RDD, we do not find robust evidence of projects getting faster at delivering PRs without CI, and we similarly do not see a speed-up in our control group that never introduced CI. Further, results obtained from a newly mined set of projects are comparable to the original findings. In conclusion, we consider the replication successful.																	2376-5992					DEC 16	2019									e245	10.7717/peerj-cs.245													
J								Airfoil Optimization of Land-Yacht Robot Based on Hybrid PSO and GA	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										PSO; GA; airfoil optimization; aerodynamic; land-yacht robot	PARTICLE SWARM OPTIMIZATION; ALGORITHM; CONVERGENCE	Airfoil optimization algorithm is studied and a hybrid PSO and GA method is proposed in this paper. After function test, it shows that algorithm is well in convergence performance, fast speed, and optimization capability. Then, the airfoil parametric expression theory is analyzed. A new airfoil is obtained after combining CFD and PSO-GA optimization. The aerodynamic of new airfoil is compared with the airfoil optimized by GA-PSO and basic airfoil NACA0018. The results indicate that new airfoil is better than the other two airfoils in lift coefficient, lift-drag ratio, and surface pressure. At last, wing-sail of new airfoil and NACA0018 wing-sail are designed and manufactured. Both of them are applied in land-yacht robot linear motion and steering motion experiment. For the linear motion, in the situation of wind speed being 15 m/s and angle of attack being 5, running speed of robot with optimized new wing-sail is 1.853 m/s. In steering motion, trajectory with new wing-sail is closer to the real situation and it gets more thrust. The experiments data verify that the simulation results are correct and PSO-GA algorithm is effective.																	0218-0014	1793-6381				DEC 15	2019	33	13							1959041	10.1142/S0218001419590419													
J								Unsupervised Learning of Multi-Sense Embedding with Matrix Factorization and Sparse Soft Clustering	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Language model; multi-sense embedding; sparse soft clustering; matrix factorization		In the natural language environment, accurately inferring the meaning of a token according to its context is crucial to understanding a sophisticated expression. However, this is not easy for a machine. The traditional language models used to train distributed word vectors are often restricted by single-sense embedding. In this paper, we develop a model called MSCvec (Multi-sense Soft Clustering Vector) for word sense disambiguation of polysemy in context. We extract the features of individual words by the co-occurrence PPMI (Positive Pointwise Mutual Information) matrix, and decompose the matrix by NMF (Nonnegative Matrix Factorization) into low-rank representations of target words, which are used as the input of an unsupervised sparse soft clustering method called Sparse Fuzzy C-means (SFCM). We use SFCM to determine the global semantic space of words, and partition the subspaces of multiple senses of a polysemous word. We relabel candidate words by the negative average log likelihood, and train multi-sense embedding with extensional vocabulary by the fastText model. Compared with the traditional static embeddings, the result shows that NMF and SFCM design can improve the performance in word similarity and relatedness tasks as well as in text classification tasks of different types of text. Accurate semantic representation of MSCvec would be necessary to produce outstanding results.																	0218-0014	1793-6381				DEC 15	2019	33	13							1951011	10.1142/S021800141951011X													
J								A Novel Multi-Pattern Solder Joint Simultaneous Segmentation Algorithm for PCB Selective Packaging Systems	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										PCB; solder joint segmentation; neighborhood feature; selective packaging system	RECOGNITION; CLASSIFICATION; INSPECTION	To prevent any negative electromagnetic influence of high-density integrated circuits, an insulation package needs to be specially designed to shield it. Aiming at the low efficiency and material waste in traditional packaging methods, a printed circuit board (PCB) selective packaging system based on a multi-pattern solder joint simultaneous segmentation algorithm and three-dimensional printing technology is introduced in this paper. Firstly, the structure of PCB selective packaging system is designed. Secondly, to solve the existing problems, such as multi-pattern solder joints which are located densely in small welding areas and are hard to be extracted in the small-area integrated circuit board, a multi-pattern solder joint simultaneous segmentation algorithm is developed based on (geometrical) neighborhood features to extract and locate the optimal PCB solder joint areas. Finally, tests using three actual PCB are carried out to compare the proposed method with traditional multi-threshold solder joint extraction methods. Test results indicate that the proposed algorithm is simple and effective. Diverse solder joints can be optimally located and simultaneously extracted from the collected PCB image, which greatly improves the filling rate of the solder joint areas and filters out false pixels. Thus, this method provides a reliable location-finding tool to help place solder points in PCB selective packaging systems.																	0218-0014	1793-6381				DEC 15	2019	33	13							2058005	10.1142/S0218001420580057													
J								Single Sample Face Recognition in the Last Decade: A Survey	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Virtual; generic; hybrid; database; performance; taxonomy	TRAINING IMAGE; SPARSE REPRESENTATION; DISCRIMINANT-ANALYSIS; ROBUST; ILLUMINATION; EIGENFACES; FEATURES; FUSION; ARRAY; FLDA	Single sample face recognition (SSFR) is a challenging research problem in which only one face image per person is available for training. Moreover, the face image may have different pose, expression, illumination, occlusion etc. rendering this problem more complex. Several methods have been suggested by various researchers in literature to solve SSFR. Here, we provide a comprehensive review of the methods proposed in the last decade for solving SSFR problem and introduce a novel taxonomy for the same. We divide SSFR methods broadly into five categories viz. (i) feature based, (ii) virtual sample generation based, (iii) generic database based, (iv) Hybrid and (v) other methods. We have also briefly reviewed the face databases used for evaluating single sample face recognition methods. Furthermore, the performance of the methods has been analyzed in terms of classification accuracy as given in literature. At last, we also suggest some future direction to the researchers and practitioners working in this fascinating research area.																	0218-0014	1793-6381				DEC 15	2019	33	13							1956009	10.1142/S0218001419560093													
J								Implementation of a Fuzzy Logic Control Strategy on a Harvester's Controller Based on MATLAB Environment	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Fuzzy logic control; algorithm migrate; STM32 core-chip controller	SENSOR	Due to the nonlinear process of grain harvesting, there is no precise mathematical model to describe the behavior of the cleaning system of a harvester. Both the classical control and modern control methods cannot fulfil the requirements. Owing to this, the intelligent control algorithm was proposed, and the fuzzy logic control (FLC) method is a type of this. At present, most FLC algorithms are proposed in a MATLAB environment. However, the control problems in reality are controlled by microcomputer controllers with different chips. The control language of the microcomputer controller is usually written in C language. It is impossible to directly migrate the algorithm between these two different languages. Therefore, it is an important issue to transplant the FLC algorithm procedure written by MATLAB to the microcomputer controller. To realize the above target, we have built a complete set of control systems for our harvester's cleaning system based on an upper computer and an STM32 core-chip controller. By means of combining FLC theory and expert knowledge, we adopted an improved FLC algorithm for the cleaning system, which is mounted in our self-designed combine harvester. Through this scheme, we have realized the objective of migrating the FLC algorithm from a MATLAB environment to the controller. The results of the experiment show that our method is reliable.																	0218-0014	1793-6381				DEC 15	2019	33	13							1959043	10.1142/S0218001419590432													
J								A Lockable Abnormal Electromagnetic Signal Joint Detection Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Electromagnetic signal; machine learning; anomaly detection; signal localization	FIXED-POINT ALGORITHMS	With the development of computers and network technologies, network security has gradually become a global problem. Network security defenses need to be carried out not only on the Internet, but also on other communication media, such as electromagnetic signals. Existing electromagnetic signal communication is easily intercepted or infiltrated. In order to effectively detect the abnormal electromagnetic signal to find out the specific location, then classify it, it is necessary to study the way of communication. The existing electromagnetic signal detection accuracy is low and cannot be located. Considering the characteristics of different power sources in different locations, combined with spark streaming technology and machine learning classification technology, a joint platform for electromagnetic signal anomaly detection based on big data analysis is proposed. The electromagnetic signal is abnormally detected by feature comparison and small signal analysis, and the position and number between the signal sources are determined by three-point positioning and signal attenuation. The experimental results show that the method can detect abnormal electromagnetic signals and classify abnormal electromagnetic signals well, the accuracy rate can reach 95%, and the positioning accuracy can reach 89%.																	0218-0014	1793-6381				DEC 15	2019	33	13							1958009	10.1142/S0218001419580096													
J								Spatial and Temporal Feature-Based Reduced Reference Quality Assessment for Rate-Varying Videos in Wireless Networks	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Video quality assessment; temporal and spatial feature; reduced reference; wireless network; rate variation	ORIENTATION SELECTIVITY; VISUAL-CORTEX; IMAGE; PREDICTION; NEURONS; MODEL	For the impact of the bitrate change of video streaming services according to the available bandwidth on user satisfaction, in this paper, we propose a spatial and temporal feature-based reduced reference (RR) quality assessment for rate-varying videos in wireless networks called STRQAW. First, simulating the orientation selectivity mechanism of the human visual system (HVS), the histogram of the orientation selectivity-based visual pattern in each frame is extracted as the spatial feature. The histogram similarity between the rate-varying video and the original video is computed as the spatial metric. Second, we extract the temporal variation of the DCT coefficients of the consecutive frame differences as the temporal feature. The temporal variation similarity between the rate-varying video and the original video is calculated as the temporal metric. Finally, we take into account the recency effect and assess the overall quality by combining the temporal and spatial metric. The experimental results using the Laboratory for Image and Video Engineering (LIVE) mobile video quality assessment (VQA) database show that STRQAW is consistent with the subjective assessment results, which means it reflects human subjective feelings well and it provides an evaluation for adjusting compression-coding rates in real time. STRQAW can be used to guide video application providers and network operators working towards satisfying end-user experiences.																	0218-0014	1793-6381				DEC 15	2019	33	13							1950021	10.1142/S0218001419500216													
J								Resource Allocation Scheme Based on Rate-Requirement for Device-to-Device Downlink Communications	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Resource allocation; context-aware; rate-requirement; device-to-device; residual RBs	NETWORKS; 5G	The rate-requirement of device-to-device (D2D) users is associated with the context information of velocity and data size of users to some extent. In this study, an efficient context-aware resource allocation scheme based on rate requirement (RARR) is proposed. This scheme consists of two allocation phases. In the rate-ensuring resource allocation phase, D2D pairs are allocated a certain amount of spectrum resource according to their rate requirement. In the allocation, the interference restricted area is limited to exclude cellular users that bring a negative capacity gain to the communication system. In the residual resource reallocation phase, surplus resources are assigned to D2D pairs according to the system fairness. Simulation results indicate that the proposed RARR scheme efficiently leads to superior performance in terms of system throughput and fairness and exhibits low complexity relative to traditional resource allocation.																	0218-0014	1793-6381				DEC 15	2019	33	13							1959040	10.1142/S0218001419590407													
J								Walking an Unknown Street with Limited Sensing	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Online algorithms; mobile robot; limited sensing; competitive analysis; street	ONLINE ALGORITHMS	This paper studies a searching problem in an unknown street. A simple polygon P with two distinguished vertices, s and t, is called a street if the two boundary chains from s to t are mutually weakly visible. We use a mobile robot to locate t starting from s. Assume that the robot has a limited sensing capability that can only detect the constructed edges (also called gaps) on the boundary of its visible region, but cannot measure any angle or distance. The robot does not have knowledge of the street in advance. We present a new competitive strategy for this problem and prove that the length of the path generated by the robot is at most 9-times longer than the shortest path. We also propose a matching lower bound to show that our strategy is optimal. Compared with the previous strategy, we further relaxed the restriction that the robot should take a marking device and use the data structure S-GNT. The analysis of our strategy is tight.																	0218-0014	1793-6381				DEC 15	2019	33	13							1959042	10.1142/S0218001419590420													
J								Explore Deep Neural Network and Reinforcement Learning to Large-scale Tasks Processing in Big Data	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Big data; cloud computing; deep network model; large-scale tasks processing; reinforcement learning	ALLOCATION; MINIMIZATION	Large-scale tasks processing based on cloud computing has become crucial to big data analysis and disposal in recent years. Most previous work, generally, utilize the conventional methods and architectures for general scale tasks to achieve tons of tasks disposing, which is limited by the issues of computing capability, data transmission, etc. Based on this argument, a fat-tree structure-based approach called LTDR (Large-scale Tasks processing using Deep network model and Reinforcement learning) has been proposed in this work. Aiming at exploring the optimal task allocation scheme, a virtual network mapping algorithm based on deep convolutional neural network and Q-learning is presented herein. After feature extraction, we design and implement a policy network to make node mapping decisions. The link mapping scheme can be attained by the designed distributed value-function based reinforcement learning model. Eventually, tasks are allocated onto proper physical nodes and processed efficiently. Experimental results show that LTDR can significantly improve the utilization of physical resources and long-term revenue while satisfying task requirements in big data.																	0218-0014	1793-6381				DEC 15	2019	33	13							1951010	10.1142/S0218001419510108													
J								An Energy Dynamic Control Algorithm Based on Reinforcement Learning for Data Centers	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Reinforcement learning; double Q-learning; dynamic energy control; energy cost reduction	GENERATION; MANAGEMENT; POWER; COST	In recent years, how to use renewable energy to reduce the energy cost of internet data center (IDC) has been an urgent problem to be solved. More and more solutions are beginning to consider machine learning, but many of the existing methods need to take advantage of some future information, which is difficult to obtain in the actual operation process. In this paper, we focus on reducing the energy cost of IDC by controlling the energy flow of renewable energy without any future information. we propose an efficient energy dynamic control algorithm based on the theory of reinforcement learning, which approximates the optimal solution by learning the feedback of historical control decisions. For the purpose of avoiding overestimation, improving the convergence ability of the algorithm, we use the double Q-method to further optimize. The extensive experimental results show that our algorithm can on average save the energy cost by 18.3% and reduce the rate of grid intervention by 26.2% compared with other algorithms, and thus has good application prospects.																	0218-0014	1793-6381				DEC 15	2019	33	13							1951009	10.1142/S0218001419510091													
J								Robust Instruments Position Estimation using Improved Kernelized Correlation Filter for Substation Patrol Robots	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Substation patrol robots; image processing; instrument position estimation; improved KCF algorithm		Substation patrol robots (SIR) play an increasingly important role in ensuring the safe operation of substations. The robust and precise position estimating of the instruments to be inspected on the images are a prerequisite for accurately detecting the target states or obtaining the target readings under all-weather environment. In order to achieve high location accuracy of instrument, this study proposed an improved kernelized correlation filter (KCF) algorithm for achieving robust instrument location on images for SPR. Firstly, multiple templates are selected for training KCF classifier parameters. Then, reliable SURF matching-point determination method is designed, and the regions including reliable matching points are selected as the candidate regions, so that the searching range is narrowed. Finally, for KCF response surface of each candidate region, Single-Peak Constraint (SPC) is designed for locating target and reducing mismatching rate. Furthermore, experiments are performed for validating the effectiveness of the proposed algorithm, in which four instruments mainly including lightning arrester monitor and transformer thermometer are selected. The experimental results show that the proposed method has higher accuracy of target localization than traditional SURF-based position estimating method.																	0218-0014	1793-6381				DEC 15	2019	33	13							1954033	10.1142/S0218001419540338													
J								Data Reconstructing Algorithm in Unreliable Links Based on Matrix Completion for Heterogeneous Wireless Sensor Networks	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Heterogeneous wireless sensor networks; data collection; data reconstruction; matrix completion	EQUATIONS	In heterogeneous wireless sensor networks, the data collection method based on compressed sensing technology is susceptible to packet loss and noise, which leads to a decrease in data reconstruction accuracy in unreliable links. Combining compressed sensing and matrix completion, we propose a clustering optimization algorithm based on structured noise matrix completion, in which the cluster head transmits the compressed sampling data and compression strategy to the base station. The algorithm we proposed can reduce the energy consumption of the node in the process of data collection, redundant data and transmission delay. The rank-1 matrix completion algorithm constructs an extremely sparse observation matrix, which is adopted by the sink node to complete the reconstruction of the whole network data. Simulation experiments show that the proposed algorithm reduces network transmission data, balances node energy consumption, improves data transmission efficiency and reconstruction accuracy, and extends the network life cycle.																	0218-0014	1793-6381				DEC 15	2019	33	13							1951012	10.1142/S0218001419510121													
J								Detection of abnormalities in mammograms using deep features	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Convolutional neural network; Breast tissue classification; Computer-aided diagnosis; Mammogram	COMPUTER-AIDED DIAGNOSIS; AUTOMATED DETECTION; NEURAL-NETWORKS; BREAST; CLASSIFICATION; MASSES; SYSTEM	The mortality rate of breast cancer can be reduced by early diagnosis and treatment. The computer-aided diagnosis (CAD) systems can effectively help physicians identify the early stage of breast cancer. The primary tool for such systems is mammograms. Usually, these images lack high quality. Furthermore, due to the irregular shape of masses, their size variability, and the apparent similarity of the masses and other dense regions of the breast tissue, it is difficult to detect and diagnose the masses. Although many image processing techniques have been presented for diagnosis of breast masses, they have not been quite successful, and this problem has been retained as a challenge yet. In this paper, a method for classifying breast tissues into normal and abnormal (i.e., cancerous) is proposed, which is based on a deep learning approach. It mainly contains a new convolutional neural network (CNN) and a decision mechanism. After a preprocessing phase, a block around each pixel is fed into a trained CNN to determine whether the pixel belongs to normal or abnormal tissues. This results in a binary map for the input suspicious tissue. Afterward, as a decision mechanism, a thresholding technique is applied to a central block on the produced binary map to label it. The new architecture of the CNN, the training scheme of the network, employing the CNN for classifying the pixels of the suspicious regions rather than the entire input, exerting an effective decision mechanism on the output of the CNN and the learning of the model parameters, helped us achieve superior results compared to state-of-the-art methods by reaching 95 and 94.68 percent for AUC and accuracy, respectively.																	1868-5137	1868-5145															10.1007/s12652-019-01639-x		DEC 2019											
J								Prediction of engineering properties of fly ash-based geopolymer using artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Artificial neural network (ANN); Prediction; Geopolymer; Strength	CONCRETE COMPRESSIVE STRENGTH; MICROSTRUCTURE	Fly ash-based geopolymer has been studied extensively in recent years due to its comparable properties to Portland cement and its environmental benefits. However, the uncertainty and complexity of design parameters, such as the SiO2/Na2O mole ratio in alkaline solution, the alkaline solution concentration in liquid phase, and the liquid-to-fly ash mass ratio (L/F), have made it very difficult to create a systematic approach for geopolymer mix design. These mix design parameters, along with fly ash properties and curing conditions (temperature and time), significantly influence key properties of the material, such as setting time and compressive strength. In this study, an artificial neural network (ANN) was used to develop models for predicting the key properties of high-calcium fly ash-based geopolymer according to its mix design parameters. The correlations between experimental measurements and ANN model predictions of setting time, compressive strength, and heat of geopolymerization were established based on the results of tests on 36, 273, and 72 geopolymer mixes, respectively. The results show that the correlations between the experimental measurements and ANN model predictions of the properties studied are all strong. ANN modeling was found to be a suitable computing method to analyze the effects of design parameters on geopolymer properties and showed thatL/Fexhibited the greatest effect on setting time, alkaline solution concentration had the greatest influence on compressive strength, and a mole ratio larger than 1.5 significantly impacted heat at the geopolymerization peak. The developed ANN models can be used as guidance for mix design of high-calcium fly ash geopolymer in engineering applications.																	0941-0643	1433-3058															10.1007/s00521-019-04662-3		DEC 2019											
J								Flexible Coordination of Flexible Limbs: Decentralized Control Scheme for Inter- and Intra-Limb Coordination in Brittle Stars' Locomotion	FRONTIERS IN NEUROROBOTICS										brittle star; decentralized control; inter-limb coordination; intra-limb coordination; resilience	ROBOTS; MORPHOLOGY; OPHIUROIDEA; SYSTEM	Conventional mobile robots have difficulties adapting to unpredictable environments or performing adequately after undergoing physical damages in realtime operation, unlike animals. We address this issue by focusing on brittle stars, an echinoderm related to starfish. Most brittle stars have five flexible arms, and they can coordinate among the arms (i.e., inter-arm coordination) as well as the many bodily degrees of freedom within each arm (i.e., intra-arm coordination). They can move in unpredictable environments while promptly adapting to those, and to their own physical damages (e.g., arm amputation). Our previous work focused on the inter-arm coordination by studying trimmed-arm brittle stars. Herein, we extend our previous work and propose a decentralized control mechanism that enables coupling between the inter-arm and intra-arm coordination. We demonstrate via simulations and real-world experiments with a brittle star-like robot that the behavior of brittle stars when they are intact and undergoing shortening or amputation of arms can be replicated.																	1662-5218					DEC 13	2019	13								104	10.3389/fnbot.2019.00104													
J								Oppositional based Laplacian grey wolf optimization algorithm with SVM for data mining in intrusion detection system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intrusion detection system; Grey wolf optimization; Support vector machine	FEATURE-SELECTION	Identifying intruders using data mining approach in recent trend provides better detection rate when compared with other classical systems. In this paper we introduced Oppositional based Laplacian grey wolf optimization algorithm for clustering the class of attacks based on the similarity and active learning of SVM classification using this optimization algorithm. The results of the proposed algorithm have been evaluated with standard metrics and compared with the recent algorithms to prove its significance. The results of the proposed algorithm show its significance when compared with the existing methodologies.																	1868-5137	1868-5145															10.1007/s12652-019-01606-6		DEC 2019											
J								Fuzzy obstacle avoidance optimization of soccer robot based on an improved genetic algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Soccer robot; Fuzzy obstacle avoidance; Fuzzy reasoning; Genetic algorithm		Due to its high-tech confrontation and entertainment, soccer robots have attracted a large number of researchers to participate in it. Robot obstacle avoidance is an active research branch in the field of intelligent robot technology, and is a comprehensive subject covering multiple disciplines. The traditional obstacle avoidance method of soccer robots has the disadvantages of poor local optimization ability and slow convergence speed, and it is easy to fall into local extreme points. Based on the active obstacle avoidance strategy of local sensors and processors, this paper studies the method of automatic extraction and optimization of fuzzy rules of fuzzy path planner by genetic algorithm. This fuzzy control method does not depend on accurate environmental information, and has a small amount of computation and learning ability. A fitness function for evaluating the validity of a rule is proposed. The simulation results show that the algorithm has good applicability.																	1868-5137	1868-5145															10.1007/s12652-019-01636-0		DEC 2019											
J								Path planning and control of soccer robot based on genetic algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Soccer robot; Path planning; Adaptive genetic algorithm; Obstacle avoidance		Studying the optimization of the soccer robot path planning is significant because of the ever-changing situation of the arena. The system itself has non-linearity. The environment also has the characteristics of time varying, which requires the robot to cooperate with each other. Combining with the characteristics of a soccer robot system, this paper proposes a genetic algorithm for the path planning algorithm of a soccer robot, which enables the robot to find a relatively short path from the point to the target. In this paper, three common path-planning methods are introduced, and the advantages and disadvantages are compared. Then, a path planning method based onS-adaptive genetic algorithm is proposed. The main innovation point is to change the crossover probability and mutation probability in genetic operation. Therefore, that it can change the probability value autonomously with the change of genetic generation and individual in the group. This method can better save the effective individual, make the algorithm converges faster, and get the optimal individual under the same conditions. The path solved byS-adaptive algorithm realizes obstacle avoidance behavior in a short time, and the path is better than the traditional genetic algorithm.																	1868-5137	1868-5145															10.1007/s12652-019-01635-1		DEC 2019											
J								Robust and lossless data privacy preservation: optimal key based data sanitization	EVOLUTIONARY INTELLIGENCE										Privacy preservation; Ant colony algorithm; Whale optimization algorithm; Data sanitization; Data restoration	ANONYMIZATION	Privacy-preserving data mining (PPDM) is the most significant approach on data security, in which more research work is under progress. This paper intends to propose a new PPDM model that includes two phases: data sanitization and restoration. Initially, association rules get extracted for proceeding the mentioned phases. The first and foremost tactic on the proposed privacy preservation model is the generation of the optimal key that is used to produce the sanitized data from the original data. The same key takes complete responsibility for processing the data restoration process by the receiver. As the key extraction plays a major role, this paper intends to propose a new hybrid algorithm; Trial based update on whale and particle swarm algorithm (TU-WPA) for selecting the optimal key. The proposed method is the combination of particle swarm optimization and whale optimization algorithm. More importantly, the research issues such as hiding failure rate, information preservation rate, false rule generation and degree of modification are minimized through the proposed sanitization and restoration processes. Finally, the performance of the proposed TU-WPA model is verified over other conventional models.																	1864-5909	1864-5917															10.1007/s12065-019-00309-3		DEC 2019											
J								Measuring the curse of population size over swarm intelligence based algorithms	EVOLVING SYSTEMS										Nature-inspired optimization; Metaheuristic; Swarm intelligence; Population size	EVOLUTIONARY ALGORITHMS; OPTIMIZATION; EFFICIENCY	Swarm-intelligence based algorithms attract researchers as they are simple and effective tools for solving highly non-linear and multi-modal real-world problems. Practical studies prove the curse of population size and its influence over swarm based algorithms. However, the specific measurements on the curse of population size and its impact are not so extensive. Literature reports varying the population size is also one kind of adaption mechanism to incorporate a well-balanced exploration and exploitation into any swarm-based algorithm. Therefore, the main focus of this study is measuring the impact of different constant population size over swarm based algorithms over different classes of functions and dimensions. Study has also attempted to find the correlation between population size, problem type, and dimension. Three well-known swarm-based algorithms, namely particle swarm optimization (PSO), Cuckoo search (CS), and Bat algorithm (BA) have been considered for the experiment. Experimental results clearly show that small population size is well-suited for CS and PSO, whereas, large population size is best for BA over any dimension and any classes of functions. There a nearly linear relationship between population size and BA's optimization efficacy. Opposite characteristics can be seen for the CS algorithm. There is an unpredictable influence of population size over the PSO algorithm's performance. There is an impact of problem type over population size selection for PSO. For unimodal, multimodal, and hybrid functions, population size 10 and 20 give the best outcomes. But for composite functions, population size between 360 and 1280 provides competitive results compared to population size 10 over dimension 10 and 30.																	1868-6478	1868-6486															10.1007/s12530-019-09318-0		DEC 2019											
J								A hybrid group search optimization: firefly algorithm-based big data framework for ancient script recognition	SOFT COMPUTING										Character recognition system; Optical character recognition (OCR); Segmentation and neural network (NN); Group search algorithm; Firefly algorithm	SYSTEM	Optical character recognition is becoming one of the widely researched areas in recent times. This research paper presents an optimization framework for ancient script recognition using the process of script or character segmentation. The proposed algorithm is based on evolutionary algorithm and capable of handing a continuous script of high-resolution data using concepts of big data. A hybrid combination of group search and firefly algorithm has been proposed in this research work and compared against recent works. Optimal classifications results are observed and recorded in this research paper.																	1432-7643	1433-7479				JUL	2020	24	14					10933	10941		10.1007/s00500-019-04596-x		DEC 2019											
J								Survey on GAN-based face hallucination with its model development	IET IMAGE PROCESSING										image resolution; face recognition; learning (artificial intelligence); high-resolution face image; input low-resolution face image; practical face applications; face verification; super-resolved face image; generic image super-resolution; high-level face recognition task; image resolution degradation; GAN-based learning; state-of-art GAN-based face hallucination		Face hallucination aims to produce a high-resolution face image from an input low-resolution face image, which is of great importance for many practical face applications, such as face recognition and face verification. Since the structure of the face image is complex and sensitive, obtaining a super-resolved face image is more difficult than generic image super-resolution. Recently, with great success in the high-level face recognition task, deep learning methods, especially generative adversarial networks (GANs), have also been applied to the low-level vision task - face hallucination. This work is to provide a model evolvement survey on GAN-based face hallucination. The principles of image resolution degradation and GAN-based learning are presented firstly. Then, a comprehensive review of the state-of-art GAN-based face hallucination methods is provided. Finally, the comparisons of these GAN-based face hallucination methods and the discussions of the related issues for future research direction are also provided.																	1751-9659	1751-9667				DEC 12	2019	13	14					2662	2672		10.1049/iet-ipr.2018.6545													
J								Image super-resolution using conditional generative adversarial network	IET IMAGE PROCESSING										learning (artificial intelligence); image reconstruction; image resolution; conditional generative adversarial network; extensive studies; single image super-resolution; high-frequency details; SISR approach; conditional GAN; SRCGAN; generator network; super-resolution images; discriminator network; SR images; ground-truth high-resolution ones; ground-truth HR image; stable generator model		Recently, extensive studies on a generative adversarial network (GAN) have made great progress in single image super-resolution (SISR). However, there still exists a significant difference between the reconstructed high-frequency and the real high-frequency details. To address this issue, this study presents an SISR approach based on conditional GAN (SRCGAN). SRCGAN includes a generator network that generates super-resolution (SR) images and a discriminator network that is trained to distinguish the SR images from ground-truth high-resolution (HR) ones. Specifically, the discriminator network uses the ground-truth HR image as a conditional variable, which guides the network to distinguish the real images from the SR images, facilitating training a more stable generator model than GAN without this guidance. Furthermore, a residual-learning module is introduced into the generator network to solve the issue of detail information loss in SR images. Finally, the network is trained in an end-to-end manner by optimizing a perceptual loss function. Extensive evaluations on four benchmark datasets including Set5, Set14, BSD100, and Urban100 demonstrate the superiority of the proposed SRCGAN over state-of-the-art methods in terms of PSNR, SSIM, and visual effect.																	1751-9659	1751-9667				DEC 12	2019	13	14					2673	2679		10.1049/iet-ipr.2018.6570													
J								Learning one-to-many stylised Chinese character transformation and generation by generative adversarial networks	IET IMAGE PROCESSING										character sets; optical character recognition; document image processing; learning (artificial intelligence); feature extraction; natural language processing; stylised Chinese character transformation; generative adversarial networks; Chinese character fonts transformation; font style-specifying mechanism		Owing to the complex structure of Chinese characters and the huge number of Chinese characters, it is very challenging and time consuming for artists to design a new font of Chinese characters. Therefore, the generation of Chinese characters and the transformation of font styles have become research hotspots. At present, most of the models on Chinese character transformation cannot generate multiple fonts, and they are not doing well in faking fonts. In this article, the authors propose a novel method of Chinese character fonts transformation and generation based on generative adversarial networks. The authors' model is able to generate multiple fonts at once through font style-specifying mechanism and it can generate a new font at the same time if the authors combine the characteristics of existing fonts.																	1751-9659	1751-9667				DEC 12	2019	13	14					2680	2686		10.1049/iet-ipr.2019.0009													
J								ACFT: adversarial correlation filter for robust tracking	IET IMAGE PROCESSING										object detection; object tracking; correlation methods; filtering theory; learning (artificial intelligence); ACFT; robust tracking; boundary effects; intrinsic circular structure; Generative Adversarial Networks; spatial regularisation; feature independence; robust features; spatial domain; background features; optimisation filter; standard tracking benchmarks; visual object tracking; GAN		Tracking based on correlation filters has demonstrated outstanding performance in recent visual object tracking studies and competitions. However, the performance is limited since the boundary effects are introduced by the intrinsic circular structure. In this study, a tracker, called adversarial correlation filter tracker (ACFT), is proposed to solve the above problem through Generative Adversarial Networks (GANs) that is specifically strong at producing realistic-looking data from noise circumstances. Especially, a mask is generated by the GANs to assist the conventional correlation filter for the spatial regularisation. By overcoming the feature independence of current regularisation in another tracker, the GANs' mask can be effectively used to identify the robust features for the target variations representation in the temporal domain. Also in the spatial domain, the background features can be substantially suppressed to obtain the optimisation filter for more reliable matching and updating. In verification, the authors evaluate the proposed tracker on the standard tracking benchmarks, and the experimental results show that their tracker outperforms favourably against other state-of-the-art trackers in the measurements of accuracy and robustness.																	1751-9659	1751-9667				DEC 12	2019	13	14					2687	2693		10.1049/iet-ipr.2018.6672													
J								ASiam: adaptive Siamese regression tracking with adversarial template generation and motion-based failure recovery	IET IMAGE PROCESSING										image motion analysis; object tracking; object detection; learning (artificial intelligence); target tracking; regression analysis; ASiam; novel adversarial template generation module; motion-based failure recovery module; temporal coherence; adaptive target template; sharp template quality; Siamese tracking backbone; motion response map; adaptive Siamese regression tracking; object tracking; temporal evolution; motion detection; motion-based failure recovery		Object tracking is challenged by the varying appearances of targets and the real-time requirement. Siamese regression trackers, being one of the most popular tracking paradigms, excel in efficiency but suffer at adaptability to cope with appearance variations. To improve their adaptability, the authors propose a new adaptive Siamese (ASiam) tracker, which integrates a novel adversarial template generation module and a motion-based failure recovery module. The template generation module exploits the temporal coherence and evolution of target appearance variations encoded in preceding tracklets and then generates an adaptive target template online which approximates the varying target in the coming frame. This generation module is optimised via adversarial learning to achieve accurate appearance prediction and sharp template quality. The generated template, together with a search region, are fed into a Siamese tracking backbone to compute an appearance response map via dense similarity computation in a sliding-window way. At frames where the Siamese tracking fails, the failure recovery module is invoked to perform deep frame differencing motion detection to provide a motion response map. By fusing different response maps, the drifted tracker can be re-calibrated. Extensive experiments on the OTB2013, OTB2015, and VOT2016 datasets prove the accuracy and efficiency of the proposed tracker.																	1751-9659	1751-9667				DEC 12	2019	13	14					2694	2705		10.1049/iet-ipr.2018.6699													
J								ApprGAN: appearance-based GAN for facial expression synthesis	IET IMAGE PROCESSING										computer vision; face recognition; image texture; computer animation; neural nets; learning (artificial intelligence); ApprGAN; appearance-based GAN; generative adversarial nets; image-to-image translation; appearance-based facial expression synthesis framework; input face image; texture generators; synthetic shape deformation; expression detail generation; cross-database synthesis; photorealistic identity-preserving expressions; photorealistic image generation; cycle-consistency; face synthesis synthesis; computer vision; animation; identity mapping; adversarial learning; image texture; quantitative verification	FACE RECOGNITION	Facial expression synthesis has drawn increasing attention in computer vision, graphics and animation. Recently, generative adversarial nets (GANs) have become a new perspective for face synthesis and have had remarkable success in generating photorealistic images and image-to-image translation. In this study, the authors present an appearance-based facial expression synthesis framework, ApprGAN, by combining shape and texture and introducing cycle consistency and identity mapping into the adversarial learning. Specifically, given an input face image, a pair of shape and texture generators are trained for synthetic shape deformation and expression detail generation, respectively. Extensive experiments on expression synthesis and cross-database synthesis were conducted, together with comparisons with the existing methods. Results of expression synthesis and quantitative verification on various databases show the effectiveness of ApprGAN in synthesising photorealistic and identity-preserving expressions and its marked improvement over the existing methods.																	1751-9659	1751-9667				DEC 12	2019	13	14					2706	2715		10.1049/iet-ipr.2018.6576													
J								Adversarial image generation by combining content and style	IET IMAGE PROCESSING										image recognition; feature extraction; learning (artificial intelligence); image texture; adversarial image generation; unique style; reference images; style feature extraction module; style specific image generation model; double-cycle training strategy; natural-content pairs; input natural images; style exchange; style-exchanged images; licence-plate image; handbags images		Images can be considered as the combination of two parts: the content and the style. The authors' approach can leverage this property by extracting a certain unique style from the reference images and combining it to generate images with new contents. With a well-defined style feature extraction module, they propose a novel framework to generate images with various styles and the same content. To train the style specific image generation model efficiently, a double-cycle training strategy is proposed: they input two natural-content pairs simultaneously, extract their style features, and exchange them twice to obtain the reconstruction of the input natural images. What is more, they apply the triplet margin loss to the style feature extracted from the images before and after style exchange and an adversarial discriminator to force the style-exchanged images to be real. They perform experiments on licence-plate image, Chinese characters, and shoes or handbags images generating, obtain photo-realistic results and remarkably improve the corresponding supervised recognition task.																	1751-9659	1751-9667				DEC 12	2019	13	14					2716	2723		10.1049/iet-ipr.2019.0103													
J								High confidence detection for moving target in aerial video	IET IMAGE PROCESSING										image segmentation; affine transforms; image motion analysis; video signal processing; image registration; edge detection; cameras; feature extraction; object detection; video surveillance; limited onboard computing resources; high confidence detection method; background compensation; three-frame-difference method; dynamic background; local feature extraction; speed-up robust feature key points; stabilisation task; global camera motion parameters; random sample consensus algorithm; detection results; higher-quality differential images; image quality assessment; accurate moving objects; edge detection algorithm; detecting performance; aerial video; moving target detection; tracking; challenge task; moving background; smaller target sizes	DIGITAL IMAGE STABILIZATION	The moving target detection and tracking in aerial video is a challenge task because of its moving background, smaller target sizes, lower resolution and limited onboard computing resources. In this study, a high confidence detection method based on background compensation and three-frame-difference method is designed, which can detect moving objects in a dynamic background accurately. First, the authors use local feature extraction and matching for image registration and demonstrate that speed-up robust feature key points are suitable for the stabilisation task. Then, they estimate the global camera motion parameters using affine transformation which are obtained by the random sample consensus algorithm. Finally, they detect moving object by three-frame-difference method. As the detection results of the frame-difference method generally exists 'empty' and noise, in order to select the two higher-quality differential images to perform the logic AND operation, they add image quality assessment to the three-frame-difference method to obtain more accurate moving objects. Moreover, the edge detection algorithm and morphological processing are integrated together to further boost the overall detecting performance. The extensive empirical evaluations on aerial videos demonstrate that the proposed detector is very promising for the various challenging scenarios.																	1751-9659	1751-9667				DEC 12	2019	13	14					2724	2734		10.1049/iet-ipr.2018.6388													
J								Combination of modified U-Net and domain adaptation for road detection	IET IMAGE PROCESSING										learning (artificial intelligence); image classification; object detection; roads; traffic engineering computing; feature extraction; road vehicles; image segmentation; computer vision; modified U-Net; scene understanding; autonomous driving; deep learning; visual road detection problem; complex illumination encounter; detection model; deep network architecture; network U-Net-prior; domain adaptation model; DAM; modified segmentation network; training images; test images; adversarial learning; monocular-vision-based methods		Road detection is one of the crucial tasks for scene understanding in autonomous driving. Recently, methods based on deep learning had rapidly grown and addressed this task excellently, because they can extract more abundant features. In this study, the authors consider the visual road detection problem as a classification for each pixel of the given image, which is road or non-road. There is complex illumination encounter in traffic applications, so that the detection model has poor adaptability. They address this problem by proposing a deep network architecture, which combines the network U-Net-prior and domain adaptation model (DAM). U-Net-prior is a modified segmentation network which integrates location prior and shape prior into U-Net. DAM is a model for reducing the gap between training images and test images, which is optimised in adversarial learning to make the features extracted from different datasets close to each other. They validate the effectiveness of each component of the algorithm, and compare the overall architecture with other state-of-the-art methods, and the results show that the architecture achieves top accuracies with the shortest run time in monocular-vision-based methods, simultaneously, compared with the methods based on other sensors, the architecture also achieves a competitive result.																	1751-9659	1751-9667				DEC 12	2019	13	14					2735	2743		10.1049/iet-ipr.2018.6696													
J								PixTextGAN: structure aware text image synthesis for license plate recognition	IET IMAGE PROCESSING										learning (artificial intelligence); feature extraction; text analysis; image segmentation; traffic engineering computing; object recognition; labelled data; text details; text regions; recognition accuracy; synthesised images; text image synthetisation; comprehensive structure-aware loss function; synthetic license plate images; specific character structures; generative adversarial networks; deep learning models; comprehensive license plate recognition; text image recognition; structure aware text image synthesis; PixTextGAN		Rapid progress on text image recognition has been achieved with the development of deep-learning techniques. However, it is still a great challenge to achieve a comprehensive license plate recognition in the real scenes, since there are no publicly available large diverse datasets for the training of deep learning models. This paper aims at synthesising of license plate images with generative adversarial networks (GAN), refraining from collecting a vast amount of labelled data. The authors thus propose a novel PixTextGAN that leverages a controllable architecture that generates specific character structures for different text regions to generate synthetic license plate images with reasonable text details. Specifically, a comprehensive structure-aware loss function is presented to preserve the key characteristic of each character region and thus to achieve appearance adaption for better recognition. Qualitative and quantitative experiments demonstrate the superiority of authors' proposed method in text image synthetisation over state-of-the-art GANs. Further experimental results of license plate recognition on ReId and CCPD dataset demonstrate that using the synthesised images by PixTextGAN can greatly improve the recognition accuracy.																	1751-9659	1751-9667				DEC 12	2019	13	14					2744	2752		10.1049/iet-ipr.2018.6588													
J								Progressive graph-based subspace transductive learning for semi-supervised classification	IET IMAGE PROCESSING										pattern classification; learning (artificial intelligence); graph theory; matrix algebra; iterative methods; optimisation; label domain; feature domain; noise points; feature information; label information; PGSTL; feature-to-label alignment; representative relation matrix; feature relationships; feature affinity matrix; high-dimensional feature space; semisupervised classification; efficient semisupervised learning technique; sufficient labelled samples; fixed subject-wise graph; progressive graph-based subspace transductive learning; iterative optimisation strategy	SCENE CLASSIFICATION; LABEL PROPAGATION; FEATURE-SELECTION; CONVEX	Graph-based transductive learning (GTL) is the efficient semi-supervised learning technique which is always employed in that sufficient labeled samples can not be obtained. Conventional GTL methods generally construct a inaccurate graph in feature domain and they are not able to align feature information with label information. To address these issues, we propose an approach called Progressive Graph-based subspace transductive learning (PGSTL) in this paper. PGSTL gradually find the intrinsic relationship between samples that more accurately aligns feature with label. Meanwhile, PGSTL develops a feature affinity matrix in the subspace of original high-dimensional feature space, which effectively reduce the interference of noise points. And then, the representative relation matrix and the feature affinity matrix are optimized by iterative optimization strategy and finally aligned. In this way, PGSTL can not only effectively reduce the interference of noisy points, but also comprehensively consider the information in the feature and label domain of data. Extensive experimental results on various benchmark datasets demonstrate that the PGSTL achieves the best performance compared to some state-of-the-art semi-supervised learning methods.																	1751-9659	1751-9667				DEC 12	2019	13	14					2753	2762		10.1049/iet-ipr.2018.6363													
J								Two-order graph convolutional networks for semi-supervised classification	IET IMAGE PROCESSING										approximation theory; learning (artificial intelligence); pattern classification; graph theory; convolutional neural nets; semisupervised classification; deep learning algorithms; natural language processing; diffusion-convolutional neural networks; GCN algorithm; one-order localised spectral graph filter; one-order polynomial; Laplacian; undirect neighbour structure information; graph structure data; two-order spectral graph convolutions; two-order approximation; two-order polynomial; abundant localised structure information; graph data; computer vision; two-order GCN; layerwise GCN; two-order graph convolutional networks; semi-supervised classification	TUTORIAL	Currently, deep learning (DL) algorithms have achieved great success in many applications including computer vision and natural language processing. Many different kinds of DL models have been reported, such as DeepWalk, LINE, diffusionconvolutional neural networks, graph convolutional networks (GCN), and so on. The GCN algorithm is a variant of convolutional neural network and achieves significant superiority by using a one-order localised spectral graph filter. However, only a one-order polynomial in the Laplacian of GCN has been approximated and implemented, which ignores undirect neighbour structure information. The lack of rich structure information reduces the performance of the neural networks in the graph structure data. In this study, the authors deduce and simplify the formula of two-order spectral graph convolutions to preserve rich local information. Furthermore, they build a layerwise GCN based on this two-order approximation, i.e. two-order GCN (TGCN) for semi-supervised classification. With the two-order polynomial in the Laplacian, the proposed TGCN model can assimilate abundant localised structure information of graph data and then boosts the classification significantly. To evaluate the proposed solution, extensive experiments are conducted on several popular datasets including the Citeseer, Cora, and PubMed dataset. Experimental results demonstrate that the proposed TGCN outperforms the state-of-art methods.																	1751-9659	1751-9667				DEC 12	2019	13	14					2763	2771		10.1049/iet-ipr.2018.6224													
J								Adversarial auto-encoder for unsupervised deep domain adaptation	IET IMAGE PROCESSING										image classification; pattern classification; unsupervised learning; learning (artificial intelligence); unsupervised deep domain adaptation; unsupervised visual domain adaptation; target domain; labelled source samples; unlabelled target samples; target domains; generative adversarial networks; novel adversarial auto-encoder; domain similarity; reconstruction information; adversarial domain adaptation		Unsupervised visual domain adaptation aims to train a classifier that works well on a target domain given labelled source samples and unlabelled target samples. The key issue in unsupervised visual domain adaptation is how to do the feature alignment between source and target domains. Inspired by the adversarial learning in generative adversarial networks, this study proposes a novel adversarial auto-encoder for unsupervised deep domain adaptation. This method incorporates the auto-encoder with the adversarial learning so that the domain similarity and reconstruction information from the decoder can be exploited to facilitate the adversarial domain adaptation in the encoder. Extensive experiments on various visual recognition tasks show that the proposed method performs favourably against or better than competitive state-of-the-art methods.																	1751-9659	1751-9667				DEC 12	2019	13	14					2772	2777		10.1049/iet-ipr.2018.6687													
J								SDCA: a novel stack deep convolutional autoencoder - an application on retinal image denoising	IET IMAGE PROCESSING										diseases; neural nets; blood vessels; medical image processing; learning (artificial intelligence); biomedical optical imaging; image denoising; convolution; eye; image segmentation; image representation; novel stack deep convolutional autoencoder; retinal image denoising; retinal fundus images; eye diseases; diabetic retinopathy; retinal vasculature; retinal conditions; visibility; noisy fundus; deep learning; denoising images; restoring features; noise level; target image; patched base training; denoising effect; standard fundus databases	LOW-DOSE CT; NETWORK	Retinal fundus images are used for the diagnosis and treatment of various eye diseases such as diabetic retinopathy, glaucoma, exudates and so on. The retinal vasculature is difficult to investigate retinal conditions due to the presence of various noises in the retinal image during the capture of the image. Removal of noise is an important aspect for better visibility and diagnosis of the noisy fundus in ophthalmology. This study represents a deep learning based approach to denoising images and restoring features using stack denoising convolutional autoencoder. The proposed scheme is implemented to restore the structural details of fundus as well as to decrease the noise level. Furthermore, the proposed model utilises shared layers with the optimal manner to reduce the noise level of the target image with minimal computational cost. To restore an image, the proposed model brings a patched base training on samples to suppress with one to one manner without any loss of information. To access the denoising effect of the proposed scheme, several standard fundus databases such as DRIVE, STARE and DIARETDB1 have been tested in this study. Comparing the efficiency of the suggested model with state-of-art methods, the proposed scheme gives better result in terms of qualitative and quantitative analysis.																	1751-9659	1751-9667				DEC 12	2019	13	14					2778	2789		10.1049/iet-ipr.2018.6582													
J								Modified tropical algebra based median filter for removing salt and pepper noise in digital image	IET IMAGE PROCESSING										singular value decomposition; filtering theory; image denoising; median filters; algebra; digital image; information damage; transmission process; filtering process; filtering algorithm; low level noise; singular value decomposition; SVD; tropical operations; modified tropical algebra; median filter; salt; pepper noise		Noise is the information damage that may occur in the image due to the changes in information during the transmission process. In order to overcome these problems, it is necessary to do filtering process on the image. Until now many filtering algorithms have been proposed to remove noise. Most existing methods only work for low level noise. In this study, the authors proposed an efficient and easy-to-understand filtering algorithm using the concept of tropical algebra and singular value decomposition (SVD). The SVD will be used to detect noise in 3 x 3 templates. Furthermore, if noise is detected then new pixels will be obtained by using the concept of tropical operations. The results of this study show that the proposed method provides better results from the existing methods in terms of quantitative and visual.																	1751-9659	1751-9667				DEC 12	2019	13	14					2790	2795		10.1049/iet-ipr.2018.6201													
J								Dense small face detection based on regional cascade multi-scale method	IET IMAGE PROCESSING										object detection; learning (artificial intelligence); feature extraction; face recognition; convolutional neural nets; local detector; nonmaximum suppression methods; object detection; dense face detection; regional cascade multiscale method; detection performance; scale invariance properties; deep convolutional neural networks; regional cascade multiscale detection method; global detector; sub-training set		In the field of object detection, the research on the problem of detecting small face is the most extensive, but when there are objects with obvious scale differences in the image, the detection performance is not obvious, which is due to the scale invariance properties of the deep convolutional neural networks. Although in recent years, there have been some methods proposed to solve this problem such as FPN and SNIP, which is based on feature pyramid. However, they have not fundamentally solved the problem. A regional cascade multi-scale detection method has been proposed. First, a global detector and several local detectors have been trained, respectively. The global detector is trained by the original training set, while the local detector is trained by the sub-training set generated by the original training set. Second, the global detector can detect object roughly and the local detectors can produce more detailed results that improve the performance of global detector. Finally, to integrate the detection results of global detector and local detectors as the output, non-maximum suppression methods are used. The method can be carried in any depth model of object detection, has good scalability, and is more suitable for dense face detection.																	1751-9659	1751-9667				DEC 12	2019	13	14					2796	2804		10.1049/iet-ipr.2018.6571													
J								Fusing texture, edge and line features for smoke recognition	IET IMAGE PROCESSING										Hough transforms; support vector machines; feature extraction; edge detection; image texture; image classification; smoke; image fusion; line information; feature extraction method; smoke recognition; Canny operator; edge image; discrete line image; local patterns; local boundary summation pattern; LBSP; binary pixel values; centre pixel; local region summation pattern; LRSP; traditional texture information; feature vector; low dimensional features; multiclass texture classification; line features; Hough transform; support vector machine	LOCAL BINARY PATTERNS; COOCCURRENCE; DESCRIPTOR	To improve recognition accuracy, the authors fuse texture, edge and line information to propose a feature extraction method for smoke recognition. The Canny operator is proposed to generate an edge image from an original image, and then adopt the Hough transform to extract straight lines from the edge image. The lines are rasterised to generate a discrete line image and two local patterns are proposed for the edge and line images. The first one is local boundary summation pattern (LBSP) that computes the sum of binary pixel values along the boundary of a local region around a centre pixel. The second one is called local region summation pattern (LRSP) that sums up the binary values of pixels in a local region around the centre pixel. Besides LBSP and LRSP, LBPs with three mapping modes (LBP_M3) to achieve traditional texture information are also extracted. Finally, the authors concatenate the histograms of LBP_M3, LBSP and LRSP to generate a feature vector, and use support vector machine for classifying and testing. Experiments show that authors' method outperforms most of existing traditional methods for smoke recognition. Although this method has low dimensional features, it also obtains good performance for multi-class texture classification.																	1751-9659	1751-9667				DEC 12	2019	13	14					2805	2812		10.1049/iet-ipr.2019.0012													
J								Capturing the spatio-temporal continuity for video semantic segmentation	IET IMAGE PROCESSING										feature extraction; image segmentation; image representation; video signal processing; neural nets; probability; video semantic segmentation; image semantic segmentation; convolutional neural network; image segmentation algorithms; video frame; temporal region continuity inherent; videos; deep neural network architecture; newly devised spatio-temporal continuity; encoding network; STC module; decoding network; high-level feature map; STC feature map; current feature representation; consecutive video frames; segmentation result		In recent years, image semantic segmentation based on a convolutional neural network has achieved many advances. However, the development of video semantic segmentation is relatively slow. Directly applying the image segmentation algorithms to each video frame separately may ignore the temporal region continuity inherent in videos. In this study, the authors propose a novel deep neural network architecture with a newly devised spatio-temporal continuity (STC) module for video semantic segmentation. Particularly, the architecture includes an encoding network, an STC module, and a decoding network. The encoding network is used to extract a high-level feature map. The STC module then uses the high-level feature map as input to extract the STC feature map. For decoding, they use four dilated convolutional layers to obtain more abstract representation and a deconvolutional layer to increase the size of the representation. Finally, they fuse the current feature representation and the previous feature representation and get the class probabilities. Thus, this architecture receives a sequence of consecutive video frames and outputs the segmentation result of the current frame. They extensively evaluate the proposed approach on the CamVid and KITTI datasets. Compared with other methods, the authors' approach not only achieves competitive performance but also has lower complexity.																	1751-9659	1751-9667				DEC 12	2019	13	14					2813	2820		10.1049/iet-ipr.2018.6479													
J								Aesthetic assessment of paintings based on visual balance	IET IMAGE PROCESSING										pattern clustering; art; image colour analysis; cosmic equilibrium rule; aesthetic effect; visual balance; aesthetic value; information aesthetics theory; evaluation method; image evaluation; image colour composition; K-means clustering method; visual centre of gravity; benchmark data set		All things follow the cosmic equilibrium rule. As one of the important factors in evaluating images of aesthetic effects, the visual balance has not attracted much attention. In this study, a novel method is proposed to quantify the law of visual balance and automatically evaluate the aesthetic value of images. In the proposed method, the authors first analyse the colour composition of images using the K-means clustering method. Then, based on information aesthetics theory and perspective principles, a calculation method is proposed to find the visual centre of gravity of images. Finally, the aesthetic effect of the image based on visual balance is evaluated according to the positional relationship between the visual centre of gravity and the physical centre of the image. With extensive experimental results, the authors demonstrate qualitatively and quantitatively that the proposed evaluation method is basically consistent with the intuitive experience of most human beings. Moreover, experiments are also conducted on a large and diversified benchmark data set that are competitive with the current state of the art.																	1751-9659	1751-9667				DEC 12	2019	13	14					2821	2828		10.1049/iet-ipr.2018.6572													
J								Homography-based traffic sign localisation and pose estimation from image sequence	IET IMAGE PROCESSING										pose estimation; traffic engineering computing; image sequences; object recognition; image matching; gradient methods; feature extraction; image classification; computer vision; cameras; video signal processing; object detection; homography-based traffic sign localisation; image sequence; vision-based method; binocular cameras; monocular cameras; robust feature correspondences; homography constraints; image pairs; objective function; traffic sign plane; attribute estimation; raw KITTI dataset; pose estimation; SIFT; SURF; size 0; 44 m; size 0; 51 m	VISION; RECOGNITION	This study proposes a vision-based method for traffic sign attribute estimation, i.e. 3D position and pose, from image sequences by binocular or monocular cameras. The method starts with acquiring robust feature correspondences based on homography constraints from image pairs. Then the objective function is designed to integrate the feature correspondences to optimise the parameters of the traffic sign plane in the 3D coordinate. Finally, the sign plane is utilised for attribute estimation. In addition, the authors provide an extension for the raw KITTI dataset, which can be utilised for 3D tasks of traffic sign localisation and pose estimation. In the experiments, three popular methods are employed for comparisons based on the publicly available BelgiumTS and KITTI datasets. The results show that the authors' method based on SIFT and SURF features can locate the traffic signs with a mean error of similar to 0.44 and 0.51 m in the BelgiumTS and KITTI datasets, respectively, and estimate the pose with a mean error of similar to 14.45 degrees in the KITTI dataset.																	1751-9659	1751-9667				DEC 12	2019	13	14					2829	2839		10.1049/iet-ipr.2019.0023													
J								Single sample description based on Gabor fusion	IET IMAGE PROCESSING										face recognition; image texture; wavelet transforms; image resolution; feature extraction; image fusion; visual databases; face image; traditional face recognition algorithms; single training sample; block sub-images; multiresolution Gabor wavelets; multiorientation; fusion criterion; fusion Gabor feature histograms; local region; local image information content model; weighting map; face databases; LWFG algorithm; single sample description; Gabor fusion; orientation Gabor wavelets; orientation Gabor wavelets; scale Gabor wavelets; local weighted fusion Gabor algorithm; partial occlusion; expression change; illumination variation	LINEAR DISCRIMINANT-ANALYSIS; FACE RECOGNITION; DECOMPOSITION; CLASSIFICATION; FILTER; PCA; LDA; BINARY; LBP	Owing to lack of enough face image and invalidation of many traditional face recognition algorithms, face recognition with single training sample is really a great challenge. To solve the above problem, this study proposes a novel local weighted fusion Gabor (LWFG) algorithm. First, one single sample is segmented into a series of block sub-images, and then, each of these sub-images is decomposed into a series of multi-resolution Gabor wavelets with multi-orientation and multi-scale. Second, different orientation Gabor wavelets with the same scale are fused. Next, different scale Gabor wavelets with the same orientation are fused according to the proposed fusion criterion. Third, the fusion Gabor feature histograms are calculated in each of the divided local regions. Meanwhile, every local region's information importance is measured by the proposed local image information content model. Finally, the fusion Gabor wavelet histograms are adaptively weighed by weighting map which calculated from information content model. This study conducted simulation experiments on different face databases under the different conditions including partial occlusion, expression change and illumination variation. The results indicated that the proposed LWFG algorithm is more effective with single training sample.																	1751-9659	1751-9667				DEC 12	2019	13	14					2840	2849		10.1049/iet-ipr.2018.6665													
J								Blind text images deblurring based on a generative adversarial network	IET IMAGE PROCESSING										learning (artificial intelligence); text analysis; image restoration; blind text images; generative adversarial network; text images deblurring; hand-crafted priors; specific kernel; text deblurring problem; semantic generation task; structural loss function; detailed loss function; multiscale generator; generated text images; realistic latent images; real-world blurry images		Recently, text images deblurring has achieved advanced development. Unlike previous methods based on hand-crafted priors or assume specific kernel, the authors recognise the text deblurring problem as a semantic generation task, which can be achieved by a generative adversarial network. The structure is an essential property of text images; thus, they propose a structural loss function and a detailed loss function to regularise the recovery of text images. Furthermore, they learn from the coarse-to-fine strategy and present a multi-scale generator, which is utilised for sharpening the generated text images. The model has a robust capability of generating realistic latent images with photo-quality effect. Extensive experiments on the synthetic and real-world blurry images have shown that the proposed network is comparable to the state-of-the-art methods.																	1751-9659	1751-9667				DEC 12	2019	13	14					2850	2858		10.1049/iet-ipr.2018.6697													
J								Salient object detection with adversarial training	IET IMAGE PROCESSING										object detection; convolutional neural nets; image recognition; generative adversarial network; image generation; adversarial training method; salient object detection models; convolutional SOD network; gated adversarial network; pixel-wise errors; ground truth salient detection maps; SOD dataset; MSRA-B; extended complex scene saliency dataset; HKU-IS dataset; DUT dataset		The generative adversarial network has been shown to produce state-of-the-art results of image generation. In this study, the authors propose a novel adversarial training method to train salient object detection (SOD) models. They train a convolutional SOD network along with a gated adversarial network that discriminates salient maps coming either from the ground truth or from the SOD network. The motivation for our approach is that the adversarial network can detect and correct pixel-wise errors between ground truth salient detection maps and the ones produced by the convolutional network. Our experiments show that the adversarial training approach leads to state-of-the-art performance on MSRA-B, extended complex scene saliency dataset, HKU-IS, DUT, and SOD dataset.																	1751-9659	1751-9667				DEC 12	2019	13	14					2859	2865		10.1049/iet-ipr.2018.6581													
J								Supervised fusion approach of local features extracted from SAR images for detecting deforestation changes	IET IMAGE PROCESSING										radar imaging; radar cross-sections; synthetic aperture radar; feature extraction; image classification; radar polarimetry; geophysical image processing; image texture; remote sensing by radar; supervised fusion approach; local features; SAR images; continuous regression; forested areas; deforestation change detection; local texture features; ALOS PALSAR; Advanced Land Observation Satellite; Array type L-band Synthetic Aperture Radar; multitemporal data; polarimetric features; NRCS classification; grey-level co-occurrence matrix texture features; different moving window; local regions; NRCS results; resulted images; supervised approach; good detection results	L-BAND SAR; MAPPING DEFORESTATION; ALOS PALSAR; FORESTS; COVER; INTEGRATION; AREAS	Deforestation has become a major problem consisting of a continuous regression of forested areas in the world, and for this purpose, an efficient detection of these changes has become more than necessary. In this work, a new method for deforestation change detection is proposed. This approach is based on a supervised fusion of local texture features extracted from SAR images. ALOS PALSAR (Advanced Land Observation Satellite Phased Array type L-band Synthetic Aperture Radar) multi-temporal data have been used in this work. Normalised radar cross-section (NRCS) and polarimetric features extracted from HH and HV polarised data allowed recognising different categories of land covers termed as NRCS classification. Grey-level co-occurrence matrix (GLCM) texture features were extracted by using a different moving window sizes applied on local regions previously obtained by binarisation of the NRCS results. A total of 300 samples of regions and five GLCM characteristics have been used here. The detection of deforestation appears clearly in the resulted images with a very satisfactory precision of the reached regions, and the obtained results of the proposed supervised approach have indeed led to very good detection results of the deforestation change.																	1751-9659	1751-9667				DEC 12	2019	13	14					2866	2876		10.1049/iet-ipr.2019.0122													
J								Statistical multidirectional line dark channel for single-image dehazing	IET IMAGE PROCESSING										image enhancement; fog; Radon transforms; image restoration; image resolution; statistical analysis; statistical multidirectional line dark channel; single-image dehazing; outdoor scenes; atmospheric degradation; fog; autonomous driving; surveillance systems; dehazing methods; dehazing techniques; restoration quality; haze effects; Radon transform; dehazing images; statistical computations; saturated areas; reduced-reference image quality dehazing assessment; full-reference metrics Structural SIMilarity index; synthetic outdoor images	CONTRAST	Outdoor scenes often contain atmospheric degradation, such as fog or haze, which deteriorate the performance of tracking, autonomous driving and surveillance systems, among others, making dehazing methods an area of considerable interest. However, some dehazing techniques are computationally demanding, generating a trade-off between time-consumption and restoration quality. A new method is proposed for improving outdoor images taken with haze effects while making them less time-consuming. The proposed method was inspired by the Radon transform and tailored for dehazing images by computing the dark channel, in addition to using statistical computations and a heuristic approach to avoid saturated areas. The results obtained were subjected to a reduced-reference image quality dehazing assessment, a full-reference metrics Structural SIMilarity (SSIM) index and peak signal-to-noise ratio (PSNR) over real-world and synthetic outdoor images. The results demonstrate that the proposed method presents an adequate balance between new visible edges, increased gradient and saturated pixels, in addition to obtaining at least a 5% increase in the SSIM index and, a 16% increase in the PSNR index, as well as being 5.37 times faster than the four dehazing methods recently introduced in the literature.																	1751-9659	1751-9667				DEC 12	2019	13	14					2877	2887		10.1049/iet-ipr.2018.6403													
J								New over-relaxed monotone fast iterative shrinkage-thresholding algorithm for linear inverse problems	IET IMAGE PROCESSING										inverse problems; gradient methods; convergence of numerical methods; parameter setting strategy; OMFISTA; over-relaxed monotone; shrinkage-thresholding algorithm; linear inverse problems; complex convergence condition; sufficient condition; OMFISTAv2; system matrix	SIGNAL; OPTIMIZATION; PROJECTION	The over-relaxed monotone fast iterative shrinkage-thresholding algorithm (OMFISTA) needs to satisfy a complex convergence condition with respect to its additional parameters. To simplify the convergence condition, this study proposes a new OMFISTA, termed OMFISTAv2, using a parameter setting strategy which will derive a simple sufficient condition with respect to the additional parameters to guarantee the convergence of OMFISTAv2. Moreover, the authors find experimentally that OMFISTAv2 can accelerate MFISTA in some cases where the system matrix is ill-conditioned or rank-deficient, while OMFISTA cannot.																	1751-9659	1751-9667				DEC 12	2019	13	14					2888	2896		10.1049/iet-ipr.2019.0600													
J								HPILN: a feature learning framework for cross-modality person re-identification	IET IMAGE PROCESSING										learning (artificial intelligence); feature extraction; image classification; video surveillance; cameras; pose estimation; image colour analysis; modified cross-modality re-identification models; HPILN; feature learning framework; cross-modality person; video surveillance systems; RGB; cameras; cross-modality variations; heterogeneous images; intra-modality variations; heterogeneous human poses; camera position; identity loss network; single-modality re-identification models; cross-modality scenario; SYSU-MM01 dataset; cumulative match characteristic curve; mean average precision	NETWORK	Most video surveillance systems use both RGB and infrared cameras, making it a vital technique to re-identify a person cross the RGB and infrared modalities. This task can be challenging due to both the cross-modality variations caused by heterogeneous images in RGB and infrared, and the intra-modality variations caused by the heterogeneous human poses, camera position, light brightness etc. To meet these challenges, a novel feature learning framework, hard pentaplet and identity loss network (HPILN), is proposed. In the framework existing single-modality re-identification models are modified to fit for the cross-modality scenario, following which specifically designed hard pentaplet loss and identity loss are used to increase the accuracy of the modified cross-modality re-identification models. Based on the benchmark of the SYSU-MM01 dataset, extensive experiments have been conducted, showing that the authors' method outperforms all existing ones in terms of cumulative match characteristic curve and mean average precision.																	1751-9659	1751-9667				DEC 12	2019	13	14					2897	2904		10.1049/iet-ipr.2019.0699													
J								Image encryption algorithm based on image hashing, improved chaotic mapping and DNA coding	IET IMAGE PROCESSING										image coding; DNA; statistical analysis; cryptography; image processing; entropy; random sequences; chaos; chaotic communication; image encryption algorithm; image hashing; improved chaotic mapping; image features; fingerprint image; binary hash sequence; generated hash sequence; improved chaotic map; Chen's chaotic system; encryption algorithms	PERMUTATION; MAP; SYSTEM	In order to improve the problems of security and robustness for existing image encryption algorithms and to reduce the security risks of encryption algorithms against statistical analysis, differential attacks, exhaustive attacks, cropping and noise attacks etc., a novel image encryption algorithm based on image hashing, improved chaotic mapping and DNA coding is proposed. Firstly, extracting the image features and evenly block after pre-processing the original image and fingerprint image. Secondly, the features are generated to be a binary hash sequence through the image hash algorithm, and the generated hash sequence is iterated as the initial parameter of the improved chaotic map and Chen's chaotic system. Finally, Chen's chaotic system is used to generate a random sequence, and the matrix generated by the improved chaotic map and the original image is subjected to DNA calculation and encoding operations to obtain an encrypted image. Experimental results show that the proposed algorithm has better performance on security, larger key space and higher key sensitivity, the pixel correlation coefficient close to 0, the information entropy close to 8, unified average changing intensity and number of pixels change rate values are close to ideal values, and it has better robustness on noise and cropping attacks.																	1751-9659	1751-9667				DEC 12	2019	13	14					2905	2915		10.1049/iet-ipr.2019.0667													
J								Defect inspection research on fabric based on template correction and primitive decomposition	IET IMAGE PROCESSING										fabrics; feature extraction; image segmentation; automatic optical inspection; defect inspection research; fabric image; variation regularity; effective anisotropy correction method; stretching distortion; corrected lattice; graphic elements; self-supervised threshold selection strategy; defect-free regions; TCPD method; star-patterned fabrics; template correction with primitive decomposition method		To accurately detect defects in patterned fabrics, a novel detection algorithm combining template correction with primitive decomposition (TCPD) method is proposed in this study. First of all, the fabric image is segmented into lattices according to variation regularity. Then, the authors propose an effective anisotropy correction method to reduce the interference of stretching and distortion between lattices. On the basis of the proposed PD method, the corrected lattice is further divided into graphic elements with smaller particle size. The smaller primitives make the boundary of the detection results more accurate. Moreover, a self-supervised threshold selection strategy is presented, which utilises the defect-free regions to obtain threshold. Furthermore, this strategy makes each primitive has corresponding criteria for judging defects. Extensive experiments demonstrate that TCPD method achieves 0.8127 true positive rate, 0.3889 positive predictive value and 0.5261 f value in star-patterned fabrics.																	1751-9659	1751-9667				DEC 12	2019	13	14					2916	2928		10.1049/iet-ipr.2018.6626													
J								Learning mean progressive scattering using binomial truncated loss for image dehazing	IET IMAGE PROCESSING										image restoration; image enhancement; image denoising; learning (artificial intelligence); image colour analysis; probability; binomial truncated loss; image dehazing; progressive dehazing network; single image haze removal problem; mean progressive scattering model; atmosphere light; unified network; coarse transmission map; progressive refinement branch; fine-scale transmission map; prediction accuracy; novel binomial; error values; error occurrences	HAZE-RELEVANT FEATURES; MODEL	In this study, the authors propose a novel progressive dehazing network to address the single image haze removal problem based on a new mean progressive scattering model. Different from methods that learn atmosphere light and transmission maps with different networks, these two variables are optimised in a unified network. Following the methodology of traditional prior-based methods that estimate a coarse transmission map first, a progressive refinement branch in the decoder has been designed to restore the fine-scale transmission map. To improve the prediction accuracy of the transmission map, a novel binomial truncated loss that assigns weights to error values according to the probabilities of error occurrences has been proposed. An ablation study is conducted to verify the effectiveness of the components in the proposed method. Experiments in the synthetic datasets and real images demonstrate that the proposed method outperforms other state-of-the-art methods.																	1751-9659	1751-9667				DEC 12	2019	13	14					2929	2939		10.1049/iet-ipr.2019.0261													
J								Towards a reactive system for managing big trajectory data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Akka; Actor-based systems; Big data; Distributed computing; Message-driven architecture; Reactive systems		Spatio-temporal events often describe the movements of an object in terms of space, time, and potential other attributes. Significant knowledge can be inferred by analysing them, either individually or atomically in form of trajectories. The trajectories can abstract additional properties and lead to deeper value. Moreover, external contextual information can be attributed to them to change their structure and lead to different perspectives. Because of this potentially valuable knowledge, nowadays indoor and outdoor tracking devices are used everywhere; generating countless events instantaneously. However, the extraction of knowledge from such heterogeneous, massive data is not a trivial task. In other terms, there is a need for a sophisticated system that is efficient in terms of distributed computing, failure handling, responsiveness, and abstraction. To answer this need, our study incorporates a fully fledged, reactive system for big trajectory data management. The system is unique of its kind because it is actor-based and features responsiveness, resiliency, and elasticity. Furthermore, our system is implemented using Scala; hence, we have the expressive power of both the Object-Oriented (OO) and Functional Programming (FP) paradigms. Allowing us to reach a higher level of abstraction to be able to process any trajectory type. The scope of this paper is to detail our system and discuss elasticity, routing strategies, load balancing, and our proper fault-tolerance mechanism. To fulfill this study, we consider the Geolife project's GPS trajectory dataset.																	1868-5137	1868-5145				OCT	2020	11	10			SI		3895	3906		10.1007/s12652-019-01625-3		DEC 2019											
J								Finding optimal pedagogical content in an adaptive e-learning platform using a new recommendation approach and reinforcement learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										E-learning; Collaborative filtering; Social filtering; Semantic similarity; Fuzzy logic; Reinforcement learning	USER SIMILARITY MODEL; GENETIC ALGORITHM; SYSTEMS	In the learning process, learners have different skills and each one has his own knowledge and his own ability to learn. The adaptive e-learning platforms try to find optimal courses for learners based on their knowledge and skills. Learning online using e-learning platforms becomes indispensable in the teaching process. Companies and scientific researchers try to find new optimal methods and approaches that can improve education online. In this paper, we propose a new recommendation approach for recommending relevant courses to learners. The proposed method is based on social filtering(using the notions of sentiment analysis) and collaborative filtering for defining the best way in which the learner must learn, and recommend courses that better much the learner's profile and social content. Our work consists also in proposing a new reinforcement learning approach which helps a learner to find the optimal learning path that can improve the quality of learning.																	1868-5137	1868-5145				OCT	2020	11	10			SI		3921	3936		10.1007/s12652-019-01627-1		DEC 2019											
J								IoT and cloud computing based automatic epileptic seizure detection using HOS features based random forest classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Epilepsy; Cloud computing; Random forest; Machine learning; Internet of things; Healthcare; Higher order spectra	EEG SIGNAL; FEATURE-EXTRACTION; SYSTEM; IDENTIFICATION; PERFORMANCE; ACCURACY; INTERNET; ENTROPY; CHANNEL; THINGS	Epilepsy, a fatal neurological disorder, has been emerged as a worldwide problem and is one of the major risks to human lives. There exists an urgent need for an efficient technique for early detection of epileptic seizures at its initial stage in order to save the lives of thousands of epileptic patients annually. Now a days, internet of things in combination with machine learning techniques and cloud computing services has emerged as a powerful technology to resolve many problems in healthcare sector. This paper also presents an automatic epileptic seizure detection system and its layered architecture for early detection of epileptic seizures using existing communication technologies in collaboration with machine learning and cloud computing. This model transmits sensed EEG signals from patient's scalp to cloud through 4G cellular network or Wi-Fi network. At cloud, EEG signals are processed using Fast Walsh Hadamard transform and higher order spectra (HOS) based feature extraction for extracting higher order statistics and entropy-based features. The correlation-based feature selection algorithm has been employed for reducing the dimensionality of EEG datasets so as to tackle the problem of large volume of data and to reduce delays in service offered to the end user. Random Forest algorithm has been employed for classification of EEG signals into three different seizure stages, viz., normal, preictal and ictal. For performance analysis, other well-known machine learning algorithms like Bayes Net, Naive Bayes, Multilayer Perceptron, Radial Basis function neural network and C4.5 Decision Tree are also considered. The simulation and testing results show that Random Forest classifier provides maximum values of classification accuracy of 99.40%, sensitivity of 99.40% and specificity of 99.66%, minimum mean square error of 0.0871 along with optimum training time of 20 ms, which makes this model more real time compatible, thereby making HOS features based Random Forest algorithm's cloud model an efficient technique for early and automatic detection of epileptic seizures in real time.																	1868-5137	1868-5145															10.1007/s12652-019-01613-7		DEC 2019											
J								UHD 8K energy-quality scalable HEVC intra-prediction SAD unit hardware using optimized and configurable imprecise adders	JOURNAL OF REAL-TIME IMAGE PROCESSING										Energy-quality scalability; Video coding; Intra-prediction; SAD; Scalable hardware design; Approximate computing	FRACTIONAL MOTION ESTIMATION; EFFICIENT VLSI ARCHITECTURE; DCT ARCHITECTURES; PERFORMANCE; DESIGN; IMPLEMENTATION	Real-time digital video coding became a mandatory feature in current consumer electronic devices due to the popularization of video applications. However, efficiently encoding videos is an extremely processing/energy-demanding task, especially at high resolutions and frame rates. Thus, the limited energy resources and the dynamically varying system status (such as workload, battery level, user settings, etc.) require energy-efficient solutions capable to support run-time energy-quality scalability. In this work, we present an energy-quality scalable SAD Unit hardware architecture for the HEVC intra-frame prediction targeting real-time processing of UHD 8K (7680 x 4320) videos at 60 frames per second. Approximate computing is used to provide energy-quality scalability by employing configurable imprecise operators. The proposed Energy-Quality scalable architecture supports four operation points: precise computing, and 3-bit, 5-bit or 7-bit imprecision. When implemented in a 45-nm technology using Nangate standard cells library and running at 269 MHz, the proposed architecture consumes from 8.42 to 7.38 mJ to process each UHD 8K frame, according to the selected imprecision level. As a drawback, the coding efficiency (measured in BD rate) is reduced from 0.28 to 1.72%. Compared to the related works, this is the only intra-frame prediction SAD unit able to provide energy-quality scalability.																	1861-8200	1861-8219															10.1007/s11554-019-00934-2		DEC 2019											
J								Improving bluetooth beacon-based indoor location and fingerprinting	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Beacon; Wireless; GPS; Indoor location; Block-chain; Crowd learning; Bluetooth; BLE; WiFi; Fingerprinting	FUSION	The complex way radio waves propagate indoors, leads to the derivation of location using fingerprinting techniques. In this cases, location is computed relying on WiFi signals strength mapping. Recent Bluetooth low energy (BLE) provides new opportunities to explore positioning. In this work is studied how BLE beacons radio signals can be used for indoor location scenarios, as well as their precision. Additionally, this paper also introduces a method for beacon-based positioning, based on signal strength measurements at key distances for each beacon. This method allows to use different beacon types, brands, and location conditions/constraints. Depending on each situation (i.e., hardware and location) it is possible to adapt the distance measuring curve to minimize errors and support higher distances, while at the same time keeping good precision. Moreover, this paper also presents a comparison with traditional positioning method, using formulas for distance estimation, and the position triangulation. The proposed study is performed inside the campus of Viseu Polytechnic Institute, and tested using a group of students, each with his smart-phone, as proof of concept. Experimental results show that BLE allows having < 1.5 m error approximately 90% of the times, and the experimental results using the proposed location detection method show that the proposed position technique has 13.2% better precision than triangulation, for distances up to 10 m.																	1868-5137	1868-5145				OCT	2020	11	10			SI		3907	3919		10.1007/s12652-019-01626-2		DEC 2019											
J								Optimal coverage along with connectivity maintenance in heterogeneous wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Coverage; Clustering; Connectivity; Network Lifetime; Energy		Heterogeneous wireless sensor network (HWSN) is a network which contains sensor nodes with dissimilar capabilities, such as varying energy, sensing range etc. In Heterogeneous wireless sensor network, Coverage along with connectivity is one of the major issues. Coverage with no connectivity is meaningless in wireless sensor networks. In most of the existing work, coverage and connectivity is not provided efficiently for HWSN. The existing methods used in HWSN to attain coverage and connectivity are static and not applied in the dynamic environment. Optimal Coverage and connectivity maintenance not provided. Energy consumption should be concentrated to maximize the network lifetime. Preserving network connectivity at the same time maximizing coverage by using the less number of energy constrained sensor nodes is the most serious problem. To overcome these issues, in this work we put forward a technique called Coverage Scheduling and Power Aware Connectivity (CSPAC). Grouping the sensors into cluster by the strategy of Dynamic cluster formation. Identifying the minimum active set to provide Q-coverage by Artificial Bee Colony Optimization technique and maximize lifetime by mapping Power Aware Scheduling and Battery Discharge Value. Then Optimal Connectivity Management Technique for reachability to provide strong connectivity. By simulation results, we give you an idea that the proposed technique enhances the network coverage and connectivity in HWSN.																	1868-5137	1868-5145															10.1007/s12652-019-01621-7		DEC 2019											
J								Performance evaluation of cybercriminal detection through cluster computing techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cybercrime; Cluster based structure; Data privacy; Gaussian Clustering; K Means Clustering technique; Fuzzy C Means and user profile analysis		The cybercrime has evolved as the years ago and the criminals use different techniques to capture the victims. Researchers have devised various techniques to detect the cybercriminal. Hence, a new technology has to be used to detect the Cybercrime. From the literature, it is analysed that many researchers have used various techniques to detect the Cybercrime. In this paper, the performance evaluation of various cybercriminal detection techniques are analysed using cluster computing techniques and Matlab is used to determine their performance. In this paper, 25 attributes are analysed to detect the cybercriminal through various techniques such as Gaussian Clustering, K Means, Fuzzy C Means and Fuzzy Clustering. Some of the attributes are taken as varying attributes and some of the attributes are taken as the nonvarying attributes. The crime clusters and genuine clusters are determined. The reasons for choosing the various attributes and the various techniques are also given. The analysis is further strengthened by changing the number of attributes used in each technique i.e., 25 attributes, 15 attributes, 10 attributes and 5 attributes. The analysis is further continued by using 3/4 of total attributes, 1/2 of total attributes and 1/4 of total attributes. The efficiency and time complexity is determined. In the paper justification of a number of attributes selected in each technique is also given. The accuracy analysis is done by comparing with Gaussian Clustering with K Means Clustering technique, Fuzzy C Means, and user profile analysis. The attributes which are significantly contributing for identifying the criminals are determined in each technique.																	1868-5137	1868-5145															10.1007/s12652-019-01605-7		DEC 2019											
J								Exhaled breath monitoring during home ventilo-therapy in COPD patients by a new distributed tele-medicine system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart healthcare; Tele-medicine; Noninvasive mechanical ventilation; COPD breath analysis; Gas sensing	ELECTRONIC-NOSE; CONTROL CHARTS; TECHNOLOGIES; FRAMEWORK; DISEASE; WEB	Chronic obstructive pulmonary disease (COPD) is a major cause of morbidity and mortality in the world, and leads to a substantial burden on healthcare services. Effective and timely management of patients with COPD has been essential to alleviate COPD exacerbation, improve the quality of life, and consequently reduce the economic burden. To achieve this, we propose an innovative tele-medicine system based on Smart Breath Analyzer (SBA) device and devoted to the tele-monitoring of exhaled air in patients suffering chronic respiratory failure and home-assisted by mechanical ventilation. In this work the main characteristics of the developed tele-medicine system are presented together with preliminary pre-clinical test results. The preliminary tests demonstrated the effective possibility to enter the SBAs in medical practice in home-ventilo-therapy protocols by suitable interpretation of the monitored parameters from a physiological point of view. Thanks to a network composed by many SBA devices that send patients data to a remote server, the doctor can tele-monitor the status of patient in order to check any state of exacerbation of the disease and consequently improve quality of life in COPD.																	1868-5137	1868-5145															10.1007/s12652-019-01618-2		DEC 2019											
J								Anomaly process detection using negative selection algorithm and classification techniques	EVOLVING SYSTEMS										Artificial immune system; Negative selection algorithm; Anomaly detection; Intrusion detection; Machine learning		Artificial immune system is derived from the biological immune system. This system is an important method for generating detectors that include self-adaption, self- regulation and self-learning which have self/non-self-detection features. This method is used in anomaly process detection where the anomaly is non-self in the system. We present a new combining technique for anomaly process detection. This combined technique is a unification of both negative selection and classification algorithm. The main aim of the proposed techniques is to increase the accuracy in this system while decreasing its training time. In this research, CICIDS 2017 and NSL-KDD dataset with different sets of features and the same number of detectors are used. This paper presents a framework for detecting anomaly processes on a host base computer system which is established on the artificial immune system. We evaluate our technique using machine learning algorithms such as: logistic regression, random forest, decision tree and K-neighbors. Moreover, we use WEKA tool classification to perform a correlation based feature selection on the dataset.																	1868-6478	1868-6486															10.1007/s12530-019-09317-1		DEC 2019											
J								Parameter identification for symbolic regression using nonlinear least squares	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Symbolic regression; Parameter identification; Nonlinear least squares; Automatic differentiation	OPTIMIZATION; ALGORITHM; SELECTION	In this paper we analyze the effects of using nonlinear least squares for parameter identification of symbolic regression models and integrate it as local search mechanism in tree-based genetic programming. We employ the Levenberg-Marquardt algorithm for parameter optimization and calculate gradients via automatic differentiation. We provide examples where the parameter identification succeeds and fails and highlight its computational overhead. Using an extensive suite of symbolic regression benchmark problems we demonstrate the increased performance when incorporating nonlinear least squares within genetic programming. Our results are compared with recently published results obtained by several genetic programming variants and state of the art machine learning algorithms. Genetic programming with nonlinear least squares performs among the best on the defined benchmark suite and the local search can be easily integrated in different genetic programming algorithms as long as only differentiable functions are used within the models.																	1389-2576	1573-7632				SEP	2020	21	3			SI		471	501		10.1007/s10710-019-09371-3		DEC 2019											
J								Real-time face attributes recognition via HPGC: horizontal pyramid global convolution	JOURNAL OF REAL-TIME IMAGE PROCESSING										Face attribute recognition; HPGC; CNN; Accuracy and efficiency; Sigmoid cross entropy loss	FACIAL ATTRIBUTES	Recognizing face attributes in the wild is a challenging problem. With the development of embedded devices, smart phone and deep learning, face attributes recognition based on deep learning has raised increasing attention. Accuracy and efficiency are the two key-points in any application which uses these face attributes as an aid system. In most of the previous papers, multi-independency classifiers are proposed; and most of them just focus on accurate rate while neglecting efficiency. This paper proposes a horizontal pyramid global convolution (HPGC) module as feature mapping operator to extract more local information; designs a light-weight attribute convolution neural network (LACNN) combining with HPGC; and utilizes sigmoid cross entropy loss function for improving the accuracy and efficiency of the face attributes recognition model. Replacing full connection or global average pooling with the proposed HPGC module, we balance the accuracy performance and computation cost. As a result, we not only get high accuracy but also reduce the computational cost. Extensive experiments results on two widely used face attribute datasets, LFW and CelebA, demonstrate that our LACNN-HPGC framework achieves significantly improved efficiency compared with state-of-the-art lightweight models for face attributes recognition.																	1861-8200	1861-8219															10.1007/s11554-019-00932-4		DEC 2019											
J								Mining traffic congestion propagation patterns based on spatio-temporal co-location patterns	EVOLUTIONARY INTELLIGENCE										Spatio-temporal data mining; Traffic congestion propagation pattern; Influence		Traffic congestion is a direct reflection of the imbalance between supply and demand for a certain period of time. Owing to the complexity of traffic roads and the propagation of congestion, the evacuation of traffic congestion for local road sections alone cannot achieve significant results. Based on the measured data of traffic flow, this paper combines the topology of the road network and the existence time of congestion to judge the spatio-temporal correlation of congestion between road sections. We proposed a spatio-temporal co-location congestion pattern mining method to discover the orderly set of roads with congestion propagation in urban traffic, and measure its influence in congestion events. The proposed method not only reveals the process of congestion propagation but also uncovers the main propagation paths leading to the large-scale congestion. Finally, we experimented with the algorithm on the traffic dataset in Guiyang city. The experimental results reveal the traffic congestion rule in Guiyang City, including the prevalent co-occurrence of congestion propagation patterns and their influence in congestion events.																	1864-5909	1864-5917				JUN	2020	13	2			SI		221	233		10.1007/s12065-019-00332-4		DEC 2019											
J								Biceps Brachii Muscle Synergy and Target Reaching in a Virtual Environment	FRONTIERS IN NEUROROBOTICS										biceps brachii; muscle synergy; upper limb posture classification; target reaching; virtual cube; myoelectric prosthesis	REAL-TIME; CLASSIFICATION; HAND	A muscular synergy is a theory suggesting that the central nervous system uses few commands to activate a group of muscles to produce a given movement. Here, we investigate how a muscle synergy extracted from a single muscle can be at the origin of different signals which could facilitate the control of modern upper limb myoelectric prostheses with many degrees of freedom. Five pairs of surface electrodes were positioned across the biceps of 12 normal subjects and electromyographic (EMG) signals were collected while their upper limbs were in eight different static postures. Those signals were used to move, within a virtual cube, a small red sphere toward different targets. With three muscular synergies extracted from the five EMG signals, a classifier was trained to identify which synergy pattern was associated with a given static posture. Later, when a posture was recognized, the result was a displacement of a red sphere toward a corner of a virtual cube presented on a computer screen. The axes of the cube were assigned to the shoulder, elbow and wrist joint while each of its the corners was associated with a static posture. The goal for subjects was to reach, one at a time, the four targets positioned at different locations and heights in the virtual cube with different sequences of postures. The results of 12 normal subjects indicate that with the muscular synergies of the biceps brachii, it was possible, but not easy for an untrained person, to reach a target on each trial. Thus, as a proof of concept, we show that features of the biceps muscular synergy have the potential to facilitate the control of upper limb myoelectric prostheses. To our knowledge, this has never been shown before.																	1662-5218					DEC 10	2019	13								100	10.3389/fnbot.2019.00100													
J								Constrained Deep Q-Learning Gradually Approaching Ordinary Q-Learning	FRONTIERS IN NEUROROBOTICS										deep reinforcement learning; deep Q network; regularization; learning stabilization; target network; constrained reinforcement learning	REINFORCEMENT; GAME; GO	A deep Q network (DQN) (Mnih et al., 2013) is an extension of Q learning, which is a typical deep reinforcement learning method. In DQN, a Q function expresses all action values under all states, and it is approximated using a convolutional neural network. Using the approximated Q function, an optimal policy can be derived. In DQN, a target network, which calculates a target value and is updated by the Q function at regular intervals, is introduced to stabilize the learning process. A less frequent updates of the target network would result in a more stable learning process. However, because the target value is not propagated unless the target network is updated, DQN usually requires a large number of samples. In this study, we proposed Constrained DQN that uses the difference between the outputs of the Q function and the target network as a constraint on the target value. Constrained DQN updates parameters conservatively when the difference between the outputs of the Q function and the target network is large, and it updates them aggressively when this difference is small. In the proposed method, as learning progresses, the number of times that the constraints are activated decreases. Consequently, the update method gradually approaches conventional Q learning. We found that Constrained DQN converges with a smaller training dataset than in the case of DQN and that it is robust against changes in the update frequency of the target network and settings of a certain parameter of the optimizer. Although Constrained DQN alone does not show better performance in comparison to integrated approaches nor distributed methods, experimental results show that Constrained DQN can be used as an additional components to those methods.																	1662-5218					DEC 10	2019	13								103	10.3389/fnbot.2019.00103													
J								Music creation and emotional recognition using neural network analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neural network; Music recognition; Feature extraction; MIDI file	CLASSIFICATION	In order to make use of computer technology to minimize the artificial factors in the composing process and achieve the goal of automatic composing, the algorithm is used to control the generation of musical notes sequence, and then compose the music melody, and finally get the complete music score. Artificial intelligence composing with audio as the research object can make computer automatically generate the arrangement and combination of music fragments to generate new music audio. The research results show that a new automatic synthetic music algorithm based on neural network is more practical than the traditional music-based music composition method. The realization process of musical note recognition by neural network is given. It can be seen from the simulation and test that the neural network-based note recognition method has more reliable performance.																	1868-5137	1868-5145															10.1007/s12652-019-01614-6		DEC 2019											
J								Context-Category Specific sequence aware Point-Of-Interest Recommender System with Multi-Gated Recurrent Unit	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										POI Recommender System; Context aware recommendation; Gated Recurrent Unit (GRU); Location Based Social Networks (LBSNs); Recurrent Neural Network (RNN)		Point-Of-Interest (POI) Recommender Systems (RSs) have huge importance in Location Based Social Networks (LBSNs) because of its effectiveness in assisting users to explore personalized locations. It also assists the LBSN providers to increase their revenue through scrutinized advertisements or services according to specific locations. For the effectiveness and accuracy of POI RSs, so many additional information such as Transition Contexts (e.g., geographical distance, time interval), Dynamic Contexts (e.g., time of the day, companion, season), and Static Contexts (e.g., POI type, features) have to be integrated along with the check-in data. The high impact of this additional information distinguishes POI recommendation approaches from other RS approaches. To address the challenges of the varying influences of the user's current contexts and the transition contexts (arises from users past task to current task) on recommendation, a Gated Recurrent Unit (GRU) architecture is proposed. It is capable of handling the effect of each category of contexts separately. The main part of the proposed Context-Category Specific Sequence Aware POI RS (CCS-POI-RS) is a Multi-GRU (MGRU), which has two added gates for handling the influences of both dynamic contexts and transition contexts. Experiments on Gowalla and Foursquare check-in data set reveal the significance of MGRU architecture through the comparison with the other state of the art GRU architectures.																	1868-5137	1868-5145															10.1007/s12652-019-01583-w		DEC 2019											
J								Best features based intrusion detection system by RBM model for detecting DDoS in cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Classification; DDoS attack; Deep learning; Feature selection; Intrusion	FEATURE-SELECTION; ANOMALY DETECTION; MITIGATION; ALGORITHM	In cloud environment, Denial-of-Service (DoS) and Distributed Denial-of-Service (DDoS) attacks poses a major challenge to the accessibility. To resolve this issue, an intrusion detection system (IDS) can be employed as a security procedure which operates on the network layer. The conventional IDS in cloud platform result to poor detection accuracy with high computational complexity. Keeping these issues in mind, in this paper, we introduce an efficient feature subset selection based classification model for the identification of DDoS attack. To detect the DDoS attack in IDS, best feature sets are selected with maximum detection by the use of Random Harmony Search (RHS) optimization model. Once the features are selected, a Deep learning based classifier model using of Restricted Boltzmann Machines (RBM) is applied to detect the DDoS. For improving the detection rate of DDoS attack, a set of seven extra layers are included among the visible and the hidden layers of the RBM. Here, precise results are attained by optimizing the hyper parameters of the presented deep RBM model. The probability distribution of the visible layer in RBM model undergoes replacement with a Gaussian distribution. For experimentation, RHS-RBM model is tested against KDD ' 99 dataset. The experimental results showed that the RHS-RBM model achieves maximum sensitivity of 99.88, specificity of 99.96, accuracy of 99.92, F-score of 99.93 and kappa value of 99.84. These obtained values of RHS-RBM model are found to be better when compared to the RBM model without the use of RHS algorithm.																	1868-5137	1868-5145															10.1007/s12652-019-01611-9		DEC 2019											
J								Robot algorithm based on neural network and intelligent predictive control	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neural network; Predictive control; Robot algorithm; Robustness		With the improvement of industrial control requirements and the development of control theory and computer technology, it is more and more urgent to study the intelligent predictive control algorithm with good control effect, strong robustness and suitable for more complicated industrial processes. This paper proposes a novel intelligent predictive control scheme that uses a neural network intelligent predictive controller to control the force/position of the robot. The controller of this neural network can arbitrarily approach the uncertain object of the industrial robot without knowing the exact structure of the system. At the same time, due to the addition of intelligent predictive control, the system is easy to calculate online and the quality of control is improved. It can be seen from the simulation results of the robot that the traditional PID can not solve the uncertain object well. With the controller designed in this paper, the robustness and rapidity of the system are improved to some extent, and good control accuracy and control effects are achieved.																	1868-5137	1868-5145															10.1007/s12652-019-01622-6		DEC 2019											
J								Making simulation results reproducible-Survey, guidelines, and examples based on Gradle and Docker	PEERJ COMPUTER SCIENCE										In-silico research; Reproducibility; Simulation		This article addresses two research questions related to reproducibility within the context of research related to computer science. First, a survey on reproducibility addressed to researchers in the academic and private sectors is described and evaluated. The survey indicates a strong need for open and easily accessible results, in particular, reproducing an experiment should not require too much effort. The results of the survey are then used to formulate guidelines for making research results reproducible. In addition, this article explores four approaches based on software tools that could bring forward reproducibility in research results. After a general analysis of tools, three examples are further investigated based on actual research projects which are used to evaluate previously introduced tools. Results indicate that the evaluated tools contribute well to making simulation results reproducible but due to conflicting requirements, none of the presented solutions fulfills all intended goals perfectly.																	2376-5992					DEC 9	2019									e240	10.7717/peerj-cs.240													
J								Nearest labelset using double distances for multi-lable classification	PEERJ COMPUTER SCIENCE										Multi-label classification; Label correlations; Nearest neighbor	ENSEMBLES	Multi-label classification is a type of supervised learning where an instance may belong to multiple labels simultaneously. Predicting each label independently has been criticized for not exploiting any correlation between labels. In this article we propose a novel approach, Nearest Labelset using Double Distances (NLDD), that predicts the labelset observed in the training data that minimizes a weighted sum of the distances in both the feature space and the label space to the new instance. The weights specify the relative tradeoff between the two distances. The weights are estimated from a binomial regression of the number of misdassified labels as a function of the two distances. Model parameters are estimated by maximum likelihood. NLDD only considers labelsets observed in the training data, thus implicitly taking into account label dependencies. Experiments on benchmark multi-label data sets show that the proposed method on average outperforms other well-known approaches in terms of 0/1 loss, and multilabel accuracy and ranks second on the F-measure (after a method called ECC) and on Hamming loss (after a method called RF-PCT).																	2376-5992					DEC 9	2019									e242	10.7717/peerj-cs.242													
J								SSN: Learning Sparse Switchable Normalization via SparsestMax	INTERNATIONAL JOURNAL OF COMPUTER VISION										Deep learning; Normalization; Classification; Optimization		Normalization method deals with parameters training of convolution neural networks (CNNs) in which there are often multiple convolution layers. Despite the fact that layers in CNN are not homogeneous in the role they play at representing a prediction function, existing works often employ identical normalizer in different layers, making performance away from idealism. To tackle this problem and further boost performance, a recently-proposed switchable normalization (SN) provides a new perspective for deep learning: it learns to select different normalizers for different convolution layers of a ConvNet. However, SN uses softmax function to learn importance ratios to combine normalizers, not only leading to redundant computations compared to a single normalizer but also making model less interpretable. This work addresses this issue by presenting sparse switchable normalization (SSN) where the importance ratios are constrained to be sparse. Unlike l(1) and l(0) regularizations that impose difficulties in tuning layer-wise regularization coefficients, we turn this sparse-constrained optimization problem into feed-forward computation by proposing SparsestMax, which is a sparse version of softmax. SSN has several appealing properties. (1) It inherits all benefits from SN such as applicability in various tasks and robustness to a wide range of batch sizes. (2) It is guaranteed to select only one normalizer for each normalization layer, avoiding redundant computations and improving interpretability of normalizer selection. (3) SSN can be transferred to various tasks in an end-to-end manner. Extensive experiments show that SSN outperforms its counterparts on various challenging benchmarks such as ImageNet, COCO, Cityscapes, ADE20K, Kinetics and MegaFace. Models and code are available at https://github.com/switchablenorms/Sparse_SwitchNorm.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2107	2125		10.1007/s11263-019-01269-y		DEC 2019											
J								Mood classification through physiological parameters	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mood recognition; Machine learning; Social robotics	RECOGNITION; EMOTION; EXPRESSION; FRAMEWORK	Future smart agents, like robots, should produce personalized behaviours based on user emotions and moods to fit more in ordinary users' activities. Besides, the emotions are also linked to human cognitive systems, thus their monitoring could be extremely useful in the case of neurodegenerative diseases such as dementia and Alzheimer. Literature works propose the use of music tracks and videos to stimulate emotions, and cameras to recorder the evoked reactions in human beings. However, these approaches may not be effective in everyday life, due to camera obstructions and different types of stimulation which can be related also with the interaction with other human beings. In this work, we investigate the Electrocardiogram, the ElectroDermal Activity and the Brain Activity signals as main informative channels, acquired through a wireless wearable sensor network. An experimental methodology was built to induce three different emotional states through social interaction. Collected data were classified with three supervised machine learning approaches with different kernels (Support Vector Machine, Decision Tree and k-nearest neighbour) considering the valence dimension and a combination of valence and arousal dimension evoked during the interaction. 34 healthy young participants were involved in the study and a total of 239 instances were analyzed. The supervised algorithms achieve an accuracy of 0.877 in the best case.																	1868-5137	1868-5145															10.1007/s12652-019-01595-6		DEC 2019											
J								A multi-agent architecture for mobile sensing systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile sensing; Multi-agent systems; Human-agent societies		Mobile sensing systems based on smartphones, connected vehicles and integrated sensors on new mobile devices have become an important alternative for the development of intelligent services in large urban environments. Massive data collection and its real-time analysis are essential for big cities to move towards energy efficiency, sustainable mobility, protection of the environment and economic sustainability. Current research and applications are mainly focused on the use of individual devices and the analysis of information on a single domain (e.g. activity recognition). However, it is still necessary to provide solutions for social problems based on smart mobile devices connected to the city. In this paper, we present an architecture for mobile sensing systems in large cities based on the intelligent agent paradigm and multi-agent systems. The presented platform provides support for multi-purpose machine learning services, implementing expert learning agents in each domain where the system collects data. Furthermore, the main challenges in mobile sensing systems such as scalability in crowded environments, handling of a large amount of data and the increasing appearance of sensing devices are addressed by the architecture due to the agent paradigm and multi-agent systems suit these demands naturally.																	1868-5137	1868-5145															10.1007/s12652-019-01608-4		DEC 2019											
J								BP neural network-based ABEP performance prediction for mobile Internet of Things communication systems	NEURAL COMPUTING & APPLICATIONS										Mobile Internet of Things; Mobile cooperative communication; Average bit error probability; Performance prediction; BP neural network	SECRECY OUTAGE; MODEL; IOT	Wireless communications play an important role in the mobile Internet of Things (IoT). For practical mobile communication systems,N-Nakagami fading channels are a better characterization thanN-Rayleigh and 2-Rayleigh fading channels. The average bit error probability (ABEP) is an important factor in the performance evaluation of mobile IoT systems. In this paper, cooperative communications is used to enhance the ABEP performance of mobile IoT systems using selection combining. To compute the ABEP, the signal-to-noise ratios (SNRs) of the direct link and end-to-end link are considered. The probability density function (PDF) of these SNRs is derived, and this is used to derive the cumulative distribution function, which is used to derive closed-form ABEP expressions. The theoretical results are confirmed by Monte-Carlo simulation. The impact of fading and other parameters on the ABEP performance is examined. These results can be used to evaluate the performance of complex environments such as mobile IoT and other communication systems. To support active complex event processing in mobile IoT, it is important to predict the ABEP performance. Thus, a back-propagation (BP) neural network-based ABEP performance prediction algorithm is proposed. We use the theoretical results to generate training data. We test the extreme learning machine (ELM), linear regression (LR), support vector machine (SVM), and BP neural network methods. Compared to LR, SVM, and ELM methods, the simulation results verify that our method can consistently achieve higher ABEP performance prediction results.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16025	16041		10.1007/s00521-019-04604-z		DEC 2019											
J								Real-time moving human detection using HOG and Fourier descriptor based on CUDA implementation	JOURNAL OF REAL-TIME IMAGE PROCESSING										GPU; CPU; CUDA; HOG; Fourier descriptor; GMM; Moving human detection; Real-time	HUMAN RECOGNITION; NETWORK; ALGORITHM; MOTION; SYSTEM; MODEL	Real-time applications of image and video processing algorithms have seen explosive growth in number and complexity over the past decade driven by consumer, scientific and defense applications exploiting inexpensive digital video cameras and networked computing device. This growth has opened up different alternatives to greatly enhance the surveillance capabilities using new architectures and parallelization strategies developed due to the increased accessibility of multicore, multi-threaded processors along with general purpose graphics processing units (GPUs). In this paper, we present a new implementation of a moving human detection algorithm on GPU based on the programming language CUDA. In our approach, the moving object is extracted by background subtraction based on the GMM (Gaussian Mixture Model) on GPU. Then, two complementary features are extracted for moving object classification. They are contour-based description: FD or Fourier Descriptor and region-based description: HOG or Histogram of Oriented Gradient. Both descriptors will then be effectively integrated to SVM (Support Vector Machine), which is able to provide the posterior probability, to achieve better performance. The implementation of such algorithm on a GPU allows a great performance in terms of execution time since it is 19 times faster than that on a CPU. Experimental results show also that the proposed approach outperforms some existing techniques and can detect pedestrians in real-time effectively.																	1861-8200	1861-8219															10.1007/s11554-019-00935-1		DEC 2019											
J								Pareto-optimal cost optimization for large scale cloud systems using joint allocation of resources	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										VM placement; Multi-objective optimization; Pareto-optimality; Cost optimization; Resource allocation	PLACEMENT; SDN; ALGORITHM; DESIGN	Optimal resource allocation in cloud systems is NP-hard due to the involvement of several conflicting objectives and unpredictable cloud traffic. To improve user satisfaction and resource utilization while minimizing end-user cost, the joint allocation of cloud resources is inevitable. In this work, we model end-user cost in cloud as the optimization objective using bandwidth and compute allocation as the decision variables. To solve the joint Virtual Machine Placement (VMP) problem we propose a single point, greedy, software-defined network (SDN)-based solution that minimizes end-user cost by making certain changes to the fat-tree datacenter architecture. Mathematically, we show that the overall objective function is convex hence solving it using a weighted-sum greedy approach will induce solutions that are Pareto-optimal. Experimental evaluations confirm up to 15% reduction in the response time and up to 14% increase in the efficiency of resources. To analyse the risks involved in deploying delay-sensitive applications over cloud and to show the effects of resource allocation approaches, we perform a risk analysis of delay-sensitivity in cloud using real-time CVD detection. The results confirm the reduced response time due to the proposed approach while maintaining the efficiency of detection.																	1868-5137	1868-5145															10.1007/s12652-019-01601-x		DEC 2019											
J								Distracted driving recognition method based on deep convolutional neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distracted driving recognition; Convolutional neural network; CNN; Deep learning; Machine learning		In recent years, traffic accidents caused by the distracted driving have been on the rise with the popularization of smart phones. How to correctly identify whether the driver is in a distracted driving state and to provide the necessary warnings for the driver to avoid potential safety risks has become one of the most concerned issues. In this paper, a distracted driving recognition method based on deep convolutional neural network is proposed for the driving image data captured by the in-vehicle camera. This method uses the PCA technology to whiten the driving image, which reduces the redundancy and correlation of the pixel matrix. At the same time, a multi-layer CNN network is constructed in the model and the key parameters of the input layer, convolution layer, pooling layer, fully connected layer and output layer are optimized as well. The results of experimental analysis show that the accuracy of the proposed method can reach 97.31%, which is higher than that of the existing machine learning algorithms. Therefore, the proposed method is effective in improving the accuracy of distracted driving recognition.																	1868-5137	1868-5145															10.1007/s12652-019-01597-4		DEC 2019											
J								A survey of semi- and weakly supervised semantic segmentation of images	ARTIFICIAL INTELLIGENCE REVIEW										Semi-supervised; Weakly supervised; Semantic segmentation; Review	DEEP; SEARCH	Image semantic segmentation is one of the most important tasks in the field of computer vision, and it has made great progress in many applications. Many fully supervised deep learning models are designed to implement complex semantic segmentation tasks and the experimental results are remarkable. However, the acquisition of pixel-level labels in fully supervised learning is time consuming and laborious, semi-supervised and weakly supervised learning is gradually replacing fully supervised learning, thus achieving good results at a lower cost. Based on the commonly used models such as convolutional neural networks, fully convolutional networks, generative adversarial networks, this paper focuses on the core methods and reviews the semi- and weakly supervised semantic segmentation models in recent years. In the following chapters, existing evaluations and data sets are summarized in details and the experimental results are analyzed according to the data set. The last part of the paper is an objective summary. In addition, it points out the possible direction of research and inspiring suggestions for future work.																	0269-2821	1573-7462				AUG	2020	53	6					4259	4288		10.1007/s10462-019-09792-7		DEC 2019											
J								DSP-based image real-time dehazing optimization for improved dark-channel prior algorithm	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image dehazing; Dark-channel prior; Software pipeline; Single instruction multiple data (SIMD); Intrinsic instructions	CONTRAST ENHANCEMENT	To solve the problem of non-real-time processing of image dehazing using traditional dark-channel prior algorithm, this work studies image real-time penetrating fog optimization technologies based on digital signal processor (DSP) devices. Using jointed optimization mechanism between algorithm and device, we can achieve real-time processing. During algorithm optimization, mean filter characterized low computation substitutes the guided filter which is the most complex in dark-channel algorithm for dehazing. In optimization of image processing task under the embedded device, we empirically construct two-step optimization strategy for raising speed of processing. Thereupon, the awful division calculation for DSP device is achieved approximately by multiplication after the reciprocal operation. We utilize the specified template which is considerably designed to realize mean filter. Thus, the division factor in the template can be calculated innovatively via shift instructions featured on DSP. The experimental results show that the optimization solution provided has realized real-time image dehazing processing for standard-definition and high-definition at frame rate of 25 fps over C6748 pure DSP device featured 456 MHz clock, at the same time the effect of penetrating fog is not remarkably degraded. The optimization methods or ideas can easily be transplanted to similar platform.																	1861-8200	1861-8219				OCT	2020	17	5					1675	1684		10.1007/s11554-019-00933-3		DEC 2019											
J								3D texture-based face recognition system using fine-tuned deep residual networks	PEERJ COMPUTER SCIENCE										3D textures; Face recognition system; Histogram of oriented gradients features; Deep learning; Residual neural networks; Fine-tuning; Tensorboard		As the technology for 3D photography has developed rapidly in recent years, an enormous amount of 3D images has been produced, one of the directions of research for which is face recognition. Improving the accuracy of a number of data is crucial in 3D face recognition problems. Traditional machine learning methods can be used to recognize 3D faces, but the face recognition rate has declined rapidly with the increasing number of 3D images. As a result, classifying large amounts of 3D image data is time-consuming, expensive, and inefficient. The deep learning methods have become the focus of attention in the 3D face recognition research. In our experiment, the endto-end face recognition system based on 3D face texture is proposed, combining the geometric invariants, histogram of oriented gradients and the fine-tuned residual neural networks. The research shows that when the performance is evaluated by the FRGC-v2 dataset, as the fine-tuned ResNet deep neural network layers are increased, the best Top-1 accuracy is up to 98.26% and the Top-2 accuracy is 99.40%. The framework proposed costs less iterations than traditional methods. The analysis suggests that a large number of 3D face data by the proposed recognition framework could significantly improve recognition decisions in realistic 3D face scenarios.																	2376-5992					DEC 2	2019									e236	10.7717/peerj-cs.236													
J								Analysis Dictionary Learning Based Classification: Structure for Robustness	IEEE TRANSACTIONS ON IMAGE PROCESSING										Discriminate analysis dictionary learning; distributed analysis dictionary learning; structured mapping; supervised learning	SPARSE REPRESENTATION; FACE RECOGNITION; ANALYSIS MODEL; ALGORITHM	A discriminative structured analysis dictionary is proposed for the classification task. A structure of the union of subspaces (UoS) is integrated into the conventional analysis dictionary learning to enhance the capability of discrimination. A simple classifier is also simultaneously included into the formulated function to ensure a more complete consistent classification. The solution of the algorithm is efficiently obtained by the linearized alternating direction method of multipliers. Moreover, a distributed structured analysis dictionary learning is also presented to address large-scale datasets. It can group(class-) independently train the structured analysis dictionaries by different machines/cores/threads, and therefore avoid a high computational cost. A consensus structured analysis dictionary and a global classifier are jointly learned in the distributed approach to safeguard the discriminative power and the efficiency of classification. Experiments demonstrate that our method achieves a comparable or better performance than the state-of-the-art algorithms in a variety of visual classification tasks. In addition, the training and testing computational complexity are also greatly reduced.																	1057-7149	1941-0042				DEC	2019	28	12					6035	6046		10.1109/TIP.2019.2919409													
J								Image Recognition by Predicted User Click Feature With Multidomain Multitask Transfer Deep Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image recognition; click prediction; transfer deep learning; multitask learning; word embedding	SUPER RESOLUTION; VIDEO SUPERRESOLUTION; SPARSE REPRESENTATION; SELF-SIMILARITY; REGULARIZATION; RESTORATION; ENHANCEMENT	The click feature of an image, defined as a user click count vector based on click data, has been demonstrated to be effective for reducing the semantic gap for image recognition. Unfortunately, most of the traditional image recognition datasets do not contain click data. To address this problem, researchers have begun to develop a click prediction model using assistant datasets containing click information and have adapted this predictor to a common click-free dataset for different tasks. This method can be customized to our problem, but it has two main limitations: 1) the predicted click feature often performs badly in the recognition task since the prediction model is constructed independently of the subsequent recognition problem and 2) transferring the predictor from one dataset to another is challenging due to the large cross-domain diversity. In this paper, we devise a multitask and multidomain deep network with varied modals (MTMDD-VM) to formulate image recognition and click prediction tasks in a unified framework. Datasets with and without click information are integrated in the training. Furthermore, a nonlinear word embedding with a position-sensitive loss function is designed to discover the visual click correlation. We evaluate the proposed method on three public dog breed image datasets, and we utilize the Clickture-Dog dataset as the auxiliary dataset that provides click data. The experimental results show that: 1) the nonlinear word embedding and position-sensitive loss function largely enhance the predicted click feature in the recognition task, realizing a 32% improvement in accuracy; 2) the multitask learning framework improves accuracies in both image recognition and click prediction; and 3) the unified training using the combined dataset with and without click data further improves the performance. Compared with the state-of-the-art methods, the proposed approach not only performs much better in accuracy but also achieves good scalability and one-shot learning ability.																	1057-7149	1941-0042				DEC	2019	28	12					6047	6076		10.1109/TIP.2019.2921861													
J								Foreground Gating and Background Refining Network for Surveillance Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; background subtraction; pairwise non-local operation; misalignment; surveillance video	NEURAL-NETWORK; ROBUST-PCA; TRACKING; IMAGE; VIDEO	Detecting objects in surveillance videos is an important problem due to its wide applications in traffic control and public security. Existing methods tend to face performance degradation because of false positive or misalignment problems. We propose a novel framework, namely, Foreground Gating and Background Refining Network (FG-BR Net), for surveillance object detection (SOD). To reduce false positives in background regions, which is a critical problem in SOD, we introduce a new module that first subtracts the background of a video sequence and then generates high-quality region proposals. Unlike previous background subtraction methods that may wrongly remove the static foreground objects in a frame, a feedback connection from detection results to background subtraction process is proposed in our model to distill both static and moving objects in surveillance videos. Furthermore, we introduce another module, namely, the background refining stage, to refine the detection results with more accurate localizations. Pairwise non-local operations are adopted to cope with the misalignments between the features of original and background frames. Extensive experiments on real-world traffic surveillance benchmarks demonstrate the competitive performance of the proposed FG-BR Net. In particular, FG-BR Net ranks on the top among all the methods on hard and sunny subsets of the UA-DETRAC detection dataset, without any bells and whistles.																	1057-7149	1941-0042				DEC	2019	28	12					6077	6090		10.1109/TIP.2019.2922095													
J								Incorporation of Structural Tensor and Driving Force Into Log-Demons for Large-Deformation Image Registration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image registration; tensor; driving force; Log-Demons algorithm; optimization	DIFFEOMORPHIC REGISTRATION	Large-deformation image registration is important in theory and application in computer vision, but is a difficult task for non-rigid registration methods. In this paper, we propose a structural Tensor and Driving force-based Log-Demons algorithm for it, named TDLog-Demons for short. The structural tensor of an image is proposed to obtain a highly accurate deformation field. The driving force is proposed to solve the registration issue of large-deformation that often causes Log-Demons to trap into local minima. It is defined as a point correspondence obtained via multisupport-region-order-based gradient histogram descriptor matching on image's boundary points. It is integrated into an exponentially decreasing form with the velocity field of Log-Demons to move the points accurately and to speed up a registration process. Consequently, the driving force-based Log-Demons can well deal with large-deformation image registration. Extensive experiments demonstrate that the TDLog-Demons not only captures large deformations at a high accuracy but also yields a smooth deformation.																	1057-7149	1941-0042				DEC	2019	28	12					6091	6102		10.1109/TIP.2019.2924168													
J								Locality Preserving Joint Transfer for Domain Adaptation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Domain adaptation; transfer learning; landmark selection; subspace learning		Domain adaptation aims to leverage knowledge from a well-labeled source domain to a poorly labeled target domain. A majority of existing works transfer the knowledge at either feature level or sample level. Recent studies reveal that both of the paradigms are essentially important, and optimizing one of them can reinforce the other. Inspired by this, we propose a novel approach to jointly exploit feature adaptation with distribution matching and sample adaptation with landmark selection. During the knowledge transfer, we also take the local consistency between the samples into consideration so that the manifold structures of samples can be preserved. At last, we deploy label propagation to predict the categories of new instances. Notably, our approach is suitable for both homogeneous- and heterogeneous-domain adaptations by learning domain-specific projections. Extensive experiments on five open benchmarks, which consist of both standard and large-scale datasets, verify that our approach can significantly outperform not only conventional approaches but also end-to-end deep models. The experiments also demonstrate that we can leverage handcrafted features to promote the accuracy on deep features by heterogeneous adaptation.																	1057-7149	1941-0042				DEC	2019	28	12					6103	6115		10.1109/TIP.2019.2924174													
J								Piecewise Classifier Mappings: Learning Fine-Grained Learners for Novel Categories With Few Examples	IEEE TRANSACTIONS ON IMAGE PROCESSING										Computer vision; fine-grained image recognition; few-shot learning; learning to learn		Humans are capable of learning a new fine-grained concept with very little supervision, e.g., few exemplary images for a species of bird, yet our best deep learning systems need hundreds or thousands of labeled examples. In this paper, we try to reduce this gap by studying the fine-grained image recognition problem in a challenging few-shot learning setting, termed few-shot fine-grained recognition (FSFG). The task of FSFG requires the learning systems to build classifiers for the novel fine-grained categories from few examples (only one or less than five). To solve this problem, we propose an end-to-end trainable deep network, which is inspired by the state-of-the-art fine-grained recognition model and is tailored for the FSFG task. Specifically, our network consists of a bilinear feature learning module and a classifier mapping module: while the former encodes the discriminative information of an exemplar image into a feature vector, the latter maps the intermediate feature into the decision boundary of the novel category. The key novelty of our model is a "piecewise mappings" function in the classifier mapping module, which generates the decision boundary via learning a set of more attainable sub-classifiers in a more parameter-economic way. We learn the exemplar-to-classifier mapping based on an auxiliary dataset in a meta-learning fashion, which is expected to be able to generalize to novel categories. By conducting comprehensive experiments on three fine-grained datasets, we demonstrate that the proposed method achieves superior performance over the competing baselines.																	1057-7149	1941-0042				DEC	2019	28	12					6116	6125		10.1109/TIP.2019.2924811													
J								Attention-Based Pedestrian Attribute Analysis	IEEE TRANSACTIONS ON IMAGE PROCESSING										Pedestrian attribute analysis; attention mechanism; pedestrian parsing	NETWORK	Recognizing the pedestrian attributes in surveillance scenes is an inherently challenging task, especially for the pedestrian images with large pose variations, complex backgrounds, and various camera viewing angles. To select important and discriminative regions or pixels against the variations, three attention mechanisms are proposed, including parsing attention, label attention, and spatial attention. Those attentions aim at accessing effective information by considering problems from different perspectives. To be specific, the parsing attention extracts discriminative features by learning not only where to turn attention to but also how to aggregate features from different semantic regions of human bodies, e.g., head and upper body. The label attention aims at targetedly collecting the discriminative features for each attribute. Different from the parsing and label attention mechanisms, the spatial attention considers the problem from a global perspective, aiming at selecting several important and discriminative image regions or pixels for all attributes. Then, we propose a joint learning framework formulated in a multi-task-like way with these three attention mechanisms learned concurrently to extract complementary and correlated features. This joint learning framework is named Joint Learning of Parsing attention, Label attention, and Spatial attention for Pedestrian Attributes Analysis (JLPLS-PAA, for short). Extensive comparative evaluations conducted on multiple large-scale benchmarks, including PA-100K, RAP, PETA, Market-1501, and Duke attribute datasets, further demonstrate the effectiveness of the proposed JLPLS-PAA framework for pedestrian attribute analysis.																	1057-7149	1941-0042				DEC	2019	28	12					6126	6140		10.1109/TIP.2019.2919199													
J								A Deep Information Sharing Network for Multi-Contrast Compressed Sensing MRI Reconstruction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Compressed sensing; multi-contrast MRI reconstruction; deep neural networks	IMAGE-RECONSTRUCTION; SEGMENTATION	Compressed sensing (CS) theory can accelerate multi-contrast magnetic resonance imaging (MRI) by sampling fewer measurements within each contrast. However, conventional optimization-based reconstruction models suffer several limitations, including a strict assumption of shared sparse support, time-consuming optimization, and "shallow" models with difficulties in encoding the patterns contained in massive MRI data. In this paper, we propose the first deep learning model for multi-contrast CS-MRI reconstruction. We achieve information sharing through feature sharing units, which significantly reduces the number of model parameters. The feature sharing unit combines with a data fidelity unit to comprise an inference block, which are then cascaded with dense connections, allowing for efficient information transmission across different depths of the network. Experiments on various multi-contrast MRI datasets show that the proposed model outperforms both state-of-the-art single-contrast and multi-contrast MRI methods in accuracy and efficiency. We demonstrate that improved reconstruction quality can bring benefits to subsequent medical image analysis. Furthermore, the robustness of the proposed model to misregistration shows its potential in real MRI applications.																	1057-7149	1941-0042				DEC	2019	28	12					6141	6153		10.1109/TIP.2019.2925288													
J								Vessel Optimal Transport for Automated Alignment of Retinal Fundus Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Retinal image registration; image alignment; blood vessel detection; optimal transport	REGISTRATION; ROBUST; INFORMATION; EXTRACTION; ALGORITHM; FEATURES; STEREO; PAIRS	Optimal transport has emerged as a promising and useful tool for supporting modern image processing applications such as medical imaging and scientific visualization. Indeed, the optimal transport theory enables great flexibility in modeling problems related to image registration, as different optimization resources can be successfully used as well as the choice of suitable matching models to align the images. In this paper, we introduce an automated framework for fundus image registration which unifies optimal transport theory, image processing tools, and graph matching schemes into a functional and concise methodology. Given two ocular fundus images, we construct representative graphs which embed in their structures spatial and topological information from the eye's blood vessels. The graphs produced are then used as input by our optimal transport model in order to establish a correspondence between their sets of nodes. Finally, geometric transformations are performed between the images so as to accomplish the registration task properly. Our formulation relies on the solid mathematical foundation of optimal transport as a constrained optimization problem, being also robust when dealing with outliers created during the matching stage. We demonstrate the accuracy and effectiveness of the present framework throughout a comprehensive set of qualitative and quantitative comparisons against several influential state-of-the-art methods on various fundus image databases.																	1057-7149	1941-0042				DEC	2019	28	12					6154	6168		10.1109/TIP.2019.2925287													
J								Compact and Low-Complexity Binary Feature Descriptor and Fisher Vectors for Video Analytics	IEEE TRANSACTIONS ON IMAGE PROCESSING										Binary features; video analysis; fisher vectors; CNN	TEXTURE CLASSIFICATION; ACTION RECOGNITION; SCALE; SURVEILLANCE; DENSE	In this paper, we propose a compact and low-complexity binary feature descriptor for video analytics. Our binary descriptor encodes the motion information of a spatio-temporal support region into a low-dimensional binary string. The descriptor is based on a binning strategy and a construction that binarizes separately the horizontal and vertical motion components of the spatio-temporal support region. We pair our descriptor with a novel Fisher Vector (FV) scheme for binary data to project a set of binary features into a fixed length vector in order to evaluate the similarity between feature sets. We test the effectiveness of our binary feature descriptor with FVs for action recognition, which is one of the most challenging tasks in computer vision, as well as gait recognition and animal behavior clustering. Several experiments on the KTH, UCF50, UCF101, CASIA-B, and TIGdog datasets show that the proposed binary feature descriptor outperforms the state-of-the-art feature descriptors in terms of computational time and memory and storage requirements. When paired with FVs, the proposed feature descriptor attains a very competitive performance, outperforming several state-of-the-art feature descriptors and some methods based on convolutional neural networks.																	1057-7149	1941-0042				DEC	2019	28	12					6169	6184		10.1109/TIP.2019.2922826													
J								Mirror, Mirror, on the Wall, Who's Got the Clearest Image of Them All?-A Tailored Approach to Single Image Reflection Removal	IEEE TRANSACTIONS ON IMAGE PROCESSING										Reflection suppression; image enhancement; optical reflection	PHYSICALLY-BASED APPROACH; TRANSPARENT LAYERS; SEPARATION	Removing reflection artefacts from a single image is a problem of both theoretical and practical interest, which still presents challenges because of the massively ill-posed nature of the problem. In this paper, we propose a technique based on a novel optimization problem. First, we introduce a simple user interaction scheme, which helps minimize information loss in the reflection-free regions. Second, we introduce an H-2 fidelity term, which preserves fine detail while enforcing the global color similarity. We show that this combination allows us to mitigate the shortcomings in structure and color preservation, which presents some of the most prominent drawbacks in the existing methods for reflection removal. We demonstrate, through numerical and visual experiments, that our method is able to outperform the state-of-the-art model-based methods and compete with recent deep-learning approaches.																	1057-7149	1941-0042				DEC	2019	28	12					6185	6197		10.1109/TIP.2019.2923559													
J								Multiscale Structure Tensor for Improved Feature Extraction and Image Regularization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image restoration; structure tensor; variable exponent; anisotropic diffusion	EDGE-DETECTION; VARIABLE EXPONENT; GRADIENT; FUNCTIONALS; JUNCTION	Regularization methods are used widely in image selective smoothing and edge preserving restoration of noisy images. Traditional methods utilize image gradients within regularization function for controlling the smoothing and can produce artifacts when noise levels are higher. In this paper, we consider a robust image adaptive exponent driven regularization for filtering noisy images with salient feature preservation. Our spatially adaptive variable exponent function depends on a continuous switch based on the eigenvalues of structure tensor which identifies noisy edges, and corners with higher accuracy. Structure tensor eigenvalues encode various image features and we consider a spatially varying continuous map which provides multiscale edge maps of natural images. By embedding the structure tensor-based exponent in a well-defined regularization model, we obtain denoising filters which are capable of obtaining good feature preserving image restoration. The GPU-based implementation computes the edge map in real time at 45-60 frames/s depending on the GPU card. Multiscale structure tensor-based spatially adaptive variable exponent provides reliable edge maps and compared with standard edge detectors it is robust under various noisy conditions. Moreover, filtering based on the multiscale variable exponent map method outperforms L0 sparse gradient-based image smoothing and related filters.																	1057-7149	1941-0042				DEC	2019	28	12					6198	6210		10.1109/TIP.2019.2924799													
J								Inertial Nonconvex Alternating Minimizations for the Image Deblurring	IEEE TRANSACTIONS ON IMAGE PROCESSING										Inertial algorithms; nonconvex method; Kurdyka-Lojasiewicz property; image deblurring; ADMM; inertial proximal ADMM	DIRECTION METHOD; PENALTY SCHEMES; CONVERGENCE; ALGORITHM; MULTIPLIERS	In image processing, total variation (TV) regularization models are commonly used to recover the blurred images. One of the most efficient and popular methods to solve the convex TV problem is the alternating direction method of multipliers (ADMM) algorithm, recently extended using the inertial proximal point method. Although all the classical studies focus on only a convex formulation, recent articles are paying increasing attention to the nonconvex methodology due to its good numerical performance and properties. In this paper, we propose to extend the classical formulation with a novel nonconvex alternating direction method of multipliers with the inertial technique (IADMM). Under certain assumptions on the parameters, we prove the convergence of the algorithm with the help of the Kurdyka-Lojasiewicz property. We also present numerical simulations on the classical TV image reconstruction problems to illustrate the efficiency of the new algorithm and its behavior compared with the well-established ADMM method.																	1057-7149	1941-0042				DEC	2019	28	12					6211	6224		10.1109/TIP.2019.2924339													
J								SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face hallucination; convolutional neural networks; generative adversarial networks; super-resolution; generative model		Though generative adversarial networks (GANs) can hallucinate high-quality high-resolution (HR) faces from low-resolution (LR) faces, they cannot ensure identity preservation during face hallucination, making the HR faces difficult to recognize. To address this problem, we propose a Siamese GAN (SiGAN) to reconstruct HR faces that visually resemble their corresponding identities. On top of a Siamese network, the proposed SiGAN consists of a pair of two identical generators and one discriminator. We incorporate reconstruction error and identity label information in the loss function of SiGAN in a pairwise manner. By iteratively optimizing the loss functions of the generator pair and the discriminator of SiGAN, we not only achieve visually-pleasing face reconstruction but also ensure that the reconstructed information is useful for identity recognition. Experimental results demonstrate that SiGAN significantly outperforms existing face hallucination GANs in objective face verification performance while achieving promising visual-quality reconstruction. Moreover, for input LR faces with unseen identities that are not part of the training dataset, SiGAN can still achieve reasonable performance.																	1057-7149	1941-0042				DEC	2019	28	12					6225	6236		10.1109/TIP.2019.2924554													
J								Project portfolio selection and scheduling under a fuzzy environment	MEMETIC COMPUTING										Project portfolio selection and planning; Multi-objective optimization; Fuzzy numbers; Inverse modeling; Gaussian process	OPTIMIZATION; ALGORITHM; TOPSIS	The problem of integrated project portfolio selection and scheduling (PPSS) is among the most important and highly pursed subjects in project management. In this study, a mathematical model and algorithm are designed specifically to assist decision makers decide which projects are to be chosen and when these projects are to be undertaken. More specifically, the PPSS problem is first formulated as a nonlinear multi-objective model with simultaneous consideration of benefit and risk factors. Due to the complexity and uncertainty involved in most real life situations, fuzzy numbers are incorporated into the model, which can provide decision makers with more flexibility. Then, an inverse modeling based multi-objective evolutionary algorithm using a Gaussian Process is presented to obtain the Pareto set. Finally, an illustrative example is used to demonstrate the high efficacy of the foregoing approach, which can provide decision makers with valuable insights into the PPSS process. The proposed algorithm is found to be more effective compared with two other popular algorithms.																	1865-9284	1865-9292				DEC	2019	11	4					391	406		10.1007/s12293-019-00282-5													
J								Trusting artificial intelligence in cybersecurity is a double-edged sword	NATURE MACHINE INTELLIGENCE												Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US$1 billion in 2016 to a US$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users' trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of 'reliable AI' for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity. Current national cybersecurity and defence strategies of several governments mention explicitly the use of AI. However, it will be important to develop standards and certification procedures, which involves continuous monitoring and assessment of threats. The focus should be on the reliability of AI-based systems, rather than on eliciting users' trust in AI.																		2522-5839				DEC	2019	1	12					557	560		10.1038/s42256-019-0109-1													
J								Predicting disease-associated mutation of metal-binding sites in proteins using a deep learning approach	NATURE MACHINE INTELLIGENCE											UBIQUITIN LIGASE; METALLOPROTEINS; INSIGHTS; SELECTIVITY; VARIANTS; DATABASE; MODES; BRCA1; UREE	Metalloproteins play important roles in many biological processes. Mutations at the metal-binding sites may functionally disrupt metalloproteins, initiating severe diseases; however, there seemed to be no effective approach to predict such mutations until now. Here we develop a deep learning approach to successfully predict disease-associated mutations that occur at the metal-binding sites of metalloproteins. We generate energy-based affinity grid maps and physiochemical features of the metal-binding pockets (obtained from different databases as spatial and sequential features) and subsequently implement these features into a multichannel convolutional neural network. After training the model, the multichannel convolutional neural network can successfully predict disease-associated mutations that occur at the first and second coordination spheres of zinc-binding sites with an area under the curve of 0.90 and an accuracy of 0.82. Our approach stands for the first deep learning approach for the prediction of disease-associated metal-relevant site mutations in metalloproteins, providing a new platform to tackle human diseases. Metals can bind to proteins to fulfil important biological functions. Predicting the features of mutated binding sites can thus help us understand the connection between specific mutations and their role in diseases.																		2522-5839				DEC	2019	1	12					561	567		10.1038/s42256-019-0119-z													
J								Prediction of drug combination effects with a minimal set of experiments	NATURE MACHINE INTELLIGENCE											CANCER; DISCOVERY; PLATFORM; SYNERGY; CELLS; PAIRS	High-throughput drug combination screening provides a systematic strategy to discover unexpected combinatorial synergies in pre-clinical cell models. However, phenotypic combinatorial screening with multi-dose matrix assays is experimentally expensive, especially when the aim is to identify selective combination synergies across a large panel of cell lines or patient samples. Here, we implement DECREASE, an efficient machine learning model that requires only a limited set of pairwise dose-response measurements for accurate prediction of drug combination synergy in a given sample. Using a compendium of 23,595 drug combination matrices tested in various cancer cell lines and malaria and Ebola infection models, we demonstrate how cost-effective experimental designs with DECREASE capture almost the same degree of information for synergy and antagonism detection as the fully measured dose-response matrices. Measuring only the matrix diagonal provides an accurate and practical option for combinatorial screening. The minimal-input web implementation enables applications of DECREASE to both pre-clinical and translational studies. Drug combinations are often an effective means of managing complex diseases, but understanding the synergies of drug combinations requires extensive resources. The authors developed an efficient machine learning model that requires only a limited set of pairwise dose-response measurements for the accurate prediction of synergistic and antagonistic drug combinations.																		2522-5839				DEC	2019	1	12					568	577		10.1038/s42256-019-0122-4													
J								Automated abnormality detection in lower extremity radiographs using deep learning	NATURE MACHINE INTELLIGENCE											DISEASES; TRENDS	Musculoskeletal disorders are a major healthcare challenge around the world. We investigate the utility of convolutional neural networks (CNNs) in performing generalized abnormality detection on lower extremity radiographs. We also explore the effect of pretraining, dataset size and model architecture on model performance to provide recommendations for future deep learning analyses on extremity radiographs, especially when access to large datasets is challenging. We collected a large dataset of 93,455 lower extremity radiographs of multiple body parts, with each exam labelled as normal or abnormal. A 161-layer densely connected, pretrained CNN achieved an AUC-ROC of 0.880 (sensitivity=0.714, specificity=0.961) on this abnormality classification task. Our findings show that a single CNN model can be effectively utilized for the identification of diverse abnormalities in highly variable radiographs of multiple body parts, a result that holds potential for improving patient triage and assisting with diagnostics in resource-limited settings. Identifying abnormalities in medical images across different viewing angles and body parts is a time-consuming task. Deep learning techniques hold great promise for supporting radiologists and improving patient triage decisions. A new study tests the viability of such approaches in resource-limited settings, exploring the effect of pretraining, dataset size and choice of deep learning model in the task of abnormality detection in lower-limb radiographs.																		2522-5839				DEC	2019	1	12					578	583		10.1038/s42256-019-0126-0													
J								A portable three-degrees-of-freedom force feedback origami robot for human-robot interactions	NATURE MACHINE INTELLIGENCE											HAPTIC FEEDBACK; DESIGN	Haptic interfaces can recreate the experience of touch and are necessary to improve human-robot interactions. However, at present, haptic interfaces are either electromechanical devices eliciting very limited touch sensations or devices that may provide more comprehensive kinesthetic cues but at the cost of their large volume: there is a clear trade-off between the richness of feedback and the device size. The design and manufacturing challenges in creating complex touch sensations from compact platforms still need to be solved. To overcome the physical limitation of miniaturizing force feedback robots, we adapted origami design principles to achieve portability, accuracy and scalable manufacturing. The result is Foldaway, a foldable origami robot that can render three-degrees-of-freedom force feedback in a compact platform that can fit in a pocket. This robotic platform can track the movement of the user's fingers, apply a force of up to 2 newtons and render stiffness up to 1.2newtons per millimetre. We experimented with different human-machine interactions to demonstrate the broad applicability of Foldaway prototypes: a portable interface for the haptic exploration of an anatomy atlas; a handheld joystick for interacting with virtual objects; and a bimanual controller for intuitive and safe teleoperation of drones. Haptic interfaces are important for the development of immersive human-machine interactions. To create a compact design with rich touch-sensitive functions, a robotic device called Foldaway, which folds flat, has been designed that can render three-degrees-of-freedom force feedback.																		2522-5839				DEC	2019	1	12					584	593		10.1038/s42256-019-0125-1													
J								Developing the knowledge of number digits in a child-like robot	NATURE MACHINE INTELLIGENCE											NEURAL-NETWORKS; GESTURE; HANDS; REPRESENTATIONS; EMBODIMENT; FREQUENCY; COUNT	Number knowledge can be boosted initially by embodied strategies such as the use of fingers. This Article explores the perceptual process of grounding number symbols in artificial agents, particularly the iCub robot-a child-like humanoid with fully functional, five-fingered hands. It studies the application of convolutional neural network models in the context of cognitive developmental robotics, where the training information is likely to be gradually acquired while operating, rather than being abundant and fully available as in many machine learning scenarios. The experimental analyses show increased efficiency of the training and similarities with studies in developmental psychology. Indeed, the proprioceptive information from the robot hands can improve accuracy in the recognition of spoken digits by supporting a quicker creation of a uniform number line. In conclusion, these findings reveal a novel way for the humanization of artificial training strategies, where the embodiment can make the robot's learning more efficient and understandable for humans. Number processing is linked to bodily systems, especially finger movements. The authors apply convolutional neural network models in the context of cognitive developmental robotics. They show that proprioceptive information in the child-like robot iCub improves accuracy and recognition of spoken digits.																		2522-5839				DEC	2019	1	12					594	605		10.1038/s42256-019-0123-3													
J								A universal information theoretic approach to the identification of stopwords	NATURE MACHINE INTELLIGENCE											TEXT	One of the most widely used approaches in natural language processing and information retrieval is the so-called bag-of-words model. A common component of such methods is the removal of uninformative words, commonly referred to as stopwords. Currently, most practitioners use manually curated stopword lists. This approach is problematic because it cannot be readily generalized across knowledge domains or languages. As a result of the difficulty in rigorously defining stopwords, there have been few systematic studies on the effect of stopword removal on algorithm performance, which is reflected in the ongoing debate on whether to keep or remove stopwords. Here we address this challenge by formulating an information theoretic framework that automatically identifies uninformative words in a corpus. We show that our framework not only outperforms other stopword heuristics, but also allows for a substantial reduction of document size in applications of topic modelling. Our findings can be readily generalized to other bag-of-words-type approaches beyond language such as in the statistical analysis of transcriptomics, audio or image corpora. To better extract meaning from natural language, some less informative words can be removed before a model is trained, which is usually done by using manually curated lists of stopwords. A new information theoretic approach can identify uninformative words automatically and more accurately.																		2522-5839				DEC	2019	1	12					606	612		10.1038/s42256-019-0112-6													
J								Modified immune network algorithm based on the Random Forest approach for the complex objects control	ARTIFICIAL INTELLIGENCE REVIEW										Complex objects control; Artificial intelligence; Artificial immune systems; Random Forest; Factor analysis; Feature extraction		Nowadays application of the methods of artificial intelligence to create automated complex objects control systems in different application areas is topical. The article presents the developed modified algorithm based on artificial immune system, in which the Random Forest algorithm is used for data pre-processing and extraction of informative signs describing the behavior of a complex object of control. There are presented the results of aircraft flight simulation based on Ailerons database with the help of WEKA software and RStudio environment. There was made the comparative analysis of the modified immune network algorithm with different data pre-processing (based on the Random Forest and factor analysis).																	0269-2821	1573-7462				DEC	2019	52	4					2457	2473		10.1007/s10462-018-9621-7													
J								The ASC-Inclusion Perceptual Serious Gaming Platform for Autistic Children	IEEE TRANSACTIONS ON GAMES										Games; Emotion recognition; Autism; Training; Tools; Monitoring; Face recognition; AI in games; autism spectrum condition (ASC); emotion recognition; inclusion; virtual computerized environment	COMPLEX EMOTION RECOGNITION; BODY MOVEMENT; ASPERGER-SYNDROME; POINT-LIGHT; GAME; INTERVENTION; PEOPLE; SKILLS	''Serious games" are becoming extremely relevant to individuals who have specific needs, such as children with an autism spectrum condition (ASC). Often, individuals with an ASC have difficulties in interpreting verbal and nonverbal communication cues during social interactions. The ASC-Inclusion EU-FP7 funded project aims to provide children who have an ASC with a platform to learn emotion expression and recognition, through play in the virtual world. In particular, the ASC-Inclusion platform focuses on the expression of emotion via facial, vocal, and bodily gestures. The platform combines multiple analysis tools, using onboard microphone and webcam capabilities. The platform utilizes these capabilities via training games, text-based communication, animations, video, and audio clips. This paper introduces current findings and evaluations of the ASC-Inclusion platform and provides detailed description for the different modalities.																	2475-1502	2475-1510				DEC	2019	11	4					328	339		10.1109/TG.2018.2864640													
J								Serious Games for Training Social Skills in Job Interviews	IEEE TRANSACTIONS ON GAMES												In this paper, we focus on experience-based role play with virtual agents to provide young adults at the risk of exclusion with social skill training. We present a scenario-based serious game simulation platform. It comes with a social signal interpretation component, a scripted and autonomous agent dialog and social interaction behavior model, and an engine for 3-D rendering of lifelike virtual social agents in a virtual environment. We show how two training systems developed on the basis of this simulation platform can be used to educate people in showing appropriate socioemotive reactions in joh interviews. Furthermore, we give an overview of four conducted studies investigating the effect of the agents' portrayed personality and the appearance of the environment on the players' perception of the characters and the learning experience.																	2475-1502	2475-1510				DEC	2019	11	4					340	351		10.1109/TG.2018.2808525													
J								Serious Games for Training Social Skills in Job Interviews	IEEE TRANSACTIONS ON GAMES										Games; Interviews; Training; Computational intelligence; Natural languages; Virtual environments; Buildings		In this paper, we focus on experience-based role play with virtual agents to provide young adults at the risk of exclusion with social skill training. We present a scenario-based serious game simulation platform. It comes with a social signal interpretation component, a scripted and autonomous agent dialog and social interaction behavior model, and an engine for 3-D rendering of lifelike virtual social agents in a virtual environment. We show how two training systems developed on the basis of this simulation platform can be used to educate people in showing appropriate socioemotive reactions in job interviews. Furthermore, we give an overview of four conducted studies investigating the effect of the agents' portrayed personality and the appearance of the environment on the players' perception of the characters and the learning experience.																	2475-1502	2475-1510				DEC	2019	11	4					340	351		10.1109/TG.2018.2808525													
J								Automated Playtesting With Procedural Personas Through MCTS With Evolved Heuristics	IEEE TRANSACTIONS ON GAMES										Games; Computational modeling; Testing; Mathematical model; Peer-to-peer computing; Psychology; Monte Carlo methods; Agent controllers; automated playtesting; play persona; player modeling		This paper describes a method for generative player modeling and its application to the automatic testing of game content using archetypal player models called procedural personas. Theoretically grounded in psychological decision theory, procedural personas are implemented using a variation of Monte Carlo tree search (MCTS) where the node selection criteria are developed using evolutionary computation, replacing the standard UCB1 criterion of MCTS. Using these personas, we demonstrate how generative player models can be applied to a varied corpus of game levels and demonstrate how different playstyles can be enacted in each level. In short, we use artificially intelligent personas to construct synthetic playtesters. The proposed approach could be used as a tool for automatic play testing when human feedback is not readily available or when quick visualization of potential interactions is necessary. Possible applications include interactive tools during game development or procedural content generation systems where many evaluations must be conducted within a short time span.																	2475-1502	2475-1510				DEC	2019	11	4					352	362		10.1109/TG.2018.2808198													
J								Automated Playtesting With Procedural Personas Through MCTS With Evolved Heuristics	IEEE TRANSACTIONS ON GAMES										Agent controllers; automated playtesting; play persona; player modeling		This paper describes a method for generative player modeling and its application to the automatic testing of game content using archetypal player models called procedural personas. Theoretically grounded in psychological decision theory, procedural personas are implemented using a variation of Monte Carlo tree search (MCTS) where the node selection criteria are developed using evolutionary computation, replacing the standard UCB1 criterion of MCTS. Using these personas, we demonstrate how generative player models can be applied to a varied corpus of game levels and demonstrate how different playstyles can be enacted in each level. In short, we use artificially intelligent personas to construct synthetic playtesters. The proposed approach could be used as a tool for automatic play testing when human feedback is not readily available or when quick visualization of potential interactions is necessary. Possible applications include interactive tools during game development or procedural content generation systems where many evaluations must he conducted within a short time span.																	2475-1502	2475-1510				DEC	2019	11	4					352	362		10.1109/TG.2018.2808198													
J								Exploration in NetHack With Secret Discovery	IEEE TRANSACTIONS ON GAMES										Planning; role-playing games	STRATEGIES; COVERAGE	Roguelike games generally feature exploration problems as a critical yet often repetitive element of gameplay. Automated approaches, however, face challenges in terms of optimality, as well as due to incomplete information, such as from the presence of secret doors. This paper presents an algorithmic approach to exploration of roguelike dungeon environments. Our design aims to minimize exploration time, balancing coverage and discovery of secret areas with resource cast. Our algorithm is based on the concept of occupancy maps popular in robotics, adapted to encourage efficient discovery of secret access points. Through extensive experimentation on NetHack maps, we show that this technique is significantly more efficient than simpler greedy approaches and an existing automated player. We further investigate optimized parameterization for the algorithm through a comprehensive data analysis. These results point toward better automation for players, as well as heuristics applicable to fully automated gameplay.																	2475-1502	2475-1510				DEC	2019	11	4					363	373		10.1109/TG.2018.2861759													
J								Exploration in NetHack With Secret Discovery	IEEE TRANSACTIONS ON GAMES										Games; Space exploration; Simultaneous localization and mapping; Automation; Planning; role-playing games	STRATEGIES; COVERAGE	Roguelike games generally feature exploration problems as a critical yet often repetitive element of gameplay. Automated approaches, however, face challenges in terms of optimality, as well as due to incomplete information, such as from the presence of secret doors. This paper presents an algorithmic approach to exploration of roguelike dungeon environments. Our design aims to minimize exploration time, balancing coverage and discovery of secret areas with resource cost. Our algorithm is based on the concept of occupancy maps popular in robotics, adapted to encourage efficient discovery of secret access points. Through extensive experimentation on NetHack maps, we show that this technique is significantly more efficient than simpler greedy approaches and an existing automated player. We further investigate optimized parameterization for the algorithm through a comprehensive data analysis. These results point toward better automation for players, as well as heuristics applicable to fully automated gameplay.																	2475-1502	2475-1510				DEC	2019	11	4					363	373		10.1109/TG.2018.2861759													
J								How the Business Model of Customizable Card Games Influences Player Engagement	IEEE TRANSACTIONS ON GAMES										Games; Business; Androids; Humanoid robots; Analytical models; Data models; Buildings; Business intelligence; clustering algorithms; data analysis; game analytics; machine learning		In this paper, we analyze the gameplay data of three popular customizable card games where players build decks prior to gameplay. We analyze the data from a player engagement perspective, how the business model affects players, how players influence the business model and provide strategic insights for players themselves. Sifa et al. found a lack of cross-game analytics, whereas Marchand and Hennig-Thurau identified a lack of understanding of how a game's business model and strategies affect players. We address both issues. The three games have similar business models but differ in one aspect: the distribution model for the cards used in the game. Our longitudinal analysis highlights this variation's impact. A uniform distribution creates a spread of decks with slowly emerging trends while a random distribution creates stripes of deck building activity that switch suddenly each update. Our method is simple, easily understandable, independent of the specific game's structure, and able to compare multiple games. It is applicable to games that release updates and enables comparison across games. Optimizing a game's updates strategy is the key, as it affects player engagement and retention, which directly influence businesses' revenues and profitability in the $95 billion global games market.																	2475-1502	2475-1510				DEC	2019	11	4					374	385		10.1109/TG.2018.2803843													
J								How the Business Model of Customizable Card Games Influences Player Engagement	IEEE TRANSACTIONS ON GAMES										Business intelligence; clustering algorithms; data analysis; game analytics; machine learning		In this paper, we analyze the gameplay data of three popular customizable card games where players build decks prior to gameplay. We analyze the data from a player engagement perspective, how the business model affects players, how players influence the business model and provide strategic insights for players themselves. Sifa et al. found a lack of crass-game analytics, whereas Marchand and Hennig-Thurau identified a lack of understanding of how a game's business model and strategies affect players. We address both issues. The three games have similar business models but differ in one aspect: the distribution model for the cards used in the game. Our longitudinal analysis highlights this variation's impact. A uniform distribution creates a spread of decks with slowly emerging trends while a random distribution creates stripes of deck building activity that switch suddenly each update. Our method is simple, easily understandable, independent of the specific game's structure, and able to compare multiple games. It is applicable to games that release updates and enables comparison across games. Optimizing a game's updates strategy is the key, as it affects player engagement and retention, which directly influence businesses' revenues and profitability in the $95 billion global games market.																	2475-1502	2475-1510				DEC	2019	11	4					374	385		10.1109/TG.2018.2803843													
J								Emulating Human Play in a Leading Mobile Card Game	IEEE TRANSACTIONS ON GAMES										Artificial intelligence (AI); digital games; Monte Carlo tree search; neural networks	INFORMATION; STRATEGIES	Monte Carlo tree search (MCTS) has become a popular solution for game artificial intelligence (AI), capable of creating strong game playing opponents. However, the emergent playstyle of agents using MCTS is not necessarily human-like, believable or enjoyable. AI Factory Spades, currently the top rated Spades game in the Google Play store, uses a variant of MCTS to control AI allies and opponents. In collaboration with the developers, we showed in a previous study that the playstyle of human players significantly differed from that of the AI players. This paper presents a method for player modeling using gameplay data and neural networks that does not require domain knowledge, and a method of biasing MCTS with such a player model to create Spades playing agents that emulate human play whilst maintaining strong, competitive performance. The methods of player modeling and biasing MCTS presented in this study are applied to the commercial codebase of AI Factory Spades, and are transferable to MCTS implementations for discrete-action games where relevant gameplay data are available.																	2475-1502	2475-1510				DEC	2019	11	4					386	395		10.1109/TG.2018.2835764													
J								Emulating Human Play in a Leading Mobile Card Game	IEEE TRANSACTIONS ON GAMES										Games; Neural networks; Production facilities; Predictive models; Hidden Markov models; Monte Carlo methods; Artificial intelligence (AI); digital games; Monte Carlo tree search; neural networks	INFORMATION; STRATEGIES	Monte Carlo tree search (MCTS) has become a popular solution for game artificial intelligence (AI), capable of creating strong game playing opponents. However, the emergent playstyle of agents using MCTS is not necessarily human-like, believable or enjoyable. AI Factory Spades, currently the top rated Spades game in the Google Play store, uses a variant of MCTS to control AI allies and opponents. In collaboration with the developers, we showed in a previous study that the playstyle of human players significantly differed from that of the AI players. This paper presents a method for player modeling using gameplay data and neural networks that does not require domain knowledge, and a method of biasing MCTS with such a player model to create Spades playing agents that emulate human play whilst maintaining strong, competitive performance. The methods of player modeling and biasing MCTS presented in this study are applied to the commercial codebase of AI Factory Spades, and are transferable to MCTS implementations for discrete-action games where relevant gameplay data are available.																	2475-1502	2475-1510				DEC	2019	11	4					386	395		10.1109/TG.2018.2835764													
J								Recommender System for Items in Dota 2	IEEE TRANSACTIONS ON GAMES										Apriori; Dota 2; Jaccard; k-medoids clustering; logistic regression; purchase prediction; recommender system; rule-based classifier		Data 2 is one of several multiplayer online battle arena games that have recently become extremely popular. A central feature of Data 2 is that players select and purchase items that are used in the game and the selections strongly affect the outcome of the game. Item recommendations and purchase predictions in Data 2 turn out to be interesting problems due to the many factors affecting the choice of items and due to the frequency by which items are purchased throughout the game. We present three recommender systems for recommending which items a player might buy throughout a match, based on commonly used purchasing strategies. These systems are a rule-based system, a logistic regression based system, and a logistic regression based system enhanced with clustering. Knowing only the player's hero and inventory items, the first two systems are able to predict the purchases by highly skilled players with accuracies ranging from 66.5% to 87.1%, depending on the hero where accuracy is defined as recommended items that were actually purchased in the next 5 min of the game. For the third system, the players are clustered by purchasing strategy which in turn group players by role and play style. Adding the clustering feature to the second version of the recommender system only improve the results slightly. The reason for this could be that the items in the player's inventory are already a strong indicator of a player's role and play style both of which were used to develop the first two recommender systems. An important question for future work is how the recommendations could be useful in practice for human or AI players.																	2475-1502	2475-1510				DEC	2019	11	4					396	404		10.1109/TG.2018.2844121													
J								Recommender System for Items in Dota 2	IEEE TRANSACTIONS ON GAMES										Apriori; Dota 2; Jaccard; k-medoids clustering; logistic regression; purchase prediction; recommender system; rule-based classifier		Dota 2 is one of several multiplayer online battle arena games that have recently become extremely popular. A central feature of Dota 2 is that players select and purchase items that are used in the game and the selections strongly affect the outcome of the game. Item recommendations and purchase predictions in Dota 2 turn out to be interesting problems due to the many factors affecting the choice of items and due to the frequency by which items are purchased throughout the game. We present three recommender systems for recommending which items a playermight buy throughout a match, based on commonly used purchasing strategies. These systems are a rule-based system, a logistic regression based system, and a logistic regression based system enhanced with clustering. Knowing only the player's hero and inventory items, the first two systems are able to predict the purchases by highly skilled players with accuracies ranging from 66.5% to 87.1%, depending on the hero where accuracy is defined as recommended items that were actually purchased in the next 5 min of the game. For the third system, the players are clustered by purchasing strategy which in turn group players by role and play style. Adding the clustering feature to the second version of the recommender system only improve the results slightly. The reason for this could be that the items in the player's inventory are already a strong indicator of a player's role and play style both of which were used to develop the first two recommender systems. An important question for future work is how the recommendations could be useful in practice for human or AI players.																	2475-1502	2475-1510				DEC	2019	11	4					396	404		10.1109/TG.2018.2844121													
J								Identifying Regional Trends in Avatar Customization	IEEE TRANSACTIONS ON GAMES										Artificial neural networks; avatars; clustering algorithms; cultural differences; data analysis; deep learning; image processing; unsupervised learning	SOCIAL IDENTITY; ONLINE; ETHNICITY; LOOKING; DESIGN; GENDER; PLAY	Since virtual identities such as social media profiles and avatars have become a common venue for self-expression, it has become important to consider the ways in which existing systems embed the values of their designers. In order to design virtual identity systems that reflect the needs and preferences of diverse users, understanding how the virtual identity construction differs between groups is important. This paper presents a new methodology that leverages deep learning and differential clustering for comparative analysis of profile images, with a case study of almost 100 000 avatars from a large online community using a popular avatar creation platform. We use novelty discovery to segment the avatars, then cluster avatars by region to identify visual trends among low- and high-novelty avatars. We find that avatar customization correlates with increased social activity, and we are able to identify distinct visual trends among the US.-region and Japan-region profiles. Among these trends, realistic, idealistic, and creative self-representation can be distinguished. We observe that the realistic self-expression mirrors regional demographics, idealistic self-expression reflects shared mass-media tropes, and creative self-expression propagates within the communities.																	2475-1502	2475-1510				DEC	2019	11	4					405	415		10.1109/TG.2018.2835776													
J								Identifying Regional Trends in Avatar Customization	IEEE TRANSACTIONS ON GAMES										Avatars; Social network services; Games; Machine learning; Training; Market research; Prototypes; Artificial neural networks; avatars; clustering algorithms; cultural differences; data analysis; deep learning; image processing; unsupervised learning	SOCIAL IDENTITY; ONLINE; ETHNICITY; LOOKING; DESIGN; GENDER; PLAY	Since virtual identities such as social media profiles and avatars have become a common venue for self-expression, it has become important to consider the ways in which existing systems embed the values of their designers. In order to design virtual identity systems that reflect the needs and preferences of diverse users, understanding how the virtual identity construction differs between groups is important. This paper presents a new methodology that leverages deep learning and differential clustering for comparative analysis of profile images, with a case study of almost 100 000 avatars from a large online community using a popular avatar creation platform. We use novelty discovery to segment the avatars, then cluster avatars by region to identify visual trends among low- and high-novelty avatars. We find that avatar customization correlates with increased social activity, and we are able to identify distinct visual trends among the U.S.-region and Japan-region profiles. Among these trends, realistic, idealistic, and creative self-representation can be distinguished. We observe that the realistic self-expression mirrors regional demographics, idealistic self-expression reflects shared mass-media tropes, and creative self-expression propagates within the communities.																	2475-1502	2475-1510				DEC	2019	11	4					405	415		10.1109/TG.2018.2835776													
J								Inside the Group: Investigating Social Structures in Player Groups and Their Influence on Activity	IEEE TRANSACTIONS ON GAMES										Destiny; game analytics; matchmaking; social networks	NETWORKS	Social features, matchmaking, and grouping functions are key elements of online multiplayer experiences. Understanding how social connections form in and around games and their relationship to in-game activity offers insights for building and maintaining player bases and for improving engagement and retention. This paper presents an analysis of the groups formed by users of the the100.io-a social matchmaking website for different commercial titles, including Destiny on which we focus in this paper. Groups formed on the100.io can be described across a range of social network related metrics. Also, the social network formed within a group is evaluated in combination with user-provided demographic and preference data. Archetypal analysis is used to classify groups into archetypes and a correlation analysis is presented covering the effect of group characteristics on in-game activity. Finally, weekly activity profiles are described. Our results indicate that group size as well as the number of moderators within a group and their connectedness to other teammembers influences a group's activity. We also identified four prototypical types of groups with different characteristics concerning composition, social cohesion, and activity.																	2475-1502	2475-1510				DEC	2019	11	4					416	425		10.1109/TG.2018.2858024													
J								Strategy Generation for Multiunit Real-Time Games via Voting	IEEE TRANSACTIONS ON GAMES										Artificial intelligence; computational intelligence; multiagent systems; machine learning algorithms	ALGORITHM SELECTION	Real time strategy (RTS) games are a challenging application for artificial intelligence (AI) methods. This is because they involve simultaneous play and adversarial reasoning that is conducted in real time in large state spaces. Many Al methods for playing RTS games rely on hard-coded strategies designed by human experts. The drawback of using such strategies is that they are often unable to adapt to new scenarios during gameplay. The contribution of this paper is a new approach, called strategy creation via voting (SCV), that uses a voting method to generate a large set of novel strategies from existing expert-based ones. Then, SCV uses an opponent modeling scheme during the game to choose which strategy from the generated pool of possibilities to use. By repeatedly choosing which strategy to use, SCV is able to adapt to different scenarios that might arise during the game. We implemented SCV as a bot for mu RTS, a recognized RTS testbed. The results of a detailed empirical study show that SCV outperforms all approaches tested in matches played on large maps and is competitive in matches played on smaller maps.																	2475-1502	2475-1510				DEC	2019	11	4					426	435		10.1109/TG.2018.2848913													
J								Strategy Generation for Multiunit Real-Time Games via Voting	IEEE TRANSACTIONS ON GAMES										Artificial intelligence; computational intelligence; multiagent systems; machine learning algorithms	ALGORITHM SELECTION	Real-time strategy (RTS) games are a challenging application for artificial intelligence (AI) methods. This is because they involve simultaneous play and adversarial reasoning that is conducted in real time in large state spaces. Many AI methods for playing RTS games rely on hard-coded strategies designed by human experts. The drawback of using such strategies is that they are often unable to adapt to new scenarios during gameplay. The contribution of this paper is a new approach, called strategy creation via voting (SCV), that uses a voting method to generate a large set of novel strategies from existing expert-based ones. Then, SCV uses an opponent modeling scheme during the game to choose which strategy from the generated pool of possibilities to use. By repeatedly choosing which strategy to use, SCV is able to adapt to different scenarios that might arise during the game. We implemented SCV as a bot for mu RTS, a recognized RTS testbed. The results of a detailed empirical study show that SCV outperforms all approaches tested in matches played on large maps and is competitive in matches played on smaller maps.																	2475-1502	2475-1510				DEC	2019	11	4					426	435		10.1109/TG.2018.2848913													
J								Standing on the Feet of Giants - Reproducibility in AI	AI MAGAZINE												A recent study implies that research presented at top artificial intelligence conferences is not documented well enough for the research to be reproduced. My objective was to investigate whether the quality of the documentation is the same for industry and academic research or if differences actually exist. My hypothesis is that industry and academic research presented at top artificial intelligence conferences is equally well documented. A total of 325 International Joint Conferences on Artificial Intelligence and Association for the Advancement of Artificial Intelligence research papers reporting empirical studies have been surveyed. Of these, 268 were conducted by academia, 47 were collaborations, and 10 were conducted by the industry. A set of 16 variables, which specifies how well the research is documented, was reviewed for each paper and each variable was analyzed individually. Three reproducibility metrics were used for assessing the documentation quality of each paper. The findings indicate that academic research does score higher than industry and collaborations on all three reproducibility metrics. Academic research also scores highest on 15 out of the 16 surveyed variables. The result is statistically significant for 3 out of the 16 variables, but none of the reproducibility metrics. The conclusion is that the results are not statistically significant, but still indicate that my hypothesis probably should be refuted. This is surprising, as the conferences use double-blind peer review and all research is judged according to the same standards.																	0738-4602					WIN	2019	40	4					9	23		10.1609/aimag.v40i4.5185													
J								Identifying Critical Contextual Design Cues Through a Machine Learning Approach	AI MAGAZINE												Given the rise of autonomous systems in transportation, medical, and manufacturing industries, there is an increasing need to understand how such systems should be designed to promote effective interactions between one or more humans working in and around these systems. Practitioners often have difficulties in conducting costly and time-consuming human-in-the-loop studies, so an analytical strategy that helps them determine whether their designs are capturing their planned intent is needed. A traditional top-down, hypothesis-driven experiment that examined whether external displays mounted on autonomous cars could effectively communicate with pedestrians led to the conclusion that the displays had no effect on safety. However, by first taking a bottom-up, data-driven machine learning approach, those segments of the population that were most affected by the external displays were identified. Then, a hypothesis-driven, within-subjects analysis of variance revealed that an external display mounted on an autonomous car that provided the vehicle's speed as opposed to commanding a go/no-go decision provided an additional 4 feet of safety for early adopters. One caveat to this approach is that the selection of a specific algorithm can significantly influence the results and more work is needed to determine the sensitivity of this approach with seemingly similar machine learning classification approaches.																	0738-4602					WIN	2019	40	4					28	39		10.1609/aimag.v40i4.4811													
J								Uncertain Context: Uncertainty Quantification in Machine Learning	AI MAGAZINE											ARCHITECTURE; NETWORK; RUNTIME	Machine learning and artificial intelligence will be deeply embedded in the intelligent systems humans use to automate tasking, optimize planning, and support decision-making. However, many of these methods can be challenged by dynamic computational contexts, resulting in uncertainty in prediction errors and overall system outputs. Therefore, it will be increasingly important for uncertainties in underlying learning-related computer models to be quantified and communicated. The goal of this article is to provide an accessible overview of computational context and its relationship to uncertainty quantification for machine learning, as well as to provide general suggestions on how to implement uncertainty quantification when doing statistical learning. Specifically, we will discuss the challenge of quantifying uncertainty in predictions using popular machine learning models. We present several sources of uncertainty and their implications on statistical models and subsequent machine learning predictions.																	0738-4602					WIN	2019	40	4					40	48		10.1609/aimag.v40i4.4812													
J								Methods of AI for Multimodal Sensing and Action for Complex Situations	AI MAGAZINE												Artificial intelligence (AI) seeks to emulate human reasoning, but is still far from achieving such results for actionable sensing in complex situations. Instead of emulating human situation understanding, machines can amplify intelligence by accessing large amounts of data, filtering unimportant information, computing relevant context, and prioritizing results (for example, answers to human queries) to provide human-machine shared context. Intelligence support can come from many contextual sources that augment data reasoning through physical, environmental, and social knowledge. We propose a decisions-to-data multimodal sensor and action through contextual agents (human or machine) that seek, combine, and make sense of relevant data. Decisions-to-data combines AI computational capabilities with human reasoning to manage data collections, perform data fusion, and assess complex situations (that is, context reasoning). Five areas of AI developments for context-based AI that cover decisions-to-data include: (1) situation modeling (data at rest), (2) measurement control (data in motion), (3) statistical algorithms (data in collect), (4) software computing (data in transit), and (5) human-machine AI (data in use). A decisions-to-data example is presented of a command-guided swarm requiring contextual data analysis, systems-level design, and user interaction for effective and efficient multimodal sensing and action.																	0738-4602					WIN	2019	40	4					50	65		10.1609/aimag.v40i4.4813													
J								Truly Autonomous Machines Are Ethical	AI MAGAZINE											BELIEF	There is widespread concern that as machines move toward greater autonomy, they may become a law unto themselves and turn against us. Yet the threat lies more in how we conceive of an autonomous machine rather than the machine itself. We tend to see an autonomous agent as one that sets its own agenda, free from external constraints, including ethical constraints. A deeper and more adequate understanding of autonomy has evolved in the philosophical literature, specifically in deontological ethics. It teaches that ethics is an internal, not an external, constraint on autonomy, and that a truly autonomous agent must be ethical. It tells us how we can protect ourselves from smart machines by making sure they are truly autonomous rather than simply beyond human control.																	0738-4602					WIN	2019	40	4					66	73		10.1609/aimag.v40i4.2863													
J								Experiments in Social Media	AI MAGAZINE												Social media platforms like Facebook and Twitter permit experiments to be performed at minimal cost on populations of a size that scientists might previously have dreamed about. For instance, one experiment on Facebook involved more than 60 million subjects. Such large-scale experiments introduce new challenges as even small effects when multiplied by a large population can have a significant impact. Recent revelations about the use of social media to manipulate voting behavior compound such concerns. It is believed that the psychometric data used by Cambridge Analytica to target US voters was collected by Dr Aleksandr Kogan from Cambridge University using a personality quiz on Facebook. There is a real risk that researchers wanting to collect data and run experiments on social media platforms in the future will face a public backlash that hinders such studies from being conducted. We suggest that stronger safeguards are put in place to help prevent this, and ensure the public retain confidence in scientists using social media for behavioral and other studies.																	0738-4602					WIN	2019	40	4					74	77		10.1609/aimag.v40i4.2868													
J								Reports of the Workshops Held at the 2019 International AAAI Conference on Web and Social Media	AI MAGAZINE												The workshop program of the Association for the Advancement of Artificial Intelligence's 13th International Conference on Web and Social Media was held at the Bavarian School of Public Policy in Munich, Germany on June 11, 2019. There were five full-day workshops, one half-day workshop, and the annual evening Science Slam in the program. The proceedings of the workshops were published in Research Topic of the Frontiers in Big Data. This report contains summaries of those workshops.																	0738-4602					WIN	2019	40	4					78	82		10.1609/aimag.v40i4.5287													
J								Report on the Thirty-Second International Florida Artificial Intelligence Research Society Conference (FLAIRS-32)	AI MAGAZINE												The Thirty-Second International Florida Artificial Intelligence Research Society Conference was held May 19-22, 2019, at the Lido Beach Resort in Sarasota, Florida, USA. The conference events included tutorials, invited speakers, special tracks, and presentations of papers, posters, and awards. The conference chair was Vasile Rus from the University of Memphis. The program cochairs were Keith Brawner from the Army Research Laboratory and Roman Bartak from Charles University, Prague. The special tracks were coordinated by Eric Bell.																	0738-4602					WIN	2019	40	4					83	84															
J								Current Approaches in Applied Artificial Intelligence: The 2019 International Conference on Industrial, Engineering, and Other Applications of Applied Intelligent Systems	AI MAGAZINE												The 32nd meeting of the International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems was held July 9-11, 2019, in Graz, Austria. The conference focus for 2019 was on automated driving, autonomous systems, robotics, and AI in tourism.																	0738-4602					WIN	2019	40	4					85	87		10.1609/aimag.v40i4.5202													
J								Estimation of 3D human hand poses with structured pose prior	IET COMPUTER VISION										feature extraction; pose estimation; image representation; computer vision; image matching; 3D human hand; multistage estimation model; SPP; novel coarse-to-fine framework; single depth image; authors; original hand; under-complete stacked denoising auto-encoder; mapping canonical hand; under-complete SDA; over-complete SDA; hand estimation accuracy; computation times; hand palm key-point; human hand palm	NETWORK	Here, the authors present multistage estimation model embedding with structured pose prior (SPP), a novel coarse-to-fine framework for real-time 3D hand estimation from single depth image. Authors' main contributions can be summarised as follows: (i) The authors proposed SPP to enforce constraints of canonical hand pose instead of original hand pose. (ii) The authors are the first to adopt under-complete stacked denoising auto-encoder (SDA) to construct pose prior by mapping canonical hand pose to latent representation. In the case of enforcing constraints of canonical hand pose, the authors empirically validate that under-complete SDA outperforms over-complete SDA in improving the hand estimation accuracy. (iii) The authors propose candidate keypoints patches (CKP) as intermediate data to conduct further hand pose refinement. Experimental evaluation on two publically available datasets shows that authors' model is competitive both in accuracy and computation time. Especially, authors' method placed first in the location of palm key-point on both two datasets, and the high accuracy of hand palm key-point plays an important role in many applications, such as that manipulator can grasp objects to specific coordinates with the guiding of human hand palm.																	1751-9632	1751-9640				DEC	2019	13	8					683	690		10.1049/iet-cvi.2018.5480													
J								Towards path-based semantic dissimilarity estimation for scene representation using bottleneck analysis	IET COMPUTER VISION										object detection; image representation; image texture; path-based semantic dissimilarity estimation; scene representation; natural images; image elements; gradual variations; clutters; path-based bottleneck analysis method; semantic information; spatial continuity; feature consistency; double-S path; similar pattern; path-based bottleneck distance; image ranking; salient object detection	SALIENCY DETECTION; VISUAL-ATTENTION	In natural images, it remains challenging to estimate dissimilarities between image elements for scene representation due to gradual variations of illuminations, textures or clutters. To tackle this problem, we utilise a path-based bottleneck analysis method that captures the semantic information between image elements to measure the dissimilarity. By integrating both the spatial continuity and feature consistency into the understanding of the semantic information, we detect the bottlenecks on the proposed double-S path to define the bottleneck distance, which demonstrates a favourable capability of grouping image elements that follow a similar pattern and separating different ones. In the experiments, the method is proved to be robust to noises and invariant to changing illumination and arbitrary scales in natural images. Tests on some challenging datasets validate the advantage of applying the path-based bottleneck distance in image ranking and salient object detection.																	1751-9632	1751-9640				DEC	2019	13	8					691	699		10.1049/iet-cvi.2018.5560													
J								HGR-Net: a fusion network for hand gesture segmentation and recognition	IET COMPUTER VISION										feature extraction; gesture recognition; image classification; image segmentation; learning (artificial intelligence); object detection; convolutional neural nets; fusion network; hand gesture segmentation; robust recognition; real-world applications; cluttered backgrounds; unconstrained environmental factors; hand segmentation; redundant information; scene background; recognition stage; two-stage convolutional neural network architecture; hand regions; segmentation stage architecture; fully convolutional residual network; segmentation sub-network; depth information; complex backgrounds; segmented images; static hand gestures; HGR-Net; spatial pyramid pooling; red-green-blue and segmented images; HGR-Net	NEURAL-NETWORK; SKIN DETECTION; TRACKING; SYSTEM	We propose a two-stage convolutional neural network (CNN) architecture for robust recognition of hand gestures, called HGR-Net, where the first stage performs accurate semantic segmentation to determine hand regions, and the second stage identifies the gesture. The segmentation stage architecture is based on the combination of fully convolutional residual network and atrous spatial pyramid pooling. Although the segmentation sub-network is trained without depth information, it is particularly robust against challenges such as illumination variations and complex backgrounds. The recognition stage deploys a two-stream CNN, which fuses the information from the red-green-blue and segmented images by combining their deep representations in a fully connected layer before classification. Extensive experiments on public datasets show that our architecture achieves almost as good as state-of-the-art performance in segmentation and recognition of static hand gestures, at a fraction of training time, run time, and model size. Our method can operate at an average of 23ms per frame.																	1751-9632	1751-9640				DEC	2019	13	8					700	707		10.1049/iet-cvi.2018.5796													
J								Attention-based spatial-temporal hierarchical ConvLSTM network for action recognition in videos	IET COMPUTER VISION										image recognition; image motion analysis; video signal processing; computer vision; spatial structures; temporal structures; ST-HConvLSTM; spatial domains; temporal domains; spatial-temporal attention module; appearance information; motion information; spatial-temporal hierarchical ConvLSTM network; human action recognition; spatial information; temporal information; spatial-temporal hierarchical convolutional long short-term memory; Kinetics datasets; HMDB51 datasets; UCF101 datasets; weighted fusion strategy		Human action recognition in videos is an important research topic in computer vision due to its wide applications. Actions naturally contain both spatial and temporal information. The key to action recognition is to model the spatial and temporal structures of actions. In this study, the authors propose an attention-based spatial-temporal hierarchical convolutional long short-term memory (ST-HConvLSTM) network to model the structures of actions in the spatial and temporal domains. The ST-HConvLSTM consists of two parts: a spatial-temporal attention module and a novel LSTM-like architecture named hierarchical ConvLSTM (HConvLSTM). The HConvLSTM can model the spatial and temporal structures of actions. The spatial-temporal attention module can figure out which part of the video is more discriminative for action recognition and makes the HConvLSTM focus on it. In addition, a weighted fusion strategy is proposed to fuse the appearance information and motion information of the video. The proposed ST-HConvLSTM is evaluated on UCF101, HMDB51 and Kinetics datasets. Experimental results show that the authors' proposed ST-HConvLSTM achieves state-of-the-art performance compared with other recent LSTM-like architectures and attention-based methods.																	1751-9632	1751-9640				DEC	2019	13	8					708	718		10.1049/iet-cvi.2018.5830													
J								CVABS: moving object segmentation with common vector approach for videos	IET COMPUTER VISION										computer vision; object detection; video signal processing; image motion analysis; image segmentation; smart foreground detection; background updating procedure; dynamic control parameters; dynamic backgrounds; objective measures; background frames; object segmentation; common vector approach; real-time computer vision applications; security systems; subspace-based background-modelling algorithm; CDNet2014 dataset; Gram-Schmidt orthogonalisation; self-learning feedback mechanism	MULTISCALE; TRACKING	Background modelling is a fundamental step for several real-time computer vision applications that requires security systems and monitoring. An accurate background model helps to detect the activity of moving objects in the video. In this work, the authors have developed a new subspace-based background-modelling algorithm using the concept of common vector approach (CVA) with Gram-Schmidt orthogonalisation. Once the background model that involves the common characteristic of different views corresponding to the same scene is acquired, a smart foreground detection and background updating procedure is applied based on dynamic control parameters. A variety of experiments is conducted on different problem types related to dynamic backgrounds. Several types of metrics are utilised as objective measures and the obtained visual results are judged subjectively. It was observed that the proposed method stands successfully for all problem types reported on CDNet2014 dataset by updating the background frames with a self-learning feedback mechanism.																	1751-9632	1751-9640				DEC	2019	13	8					719	729		10.1049/iet-cvi.2018.5642													
J								Moving vehicle tracking based on improved tracking-learning-detection algorithm	IET COMPUTER VISION										target tracking; object tracking; learning (artificial intelligence); Kalman filters; object detection; video streaming; video signal processing; illumination variation; square root cubature Kalman filter; improved tracking precision; TLD datasets; ITLD; tracking accuracy; improved tracking-learning-detection algorithm; video streams; tracking failures; TLD methods; improved TLD tracking algorithm; object occlusion; long-term single-target moving vehicle tracking; fast retina keypoint feature; normalised cross-correlation coefficient	OBJECT TRACKING	This study addresses the tracking-learning-detection (TLD) algorithm for long-term single-target tracking of moving vehicle from video streams. The problems leading to tracking failures in existing TLD methods are discovered, and an improved TLD (ITLD) tracking algorithm is proposed which is more robust to object occlusion and illumination variation. A square root cubature Kalman filter (SRCKF) is employed in the tracker of TLD to predict the position of the object when occlusion occurs. Besides, this study introduces fast retina keypoint (FREAK) feature into the tracker to alleviate the instability caused by illumination variation or scale variation. The overlap comparison and the normalised cross-correlation coefficient (NCC) are introduced to the integrator of the TLD to obtain reliable bounding boxes with improved tracking precision. Experiments are conducted to compare the performance of the state-of-the-art trackers and the proposed method, using the object tracking benchmark that includes 50 video sequences (OTB-50) and TLD datasets. The experimental results show that the proposed ITLD outperforms on both tracking accuracy and robustness. The proposed method can track a moving vehicle even when it is temporally totally occluded.																	1751-9632	1751-9640				DEC	2019	13	8					730	741		10.1049/iet-cvi.2018.5787													
J								Adaptive learning feature pyramid for object detection	IET COMPUTER VISION										feature extraction; image representation; image fusion; object detection; learning (artificial intelligence); convolutional neural nets; adaptive learning feature pyramid; inconsistent detection performance; object detection models; feature pyramid network; FPN; multiscale feature maps; feature fusion strategy; learning ability; suboptimal performance; cross-scale feature fusion network; CSFF; low-level location feature maps; high-level semantic feature maps; deconvolution layer; feature map; base network; end-to-end training manner; convolutional neural network		Inconsistent detection performance for objects of different scales lies in many state-of-the-art object detection models. The feature pyramid network (FPN) alleviates this problem by fusing multi-scale feature maps through a top-down path. However, the features fusion strategy used in FPN lacks learning ability, which may result in suboptimal performance of the model. In this study, the authors propose a cross-scale feature fusion network (CSFF) to fuse the low-level location feature maps with the high-level semantic feature maps. The CSFF first embeds a dilated convolution and deconvolution layer into the top-down path of the FPN to enhance the learning ability of feature fusion. After that, an attention module is applied to suppress distraction and interference in the feature map. Each component of the CSFF is highly decoupled and can easily cooperate with a base network in an end-to-end training manner. In this study, they combine the CSFF with faster region with convolutional neural network and conduct a series of experiments on the PASCAL VOC 2007 and 2012 object detection datasets. Without any bells and whistles, the CSFF achieves a considerable detection improvement over the baseline network.																	1751-9632	1751-9640				DEC	2019	13	8					742	748		10.1049/iet-cvi.2018.5654													
J								Neural aesthetic image reviewer	IET COMPUTER VISION										regression analysis; image processing; feature extraction; image classification; learning (artificial intelligence); Neural aesthetic image reviewer; image aesthetics; high- score; low-aesthetic score; Neural Aesthetic Image Reviewer; plausible rating score; shared aesthetically semantic layers; AVA-Reviews dataset; aesthetic images	PHOTO	Recently, there is a rising interest in perceiving image aesthetics. The existing works deal with image aesthetics as a classification or regression problem. To extend the cognition from rating to reasoning, a deeper understanding of aesthetics should be based on revealing why a high- or low-aesthetic score should be assigned to an image. From such a point of view, the authors propose a model referred to as Neural Aesthetic Image Reviewer, which can not only give an aesthetic score for an image, but also generate a textual description explaining why the image leads to a plausible rating score. Specifically, they propose three models based on shared aesthetically semantic layers and task-specific embedding layers at a high level for performance improvement on different tasks. To facilitate researches on this problem, they collect the AVA-Reviews dataset, which contains 52,118 images and 312,708 comments in total. Through multi-task learning, the proposed models can rate aesthetic images as well as produce comments in an end-to-end manner. It is confirmed that the proposed models outperform the baselines according to the performance evaluation on the AVA-Reviews dataset. Moreover, they demonstrate experimentally that the authors' model can generate textual reviews related to aesthetics, which are consistent with human perception.																	1751-9632	1751-9640				DEC	2019	13	8					749	758		10.1049/iet-cvi.2019.0361													
J								Data envelopment analysis with neutrosophic inputs and outputs	EXPERT SYSTEMS										data envelopment analysis; interval efficiency; neutrosophic theory; triangular neutrosophic number	DECISION-MAKING UNITS; ANALYSIS DEA; FUZZY DATA; EFFICIENCY; RANKING; PERFORMANCE	Since the recent appearance of neutrosophic theory as a generalization of fuzzy and intuitionistic fuzzy theories, many multicriteria decision methods have adopted this theory to deal with incomplete and indeterminate data. However, it has not yet been applied to the data envelopment analysis (DEA) methodology. Therefore, this study presents a DEA model with triangular neutrosophic inputs and outputs that considers the truth, indeterminacy, and falsity degrees of each data value. As an alternative, a parametric approach based on what we term the variation degree of a triangular neutrosophic number is developed. This approach transforms a neutrosophic DEA model into an interval DEA model that can be solved using one of many existing techniques. Interval efficiency scores obtained from our numerical example show the flexibility and authenticity of the proposed approach.																	0266-4720	1468-0394				DEC	2019	36	6							e12453	10.1111/exsy.12453													
J								Text analytics for big data using rough-fuzzy soft computing techniques	EXPERT SYSTEMS										big data; clustering; feature selection; fuzzy set; rough set; text mining		Text mining or analytics is important for various applications such as market analysis and biomedical purposes because it enables the efficient retrieval of information from large datasets. During the analysis, increasing the dimensionality of the data reduces the performance of an entire system because doing so may retrieve irrelevant text, which creates errors. Therefore, this paper introduces big data and data mining techniques to analyse large volumes of information while mining texts, emails, blogs, online forums, news, and call centre documents. Initially, the data are collected from various sources that contain noise, which is removed by applying normalization techniques. Data mining techniques eliminate the irrelevant information and noise, and the relevant features are selected using the rough set-based particle swarm optimization algorithm. The selected features are formed as a cluster using a fuzzy set with the particle swarm optimization algorithm, which improves the efficiency of the mining process. Then, the efficiency of the system is evaluated using the University of California Irvine Machine Learning Repository knowledge process mining database, along with the sum of the intra cluster distances, the mean squared error rate, and the accuracy.																	0266-4720	1468-0394				DEC	2019	36	6							e12463	10.1111/exsy.12463													
J								Alternating optimization to solve penalized regression-based clustering model	EXPERT SYSTEMS										DC-ADMM; DC-CD; penalized regression-based clustering (PRClust); sum-of-norm (SON) clustering	FUZZY C-MEANS	Two previously proposed heuristic algorithms for solving penalized regression-based clustering model (PRClust) are (a) an algorithm that combines the difference-of-convex programming with a coordinate-wise descent (DC-CD) algorithm and (b) an algorithm that combines DC with the alternating direction method of multipliers (DC-ADMM). In this paper, a faster method is proposed for solving PRClust. DC-CD uses p x n x (n - 1)/2 slack variables to solve PRClust, where n is the number of data and p is the number of their features. In each iteration of DC-CD, these slack variable and cluster centres are updated using a second-order cone programming (SOCP). DC-ADMM uses p x n x (n - 1) slack variables. In each iteration of DC-ADMM, these slack variables and cluster centres are updated using ADMM. In this paper, PRClust is reformulated into an equivalent model to be solved using alternating optimization. Our proposed algorithm needs only n x (n - 1)/2 slack variables, which is much less than that of DC-CD and DC-ADMM and updates them analytically using a simple equation in each iteration of the algorithm. Our proposed algorithm updates only cluster centres using an SOCP. Therefore, our proposed SOCP is much smaller than that of DC-CD, which is used to update both cluster centres and slack variables. Experimental results on real datasets confirm that our proposed method is faster and much faster than DC-ADMM and DC-CD, respectively.																	0266-4720	1468-0394				DEC	2019	36	6							e12462	10.1111/exsy.12462													
J								A clustering-based feature selection framework for handwritten Indic script classification	EXPERT SYSTEMS										feature selection; handwritten text; Indic scripts; K-means clustering; modified log-Gabor transform; script classification; symmetrical uncertainty	HARMONY SEARCH; IDENTIFICATION; ALGORITHM; SYSTEM	In India, which has numerous officially recognized scripts, there is a primary need for categorizing the documents on the basis of the scripts used therein. Identification of script used in a document is essential for its effective handling both manually and digitally. Identification of script in a document image is an important research problem in the pattern recognition field, which, at times, suffers from the issue of growing dimensionality of the feature vector and requires an efficient feature selection technique. Keeping this fact in mind, in this paper, we propose a clustering-based filter feature selection framework in order to extract an optimal and effective feature subset from the original feature vector. The present feature selection methodology is evaluated on a script classification problem involving handwritten documents in 12 major Indic scripts. Experiments are done at word-level, text-line-level, and block-level. Experiments demonstrate that a reasonable increment in classification accuracy has been realized using comparatively lesser number of features. The proposed framework for feature selection is computationally inexpensive and can be applied to other pattern recognition problems as well.																	0266-4720	1468-0394				DEC	2019	36	6							e12459	10.1111/exsy.12459													
J								A cumulative belief degree approach for group decision-making problems with heterogeneous information	EXPERT SYSTEMS										cumulative belief degree; heterogeneous information; multiple attribute group decision making	LINGUISTIC PREFERENCE RELATIONS; CONSENSUS MODEL; AGGREGATION OPERATORS; LOCATION SELECTION; FUZZY NUMBERS; TOPSIS; FUSION; WEIGHTS; SYSTEM; SETS	In some complex group decision-making (GDM) problems, the information needing to be processed may be heterogeneous. This may involve consideration of objective and subjective criteria by experts who have their own particular set of criteria, their own preference format for assessing alternatives under these criteria, and who may themselves be assigned differing importance weights as experts. This paper presents a cumulative belief degree approach to cope with heterogeneous information in multiple attribute GDM problems. The proposed approach focuses to aggregate subjective expert assessments and objective criteria that are presented in various representation formats and scales. The methodology employs transformation formulae for several preference representation scales to belief structure, including 2-tuple representation, classical fuzzy sets, hesitant fuzzy sets, and intuitionistic fuzzy sets. Aggregation formulae are proposed to combine expert criteria evaluations and find a collective preference. A consensus degree is calculated for measuring the agreement between the experts. An illustrative example is presented to clarify the steps of the methodology, and validity of the approach is assured through comparative analysis with the existing methods.																	0266-4720	1468-0394				DEC	2019	36	6							e12458	10.1111/exsy.12458													
J								A novel end-to-end deep learning scheme for classifying multi-class motor imagery electroencephalography signals	EXPERT SYSTEMS										brain computer interface; deep learning; deep belief network; stack sparse autoencoder; multi-class classification	BRAIN-COMPUTER INTERFACE; NEURAL-NETWORKS; CHANNEL SELECTION; DAMAGE DETECTION; EEG; CLASSIFICATION; METHODOLOGY; COMBINATION; PATTERNS; NOISE	An important subfield of brain-computer interface is the classification of motor imagery (MI) signals where a presumed action, for example, imagining the hands' motions, is mentally simulated. The brain dynamics of MI is usually measured by electroencephalography (EEG) due to its noninvasiveness. The next generation of brain-computer interface systems can benefit from the generative deep learning (GDL) models by providing end-toend (e2e) machine learning and increasing their accuracy. In this study, to exploit the e2e-property of deep learning models, a novel GDL methodology is proposed where only minimal objective-free preprocessing steps are needed. Furthermore, to deal with the complicated multi-class MI-EEG signals, an innovative multilevel GDL-based classifying scheme is proposed. The effectiveness of the proposed model and its robustness against noisy MI-EEG signals is evaluated using two different GDL models, that is, deep belief network and stacked sparse autoencoder in e2e manner. Experimental results demonstrate the effectiveness of the proposed methodology with improved accuracy compared with the widely used filter bank common spatial patterns algorithm.																	0266-4720	1468-0394				DEC	2019	36	6							e12494	10.1111/exsy.12494													
J								A < word, part of speech > embedding model for text classification	EXPERT SYSTEMS										embedding model; part of speech; text classification; word embeddings		Existing word embeddings learning algorithms only employ the contexts of words, but different text documents use words and their relevant parts of speech very differently. Based on the preceding assumption, in order to obtain appropriate word embeddings and further improve the effect of text classification, this paper studies in depth a representation of words combined with their parts of speech. First, using the parts of speech and context of words, a more expressive word embeddings can be obtained. Further, to improve the efficiency of look-up tables, we construct a two-dimensional table that is in the <word, part of speech> format to represent words in text documents. Finally, the two-dimensional table and a Bayesian theorem are used for text classification. Experimental results show that our model has achieved more desirable results on standard data sets. And it has more preferable versatility and portability than alternative models.																	0266-4720	1468-0394				DEC	2019	36	6							e12460	10.1111/exsy.12460													
J								Evaluating the short-term effect of cross-market discounts in purchases using neural networks: A case in retail sector	EXPERT SYSTEMS										cross-market discounts; linear regression; neural networks; regression trees; retail promotions; support vector regression; time series	NOVELTY DETECTION; PRICE PROMOTIONS; TIME-SERIES; BRAND; STRATEGY; DEMAND; UNCERTAINTY; CONFIDENCE; ENSEMBLE; CATEGORY	Promotional tools such as cross-market discounts have been increasingly used as a means to increase customer satisfaction and sales. This paper aims to assess whether the implementation of a cross-market discount campaign by a retailing company encouraged customers to increase their purchases level. It contributes to the literature by using neural networks to detect novelties in a real context involving crossmarket discounts. Besides the computation of point predictions, the methodology proposed involves the estimation of neural networks prediction intervals. Sales predictions are compared with the observed values in order to detect significant changes in customers' spending. The use of neural networks is validated through the comparison with the forecasting estimates of support vector regression, regression trees, and linear regression. The results reveal that the promotional campaign under analysis did not significantly impact the sales of the rewarded customers.																	0266-4720	1468-0394				DEC	2019	36	6							e12452	10.1111/exsy.12452													
J								Ranking manufacturing processes from the quality management perspective in the automotive industry	EXPERT SYSTEMS										automotive industry; ELECTRE III; fuzzy group decision making; fuzzy sets; quality performance	GROUP DECISION-MAKING; PERFORMANCE EVALUATION; INTEGRATED PRODUCT; ELECTRE METHOD; FUZZY NUMBERS; SELECTION; TOPSIS; DESIGN; SAFETY; MODEL	The aim of this study is to propose a fuzzy decision-making model to rank manufacturing processes from the quality management perspective in the automotive industry. This paper proposes a model for improving quality management through the assessment and ranking of manufacturing subprocesses with respect to key performance indicators (KPIs). The developed model, supported with the fuzzy extended ELECTRE III, allows for the determination of subprocesses' rank. An illustrative example indicates that the proposed model could be very useful in everyday business operations as total quality management asset. The model can handle all uncertain and vague input data by applying the theory of fuzzy sets. The research also suggests different managerial implications because it provides an adequate tool for overall quality improvement. The number of treated KPIs is relatively high, so ELECTRE III method gives an advantage over other multicriteria analysis methods because it embraces less subjective thinking and demands slightly less experts' knowledge during the process of decision making and assessment.																	0266-4720	1468-0394				DEC	2019	36	6							e12451	10.1111/exsy.12451													
J								Stability analysis in identification of interval type-2 adaptive neuro-fuzzy inference system: Contribution to a novel Lyapunov function	EXPERT SYSTEMS										gradient descent; identification; IT2ANFIS; Lyapunov function; recursive least square	PARTICLE SWARM OPTIMIZATION; LEARNING ALGORITHM; TRAINING ANFIS; LOGIC SYSTEMS; DESIGN; NETWORKS; SUGENO	In recent years, control research has strongly highlighted the issue of training stability in the identification of non-linear systems. This paper investigates the stability analysis of an interval type-2 adaptive neuro-fuzzy inference system (IT2ANFIS) as an identifier through a novel Lyapunov function. In so doing, stability analysis is initially conducted on the IT2ANFIS identifier, while performing the online training of both the antecedent and the consequent parameters by the gradient descent (GD) algorithm. In addition, the same stability analysis is carried out when the antecedent and the consequent parameters are trained by GD and forgetting factor recursive least square (FRLS) algorithms, respectively (GD + FRLS). A novel Lyapunov function is proposed in this study in order for the identifier stability to attain the required conditions. These conditions determine the permissible boundaries for the covariance matrix and the learning rates at every iteration of the identification procedure. Stability analysis reveals that wide range of learning rates is obtained. Furthermore, simulation results indicate that when the permissible boundaries are selected according to the proposed stability analysis, a stable identification process with appropriate performance is achieved.																	0266-4720	1468-0394				DEC	2019	36	6							e12457	10.1111/exsy.12457													
J								Cluster validation in clustering-based one-class classification	EXPERT SYSTEMS										clustering; masquerade detection; one-class classification; validity index	ONE-CLASS CLASSIFIERS; STATISTICAL COMPARISONS; MASQUERADE DETECTION; NOVELTY DETECTION; ENSEMBLE; VARIANCE; MODEL; BIAS	Reconstruction-based one-class classification has shown to be very effective in a number of domains. This approach works by attempting to capture the underlying structure of the normal class, typically, by means of clusters of objects. It has the main disadvantage, however, that one has to indicate the number of clusters in advance, for this yields an efficient way of computing a clustering. In this paper, we introduce a new algorithm, OCKRA++, which achieves a better performance, by enhancing a clustering-based one-class ensemble classifier (OCKRA) with a cluster validity index that is used to set the best number of clusters during the classifier's training process. We have thoroughly tested OCKRA++ in a particular domain, namely masquerade detection. For this purpose, we have used the Windows-Users and-Intruder simulation Logs data set repository, which contains 70 different masquerade data sets. We have found that OCKRA++ is currently the algorithm that achieves the best area under the curve, with a significant difference, in masquerade detection using the file system navigation approach.																	0266-4720	1468-0394				DEC	2019	36	6							e12475	10.1111/exsy.12475													
J								Allocating natural resource reduction amounts: A data envelopment analysis based-approach considering production technology heterogeneity	EXPERT SYSTEMS										data envelopment analysis; natural resource conservation; production technology heterogeneity; reduction amount allocation; setting targets	DIRECTIONAL DISTANCE FUNCTION; CO2 EMISSION PERFORMANCE; DEA-BASED APPROACH; META-FRONTIER; ENVIRONMENTAL EFFICIENCY; ENERGY EFFICIENCY; FIXED COST; CHINA; GAPS	Previous studies resource allocation methods based on data envelopment analysis assume that all the assessed decision-making units share a common production technology, and all decision-making units become efficient after the resources are allocated. However, in the real world, production technology tends to be heterogeneous among the decision-making units because of the differences in economic development, geographic location, and market conditions. Correspondingly, when some decision-making units are far away from the efficient frontier, they may not become efficient easily using the resources allocated to them. In this paper, we propose a data envelopment analysis-based approach which considers production technology heterogeneity among decision-making units when allocating resource reduction amounts to each. In our model, the decision-making units are divided into subgroups based on their economic development level, an important indicator directly reflecting each decision-making unit's production technology level. Each subgroup has its specific production technology, and the decision-making units in the same subgroup have a similar technology level, which allows better identification of how the production of those decision-making units can change when their resource inputs change. We present an empirical example using China's mainland provinces as decision-making units to demonstrate the practicability and applicability of our proposed model.																	0266-4720	1468-0394				DEC	2019	36	6							e12449	10.1111/exsy.12449													
J								Cascading failures in complex networks caused by overload attacks	JOURNAL OF HEURISTICS										Cascading failures; Complex networks; Overload attack		Complex networks are known to be vulnerable to the failure of components in terms of structural robustness. An as yet less researched topic is dynamical robustness, which refers to the ability of a network to maintain its dynamical activity against local disturbances. This paper introduces a new type of attack-the overload attack-to disturb the network's dynamical activity. The attack is based on the load redistribution model for sequential attacks. The main contribution are heuristics to assess the vulnerability of complex networks with respect to this type of attack. The effectiveness of the heuristics is demonstrated with an application for real power networks.																	1381-1231	1572-9397				DEC	2019	25	6					837	859		10.1007/s10732-019-09413-0													
J								An evolutionary hybrid search heuristic for monitor placement in communication networks	JOURNAL OF HEURISTICS										Network monitoring; Evolutionary algorithm; Local search; Hybrid search heuristic; Population injection	LOCAL SEARCH; ALGORITHM; OPTIMIZATION; CENTRALITY	In this paper, a heuristic method for the optimal placement of monitors in communication networks is proposed. In order to be able to make informed decisions, a first step towards securing a communication network is deploying an adequate sensor infrastructure. However, appropriate monitoring should take into account the priority of the communication links as well as the location of monitors. The goal is to cover the whole network with the minimum investment and impact on performance, i.e., the optimal amount and positions of monitors in the network. In order to be able to counteract dynamic changes in those networks, e.g., link failures, attacks, or entering and leaving nodes, this work focuses on swiftly obtaining results having an acceptable quality. To achieve this goal, an effective hybrid search heuristic is introduced, combining the computational efficiency of a greedy local search method with the robustness of evolution-based heuristics. It is shown that this approach works well on synthetic benchmark instances and real-world network models, having up to millions of nodes, by comparing the performance of a common evolutionary algorithm (EA) to its hybrid search counterparts. It is observed that the hybrid search heuristics produce good solutions on the instances under study in a reasonable amount of time. Regarding the fitness of the solutions found, the hybrid approach outperforms the common EA in all the experiments. Moreover, on all problem instances, the hybrid EA finds the best solutions significantly earlier in the search process, which is key when monitoring a communication infrastructure which is subject to change.																	1381-1231	1572-9397				DEC	2019	25	6					861	899		10.1007/s10732-019-09414-z													
J								Dynamic heuristic acceleration of linearly approximated SARSA(lambda): using ant colony optimization to learn heuristics dynamically	JOURNAL OF HEURISTICS										Dynamic heuristics; Reinforcement learning; Ant colony optimization	REINFORCEMENT; ALGORITHM	Heuristically accelerated reinforcement learning (HARL) is a newfamily of algorithms that combines the advantages of reinforcement learning (RL) with the advantages of heuristic algorithms. To achieve this, the action selection strategy of the standard RL algorithm is modified to take into account a heuristic running in parallel with the RL process. This paper presents two approximated HARL algorithms that make use of pheromone trails to improve the behaviour of linearly approximated SARSA(lambda) by dynamically learning a heuristic function through the pheromone trails. The proposed dynamic algorithms are evaluated in comparison to linearly approximated SARSA(lambda), and heuristically accelerated SARSA(lambda) using a static heuristic in three benchmark scenarios: the mountain car, the mountain car 3D and the maze scenarios.																	1381-1231	1572-9397				DEC	2019	25	6					901	932		10.1007/s10732-019-09408-x													
J								An application of generalised simulated annealing towards the simultaneous modelling and clustering of glaucoma	JOURNAL OF HEURISTICS										Generalised simulated annealing; Visual field; Glaucoma; Optimisation	VISUAL-FIELD; OPTIMIZATION; ALGORITHM; SELECTION; SEVERITY	Optimisation methods are widely used in complex data analysis, and as such, there is a need to develop techniques that can explore huge search spaces in an efficient and effective manner. Generalised simulated annealing is a continuous optimisation method which is an advanced version of the commonly used simulated annealing technique. The method is designed to search for the global optimum solution and avoid being trapped in local optima. This paper presents an application of a specially adapted generalised simulated annealing algorithm applied to a discrete problem, namely simultaneous modelling and clustering of visual field data. Visual field data is commonly used in managing glaucoma, a disease which is the second largest cause of blindness in the developing world. The simultaneous modelling and clustering is a model based clustering technique aimed at finding the best grouping of visual field data based upon prediction accuracy. The results using our tailored optimisation method show improvements in prediction accuracy and our proposed method appears to have an efficient search in terms of convergence point compared to traditional techniques. Our method is also tested on synthetic data and the results verify that generalised simulated annealing locates the optimal clusters efficiently as well as improving prediction accuracy.																	1381-1231	1572-9397				DEC	2019	25	6					933	957		10.1007/s10732-019-09415-y													
J								Creating dispatching rules by simple ensemble combination	JOURNAL OF HEURISTICS										Genetic programming; Dispatching rules; Unrelated machines environment; Ensemble learning; Scheduling	SCHEDULING HEURISTICS; DESIGN	Dispatching rules are often the method of choice for solving scheduling problems since they are fast, simple, and adaptive approaches. In recent years genetic programming has increasingly been used to automatically create dispatching rules for various scheduling problems. Since genetic programming is a stochastic approach, it needs to be executed several times to ascertain that good dispatching rules were obtained. This paper analyses whether combining several dispatching rules into an ensemble leads to performance improvements over the individual dispatching rules. Two methods for creating ensembles of dispatching rules, based on the sum and vote methods applied in machine learning, are used and their effectiveness is analysed with regards to the size of the ensemble, the genetic programming method used to generate the dispatching rules, the size of the evolved dispatching rules, and the method used for creating the ensembles. The results demonstrate that the generated ensembles achieve significant improvements over individual automatically generated dispatching rules.																	1381-1231	1572-9397				DEC	2019	25	6					959	1013		10.1007/s10732-019-09416-x													
J								Predicting effective control parameters for differential evolution using cluster analysis of objective function features	JOURNAL OF HEURISTICS										Tuning evolutionary algorithms; Differential evolution; Unsupervised learning; Optimization methods; Evolutionary algorithms	OPTIMIZATION	A methodology is introduced which uses three simple objective function features to predict effective control parameters for differential evolution. This is achieved using cluster analysis techniques to classify objective functions using these features. Information on prior performance of various control parameters for each classification is then used to determine which control parameters to use in future optimisations. Our approach is compared to state-of-the-art adaptive and non-adaptive techniques. Two accepted bench mark suites are used to compare performance and in all cases we show that the improvement resulting from our approach is statistically significant. The majority of the computational effort of this methodology is performed off-line, however even when taking into account the additional on-line cost our approach outperforms other adaptive techniques. We also investigate the key tuning parameters of our methodology, such as number of clusters, which further support the finding that the simple features selected are predictors of effective control parameters. The findings presented in this paper are significant because they show that simple to calculate features of objective functions can help to select control parameters for optimisation algorithms. This can have an immediate positive impact on the application of these optimisation algorithms on real world problems, where it is often difficult to select effective control parameters.																	1381-1231	1572-9397				DEC	2019	25	6					1015	1031		10.1007/s10732-019-09419-8													
J								An improved differential evolution algorithm for solving a distributed assembly flexible job shop scheduling problem	MEMETIC COMPUTING										Distributed assembly flexible job shop scheduling problem; Improved differential evolution algorithm; Balanced algorithm; Earliness/tardiness; Cost	SEARCH	The single-factory manufacturing is gradually transiting to the multi-factory collaborative production with the globalization. The decentralization of resources and the heterogeneity of the production modes make it difficult to solve this kind of problem. Therefore, the distributed assembly flexible job shop scheduling problem (DAFJSP) is studied. DAFJSP can be decomposed into several flexible job shop scheduling problems and several single machine factory scheduling problems. To begin with, a mixed integer linear programming model for the DAFJSP is formulated to minimize the earliness/tardiness and the total cost simultaneously. Then, an improved differential evolution simulated annealing algorithm (IDESAA) is proposed. The balanced scheduling algorithm is designed to trade off the two objectives. Two crossover and mutation operators are designed. Due to its strong robustness, simulated annealing is integrated to local search the best Pareto solutions. The greedy idea combined with the Non-Dominated Sorted selection is employed to select the offspring. Finally, comprehensive experiments are conducted and the results show that the proposed algorithm can solve DAFJSP effectively and efficiently.																	1865-9284	1865-9292				DEC	2019	11	4					335	355		10.1007/s12293-018-00278-7													
J								An intelligent scheduling algorithm for complex manufacturing system simulation with frequent synchronizations in a cloud environment	MEMETIC COMPUTING										Frequent synchronizations; Intelligent manufacturing; Manufacturing system; Performance estimation; Parallel and distributed simulation; Resource allocation	GENETIC ALGORITHM	For cloud-based, large-scale complex manufacturing system simulation (CMSS), allocating appropriate service instances (virtual machines or nodes) is a promising way to improve execution efficiency. However, the complex interactions among and frequent aperiodic synchronizations of the entities of a CMSS make it challenging to estimate the influence of service instances' computing power and network latency on the execution efficiency. This hinders the appropriate allocation of service instances for CMSS. To solve this problem, we construct a performance estimation model (PEM) using the executed events and synchronization algorithms to evaluate the running time of CMSS on different service instance combinations. Further, an intelligent scheduling algorithm that introduces PEM as fitness function is proposed to search for a near-optimal allocation scheme of CMSS service instances. To be specific, the PEM-based optimization algorithm (PEMOA) incorporates simulated annealing into the mutation phase of a genetic algorithm to strengthen its local searching ability. A series of experiments were performed on a computer cluster to compare the proposed PEMOA with two representative algorithms: an adapted first-come-first-service-based and the max-min-based allocation algorithms. The experimental results demonstrate that the PEMOA can reduce the running time by more than 7%. In particular, the improvement of PEMOA increases when the manufacturing system simulation is communication-intensive or spans a small number of service instance combinations.																	1865-9284	1865-9292				DEC	2019	11	4					357	370		10.1007/s12293-019-00284-3													
J								Mathematical modeling and a discrete artificial bee colony algorithm for the welding shop scheduling problem	MEMETIC COMPUTING										Welding; Scheduling; Discrete artificial bee colony algorithm; Welding shop scheduling	OPTIMIZATION	The welding process which is one of the most important assembly processes is widespread in the modern manufacturing industry, including aerospace, automotive and engineering machinery. The welding shop scheduling greatly impacts the efficiency of whole production system. However, few studies on the welding shop scheduling problem (WSSP) were reported. In this paper, a mathematical model and an improved discrete artificial bee colony algorithm (DABC) are proposed for the WSSP. Firstly, it is defined where multi-machine can process one job at the same time in the WSSP. Secondly, the mathematical models of WSSP have been constructed. Thirdly, an effective DABC is proposed to solve the WSSP, considering job permutation and machine allocation simultaneously. To improve the performance of proposed DABC algorithm, the effective operators have been designed. Three instances with different scales are used to evaluate the effectiveness of proposed algorithm. The comparisons with other two algorithms including genetic algorithm and grey wolf optimizer are also provided. Experimental results show that the proposed model and algorithm achieve good performance. Finally, the proposed model and DABC algorithm are applied in a real-world girder welding shop from a crane company in China. The results show that proposed model and algorithm reduces 55.17% production time comparing with the traditional algorithm and the scheduled machine allocation provides more reasonable arrangements for workers and machine loads.																	1865-9284	1865-9292				DEC	2019	11	4					371	389		10.1007/s12293-019-00283-4													
J								Multi-objective flow shop scheduling with limited buffers using hybrid self-adaptive differential evolution	MEMETIC COMPUTING										Parameter self-adaptive; Differential evolution; Local search operators; Flow shop scheduling; Multi-objective optimization	ALGORITHM; OPTIMIZATION; TARDINESS	In this paper, a self-adaptive differential evolution (DE) algorithm is designed to solve multi-objective flow shop scheduling problems with limited buffers (FSSPwLB). The makespan and the largest job delay are treated as two separate objectives which are optimized simultaneously. To improve the performance of the proposed algorithm and eliminate the difficulty of setting parameters, an adaptive mechanism is designed and incorporated into DE. Moreover, various local search and hybrid metaheuristic methods are presented and compared to improve the convergence. Through the analysis of the experimental results, the proposed algorithm is able to tackle the FSSPwLB problems effectively by generating superior and stable scheduling strategies.																	1865-9284	1865-9292				DEC	2019	11	4					407	422		10.1007/s12293-019-00290-5													
J								Estimation of distribution evolution memetic algorithm for the unrelated parallel-machine green scheduling problem	MEMETIC COMPUTING										Green scheduling; Unrelated parallel-machine; Estimation of distribution algorithm	FLEXIBLE JOB-SHOP; OPTIMIZATION; CONSUMPTION; MAKESPAN	With the increasing concern on greenhouse gas emissions, green scheduling decision in the manufacturing factory is gaining more and more attention. This paper addresses the unrelated parallel machine green scheduling problem (UPMGSP) with criteria ofminimizing themakespan and the total carbon emission. To solve the problem, the estimation of distribution evolution memetic algorithm (EDEMA) is proposed. Firstly, based on the minimum machine load first principle, the initialization of the population is proposed. Second, a multi-objective non-dominated sorting approach and the crowding distance are adopted to improve the diversity of individual. Third, to estimate the probability distribution of the solution space, a probability model is presented to enhance the searching ability. Third, five neighbourhood searching operators are designed to handle the job-to-machine assignment. Moreover, the population catastrophe is used to maintain the sustainable diversity of the population. Finally, based on the randomly generated instances of the UPMGSP, extensive computational tests are carried out. The obtained computational results show that the EDEMA has the better searching capability and the better objective value than those of the non-dominated sorting genetic algorithm II and the estimation of distribution evolution algorithm (EDEA) in solving the UPMGSP.																	1865-9284	1865-9292				DEC	2019	11	4					423	437		10.1007/s12293-019-00295-0													
J								Evaluation of word spotting under improper segmentation scenario	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION											TEXT LINE; RETRIEVAL; DOCUMENTS	Word spotting is an important recognition task in large-scale retrieval of document collections. In most of the cases, methods are developed and evaluated assuming perfect word segmentation. In this paper, we propose an experimental framework to quantify the goodness that word segmentation has on the performance achieved byword spotting methods in identical unbiased conditions. The framework consists of generating systematic distortions on segmentation and retrieving the original queries from the distorted dataset. We have tested our framework on several established and state-of-the-art methods using George Washington and Barcelona Marriage Datasets. The experiments done allow for an estimate of the end-to-end performance of word spotting methods.																	1433-2833	1433-2825				DEC	2019	22	4					361	374		10.1007/s10032-019-00338-9													
J								Patch-based offline signature verification using one-class hierarchical deep learning	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Offline signature verification; Patch-wise; Hierarchical convolutional neural network; One-class classification	SUPPORT	Automatic processing of offline signature verification (in general) can be considered as a low-cost solution to problems in biometrics in comparison with other solutions (e. g. fingerprint, face verification, etc.). This study aims to present a novel writer-dependent approach to verifying an individual's signature through offline image patches of their handwriting. The proposed approach is based on hierarchical one-class convolutional neural network for learning only genuine signatures with different feature levels. Since forgeries are not available for each user enrolled in a real application scenario, this study considers signature verification as a one-class problem. In addition, to achieve a clear structure in image, designing hierarchical network architecture based on the coarse-to-fine principle can lead to more precise results. With lower-level features, the network presents a higher visual quality at the boundary area revealing similarities between genuine signatures, while higher-level features can discriminate the quality of the pen strokes to predict forgeries from genuine signatures. The presented system was tested on two Persian databases (PHBC and UTSig) as well as two Latin databases (MCYT-75 and CEDAR). The results of the analyses produced by this method were generally better and more exact in terms of the four signature databases compared with the present state-of-the-art results.																	1433-2833	1433-2825				DEC	2019	22	4					375	385		10.1007/s10032-019-00331-2													
J								HWNet v2: an efficient word image representation for handwritten documents	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Word image representation; Handwritten and historical documents; Word spotting; Convolutional neural networks		We present a framework for learning an efficient holistic representation for handwritten word images. The proposed method uses a deep convolutional neural network with traditional classification loss. The major strengths of our work lie in: (i) the efficient usage of synthetic data to pre-train a deep network, (ii) an adapted version of the ResNet-34 architecture with the region of interest pooling (referred to as HWNet v2) which learns discriminative features for variable sized word images, and (iii) a realistic augmentation of training data with multiple scales and distortions which mimics the natural process of handwriting. We further investigate the process of transfer learning to reduce the domain gap between synthetic and real domain and also analyze the invariances learned at different layers of the network using visualization techniques proposed in the literature. Our representation leads to a state-of-the-art word spotting performance on standard handwritten datasets and historical manuscripts in different languages with minimal representation size. On the challenging IAM dataset, our method is first to report an mAP of around 0.90 for word spotting with a representation size of just 32 dimensions. Furthermore, we also present results on printed document datasets in English and Indic scripts which validates the generic nature of the proposed framework for learning word image representation.																	1433-2833	1433-2825				DEC	2019	22	4					387	405		10.1007/s10032-019-00336-x													
J								HanFont: large-scale adaptive Hangul font recognizer using CNN and font clustering	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Font recognition; Font clustering; Large-scale classification; Convolutional neural networks; Deep learning		We propose a large-scale Hangul font recognizer that is capable of recognizing 3300 Hangul fonts. Large-scale Hangul font recognition is a challenging task. Typically, Hangul fonts are distinguished by small differences in detailed shapes, which are often ignored by the recognizer. There are additional issues in practical applications, such as the existence of almost indistinguishable fonts and the release of new fonts after the training of the recognizer. Only a few recently developed font recognizers are scalable enough to recognize thousands of fonts, most of which focus on the fonts for western languages. The proposed recognizer, HanFont, is composed of a convolutional neural network (CNN) model designed to effectively distinguish the detailed shapes. HanFont also contains a font clustering algorithm to address the issues caused by indistinguishable fonts and untrained new fonts. In the experiments, HanFont exhibits a recognition rate of 94.11% for 3300 Hangul fonts including numerous similar fonts, which is 2.49% higher than that of ResNet. The cluster-level recognition accuracy of HanFont was 99.47% when the 3300 fonts were grouped into 1000 clusters. In a test on 100 new fonts without retraining the CNN model, HanFont exhibited 57.87% accuracy. The average accuracy for the top 56 untrained fonts was 75.76%.																	1433-2833	1433-2825				DEC	2019	22	4					407	416		10.1007/s10032-019-00337-w													
J								A novel feature transform framework using deep neural network for multimodal floor plan retrieval	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Floor plan; Convolutional neural network; Autoencoder; Cyclic GAN; Content-based image retrieval; Graphics recognition; Document image analysis; Pattern recognition		In recent past, there has been a steep increase in the use of online platforms for the search of desired products. Real estate industry is no exception and has started initiating rent/sale of houses through online platforms. In this paper, we propose a deep neural network framework to facilitate automatic search of homes based on their floor plans. The salient features of this framework are that the query can be either an image (existing floor plan) or a sketch through a sketch pad interface. Our proposed framework automatically determines the type of query (image or sketch) and retrieves similar floor plan images from the database. The critical contributions of our proposed approach are: (1) a novel unified floor plan retrieval framework using multimodal query, i.e., an intuitive and convenient sketch query mode as well as a query by example mode; (2) a conjunction of autoencoder, Cyclic GAN and CNN for the task of domain mapping and floor plan image retrieval. We have reported results of extensive experimentation and comparison with baseline results to establish the effectiveness of our approach.																	1433-2833	1433-2825				DEC	2019	22	4					417	429		10.1007/s10032-019-00340-1													
J								A survey paper on secret image sharing schemes	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Secret image sharing; Steganography; Threshold scheme; Visual cryptography; Watermarking	STEGANOGRAPHY; AUTHENTICATION; LESS	With the speedy progress of network technology and internet applications, protection of digitized data against unauthorized access and modification has become a paramount issue. In order to face this challenge, numerous secret image sharing schemes have been introduced. Secret image sharing scheme is a method used for safeguarding the sensitive digitized image against illegal copying and tempering. The secret image is divided into many random shares in such a way that each share does not reveal any information to the intruders. In this paper, we present a comprehensive survey from over 100 papers which explains the new approaches and challenges. This paper also provides a comparative analysis of different methods based on different properties.																	2192-6611	2192-662X				DEC	2019	8	4					195	215		10.1007/s13735-018-0161-3													
J								An efficient content-basedmedical image indexing and retrieval using local texture feature descriptors	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Local binary pattern (LBP); Threshold local binary AND pattern (TLBAP); Local adjacent neighborhood average difference pattern (LANADP); Feature extraction; Medical image retrieval	BINARY PATTERNS; COOCCURRENCE PATTERN; EXTREMA; CLASSIFICATION; COLOR; FACE; MRI; RECOGNITION	This paper presents an efficient medical image indexing and retrieval method using two new proposed feature descriptors named as threshold local binary AND pattern (TLBAP) and local adjacent neighborhood average difference pattern (LANADP). In basic local binary pattern (LBP), every center pixel is considered as a threshold to generate the binary pattern, whereas in the proposed method a threshold value is calculated using the highest pixel intensity of the neighboring pixels to construct the threshold local binary pattern (TLBP). Thereafter, logical AND operation is performed between LBP and TLBP pattern to produce TLBAP pattern. The objective of the other feature descriptor named here as LANADP is to explore the relationship of neighboring pixels with its adjacent neighbors in vertical, horizontal and diagonal directions. In the proposed work, both TLBAP and LANADP features are concatenated in the form of the histograms to generate the final features vector and the performance of the system is evaluated. To test the effectiveness of the proposed method, three publicly available medical image databases, namely OASIS-MRI brain images, NEMA-CT images and VIA/ELCAP-CT images, are used. Two measures, viz. average retrieval precision and average retrieval rate, have been used to evaluate the performance of the method proposed which is further compared with some existing local pattern-based methods. The experimental results show that the proposed methods give better results as compared to the other existing methods considered in this study.																	2192-6611	2192-662X				DEC	2019	8	4					217	231		10.1007/s13735-019-00176-9													
J								Content-based medical image retrieval of CT images of liver lesions using manifold learning	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Focal liver lesions; Manifold learning; Image retrieval; Liver tumors; Multi-phase CT images		Accurate retrieval of liver CT images can help a specialist to decide on the type of lesion and treatment planning. However, the complex texture of the abnormality and its nonlinear characteristic reduces the recognition rate of a retrieval system. In this paper, we propose how to represent an abnormal region of a liver by individual attributes of a multi-phase CT image. The indexing of a medical image database is represented by a correlation graph distance, which considers nonlinear behavior of the feature space as well. The results showed that the average recall was improved by 7.5% using the proposed feature vector. Concerning a complex scheme for lesion representation and the manifold indexing technique, the recall of the system was increased by twice. The proposed indexing and feature representation prove the potential of our method in content-based medical image retrieval systems.																	2192-6611	2192-662X				DEC	2019	8	4					233	240		10.1007/s13735-019-00179-6													
J								Neutrosophic soft set decision making for stock trending analysis	EVOLVING SYSTEMS										Neutrosophic soft sets; Soft sets; Stock trending; Stock parameters; Open; Close; High; Low; Adjacent close	AGGREGATION OPERATORS	In this paper, we point out a major issue of stock market regarding trending scenario of trades where data exactness, accuracy of expressing data and uncertainty of values (closing point of the day) are lacked. We use neutrosophic soft sets (NSS) consisting of three factors (True, Uncertainty and False) to deal with exact state of data in several directions. A new approach based on NSS is proposed for stock value prediction based on real data from last 7 years. It calculates the stock price based on the factors like "open", "high", "low" and "adjacent close". The highest score value retrieved from the score function is used to determine which opening price and high price decide the closing price from the above mentioned four factors.																	1868-6478	1868-6486				DEC	2019	10	4					621	627		10.1007/s12530-018-9247-7													
J								A Survey on 3D Visual Tracking of Multicopters	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Multicopter; three-dimensional (3D) visual tracking; camera placement; camera calibration; pose estimation	OPTIMAL CAMERA PLACEMENT; POSE ESTIMATION; SELF-CALIBRATION; NAVIGATION; SENSOR	Three-dimensional (3D) visual tracking of a multicopter (where the camera is fixed while the multicopter is moving) means continuously recovering the six-degree-of-freedom pose of the multicopter relative to the camera. It can be used in many applications, such as precision terminal guidance and control algorithm validation for multicopters. However, it is difficult for many researchers to build a 3D visual tracking system for multicopters (VTSMs) by using cheap and off-the-shelf cameras. This paper firstly gives an overview of the three key technologies of a 3D VTSMs: multi-camera placement, multi-camera calibration and pose estimation for multicopters. Then, some representative 3D visual tracking systems for multicopters are introduced. Finally, the future development of the 3D VTSMs is analyzed and summarized.																	1476-8186	1751-8520				DEC	2019	16	6					707	719		10.1007/s11633-019-1199-2													
J								Computational Psycholinguistics	COMPUTATIONAL LINGUISTICS																													0891-2017	1530-9312				DEC	2019	45	4					607	626		10.1162/COLI_a_00359													
J								Discourse in Multimedia: A Case Study in Extracting Geometry Knowledge from Textbooks	COMPUTATIONAL LINGUISTICS											INFORMATION EXTRACTION; GENERATION; DIAGRAM; TEXT	To ensure readability, text is often written and presented with due formatting. These text formatting devices help the writer to effectively convey the narrative. At the same time, these help the readers pick up the structure of the discourse and comprehend the conveyed information. There have been a number of linguistic theories on discourse structure of text. However, these theories only consider unformatted text. Multimedia text contains rich formatting features that can be leveraged for various NLP tasks. In this article, we study some of these discourse features in multimedia text and what communicative function they fulfill in the context. As a case study, we use these features to harvest structured subject knowledge of geometry from textbooks. We conclude that the discourse and text layout features provide information that is complementary to lexical semantic information. Finally, we show that the harvested structured knowledge can be used to improve an existing solver for geometry problems, making it more accurate as well as more explainable.																	0891-2017	1530-9312				DEC	2019	45	4					627	665		10.1162/COLI_a_00360													
J								Automatic Identification and Production of Related Words for Historical Linguistics	COMPUTATIONAL LINGUISTICS												Language change across space and time is one of the main concerns in historical linguistics. In this article, we develop tools to assist researchers and domain experts in the study of language evolution. First, we introduce a method to automatically determine whether two words are cognates. We propose an algorithm for extracting cognates from electronic dictionaries that contain etymological information. Having built a data set of related words, we further develop machine learning methods based on orthographic alignment for identifying cognates. We use aligned subsequences as features for classification algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates. Second, we extend the method to a finer-grained level, to identify the type of relationship between words. Discriminating between cognates and borrowings provides a deeper insight into the history of a language and allows a better characterization of language relatedness. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind. Third, we develop a machine learning method for automatically producing related words. We focus on reconstructing proto-words, but we also address two related sub-problems, producing modern word forms and producing cognates. The task of reconstructing proto-words consists of recreating the words in an ancient language from its modern daughter languages. Having modern word forms in multiple Romance languages, we infer the form of their common Latin ancestors. Our approach relies on the regularities that occurred when words entered the modern languages. We leverage information from several modern languages, building an ensemble system for reconstructing proto-words. We apply our method to multiple data sets, showing that our approach improves on previous results, also having the advantage of requiring less input data, which is essential in historical linguistics, where resources are generally scarce.																	0891-2017	1530-9312				DEC	2019	45	4					667	704		10.1162/COLI_a_00361													
J								Syntactically Meaningful and Transferable Recursive Neural Networks for Aspect and Opinion Extraction	COMPUTATIONAL LINGUISTICS												In fine-grained opinion mining, extracting aspect terms (a.k.a. opinion targets) and opinion terms (a.k.a. opinion expressions) from user-generated texts is the most fundamental task in order to generate structured opinion summarization. Existing studies have shown that the syntactic relations between aspect and opinion words play an important role for aspect and opinion terms extraction. However, most of the works either relied on predefined rules or separated relation mining with feature learning. Moreover, these works only focused on single-domain extraction, which failed to adapt well to other domains of interest where only unlabeled data are available. In real-world scenarios, annotated resources are extremely scarce for many domains, motivating knowledge transfer strategies from labeled source domain(s) to any unlabeled target domain. We observe that syntactic relations among target words to be extracted are not only crucial for single-domain extraction, but also serve as invariant "pivot" information to bridge the gap between different domains. In this article, we explore the constructions of recursive neural networks based on the dependency tree of each sentence for associating syntactic structure with feature learning. Furthermore, we construct transferable recursive neural networks to automatically learn the domain-invariant fine-grained interactions among aspect words and opinion words. The transferability is built on an auxiliary task and a conditional domain adversarial network to reduce domain distribution difference in the hidden spaces effectively in word level through syntactic relations. Specifically, the auxiliary task builds structural correspondences across domains by predicting the dependency relation for each path of the dependency tree in the recursive neural network. The conditional domain adversarial network helps to learn domain-invariant hidden representation for each word conditioned on the syntactic structure. In the end, we integrate the recursive neural network with a sequence labeling classifier on top that models contextual influence in the final predictions. Extensive experiments and analysis are conducted to demonstrate the effectiveness of the proposed model and each component on three benchmark data sets.																	0891-2017	1530-9312				DEC	2019	45	4					705	736		10.1162/COLI_a_00362													
J								Scalable Micro-planned Generation of Discourse from Structured Data	COMPUTATIONAL LINGUISTICS												We present a framework for generating natural language description from structured data such as tables; the problem comes under the category of data-to-text natural language generation (NLG). Modern data-to-text NLG systems typically use end-to-end statistical and neural architectures that learn from a limited amount of task-specific labeled data, and therefore exhibit limited scalability, domain-adaptability, and interpretability. Unlike these systems, ours is a modular, pipeline-based approach, and does not require task-specific parallel data. Rather, it relies on monolingual corpora and basic off-the-shelf NLP tools. This makes our system more scalable and easily adaptable to newer domains. Our system utilizes a three-staged pipeline that: (i) converts entries in the structured data to canonical form, (ii) generates simple sentences for each atomic entry in the canonicalized representation, and (iii) combines the sentences to produce a coherent, fluent, and adequate paragraph description through sentence compounding and co-reference replacement modules. Experiments on a benchmark mixed-domain data set curated for paragraph description from tables reveals the superiority of our system over existing data-to-text approaches. We also demonstrate the robustness of our system in accepting other popular data sets covering diverse data types such as knowledge graphs and key-value maps.																	0891-2017	1530-9312				DEC	2019	45	4					737	763		10.1162/COLI_a_00363													
J								Argument Mining: A Survey	COMPUTATIONAL LINGUISTICS											DIALOGUE; CARNEADES; AGREEMENT; TEXTS; MODEL	Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.																	0891-2017	1530-9312				DEC	2019	45	4					765	818		10.1162/COLI_a_00364													
J								How to Distinguish Languages and Dialects	COMPUTATIONAL LINGUISTICS											MUTUAL INTELLIGIBILITY	The terms "language" and "dialect" are ingrained, but linguists nevertheless tend to agree that it is impossible to apply a non-arbitrary distinction such that two speech varieties can be identified as either distinct languages or two dialects of one and the same language. A database of lexical information for more than 7,500 speech varieties, however, unveils a strong tendency for linguistic distances to be bimodally distributed. For a given language group the linguistic distances pertaining to either cluster can be teased apart, identifying a mixture of normal distributions within the data and then separating them fitting curves and finding the point where they cross. The thresholds identified are remarkably consistent across data sets, qualifying their mean as a universal criterion for distinguishing between language and dialect pairs. The mean of the thresholds identified translates into a temporal distance of around one to one-and-a-half millennia (1,075-1,635 years).																	0891-2017	1530-9312				DEC	2019	45	4					823	831		10.1162/COLI_a_00366													
J								Data-Based Optimal Control of Multiagent System A Reinforcement Learning Design Approach	IEEE TRANSACTIONS ON CYBERNETICS										Consensus; data-based control; optimal cooperative control; reinforcement learning	DIFFERENTIAL GRAPHICAL GAMES; ADAPTIVE OPTIMAL-CONTROL; SYNCHRONIZATION; FEEDBACK	This paper studies an optimal consensus tracking problem of heterogeneous linear multiagent systems. By introducing tracking error dynamics, the optimal tracking problem is reformulated as finding a Nash-equilibrium solution to multiplayer games, which can be done by solving associated coupled Hamilton-Jacobi equations. A data-based error estimator is designed to obtain the data-based control for the multiagent systems. Using the quadratic functional to approximate every agent's value function, we can obtain the optimal cooperative control by the input-output (I/O) Q-learning algorithm with a value iteration technique in the least-square sense. The control law solves the optimal consensus problem for multiagent systems with measured I/O information, and does not rely on the model of multiagent systems. A numerical example is provided to illustrate the effectiveness of the proposed algorithm.																	2168-2267	2168-2275				DEC	2019	49	12					4441	4449		10.1109/TCYB.2018.2868715													
J								A Too to Design Interactive Characters Based on Embodied Cognition	IEEE TRANSACTIONS ON GAMES										Artificial intelligence (AI); character modeling; decision making; virtual reality	BEHAVIOR	Creating interactive characters is one of the most challenging tasks of videogame design. In order to facilitate such an endeavor, we introduce a decisional and behavior synthesis architecture integrated in the game engine Unity3D. A distinguishing feature of this architecture is that it embraces embodied cognition principles and uses them as implementation requirements. From these, we derive an architecture, which is based on a novel combination of previously proposed systems, together with some simplifications. We also argue that the architecture proposed has properties-modularity, scalability, and stability-which can he beneficial for its practical industrial adoption, particularly in the context of the recent improvement in machine learning techniques. Artificial intelligence is a quite technical domain, and we believe such a tool can facilitate interactive character creation by creative minds in industrial applications.																	2475-1502	2475-1510				DEC	2019	11	4					311	319		10.1109/TCIAIG.2017.2755699													
J								Implementing Adaptive Game Difficulty Balancing in Serious Games	IEEE TRANSACTIONS ON GAMES										1.2.1 [Artificial intelligence]: Applications and Expert systems-games	ADAPTATION; EXPERIENCE	The ability to engage and retain players is perceived as a major factor in the success of games. However, the end-goal of retention differs between entertainment and serious contexts. For an entertainment game, engagement and retention are linked to monetization; for a serious game, this needs to persist for as long as is required for learning or behavioral objectives to he met. User engagement is strongest when a balance is achieved between difficulty and skill, leading to a state of "flow." Hence, adapting difficulty could lead to increased and sustained engagement. Implementing this requires the identification of variables linked to mechanics, manipulated based upon a player performance model. In some cases, this is possible by adjusting simple properties of objects, though more comprehensive solutions require extending or adapting content applying procedural techniques. This paper proposes a six step plan, validated against two case studies: an existing serious game, with easily manipulated parameters, and a platformer game built from scratch, where additional content is required, showing the process for different mechanics. To explore limitations, the results of two small-scale user evaluations with 45 users in total, are reported, contributing to the understanding of how adaptive difficulty might be implemented and received.																	2475-1502	2475-1510				DEC	2019	11	4					320	327		10.1109/TG.2018.2791019													
J								The ASC-Inclusion Perceptual Serious Gaming Platform for Autistic Children	IEEE TRANSACTIONS ON GAMES										AI in games; autism spectrum condition (ASC); emotion recognition; inclusion; virtual computerized environment	COMPLEX EMOTION RECOGNITION; BODY MOVEMENT; ASPERGER-SYNDROME; POINT-LIGHT; GAME; INTERVENTION; PEOPLE; SKILLS	"Serious games" are becoming extremely relevant to individuals who have specific needs, such as children with an autism spectrum condition (ASC). Often, individuals with an ASC have difficulties in interpreting verbal and nonverbal communication cues during social interactions. The ASC-Inclusion EU-FP7 funded project aims to provide children who have an ASC with a platform to learn emotion expression and recognition, through play in the virtual world. In particular, the ASC-Inclusion platform focuses on the expression of emotion via facial, vocal, and bodily gestures. The platform combines multiple analysis tools, using onboard microphone and webcam capabilities. The platform utilizes these capabilities via training games, text-based communication, animations, video, and audio clips. This paper introduces current findings and evaluations of the ASC-Inclusion platform and provides detailed description for the different modalities.																	2475-1502	2475-1510				DEC	2019	11	4					328	339		10.1109/TG.2018.2864640													
J								Inside the Group: Investigating Social Structures in Player Groups and Their Influence on Activity	IEEE TRANSACTIONS ON GAMES										Destiny; game analytics; matchmaking; social networks	NETWORKS	Social features, matchmaking, and grouping functions are key elements of online multiplayer experiences. Understanding how social connections form in and around games and their relationship to in-game activity offers insights for building and maintaining player bases and for improving engagement and retention. This paper presents an analysis of the groups formed by users of the the100 . io-a social matchmaking wehsite for different commercial titles, including Destiny on which we focus in this paper. Groups formed on the100 . io can be described across a range of social network related metrics. Also, the social network formed within a group is evaluated in combination with user-provided demographic and preference data. Archetypal analysis is used to classify groups into archetypes and a correlation analysis is presented covering the effect of group characteristics on in-game activity. Finally, weekly activity profiles are described. Our results indicate that group size as well as the number of moderators within a group and their connectedness to other team members influences a group's activity. We also identified four prototypical types of groups with different characteristics concerning composition, social cohesion, and activity.																	2475-1502	2475-1510				DEC	2019	11	4					416	425		10.1109/TG.2018.2858024													
J								Mixture of Joint Nonhomogeneous Markov Chains to Cluster and Model Water Consumption Behavior Sequences	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Nonhomogeneous Markov models; categorical time series; clustering; forecasting; water consumption behavior	DEMAND; CLASSIFICATION; CONSERVATION; ATTITUDES	The emergence of smart meters has fostered the collection of massive data that support a better understanding of consumer behaviors and better management of water resources and networks. The main focus of this article is to analyze consumption behavior over time; thus, we first identify the main weekly consumption patterns. This approach allows each meter to be represented by a categorical series, where each category corresponds to a weekly consumption behavior. By considering the resulting consumption behavior sequences, we propose a new methodology based on a mixture of nonhomogeneous Markov models to cluster these categorical time series. Using this method, the meters are described by the Markovian dynamics of their cluster. The latent variable that controls cluster membership is estimated alongside the parameters of the Markov model using a novel classification expectation maximization algorithm. A specific entropy measure is formulated to evaluate the quality of the estimated partition by considering the joint Markovian dynamics. The proposed clustering model can also be used to predict future consumption behaviors within each cluster. Numerical experiments using real water consumption data provided by a water utility in France and gathered over 19 months are conducted to evaluate the performance of the proposed approach in terms of both clustering and prediction. The results demonstrate the effectiveness of the proposed method.																	2157-6904	2157-6912				DEC	2019	10	6							71	10.1145/3347452													
J								Secure Deduplication System with Active Key Update and Its Application in IoT	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Deduplication; convergent encryption; key update; edge computing		The rich cloud services in the Internet of Things create certain needs for edge computing, in which devices should be able to handle storage tasks securely, reliably, and efficiently. When processing the storage requests from edge devices, each cloud server is supposed to eliminate duplicate copies of repeating data to reduce the amount of storage space and save on bandwidth. To protect data confidentiality while supporting deduplication, some convergent-encryption-based techniques have been proposed to encrypt the data before uploading. However, all these works cannot meet two requirements while preventing brute-force attacks: (i) power-constrained edge nodes should update encryption keys efficiently when an edge node is abandoned; and (ii) the access privacy of edge nodes should be guaranteed. In this article, we propose a novel encryption scheme for secure chunk-level deduplication. Based on this scheme, we present two constructions of the secure deduplication system that support an efficient key update protocol. The key update protocol does not involve any edge node in computational tasks, so that the deduplication system can adopt an active key update strategy. Moreover, one of our constructions, which is called advance construction, can provide access privacy assurances for edge nodes. The security analysis is given in terms of the proposed threat model. The experimental analysis demonstrates that the proposed deduplication system is practical.																	2157-6904	2157-6912				DEC	2019	10	6							69	10.1145/3356468													
J								Using Sparse Representation to Detect Anomalies in Complex WSNs	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Dependency relationships networks; Sparse Representation; anomaly detection; WSNs	NETWORKS; FRAMEWORK; PROTOCOL; SCHEMES	In recent years, wireless sensor networks (WSNs) have become an active area of research for monitoring physical and environmental conditions. Due to the interdependence of sensors, a functional anomaly in one sensor can cause a functional anomaly in another sensor, which can further lead to the malfunctioning of the entire sensor network. Existing research work has analysed faulty sensor anomalies but fails to show the effectiveness throughout the entire interdependent network system. In this article, a dictionary learning algorithm based on a non-negative constraint is developed, and a sparse representation anomaly node detection method for sensor networks is proposed based on the dictionary learning. Through experiment on a specific thermal power plant in China, we verify the robustness of our proposed method in detecting abnormal nodes against four state of the art approaches and proved our method is more robust. Furthermore, the experiments are conducted on the obtained abnormal nodes to prove the interdependence of multi-layer sensor networks and reveal the conditions and causes of a system crash.																	2157-6904	2157-6912				DEC	2019	10	6							64	10.1145/3331147													
J								Edge-enabled Disaster Rescue: A Case Study of Searching for Missing People	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Edge computing; disaster rescue; finding missing people; face recognition; time-efficient	INTERNET	In the aftermath of earthquakes, floods, and other disasters, photos are increasingly playing more significant roles, such as finding missing people and assessing disasters, in rescue and recovery efforts. These disaster photos are taken in real time by the crowd, unmanned aerial vehicles, and wireless sensors. However, communications equipment is often damaged in disasters, and the very limited communication bandwidth restricts the upload of photos to the cloud center, seriously impeding disaster rescue endeavors. Based on edge computing, we propose Echo, a highly time-efficient disaster rescue framework. By utilizing the computing, storage, and communication abilities of edge servers, disaster photos are preprocessed and analyzed in real time, and more specific visuals are immensely helpful for conducting emergency response and rescue. This article takes the search for missing people as a case study to show that Echo can be more advantageous in terms of disaster rescue. To greatly conserve valuable communication bandwidth, only significantly associated images are extracted and uploaded to the cloud center for subsequent facial recognition. Furthermore, an adaptive photo detector is designed to utilize the precious and unstable communication bandwidth effectively, as well as ensure the photo detection precision and recall rate. The effectiveness and efficiency of the proposed method are demonstrated by simulation experiments.																	2157-6904	2157-6912				DEC	2019	10	6							63	10.1145/3331146													
J								A Trust Computing-based Security Routing Scheme for Cyber Physical Systems	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Cyber physical systems; data collection; malicious attacks; trust-based routing		Security is a pivotal issue for the development of Cyber Physical Systems (CPS). The trusted computing of CPS includes the complete protection mechanisms, such as hardware, firmware, and software, the combination of which is responsible for enforcing a system security policy. A Trust Detection-based Secured Routing (TDSR) scheme is proposed to establish security routes from source nodes to the data center under malicious environment to ensure network security. In the TDSR scheme, sensor nodes in the routing path send detection routing to identify relay nodes' trust. And then, data packets are routed through trustworthy nodes to sink securely. In the TDSR scheme, the detection routing is executed in those nodes that have abundant energy; thus, the network lifetime cannot be affected. Performance evaluation through simulation is carried out for success of routing ratio, compromised node detection ratio, and detection routing overhead. The experiment results show that the performance can be improved in the TDSR scheme compared to previous schemes.																	2157-6904	2157-6912				DEC	2019	10	6							61	10.1145/3321694													
J								Deep Reinforcement Learning for Vehicular Edge Computing: An Intelligent Offloading System	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY											NONORTHOGONAL MULTIPLE-ACCESS; NETWORKS; ALLOCATION; PROTOCOL; UTILITY	The development of smart vehicles brings drivers and passengers a comfortable and safe environment. Various emerging applications are promising to enrich users' traveling experiences and daily life. However, how to execute computing-intensive applications on resource-constrained vehicles still faces huge challenges. In this article, we construct an intelligent offloading system for vehicular edge computing by leveraging deep reinforcement learning. First, both the communication and computation states are modelled by finite Markov chains. Moreover, the task scheduling and resource allocation strategy is formulated as a joint optimization problem to maximize users' Quality of Experience (QoE). Due to its complexity, the original problem is further divided into two sub-optimization problems. A two-sided matching scheme and a deep reinforcement learning approach are developed to schedule offloading requests and allocate network resources, respectively. Performance evaluations illustrate the effectiveness and superiority of our constructed system.																	2157-6904	2157-6912				DEC	2019	10	6							60	10.1145/3317572													
J								Efficient and Privacy-preserving Fog-assisted Health Data Sharing Scheme	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Fog computing; access control; data sharing; e-healthcare; privacy-preservation		Pervasive data collected from e-healthcare devices possess significant medical value through data sharing with professional healthcare service providers. However, health data sharing poses several security issues, such as access control and privacy leakage, as well as faces critical challenges to obtain efficient data analysis and services. In this article, we propose an efficient and privacy-preserving fog-assisted health data sharing (PFHDS) scheme for e-healthcare systems. Specifically, we integrate the fog node to classify the shared data into different categories according to disease risks for efficient health data analysis. Meanwhile, we design an enhanced attribute-based encryption method through combination of a personal access policy on patients and a professional access policy on the fog node for effective medical service provision. Furthermore, we achieve significant encryption consumption reduction for patients by offloading a portion of the computation and storage burden from patients to the fog node. Security discussions show that PFHDS realizes data confidentiality and fine-grained access control with collusion resistance. Performance evaluations demonstrate cost-efficient encryption computation, storage and energy consumption.																	2157-6904	2157-6912				DEC	2019	10	6							68	10.1145/3341104													
J								Energy-efficient Static Task Scheduling on VFI-based NoC-HMPSoCs for Intelligent Edge Devices in Cyber-physical Systems	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										CPS; SNS; task; DAG; mapping; scheduling; contention; heterogeneous; VFI-NoC-HMPSoCs; energy-efficiency	REAL-TIME TASKS; OPTIMIZATION; NETWORKS; MANAGEMENT; CHIP	The interlinked processing units in modern Cyber-Physical Systems (CPS) creates a large network of connected computing embedded systems. Network-on-Chip (NoC)-based Multiprocessor System-on-Chip (MPSoC) architecture is becoming a de facto computing platform for real-time applications due to its higher performance and Quality-of-Service (QoS). The number of processors has increased significantly on the multiprocessor systems in CPS; therefore, Voltage Frequency Island (VFI) has been recently adopted for effective energy management mechanism in the large-scale multiprocessor chip designs. In this article, we investigated energy-efficient and contention-aware static scheduling for tasks with precedence and deadline constraints on intelligent edge devices deploying heterogeneous VFI-based NoC-MPSoCs (VFI-NoC-HMPSoC) with DVFS-enabled processors. Unlike the existing population-based optimization algorithms, we proposed a novel population-based algorithm called ARSH-FATI that can dynamically switch between explorative and exploitative search modes at run-time. Our static scheduler ARHS-FATI collectively performs task mapping, scheduling, and voltage scaling. Consequently, its performance is superior to the existing state-of-the-art approach proposed for homogeneous VFI-based NoC-MPSoCs. We also developed a communication contention-aware Earliest Edge Consistent Deadline First (EECDF) scheduling algorithm and gradient descent-inspired voltage scaling algorithm called Energy Gradient Decent (EGD). We introduced a notion of Energy Gradient (EG) that guides EGD in its search for island voltage settings and minimize the total energy consumption. We conducted the experiments on eight real benchmarks adopted from Embedded Systems Synthesis Benchmarks (E3S). Our static scheduling approach ARSH-FATI outperformed state-of-the-art technique and achieved an average energy-efficiency of similar to 24% and similar to 30% over CA-TMES-Search and CA-TMES-Quick, respectively.																	2157-6904	2157-6912				DEC	2019	10	6							66	10.1145/3336121													
J								Crowdsourcing Mechanism for Trust Evaluation in CPCS Based on Intelligent Mobile Edge Computing	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Crowdsourcing; mobile edge computing; artificial intelligence; trust evaluation	HASHING-BASED APPROACH; SERVICE RECOMMENDATION; FOG; COLLECTION; EFFICIENT; SELECTION; PRIVACY; SCHEME	Both academia and industry have directed tremendous interest toward the combination of Cyber Physical Systems and Cloud Computing, which enables a new breed of applications and services. However, due to the relative long distance between remote cloud and end nodes, Cloud Computing cannot provide effective and direct management for end nodes, which leads to security vulnerabilities. In this article, we first propose a novel trust evaluation mechanism using crowdsourcing and Intelligent Mobile Edge Computing. The mobile edge users with relatively strong computation and storage ability are exploited to provide direct management for end nodes. Through close access to end nodes, mobile edge users can obtain various information of the end nodes and determine whether the node is trustworthy. Then, two incentive mechanisms, i.e., Trustworthy Incentive and Quality-Aware Trustworthy Incentive Mechanisms, are proposed for motivating mobile edge users to conduct trust evaluation. The first one aims to motivate edge users to upload their real information about their capability and costs. The purpose of the second one is to motivate edge users to make trustworthy effort to conduct tasks and report results. Detailed theoretical analysis demonstrates the validity of Quality-Aware Trustworthy Incentive Mechanism from data trustfulness, effort trustfulness, and quality trustfulness, respectively. Extensive experiments are carried out to validate the proposed trust evaluation and incentive mechanisms. The results corroborate that the proposed mechanisms can efficiently stimulate mobile edge users to perform evaluation task and improve the accuracy of trust evaluation.																	2157-6904	2157-6912				DEC	2019	10	6							62	10.1145/3324926													
J								Comparison and Modelling of Country-level Microblog User and Activity in Cyber-physical-social Systems Using Weibo and Twitter Data	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Weibo; microblogging	INTERNET; THINGS	As the rapid growth of social media technologies continues, Cyber-Physical-Social System (CPSS) has been a hot topic in many industrial applications. The use of "microblogging" services, such as Twitter, has rapidly become an influential way to share information. While recent studies have revealed that understanding and modelling microblog user behaviour with massive users' data in social media are keen to success of many practical applications in CPSS, a key challenge in literatures is that diversity of geography and cultures in social media technologies strongly affect user behaviour and activity. The motivation of this article is to understand differences and similarities between microblogging users from different countries using social media technologies, and to attempt to design a Country-Level Micro- Blog User (CLMB) behaviour and activity model for supporting CPSS applications. We proposed a CLMB model for analysing microblogging user behaviour and their activity across different countries in the CPSS applications. The model has considered three important characteristics of user behaviour in microblogging data, including content of microblogging messages, user emotion index, and user relationship network. We evaluated CLBM model under the collected microblog dataset from 16 countries with the largest number of representative and active users in the world. Experimental results show that (1) for some countries with small population and strong cohesiveness, users pay more attention to social functionalities of microblogging service; (2) for some countries containing mostly large loose social groups, users use microblogging services as a news dissemination platform; (3) users in countries whose social network structure exhibits reciprocity rather than hierarchy will use more linguistic elements to express happiness in microblogging services.																	2157-6904	2157-6912				DEC	2019	10	6							65	10.1145/3339474													
J								A Visual Analysis Approach for Understanding Durability Test Data of Automotive Products	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Industry 4.0; smart manufacturing; visual analysis; durability test; automotive starter	BIG DATA; VISUALIZATION; ANALYTICS	People face data-rich manufacturing environments in Industry 4.0. As an important technology for explaining and understanding complex data, visual analytics has been increasingly introduced into industrial data analysis scenarios. With the durability test of automotive starters as background, this study proposes a visual analysis approach for understanding large-scale and long-term durability test data. Guided by detailed scenario and requirement analyses, we first propose a migration-adapted clustering algorithm that utilizes a segmentation strategy and a group of matching-updating operations to achieve an efficient and accurate clustering analysis of the data for starting mode identification and abnormal test detection. We then design and implement a visual analysis system that provides a set of user-friendly visual designs and lightweight interactions to help people gain data insights into the test process overview, test data patterns, and durability performance dynamics. Finally, we conduct a quantitative algorithm evaluation, case study, and user interview by using real-world starter durability test datasets. The results demonstrate the effectiveness of the approach and its possible inspiration for the durability test data analysis of other similar industrial products.																	2157-6904	2157-6912				DEC	2019	10	6							70	10.1145/3345640													
J								Lightweight Convolution Neural Networks for Mobile Edge Computing in Transportation Cyber Physical Systems	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Convolutional neural network; model compression; factorization; mobile edge computing; cyber physical systems; Jetson TX2 module	CHALLENGES; VEHICLES; SMOTE	Cloud computing extends Transportation Cyber-Physical Systems (T-CPS) with provision of enhanced computing and storage capability via offloading computing tasks to remote cloud servers. However, cloud computing cannot fulfill the requirements such as low latency and context awareness in T-CPS. The appearance of Mobile Edge Computing (MEC) can overcome the limitations of cloud computing via offloading the computing tasks at edge servers in approximation to users, consequently reducing the latency and improving the context awareness. Although MEC has the potential in improving 'peps, it is incapable of processing computational-intensive tasks such as deep learning algorithms due to the intrinsic storage and computing capability constraints. Therefore, we design and develop a lightweight deep learning model to support MEC applications in T-CPS. In particular, we put forth a stacked convolutional neural network (CNN) consisting of factorization convolutional layers alternating with compression layers (namely, lightweight CNN-FC). Extensive experimental results show that our proposed lightweight CNN-FC can greatly decrease the number of unnecessary parameters, thereby reducing the model size while maintaining the high accuracy in contrast to conventional CNN models. In addition, we also evaluate the performance of our proposed model via conducting experiments at a realistic MEC platform. Specifically, experimental results at this MEC platform show that our model can maintain the high accuracy while preserving the portable model size.																	2157-6904	2157-6912				DEC	2019	10	6							67	10.1145/3339308													
J								Neural Image Caption Generation with Weighted Training and Reference	COGNITIVE COMPUTATION										Image captioning; Reference; Long short-term memory; Encoder-decoder	RANKING	Image captioning, which aims to automatically generate a sentence description for an image, has attracted much research attention in cognitive computing. The task is rather challenging, since it requires cognitively combining the techniques from both computer vision and natural language processing domains. Existing CNN-RNN framework-based methods suffer from two main problems: in the training phase, all the words of captions are treated equally without considering the importance of different words; in the caption generation phase, the semantic objects or scenes might be misrecognized. In our paper, we propose a method based on the encoder-decoder framework, named Reference based Long Short Term Memory (R-LSTM), aiming to lead the model to generate a more descriptive sentence for the given image by introducing reference information. Specifically, we assign different weights to the words according to the correlation between words and images during the training phase. We additionally maximize the consensus score between the captions generated by the captioning model and the reference information from the neighboring images of the target image, which can reduce the misrecognition problem. We have conducted extensive experiments and comparisons on the benchmark datasets MS COCO and Flickr30k. The results show that the proposed approach can outperform the state-of-the-art approaches on all metrics, especially achieving a 10.37% improvement in terms of CIDEr on MS COCO. By analyzing the quality of the generated captions, we come to a conclusion that through the introduction of reference information, our model can learn the key information of images and generate more trivial and relevant words for images.																	1866-9956	1866-9964				DEC	2019	11	6			SI		763	777		10.1007/s12559-018-9581-x													
J								A Novel Deep Density Model for Unsupervised Learning	COGNITIVE COMPUTATION										Deep density model; Mixtures of factor analyzers; Common component factor loadings; Dimensionality reduction	FACTOR ANALYZERS; DIMENSIONALITY; MIXTURES	Density models are fundamental in machine learning and have received a widespread application in practical cognitive modeling tasks and learning problems. In this work, we introduce a novel deep density model, referred to as deep mixtures of factor analyzers with common loadings (DMCFA), with an efficient greedy layer-wise unsupervised learning algorithm. The model employs a mixture of factor analyzers sharing common component loadings in each layer. The common loadings can be considered to be a feature selection or reduction matrix which makes this new model more physically meaningful. Importantly, sharing common components is capable of reducing both the number of free parameters and computation complexity remarkably. Consequently, DMCFA makes inference and learning rely on a dramatically more succinct model and avoids sacrificing its flexibility in estimating the data density by utilizing Gaussian distributions as the priors. Our model is evaluated on five real datasets and compared to three other competitive models including mixtures of factor analyzers (MFA), MFA with common loadings (MCFA), deep mixtures of factor analyzers (DMFA), and their collapsed counterparts. The results demonstrate the superiority of the proposed model in the tasks of density estimation, clustering, and generation.																	1866-9956	1866-9964				DEC	2019	11	6			SI		778	788		10.1007/s12559-018-9566-9													
J								Clustering of Remote Sensing Imagery Using a Social Recognition-Based Multi-objective Gravitational Search Algorithm	COGNITIVE COMPUTATION										Social recognition; Swarm intelligence; Multi-objective optimization (MOO); Gravitational search algorithm (GSA); Remote sensing image classification	GENETIC ALGORITHM; PIXEL CLASSIFICATION; OPTIMIZATION; SELECTION	Cognitively inspired swarm intelligence algorithms (SIAs) have attracted much attention in the research area of clustering since it can give machine the ability of self-learning to achieve better classification results. Recently, the SIA-based multi-objective optimization (MOO) methods have shown their superiorities in data clustering. However, their performances are limited when applying to the clustering of remote sensing imagery (RSI). To construct an excellent MOO-based clustering method, this paper presents a social recognition-based multi-objective gravitational search algorithm (SMGSA) to achieve simultaneous optimization of two conflicting cluster validity indices, i.e., the Xie-Beni (XB) index and the J(m) index. In the SMGSA, searching particles not only are guided by those elite particles stored in an external archive by the gravitational force but also learn from the social recognition of the whole population through the position difference. SMGSA thereby formed with outstanding exploitation ability. Comparison experiments on two public RSI data sets, including a moderate aerial image and a hyperspectral, validated that the MOO-based clustering methods could obtain more accurate results than the single validity index-based method. Moreover, the SMGSA-based method can achieve superior results than that of the multi-objective gravitational search algorithm without social recognition ability. The proposed SMGSA performs favorable balance between the two conflicting cluster validity indices and achieves preferable classification for the RSI. In addition, this study indicates that the swarm intelligence-based cognitive computing is potential for the intelligent interpretation and understanding of complicated remote sensing scene.																	1866-9956	1866-9964				DEC	2019	11	6			SI		789	798		10.1007/s12559-018-9582-9													
J								Determination of Temporal Stock Investment Styles via Biclustering Trading Patterns	COGNITIVE COMPUTATION										Machine learning; Biclustering; Technical analysis; Trading rules; Investment styles	NEURAL-NETWORK; PRICE; DISCOVERY; MACHINE; MARKETS; SYSTEM; RULES; MODEL	Due to the effects of many deterministic and stochastic factors, it has always been a challenging goal to gain good profits from the stock market. Many methods based on different theories have been proposed in the past decades. However, there has been little research about determining the temporal investment style (i.e., short term, middle term, or long term) for the stock. In this paper, we propose a method to find suitable stock investment styles in terms of investment time. Firstly, biclustering is applied to a matrix that is composed of technical indicators of each trading day to discover trading patterns (regarded as trading rules). Subsequently a k-nearest neighbor (KNN) algorithmis employed to transform the trading rules to the trading actions (i.e., the buy, sell, or no-action signals). Finally, a min-max and quantization strategy is designed for determination of the temporal investment style of the stock. The proposed method was tested on 30 stocks from US bear, bull, and flat markets. The experimental results validate its usefulness.																	1866-9956	1866-9964				DEC	2019	11	6			SI		799	808		10.1007/s12559-019-9626-9													
J								A New Algorithm for SAR Image Target Recognition Based on an Improved Deep Convolutional Neural Network	COGNITIVE COMPUTATION										Synthetic-aperture radar (SAR) images; Automatic target recognition (ATR); Deep convolutional neural network (DCNN); Support vector machine (SVM); Class separation information	SPARSE REPRESENTATION; FEATURE-EXTRACTION; DIMENSIONALITY; PERFORMANCE; CLASSIFIER; REDUCTION; FUSION	In an attempt to exploit the automatic feature extraction ability of biologically-inspired deep learning models, and enhance the learning of target features, we propose a novel deep learning algorithm. This is based on a deep convolutional neural network (DCNN) trained with an improved cost function, and combined with a support vector machine (SVM). Specifically, class separation information, which explicitly facilitates intra-class compactness and inter-class separability in the process of learning features, is added to an improved cost function as a regularization term, to enhance the DCNN's feature extraction ability. The enhanced DCNN is applied to learn the features of Synthetic Aperture Radar (SAR) images, and the SVM is utilized to map features into output labels. Simulation experiments are performed using benchmark SAR image data from the Moving and Stationary Target Acquisition and Recognition (MSTAR) database. Comparative results demonstrate the effectiveness of our proposed method, with an average accuracy of 99% on ten types of targets, including variants and articulated targets. We conclude that our proposed DCNN method has significant potential to be exploited for SAR image target recognition, and can serve as a new benchmark for the research community.																	1866-9956	1866-9964				DEC	2019	11	6			SI		809	824		10.1007/s12559-018-9563-z													
J								RGB-D Scene Classification via Multi-modal Feature Learning	COGNITIVE COMPUTATION										Deep learning; Local fine-tuning; Convolutional neural networks; RGB-D scene classification	REPRESENTATION; RECOGNITION; RETRIEVAL	Most of the past deep learning methods which are proposed for RGB-D scene classification use global information and directly consider all pixels in the whole image for high-level tasks. Such methods cannot hold much information about local feature distributions, and simply concatenate RGB and depth features without exploring the correlation and complementarity between raw RGB and depth images. From the human vision perspective, we recognize the category of one unknown scene mainly relying on the object-level information in the scene which includes the appearance, texture, shape, and depth. The structural distribution of different objects is also taken into consideration. Based on this observation, constructing mid-level representations with discriminative object parts would generally be more attractive for scene analysis. In this paper, we propose a new Convolutional Neural Networks (CNNs)-based local multi-modal feature learning framework (LM-CNN) for RGB-D scene classification. This method can effectively capture much of the local structure from the RGB-D scene images and automatically learn a fusion strategy for the object-level recognition step instead of simply training a classifier on top of features extracted from both modalities. The experimental results on two popular datasets, i.e., NYU v1 depth dataset and SUN RGB-D dataset, show that our method with local multi-modal CNNs outperforms state-of-the-art methods.																	1866-9956	1866-9964				DEC	2019	11	6			SI		825	840		10.1007/s12559-018-9580-y													
J								Ensemble p-Laplacian Regularization for Scene Image Recognition	COGNITIVE COMPUTATION										Manifold regularization; Semi-supervised learning; Ensemble p-Laplacian regularization	HESSIAN REGULARIZATION	Recently, manifold regularized semi-supervised learning (MRSSL) received considerable attention, because it successfully exploits the geometry of the intrinsic data probability distribution to leverage the performance of a learning model. As a natural nonlinear generalization of graph Laplacian, p-Laplacian has been proved having the rich theoretical foundations to better preserve the local structure. However, it is difficult to determine the fitting graph p-Lapalcian, i.e., the parameter p, which is a critical factor for the performance of graph p-Laplacian. Therefore, we develop an ensemble p-Laplacian regularization (EpLapR) to fully approximate the intrinsic manifold of the data distribution. EpLapR incorporates multiple graphs into a regularization term in order to sufficiently explore the complementation of graph p-Laplacian. Specifically, we construct a fused graph by introducing an optimization approach to assign suitable weights on different p value graphs. And then, we conduct semi-supervised learning framework on the fused graph. Extensive experiments on UC-Merced dataset and Scene 15 dataset demonstrate the effectiveness and efficiency of the proposed method.																	1866-9956	1866-9964				DEC	2019	11	6			SI		841	854		10.1007/s12559-019-09637-z													
J								CSA-DE/EDA: a Novel Bio-inspired Algorithm for Function Optimization and Segmentation of Brain MR Images	COGNITIVE COMPUTATION										Bio-inspired computing; Clonal selection algorithm (CSA); Differential evolution (DE); Estimation of distribution algorithm(EDA); Image segmentation	CLONAL SELECTION ALGORITHM; RANDOM-FIELD MODEL; REGISTRATION	The clonal selection algorithm (CSA), which describes the basic features of an immune response to an antigenic stimulus, has drawn a lot of attention in the biologically inspired computing community, due to its highly adaptive and easy-to-implement nature. Despite many successful applications, CSA still suffers from limited ability to explore the solution space. In this paper, we incorporate the differential evolution (DE) algorithm and the estimation of distribution algorithm (EDA) into CSA, and thus propose a novel bio-inspired algorithm referred to as CSA-DE/EDA. In the proposed algorithm, the hypermutation and receptor editing processes are implemented based on DE and EDA, which provide improved local and global search ability, respectively. We have applied the proposed algorithm to five commonly used benchmark functions for optimization and brain magnetic resonance (MR) image segmentation. Our comparative experimental results show that the proposed CSA-DE/EDA algorithm outperforms several bio-inspired computing techniques. CSA-DE/EDA is a compelling bio-inspired algorithm for optimization tasks.																	1866-9956	1866-9964				DEC	2019	11	6			SI		855	868		10.1007/s12559-019-09663-x													
J								Unsupervised Object Transfiguration with Attention	COGNITIVE COMPUTATION										Multi-modalities; Object transfiguration; Image-to-image translation; Generative Adversarial Networks (GANs); Attention mechanism; Deep learning	SALIENCY DETECTION; DEEP; FRAMEWORK	Object transfiguration is a subtask of the image-to-image translation, which translates two independent image sets and has a wide range of applications. Recently, some studies based on Generative Adversarial Network (GAN) have achieved impressive results in the image-to-image translation. However, the object transfiguration task only translates regions containing target objects instead of whole images; most of the existing methods never consider this issue, which results in mistranslation on the backgrounds of images. To address this problem, we present a novel pipeline called Deep Attention Unit Generative Adversarial Networks (DAU-GAN). During the translating process, the DAU computes attention masks that point out where the target objects are. DAU makes GAN concentrate on translating target objects while ignoring meaningless backgrounds. Additionally, we construct an attention-consistent loss and a background-consistent loss to compel our model to translate intently target objects and preserve backgrounds further effectively. We have comparison experiments on three popular related datasets, demonstrating that the DAU-GAN achieves superior performance to the state-of-the-art. We also export attention masks in different stages to confirm its effect during the object transfiguration task. The proposed DAU-GAN can translate object effectively as well as preserve backgrounds information at the same time. In our model, DAU learns to focus on the most important information by producing attention masks. These masks compel DAU-GAN to effectively distinguish target objects and backgrounds during the translation process and to achieve impressive translation results in two subsets of ImageNet and CelebA. Moreover, the results show that we cannot only investigate the model from the image itself but also research from other modal information.																	1866-9956	1866-9964				DEC	2019	11	6			SI		869	878		10.1007/s12559-019-09633-3													
J								User Evaluations on Sentiment-based Recommendation Explanations	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Recommender systems; explanation interfaces; sentiment analysis; product reviews; user study; eye-tracking experiment; user perceptions	TAXONOMY	The explanation interface has been recognized as important in recommender systems because it can allow users to better judge the relevance of recommendations to their preferences and, hence, make more informed decisions. In different product domains, the specific purpose of explanation can be different. For high-investment products (e.g., digital cameras, laptops), how to educate the typical type of new buyers about product knowledge and, consequently, improve their preference certainty and decision quality is essentially crucial. With this objective, we have developed a novel tradeoff-oriented explanation interface that particularly takes into account sentiment features as extracted from product reviews to generate recommendations and explanations in a category structure. In this manuscript, we first reported the results of an earlier user study (in both before-after and counter-balancing setups) that compared our prototype system with the traditional one that purely considers static specifications for explanations. This experiment revealed that adding sentiment-based explanations can significantly increase users' product knowledge, preference certainty, perceived information usefulness, perceived recommendation transparency and quality, and purchase intention. In order to further identify the reason behind users' perception improvements on the sentiment-based explanation interface, we performed a follow-up lab controlled eye-tracking experiment that investigated how users viewed information and compared products on the interface. This study shows that incorporating sentiment features into the tradeoff-oriented explanations can significantly affect users' eye-gaze pattern. They were stimulated to not only notice bottom categories of products, but also, more frequently, to compare products across categories. The results also disclose users' inherent information needs for sentiment-based explanations, as they allow users to better understand the recommended products and gain more knowledge about static specifications.																	2160-6455	2160-6463				DEC	2019	9	4							20	10.1145/3282878													
J								EventAction: A Visual Analytics Approach to Explainable Recommendation for Event Sequences	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Similarity; personal record; multidimensional data visualization; temporal visualization; decision making; visual analytics	SIMILARITY MEASURES	People use recommender systems to improve their decisions; for example, item recommender systems help them find films to watch or books to buy. Despite the ubiquity of item recommender systems, they can be improved by giving users greater transparency and control. This article develops and assesses interactive strategies for transparency and control, as applied to event sequence recommender systems, which provide guidance in critical life choices such as medical treatments, careers decisions, and educational course selections. This article's main contribution is the use of both record attributes and temporal event information as features to identify similar records and provide appropriate recommendations. While traditional item recommendations are based on choices by people with similar attributes, such as those who looked at this product or watched this movie, our event sequence recommendation approach allows users to select records that share similar attribute values and start with a similar event sequence. Then users see how different choices of actions and the orders and times between them might lead to users' desired outcomes. This paper applies a visual analytics approach to present and explain recommendations of event sequences. It presents a workflow for event sequence recommendation that is implemented in EventAction and reports on three case studies in two domains to illustrate the use of generating event sequence recommendations based on personal histories. It also offers design guidelines for the construction of user interfaces for event sequence recommendation and discusses ethical issues in dealing with personal histories.																	2160-6455	2160-6463				DEC	2019	9	4							21	10.1145/3301402													
J								Human-in-the-loop Learning for Personalized Diet Monitoring from Unstructured Mobile Data	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Diet monitoring; wireless health; mobile computing; human-in-the-loop learning; unstructured data; real-time prompting; combinatorial optimization; assignment problem	LOCALIZATION; ACCURACY; SCIENCE; IMAGE	Lifestyle interventions with the focus on diet are crucial in self-management and prevention of many chronic conditions, such as obesity, cardiovascular disease, diabetes, and cancer. Such interventions require a diet monitoring approach to estimate overall dietary composition and energy intake. Although wearable sensors have been used to estimate eating context (e.g., food type and eating time), accurate monitoring of dietary intake has remained a challenging problem. In particular, because monitoring dietary intake is a self-administered task that involves the end-user to record or report their nutrition intake, current diet monitoring technologies are prone to measurement errors related to challenges of human memory, estimation, and bias. New approaches based on mobile devices have been proposed to facilitate the process of dietary intake recording. These technologies require individuals to use mobile devices such as smartphones to record nutrition intake by either entering text or taking images of the food. Such approaches, however, suffer from errors due to low adherence to technology adoption and time sensitivity to the dietary intake context. In this article, we introduce EZNutriPal,(1) an interactive diet monitoring system that operates on unstructured mobile data such as speech and free-text to facilitate dietary recording, real-time prompting, and personalized nutrition monitoring. EZNutriPal features a natural language processing unit that learns incrementally to add user-specific nutrition data and rules to the system. To prevent missing data that are required for dietary monitoring (e.g., calorie intake estimation), EZNutriPal devises an interactive operating mode that prompts the end-user to complete missing data in real-time. Additionally, we propose a combinatorial optimization approach to identify the most appropriate pairs of food names and food quantities in complex input sentences. We evaluate the performance of EZNutriPal using real data collected from 23 human subjects who participated in two user studies conducted in 13 days each. The results demonstrate that EZNutriPal achieves an accuracy of 89.7% in calorie intake estimation. We also assess the impacts of the incremental training and interactive prompting technologies on the accuracy of nutrient intake estimation and show that incremental training and interactive prompting improve the performance of diet monitoring by 49.6% and 29.1%, respectively, compared to a system without such computing units.																	2160-6455	2160-6463				DEC	2019	9	4							23	10.1145/3319370													
J								Toward a Unified Theory of Learned Trust in Interpersonal and Human-Machine Interactions	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Trust; trustworthiness; trust propensity; learned trust; computational cognitive model; unified theories	IMPRESSION-FORMATION; MODEL; METAANALYSIS; PROPENSITY; AUTOMATION; GAMES; TRUSTWORTHINESS; COMMITMENT; DISTRUST; DYNAMICS	A proposal for a unified theory of learned trust implemented in a cognitive architecture is presented. The theory is instantiated as a computational cognitive model of learned trust that integrates several seemingly unrelated categories of findings from the literature on interpersonal and human-machine interactions and makes unintuitive predictions for future studies. The model relies on a combination of learning mechanisms to explain a variety of phenomena such as trust asymmetry, the higher impact of early trust breaches, the black-hat/white-hat effect, the correlation between trust and cognitive ability, and the higher resilience of interpersonal as compared to human-machine trust. In addition, the model predicts that trust decays in the absence of evidence of trustworthiness or untrustworthiness. The implications of the model for the advancement of the theory on trust are discussed. Specifically, this work suggests two more trust antecedents on the trustor's side: perceived trust necessity and cognitive ability to detect cues of trustworthiness.																	2160-6455	2160-6463				DEC	2019	9	4							24	10.1145/3230735													
J								A User-adaptive Modeling for Eating Action Identification from Wristband Time Series	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Wearable; time-series data modeling; gesture recognition; diet monitoring	PATTERNS; DIET; RISK	Eating activity monitoring using wearable sensors can potentially enable interventions based on eating speed to mitigate the risks of critical healthcare problems such as obesity or diabetes. Eating actions are poly-componential gestures composed of sequential arrangements of three distinct components interspersed with gestures that may be unrelated to eating. This makes it extremely challenging to accurately identify eating actions. The primary reasons for the lack of acceptance of state-of-the-art eating action monitoring techniques include the following: (i) the need to install wearable sensors that are cumbersome to wear or limit the mobility of the user, (ii) the need for manual input from the user, and (iii) poor accuracy in the absence of manual inputs. In this work, we propose a novel methodology, IDEA, that performs accurate eating action identification within eating episodes with an average F1 score of 0.92. This is an improvement of 0.11 for precision and 0.15 for recall for the worst-case users as compared to the state of the art. IDEA uses only a single wristband and provides feedback on eating speed every 2 min without obtaining any manual input from the user.																	2160-6455	2160-6463				DEC	2019	9	4							22	10.1145/3300149													
J								Learning from Sets of Items in Recommender Systems	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										ecommender systems; collaborative filtering; user behavior modeling; matrix factorization		Most of the existing recommender systems use the ratings provided by users on individual items. An additional source of preference information is to use the ratings that users provide on sets of items. The advantages of using preferences on sets are twofold. First, a rating provided on a set conveys some preference information about each of the set's items, which allows us to acquire a user's preferences for more items than the number of ratings that the user provided. Second, due to privacy concerns, users may not be willing to reveal their preferences on individual items explicitly but may be willing to provide a single rating to a set of items, since it provides some level of information hiding. This article investigates two questions related to using set-level ratings in recommender systems. First, how users' item-level ratings relate to their set-level ratings. Second, how collaborative filtering-based models for item-level rating prediction can take advantage of such set-level ratings. We have collected set-level ratings from active users of Movielens on sets of movies that they have rated in the past. Our analysis of these ratings shows that though the majority of the users provide the average of the ratings on a set's constituent items as the rating on the set, there exists a significant number of users that tend to consistently either under- or over-rate the sets. We have developed collaborative filtering-based methods to explicitly model these user behaviors that can be used to recommend items to users. Experiments on real data and on synthetic data that resembles the under- or over-rating behavior in the real data demonstrate that these models can recover the overall characteristics of the underlying data and predict the user's ratings on individual items.																	2160-6455	2160-6463				DEC	2019	9	4							19	10.1145/3326128													
J								LAMB-DASH: a DASH-HEVC adaptive streaming algorithm in a sharing bandwidth environment for heterogeneous contents and dynamic connections in practice	JOURNAL OF REAL-TIME IMAGE PROCESSING										Adaptive streaming; DASH; HEVC; QoE; Dense client environments	EFFICIENCY	HTTP Adaptive Streaming (HAS) offers media players the possibility to dynamically select the most appropriate bitrate according to the connectivity performance. A best-effort strategy to take instant decisions could dramatically damage the overall Quality of Experience (QoE) with re-buffering times, and potential image freezes along with quality fluctuations. This is more critical in environments where multiple clients share the available bandwidth. Here, clients compete for the best connectivity. To address this issue, we propose LAMB-DASH, an online algorithm that, based on the historical probability of the playout session, improves the Quality Level (QL) chunk Mean Opinion Score (c-MOS). LAMB-DASH is designed for heterogeneous contents and changeable connectivity performance. It removes the need to access a probability distribution to specific parameters and conditions in advance. This way, LAMB-DASH focuses on the fast response and on the reduced computing overhead to provide a universal bitrate selection criterion. This paper validates the proposed solution in a real environment which considers live and on-demand Dynamic Adaptive Streaming over HTTP (DASH) and High-Efficiency Video Coding (HEVC) services implemented on top of GStreamer clients.																	1861-8200	1861-8219				DEC	2019	16	6					2159	2171		10.1007/s11554-017-0728-x													
J								A grammar inference approach for language self-adaptation and evolution in digital ecosystems	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Digital ecosystem; Interaction; Grammar; Socialization; Language evolution	CHALLENGES	Socialization is the essential building process of any society in natural ecosystems. Effective socialization processes have been investigated for both "biotic" (human) and "abiotic" (virtual) entities, also within digital ecosystems in the perspective of common and self-adaptive languages. In this paper, we propose an approach for socialization, language self-adaptation, and evolution that enables an effective communicative interaction among digital entities acting in a digital ecosystem. The proposed method relies on an adaptable and extensible grammatical formalism, named Digital Ecosystem Grammar (DEG). This grammar enables digital entities to interpret the messages sent by other entities by using interaction, learning and evolution actions. Moreover, a grammar learning algorithm is applied to provide the self-adaptation mechanisms that allow the digital environment to adapt the interaction language according to new messages. The approach was suitable to support the characteristics of self-adaptation, context-awareness, evolvability, and semanticity of a digital ecosystem language.																	0925-9902	1573-7675				DEC	2019	53	3			SI		409	430		10.1007/s10844-019-00566-9													
J								A framework for aspect based sentiment analysis on turkish informal texts	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Aspect; Sentiment analysis; Informal text; Aspect extraction; Aspect-sentiment mapping		The web provides a suitable media for users to share opinions on various topics, including consumer products, events or news. In most of such content, authors express different opinions on different features (i.e., aspects) of the topic. It is a common practice to express a positive opinion on one aspect and a negative opinion on another aspect within the same post. Conventional sentiment analysis methods do not capture such details, rather an overall sentiment score is generated. In aspect based sentiment analysis, the opinions expressed for each aspect are extracted separately. To this aim, basically a two-phased approach is used. The first phase is aspect extraction, which is the detection of words that correspond to aspects of the topic. Once aspects are available, the next phase is to match aspects with the sentiment words in the text. In this work, we present a framework for the aspect based sentiment analysis problem on Turkish informal texts. We particularly emphasize the following contributions: for the first phase, improvements for aspect extraction as an unsupervised method, and for the second phase, enhancements for two cases, extracting implicit aspects and detecting sentiment words whose polarity depends on the aspect. Additionally, we present a tool including the implementations of the proposed algorithms, and a GUI to visualize the analysis results. The experiments are conducted on a collection of Turkish informal texts from an online products forum.																	0925-9902	1573-7675				DEC	2019	53	3			SI		431	451		10.1007/s10844-019-00565-w													
J								Crowd Sourced Semantic Enrichment (CroSSE) for knowledge driven querying of digital resources	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Semantic enrichment; Crowd-sourcing; Data integration framework	INTEGRATION	Today, most information sources provide factual, objective knowledge, but they fail to capture personalized contextual knowledge which could be used to enrich the available factual data and contribute to their interpretation, in the context of the knowledge of the user who queries the system. This would require a knowledge framework which can accommodate both objective data and semantic enrichments that capture user provided knowledge associated to the factual data in the database. Unfortunately, most conventional DBMSs lack the flexibilities necessary (a) to prevent the data and metadata, evolve quickly with changing application requirements and (b) to capture user-provided and/or crowdsourced data and knowledge for more effective decision support. In this paper, we present Crowd-Sourced Semantic Enrichment (CroSSE) knowledge framework which allows traditional databases and semantic enrichment modules to coexist. CroSSE provides a novel Semantically Enriched SQL (SESQL) language to enrich SQL queries with information from a knowledge base containing semantic annotations. We describe CroSSE and SESQL with examples taken from our SmartGround EU project.																	0925-9902	1573-7675				DEC	2019	53	3			SI		453	480		10.1007/s10844-019-00559-8													
J								Hollow-tree: a metric access method for data with missing values	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Missing values; Missing at random; Similarity search; Fractal dimension		Similarity search is fundamental to store and retrieve large volumes of complex data required by many real world applications. A useful mechanism for such concept is the query-by-similarity. Based on their topological properties, metric similarity functions can be used to index sets of data which can be queried effectively and efficiently by the so-called metric access methods. However, data produced by various application domains and the varying data types handled often lead to missing data, hence, they do not follow the metric similarity requirements. As a consequence, missing data cause distortions in the index structure and yield bias in the query answer. In this paper, we propose the Hollow-tree, a novel access method aimed at successfully retrieving data with missing attribute values. It employs new strategies for indexing and searching data elements, capable of handling the missing data issues when the cause of missingness is ignorable. The indexing strategy is based on a family of distance functions that allow measuring the distance between elements with missing values, along with a set of policies able to organize the elements in the index without causing distortions to its internal structure. The searching strategy employs fractal dimension property of the data to achieve accurate query answer while considering data with missing values part of the response. Results from experiments performed on a variety of real and synthetic data sets showed that, while other metric access methods deteriorate with small amounts of missing values, the Hollow-tree maintains a remarkable performance with almost 100% of precision and recall for range queries and more than 90% for k-nearest neighbor queries, for up to 40% of missing values.																	0925-9902	1573-7675				DEC	2019	53	3			SI		481	508		10.1007/s10844-019-00567-8													
J								Skyline-based dissimilarity of images	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Image databases; Image descriptors; Skyline algorithms; Hashing techniques	QUANTIZATION; COLOR	Large image collections are being used in many modern applications. In this paper, we aim at capturing the intrinsic dissimilarities of image descriptors in large image collections, i.e., to detect dissimilar (or else diverse) images without defining an explicit similarity or distance measure. Towards this goal, we adopt skyline processing techniques for large image databases, based on their high-dimensional descriptor vectors. The novelty of the proposed methodology lies in the use of skyline techniques empowered by state-of-the-art hashing schemes to enable effective data partitioning and indexing in secondary memory, towards supporting large image databases. The proposed approach is evaluated experimentally by using three real-world image datasets. Performance evaluation results demonstrate that images lying on the skyline have significantly different characteristics, which depend on the type of the descriptor. Thus, these skyline items may be used as seeds to apply clustering in large image databases. In addition, we observe that skyline processing using hash-based indexing structures is significantly faster than index-free skyline computation and also more efficient than skyline computation with hierarchical indexing structures. Based on our results, the proposed approach is both efficient (regarding runtime) and effective (with respect to image diversity) and therefore can be used as a base for more complex data mining tasks such as clustering.																	0925-9902	1573-7675				DEC	2019	53	3			SI		509	545		10.1007/s10844-019-00571-y													
J								Safe disassociation of set-valued datasets	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Disassociation; Cover problem; Data privacy; Set-valued; Privacy preserving		Disassociation is a bucketization based anonymization technique that divides a set-valued dataset into several clusters to hide the link between individuals and their complete set of items. It increases the utility of the anonymized dataset, but on the other side, it raises many privacy concerns, one in particular, is when the items are tightly coupled to form what is called, a cover problem. In this paper, we present safe disassociation, a technique that relies on partial suppression, to overcome the aforementioned privacy breach encountered when disassociating set-valued datasets. Safe disassociation allows the k(m)-anonymity privacy constraint to be extended to a bucketized dataset and copes with the cover problem. We describe our algorithm that achieves the safe disassociation and we provide a set of experiments to demonstrate its efficiency.																	0925-9902	1573-7675				DEC	2019	53	3			SI		547	562		10.1007/s10844-019-00568-7													
J								Business information architecture for successful project implementation based on sentiment analysis in the tourist sector	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Data governance; Enterprise architecture; Business model; Process improvement; Tourism management	EXTRACTION	In the today's market, there is a wide range of failed IT projects in specialized small and medium-sized companies because of poor control in the gap between the business and its vision. In other words, acquired goods are not being sold, a scenario which is very common in tourism retail companies. These companies buy a number of travel packages from big companies and due to lack of demand for these packages, they expire, becoming an expense, rather than an investment. To solve this problem, we propose to detect the problems that limit a company by re-engineering the processes, enabling the implementation of a business architecture based on sentimental analysis, allowing small and medium-sized tourism enterprises (SMEs) to make better decisions and analyze the information that most possess, without knowing how to exploit it. In addition, a case study was applied using a real company, comparing data before and after using the proposed model in order to validate feasibility of the applied model.																	0925-9902	1573-7675				DEC	2019	53	3			SI		563	585		10.1007/s10844-019-00564-x													
J								Collective Adaptation through Multi-Agents Ensembles: The Case of Smart Urban Mobility	ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS										Socio-technical systems; collective adaptation; ensembles; smart mobility; multi-agent systems	COALITION-FORMATION; COMMUNICATION	Modern software systems are becoming more and more socio-technical systems composed of distributed and heterogeneous agents from a mixture of people, their environment, and software components. These systems operate under continuous perturbations due to the unpredicted behaviors of people and the occurrence of exogenous changes in the environment. In this article, we introduce a notion of ensembles for which, systems with collective adaptability can be built as an emergent aggregation of autonomous and self-adaptive agents. Building upon this notion of ensemble, we present a distributed adaptation approach for systems composed by ensembles: collections of agents with their respective roles and goals. In these systems, adaptation is triggered by the run-time occurrence of an extraordinary circumstance, called issue. It is handled by an issue resolution process that involves agents affected by the issue to collaboratively adapt with minimal impact on their own preferences. Central to our approach is the implementation of a collective adaptation engine (CAE) able to solve issues in a collective fashion. The approach is instantiated in the context of a smart mobility scenario through which its main features are illustrated. To demonstrate the approach in action and evaluate it, we exploit the DeMOCAS framework, simulating the operation of an urban mobility scenario. We have executed a set of experiments with the goal to show how the CAE performs in terms of feasibility and scalability. With this approach, we are able to demonstrate how collective adaptation opens up new possibilities for tackling urban mobility challenges making it more sustainable respect to selfish and competitive behaviours.																	1556-4665	1556-4703				DEC	2019	14	2							6	10.1145/3355562													
J								Controlling Interactions with Libraries in Android Apps Through Runtime Enforcement	ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS										Proactive library; self-healing; Android; resource leak; resource usage; policy enforcement; runtime enforcement		Android applications are executed on smartphones equipped with a variety of resources that must be properly accessed and controlled, otherwise the correctness of the executions and the stability of the entire environment might be negatively affected. For example, apps must properly acquire, use, and release microphones, cameras, and other multimedia devices, otherwise the behavior of the apps that use the same resources might be compromised. Unfortunately, several apps do not use resources correctly, for instance, due to faults and inaccurate design decisions. By interacting with these apps, users may experience unexpected behaviors, which in turn may cause instability and sporadic failures, especially when resources are accessed. In this article, we present an approach that lets users protect their environment from the apps that use resources improperly by enforcing the correct usage protocol. This is achieved by using software enforcers that can observe executions and change them when necessary. For instance, enforcers can detect that a resource has been acquired but not released and automatically perform the release operation, thus giving the possibility to use that same resource to the other apps. The main idea is that software libraries, in particular, the ones controlling access to resources, can be augmented with enforcers that can be activated and deactivated on demand by users to protect their environment from unwanted app behaviors. We call the software libraries augmented with one or more enforcers proactive libraries, because the activation of the enforcer decorates the library with proactive behaviors that can guarantee the correctness of the execution despite the invocation of the operations implemented by the library. For example, enforcers can detect that a resource has not been released on time and proactively release it. Our experimental results with 27 possible misuses of resources in real Android apps reveal that proactive libraries are able to effectively correct library misuses with negligible runtime overheads.																	1556-4665	1556-4703				DEC	2019	14	2							8	10.1145/3368087													
J								Supporting Dynamic Workflows with Automatic Extraction of Goals from BPMN	ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS										Dynamic Workflow; Self-Adaptive Systems; Business Process Goals	VERIFICATION	Organizations willing to employ workflow technology have to be prepared to undertake a significant investment of time and effort due to the exceptionally dynamic nature of the business environment. Today, it is unlikely that processes are modeled once to be repeatedly executed without any changes. Goal-oriented dynamic workflows are a promising approach to provide flexibility to the execution of business processes. Many goal-oriented frameworks exist in the literature to be used for the purpose. However, modeling goals is a burden for the business analyst. This work proposes an automatic approach for extracting goals from a business process for supporting adaptive workflows. The approach consists of a static analysis of the global workflow state. Goals derive from individual BPMN elements and their interactions. For validating the theory, we developed the BPMN2Goals tool, which has been used for supporting a middleware for self-adaptation.																	1556-4665	1556-4703				DEC	2019	14	2							7	10.1145/3355488													
J								On parameter estimation with the Wasserstein distance	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										Wasserstein distance; parameter inference; optimal transport; minimum distance estimation	CENTRAL LIMIT-THEOREMS; MINIMUM DISTANCE; APPROXIMATE; ALGORITHM; MOMENTS	Statistical inference can be performed by minimizing, over the parameter space, the Wasserstein distance between model distributions and the empirical distribution of the data. We study asymptotic properties of such minimum Wasserstein distance estimators, complementing results derived by Bassetti, Bodini and Regazzini in 2006. In particular, our results cover the misspecified setting, in which the data-generating process is not assumed to be part of the family of distributions described by the model. Our results are motivated by recent applications of minimum Wasserstein estimators to complex generative models. We discuss some difficulties arising in the numerical approximation of these estimators. Two of our numerical examples (g-and-kappa and sum of log-normals) are taken from the literature on approximate Bayesian computation and have likelihood functions that are not analytically tractable. Two other examples involve misspecified models.																	2049-8764	2049-8772				DEC	2019	8	4					657	676		10.1093/imaiai/iaz003													
J								Estimating matching affinity matrices under low-rank constraints	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										inverse optimal transport; rank-constrained estimation; bipartite matching; marriage market		In this paper, we address the problem of estimating transport surplus (a.k.a. matching affinity) in high-dimensional optimal transport problems. Classical optimal transport theory specifies the matching affinity and determines the optimal joint distribution. In contrast, we study the inverse problem of estimating matching affinity based on the observation of the joint distribution, using an entropic regularization of the problem. To accommodate high dimensionality of the data, we propose a novel method that incorporates a nuclear norm regularization that effectively enforces a rank constraint on the affinity matrix. The low-rank matrix estimated in this way reveals the main factors that are relevant for matching.																	2049-8764	2049-8772				DEC	2019	8	4					677	689		10.1093/imaiai/iaz015													
J								Uncoupled isotonic regression via minimum Wasserstein deconvolution	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										isotonic regression; coupling; moment matching; deconvolution; minimum Kantorovich distance estimation	CONVERGENCE; MONOTONE; BOUNDS; RATES	Isotonic regression is a standard problem in shape-constrained estimation where the goal is to estimate an unknown non-decreasing regression function f from independent pairs (x(i), y(i)) where E[y(i)] = f (xi()), i = 1,...n. While this problem is well understood both statistically and computationally, much less is known about its uncoupled counterpart, where one is given only the unordered sets {x(1),...,x(n)} and {y(1),...,y(n)}. In this work, we leverage tools from optimal transport theory to derive minimax rates under weak moments conditions on yi and to give an efficient algorithm achieving optimal rates. Both upper and lower bounds employ moment-matching arguments that are also pertinent to learning mixtures of distributions and deconvolution.																	2049-8764	2049-8772				DEC	2019	8	4					691	717		10.1093/imaiai/iaz006													
J								Data-driven regularization of Wasserstein barycenters with an application to multivariate density registration	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										Wasserstein barycenter; density estimation; entropy regularized transport distances; Goldenshluger-Lepski's method	ALGORITHM	We present a framework to simultaneously align and smoothen data in the form of multiple point clouds sampled from unknown densities with support in a d-dimensional Euclidean space. This work is motivated by applications in bioinformatics where researchers aim to automatically homogenize large datasets to compare and analyze characteristics within a same cell population. Inconveniently, the information acquired is most certainly noisy due to misalignment caused by technical variations of the environment. To overcome this problem, we propose to register multiple point clouds by using the notion of regularized barycenters (or Frechet mean) of a set of probability measures with respect to the Wasserstein metric. The first approach consists in penalizing a Wasserstein barycenter with a convex functional as recently proposed in [5]. The second strategy is to transform the Wasserstein metric itself into an entropy regularized transportation cost between probability measures as introduced in [12]. The main contribution of this work is to propose data-driven choices for the regularization parameters involved in each approach using the Goldenshluger-Lepski's principle. Simulated data sampled from Gaussian mixtures are used to illustrate each method, and an application to the analysis of flow cytometry data is finally proposed. This way of choosing of the regularization parameter for the Sinkhorn barycenter is also analyzed through the prism of an oracle inequality that relates the error made by such data-driven estimators to the one of an ideal estimator.																	2049-8764	2049-8772				DEC	2019	8	4					719	755		10.1093/imaiai/iaz023													
J								The Gromov-Wasserstein distance between networks and stable network invariants	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										asymmetric networks; Gromov-Wasserstein distances; network data analysis; metric measure spaces	SCALING ALGORITHMS; MATRICES	We define a metric-the network Gromov-Wasserstein distance-on weighted, directed networks that is sensitive to the presence of outliers. In addition to proving its theoretical properties, we supply network invariants based on optimal transport that approximate this distance by means of lower bounds. We test these methods on a range of simulated network datasets and on a dataset of real-world global bilateral migration. For our simulations, we define a network generative model based on the stochastic block model. This may be of independent interest for benchmarking purposes.																	2049-8764	2049-8772				DEC	2019	8	4					757	787		10.1093/imaiai/iaz026													
J								Adaptive optimal transport	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										optimal transport; entropy; minimax		An adaptive, adversarial methodology is developed for the optimal transport problem between two distributions mu and nu, known only through a finite set of independent samples (x(i)) i=1..n and (y(j)) j=1..m. The methodology automatically creates features that adapt to the data, thus avoiding reliance on a priori knowledge of the distributions underlying the data. Specifically, instead of a discrete point-by-point assignment, the new procedure seeks an optimal map T(x) defined for all x, minimizing the Kullback-Leibler divergence between (T(x(i))) and the target (y(j)). The relative entropy is given a sample-based, variational characterization, thereby creating an adversarial setting: as one player seeks to push forward one distribution to the other, the second player develops features that focus on those areas where the two distributions fail to match. The procedure solves local problems that seek the optimal transfer between consecutive, intermediate distributions between mu and nu. As a result, maps of arbitrary complexity can be built by composing the simple maps used for each local problem. Displaced interpolation is used to guarantee global from local optimality. The procedure is illustrated through synthetic examples in one and two dimensions.																	2049-8764	2049-8772				DEC	2019	8	4					789	816		10.1093/imaiai/iaz008													
J								A central limit theorem for L-p transportation cost on the real line with application to fairness assessment in machine learning	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										optimal transport; Monge-Kantorovich distance; central limit theorem; fair learning	WASSERSTEIN DISTANCE; GOODNESS; ASYMPTOTICS; TESTS	We provide a central limit theorem for the Monge-Kantorovich distance between two empirical distributions with sizes n and m, W-p(P-n, Q(m)), p >= 1, for observations on the real line. In the case p > 1 our assumptions are sharp in terms of moments and smoothness. We prove results dealing with the choice of centring constants. We provide a consistent estimate of the asymptotic variance, which enables to build two sample tests and confidence intervals to certify the similarity between two distributions. These are then used to assess a new criterion of data set fairness in classification.																	2049-8764	2049-8772				DEC	2019	8	4					817	849		10.1093/imaiai/iaz016													
J								The Unbearable Shallow Understanding of Deep Learning	MINDS AND MACHINES										Artificial neural networks; Deep learning; Philosophy of science; Heuristic appraisal; Visual system	INVARIANT OBJECT RECOGNITION; NEURAL-NETWORKS; FUNCTIONAL ARCHITECTURE; RECEPTIVE-FIELDS; MODEL; REPRESENTATIONS; RECONSTRUCTION; CONNECTIONISM; NEOCOGNITRON; ORGANIZATION	This paper analyzes the rapid and unexpected rise of deep learning within Artificial Intelligence and its applications. It tackles the possible reasons for this remarkable success, providing candidate paths towards a satisfactory explanation of why it works so well, at least in some domains. A historical account is given for the ups and downs, which have characterized neural networks research and its evolution from "shallow" to "deep" learning architectures. A precise account of "success" is given, in order to sieve out aspects pertaining to marketing or sociology of research, and the remaining aspects seem to certify a genuine value of deep learning, calling for explanation. The alleged two main propelling factors for deep learning, namely computing hardware performance and neuroscience findings, are scrutinized, and evaluated as relevant but insufficient for a comprehensive explanation. We review various attempts that have been made to provide mathematical foundations able to justify the efficiency of deep learning, and we deem this is the most promising road to follow, even if the current achievements are too scattered and relevant for very limited classes of deep neural models. The authors' take is that most of what can explain the very nature of why deep learning works at all and even very well across so many domains of application is still to be understood and further research, which addresses the theoretical foundation of artificial learning, is still very much needed.																	0924-6495	1572-8641				DEC	2019	29	4					515	553		10.1007/s11023-019-09512-8													
J								Algorithmic Decision-Making and the Control Problem	MINDS AND MACHINES										Control; Artificial intelligence; Human-in-the-loop; Human-machine systems; Human-computer interaction; Human factors; Ironies of automation; Machine learning	AUTOMATION; IRONIES; SYSTEM	The danger of human operators devolving responsibility to machines and failing to detect cases where they fail has been recognised for many years by industrial psychologists and engineers studying the human operators of complex machines. We call it "the control problem", understood as the tendency of the human within a human-machine control loop to become complacent, over-reliant or unduly diffident when faced with the outputs of a reliable autonomous system. While the control problem has been investigated for some time, up to this point its manifestation in machine learning contexts has not received serious attention. This paper aims to fill that gap. We argue that, except in certain special circumstances, algorithmic decision tools should not be used in high-stakes or safety-critical decisions unless the systems concerned are significantly "better than human" in the relevant domain or subdomain of decision-making. More concretely, we recommend three strategies to address the control problem, the most promising of which involves a complementary (and potentially dynamic) coupling between highly proficient algorithmic tools and human agents working alongside one another. We also identify six key principles which all such human-machine systems should reflect in their design. These can serve as a framework both for assessing the viability of any such human-machine system as well as guiding the design and implementation of such systems generally.																	0924-6495	1572-8641				DEC	2019	29	4					555	578		10.1007/s11023-019-09513-7													
J								The Design of GDPR-Abiding Drones Through Flight Operation Maps: A Win-Win Approach to Data Protection, Aerospace Engineering, and Risk Management	MINDS AND MACHINES										Data protection; Data protection impact assessment (DPIA); Design; Drones; GDPR; Risk management; Unmanned aerial system (UAS); Unmanned aerial vehicle (UAV)		Risk management is a well-known method to face technological challenges through a win-win combination of protective and proactive approaches, fostering the collaboration of operators, researchers, regulators, and industries for the exploitation of new markets. In the field of autonomous and unmanned aerial systems, or UAS, a considerable amount of work has been devoted to risk analysis, the generation of ground risk maps, and ground risk assessment by estimating the fatality rate. The paper aims to expand this approach with a tool for managing data protection risks raised by drones through the design of flight maps. The tool should allow UAS operators choosing the best air corridor for their drones based on the so-called privacy by design principle pursuant to Article 25 of the EU data protection regulation, the GDPR. Among the manifold applications of this approach, the design of fly zones for drones can be tailored for public authorities in the phase of authorization of new operations, much as for national Data Protection authorities that have to control the lawfulness of personal data processing by UAS operations. The overall aim is to present the first win-win approach to data protection issues, aerospace engineering challenges, and risk management methods for the threats posed by this technology.																	0924-6495	1572-8641				DEC	2019	29	4					579	601		10.1007/s11023-019-09511-9													
J								Mirrored Orthogonal Sampling for Covariance Matrix Adaptation Evolution Strategies	EVOLUTIONARY COMPUTATION										Convergence of numerical methods; evolution strategies; mirrored orthogonal sampling		Generating more evenly distributed samples in high dimensional search spaces is the major purpose of the recently proposed mirrored sampling technique for evolution strategies. The diversity of the mutation samples is enlarged and the convergence rate is therefore improved by the mirrored sampling. Motivated by the mirrored sampling technique, this article introduces a new derandomized sampling technique called mirrored orthogonal sampling. The performance of this new technique is both theoretically analyzed and empirically studied on the sphere function. In particular, the mirrored orthogonal sampling technique is applied to the well-known Covariance Matrix Adaptation Evolution Strategy (CMA-ES). The resulting algorithm is experimentally tested on the well-known Black-Box Optimization Benchmark (BBOB). By comparing the results from the benchmark, mirrored orthogonal sampling is found to outperform both the standard CMA-ES and its variant using mirrored sampling.																	1063-6560	1530-9304				DEC	2019	27	4					699	725		10.1162/evco_a_00251													
J								Every Local Minimum Value Is the Global Minimum Value of Induced Model in Nonconvex Machine Learning	NEURAL COMPUTATION												For nonconvex optimization in machine learning, this article proves that every local minimum achieves the globally optimal value of the perturbable gradient basis model at any differentiable point. As a result, nonconvex machine learning is theoretically as supported as convex machine learning with a handcrafted basis in terms of the loss at differentiable local minima, except in the case when a preference is given to the handcrafted basis over the perturbable gradient basis. The proofs of these results are derived under mild assumptions. Accordingly, the proven results are directly applicable to many machine learning models, including practical deep neural networks, without any modification of practical methods. Furthermore, as special cases of our general results, this article improves or complements several state-of-the-art theoretical results on deep neural networks, deep residual networks, and overparameterized deep neural networks with a unified proof technique and novel geometric insights. A special case of our results also contributes to the theoretical foundation of representation learning.																	0899-7667	1530-888X				DEC	2019	31	12					2293	2323		10.1162/neco_a_01234													
J								FUNDAMENTAL LIMITATIONS OF THE DECAY OF GENERALIZED ENERGY IN CONTROLLED (DISCRETE-TIME) NONLINEAR SYSTEMS SUBJECT TO STATE AND INPUT CONSTRAINTS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										decay rate maximization; Lyapunov function; nonlinear control systems	LINEAR-SYSTEMS; PERFORMANCE ANALYSIS; LYAPUNOV FUNCTIONS	This paper is devoted to the analysis of fundamental limitations regarding closed-loop control performance of discrete-time nonlinear systems subject to hard constraints (which are nonlinear in state and manipulated input variables). The control performance for the problem of interest is quantified by the decline (decay) of the generalized energy of the controlled system. The paper develops (upper and lower) barriers bounding the decay of the system's generalized energy, which can be achieved over a set of asymptotically stabilizing feedback laws. The corresponding problem is treated without the loss of generality, resulting in a theoretical framework that provides a solid basis for practical implementations. To enhance understanding, the main results are illustrated in a simple example.																	1641-876X	2083-8492				DEC	2019	29	4					629	639		10.2478/amcs-2019-0046													
J								MULTIQUERY MOTION PLANNING IN UNCERTAIN SPACES: INCREMENTAL ADAPTIVE RANDOMIZED ROADMAPS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										motion planning; uncertainty; roadmaps; sampling; obstacle avoidance	ALGORITHM	Sampling-based motion planning is a powerful tool in solving the motion planning problem for a variety of different robotic platforms. As its application domains grow, more complicated planning problems arise that challenge the functionality of these planners. One of the main challenges in the implementation of a sampling-based planner is its weak performance when reacting to uncertainty in robot motion, obstacles motion, and sensing noise. In this paper, a multi-query sampling-based planner is presented based on the optimal probabilistic roadmaps algorithm that employs a hybrid sample classification and graph adjustment strategy to handle diverse types of planning uncertainty such as sensing noise, unknown static and dynamic obstacles and an inaccurate environment map in a discrete-time system. The proposed method starts by storing the collision-free generated samples in a matrix-grid structure. Using the resulting grid structure makes it computationally cheap to search and find samples in a specific region. As soon as the robot senses an obstacle during the execution of the initial plan, the occupied grid cells are detected, relevant samples are selected, and in-collision vertices are removed within the vision range of the robot. Furthermore, a second layer of nodes connected to the current direct neighbors are checked against collision, which gives the planner more time to react to uncertainty before getting too close to an obstacle. The simulation results for problems with various sources of uncertainty show a significant improvement compared with similar algorithms in terms of the failure rate. the processing time and the minimum distance from obstacles. The planner is also successfully implemented and tested on a TurtleBot in four different scenarios with uncertainty.																	1641-876X	2083-8492				DEC	2019	29	4					641	654		10.2478/amcs-2019-0047													
J								PI OBSERVER DESIGN FOR A CLASS OF NONDIFFERENTIALLY FLAT SYSTEMS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										model-free-base PI reduced-order observer; synchronization; nondifferentially flat system; algebraic observability condition; uniform ultimate boundedness	SYNCHRONIZATION; STABILIZATION	This paper presents a methodology and design of a model-free-based proportional-integral reduced-order observer for a class of nondifferentially flat systems. The problem is tackled from a differential algebra point of view, that is. the state observer for nondifferentially flat systems is based on algebraic differential polynomials of the output. The observation problem is treated together with that of a synchronization between a chaotic system and the designed observer. Some basic notions of differential algebra and concepts related to chaotic synchronization are introduced. The PI observer design methodology is given and it is proven that the estimation error is uniformly ultimately bounded. To exemplify the effectiveness of the PI observer, some cases and their respective numerical simulation results are presented.																	1641-876X	2083-8492				DEC	2019	29	4					655	665		10.2478/amcs-2019-0048													
J								A SPECTRAL METHOD OF THE ANALYSIS OF LINEAR CONTROL SYSTEMS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										anisotropy-based control theory; anisotropic norm; H-2-norm; H-infinity-norm		A spectral method of the analysis of linear control systems is considered. Within the framework of this approach the sigma-entropy of input signals and the sigma-entropy norm of systems are introduced. The invariance of the introduced norm makes it possible to get invariant results of sigma-entropy analysis.																	1641-876X	2083-8492				DEC	2019	29	4					667	679		10.2478/amcs-2019-0049													
J								STABILITY AND TRANSPARENCY OF DELAYED BILATERAL TELEOPERATION WITH HAPTIC FEEDBACK	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										teleoperation; force control; stability; transparency	POSITION; CONTROLLER; INTERNET; IMPEDANCE; TRACKING	Haptic guidance can improve control accuracy in bilateral teleoperation. With haptic sensing, the human operator feels that he grabs the robot on the remote side. There are results on the stability and transparency analysis of teleoperation without haptic guidance, and the analysis of teleoperation with haptic feedback is only for linear and zero time-delay systems. In this paper, we consider more general cases: the bilateral teleoperation systems have time-varying communication delays, the whole systems are nonlinear, and they have force feedback. By using the admittance human operator model, we propose a new control scheme with the interaction passivity of the teleoperator. The stability and transparency of the master-slave system are proven with the Lyapunov-Krasovskii method. Numerical simulations illustrate the efficiency of the proposed control methods.																	1641-876X	2083-8492				DEC	2019	29	4					681	692		10.2478/amcs-2019-0050													
J								A TRAJECTORY PLANNING BASED CONTROLLER TO REGULATE AN UNCERTAIN 3D OVERHEAD CRANE SYSTEM	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										overhead crane; adaptive control; passivity; trajectory planning; Barbalat's lemma	PREDICTIVE CONTROL; ADAPTIVE-CONTROL	We introduce a control strategy to solve the regulation control problem. from the perspective of trajectory planning. for an uncertain 3D overhead crane. The proposed solution was developed based on an adaptive control approach that takes advantage of the passivity properties found in this kind of systems. We use a trajectory planning approach to preserve the accelerations and velocities inside of realistic ranges. to maintaining the payload movements as close as possible to the origin. To this end, we carefully chose a suitable S-curve based on the Bezier spline, which allows us to efficiently handle the load translation problem, considerably reducing the load oscillations. To perform the convergence analysis, we applied the traditional Lyapunov theory, together with Barbalat's lemma. We assess the effectiveness of our control strategy with convincing numerical simulations.																	1641-876X	2083-8492				DEC	2019	29	4					693	702		10.2478/amcs-2019-0051													
J								ROBUST EXTREMUM SEEKING FOR A SECOND ORDER UNCERTAIN PLANT USING A SLIDING MODE CONTROLLER	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										convex optimization; extremum seeking; continuous-time gradient algorithm; dynamical constrained optimization; unknown functions		This paper suggests a novel continuous-time robust extremum seeking algorithm for an unknown convex function constrained by a dynamical plant with uncertainties. The main idea of the proposed method is to develop a robust closed-loop controller based on sliding modes where the sliding surface takes the trajectory around a zone of the optimal point. We assume that the output of the plant is given by the states and a measure of the function. We show the stability and zone-convergence of the proposed algorithm. In order to validate the proposed method, we present a numerical example.																	1641-876X	2083-8492				DEC	2019	29	4					703	712		10.2478/amcs-2019-0052													
J								A CONSERVATIVE SCHEME WITH OPTIMAL ERROR ESTIMATES FOR A MULTIDIMENSIONAL SPACE-FRACTIONAL GROSS-PITAEVSKII EQUATION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										generalized Gross-Pitaevskii system; Riesz fractional diffusion; discrete uniform Sobolev inequality; conservative method; optimal error bounds	FINITE-DIFFERENCE SCHEMES; NONLINEAR-WAVE EQUATION; NUMERICAL-SOLUTION; PRESERVING METHOD; CALCULUS; SYSTEMS; VORTEX	The present work departs from an extended form of the classical multi-dimensional Gross-Pitaevskii equation, which considers fractional derivatives of the Riesz type in space, a generalized potential function and angular momentum rotation. It is well known that the classical system possesses functionals which are preserved throughout time. It is easy to check that the generalized fractional model considered in this work also possesses conserved quantities, whence the development of conservative and efficient numerical schemes is pragmatically justified. Motivated by these facts, we propose a finite-difference method based on weighted-shifted Grunwald differences to approximate the solutions of the generalized Gross- Pitaevskii system. We provide here a discrete extension of the uniform Sobolev inequality to multiple dimensions, and show that the proposed method is capable of preserving discrete forms of the mass and the energy of the model. Moreover, we establish thoroughly the stability and the convergence of the technique, and provide some illustrative simulations to show that the method is capable of preserving the total mass and the total energy of the generalized system.																	1641-876X	2083-8492				DEC	2019	29	4					713	723		10.2478/amcs-2019-0053													
J								AN LMI-BASED HEURISTIC ALGORITHM FOR VERTEX REDUCTION IN LPV SYSTEMS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										linear parameter varying (LPV) paradigm; linear matrix inequality (LMI); Gershgorin circles; gain scheduling; controller design	FEEDBACK CONTROL; DESIGN; MODELS; ROBUSTNESS; TRACKING	The linear parameter varying (LPV) approach has proved to be suitable for controlling many non-linear systems. However, for those which are highly non-linear and complex, the number of scheduling variables increases rapidly. This fact makes the LPV controller implementation not feasible for many real systems due to memory constraints and computational burden. This paper considers the problem of reducing the total number of LPV controller gains by determining a heuristic methodology that combines two vertices of a polytopic LPV model such that the same gain can be used in both vertices. The proposed algorithm, based on the use of the Gershgorin circles, provides a combinability ranking for the different vertex pairs, which helps in solving the reduction problem in fewer attempts. Simulation examples are provided in order to illustrate the main characteristics of the proposed approach.																	1641-876X	2083-8492				DEC	2019	29	4					725	737		10.2478/amcs-2019-0054													
J								AN ADAPTIVE IDENTIFICATION METHOD BASED ON THE MODULATING FUNCTIONS TECHNIQUE AND EXACT STATE OBSERVERS FOR MODELING AND SIMULATION OF A NONLINEAR MISO GLASS MELTING PROCESS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										continuous systems; modulating functions; system identification; exact state observer; glass forehearths	PARAMETER	The paper presents new concepts of the identification method based on modulating functions and exact state observers with its application for identification of a real continuous-time industrial process. The method enables transformation of a system of differential equations into an algebraic one with the same parameters. Then, these parameters can be estimated using the least-squares approach. The main problem is the nonlinearity of the MISO process and its noticeable transport delays. It requires specific modifications to be introduced into the basic identification algorithm. The main goal of the method is to obtain on-line a temporary linear model of the process around the selected operating point, because fast methods for tuning PID controller parameters for such a model are well known. Hence, a special adaptive identification approach with a moving window is proposed, which involves using on-line registered input and output process data. An optimal identification method for a MISO model assuming decomposition to many inner SISO systems is presented. Additionally, a special version of the modulating functions method, in which both model parameters and unknown delays are identified, is tested on real data sets collected from a glass melting installation.																	1641-876X	2083-8492				DEC	2019	29	4					739	757		10.2478/amcs-2019-0055													
J								AN AUTOMATIC COLLISION AVOIDANCE ALGORITHM FOR MULTIPLE MARINE SURFACE VEHICLES	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										unmanned surface vehicles; obstacle avoidance; control allocation; constrained convex optimization	DECISION-MAKING; SHIP; NAVIGATION; OPTIMIZATION; COLREGS; FUZZY; MODEL	In recent years, unmanned surface vehicles have been widely used in various applications from military to civil domains. Seaports are crowded and ship accidents have increased. Thus, collision accidents occur frequently mainly due to human errors even though international regulations for preventing collisions at seas (COLREGs) have been established. In this paper, we propose a real-time obstacle avoidance algorithm for multiple autonomous surface vehicles based on constrained convex optimization. The proposed method is simple and fast in its implementation, and the solution converges to the optimal decision. The algorithm is combined with the PD-feedback linearization controller to track the generated path and to reach the target safely. Forces and azimuth angles are efficiently distributed using a control allocation technique. To show the effectiveness of the proposed collision-free path-planning algorithm, numerical simulations are performed.																	1641-876X	2083-8492				DEC	2019	29	4					759	768		10.2478/amcs-2019-0056													
J								USING INFORMATION ON CLASS INTERRELATIONS TO IMPROVE CLASSIFICATION OF MULTICLASS IMBALANCED DATA: A NEW RESAMPLING ALGORITHM	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										imbalanced data; multi-class learning; re-sampling; data difficulty factors; similarity degrees		The relations between multiple unbalanced classes can be handled with a specialized approach which evaluates types of examples' difficulty based on an analysis of the class distribution in the examples' neighborhood. additionally exploiting information about the similarity of neighboring classes. In this paper, we demonstrate that such an approach can be implemented as a data preprocessing technique and that it can improve the performance of various classifiers on multiclass Unbalanced datasets. It has led us to the introduction of a new resampling algorithm, called Similarity Oversampling and Undersampling Preprocessing (SOUP), which resamples examples according to their difficulty. Its experimental evaluation on real and artificial datasets has shown that it is competitive with the most popular decomposition ensembles and better than specialized preprocessing techniques for multi-imbalanced problems.																	1641-876X	2083-8492				DEC	2019	29	4					769	781		10.2478/amcs-2019-0057													
J								REVISITING THE OPTIMAL PROBABILITY ESTIMATOR FROM SMALL SAMPLES FOR DATA MINING	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										probability estimation; small samples; minimal error; m-estimate	STATISTICAL COMPARISONS; MULTIPLE; CLASSIFIERS	Estimation of probabilities from empirical data samples has drawn close attention in the scientific community and has been identified as a crucial phase in many machine learning and knowledge discovery research projects and applications. In addition to trivial and straightforward estimation with relative frequency, more elaborated probability estimation methods from small samples were proposed and applied in practice (e.g., Laplace's rule, the m-estimate). Piegat and Landowski (2012) proposed a novel probability estimation method from small samples Ep(h root 2) that is optimal according to the mean absolute error of the estimation result. In this paper we show that, even though the articulation of Piegat's formula seems different, it is in fact a special case of the m-estimate, where p(a) = 1/2 and m = root 2 the context of an experimental framework, we present an in-depth analysis of several probability estimation methods with respect to their mean absolute errors and demonstrate their potential advantages and disadvantages. We extend the analysis from single instance samples to samples with a moderate number of instances. We define small samples for the purpose of estimating probabilities as samples containing either less than four successes or less than four failures and justify the definition by analysing probability estimation errors on various sample sizes.																	1641-876X	2083-8492				DEC	2019	29	4					783	796		10.2478/amcs-2019-0058													
J								PASSWORD-AUTHENTICATED GROUP KEY ESTABLISHMENT FROM SMOOTH PROJECTIVE HASH FUNCTIONS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										group key exchange; password authentication; smooth projective hashing	EXCHANGE SECURE; PARADIGM; 2-PARTY	Password-authenticated key exchange (PAKE) protocols allow users sharing a password to agree upon a high entropy secret. Thus, they can be implemented without complex infrastructures that typically involve public keys and certificates. In this paper, a provably secure password-authenticated protocol for group key establishment in the common reference string (CRS) model is presented. While prior constructions of the group (PAKE) can be found in the literature, most of them rely on idealized assumptions, which we do not make here. Furthermore, our protocol is quite efficient, as regardless of the number of involved participants it can be implemented with only three communication rounds. We use a (by now classical) trick of Burmester and Desmedt for deriving group key exchange protocols using a two-party construction as the main building block. In our case, the two-party PAKE used as a base is a one-round protocol by Katz and Vaikuntanathan, which in turn builds upon a special kind of smooth projective hash functions (KV-SPHFs). Smooth projective hash functions (SPHFs) were first introduced by Cramer and Shoup (2002) as a valuable cryptographic primitive for deriving provable secure encryption schemes. These functions and their variants proved useful in many other scenarios. We use here as a main tool a very strong type of SPHF, introduced by Katz and Vaikuntanathan for building a one-round password based two party key exchange protocol. As evidenced by Ben Hamouda et al. (2013). KV-SPHFs can be instantiated on Cramer-Shoup ciphertexts, thus yielding very efficient (and pairing free) constructions.																	1641-876X	2083-8492				DEC	2019	29	4					797	815		10.2478/amcs-2019-0059													
J								IMPROVED REFERENCE IMAGE ENCRYPTION METHODS BASED ON 2(K) CORRECTION IN THE INTEGER WAVELET DOMAIN	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										visually meaningful image encryption; 2(k) correction; discrete wavelet transform; least significant bits embedding	SCHEME; WATERMARKING; ROBUST; ALGORITHM	Many visually meaningful image encryption (VMIE) methods have been proposed in the literature using reference encryption. However, the most important problems of these methods are low visual quality and blindness. Owing to the low visual quality, the pre-encrypted image can be analyzed simply from the reference image and, in order to decrypt nonblind methods, users should use original reference images. In this paper, two novel reference image encryption methods based on the integer DWT (discrete wavelet transform) using 2(k) correction are proposed. These methods are blind and have high visual quality, as well as short execution times. The main aim of the proposed methods is to solve the problem of the three VMIE methods existing in the literature. The proposed methods mainly consist of the integer DWT, pre-encrypted image embedding by kLSBs (k least significant bits) and 2(k) correction. In the decryption phase, the integer DWT and pre-encrypted image extraction with the mod operator are used. Peak signal-to-noise ratio (PSNR) measures the performances of the proposed methods. Experimental results clearly illustrate that the proposed methods improve the visual quality of the reference image encryption methods. Overall, 2(k) correction and kLSBs provide high visual quality and blindness.																	1641-876X	2083-8492				DEC	2019	29	4					817	829		10.2478/amcs-2019-0060													
J								A Pareto-Based Hybrid Whale Optimization Algorithm with Tabu Search for Multi-Objective Optimization	ALGORITHMS										Multi-Objective Optimization; Multi-Objective Problems; Pareto Optimization; Swarm Intelligence; Tabu Search; Whale Optimization Algorithm	EVOLUTIONARY ALGORITHM	Multi-Objective Problems (MOPs) are common real-life problems that can be found in different fields, such as bioinformatics and scheduling. Pareto Optimization (PO) is a popular method for solving MOPs, which optimizes all objectives simultaneously. It provides an effective way to evaluate the quality of multi-objective solutions. Swarm Intelligence (SI) methods are population-based methods that generate multiple solutions to the problem, providing SI methods suitable for MOP solutions. SI methods have certain drawbacks when applied to MOPs, such as swarm leader selection and obtaining evenly distributed solutions over solution space. Whale Optimization Algorithm (WOA) is a recent SI method. In this paper, we propose combining WOA with Tabu Search (TS) for MOPs (MOWOATS). MOWOATS uses TS to store non-dominated solutions in elite lists to guide swarm members, which overcomes the swarm leader selection problem. MOWOATS employs crossover in both intensification and diversification phases to improve diversity of the population. MOWOATS proposes a new diversification step to eliminate the need for local search methods. MOWOATS has been tested over different benchmark multi-objective test functions, such as CEC2009, ZDT, and DTLZ. Results present the efficiency of MOWOATS in finding solutions near Pareto front and evenly distributed over solution space.																		1999-4893				DEC	2019	12	12							261	10.3390/a12120261													
J								Some Results on Shop Scheduling with S-Precedence Constraints among Job Tasks	ALGORITHMS										job shop; flow shop; s-precedence constraints; exact algorithms; complexity	PARALLEL MACHINES	We address some special cases of job shop and flow shop scheduling problems with s-precedence constraints. Unlike the classical setting, in which precedence constraints among the tasks of a job are finish-start, here the task of a job cannot start before the task preceding it has started. We give polynomial exact algorithms for the following problems: a two-machine job shop with two jobs when recirculation is allowed (i.e., jobs can visit the same machine many times), a two-machine flow shop, and an m-machine flow shop with two jobs. We also point out some special cases whose complexity status is open.																		1999-4893				DEC	2019	12	12							250	10.3390/a12120250													
J								SVM-Based Multiple Instance Classification via DC Optimization	ALGORITHMS										multiple instance learning; support vector machine; DC optimization; nonsmooth optimization	BUNDLE METHOD; CONIC SEPARATION; LEARNING-METHOD	A multiple instance learning problem consists of categorizing objects, each represented as a set (bag) of points. Unlike the supervised classification paradigm, where each point of the training set is labeled, the labels are only associated with bags, while the labels of the points inside the bags are unknown. We focus on the binary classification case, where the objective is to discriminate between positive and negative bags using a separating surface. Adopting a support vector machine setting at the training level, the problem of minimizing the classification-error function can be formulated as a nonconvex nonsmooth unconstrained program. We propose a difference-of-convex (DC) decomposition of the nonconvex function, which we face using an appropriate nonsmooth DC algorithm. Some of the numerical results on benchmark data sets are reported.																		1999-4893				DEC	2019	12	12							249	10.3390/a12120249													
J								FPT Algorithms for Diverse Collections of Hitting Sets	ALGORITHMS										solution diversity; fixed-parameter tractability; hitting sets; vertex cover; feedback vertex set; Hamming distance	COMPLEXITY; NUMBER	In this work, we study the d-Hitting Set and Feedback Vertex Set problems through the paradigm of finding diverse collections of r solutions of size at most k each, which has recently been introduced to the field of parameterized complexity. This paradigm is aimed at addressing the loss of important side information which typically occurs during the abstraction process that models real-world problems as computational problems. We use two measures for the diversity of such a collection: the sum of all pairwise Hamming distances, and the minimum pairwise Hamming distance. We show that both problems are fixed-parameter tractable in k+r for both diversity measures. A key ingredient in our algorithms is a (problem independent) network flow formulation that, given a set of 'base' solutions, computes a maximally diverse collection of solutions. We believe that this could be of independent interest.																		1999-4893				DEC	2019	12	12							254	10.3390/a12120254													
J								Parameterized Algorithms in Bioinformatics: An Overview	ALGORITHMS										fixed-parameter tractability; genome assembly; sequence analysis; comparative genomics; haplotyping; phylogenetics; agreement forests	MULTIPLE SEQUENCE ALIGNMENT; MAXIMUM AGREEMENT FOREST; TREE CONTAINMENT-PROBLEM; LINEAR-TIME ALGORITHM; FIXED-PARAMETER; HYBRIDIZATION NUMBER; APPROXIMATION ALGORITHMS; HAPLOTYPE INFERENCE; PHYLOGENETIC TREES; PURE PARSIMONY	Bioinformatics regularly poses new challenges to algorithm engineers and theoretical computer scientists. This work surveys recent developments of parameterized algorithms and complexity for important NP-hard problems in bioinformatics. We cover sequence assembly and analysis, genome comparison and completion, and haplotyping and phylogenetics. Aside from reporting the state of the art, we give challenges and open problems for each topic.																		1999-4893				DEC	2019	12	12							256	10.3390/a12120256													
J								Application and Evaluation of Surrogate Models for Radiation Source Search	ALGORITHMS										surrogate modeling; bayesian inference; radiation source localization	OPTIMIZATION	Surrogate models are increasingly required for applications in which first-principles simulation models are prohibitively expensive to employ for uncertainty analysis, design, or control. They can also be used to approximate models whose discontinuous derivatives preclude the use of gradient-based optimization or data assimilation algorithms. We consider the problem of inferring the 2D location and intensity of a radiation source in an urban environment using a ray-tracing model based on Boltzmann transport theory. Whereas the code implementing this model is relatively efficient, extension to 3D Monte Carlo transport simulations precludes subsequent Bayesian inference to infer source locations, which typically requires thousands to millions of simulations. Additionally, the resulting likelihood exhibits discontinuous derivatives due to the presence of buildings. To address these issues, we discuss the construction of surrogate models for optimization, Bayesian inference, and uncertainty propagation. Specifically, we consider surrogate models based on Legendre polynomials, multivariate adaptive regression splines, radial basis functions, Gaussian processes, and neural networks. We detail strategies for computing training points and discuss the merits and deficits of each method.																		1999-4893				DEC	2019	12	12							269	10.3390/a12120269													
J								Storage Efficient Trajectory Clustering and k-NN for Robust Privacy Preserving Spatio-Temporal Databases	ALGORITHMS										Hough transformation; k-anonymity; privacy preservation; trajectories compression	FRAMEWORK	The need to store massive volumes of spatio-temporal data has become a difficult task as GPS capabilities and wireless communication technologies have become prevalent to modern mobile devices. As a result, massive trajectory data are produced, incurring expensive costs for storage, transmission, as well as query processing. A number of algorithms for compressing trajectory data have been proposed in order to overcome these difficulties. These algorithms try to reduce the size of trajectory data, while preserving the quality of the information. In the context of this research work, we focus on both the privacy preservation and storage problem of spatio-temporal databases. To alleviate this issue, we propose an efficient framework for trajectories representation, entitled DUST (DUal-based Spatio-temporal Trajectory), by which a raw trajectory is split into a number of linear sub-trajectories which are subjected to dual transformation that formulates the representatives of each linear component of initial trajectory; thus, the compressed trajectory achieves compression ratio equal to M:1. To our knowledge, we are the first to study and address k-NN queries on nonlinear moving object trajectories that are represented in dual dimensional space. Additionally, the proposed approach is expected to reinforce the privacy protection of such data. Specifically, even in case that an intruder has access to the dual points of trajectory data and try to reproduce the native points that fit a specific component of the initial trajectory, the identity of the mobile object will remain secure with high probability. In this way, the privacy of the k-anonymity method is reinforced. Through experiments on real spatial datasets, we evaluate the robustness of the new approach and compare it with the one studied in our previous work.																		1999-4893				DEC	2019	12	12							266	10.3390/a12120266													
J								An Unknown Radar Emitter Identification Method Based on Semi-Supervised and Transfer Learning	ALGORITHMS										semi-supervised learning; transfer learning; radar emitter		Aiming at the current problem that it is difficult to deal with an unknown radar emitter in the radar emitter identification process, we propose an unknown radar emitter identification method based on semi-supervised and transfer learning. Firstly, we construct the support vector machine (SVM) model based on transfer learning, using the information of labeled samples in the source domain to train in the target domain, which can solve the problem that the training data and the testing data do not satisfy the same-distribution hypothesis. Then, we design a semi-supervised co-training algorithm using the information of unlabeled samples to enhance the training effect, which can solve the problem that insufficient labeled data results in inadequate training of the classifier. Finally, we combine the transfer learning method with the semi-supervised learning method for the unknown radar emitter identification task. Simulation experiments show that the proposed method can effectively identify an unknown radar emitter and still maintain high identification accuracy within a certain measurement error range.																		1999-4893				DEC	2019	12	12							271	10.3390/a12120271													
J								Linking Scheduling Criteria to Shop Floor Performance in Permutation Flowshops	ALGORITHMS										scheduling; shop floor performance; flowshop; manufacturing	COMPLETION-TIME VARIANCE; COLONY OPTIMIZATION ALGORITHM; MINIMIZE MAKESPAN; N-JOB; HEURISTICS; BENCHMARKS; SEQUENCES; FLOWTIME	The goal of manufacturing scheduling is to allocate a set of jobs to the machines in the shop so these jobs are processed according to a given criterion (or set of criteria). Such criteria are based on properties of the jobs to be scheduled (e.g., their completion times, due dates); so it is not clear how these (short-term) criteria impact on (long-term) shop floor performance measures. In this paper, we analyse the connection between the usual scheduling criteria employed as objectives in flowshop scheduling (e.g., makespan or idle time), and customary shop floor performance measures (e.g., work-in-process and throughput). Two of these linkages can be theoretically predicted (i.e., makespan and throughput as well as completion time and average cycle time), and the other such relationships should be discovered on a numerical/empirical basis. In order to do so, we set up an experimental analysis consisting in finding optimal (or good) schedules under several scheduling criteria, and then computing how these schedules perform in terms of the different shop floor performance measures for several instance sizes and for different structures of processing times. Results indicate that makespan only performs well with respect to throughput, and that one formulation of idle times obtains nearly as good results as makespan, while outperforming it in terms of average cycle time and work in process. Similarly, minimisation of completion time seems to be quite balanced in terms of shop floor performance, although it does not aim exactly at work-in-process minimisation, as some literature suggests. Finally, the experiments show that some of the existing scheduling criteria are poorly related to the shop floor performance measures under consideration. These results may help to better understand the impact of scheduling on flowshop performance, so scheduling research may be more geared towards shop floor performance, which is sometimes suggested as a cause for the lack of applicability of some scheduling models in manufacturing.																		1999-4893				DEC	2019	12	12							263	10.3390/a12120263													
J								Solving Integer Linear Programs by Exploiting Variable-Constraint Interactions: A Survey	ALGORITHMS										integer linear programming; parameterized complexity; treewidth; treedepth	CLIQUE-WIDTH; COMPLEXITY; ALGORITHM	Integer Linear Programming (ILP) is among the most successful and general paradigms for solving computationally intractable optimization problems in computer science. ILP is NP-complete, and until recently we have lacked a systematic study of the complexity of ILP through the lens of variable-constraint interactions. This changed drastically in recent years thanks to a series of results that together lay out a detailed complexity landscape for the problem centered around the structure of graphical representations of instances. The aim of this survey is to summarize these recent developments, put them into context and a unified format, and make them more approachable for experts from many diverse backgrounds.																		1999-4893				DEC	2019	12	12							248	10.3390/a12120248													
J								Graph Theory Approach to the Vulnerability of Transportation Networks	ALGORITHMS										vulnerability; domination; edge-domination; transportation networks; bondage-connected number; weighted bondage-connected number	DOMINATING SETS; BONDAGE NUMBER; SYSTEMS; RELIABILITY; RESILIENCE	Nowadays, transport is the basis for the functioning of national, continental, and global economies. Thus, many governments recognize it as a critical element in ensuring the daily existence of societies in their countries. Those responsible for the proper operation of the transport sector must have the right tools to model, analyze, and optimize its elements. One of the most critical problems is the need to prevent bottlenecks in transport networks. Thus, the main aim of the article was to define the parameters characterizing the transportation network vulnerability and select algorithms to support their search. The parameters proposed are based on characteristics related to domination in graph theory. The domination, edge-domination concepts, and related topics, such as bondage-connected and weighted bondage-connected numbers, were applied as the tools for searching and identifying the bottlenecks in transportation networks. Furthermore, the algorithms for finding the minimal dominating set and minimal (maximal) weighted dominating sets are proposed. This way, the exemplary academic transportation network was analyzed in two cases: stationary and dynamic. Some conclusions are presented. The main one is the fact that the methods given in this article are universal and applicable to both small and large-scale networks. Moreover, the approach can support the dynamic analysis of bottlenecks in transport networks.																		1999-4893				DEC	2019	12	12							270	10.3390/a12120270													
J								Modeling and Solving Scheduling Problem with m Uniform Parallel Machines Subject to Unavailability Constraints	ALGORITHMS										uniform parallel machines; unavailability constraints; makespan; quadratic programming; optimal algorithm	AVAILABILITY; MINIMIZE	The problem investigated in this paper is scheduling on uniform parallel machines, taking into account that machines can be periodically unavailable during the planning horizon. The objective is to determine planning for job processing so that the makespan is minimal. The problem is known to be NP-hard. A new quadratic model was developed. Because of the limitation of the aforementioned model in terms of problem sizes, a novel algorithm was developed to tackle big-sized instances. This consists of mainly two phases. The first phase generates schedules using a modified Largest Processing Time (LPT)-based procedure. Then, theses schedules are subject to further improvement during the second phase. This improvement is obtained by simultaneously applying pairwise job interchanges between machines. The proposed algorithm and the quadratic model were implemented and tested on variously sized problems. Computational results showed that the developed quadratic model could optimally solve small- to medium-sized problem instances. However, the proposed algorithm was able to optimally solve large-sized problems in a reasonable time.																		1999-4893				DEC	2019	12	12							247	10.3390/a12120247													
J								Pre and Postprocessing for JPEG to Handle Large Monochrome Images	ALGORITHMS										image compression; JPEG; hyperbolic function; PSNR; SSIM; compression ratio; optimization	COMPRESSION; ENHANCEMENT	Image compression is one of the most important fields of image processing. Because of the rapid development of image acquisition which will increase the image size, and in turn requires bigger storage space. JPEG has been considered as the most famous and applicable algorithm for image compression; however, it has shortfalls for some image types. Hence, new techniques are required to improve the quality of reconstructed images as well as to increase the compression ratio. The work in this paper introduces a scheme to enhance the JPEG algorithm. The proposed scheme is a new method which shrinks and stretches images using a smooth filter. In order to remove the blurring artifact which would be developed from shrinking and stretching the image, a hyperbolic function (tanh) is used to enhance the quality of the reconstructed image. Furthermore, the new approach achieves higher compression ratio for the same image quality, and/or better image quality for the same compression ratio than ordinary JPEG with respect to large size and more complex content images. However, it is an application for optimization to enhance the quality (PSNR and SSIM), of the reconstructed image and to reduce the size of the compressed image, especially for large size images.																		1999-4893				DEC	2019	12	12							255	10.3390/a12120255													
J								Damage Diagnosis of Reactive Powder Concrete under Fatigue Loading Using 3D Laser Scanning Technology	ALGORITHMS										3D laser scanning; image processing; texture analysis; gray level co-occurrence matrix; damage monitoring; feature extraction	IDENTIFICATION; MECHANISMS; BEHAVIOR	Damage mechanisms of Reactive Powder Concrete (RPC) under fatigue loading are investigated using the 3D laser scanning technology. An independently configured 3D laser scanning system is used to monitor the damaging procedure. Texture analysis technique is also applied to enhance the understanding of the damage mechanisms of RPC under fatigue loading. In order to obtain the characteristic parameters of point cloud data, a point clouds projection algorithm is proposed. Damage evolution is described by the change of point cloud data of the damage in the 2D plane and 3D space during fatigue loading. The Gray Level Co-occurrence Matrix (GLCM) method is used to extract the characteristic parameters to evaluate the statue of the structural. Angular Second Moment and Cluster Shadow of typical sensitive characteristic indexes is screened by using the Digital Feature Screening. The reliability of the damage index was verified by image texture analysis and data expansion. Indexes extracted in this paper can be used as a new structural health monitoring indicator to assess health condition.																		1999-4893				DEC	2019	12	12							260	10.3390/a12120260													
J								Improved Bilateral Filtering for a Gaussian Pyramid Structure-Based Image Enhancement Algorithm	ALGORITHMS										bilateral filtering; Gaussian down-sampling; luminance; enhancement; Retinex	RETINEX THEORY; COLOR	To address the problem of unclear images affected by occlusion from fog, we propose an improved Retinex image enhancement algorithm based on the Gaussian pyramid transformation. Our algorithm features bilateral filtering as a replacement for the Gaussian function used in the original Retinex algorithm. Operation of the technique is as follows. To begin, we deduced the mathematical model for an improved bilateral filtering function based on the spatial domain kernel function and the pixel difference parameter. The input RGB image was subsequently converted into the Hue Saturation Intensity (HSI) color space, where the reflection component of the intensity channel was extracted to obtain an image whose edges were retained and are not affected by changes in brightness. Following reconversion to the RGB color space, color images of this reflection component were obtained at different resolutions using Gaussian pyramid down-sampling. Each of these images was then processed using the improved Retinex algorithm to improve the contrast of the final image, which was reconstructed using the Laplace algorithm. Results from experiments show that the proposed algorithm can enhance image contrast effectively, and the color of the processed image is in line with what would be perceived by a human observer.																		1999-4893				DEC	2019	12	12							258	10.3390/a12120258													
J								Observations on the Computation of Eigenvalue and Eigenvector Jacobians	ALGORITHMS										eigenvector; eigenvalue; Jacobian	CORRESPONDING EIGENVECTORS; COMPLEX EIGENVECTORS; DERIVATIVES; DISTINCT	Many scientific and engineering problems benefit from analytic expressions for eigenvalue and eigenvector derivatives with respect to the elements of the parent matrix. While there exists extensive literature on the calculation of these derivatives, which take the form of Jacobian matrices, there are a variety of deficiencies that have yet to be addressed-including the need for both left and right eigenvectors, limitations on the matrix structure, and issues with complex eigenvalues and eigenvectors. This work addresses these deficiencies by proposing a new analytic solution for the eigenvalue and eigenvector derivatives. The resulting analytic Jacobian matrices are numerically efficient to compute and are valid for the general complex case. It is further shown that this new general result collapses to previously known relations for the special cases of real symmetric matrices and real diagonal matrices. Finally, the new Jacobian expressions are validated using forward finite differencing and performance is compared with another technique.																		1999-4893				DEC	2019	12	12							245	10.3390/a12120245													
J								Planning the Schedule for the Disposal of the Spent Nuclear Fuel with Interactive Multiobjective Optimization	ALGORITHMS										achievement scalarizing functions; interactive method; multiobjective optimization; nonsmooth optimization; spent nuclear fuel disposal	ACHIEVEMENT SCALARIZING FUNCTION; BUNDLE METHOD; NONSMOOTH; CANISTERS	Several countries utilize nuclear power and face the problem of what to do with the spent nuclear fuel. One possibility, which is under the scope in this paper, is to dispose of the fuel assemblies in the disposal facility. Before the assemblies can be disposed of, they must cool down their decay heat power in the interim storage. Next, they are loaded into canisters in the encapsulation facility, and finally, the canisters are placed in the disposal facility. In this paper, we model this process as a nonsmooth multiobjective mixed-integer nonlinear optimization problem with the minimization of nine objectives: the maximum number of assemblies in the storage, maximum storage time, average storage time, total number of canisters, end time of the encapsulation, operation time of the encapsulation facility, the lengths of disposal and central tunnels, and total costs. As a result, we obtain the disposal schedule i.e., amount of canisters disposed of periodically. We introduce the interactive multiobjective optimization method using the two-slope parameterized achievement scalarizing functions which enables us to obtain systematically several different Pareto optimal solutions from the same preference information. Finally, a case study adapting the disposal in Finland is given. The results obtained are analyzed in terms of the objective values and disposal schedules.																		1999-4893				DEC	2019	12	12							252	10.3390/a12120252													
J								Exploring Travelers' Characteristics Affecting their Intention to Shift to Bike-Sharing Systems due to a Sophisticated Mobile App	ALGORITHMS										bike-sharing systems; mobile application (app); ICT; classification tree; binary logit		Many cities have already installed bike-sharing systems for several years now, but especially in recent years with the rise of micro-mobility, many efforts are being made worldwide to improve the operation of these systems. Technology has an essential role to play in the success of micro-mobility schemes, including bike-sharing systems. In this paper, it is examined if a state-of-the-art mobile application (app) can contribute to increasing the usage levels of such a system. It is also seeking to identify groups of travelers, who are more likely to be affected by the sophisticated app. With this aim, a questionnaire survey was designed and addressed to the users of the bike-sharing system of the city of Thessaloniki, Greece, as well as to other residents of the city. Through a descriptive analysis, the most useful services that an app can provide are identified. Most importantly, two different types of predictive models (i.e., classification tree and binary logit model) were applied in order to identify groups of users who are more likely to shift to or to use the bike-sharing system due to the sophisticated app. The results of the two predictive models confirm that people of younger ages and those who are not currently users of the system are those most likely to be attracted to the system due to such an app. Other factors, such as car usage frequency, education, and income also appeared to have slight impact on travelers' intention to use the system more often due to the app.																		1999-4893				DEC	2019	12	12							264	10.3390/a12120264													
J								Estimation of Reliability in a Multicomponent Stress-Strength System for the Exponentiated Moment-Based Exponential Distribution	ALGORITHMS										exponentiated moment-based exponential distribution; reliability; stress; strength; maximum likelihood; Monte Carlo simulation	P(Y-LESS-THAN-X); INFERENCE	A multicomponent system of k components with independent and identically distributed random strengths X1,X2, horizontal ellipsis Xk, with each component undergoing random stress, is in working condition if and only if at least s out of k strengths exceed the subjected stress. Reliability is measured while strength and stress are obtained through a process following an exponentiated moment-based exponential distribution with different shape parameters. Reliability is gauged from the samples using maximum likelihood (ML) on the computed distributions of strength and stress. Asymptotic estimates of reliability are compared using Monte Carlo simulation. Application to forest data and to breaking strengths of jute fiber shows the usefulness of the model.																		1999-4893				DEC	2019	12	12							246	10.3390/a12120246													
J								Using Interval Analysis to Compute the Invariant Set of a Nonlinear Closed-Loop Control System	ALGORITHMS										interval analysis; Lyapunov stability; sliding mode control; invariant sets	SLIDING MODE CONTROL; CONTROL DESIGN; STATE; OBSERVERS	In recent years, many applications, as well as theoretical properties of interval analysis have been investigated. Without any claim for completeness, such applications and methodologies range from enclosing the effect of round-off errors in highly accurate numerical computations over simulating guaranteed enclosures of all reachable states of a dynamic system model with bounded uncertainty in parameters and initial conditions, to the solution of global optimization tasks. By exploiting the fundamental enclosure properties of interval analysis, this paper aims at computing invariant sets of nonlinear closed-loop control systems. For that purpose, Lyapunov-like functions and interval analysis are combined in a novel manner. To demonstrate the proposed techniques for enclosing invariant sets, the systems examined in this paper are controlled via sliding mode techniques with subsequently enclosing the invariant sets by an interval based set inversion technique. The applied methods for the control synthesis make use of a suitably chosen Grobner basis, which is employed to solve Bezout's identity. Illustrating simulation results conclude this paper to visualize the novel combination of sliding mode control with an interval based computation of invariant sets.																		1999-4893				DEC	2019	12	12							262	10.3390/a12120262													
J								Finding Patterns in Signals Using Lossy Text Compression	ALGORITHMS										data compression; run-length; RRLE; periods; robotics; signals	LINEAR-TIME; REPETITIONS; ALGORITHM	Whether the source is autonomous car, robotic vacuum cleaner, or a quadcopter, signals from sensors tend to have some hidden patterns that repeat themselves. For example, typical GPS traces from a smartphone contain periodic trajectories such as "home, work, home, work, MIDLINE HORIZONTAL ELLIPSIS". Our goal in this study was to automatically reverse engineer such signals, identify their periodicity, and then use it to compress and de-noise these signals. To do so, we present a novel method of using algorithms from the field of pattern matching and text compression to represent the "language" in such signals. Common text compression algorithms are less tailored to handle such strings. Moreover, they are lossless, and cannot be used to recover noisy signals. To this end, we define the recursive run-length encoding (RRLE) method, which is a generalization of the well known run-length encoding (RLE) method. Then, we suggest lossy and lossless algorithms to compress and de-noise such signals. Unlike previous results, running time and optimality guarantees are proved for each algorithm. Experimental results on synthetic and real data sets are provided. We demonstrate our system by showing how it can be used to turn commercial micro air-vehicles into autonomous robots. This is by reverse engineering their unpublished communication protocols and using a laptop or on-board micro-computer to control them. Our open source code may be useful for both the community of millions of toy robots users, as well as for researchers that may extend it for further protocols.																		1999-4893				DEC	2019	12	12							267	10.3390/a12120267													
J								Construction Method of Probabilistic Boolean Networks Based on Imperfect Information	ALGORITHMS										gene regulatory network; linear programming; matrix-based representation; probabilistic Boolean network	CONTROLLABILITY; DESIGN	A probabilistic Boolean network (PBN) is well known as one of the mathematical models of gene regulatory networks. In a Boolean network, expression of a gene is approximated by a binary value, and its time evolution is expressed by Boolean functions. In a PBN, a Boolean function is probabilistically chosen from candidates of Boolean functions. One of the authors has proposed a method to construct a PBN from imperfect information. However, there is a weakness that the number of candidates of Boolean functions may be redundant. In this paper, this construction method is improved to efficiently utilize given information. To derive Boolean functions and those selection probabilities, the linear programming problem is solved. Here, we introduce the objective function to reduce the number of candidates. The proposed method is demonstrated by a numerical example.																		1999-4893				DEC	2019	12	12							268	10.3390/a12120268													
J								The Research of Improved Active Disturbance Rejection Control Algorithm for Particleboard Glue System Based on Neural Network State Observer	ALGORITHMS										particleboard glue mixing and dosing system; active disturbance rejection control; neural network state observer; time-delay system	SLIDING MODE CONTROL; PID CONTROLLER; ORDER	For achieving high-performance control for a particleboard glue mixing and dosing control system, which is a time-delay system in low frequency working conditions, an improved active disturbance rejection controller is proposed. In order to reduce overshoot caused by a given large change between the actual output and expected value of the control object, a tracking differentiator (TD) is used to arrange the appropriate excesses. Through the first-order approximation of the time-delay link, the time-delay system is transformed into an output feedback problem with unknown function. Using the neural network state observer (NNSO), a sliding mode control law is used to achieve the accurate and fast tracking of the output signal. Finally, the numerical simulation results verify the effectiveness and feasibility of the proposed method.																		1999-4893				DEC	2019	12	12							259	10.3390/a12120259													
J								A General Integrated Method for Design Analysis and Optimization of Missile Structure	ALGORITHMS										missile modeling; structure analysis; parameterization; CST; UG secondary development; surrogate model; optimization strategy	SHAPE	In the demonstration phase of a missile scheme, to obtain the optimum proposal, designers need to modify the parameters of the overall structure frequently and significantly, and perform the structural analysis repeatedly. In order to reduce the manual workload and improve the efficiency of research and development, a general integrated method of missile structure modeling, analysis and optimization was proposed. First, CST (Class and Shape transformation functions) parametric method was used to describe the general structure of the missile. The corresponding software geometric modeling and FEM (Finite Element Method) analyzing of the missile were developed in C/C++ language on the basis of the CST parametric method and UG (Unigraphics) secondary development technology. Subsequently, a novel surrogate model-based optimation strategy was proposed to obtain a relatively light mass missile structure under existing shape size. Eventually, different missile models were used to verify the validity of the method. After executing the structure modeling, analysis and optimization modules, satisfactory results can be obtained that demonstrated the stability and adaptability of the proposed method. The method presented saves plenty of time comparing to the traditional manual modeling and analysis method, which provides a valuable technique to improve the efficiency of research and development.																		1999-4893				DEC	2019	12	12							257	10.3390/a12120257													
J								Target Image Mask Correction Based on Skeleton Divergence	ALGORITHMS										skeleton divergence; edge merging; TOPSIS; simultaneous optimization binary; mask correction	REGISTRATION	Traditional approaches to modeling and processing discrete pixels are mainly based on image features or model optimization. These methods often result in excessive shrinkage or expansion of the restored pixel region, inhibiting accurate recovery of the target pixel region shape. This paper proposes a simultaneous source and mask-images optimization model based on skeleton divergence that overcomes these problems. In the proposed model, first, the edge of the entire discrete pixel region is extracted through bilateral filtering. Then, edge information and Delaunay triangulation are used to optimize the entire discrete pixel region. The skeleton is optimized with the skeleton as the local optimization center and the source and mask images are simultaneously optimized through edge guidance. The technique for order of preference by similarity to ideal solution (TOPSIS) and point-cloud regularization verification are subsequently employed to provide the optimal merging strategy and reduce cumulative error. In the regularization verification stage, the model is iteratively simplified via incremental and hierarchical clustering, so that point-cloud sampling is concentrated in the high-curvature region. The results of experiments conducted using the moving-target region in the RGB-depth (RGB-D) data (Technical University of Munich, Germany) indicate that the proposed algorithm is more accurate and suitable for image processing than existing high-performance algorithms.																		1999-4893				DEC	2019	12	12							251	10.3390/a12120251													
J								Enhanced Knowledge Graph Embedding by Jointly Learning Soft Rules and Facts	ALGORITHMS										knowledge graph; knowledge graph embedding; logical rule; link prediction		Combining first order logic rules with a Knowledge Graph (KG) embedding model has recently gained increasing attention, as rules introduce rich background information. Among such studies, models equipped with soft rules, which are extracted with certain confidences, achieve state-of-the-art performance. However, the existing methods either cannot support the transitivity and composition rules or take soft rules as regularization terms to constrain derived facts, which is incapable of encoding the logical background knowledge about facts contained in soft rules. In addition, previous works performed one time logical inference over rules to generate valid groundings for modeling rules, ignoring forward chaining inference, which can further generate more valid groundings to better model rules. To these ends, this paper proposes Soft Logical rules enhanced Embedding (SoLE), a novel KG embedding model equipped with a joint training algorithm over soft rules and KG facts to inject the logical background knowledge of rules into embeddings, as well as forward chaining inference over rules. Evaluations on Freebase and DBpedia show that SoLE not only achieves improvements of 11.6%/5.9% in Mean Reciprocal Rank (MRR) and 18.4%/15.9% in HITS@1 compared to the model on which SoLE is based, but also significantly and consistently outperforms the state-of-the-art baselines in the link prediction task.																		1999-4893				DEC	2019	12	12							265	10.3390/a12120265													
J								Walking Gait Phase Detection Based on Acceleration Signals Using LSTM-DNN Algorithm	ALGORITHMS										gait phase detection; long short-term memory network; deep neural network; acceleration signal	PRINCIPAL COMPONENT ANALYSIS; SENSOR; PARAMETERS; NETWORKS; PATTERNS; CONTACT; MODELS	Gait phase detection is a new biometric method which is of great significance in gait correction, disease diagnosis, and exoskeleton assisted robots. Especially for the development of bone assisted robots, gait phase recognition is an indispensable key technology. In this study, the main characteristics of the gait phases were determined to identify each gait phase. A long short-term memory-deep neural network (LSTM-DNN) algorithm is proposed for gate detection. Compared with the traditional threshold algorithm and the LSTM, the proposed algorithm has higher detection accuracy for different walking speeds and different test subjects. During the identification process, the acceleration signals obtained from the acceleration sensors were normalized to ensure that the different features had the same scale. Principal components analysis (PCA) was used to reduce the data dimensionality and the processed data were used to create the input feature vector of the LSTM-DNN algorithm. Finally, the data set was classified using the Softmax classifier in the full connection layer. Different algorithms were applied to the gait phase detection of multiple male and female subjects. The experimental results showed that the gait-phase recognition accuracy and F-score of the LSTM-DNN algorithm are over 91.8% and 92%, respectively, which is better than the other three algorithms and also verifies the effectiveness of the LSTM-DNN algorithm in practice.																		1999-4893				DEC	2019	12	12							253	10.3390/a12120253													
J								Pruning High-Similarity Clusters to Optimize Data Diversity when Building Ensemble Classifiers	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Ensemble classification; clustering; diversity; dynamic selection; Jaccard Index; pruning	CLASSIFICATION; REGRESSION; SELECTION; FRAMEWORK	Diversity is a key component for building a successful ensemble classifier. One approach to diversifying the base classifiers in an ensemble classifier is to diversify the data they are trained on. While sampling approaches such as bagging have been used for this task in the past, we argue that since they maintain the global distribution, they do not create diversity. Instead, we make a principled argument for the use of k-means clustering to create diversity. Expanding on previous work, we observe that when creating multiple clusterings with multiple k values, there is a risk of different clusterings discovering the same clusters, which would in turn train the same base classifiers. This would bias the ensemble voting process. We propose a new approach that uses the Jaccard Index to detect and remove similar clusters before training the base classifiers, not only saving computation time, but also reducing classification error by removing repeated votes. We empirically demonstrate the effectiveness of the proposed approach compared to the state of the art on 19 UCI benchmark datasets.																	1469-0268	1757-5885				DEC	2019	18	4							1950027	10.1142/S1469026819500275													
J								Touch-Based Active Cloud Authentication Using Traditional Machine Learning and LSTM on a Distributed Tensorflow Framework	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Active authentication; behavioral biometrics; touch pattern; distributed tensorflow; mobile cloud computing		In this modern world, mobile devices have been paired with the cloud environment to scale the voluminous amount of generated data. The implementation comes at the cost of privacy as proprietary data can be stolen in transit to the cloud, or victims' phones can be seized along with synced data from cloud. The attacker can gain access to the phone through shoulder surfing, or even spoofing attacks. Our approach is to mitigate this issue by proposing an active cloud authentication framework using touch biometric pattern. To the best of our knowledge, active cloud authentication using touch dynamics for mobile cloud computing has not been explored in the literature. This research creates a proof of concept that will lead into a simulated cloud framework for active authentication. Given the amount of data captured by the mobile device from user activity, it can be a computationally intensive process for the mobile device to handle with such limited resources. To solve this, we simulated a post-transmission process of data to the cloud so that we could implement the authentication process within the cloud. We evaluated the touch data using traditional machine learning algorithms, such as Random Forest (RF), Support Vector Machine (SVM), and also using a deep learning classifier, the Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) algorithms. The novelty of this work is twofold. First, we develop a distributed tensorflow framework for cloud authentication using touch biometric pattern. This framework helps alleviate the drawback of the computationally intensive recognition of the substantial amount of raw data from the user. Second, we apply the RF, SVM, and a deep learning classifier, the LSTM-RNN, on the touch data to evaluate the performance of the proposed authentication scheme. The proposed approach shows a promising performance with an accuracy of 99.0361% using RF on the distributed tensorflow framework.																	1469-0268	1757-5885				DEC	2019	18	4							1950022	10.1142/S1469026819500226													
J								WOADF: Whale Optimization Integrated Adaptive Dragonfly Algorithm Enabled with the TDD Properties for Model Transformation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Model transformations; CLD; adaptive dragonfly; WOA; relational schema	UML	Model Transformation (MT) has led the researchers to concentrate more in the field of software engineering. MT focuses mainly on transforming the input model to the target model to make it easily understandable. For the transformation, using optimal rules among a set of rules makes the design simpler. This paper proposes an algorithm, namely Whale Optimization integrated Adaptive Dragonfly (WOADF) algorithm, which integrates Adaptive Dragonfly (ADF) algorithm and Whale Optimization Algorithm (WOA), for transforming class diagrams (CLDs) to Relational Schema (RS). Further, the UML CLD is transformed into the RS model based on specific rules incorporated by the proposed WOADF algorithm. The fitness function of the proposed model is evaluated to select the optimal rule, by including the test cases to evaluate the optimal blocks. Then, the optimal blocks obtained from the proposed WOADF algorithm are used for achieving the transformation from CLD to the RS model. The effectiveness of the proposed WOADF algorithm is checked with Automatic Correctness (AC) and fitness values and is evaluated to be the best when compared to other existing techniques with maximum AC value measured to be 0.812 and fitness value to be 0.897, respectively.																	1469-0268	1757-5885				DEC	2019	18	4							1950026	10.1142/S1469026819500263													
J								An Empirical Evaluation of User Movement Data on Smartphones	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Behavioral biometrics; user movement; accelerometer data; deep learning; neural networks	RECOGNITION	Movement data can be collected and used to add new security features and functionality to users' mobile devices. Measuring a user's movement using mobile devices allows for the use of behavioral biometrics. This assessment could introduce a shift in our current methods for securing mobile devices: instead of physical attributes like fingerprints or our face, the use of behavioral attributes like the way we walk or perform some personal activity. In this paper, an empirical evaluation of different classification techniques is conducted on user movement data. The datasets used in this empirical evaluation contain accelerometer data that were collected during various experiments from several mobile devices, including smartphones, smart watches, and other accelerometer sensors. We aggregated the user movement data and provided them as input into five traditional machine learning algorithms. The classification performances of the data were compared with a deep learning technique, the Long Short-Term Memory-Recurrent Neural Network (LSTM-RNN). The LSTM-RNN achieved its highest accuracy at 89% compared to 97% from a traditional machine learning algorithm, specifically the k-Nearest Neighbor (k-NN) algorithm on wrist-worn accelerometer data, thus showing the LSTM to be a less viable option.																	1469-0268	1757-5885				DEC	2019	18	4							1950025	10.1142/S1469026819500251													
J								Efficient Computation Offloading in Mobile Cloud Computing with Nature-Inspired Algorithms	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Mobile cloud computing; application offloading; workflow; nature inspired algorithms; swarm intelligence; evolutionary algorithms	PSO	The ubiquitous presence of smart phones and other hand-held computing devices has resulted in a growing feasibility to utilize them as computing resources. However, these mobile devices are constrained in battery and may not possess adequate capability for computationally intensive tasks. Cloud computing allows mobile devices to address their inherent challenges by making it possible to offload computation, completely or partially, to powerful cloud servers. This enables mobile devices to act as compute resources; though, it also results in cost of using cloud servers as well as communication cost involved in offloading. The paper models the computation offloading problem as an optimization problem and makes use of nature-inspired algorithms for deciding whether a task should be executed locally on a mobile device or offloaded to the cloud. The study was performed over four algorithms, namely Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization (PSO) and Shuffled Frog Leaping Algorithm (SFLA). Experimental analysis revealed that these algorithms outperform exhaustive search technique by providing a near optimal solution in a reasonable time even for large workflows. Results also establish that GA outperforms DE, PSO and SFLA by around 45%, 65% and 42%, respectively by reducing an application's overall execution cost.																	1469-0268	1757-5885				DEC	2019	18	4							1950023	10.1142/S1469026819500238													
J								Regressive Whale Optimization for Workflow Scheduling in Cloud Computing	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Multi-objective task scheduling; cloud computing; virtual machines; regressive whale optimization; regressive model	ENHANCED GENETIC ALGORITHM; SCIENTIFIC WORKFLOWS; COST; WORKLOAD; TASKS	Cloud computing is the advancing technology that aims at providing services to the customers with the available resources in the cloud environment. When the multiple users request service from the cloud server, there is a need of the proper scheduling of the resources to attain good customer satisfaction. Therefore, this paper proposes the Regressive Whale Optimization (RWO) algorithm for workflow scheduling in the cloud computing environment. RWO is the meta-heuristic algorithm, which schedules the task depending on a fitness function. Here, the fitness function is defined based on three major constraints, such as resource utilization, energy, and the Quality of Service (QoS). Therefore, the proposed task scheduling requires minimum time and cost for executing the task in the virtual machines. The performance of the proposed method is analyzed using the four experimental setups, and the results of the analysis prove that the proposed multi-objective task scheduling method performs well than the existing methods. The evaluation metrics considered for analyzing the performance of the proposed workflow scheduling method are resource utilization, energy, cost, and time. Resource utilization is the process of making the most of the resources available for performing tasks. Energy is the quantitative property of the resource to perform tasks. The proposed method attains the maximum resource utilization at a rate of 0.0334, minimal rate of energy, scheduling cost, and time as 0.2291, 0.0181, and 0.0007, respectively.																	1469-0268	1757-5885				DEC	2019	18	4							1950024	10.1142/S146902681950024X													
J								Robust Local Descriptor for Color Object Recognition	TRAITEMENT DU SIGNAL										color object recognition; hue; oriented descriptor; SVM; visual information		Image category recognition is important to access visual information on the level of objects and scene types. In this paper, we propose a new approach for color object recognition using the powerful information provided by the color. This approach is based on the combination of Gray-Edge color constancy, hue components in HSV (hue, saturation, value) color space and cell and bin ideas used in the HOG (Histograms of Gradients) descriptors. The proposed oriented descriptor benefits of the invariance of hues against light intensity change, light intensity shift and light intensity change and shift, and solve its missing of invariance against light color change by using Gray-Edge color constancy. Moreover, the use of cells and bins in this proposed descriptor building boost its invariance the geometric and photo-metric transformation and increases the recognition rate. SVM classifiers (Support Vector Machine) which is a strong classification method known for its flexibility and its power of generalization are used for the training and recognition steps. The proposed method is evaluated on two publicly available datasets including Columbia Object Image Library and The Amsterdam Library of Object Images and obtained a recognition rate of 95.64% and 96.48% - clearly showing the exceptional performance compared to existing methods.																	0765-0019	1958-5608				DEC	2019	36	6					471	482		10.18280/ts.360601													
J								Novel Method for Brain Tumor Classification Based on Use of Image Entropy and Seven Hu's Invariant Moments	TRAITEMENT DU SIGNAL										artificial neural networks; medical images processing; images classification; brain tumor	WAVELET ENTROPY; SEGMENTATION; MRI; DIAGNOSIS; SYSTEM; LEVEL	In this paper, a novel system is proposed for automating the process of brain tumor classification in magnetic resonance (MR) images. The proposed system has been validated on a database composed of 90 brain MR images belonging to different persons with several types of tumors. The images were arranged into 6 classes of brain tumors with 15 samples for each class. Each MR image of the brain is represented by a feature vector composed of several parameters extracted by two methods: the image entropy and the seven Hu's invariant moments. These two methods are applied on selected zones obtained by sliding a window along the MR image of the brain. The size of the used sliding window is 16x16 pixels for the first method (image entropy) and 64x64 pixels for the second method (seven Hu's invariant moments). To implement the classification, a multilayer perceptron trained with the gradient backpropagation algorithm has been used. The obtained results are very encouraging; the resulting system properly classifies 97.77% of the images of the used database.																	0765-0019	1958-5608				DEC	2019	36	6					483	491		10.18280/ts.360602													
J								Evaluation of the EEG Signals and Eye Tracker Data for Working Different N-back Modes	TRAITEMENT DU SIGNAL										electroencephalography; eye tracking; wavelet transforms; N-back test	THETA-OSCILLATIONS; ALPHA-BAND; SYNCHRONIZATION; PERFORMANCE; TASK	In this study, it is aimed to determine the effects of different modes N-back test which is one of the measurement tools frequently used in measurement of working memory, on the Electroencephalography (EEG) and Eye-tracker data. Eight healthy volunteers participated in this study. The volunteers performed tasks inducing stress and mental fatigue for almost 4 minutes. Each experimental task consists of 72-seconds stress and fatigue inducing 1-position back, 2-position-color and 2-position-image test sessions and three evaluation sessions performed for task. During these sessions, the volunteers were assessed using EEG, Eye Tracker and Visual analogue scale (VAS). VAS was also used to evaluate perceived stress and mental fatigue before and after the N-back test. Power values of EEG signals from volunteers for different test modes were evaluated according to test scores in theta, alpha, beta, and low gamma bands. According to the obtained results, the power values in each subband of the EEG change according to the test mode and the test scores. Beta, alpha and theta frequency bands' power in the frontal cortex (AF7+AF8) increased with then back test score and difficulty level of the game. When the task gets harder, it shows that the heat map of eye tracking is spread over a wider area. The VAS scores for both mental fatigue and stress increased after the N-back test with low and high test scores. According to the graphical results obtained from EEG with subjective evaluation, stress and mental fatigue increased with N-back tests. These results help understanding of the physiological changes of stress and mental fatigue and contribute to improve new approaches to assess stress and mental fatigue.																	0765-0019	1958-5608				DEC	2019	36	6					493	500		10.18280/ts.360603													
J								Identification of Vortex Structures in Flow Field Images Based on Convolutional Neural Network and Dynamic Mode Decomposition	TRAITEMENT DU SIGNAL										image processing; vortex identification; convolutional neural network (CNN); dynamic mode decomposition (DMD)	CLASSIFICATION	The generation and evolution of turbulence are affected by the way vortex structures emerge and interact with each other. The quick identification of vortex in flow field will offer a simple and easy way to explore the mechanism of turbulent flow. This paper combines convolutional neural network (CNN) and dynamic mode decomposition (DMD) into a novel vortex structure identification method, based on the flow field images obtained by simulation of the flow around circular cylinder and experiments of synthetic jet actuators. DMD was adopted to modally decompose the instantaneous image sequence of the flow field over a period and the first primary mode was added to the limit loss function of CNN vortex identification. Then a novel CNN topology was designed and trained by flow field image sequence. The proposed neural network managed to recognize 96% of the vortex structures correctly in the flow field images of the flow around circular cylinder and the synthetic jet flow. The research results provide a new way to identify vortex structures.																	0765-0019	1958-5608				DEC	2019	36	6					501	506		10.18280/ts.360604													
J								Gender Classification in Human Face Images for Smart Phone Applications Based on Local Texture Information and Evaluated Kullback-Leibler Divergence	TRAITEMENT DU SIGNAL										gender classification; human recognition; Improved local binary patterns; facial images; Kullback-Leibler divergence ratio; smart phone applications	BINARY PATTERNS; IDENTIFICATION; SCALE	One of the main steps in human identification is gender classification which can increase the identification accuracy. In many smart phone applications, human identification plays an important role in different reasons such as login permission, sign up certificates, etc. So, accurate gender classification algorithms may increase the accuracy of smart phone applications and reduce its complexity. Also, one of the benefits of gender classification algorithms is for parents to monitor the social network contacts of their child in terms of gender. Different methods have been proposed to do it accurately so far. In all methods, classification accuracy is the main challenge for researchers. But, in smart phone applications, some challenges such as rotation, gray scale variations may reduce the accuracy. In this respect, a rotation invariant approach is proposed in this paper to classify genders in human face images based on improved version of local binary patters (ILBP). Local binary pattern (LBP) is a texture descriptor, which extract local contrast and spatial structure information. Some issues such as noise sensitivity, rotation sensitivity and low discriminative features can be considered as disadvantages of the basic LBP. ILBP solves the above disadvantages using a new theory for binary patterns categorization. The proposed approach includes two stages. First of all, a feature vector is extracted for human face images based on ILBP. Next, Kullback-Leibler divergence classifier is used to classify gender. In this paper Kullback Leibler classifier is evaluated based on log likelihood ratio as distance measure. In the result part, two databases, self-collected and ICPR are used as human face database. Results are compared by different well known methods in this literature that shows the high quality of the proposed approach in terms of accuracy rate. Other main advantages of our approach are rotation invariant, low noise sensitivity, size invariant and low computational complexity. The proposed approach decreases the computational complexity of smartphone applications because of reducing the number of database comparisons. It can also improve performance of the synchronous applications in the smartphones because of memory and CPU usage reduction.																	0765-0019	1958-5608				DEC	2019	36	6					507	514		10.18280/ts.360605													
J								An Innovative Beam Hardening Correction Method for Computed Tomography Systems	TRAITEMENT DU SIGNAL										computed tomography (CT); equivalent tissue length; trinomial fitting; water; bone	ARTIFACTS; IMAGES	Currently, beam hardening is mostly corrected based on image or projection. However, the physical meanings of the correction process are not unclear. To solve the problem, this paper puts forward a hardening correction method with clear physical meanings for computed tomography (CT) beams. First, the CT image was divided into equivalent tissues of water and bone according to the CT value of each pixel. Next, each equivalent tissue was projected to obtain the total length of that type of tissue on each projection ray. According to the acquired equivalent tissue length, a polychromatic projection model was constructed based on trinomial fitting, and applied to approximate the actual projection data. The coefficients were solved by the least squares (LS) method, and used to solve the nonlinear terms, i.e. the incorrect terms. After that, the incorrect terms were removed from the original projection, producing the corrected projection. Finally, the artefact-free image was reconstructed based on the corrected projection. Through experiments on actual CT scan data, the proposed model was proved to effectively remove the hardening artefacts of X-rays, and improve the quantification accuracy of the CT images. Thus, the proposed trinomial fitting hardening correction method is an effective strategy with clear physical meanings. The research results have great application potential in clinical CT and other CT systems.																	0765-0019	1958-5608				DEC	2019	36	6					515	520		10.18280/ts.360606													
J								Spinal Cord Based Kidney Segmentation Using Connected Component Labeling and K-Means Clustering Algorithm	TRAITEMENT DU SIGNAL										biomedical imaging; clustering algorithms; image processing; image segmentation	ULTRASOUND	Abdominal computed tomography (CT) data are often used in the diagnosis and treatment of patients. Segmentation of viscera on abdominal imaging facilitates diagnosis and focus upon the areas of interest. Kidney segmentation by abdominal imaging is complicated by the proximity of various organs and the similarities between abdominal tissues. Here we propose two fully automated approaches to kidney segmentation and discuss their performance. A fully automated approach was preferred to accelerate the decision-making process of the physician to eliminate the disadvantage of manual and semi-automatic segmentations. Each of the proposed methods essentially consist of three stages. Since the spine was used as reference in the study, the images were first treated to define the coordinates of the spine. Second, kidney fields were obtained using the Connected Component Labeling (CCL) and the K-means clustering algorithms. Last, the kidneys were segmented by applying different filters according to the method. A manual segmentation was then performed by specialist physicians. The performance of the tested algorithms was made by comparison to the manual segmentation results, using the Dice Similarity Coefficient, the Figures of Merit and Jaccard Similarity Index. Based on our analyses, acceptable success rates were achieved by the proposed methodologies. These automated systems are expected to be helpful during clinical diagnosis, medical training and future studies on kidney cancer diagnosis.																	0765-0019	1958-5608				DEC	2019	36	6					521	527		10.18280/ts.360607													
J								Compressive Sensing Based Two-Dimensional DOA Estimation Using L-Shaped Array in a Hostile Environment	TRAITEMENT DU SIGNAL										compressive sensing; L-shaped array antenna; orthogonal matching pursuit algorithm; sparse sampling; two-dimensional DOA estimation	OF-ARRIVAL; MATRIX	In this paper, two-dimensional (2-D) direction of arrival (DOA) estimation problem with an L-shaped array is investigated. One of the major areas of concern of modern urban combat is to locate lives trapped in a building in the presence of enemy jamming conditions at very low signal-to-noise ratio. This study provides a suitable design of a tracking system that enables location of trapped survivors in hostile situation. A compressive sensing (CS) based model is proposed for an L-shaped array which offers more array aperture with reduced computational complexity. By exploiting the signal sparsity in the spatial domain, the problem of DOA estimation is transformed to the sparse reconstruction problem. To solve the reconstruction problem efficiently, the Orthogonal Matching Pursuit (OMP) algorithm is used in which single snapshot is sufficient to recover exact target locations. The results are compared with the standard Multiple Signal Classification (MUSIC) algorithm for L-shaped array in terms of recovery, root mean square error (RMSE), probability of resolution, computational complexity, failure rate and reconstruction time. Simulation shows that the proposed method considerably improves the DOA estimation performance at low signal-to-noise ratio (SNR).																	0765-0019	1958-5608				DEC	2019	36	6					529	538		10.18280/ts.360608													
J								A Wavelet-Based Self-adaptive Hierarchical Thresholding Algorithm and Its Application in Image Denoising	TRAITEMENT DU SIGNAL										wavelet analysis; image denoising; parametric construction of biorthogonal wavelet; self-adaptive hierarchal thresholding	TRANSFORM; CURVELET; FILTER	This paper attempts to construct a suitable wavelet for image denoising based on wavelet thresholding algorithm. First, the author discussed how image thresholding is affected by the wavelet orthogonality and bi-orthogonality, the features of vanishing moments and the odd or even symmetry of the decomposition end filter. The discussion shows that the most desirable wavelet for image denoising is the biorthogonal wavelet, in which the decomposition end filter has zero point even symmetry, the low-pass decomposition enjoys a wide support interval, and the high-pass decomposition filter has a short support and attenuates fast. On this basis, three zero point even symmetric biorthogonal wavelets with different vanishing moment features were developed through the parametric construction of fixed-length tightly-supported (FLTS) biorthogonal wavelet, and a self-adaptive hierarchical thresholding algorithm was designed. The simulation results show that the developed wavelets have excellent denoising ability and enhance the images with rich details. Coupled with the self-adaptive hierarchical thresholding algorithm, these wavelets can effectively improve the image quality.																	0765-0019	1958-5608				DEC	2019	36	6					539	547		10.18280/ts.360609													
J								A Comparative Study of Object Classification Methods Using 3D Zernike Moment on 3D Point Clouds	TRAITEMENT DU SIGNAL										3D; classification; machine learning; point cloud; pointnet; Zernike moment	EXTRACTION	The point clouds provide responsive geometric representation on many applications. The classification of objects through point clouds is one of the popular subjects of recent years. In this study, we introduce the potential of the 3D Zernike Moment approach for the object classification on the 3D point cloud. Zernike Moment (ZM) has utilized as a feature extractor of point clouds. This paper presents a comparative study of the state-of-the-art classification methods with respect to machine learning algorithms and PointNet which have been developed for classification by Stanford University. Object classification has been applied to a dataset with labeled 3D Zernike Moment features inferences obtained from the 3D point cloud. The performance of the developed method is verified by comparing the experimental results on the Washington RGB-D Object Dataset which consists of forty-five different household objects as point cloud data. Fine Gaussian SVM gives the best results in accuracy (96.0%) according to the results obtained with built-in cross-validation results. The results of the proposed classification of 3D Point Clouds on the 3D Zernike Moment features have significantly higher accuracy. The classification of 3D Zernike Moments on point cloud compared to directly point cloud classification is efficient and effective and lower complexity computation is obtained. It is emphasized that the 3D Zernike Moment features can be optimized for classification. In general, the comparative validation results have been reached a high accuracy in the proposed method. In the future, 3D Zernike Moment feature extractions are emphasized for the usability of classification operations on 3D data.																	0765-0019	1958-5608				DEC	2019	36	6					549	555		10.18280/ts.360610													
J								A Micro-expression Recognition Algorithm for Students in Classroom Learning Based on Convolutional Neural Network	TRAITEMENT DU SIGNAL										convolutional neural network (CNN); micro-expression recognition; deep learning; face detection; classroom learning	FACIAL EXPRESSION; POSITIVE EMOTIONS	In classroom teaching, the teachers should adjust the teaching strategy and improve the teaching effect based on the expression and learning state of each student. This paper mainly develops a micro-expression recognition algorithm for students in classroom learning, based on convolutional neural network (CNN) and automatic face detection. Specifically, the multitask deep convolution network (DNN) was adopted to detect the landmark points of human face, and a hybrid DNN was designed to extract the optical-flow features of micro-expression. The extracted features were improved through redundancy removal and dimensionality reduction. The rationality of our algorithm was proved through a comparative experiment on real-world databases and an application in classroom teaching. The research results provide a new direction for applying deep learning in face recognition.																	0765-0019	1958-5608				DEC	2019	36	6					557	563		10.18280/ts.360611													
J								Robust Optimized Discrete Wavelet Transform-Singular Value Decomposition Based Video Watermarking	TRAITEMENT DU SIGNAL										ABC; DWT; imperceptibility; robustness; SVD transform	BEE COLONY ALGORITHM; SCHEME	The effortless editing, interchanging and replication of multimedia data on the Internet is growing exponentially and has created copyright protection uncertainties for content providers. Thus, in order to discourage illegal duplication and to attain the required level of protection to digital data, digital watermarking is found to be a feasible solution. Thus, this paper proposes a video watermarking technique by exploring Discrete Wavelet Transform (DWT) and Singular Value Decomposition (SVD) transform in addition to Artificial Bee colony (ABC) Optimization algorithm. In this paper, DWT is applied on every luminance frame which is divided into 8x8 blocks of the video 'V' thus producing distinct frequency sub-bands. Out of them, LL band is selected for watermark insertion. Later SVD transform is implemented on the selected dwt blocks of LL bands of all frames. The starting indices of best blocks are obtained adaptively rather than manually through ABC algorithm. At the receiving part, retrieval of watermark contents is achieved by a similar evaluation scheme practiced during the embedding procedure. The proposed optimized DWT-SVD based video watermarking method has been evaluated in the presence of video processing attacks and simulation results proved that due to cascading of two powerful mathematical transforms DWT and SVD in addition to ABC algorithm the proposed video watermarking method endures all attacks and aptly extracts the concealed watermark without significant degradation in the video quality of the watermarked video. Thus when the Peak Signal to Noise Ratio (PSNR) and Normalized Correlation (NC) performance of the proposed algorithm is correlated with other related techniques it is found that the PSNR of the proposed method is above 53 dB for all set of videos and Robustness of the scheme is superior than the existing schemes for similar set of videos in terms of NC.																	0765-0019	1958-5608				DEC	2019	36	6					565	573		10.18280/ts.360612													
J								A Fast Recognition Algorithm of Online Social Network Images Based on Deep Learning	TRAITEMENT DU SIGNAL										online social network (OSN); image recognition; deep learning; image classification; support vector machine (SVM)	CLASSIFICATION	In recent years, a massive number of images have generated on the online social network (OSN). This calls for an efficient and rapid way to extract the information from the OSN images. This paper puts forward an OSN image classification method based on improved deep belief network (DBN) and support vector machine (SVM). In the proposed method, the image classification is enhanced by improving the self-adaptive learning rate based on incremental discrimination of reconstruction error and the weight update criteria with increasing momentum. The effectiveness of our method was confirmed through an image recognition experiment on OSN images obtained from Sina Weibo public platform, in comparison with four commonly used classification methods. The research results shed new light on feature extraction and classification of OSN images.																	0765-0019	1958-5608				DEC	2019	36	6					575	580		10.18280/ts.360613													
J								A Hierarchical Dimension Reduction Approach for Big Data with Application to Fault Diagnostics	BIG DATA RESEARCH										Big data; Fault diagnosis; Distance measure; Classification; Mahalanobis distance; Edgeworth expansion	CHALLENGES; GENE	About four zetta bytes of data, which falls into the category of big data, is generated by complex manufacturing systems annually. Big data can be utilized to improve the efficiency of an aging manufacturing system, provided, several challenges are handled. In this paper, a novel methodology is presented to detect faults in manufacturing systems while overcoming some of these challenges. Specifically, a generalized distance measure is proposed in conjunction with a novel hierarchical dimension reduction (HDR) approach. It is shown that the HDR can tackle challenges that are frequently observed during distance calculation in big data scenarios, such as norm concentration, redundant dimensions, and a non-invertible correlation matrices. Subsequently, a probabilistic methodology is developed for isolation and detection of faults. Here, Edgeworth expansion based expressions are derived to approximate the density function of the data. The performance of the dimension reduction methodology is demonstrated to be efficient with simulation results involving the use of big data sets. It is shown that HDR is able to explain almost 90% of the total information. Furthermore, the proposed dimension reduction methodology is seen to outperform standard dimension reduction approaches and is able to improve the performance of standard classification methodologies in high dimensional scenarios. (c) 2019 Elsevier Inc. All rights reserved.																	2214-5796					DEC	2019	18								100121	10.1016/j.bdr.2019.100121													
J								Towards Adaptive Ontology Visualization - Predicting User Success from Behavioral Data	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Eye tracking; machine learning; ontology visualization; user prediction; adaptive visualization	EYE-MOVEMENTS; SYSTEM; DESIGN	Ontology visualization plays an important role in human data interaction by offering clarity and insight for complex structured datasets. Recent usability studies of ontology visualization techniques have added to our understanding of desired features when assisting users in the interactive process. However, user behavioral data such as eye gaze and event logs have largely been used as indirect evidence to explain why a user may have carried out certain tasks in a controlled environment, as opposed to direct input that informs the underlying visualization system. Although findings from usability studies have contributed to the refinement of ontology visualizations as a whole, the visualization techniques themselves remain a one-size-fits-all approach, where all users are presented with the same visualizations and interactive features. By contrast, this paper investigates the feasibility of using behavioral data, such as user gaze and event logs, as real-time indicators of how appropriate or effective a given visualization may be for a specific user at a moment in time, which in turn may be used to inform the adaptation of the visualization to the user on the fly. To this end, we apply established predictive modeling techniques in Machine Learning to predict user success using gaze data and event logs. We present a detailed analysis from a controlled experiment and demonstrate such predictions are not only feasible, but can also be significantly better than a baseline classifier during visualization usage. These predictions can then be used to drive the adaptations of visual systems in providing ad hoc visualizations on a per user basis, which in turn may increase individual user success and performance. Furthermore, we demonstrate the prediction performance using several different feature sets, and report on the results generated from several notable classifiers, where a decision tree-based learning model using a boosting algorithm produced the best overall results.																	1793-351X	1793-7108				DEC	2019	13	4			SI		431	452		10.1142/S1793351X1940018X													
J								Evaluation of Feature Learning Methods for Voice Disorder Detection	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Voice disorder detection; deep learning; feature learning	FREQUENCY CEPSTRAL COEFFICIENTS; AUTOMATIC DETECTION; HEALTH-CARE	Voice disorder is a frequently encountered health issue. Many people, however, either cannot a (R) ord to visit a professional doctor or neglect to take good care of their voice. In order to give a patient a preliminary diagnosis without using professional medical devices, previous research has shown that the detection of voice disorders can be carried out by utilizing machine learning and acoustic features extracted from voice recordings. Considering the increasing popularity of deep learning, feature learning and transfer learning, this study explores the possibilities of using these methods to assign voice recordings into one of two classes-Normal and Pathological. While the results show the general viability of deep learning and feature learning for the automatic recognition of voice disorders, they also lead to discussions on how to choose a pre-trained model when using transfer learning for this task. Furthermore, the results demonstrate the shortcomings of the existing datasets for voice disorder detection such as insu +/- cient dataset size and lack of generality.																	1793-351X	1793-7108				DEC	2019	13	4			SI		453	470		10.1142/S1793351X19400191													
J								Decrease Product Rating Uncertainty Through Focused Reviews Solicitation	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Review solicitation; customer reviews; review analysis; sentiment analysis	ONLINE; TIME	Customer reviews are an essential resource to reduce an online product's uncertainty, which has been shown to be a critical factor for its purchase decision. Existing e-commerce platforms typically ask users to write free-form text reviews, which are sometimes augmented by a small set of predefined questions, e.g. "rate the product description's accuracy from 1 to 5." In this paper, we argue that this "passive" style of review solicitation is suboptimal in achieving low-uncertainty "review profiles" for products. Its key drawback is that some product aspects receive a very large number of reviews while other aspects do not have enough reviews to draw confident conclusions. Therefore, we hypothesize that we can achieve lower-uncertainty review profiles by carefully selecting which aspects users are asked to rate. To test this hypothesis, we propose various techniques to dynamically select which aspects to ask users to rate given the current review profile of a product. We use Bayesian inference principles to define reasonable review profile uncertainty measures; specifically, via an aspect's rating variance. We compare our proposed aspect selection techniques to several baselines on several review profile uncertainty measures. Experimental results on two real-world datasets show that our methods lead to better review profile uncertainty compared to aspect selection baselines and traditional passive review solicitations. Moreover, we present and evaluate a hybrid solicitation method that combines the advantages of both active and passive review solicitations.																	1793-351X	1793-7108				DEC	2019	13	4			SI		471	495		10.1142/S1793351X19400208													
J								Deep Learning-Based Stair Segmentation and Behavioral Cloning for Autonomous Stair Climbing	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Behavioral cloning; semantic segmentation; stair climbing robot		Mobile robots are widely used in the surveillance industry, for military and industrial applications. In order to carry out surveillance tasks like urban search and rescue operation, the ability to traverse stairs is of immense significance. This paper presents a deep learning-based approach for semantic segmentation of stairs, behavioral cloning for stair alignment, and a novel mechanical design for an autonomous stair climbing robot. The main objective is to solve the problem of locomotion over staircases with the proposed implementation. Alignment of a robot with stairs in an image is a traditional problem, and the most recent approaches are centered around hand-crafted texture-based Gabor filters and stair detection techniques. However, we could arrive at a more scalable and robust pipeline for alignment schemes. The proposed deep learning technique eliminates the need for manual tuning of parameters of the edge detector, the Hough accumulator and PID constants. The empirical results and architecture of stair alignment pipeline are demonstrated in this paper.																	1793-351X	1793-7108				DEC	2019	13	4			SI		497	512		10.1142/S1793351X1940021X													
J								Learning-Based Adaptive Management of QoS and Energy for Mobile Robotic Missions	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Robotic run-time adaptation; reinforcement learning; non-functional requirements; quality of service; energy efficiency; autonomous mobile robots		Mobile robotic systems are normally confronted with the shortage of on-board resources such as computing capabilities and energy, as well as significantly influenced by the dynamics of surrounding environmental conditions. This context requires adaptive decisions at run-time that react to the dynamic and uncertain operational circumstances for guaranteeing the performance requirements while respecting the other constraints. In this paper, we propose a reinforcement learning (RL)-based approach for Quality of Service QoS and energy-aware autonomous robotic mission manager. The mobile robotic mission manager leverages the idea of (RL) by monitoring actively the state of performance and energy consumption of the mission and then selecting the best mapping parameter configuration by evaluating an accumulative reward feedback balancing between QoS and energy. As a case study, we apply this methodology to an autonomous navigation mission. Our simulation results demonstrate the efficiency of the proposed management framework and provide a promising solution for the real mobile robotic systems.																	1793-351X	1793-7108				DEC	2019	13	4			SI		513	539		10.1142/S1793351X19400221													
J								Motion Planning and Control with Randomized Payloads on Real Robot Using Deep Reinforcement Learning	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Deep reinforcement learning; motion control; decision-making	MOBILE ROBOT; NAVIGATION	In this study, a unified motion planner with low level controller for continuous control of a differential drive mobile robot under variable payload values is presented. The deep reinforcement agent takes 11-dimensional state vector as input and calculates each wheel's torque value as a 2-dimensional output vector. These torque values are fed into the dynamic model of the robot, and lastly steering commands are gathered. In previous studies, intersection navigation solutions that uses deep-RL methods, have not been considered with variable payloads. This study is focused specifically on service robotic applications where payload is subject to change. In this study, deep-RL-based motion planning is performed by considering both kinematic and dynamic constraints. According to the simulations in a dynamic environment, the agent successfully navigates to target with 98.2% success rate in test time with unseen payload masses during training. Another agent is also trained without payload randomization for comparison. Results show that our agent outperforms the other agent, that is not aware of its own payload, with more than 40% gap. Our agent is also compared with the Time-to-Collision (TTC) algorithm. It is observed that our agent uses far less time than TTC to accomplish the mission while success rates of two methods are same. Lastly, the proposed method is applied on a real robot in order to show the real-time applicability of the approach.																	1793-351X	1793-7108				DEC	2019	13	4			SI		541	563		10.1142/S1793351X19400233													
J								End-to-End Trained Sparse Coding Network with Spatial Pyramid Pooling for Image Classification	NEURAL PROCESSING LETTERS										Sparse coding network; Spatial pyramid pooling; Image classification; Deep convolutional network	THRESHOLDING ALGORITHM	Spatial pyramid matching using sparse coding (ScSPM) has become an efficient method and a benchmark in image classification. However, since it is unsupervised, the trained dictionary may be suboptimal. To further improve classification accuracy, in this paper we propose a sparse coding network with spatial pyramid pooling based on the end-to-end deep learning approach. In our new system, the minimization problem in sparse coding can be modeled as a feed-forward neural network and image features can be extracted by the deep convolutional network. By minimizing the final classifier loss using the end-to-end deep learning method, the sparse coding network can be trained in a supervised way. Our proposed model is tested on three image databases and in terms of classification accuracy, it significantly outperforms ScSPM. Compared with other image classification approaches based on deep learning, it can also achieve a noticeable improvement.																	1370-4621	1573-773X				DEC	2019	50	3					2021	2036		10.1007/s11063-018-9967-5													
J								Breast Tumor Classification Using Fast Convergence Recurrent Wavelet Elman Neural Networks	NEURAL PROCESSING LETTERS										Breast tumor classification; Fine needle aspirate; Recurrent wavelet Elman neural network; Optimal learning rate	DIAGNOSIS; SYSTEM; RULES	This paper develops an intelligent classification system for breast tumors that uses fine needle aspirate image data. A recurrent wavelet Elman neural network is used to classify the breast tumor as either benign or malignant. The structure of the RWENN uses different wavelet functions for hidden layers so that the generalization and search space are significantly greater than those of a conventional neural network. In this paper, there is also a stable convergence analysis of the RWENN classifier and the optimal learning rates are derived to guarantee the fastest convergence for the classification system. The performance of the developed classifier is compared with the Matlab neural network pattern recognition toolbox and other literature that uses a tenfold cross validation on the Wisconsin breast cancer dataset. The simulation results show that the proposed RWENN classifier has better classification results than other existing methods.																	1370-4621	1573-773X				DEC	2019	50	3					2037	2052		10.1007/s11063-018-9931-4													
J								Exponential Synchronization of Inertial Memristor-Based Neural Networks with Time Delay Using Average Impulsive Interval Approach	NEURAL PROCESSING LETTERS										Synchronization; Inertial memristor-based neural networks; Average impulsive interval approach; Impulsive controller	MATRIX MEASURE STRATEGIES; PERIODIC-SOLUTIONS; STABILITY ANALYSIS; VARYING DELAYS	This paper deals with the impulsive synchronization problem for a class of inertial memristor-based neural networks (IMNNs) with time delays by applying average impulsive interval approach. By adopting proper variable transformation, the original system can be converted into first-order differential equations. By utilizing Lyapunov theory, theory of differential inclusion, Halanay inequality and average impulsive interval approach, we attain some adequate conditions that make sure the exponential synchronization of IMNNs under the impulsive control technique. Moreover some delay-dependent conditions for delayed impulsive synchronization of the considered system is obtained. Finally, numerical simulations are offered to exhibit the capacity of our theoretical findings.																	1370-4621	1573-773X				DEC	2019	50	3					2053	2071		10.1007/s11063-019-09982-y													
J								Finite-Time and Fixed-Time Synchronization of Complex Networks with Discontinuous Nodes via Quantized Control	NEURAL PROCESSING LETTERS										Finite-time synchronization; Fixed-time synchronization; Complex networks; Quantized control	CHAOTIC NEURAL-NETWORKS; FEEDBACK STABILIZATION; GLOBAL CONVERGENCE; MIXED DELAYS; SYSTEMS; DYNAMICS; DESIGN	This paper investigates finite-time (FET) and fixed-time (FDT) synchronization of discontinuous complex networks (CNs) via quantized controllers. These control schemes can take full advantage of limited communication resources. By designing Lyapunov function and using different control schemes, several sufficient conditions are proposed such that the dynamical CNs are able to realize synchronization within a settling time. The settling time is related to the initial values of the considered systems using FET control, while it is regardless of the initial values when a special case of FET control named FDT control is utilized. Moreover, FET and FDT synchronization of discontinuous CNs are also considered via some existing controllers without logarithmic quantization, respectively. Numerical simulations are presented to demonstrate the theoretical results.																	1370-4621	1573-773X				DEC	2019	50	3					2073	2086		10.1007/s11063-019-09985-9													
J								Hessian Regularized Distance Metric Learning for People Re-Identification	NEURAL PROCESSING LETTERS										Metric learning; Person re-identification; Hessian energy; Manifold regularization	PERSON REIDENTIFICATION; KISS	Distance metric learning is a vital issue in people re-identification. Although numerous algorithms have been proposed, it is still challenging especially when the labeled information is few. Manifold regularization can take advantage of labeled and unlabeled information and achieve promising performance in a unified metric learning framework. In this paper, we propose Hessian regularized distance metric learning for people re-identification. Particularly, the second-order Hessian energy prefers functions whose values vary linearly with respect to geodesic distance. Hence Hessian regularization allows us to preserve the geometry of the intrinsic data probability distribution better and then promotes the performance when there is few labeled information. We conduct extensive experiments on the popular VIPeR dataset, CUHK Campus dataset and CUHK03 dataset. The encouraging results suggest that manifold regularization can boost distance metric learning and the proposed Hessian regularized distance metric learning algorithm outperforms the traditional manifold regularized distance metric learning algorithms including graph Laplacian regularization algorithm.																	1370-4621	1573-773X				DEC	2019	50	3					2087	2100		10.1007/s11063-019-10000-4													
J								An Ensemble Classification Algorithm Based on Information Entropy for Data Streams	NEURAL PROCESSING LETTERS										Data streams; Data mining; Concept drift; Information entropy; Ensemble classification	EXTREME LEARNING-MACHINE; CONCEPT-DRIFTING DATA	Data stream mining has attracted much attention from scholars. In recent researches, ensemble classification has been wide aplied in concept drift detection; however, most of them regard classification accuracy as a criterion for judging whether concept drift happens or not. Information entropy is an important and effective method for measuring uncertainty. Based on the information entropy theory, a new algorithm using information entropy to evaluate a classification result is developed. It utilizes the methods of ensemble learning and the weight of each classifier is decided by the entropy of the result produced by an ensemble classifiers system. When the concept in data stream changes, the classifiers whose weight are below a predefined threshold will be abandoned to adapt to a new concept. In the experimental analysis, the proposed algorithm and six comparision algorithms are executed on six experimental data sets. The results show that the proposed method can not only handle concept drift effectively, but also have a better performance than the comparision algorithms.																	1370-4621	1573-773X				DEC	2019	50	3					2101	2117		10.1007/s11063-019-09995-7													
J								The Intermittent Control Synchronization of Complex-Valued Memristive Recurrent Neural Networks with Time-Delays	NEURAL PROCESSING LETTERS										Complex-valued recurrent neural networks; Memeristor; Synchronization; Intermittent control; Differential inclusion; Time-delays	EXPONENTIAL STABILITY; ANTI-SYNCHRONIZATION; MIXED DELAYS; MANIPULATORS; FEEDBACK; SYSTEMS	In this paper, the intermittent control synchronization of complex-valued memristive recurrent neural networks with time-delays is investigated. As a generalization on the real-valued memristive recurrent neural networks, complex-valued memristive recurrent neural networks own more complicated properties. In complex-valued domain, bounded and analytic complex-valued activation functions do not exist. Some assumptions about activation functions in real-valued domain cannot be applied directly to complex-valued fields. By appropriate transformation, complex-valued memristive recurrent neural networks can be divided into real parts and imaginary parts, which can avoid discussing the bounded and analytic. In the framework of differential inclusion theory and Lyapunov method, sufficient criteria of intermittent control synchronization are established. Finally, a simulation is given to verify the validity and feasibility of the sufficient conditions.																	1370-4621	1573-773X				DEC	2019	50	3					2119	2139		10.1007/s11063-019-09988-6													
J								Relationships Self-Learning Based Gender-Aware Age Estimation	NEURAL PROCESSING LETTERS										Age estimation; Cumulative attribute; Correlation learning strategy; Gender-aware age estimation	RECOGNITION	In biometrics research, face-appearance based age estimation (AE) becomes an important topic and has attracted a great deal of attention due to its potential applications. To achieve the goal of AE, a variety of methods have been proposed in the literature, among which the cumulative attribute (CA) coding based methods have achieved promising performance by preserving both ordinality and neighbor-similarity of ages. However, the sub-regressors responsible for regressing each of the CA coding elements are learned separately, while all of them are trained on the same dataset, leading to that potential correlation relationships of inter/intra-CA coding are not exploited. To this end, we herein propose a novel correlation learning method to model and utilize such inter/intra-CA relationships for AE, through self-learning from the training data. In addition, we extend the proposed method to perform gender-aware AE by further exploiting the correlations between and within gender groups. Furthermore, we introduce an alternating optimization algorithm for the proposed methods. Extensive experiments are conducted to demonstrate that the proposed methods can significantly improve the accuracy of AE, and more importantly that they can model well both inter/intra CA coding and gender relationships, regardless whether they are related (positive or negative) or not.																	1370-4621	1573-773X				DEC	2019	50	3					2141	2160		10.1007/s11063-019-09993-9													
J								Two-Stage Method for Diagonal Recurrent Neural Network Identification of a High-Power Continuous Microwave Heating System	NEURAL PROCESSING LETTERS										Diagonal recurrent neural network; High-power continuous microwave heating system; Fast recursive algorithm; Lyapunov stability criterion; Computational complexity	NONLINEAR DYNAMICAL-SYSTEMS; LEAST-SQUARES METHODS; GENETIC ALGORITHM; OPTIMIZATION; MANIPULATORS; PREDICTION; MODELS	This paper proposes a diagonal recurrent neural network (DRNN) based identification scheme to handle the complexity and nonlinearity of high-power continuous microwave heating system (HPCMHS). The new DRNN design involves a two-stage training process that couples an efficient forward model selection technique with gradient-based optimization. In the first stage, an impact recurrent network structure is obtained by a fast recursive algorithm in a stepwise forward procedure. To ensure stability, update rules are further developed using Lyapunov stability criterion to tune parameters of reduced size model at the second stage. The proposed approach is tested with an experimental regression problem and a practical HPCMHS identification, and the results are compared with four typical network models. The results show that the new design demonstrates improved accuracy and model compactness with reduced computational complexity over the existing methods.																	1370-4621	1573-773X				DEC	2019	50	3					2161	2182		10.1007/s11063-019-09992-w													
J								Global Exponential Synchronization of Delayed Complex-Valued Recurrent Neural Networks with Discontinuous Activations	NEURAL PROCESSING LETTERS										Complex-valued neural network; Exponential synchronization; State feedback control	FINITE-TIME SYNCHRONIZATION; CHAOTIC SYSTEMS; MIXED DELAYS; STABILITY; CONVERGENCE	In this paper, we are concerned with the exponential synchronization for a class of two delayed complex-valued recurrent neural networks (CVRNNs) with discontinuous neuron activations. By separating CVRNNs into real and imaginary parts, forming an equivalent real-valued subsystems, under the framework of differential inclusions, novel state feedback controllers are designed and novel criteria are established to ensure the exponential stability of error system, and thus the drive system exponentially synchronize with the response system. The obtained results are essentially new and complement previously known ones. The practicability of theoretical results is also supported via a numerical example.																	1370-4621	1573-773X				DEC	2019	50	3					2183	2200		10.1007/s11063-018-09970-8													
J								Impulsive Stabilization and Synchronization of Fractional-Order Complex-Valued Neural Networks	NEURAL PROCESSING LETTERS										Complex-valued neural network; Fractional-order; Impulsive stabilization; Synchronization; Fractional-order differential inequality	STABILITY ANALYSIS; EXPONENTIAL SYNCHRONIZATION; ANTI-SYNCHRONIZATION; UNIFORM STABILITY; SYSTEMS; STRATEGIES; DELAY	This paper focuses on the impulsive stabilization of fractional-order complex-valued neural networks. Based on impulsive control and some fractional-order differential inequalities, some valid criteria are achieved to ensure the global asymptotic stabilization of the addressed networks. The maximal impulsive strength and the maximal impulsive interval are also given. Under certain conditions, some sufficient conditions are derived to ensure the global alpha-exponential stability of the equilibrium point. Compared to the traditional linear feedback control, the impulsive control strategy only needs small control gains and shorter time to achieve global stabilization. When employing the impulsive control to the error system, a parallel criterion regarding to the complete synchronization of the drive-response systems is also generated. The effectiveness and advantages of the proposed methods are confirmed through simulation results.																	1370-4621	1573-773X				DEC	2019	50	3					2201	2218		10.1007/s11063-019-10002-2													
J								Bifurcation Analysis for Simplified Five-Neuron Bidirectional Associative Memory Neural Networks with Four Delays	NEURAL PROCESSING LETTERS										BAM neural networks; Stability; Hopf bifurcation; Time delay	FREQUENCY-DOMAIN APPROACH; GLOBAL HOPF-BIFURCATION; 2-NEURON SYSTEM; DISTRIBUTED DELAYS; LINEAR-STABILITY; MODEL; DISCRETE	The paper deals with the stability and bifurcation analysis of a class of simplified five-neuron bidirectional associative memory neural networks with four delays. By discussing the characteristic transcendental equation and applying Hopf bifurcation theory, some sufficient conditions which guarantee the local stability and the existence of Hopf bifurcation of the neural networks are established. With the aid of the normal form theory and center manifold theory, we obtain some specific formulae to determine the stability and the direction of the Hopf bifurcation. Computer simulations are implemented to explain the key mathematical predictions. The paper ends with a brief conclusion.																	1370-4621	1573-773X				DEC	2019	50	3					2219	2245		10.1007/s11063-019-10006-y													
J								Online Learning for Time Series Prediction of AR Model with Missing Data	NEURAL PROCESSING LETTERS										Sparse; Online learning; Time series prediction; L-1 regularization; Loss function; Missing data		Recently online learning algorithm is applied to time series prediction with missing data without the strict assumption on the noise terms. The existing algorithm only uses the observed data to predict time series, which does not impute the missing data and costs most time to count the number of common missing observation in any two sub-sequences of time series with constant length. In this paper, we consider online learning algorithm for time series prediction with missing data, which use the estimated values to impute the missing data. We firstly propose an online algorithm for the standard autoregressive (AR) model with missing data. In addition, the length of coefficients vector is added to utilize more previous time series data under an improper setting. On this basis, two sparse online learning algorithms are proposed, one is for alpha-exp-concave loss functions and the other is for general loss functions. Our theorem guarantees the algorithms to approach the performance of the best AR coefficients vector in hindsight. Furthermore, we conduct a set of experiments on real data to show that our algorithm can achieve much the same error compared to the state-of-the-art algorithm and need less time.																	1370-4621	1573-773X				DEC	2019	50	3					2247	2263		10.1007/s11063-019-10007-x													
J								Surface Electromyography-Based Daily Activity Recognition Using Wavelet Coherence Coefficient and Support Vector Machine	NEURAL PROCESSING LETTERS										Electromyography; Activity recognition; Wavelet transforms; Wavelet coherence coefficients; Support vector machine	EMG; SEMG	Daily activity monitoring plays an important role among frail or elderly people and has caught attention. Surface electromyography (sEMG) can extract the feature of activity, but it is not stable because of electrode displacement, postural changes, and individual-dependent features, such as the condition of muscles, subcutaneous fat, and skin surface. To effectively extract the feature of sEMG signal, we proposed a new method of feature extraction based on coherence analysis. The sEMG signals were recorded from gastrocnemius, tibialis anterior, rectus femoris, and semitendinosus. After de-noising, sEMG signals were decomposed into 32-scale by wavelet transformation, and their wavelet coefficients were employed to calculate wavelet coherence coefficients (WCC). We employed T test to find out if the coherence between sEMG signals was statistically different among six activities. The 32nd scale WCC of RF-ST and ST-TA as eigenvector was entered into the support vector machine (SVM). The six activities, namely, standing, walking, running, stair-ascending, stair-descending, and falling, were successfully identified by the WCC feature with the SVM classifier.																	1370-4621	1573-773X				DEC	2019	50	3					2265	2280		10.1007/s11063-019-10008-w													
J								Feature Selection for Recognition of Online Handwritten Bangla Characters	NEURAL PROCESSING LETTERS										Feature selection; Krill-herd optimization; Online handwriting recognition; Bangla script; Quad-tree based image segmentation	GENETIC ALGORITHMS; KRILL HERD; OPTIMIZATION; CLASSIFIERS	Feature selection through optimization techniques provides an interesting approach to minimize computational time with enhanced prediction capability, and has better cognizance of data in any pattern recognition application. This paper is an extended version of previously published work (Sen et al. in: 7th IAPR TC3 workshop on artificial neural networks in pattern recognition, Ulm, Germany, pp 246-256, 2016), where a quad-tree based image segmentation approach has been discussed to estimate some topological and shape based features (192-attributed) for the recognition of online handwritten Bangla characters. The previous work achieved a recognition accuracy of 98.5% on a database consisting of 10,000 handwritten Bangla characters. In this paper, parameters, used in the previous version during feature estimation, are tuned to improve the performance of the overall system. Thereafter, krill-herd a bio-inspired, meta-heuristic algorithm has been applied to find the optimal feature vector by reducing the dimension of the original feature vector. The reduced feature vector has been fed to Sequential Minimal Optimization classifier for the recognition of the same online handwritten Bangla character database used in previous work. It has been observed that result obtained with this optimal feature set is almost equivalent as the result produced by the entire feature vector.																	1370-4621	1573-773X				DEC	2019	50	3					2281	2304		10.1007/s11063-019-10010-2													
J								Short-Term Traffic Flow Prediction Based on Least Square Support Vector Machine with Hybrid Optimization Algorithm	NEURAL PROCESSING LETTERS										Least square support vector machine; Traffic flow prediction; Particle swarm optimization; Genetic algorithm	MODEL; REAL; REGRESSION; NETWORKS; SYNCHRONIZATION; CONVERGENCE; MOMENTUM; VOLUME	Accurate short-term traffic flow prediction plays an indispensable role for solving traffic congestion. However, the structure of traffic data is nonlinear and complicated. It is a challenge to get high precision. The least square support vector machine (LSSVM) has powerful capabilities for time series and nonlinear regression prediction problems if it can select appropriate parameters. To search the optimal parameters of LSSVM, this paper proposes a hybrid optimization algorithm which combines particle swarm optimization (PSO) with genetic algorithm. The main contributions are twofold: (1) A hybrid optimization method is proposed, which can skip the local optimal pitfall with less learning time by introducing a selection strategy, crossover and mutation operators into PSO; (2) the crossover and mutation operators are controlled by adaptive probability functions. The crossover and mutation probabilities increase when the population fitness is concentrated, and decrease when the fitness is dispersed. It can effectively improve the precision and speed of convergence. The proposed model is verified based on the measured data. The experimental results show that our new model yields better prediction ability and relatively high computational efficiency compared with other related models.																	1370-4621	1573-773X				DEC	2019	50	3					2305	2322		10.1007/s11063-019-09994-8													
J								Alignment Based Feature Selection for Multi-label Learning	NEURAL PROCESSING LETTERS										Multi-label learning; Feature selection; Kernel alignment; Label weight	KERNEL ALIGNMENT; NEURAL-NETWORKS; CLASSIFICATION	Multi-label learning deals with data sets in which each example is assigned with a set of labels, and the goal is to construct a learning model to predict the label set for unseen examples. Multi-label data sets share the same problems with single-label data sets that usually possess high-dimensional features and may exist redundancy features, which will influence the performance of the algorithm. Thus it is obviously necessary to address feature selection in multi-label learning. Meanwhile, information among labels play an important role in multi-label learning, thereby it is significance to measure information among labels in order to improve the performance of learning algorithms. In this paper, we introduce kernel alignment into multi-label learning to measure the consistency between feature space and label space by which features are ranked and selected. Firstly we define an ideal kernel in label space as a convex combination of ideal kernels defined by each label, and a linear combination of kernels where each kernel corresponds to a feature. Secondly, through maximizing the kernel alignment value between linear combination kernel and ideal kernel, both weights in the two defined kernels are learned in this process simultaneously, and the learned weights of labels can be employed as the degree of labeling importance regarded as a kind of information among labels. Finally, features are ranked according to their weights in linear combined kernel, and a proper feature subset consisting of top ranking features is selected. Thus a novel method of feature selection for multi-label learning is developed which can learn and address importance degree of labels automatically, and effectiveness of this method is demonstrated by experimental comparisons.																	1370-4621	1573-773X				DEC	2019	50	3					2323	2344		10.1007/s11063-019-10009-9													
J								Artificial Neural Networks with Random Weights for Incomplete Datasets	NEURAL PROCESSING LETTERS										Neural networks; Missing data; Unscented transform	DISTANCE ESTIMATION; MISSING DATA; CLASSIFICATION; ALGORITHMS; MACHINE; MIXTURE	In this paper, we propose a method to design Neural Networks with Random Weights in the presence of incomplete data. We present a method, under the general assumption that the data is missing-at-random, to estimate the weights of the output layer as a function of the uncertainty of the missing data estimates. The proposed method uses the Unscented Transform to approximate the expected values and the variances of the training examples after the hidden layer. We model the input data as a Gaussian Mixture Model with parameters estimated via a maximum likelihood approach. The validity of the proposed method is empirically assessed under a range of conditions on simulated and real problems. We conduct numerical experiments to compare the performance of the proposed method to the performance of popular, parametric and non-parametric, imputation methods. By the results observed in the experiments, we conclude that our proposed method consistently outperforms its counterparts.																	1370-4621	1573-773X				DEC	2019	50	3					2345	2372		10.1007/s11063-019-10012-0													
J								Pinning Adaptive and Exponential Synchronization of Fractional-Order Uncertain Complex Neural Networks with Time-Varying Delays	NEURAL PROCESSING LETTERS										Fractional-order; Complex neural networks; Synchronization; Pinning control; Time-varying delays	DYNAMICAL NETWORKS	This paper investigates the adaptive and exponential synchronization control problem of fractional-order complex neural networks (FOCNN) with uncertain and time-varying delays. Based on the stability theorem of fractional-order dynamics system, an adaptive pinning controller is designed to achieve the asymptotic synchronization of uncertain FOCNN under the corresponding update law, which is only dependent on the states of complex neural networks. Further, some sufficient criteria are derived to ensure exponential synchronization of the derive system and response system by utilizing the Lyapunov stability theory. Finally, the effectiveness of the proposed theoretical results are demonstrated by two numerical examples of uncertain FOCNN with time-varying delays.																	1370-4621	1573-773X				DEC	2019	50	3					2373	2388		10.1007/s11063-019-10014-y													
J								A Novel Noise-Enhanced Back-Propagation Technique for Weak Signal Detection in Neyman-Pearson Framework	NEURAL PROCESSING LETTERS										Signal detection; Neyman-Pearson framework; Binary hypothesis	STOCHASTIC RESONANCE	In this paper, we propose a noise enhanced neural network based detector. The proposed method can detect the known weak signal in additive non-Gaussian noise. Carefully injected noise in a neural network enhances the weak signal detection performance. During training, the back-propagation algorithm achieves less error and it converges faster with the addition of the external noise. The optimum value of external noise is calculated theoretically and justified by simulation. This method excels over the traditional neural network based detectors in terms of its performance characteristics i.e., the probability of detection (P-D) at some specified probability of false alarm (P-FA). Performance of the noise enhanced neural network based detector under several signal-to-noise ratio environments are also compared with state-of-the-art detectors.																	1370-4621	1573-773X				DEC	2019	50	3					2389	2406		10.1007/s11063-019-10013-z													
J								Finite-Time and Fixed-Time Synchronization of Inertial Cohen-Grossberg-Type Neural Networks with Time Varying Delays	NEURAL PROCESSING LETTERS										Inertial Cohen-Grossberg-type; Neural networks; Finite-time synchronization; Fixed-time synchronization; Time-varying delays	GLOBAL EXPONENTIAL STABILITY; MIXED DELAYS; SYSTEMS; STABILIZATION	This paper is devoted to studying the finite-time and fixed-time of inertial Cohen-Grossberg type neural networks (ICGNNs) with time varying delays. First, by constructing a proper variable substitution, the original (ICGNNs) can be rewritten as first-order differential system. Second, by utilizing feedback controllers and constructing suitable Lyapunov functionals, several new sufficient conditions guaranteeing the finite-time and the fixed-time synchronization of ICGNNs with time varying delays are obtained based on different finite-time synchronization analysis techniques. The obtained sufficient conditions are simple and easy to verify. Numerical simulations are given to illustrate the effectiveness of the theoretical results.																	1370-4621	1573-773X				DEC	2019	50	3					2407	2436		10.1007/s11063-019-10018-8													
J								Remaining Life Prediction Method for Rolling Bearing Based on the Long Short-Term Memory Network	NEURAL PROCESSING LETTERS										Long short-term memory; Life prediction; Related-similarity features; Degradation; Feature parameter		A residual life prediction method based on the long short-term memory (LSTM) was proposed for remaining useful life (RUL) prediction in this paper. Firstly, feature parameters were extracted from time domain, frequency domain, time-frequency domain and related-similarity features; then three feature evaluation indicators were defined to select feature parameters that could better represent the degradation process of bearings and constructed the feature set with the time factor. The data of the feature set was used to train the LSTM network prediction model, and then the RUL was predicted by the trained neural network. The full life test of rolling bearing was provided to demonstrate that this method could accurately predict the remaining life of the rolling bearing, and the result was compared with the prediction results of BP neural network and support vector regression machine to verify the effectiveness.																	1370-4621	1573-773X				DEC	2019	50	3					2437	2454		10.1007/s11063-019-10016-w													
J								Application of Mind Evolutionary Algorithm and Artificial Neural Networks for Prediction of Profile and Flatness in Hot Strip Rolling Process	NEURAL PROCESSING LETTERS										Artificial neural network; Mind evolutionary algorithm; Principal component analysis; Genetic algorithm; Profile and flatness prediction; Hot strip rolling	MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; CONTROL MODEL; MILL; DECOMPOSITION; STRATEGY; FORCE	Strip shape prediction is one of the most important technical to improve the quality of products in hot strip rolling process. In this paper, three hybrid models, including GA-MLP, MEA-MLP and PCA-MEA-MLP, are proposed for profile and flatness predictions by combining genetic algorithm (GA), mind evolutionary algorithm (MEA), principal component analysis (PCA) and multi-layer perceptron (MLP) neural networks. Mean absolute error (MAE), mean absolute percentage error, root mean squared error are adapted to evaluate the performance of the models. The results show that the data-driven model based on intelligent algorithm optimization neural networks can achieve good prediction of profile and flatness. Comparing with the hybrid GA-MLP model, the training speed of the hybrid MEA-MLP model is faster and the training time is greatly reduced. The model establishing with the input data after dimensionality reduction by PCA can reduce training time and become simple. The innovation of this paper is to propose a data-driven fast response model based on intelligent algorithm optimization neural network to replace the traditional mechanism model based on mathematical formula analysis to study complex, non-linear strip shape control in hot rolling process.																	1370-4621	1573-773X				DEC	2019	50	3					2455	2479		10.1007/s11063-019-10021-z													
J								Stable and Refined Style Transfer Using Zigzag Learning Algorithm	NEURAL PROCESSING LETTERS										Style transfer; Neural networks; Deep learning; Painting transfer		Recently, style transfer based on the convolutional neural network has achieved remarkable results. In this paper, we extend the original neural style transfer algorithm to ameliorate the instability in the reconstruction of certain structural information, and improve the ghosting artefacts in the background of image which with low texture and homogeneous areas. For that end, we adopt zigzag learning strategy: The model parameters are optimized to an intermediate target firstly, then let the model converge to the final goal. We show the zigzag learning to multi-sample model which is fabricated from resampling the style input and to loss function that is split into two sections. And also, we demonstrate experimentally the effectiveness of the proposed algorithm and provide its theoretical analysis. Finally we show how to integrate the zigzag learning strategy in fast neural style transfer framework.																	1370-4621	1573-773X				DEC	2019	50	3					2481	2492		10.1007/s11063-019-10024-w													
J								Dynamic Optimization of Neuron Systems with Leakage Delay and Distributed Delay via Hybrid Control	NEURAL PROCESSING LETTERS										Neuron system; Hopf bifurcation; Oscillation; Hybrid control; Leakage delay; Strong kernel	ORDER PD CONTROL; HOPF BIFURCATIONS; NETWORK MODEL; STABILITY; SYNCHRONIZATION; DISCRETE	This paper proposes a neuron system with both leakage delay and distributed delay. Typical dynamics including the local stability and Hopf bifurcation analysis are investigated. Then, a hybrid controller is designed to control the Hopf bifurcation of the proposed system. By regarding the leakage delay as the bifurcation parameter and further probing into the associated characteristic equation, we find that the system can generate a Hopf bifurcation when the bifurcation parameter passes through some critical value. Besides, the controller is capable of altering the bifurcation point and achieving desirable dynamics by changing the feedback gain parameter. Finally, numerical simulations are illustrated to substantiate the theoretical analysis.																	1370-4621	1573-773X				DEC	2019	50	3					2493	2514		10.1007/s11063-019-10023-x													
J								Synchronization of Coupled Complex-Valued Impulsive Neural Networks with Time Delays	NEURAL PROCESSING LETTERS										Complex-valued neural networks; Synchronization; Impulsive effects; Time delays	STABILITY	In this paper, we mainly investigate synchronization of complex-valued neural networks (CVNNs) with time delays and impulsive effects. By using Lyapunov method and some inequality techniques, some sufficient conditions for the synchronization of CVNNs with time delays and impulsive effects are proposed. Finally, a numerical example based on small-world networks is presented to demonstrate correctness of the theoretical results.																	1370-4621	1573-773X				DEC	2019	50	3					2515	2527		10.1007/s11063-019-10028-6													
J								Memory-based State Estimation of T-S Fuzzy Markov Jump Delayed Neural Networks with Reaction-Diffusion Terms	NEURAL PROCESSING LETTERS										Delayed neural networks; Markov jump; Memory-based control; Reaction-diffusion terms; T-S fuzzy model	EXPONENTIAL STABILITY; H-INFINITY; SYNCHRONIZATION; DESIGN; DISSIPATIVITY; PARAMETERS; SYSTEMS	This paper investigates the problem of state estimation for Takagi-Sugeno (T-S) fuzzy Markov jump delayed neural networks with reaction-diffusion terms. A memory-based control scheme that contains a constant signal transmission delay is adopted, which is the first attempt to handle the issue of state estimation for fuzzy neural networks. Firstly, several conditions that guarantee the stability of the considered system are derived. Then, the fuzzy memory-based controller design scheme is proposed. Finally, three numerical examples are given to demonstrate the validity of the proposed method.																	1370-4621	1573-773X				DEC	2019	50	3					2529	2546		10.1007/s11063-019-10026-8													
J								Synchronizing Chaotic Systems with Uncertain Model and Unknown Interference Using Sliding Mode Control and Wavelet Neural Networks	NEURAL PROCESSING LETTERS										Synchronizing chaotic systems; Uncertain model; Unknown interference; Sliding mode control; Wavelet neural networks	PROJECTIVE SYNCHRONIZATION; PARAMETERS; DESIGN; DELAY	A method using sliding mode control (SMC) and wavelet neural networks (WNN) is proposed, investigated and exploited for synchronizing master and slave chaotic systems with uncertain model and unknown interference. In this paper, integral sliding surface and applying WNN for approximating uncertain model and unknown interference are further developed for designing adaptive sliding mode controller. Mexican hat wavelet function is used as activation function in WNN. The adaptive laws of network parameters are derived in the sense of Lyapunov stability analysis so that the tracking errors and convergence of the weights can be guaranteed. The error of synchronization of master-slave chaotic systems can be reached desired level in limited time by using Li function in SMC. Illustrative examples are provided and analyzed to substantiate the efficacy of proposed method for solving the problem of synchronizing master and slave chaotic systems.																	1370-4621	1573-773X				DEC	2019	50	3					2547	2565		10.1007/s11063-019-10034-8													
J								Generalized Regression Neural Network Optimized by Genetic Algorithm for Solving Out-of-Sample Extension Problem in Supervised Manifold Learning	NEURAL PROCESSING LETTERS										Manifold learning; Dimensionality reduction; Out-of-sample extension; Genetic algorithm; Generalized regression neural network; Optimization	NONLINEAR DIMENSIONALITY REDUCTION; CLASSIFICATION; MACHINE; MODEL	With the advent of big data, massive amounts of high-dimensional data have been accumulated in many fields. The assimilation and processing of such high-dimensional data can be particularly challenging. Manifold learning offers a means for effectively dealing with this challenge. However, the results of applying manifold learning to supervised classification have remained unsatisfactory. The out-of-sample extension problem is a critical issue that must be properly solved in this regard. Genetic algorithms (GAs) have excellent global search capabilities. This paper proposes a generalized regression neural network (GRNN) optimized by a GA for the solution of the out-of-sample extension problem. The prediction performance of a GRNN mainly depends on the appropriateness of the chosen smoothing factor. The essence of the GA optimization is the determination of the optimal smoothing factor of the GRNN, the optimized form of which is subsequently used to forecast the low-dimensional embeddings of the test samples. A GA can be used to obtain a better smoothing factor in a larger search space, resulting in enhanced prediction performance. Experiments were performed to enable a detailed analysis of the important parameters that affect the performance of the proposed algorithm. The results confirmed the effectiveness of the algorithm.																	1370-4621	1573-773X				DEC	2019	50	3					2567	2593		10.1007/s11063-019-10022-y													
J								No-Reference Video Quality Assessment Based on the Temporal Pooling of Deep Features	NEURAL PROCESSING LETTERS										No-reference video quality assessment; Convolutional neural network	NEURAL-NETWORKS; PREDICTION	Video quality assessment (VQA) is an important element of various applications ranging from automatic video streaming to display technology. Furthermore, visual quality measurements require a balanced investigation of visual content and features. Previous studies have shown that the features extracted from a pretrained convolutional neural network are highly effective for a wide range of applications in image processing and computer vision. In this study, we developed a novel architecture for no-reference VQA based on the features obtained from pretrained convolutional neural networks, transfer learning, temporal pooling, and regression. In particular, we obtained solutions by only applying temporally pooled deep features and without using manually derived features. The proposed architecture was trained based on the recently published Konstanz natural video quality database (KoNViD-1k), which contains 1200 video sequences with authentic distortion unlike other publicly available databases. The experimental results obtained based on KoNViD-1k demonstrated that the proposed method performed better than other state-of-the-art algorithms. Furthermore, these results were confirmed by tests using the LIVE VQA database, which contains artificially distorted videos.																	1370-4621	1573-773X				DEC	2019	50	3					2595	2608		10.1007/s11063-019-10036-6													
J								A New Complex-Valued Polynomial Model	NEURAL PROCESSING LETTERS										Complex-valued; Polynomial model; Prediction; Classification	NEURAL-NETWORK; CLASSIFICATION PROBLEM; IMAGE CLASSIFICATION; PREDICTION; ENSEMBLE; SYSTEM	This paper presents a novel complex-valued polynomial model (CPM) for real-valued prediction and classification problems. In a CPM, function, independent variables and dependent variables are complex-valued. Before CPM optimization, real-valued data need to be converted into complex values. As the linear version of additive tree model, additive expression tree is proposed to optimize the complex-valued structure of CPM. Real parts and imaginary parts of the complex-valued coefficients are encoded into a chromosome and brain storm optimization is utilized to evolve the complex-valued coefficients of CPM. CPM is utilized to predict three financial datasets and classify n-class problems. The prediction results show that CPM presents higher forecasting accuracy than real-valued polynomial model, other real-valued neural networks and ordinary differential equation. The classification performance of CPM is compared with existing methods on IRIS, Liver and Ionosphere datasets. And the results reveal that CPM performs better than well-established and newly proposed real-valued classifiers.																	1370-4621	1573-773X				DEC	2019	50	3					2609	2626		10.1007/s11063-019-10042-8													
J								Generalizing the Convolution Operator in Convolutional Neural Networks	NEURAL PROCESSING LETTERS										Generalized convolutional neural networks; Generalized convolution operators; L2 family of generalized convolution operators; Kernel methods; Back-propagation	NEOCOGNITRON; RECOGNITION; EQUATIONS; MODEL	Convolutional neural networks (CNNs) have become an essential tool for solving many machine vision and machine learning problems. A major element of these networks is the convolution operator which essentially computes the inner product between a weight vector and the vectorized image patches extracted by sliding a window in the image planes of the previous layer. In this paper, we propose two classes of surrogate functions for the inner product operation inherent in the convolution operator and so attain two generalizations of the convolution operator. The first one is based on the class of positive definite kernel functions where their application is justified by the kernel trick. The second one is based on the class of similarity measures defined according to some distance function. We justify this by tracing back to the basic idea behind the neocognitron which is the ancestor of CNNs. Both of these methods are then further generalized by allowing a monotonically increasing function (possibly depending on the weight vector) to be applied subsequently. Like any trainable parameter in a neural network, the template pattern and the parameters of the kernel/distance function are trained with the back-propagation algorithm. As an aside, we use the proposed framework to justify the use of sine activation function in CNNs. Additionally, we discovered a family of generalized convolution operators which is based on the convex combination of the dot-product and the negative squared Euclidean distance functions. Our experiments on the MNIST dataset show that the performance of ordinary CNNs can be achieved by generalized CNNs based on weighted L1/L2 distances, proving the applicability of the proposed generalization of the convolutional neural networks.																	1370-4621	1573-773X				DEC	2019	50	3					2627	2646		10.1007/s11063-019-10043-7													
J								Learning Morpheme Representation for Mongolian Named Entity Recognition	NEURAL PROCESSING LETTERS										Named entity recognition; Mongolian morpheme representation; Language model auxiliary loss		Traditional approaches to Mongolian named entity recognition heavily rely on the feature engineering. Even worse, the complex morphological structure of Mongolian words made the data more sparsity. To alleviate the feature engineering and data sparsity in Mongolian named entity recognition, we propose a framework of recurrent neural networks with morpheme representation. We then study this framework in depth with different model variants. More specially, the morpheme representation utilizes the characteristic of classical Mongolian script, which can be learned from unsupervised corpus. Our model will be further augmented by different character representations and auxiliary language model losses which will extract context knowledge from scratch. By jointly decoding by Conditional Random Field layer, the model could learn the dependence between different labels. Experimental results show that feeding the morpheme representation into neural networks outperforms the word representation. The additional character representation and morpheme language model loss also improve the performance.																	1370-4621	1573-773X				DEC	2019	50	3					2647	2664		10.1007/s11063-019-10044-6													
J								Online Hard Region Mining for Semantic Segmentation	NEURAL PROCESSING LETTERS										Hard region mining; Semantic segmentation; Online bootstrapping; CNNs; FCN		Recent advances in semantic segmentation have made significant progress by enlarging the reception fields or capturing contextual information. Semantic segmentation is considered as a per-pixel classification problem. Hard discriminate region existing in an image will limit segmentation accuracy. In this work, we propose an approach to increase the attention to local semantic segmentation performance by region-based hard region mining. To analyse the performance on three popular semantic segmentation datasets, including PASCAL VOC 2012, PASCAL Context and Camvid, we experiment two different semantic segmentation networks, Deeplab v3 and FCN. Our experimental results show consistent improvement, which demonstrating the efficacy of our approach.																	1370-4621	1573-773X				DEC	2019	50	3					2665	2679		10.1007/s11063-019-10047-3													
J								An Analysis of IRL-Based Optimal Tracking Control of Unknown Nonlinear Systems with Constrained Input	NEURAL PROCESSING LETTERS										Optimal tracking control; Nonlinear systems; Constrained input; Integral reinforcement learning (IRL); Neural networks (NN)	POLICY ITERATION; TIME-SYSTEMS	In this paper, a comparison is addressed between two methods, that is, the optimal tracking control methods of unknown nonlinear systems with and without constrained input. Firstly, the optimal tracking problem for a class of affine nonlinear system is formulated. The tracking cost functions are also defined, both for the two methods. The two methods are proved to be equivalent as the actuator bound is large enough. Integral reinforcement learning (IRL) algorithm is employed to solve the optimal tracking problem by using only system data. To facilitate the implementation of the IRL algorithm, the actor-critic neural network technique and the least squares method are employed in approximating the unknown weights iteratively. In the simulation, a detailed comparison is given to demonstrate the relationship between the two methods in the aspects of control input and tracking cost value.																	1370-4621	1573-773X				DEC	2019	50	3					2681	2700		10.1007/s11063-019-10029-5													
J								The Parameter Identification of PMSM Based on Improved Cuckoo Algorithm	NEURAL PROCESSING LETTERS										Permanent magnet synchronous motor; Cloud membership; Cuckoo algorithm; Parameter identification; Fuzzy	STRUCTURAL DESIGN; OPTIMIZATION; SEARCH	In view of the multi-parameter identification problem of permanent magnet synchronous motor, a kind of parameter identification method was proposed based on an improved cuckoo search algorithm. Cuckoo search algorithm has the advantages of simple, less parameters, fast convergence etc., but it also has the defects of premature convergence and low computation accuracy. In view of the deficiency of cuckoo search algorithm, the fuzzy reasoning based on the cloud membership was designed to adjust the probability of an alien egg being discovered by host birds and adaptive variable step method was used to adjust the step size of Levy flights. The improved algorithm can accelerate the convergence speed and improve the local and global optimizing ability by increasing the diversity of the population. The multi-parameter identification results of permanent magnet synchronous motor show that the improved cuckoo algorithm can effectively identify the motor parameters, and compared with the traditional cuckoo algorithm, the effectiveness and superior performance are tested.																	1370-4621	1573-773X				DEC	2019	50	3					2701	2715		10.1007/s11063-019-10052-6													
J								Increasing Capacity of Association Memory by Means of Synaptic Clustering	NEURAL PROCESSING LETTERS										Association memory; Dendritic nonlinearity; Higher-order correlations; Synaptic clustering	NEURAL-NETWORKS; PLASTICITY; DENDRITES; SPARSE; MODEL	Making association is an essential property of human cognition. Systems that try to mimic this process and to make a coherent model of the world should have robust and high capacity association memory. Findings of nonlinear properties of dendritic tree suggest an alternative way how neurons can store associations. In this paper, we present a minimalistic neuron model with clustered synapses and show that it provides much higher association memory capacity compared to traditional models. Due to properties of sparse activation and tracking higher-order correlations in the input pattern an individual neuron can recognize thousands of patterns. Theoretical examination shows that this high capacity is reached because learning exact combinations of active neurons extends the dimension of an input space and thus increases pattern separability. We argue that such beneficial computational properties is realized in biological neural networks through synaptic clustering and sustaining sparse activity in memory-related areas.																	1370-4621	1573-773X				DEC	2019	50	3					2717	2730		10.1007/s11063-019-10051-7													
J								Sample Based Fast Adversarial Attack Method	NEURAL PROCESSING LETTERS										Deep learning; Deep neural network; Adversarial sample; Principle component analysis		Deep neural network (DNN) brings the rapid development of pattern recognition algorithms. However, a large number of experiments show that there are some vulnerabilities in DNNs. Though many adversarial samples generating algorithms has been proposed, most of them based on some known information of attacked model. We proposed a new fast black-box adversarial attack algorithm purely based on data samples. First, we find the key difference between different classes based on principle component analysis and calculate the difference vector. During attacking, we just drive a sample to the target class (for target adversarial) or the nearest other class (for misclassification adversarial). The minimum modification to create an target adversarial sample is obtained by bi-section line search along the difference vector from current class to target class. For misclassification adversarial attack, the minimum modification among all other classes is given. Experimental results show that the proposed algorithm generating comparable adversarial samples much fast then classical attack algorithms.																	1370-4621	1573-773X				DEC	2019	50	3					2731	2744		10.1007/s11063-019-10058-0													
J								A Robust Sentiment Analysis Method Based on Sequential Combination of Convolutional and Recursive Neural Networks	NEURAL PROCESSING LETTERS										Deep learning; Sentiment analysis; Convolutional neural network; Recursive neural network; Combinational model	MODELS	With explosive development of the World Wide Web, an enormous amount of text information containing users' feeling, emotions and opinions has been generated and is increasingly employed by individuals and companies for making decisions. Whereas unstructured form of data must be analyzed to extract and summarize the opinions in them, sentiment analysis has changed to a significant research area in the field of Natural Language Processing. In this regard, deep learning methods have attracted a lot of attentions in recent years and various deep learning models have been proven as effective network architectures for the task of sentiment analysis. However, each of them has its potentials and weak points. To eliminate their drawbacks and make optimal use of their benefits, convolutional and recursive neural network are merged into a new robust model in this paper. The proposed model employs recursive neural network due to its tree structure as a substitute of pooling layer in the convolutional network with the aim of capturing long-term dependencies and reducing the loss of local information. The proposed model is validated on Stanford Sentiment Treebank by conducting a series of experiments and empirical results revealed that our model outperforms basic convolutional and recursive neural networks while requires fewer parameters.																	1370-4621	1573-773X				DEC	2019	50	3					2745	2761		10.1007/s11063-019-10049-1													
J								Time Series Forecasting Using Neural Networks: Are Recurrent Connections Necessary?	NEURAL PROCESSING LETTERS										Time series forecasting; Neural networks; Recurrent neural networks; Resilient propagation; Particle swarm optimization; Cooperative quantum particle swarm optimization	PARTICLE SWARM; CONVERGENCE; PSO	Artificial neural networks (NNs) are widely used in modeling and forecasting time series. Since most practical time series are non-stationary, NN forecasters are often implemented using recurrent/delayed connections to handle the temporal component of the time varying sequence. These recurrent/delayed connections increase the number of weights required to be optimized during training of the NN. Particle swarm optimization (PSO) is now an established method for training NNs, and was shown in several studies to outperform the classical backpropagation training algorithm. The original PSO was, however, designed for static environments. In dealing with non-stationary data, modified versions of PSOs for optimization in dynamic environments are used. These dynamic PSOs have been successfully used to train NNs on classification problems under non-stationary environments. This paper formulates training of a NN forecaster as dynamic optimization problem to investigate if recurrent/delayed connections are necessary in a NN time series forecaster when a dynamic PSO is used as the training algorithm. Experiments were carried out on eight forecasting problems. For each problem, a feedforward NN (FNN) is trained with a dynamic PSO algorithm and the performance is compared to that obtained from four different types of recurrent NNs (RNN) each trained using gradient descent, a standard PSO for static environments and the dynamic PSO algorithm. The RNNs employed were an Elman NN, a Jordan NN, a multirecurrent NN and a time delay NN. The performance of these forecasting models were evaluated under three different dynamic environmental scenarios. The results show that the FNNs trained with the dynamic PSO significantly outperformed all the RNNs trained using any of the other algorithms considered. These findings highlight that recurrent/delayed connections are not necessary in NNs used for time series forecasting (for the time series considered in this study) as long as a dynamic PSO algorithm is used as the training method.																	1370-4621	1573-773X				DEC	2019	50	3					2763	2795		10.1007/s11063-019-10061-5													
J								Exponential Stabilization for Hybrid Recurrent Neural Networks by Delayed Noises Rooted in Discrete Observations of State and Mode	NEURAL PROCESSING LETTERS										Hybrid recurrent neural networks; Random noises; Exponential stabilization; Discrete observations; Delay	VARYING NONLINEAR-SYSTEMS; DIFFERENTIAL-EQUATIONS; MISSING MEASUREMENTS; BROWNIAN NOISE; STABILITY; SUPPRESSES; GROWTH; DESTABILIZATION; SYNCHRONIZATION; OPTIMIZATION	Recently, the random noises derived from discrete state observations are creatively designed to realize the role of stabilization for deterministic systems in the existing result. However, for a hybrid neural network, except for the factor of discrete state observations, one always needs to consider the factors of delays and discrete mode identifications. Hence, taking delays and discrete mode identifications into account for random noises is more reasonable and practical than the original work. Motivated by the idea above, this brief is to design delayed random noises derived from discrete state observations and discrete mode identifications to almost surely exponentially stabilize an unstable hybrid recurrent neural networks, by virtue of M-matrix and stochastic analysis methods.																	1370-4621	1573-773X				DEC	2019	50	3					2797	2819		10.1007/s11063-019-10059-z													
J								Extended H-infinity Synchronization Control for Switched Neural Networks with Multi Quantization Densities Based on a Persistent Dwell-Time Approach	NEURAL PROCESSING LETTERS										Switched neural networks; Synchronization control; Persistent dwell-time; Multi quantization densities	STATE ESTIMATION; EXPONENTIAL SYNCHRONIZATION; LINEAR-SYSTEMS; STABILITY; STABILIZATION; DELAYS	This paper thoroughly investigates the synchronization control issue for the switched neural networks. The more comprehensive comparatively switching rule, persistent dwell-time, is applied to actuate the aforementioned neural networks. For tackling the problem caused by the transmission of tremendous data, the quantizer is utilized. The objective is to establish the mixed controller with multi quantization densities for the synchronization error neural networks to meet the various accuracy requirements of the transmitted data. Whereafter, the sufficient conditions of the extended H-infinity performance and global uniform exponential stability for the synchronization error neural networks are constructed. Conclusively, the capability of the proposed mixed controller is elucidated through a numerical example.																	1370-4621	1573-773X				DEC	2019	50	3					2821	2841		10.1007/s11063-019-10064-2													
J								A Stable, Unified Density Controlled Memetic Algorithm for Gene Regulatory Network Reconstruction Based on Sparse Fuzzy Cognitive Maps	NEURAL PROCESSING LETTERS										Fuzzy cognitive maps; Density controlled operators; Self-learning strategy; Memetic algorithm	OPTIMIZATION; PREDICTION; CLASSIFICATION	Gene regulatory networks (GRNs) denote the interrelation among genes in the genomic level. GRNs have a sparse network structures, and as a simulation of GRNs, the density of The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge is less than 5%. So using sparse models to represent GRNs is a meaningful task. Fuzzy cognitive maps (FCMs) have been used to reconstruct GRNs. However, the networks learned by automated derivate-free methods is much denser than those in practical applications. Moreover, the performance of current sparse FCM learning algorithms is worse than what we expect. Therefore, proposing a fast, simple and sparse FCM learning algorithm is a realistic demand. Here, we propose a new unified algorithm: Density Controlled Memetic Algorithm (DC-MA) for learning sparse FCMs. As a simple and good-performance algorithm, memetic algorithm (MA) is chosen as the framework of DC-MA. In DC-MA, a new crossover operator and a mutation operator are designed to optimize the target, control the density and ensure the stability; the local search is used to improve the accuracy and a special self-learning operator is proposed to adjust density. To test the effectiveness of our algorithm, DC-MA is performed on synthetic data with varying sizes and densities. The results show that DC-MA obtains good performance in learning sparse FCMs from time series. On the benchmark datasets DREAM3, DREAM4 and large-scale GRN reconstruction DREAM5 dataset, DC-MA shows high accuracy. The good performance in learning sparse FCMs shows the effectiveness of DC-MA, and the simplicity and scalability of the framework ensure that DC-MA can be adapted to a wide range of needs.																	1370-4621	1573-773X				DEC	2019	50	3					2843	2870		10.1007/s11063-019-10056-2													
J								Finite-Time Anti-synchronization of Multi-weighted Coupled Neural Networks With and Without Coupling Delays	NEURAL PROCESSING LETTERS										Finite-time anti-synchronization; Coupled neural networks; Multiple weights; Robust anti-synchronization; Switching topology	COMPLEX DYNAMICAL NETWORKS; VARIABLE CHAOTIC SYSTEMS; ROBUST SYNCHRONIZATION; UNKNOWN-PARAMETERS; ADAPTIVE-CONTROL; STABILIZATION; DESIGN	The multi-weighted coupled neural networks (MWCNNs) models with and without coupling delays are investigated in this paper. Firstly, the finite-time anti-synchronization of MWCNNs with fixed topology and switching topology is analyzed respectively by utilizing Lyapunov functional approach as well as some inequality techniques, and several anti-synchronization criteria are put forward for the considered networks. Furthermore, when the parameter uncertainties appear in MWCNNs, some conditions for ensuring robust finite-time anti-synchronization are obtained. Similarly, we also consider the finite-time anti-synchronization and robust finite-time anti-synchronization for MWCNNs with coupling delays under fixed and switched topologies respectively. Lastly, two numerical examples with simulations are provided to confirm the effectiveness of these derived results.																	1370-4621	1573-773X				DEC	2019	50	3					2871	2898		10.1007/s11063-019-10069-x													
J								Learning Distance Metric for Support Vector Machine: A Multiple Kernel Learning Approach	NEURAL PROCESSING LETTERS										Metric learning; Multiple kernel learning; Gaussian RBF kernel; Support vector machines	CLASSIFICATION; RECOGNITION	Recent work in distance metric learning has significantly improved the performance in k-nearest neighbor classification. However, the learned metric with these methods cannot adapt to the support vector machines (SVM), which are amongst the most popular classification algorithms using distance metrics to compare samples. In order to investigate the possibility to develop a novel model for joint learning distance metric and kernel classifier, in this paper, we provide a new parameterization scheme for incorporating the squared Mahalanobis distance into the Gaussian RBF kernel, and formulate kernel learning into a generalized multiple kernel learning framework, gearing towards SVM classification. We demonstrate the effectiveness of the proposed algorithm on the UCI machine learning datasets of varying sizes and difficulties and two real-world datasets. Experimental results show that the proposed model achieves competitive classification accuracies and comparable execution time by using spectral projected gradient descent optimizer compared with state-of-the-art methods.																	1370-4621	1573-773X				DEC	2019	50	3					2899	2923		10.1007/s11063-019-10053-5													
J								Double-Key Secure for N-1-N Sound Record Data (SRD) by the Drive-Response of BAM NNs	NEURAL PROCESSING LETTERS										Bidirectional associative memory; Dynamical signal; Neural networks; Secure; Sound record data	TIME-VARYING DELAY; NEURAL-NETWORKS; LEAKAGE DELAY; SYNCHRONIZATION; ENCRYPTION; BIFURCATION	In this work, the problem of N sound record data (audio) encryption based on double-key secure is proposed. The first key is the math tricks to cumulate N audio files in a single file. The second key is the values of the parameters A, B, D, (a) over tilde, (b) over tilde, (D) over tilde, of the constructed drive-response bidirectional associative memory neural networks to be found by suitable Lyapunov-Krasovskii functional and satisfying the linear matrix inequality to obtain the dynamical signal (irregular), which are used to encrypt an audio file. Further, the key sensitivity of 1e - 10 of the proposed method are large adequate key space to make hacker's attack infeasible. Numerical simulations, cryptanalysis of the proposed scheme are provided to show the best performance.																	1370-4621	1573-773X				DEC	2019	50	3					2925	2944		10.1007/s11063-019-10067-z													
J								T-S Fuzzy Model Identification with Sparse Bayesian Techniques	NEURAL PROCESSING LETTERS										T-S fuzzy system; Sparse representation; Sparse Bayesian techniques; AOMP algorithm	SYSTEM-IDENTIFICATION; ALGORITHM	This paper introduces a novel method for fuzzy modeling based on sparse Bayesian techniques. The sparse representation problems in the Takagi-Sugeno (T-S) fuzzy system identification are studied, which is to establish a T-S fuzzy system with a adaptive number of fuzzy rules and simultaneously have a minimal number of nonzero consequent parameters. The proposed method is called sparse Bayesian fuzzy inference systems (B-sparseFIS). There are two main procedures in the paper. Firstly, initial fuzzy rule of antecedent part is extracted automatically by an AP clustering method. By using the algorithm of adaptive block orthogonal matching pursuit, the number of rules is computed statistically then the main important fuzzy rules can be selected. In the algorithm, the redundant rules are eliminated for better model accuracy and generalization performance; secondly, an adaptive B-sparseFIS is exploited. The consequence of fuzzy system is identified and simplified with sparse Bayesian techniques such that many consequent parameters will approximate to zero. Four examples are provided to illustrate the effectiveness of the proposed algorithm. Furthermore, the performances of the algorithm are validated through the results of statistical analyses including parameter estimate error, MSE, NRMSE, etc.																	1370-4621	1573-773X				DEC	2019	50	3					2945	2962		10.1007/s11063-019-10071-3													
J								Optimal Output Feedback Control of Nonlinear Partially-Unknown Constrained-Input Systems Using Integral Reinforcement Learning	NEURAL PROCESSING LETTERS										Output feedback optimal control; Neural network observer; Partially-unknown dynamics; Constrained-input; Online synchronous integral reinforcement learning	ADAPTIVE OPTIMAL-CONTROL; SWITCHED NEURAL-NETWORKS; POLICY ITERATION; LINEAR-SYSTEMS; ALGORITHM	The problem of output feedback optimal control of the partially-unknown nonlinear systems with constrained-input is investigated in this paper. Firstly, a neural network observer is proposed to estimate the unmeasurable system states. Secondly, synchronous integral reinforcement learning (SIRL) algorithm is used to solve the Hamilton-Jacobi-Bellman (HJB) equation associated with non-quadratic cost function and the optimal controller is obtained without knowing the system drift dynamics. This algorithm is implemented by a critic-actor structure and the novel weight update laws of the neural networks are designed and tuned simultaneously. Moreover, the weight estimation errors of all neural networks are proven uniformly ultimately bounded (UUB), and the stability of the whole closed-loop system is also guaranteed. Finally, two numerical simulation examples support the effectiveness of the proposed methods.																	1370-4621	1573-773X				DEC	2019	50	3					2963	2989		10.1007/s11063-019-10072-2													
J								A Note on Liao's Recurrent Neural-Network Learning for Discrete Multi-stage Optimal Control Problems	NEURAL PROCESSING LETTERS										Backpropagation; Optimal control gradient methods; Deep feed-forward neural-network learning	BACKPROPAGATION; DERIVATION	The roots of neural-network backpropagation (BP) may be traced back to classical optimal-control gradient procedures developed in early 1960s. Hence, BP can directly apply to a general discrete N-stage optimal control problem that consists of N stage costs plus a terminal state cost. In this journal (Liao in Neural Process Lett 10:195-200, 1999), given such a multi-stage optimal control problem, Liao has turned it into a problem involving a terminal state cost only (via classical transformation), and then claimed that BP on the transformed problem leads to new recurrent neural network learning. The purpose of this paper is three-fold: First, the classical terminal-cost transformation yields no particular benefit for BP. Second, two simulation examples (with and without time lag) demonstrated by Liao can be regarded naturally as deep feed-forward neural-network learning rather than as recurrent neural-network learning from the perspective of classical optimal-control gradient methods. Third, BP can readily deal with a general history-dependent optimal control problem (e.g., involving time-lagged state and control variables) owing to Dreyfus's 1973 extension of BP. Throughout the paper, we highlight systematic BP derivations by employing the recurrence relation of nominal cost-to-go action-value functions based on the stage-wise concept of dynamic programming.																	1370-4621	1573-773X				DEC	2019	50	3					3009	3018		10.1007/s11063-019-09986-8													
J								A Novel Genetically Optimized Convolutional Neural Network for Traffic Sign Recognition: A New Benchmark on Belgium and Chinese Traffic Sign Datasets	NEURAL PROCESSING LETTERS										Traffic sign recognition; Convolutional neural network; Domain transfer learning; Genetic algorithms; Ternary crossover	ALGORITHM	Traffic signs are a key constituent of the road network and prove to be very useful for warning and guiding the drivers. In intelligent transport systems, traffic sign recognition (TSR) is indispensable for autonomous driving. However, due to the complex outdoor environment, real-time recognition of traffic signs is much more challenging in comparison with many other pattern recognition tasks. Convolutional neural networks (CNNs) have an exceptional capability of recognizing patterns and are one of the most popular deep learning techniques. Finding the optimal configuration of a CNN for a task is a major challenge and is an active area of research. Genetic algorithm (GA) is a meta-heuristic approach well-known for its optimization power. In this paper, we propose a novel deep learning technique based on the concept of domain transfer learning for the recognition of traffic signs. This technique utilizes a newly proposed variant of the GA for finding the optimal values of the number of epochs and the learning rate parameter for each layer of the pre-trained CNN model (VGG-16). To examine the effectiveness of our technique, we apply it to the following two benchmark datasets of TSR: Belgium Traffic Sign Classification (BTSC) dataset and Chinese Traffic Sign Dataset (TT100K). The results indicate that our model outperforms all the existing approaches applied to these datasets and gives a new benchmark of the recognition accuracies of 99.16% for the BTSC and 96.28% for the TT100K datasets, thus establishing the robustness of our model.																	1370-4621	1573-773X				DEC	2019	50	3					3019	3043		10.1007/s11063-019-09991-x													
J								Projection Recurrent Neural Network Model: A New Strategy to Solve Weapon-Target Assignment Problem	NEURAL PROCESSING LETTERS										Weapon-target assignment problem; Nonlinear optimization problem; Projection recurrent neural network; Global exponential stability; Projection function	PROGRAMMING-PROBLEMS; NEURODYNAMIC MODEL; MANIPULATORS; ALGORITHMS	In the present research, we are going to obtain the solution of the Weapon-Target Assignment (WTA) problem. According to our search in the scientific reported papers, this is the first scientific attempt for resolving of WTA problem by projection recurrent neural network (RNN) models. Here, by reformulating the original problem to an unconstrained problem a projection RNN model as a high-performance tool to provide the solution of the problem is proposed. In continuous, the global exponential stability of the system was proved in this research. In the final step, some numerical examples are presented to depict the performance and the feasibility of the method. Reported results were compared with some other published papers.																	1370-4621	1573-773X				DEC	2019	50	3					3045	3057		10.1007/s11063-019-10068-y													
J								Visual Cognition-Inspired Multi-View Vehicle Re-Identification via Laplacian-Regularized Correlative Sparse Ranking	COGNITIVE COMPUTATION										Vehicle re-identification; Laplacian-regularized correlative sparse ranking; Multi-view; Deep feature		Vehicle re-identification has gradually gained attention and widespread applications. However, most of the existing methods learn the discriminative features for identities by single-feature channel only. It is worth noting that visual cognition of the human eyes is a multi-channel system which usually seeks a sparse representation. Therefore, integrating the multi-view information in sparse representation is a natural way to boost computer vision tasks in challenging scenarios. In this paper, we propose to mine multi-view deep features via Laplacian-regularized correlative sparse ranking for vehicle re-identification. Specifically, first, we employ multiple baseline networks to generate features. Then, we explore the feature correlation via enforcing the correlation term into the multi-view Laplacian sparse ranking framework. The original rankings are obtained by the reconstruction coefficients between the probe and gallery. Finally, we utilize a re-ranking technique to further boost performance. Experimental results on public benchmark VeRi-776 and VehicleID datasets demonstrate that our approach outperforms state-of-the-art approaches. The Laplacian-regularized correlative sparse ranking as a general framework can be used in any multi-view feature fusion and will obtain more competitive results.																	1866-9956	1866-9964															10.1007/s12559-019-09687-3		NOV 2019											
J								A survey of energy-aware cluster head selection techniques in wireless sensor network	EVOLUTIONARY INTELLIGENCE										Wireless sensor networks; Clustering; Cluster head selection; Low-energy adaptive clustering hierarchy; Sensor nodes; Network lifetime; Energy	PARTICLE SWARM OPTIMIZATION; ROUTING PROTOCOL; LION OPTIMIZATION; ALGORITHM	Recently, wireless sensor networks (WSNs) are becoming very famous as they are inexpensive and easy to maintain and manage. The network contains a group of sensor nodes, which are capable of sensing, computing, and transmitting. Energy efficiency is one of the most important challenging problems in WSN. Sensor nodes have inadequate energy and installed in remote areas. Hence, it is difficult to restore the batteries in WSN. Therefore, to maximize the network lifetime, appropriate clustering techniques and cluster head (CH) selection methods should be implemented. The main idea behind the clustering technique is that it clusters the sensor nodes and reduces the composed data simultaneously and then, it broadcasts the data. In this process, CH selection is an essential part. Therefore, this survey paper provides an overview of the clustering techniques for reducing energy consumption by reviewing several CH selection techniques in WSN that provide high energy efficiency. Several techniques have been employed for CH selection based on partitional clustering, optimization, low-energy adaptive clustering hierarchy, hierarchical, distributed, and other classification methods. Finally, an analysis is done based on the implementation tools, metrics employed, accuracy, and achievements of the considered CH selection techniques.																	1864-5909	1864-5917															10.1007/s12065-019-00308-4		NOV 2019											
J								FPGA-accelerated textured surface defect segmentation based on complete period Fourier reconstruction	JOURNAL OF REAL-TIME IMAGE PROCESSING										Complete period truncation; One-dimensional Fourier reconstruction; Resampling; FPGA acceleration; LCD defect segmentation	INSPECTION	Real-time detection of surface defects in high-generation, large-size Liquid Crystal Display (LCD) panels is a serious challenge for both image algorithms and processing speed. For the defect detection of Thin Film Transistor-Liquid Crystal Display (TFT-LCD) images, effectively processing the periodic texture composed of gate lines and data lines is a prerequisite for the success of the algorithm. The traditional one-dimensional Fourier reconstruction algorithm uses a filter-based method to remove most of the texture, but due to the spectral leakage problem, the image boundary cannot be effectively processed. The compensation based on the period extension introduces a more complex ringing effect at the image connection. Starting from the implicit periodic principle of Fourier transform, we propose a strategy of complete period truncation based on subpixel period, which completely eliminates the boundary texture. Furthermore, we fully exploit the potential of parallel execution of the algorithm, resampling the liquid crystal segments truncated in the complete period to an integer power length of 2. The FPGA structure of one-dimensional Fourier reconstruction defect segmentation algorithm with dual-task parallelism and two-pixel parallelism is designed and the calculation bandwidth of 500 MB/s can be realized at 125-MHz clock frequency. We demonstrate the superiority of the proposed method qualitatively and quantitatively. The one-dimensional Fourier reconstruction algorithm based on the complete period truncation can effectively detect various defects such as spots, scratches, fibers and dirt, and the false-positive rate of defects has been reduced by half. The resampling-based Fourier transform speeds up the computational process and the FPGA parallel acceleration architecture is three times faster than comparable server CPUs, reducing the scan detection time of the entire 8.5-generation LCD panel to 8.5 s.																	1861-8200	1861-8219				OCT	2020	17	5					1659	1673		10.1007/s11554-019-00927-1		NOV 2019											
J								Study on the influence of different menu sizes on the operation performance of touch screen	EVOLUTIONARY INTELLIGENCE										Menu sizes; Touch screen; Operation performance; Human-computer interaction; Information system design		The key size of the touchscreen menu will significantly affect the user's operation experience and task completion performance. This paper explores the influence of different shortcut menu sizes on users' operation performance in the touch-screen application situations with two tilting angles (6.6 degrees and 18 degrees). The results show that the tilt angles of the two display screens have no significant influence on the response time of operators and the change trend of menu size. When the shortcut menu size is less than 2.5 x 2.5 cm, the operator's response time increases significantly with the reduction of shortcut menu size. When the shortcut menu size is more than 2.5 x 2.5 cm, the increase of size has no significant impact on the operator's response time. The shortcut menu recommends sizes between 2.0 x 2.0 cm and 2.5 x 2.5 cm, with 2.25 x 2.25 cm being a key size value. The research content can be used as the supporting condition of information system design.																	1864-5909	1864-5917															10.1007/s12065-019-00316-4		NOV 2019											
J								A smart inertial system for fall detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ADL; Classification; Embedded architecture; Fall detection	PEOPLE	The high incidence of falls in elderly and people with neurological disease, represents a serious issue which can have tremendous consequences. To reduce the effect on the society of this phenomenon, scientist in every part of the world, have tried to develop solutions ranging from gait training to more technological approaches such as the assistive device. Although it is evident that the scientific community has developed different and valuable paradigms for the ADLs and fall detection issue, in selecting one of the many possible solution, one has to consider the different advantages and disadvantages in terms of performance, user acceptance and power requirements. In order to meet the necessity to develop solutions and paradigms that can be suitable for battery-powered operations, with specific focus to low power embedded systems, in this paper a Fall and ADLs classification paradigm exploiting an event correlated approach is proposed. It is based on the maximum value of the correlation between two signals implemented in an ad-hoc developed embedded system based on an STmicroelectronics microcontroller equipped with sensors and communication facilities. Results confirm the suitability of the presented methodology showing an average sensitivity (S-e) of 0.97 and an average specificity (S-p) of 0.97.																	1868-5137	1868-5145															10.1007/s12652-019-01573-y		NOV 2019											
J								Human-autonomous devices for characteristic analysis of pompeii trap in American finance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Information entropy; Pompeii trap; Financial risk; Risk measurement; Humanized computing; Human-autonomous devices	INFORMATION ENTROPY; RISK	Effective prevention of traps in financial networks has become a focus of attention. Faced with the diversity of financial data and the complexity of financial traps, how to measure the risk value of investment behavior is very important. After analyzing the nature of risk, this paper puts forward an analysis of the characteristics of Pompeii trap in American finance based on human autonomous equipment. The feature analysis method based on human autonomous equipment is one of many risk measurement methods. On this basis, the information entropy risk function is established, and the validity of the model can be proved by experiments. The experimental results show that the model can help users avoid the so-called risk-free, high-yield, virtual investment projects, and avoid falling into the financial Ponzi scheme of borrowing new and repaying old.																	1868-5137	1868-5145															10.1007/s12652-019-01584-9		NOV 2019											
J								Imaging the search space: a nature-inspired metaheuristic extension	EVOLUTIONARY INTELLIGENCE										Search space imaging; Metaheuristics; Optimization; Particle swarm optimization; Population initialization	OPTIMIZATION; POPULATION	Humans have a long history of exploration throughout which they have devised many imaging technologies such as telescopes, radars and satellites to increase the level of effectiveness and success of their expeditions. This paper proposes the use of imaging concepts to support the search effort of metaheuristics that deploy expedition teams simulating among other things ants, birds and chromosomes to explore the search space of optimization problems. The research involves proposing and developing a set of experimental imaging techniques. Another purpose of the paper is to measure the effectiveness of those proposed imaging techniques on improving the performance of metaheuristic searches that start from initial populations. As a case study an extend to Particle Swarm Optimization metaheuristic algorithm has been performed by implementing and incorporating the proposed imaging techniques and benchmarking them on a platform for comparing continuous optimizers in a black box setting called COCO. The performance of the developed techniques has been evaluated against each other, and against the particle swarm optimization algorithm alone based on the criterion of how many function evaluations were required to reach the set of target values defined by COCO platform. The results show that the use of imaging could produce better results.																	1864-5909	1864-5917															10.1007/s12065-019-00325-3		NOV 2019											
J								Comparison of two methods of removing EOG artifacts for use in a motor imagery-based brain computer interface	EVOLVING SYSTEMS										Blind source separation; Artifact removal; Independent component analysis; Brain computer interface; Classification	BLIND SOURCE SEPARATION; INDEPENDENT COMPONENT ANALYSIS; COMMON SPATIAL-PATTERNS; SINGLE-TRIAL EEG; OCULAR ARTIFACTS; VECTOR ANALYSIS; FILTERS; ICA; CLASSIFICATION; COMMUNICATION	Blind Source Separation (BSS) methods are signal processing tools that are widely utilized for Electroencephalogram (EEG) data analysis. The BSS separates a set of recorded EEG signals into a set of components. Note that the EEG measurements are often contaminated with various types of artifacts such as eye movements and blinks (Electrooculogram, EOG) that make data analysis very difficult. Therefore, we present a comparative study between two methods of removing EOG artifacts to be use in a Motor Imagery-based Brain Computer Interface (MI-BCI). In literature, a method of artifact removal based on Independent Component Analysis (ICA) using the concepts of Renyi's entropy and kurtosis has been presented that we call this method ICA (the first method). However, applying the ICA method to EEG signal directly can cause the significant distortion of the clean EEG signal because this method completely removes independent components capturing artifacts. In this study, we improve this method by applying Wavelet Denoising (WD) only to segments affected by artifacts from the detected artifactual components instead of the complete removal of these components. This method is called ICA-WD (the second method). The quality of removing the artifact is evaluated using the Relative Root-Mean-Squared Error and Average Correlation Coefficient criteria which are computed between the original and processed signals. The results show that the ICA-WD removes the EOG artifacts better than the ICA. Next, we apply the ICA-WD method to dataset 2a of BCI competition IV for the EOG artifact removal. Then, the cleaned EEG signals are employed for feature extraction and classification. The results indicate that the proposed method outperforms the three best competitors and another study performed on the BCI competition IV dataset 2a.																	1868-6478	1868-6486															10.1007/s12530-019-09311-7		NOV 2019											
J								Using demographics toward efficient data classification in citizen science: a Bayesian approach	PEERJ COMPUTER SCIENCE										Citizen science; Bayesian estimation; Data classification; Algorithms	DESIGN INTERVENTIONS; TOOL	Public participation in scientific activities, often called citizen science, offers a possibility to collect and analyze an unprecedentedly large amount of data. However, diversity of volunteers poses a challenge to obtain accurate information when these data are aggregated. To overcome this problem, we propose a classification algorithm using Bayesian inference that harnesses diversity of volunteers to improve data accuracy. In the algorithm, each volunteer is grouped into a distinct class based on a survey regarding either their level of education or motivation to citizen science. We obtained the behavior of each class through a training set, which was then used as a prior information to estimate performance of new volunteers. By applying this approach to an existing citizen science dataset to classify images into categories, we demonstrate improvement in data accuracy, compared to the traditional majority voting. Our algorithm offers a simple, yet powerful, way to improve data accuracy under limited effort of volunteers by predicting the behavior of a class of individuals, rather than attempting at a granular description of each of them.																	2376-5992					NOV 25	2019									e239	10.7717/peerj-cs.239													
J								Visual topic models for healthcare data clustering	EVOLUTIONARY INTELLIGENCE										Visual topic model; Social data; Visual clustering; Cosine based metric; Health tendency		Social media is a great source to search health-related topics for envisages solutions towards healthcare. Topic models originated from Natural Language Processing that is receiving much attention in healthcare areas because of interpretability and its decision making, which motivated us to develop visual topic models. Topic models are used for the extraction of health topics for analyzing discriminative and coherent latent features of tweet documents in healthcare applications. Discovering the number of topics in topic models is an important issue. Sometimes, users enable an incorrect number of topics in traditional topic models, which leads to poor results in health data clustering. In such cases, proper visualizations are essential to extract information for identifying cluster trends. To aid in the visualization of topic clouds and health tendencies in the document collection, we present hybrid topic modeling techniques by integrating traditional topic models with visualization procedures. We believe proposed visual topic models viz., Visual Non-Negative Matrix Factorization (VNMF), Visual Latent Dirichlet Allocation (VLDA), Visual intJNon-negative Matrix Factorization (VintJNMF), and Visual Probabilistic Latent Schematic Indexing (VPLSI) are promising methods for extracting tendency of health topics from various sources in healthcare data clustering. Standard and benchmark social health datasets are used in an experimental study to demonstrate the efficiency of proposed models concerning clustering accuracy (CA), Normalized Mutual Information (NMI), precision (P), recall (R), F-Score (F) measures and computational complexities. VNMF visual model performs significantly at an increased rate of 32.4% under cosine based metric in the display of visual clusters and an increased rate of 35-40% in performance measures compared to other visual methods on different number of health topics.																	1864-5909	1864-5917															10.1007/s12065-019-00300-y		NOV 2019											
J								CASR: a context-aware residual network for single-image super-resolution	NEURAL COMPUTING & APPLICATIONS										Context-aware residual network; Channel and spatial attention scheme; Inception block; Single-image super-resolution	COMPUTATION OFFLOADING METHOD; SERVICE RECOMMENDATION; PRIVACY PRESERVATION; SELECTION	With the significant power of deep learning architectures, researchers have made much progress on super-resolution in the past few years. However, due to low representational ability of feature maps extracted from nature scene images, directly applying deep learning architectures for super-resolution could result in poor visual effects. Essentially, unique characteristics like low-frequency information should be emphasized for better shape reconstruction, other than treated equally across different patches and channels. To ease this problem, we propose a lightweight context-aware deep residual network named as CASR network, which appropriately encodes channel and spatial attention information to construct context-aware feature map for single-image super-resolution. We firstly design a task-specified inception block with a novel structure of astrous filters and specially chosen kernel size to extract multi-level information from low-resolution images. Then, a Dual-Attention ResNet module is applied to capture context information by dually connecting spatial and channel attention schemes. With high representational ability of context-aware feature map, CASR can accurately and efficiently generate high-resolution images. Experiments on several popular datasets show the proposed method has achieved better visual improvements and superior efficiencies than most of the existing studies.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14533	14548		10.1007/s00521-019-04609-8		NOV 2019											
J								Sales switching decision to the online platform of liquefied petroleum gas enterprise with asymmetric information	EVOLUTIONARY INTELLIGENCE										Principal-agent; Asymmetric information; Liquefied petroleum gas; Switching	SUPPLY CHAIN; COSTS	With the development of the internet technology, many energy companies sell products via online platform, e.g., Irving Oil gift cards are sold at Amazon. Liquefied petroleum gas (LPG), as an important kind of energy, is widely used in many area where the natural gas can not reach, especially in the developing countries. Although the sales of the LPG need the order information from the customers, and the order can be sent through the online platform, the online platform's cost information is unknown to the LPG. In this paper, we consider the switching problem of LPG sale to the online platform, and establish the LPG enterprize's optimal switching model with asymmetric information. Then we derive the optimal solution through the analysis of the incentive constraint and the participation constraint to the online platform. Moreover, the comparison with the optimal solution with symmetric information is also addressed. Finally, a numerical example is given to illustrate the effectiveness of the proposed model and the switching strategy.																	1864-5909	1864-5917															10.1007/s12065-019-00320-8		NOV 2019											
J								Logical reduction of metarules	MACHINE LEARNING										Inductive logic programming; Meta-interpretive learning; Logical reduction; Program induction; Inductive programming	REDUNDANCY; BIAS	Many forms of inductive logic programming (ILP) usemetarules, second-order Horn clauses, to define the structure of learnable programs and thus the hypothesis space. Deciding which metarules to use for a given learning task is a major open problem and is a trade-off between efficiency and expressivity: the hypothesis space grows given more metarules, so we wish to use fewer metarules, but if we use too few metarules then we lose expressivity. In this paper, we study whether fragments of metarules can be logically reduced to minimal finite subsets. We consider two traditional forms of logical reduction: subsumption and entailment. We also consider a new reduction technique calledderivation reduction, which is based on SLD-resolution. We compute reduced sets of metarules for fragments relevant to ILP and theoretically show whether these reduced sets are reductions for more general infinite fragments. We experimentally compare learning with reduced sets of metarules on three domains: Michalski trains, string transformations, and game rules. In general, derivation reduced sets of metarules outperform subsumption and entailment reduced sets, both in terms of predictive accuracies and learning times.																	0885-6125	1573-0565				JUL	2020	109	7			SI		1323	1369		10.1007/s10994-019-05834-x		NOV 2019											
J								Semantic association computation: a comprehensive survey	ARTIFICIAL INTELLIGENCE REVIEW										Semantic association computation; Knowledge sources; Semantic similarity; Semantic relatedness; Semantic distance; Semantic models	SIMILARITY; RELATEDNESS; WIKIPEDIA; WORDNET; WEB; KNOWLEDGE; REPRESENTATIONS; ONTOLOGY; WORD2VEC; VECTOR	Semantic association computation is the process of quantifying the strength of a semantic connection between two textual units, based on different types of semantic relations. Semantic association computation is a key component of various applications belonging to a multitude of fields, such as computational linguistics, cognitive psychology, information retrieval and artificial intelligence. The field of semantic association computation has been studied for decades. The aim of this paper is to present a comprehensive survey of various approaches for computing semantic associations, categorized according to their underlying sources of background knowledge. Existing surveys on semantic computation have focused on a specific aspect of semantic associations, such as utilizing distributional semantics in association computation or types of spatial models of semantic associations. However, this paper has put a multitude of computational aspects and factors in one picture. This makes the article worth reading for those researchers who want to start off in the field of semantic associations computation. This paper introduces the fundamental elements of the association computation process, evaluation methodologies and pervasiveness of semantic measures in a variety of fields, relying on natural language semantics. Along the way, there is a detailed discussion on the main categories of background knowledge sources, classified as formal and informal knowledge sources, and the underlying design models, such as spatial, combinatorial and network models, that are used in the association computation process. The paper classifies existing approaches of semantic association computation into two broad categories, based on their utilization of background knowledge sources:knowledge-richapproaches; andknowledge-leanapproaches. Each category is divided further into sub-categories, according to the type of underlying knowledge sources and design models of semantic association. A comparative analysis of strengths and limitations of various approaches belonging to each research stream is also presented. The paper concludes the survey by analyzing the pivotal factors that affect the performance of semantic association measures.																	0269-2821	1573-7462				AUG	2020	53	6					3849	3899		10.1007/s10462-019-09781-w		NOV 2019											
J								Predicting firm failure in the software industry	ARTIFICIAL INTELLIGENCE REVIEW										Decision support systems; Data mining; Imbalanced data; Software industry	BANKRUPTCY PREDICTION; FINANCIAL DISTRESS; CORPORATE FAILURE; STRATEGIES; CLASSIFICATION; CAPABILITIES; REGRESSION; ENSEMBLES; NETWORKS; SURVIVAL	Firm failure rate in the software industry is significantly higher than other industries. Due to the wide use of software products and services, failure in the software industry has implications on the industry itself as well as the economy at the local, national and global levels. This study compares the classification performance of thirteen approaches in terms of predicting firm failure in the US software industry. Seven measures are used to evaluate the classifiers' performance. We use synthetic minority oversampling technique (SMOTE), SMOTEBoost and SMOTEBagging to account for the data imbalance issue. In order to give managers enough time to develop strategies and take the necessary actions to reduce the likelihood of failing, we use 20 financial indicators collected 4 years before the last available date about each firm. Our findings show that embedding SMOTE into boosting and bagging algorithms is better than preprocessing data using SMOTE before learning the classifier. According to the sensitivity analysis, research and development expense is the most significant predictor of firm failure followed by net sales and total revenue. Our results can be used by managers as a decision support tool to identify high-risk firms at an early stage and take the necessary actions to prevent a firm from failing. The early prediction of firm failure will allow software firms to modularize their products or services into specific "features" and offer them as "digital services" using new business models or combine these services with partner firms' services to create new products and address evolving customer expectations. Moreover, the early prediction of firm failure in the software industry calls on firms, both new and those in the growth stage, to componentize their design for adaptability and to build agility in the way firms use their resource mix to address both market gaps as well as operational gaps.																	0269-2821	1573-7462				AUG	2020	53	6					4161	4182		10.1007/s10462-019-09789-2		NOV 2019											
J								Low delay error resilience algorithm for H.265|HEVC video transmission	JOURNAL OF REAL-TIME IMAGE PROCESSING										H; 265|HEVC; Video error resilience; Low delay; Error-prone condition; Video transmission	RATE-CONTROL SCHEME; HEVC; COMMUNICATION; ALLOCATION; H.264/AVC	Transmission of high-resolution compressed video on unreliable transmission channels with time-varying characteristics such as wireless channels can adversely affect the decoded visual quality at the decoder side. This task becomes more challenging when the video codec computational complexity is an essential factor for low delay video transmission. High-efficiency video coding (H.265|HEVC) standard is the most recent video coding standard produced by ITU-T and ISO/IEC organisations. In this paper, a robust error resilience algorithm is proposed to reduce the impact of erroneous H.265|HEVC bitstream on the perceptual video quality at the decoder side. The proposed work takes into consideration the compatibility of the algorithm implementations with and without feedback channel update. The proposed work identifies and locates the frame's most sensitive areas to errors and encodes them in intra mode. The intra-refresh map is generated at the encoder by utilising a grey projection method. The conducted experimental work includes testing the codec performance with the proposed work in error-free and error-prone conditions. The simulation results demonstrate that the proposed algorithm works effectively at high packet loss rates. These results come at the cost of a slight increase in the encoding bit rate overhead and computational processing time compared with the default HEVC HM16 reference software.																	1861-8200	1861-8219															10.1007/s11554-019-00923-5		NOV 2019											
J								Analysis of notations for modeling user interaction scenarios in ubiquitous collaborative systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										User interaction modeling notation; Analysis of visual notation; Ubiquitous collaborative systems; People-driven processes	REQUIREMENTS	Ubiquitous collaborative systems are difficult to design; particularly those where the participants are human beings and there is not a pre-established workflow to coordinate the activities conducted by them. A key challenge for designers of these systems is to envision and represent scenarios where the interaction among users can take place, and thus provide appropriate services to the supporting application. A few modeling languages and notations have been proposed for specifying interaction scenarios among users, but none of them has been broadly adopted by systems' designers, probably because there is not clear evidence that helps engineers decide what notation to use. This paper reviews the three main visual notations proposed to model computer-mediated interaction scenarios and presents an experimental study that analyses not only the usability and usefulness of these notations, but also the tensions among these aspects. The study results help designers identify suitable user interaction representations to support the requirement elicitation and analysis during the development of ubiquitous collaborative systems. The results can also be used to improve the usability and usefulness of other visual notations, and the relationship between these two aspects. In this sense, designers of modeling languages can take advantage of the study findings to generate new proposals or improve the existing ones.																	1868-5137	1868-5145															10.1007/s12652-019-01578-7		NOV 2019											
J								Enhancing street-level interactions in smart cities through interactive and modular furniture	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart cities; Interactive kiosk; Smart furniture; Digital signage; E-services	INFORMATION KIOSKS; HEALTH; FACILITIES; DELIVERY	The idea of smart cities is becoming part of our reality and it represents the transformation that urban areas should undergo to create more sustainable and efficient spaces. Through the combination of technology to gather data, people to produce them and stakeholders' creativity, the smart cities provide tools and mechanisms that bring greater value to the environment and, in the end, improve citizens quality of life. Although the concept of a smart city is evolving fast in terms of technology adoption and user involvement, the new interactive objects that will be deployed in those smart environments to create street-level interactions are still dubious. Hence, this article contributes to presenting the implementation of a multifunctional digital system, in the form of smart furniture, to be deployed in the smart cities. The proposed smart object is a modular and customizable kiosk, resembling a totem, specifically designed to fit into this digital transformation and to respond to users' needs by offering contextualized information and services. The included integrated hardware elements interact digitally and physically with users, sense environmental conditions and send the captured data to the cloud, where remote management tools allow to control and configure the whole setup. The presented smart totem has been customized for two different use-cases to highlight the adaptability of the system to fit for different purposes. Moreover, one of the use-cases has been thoroughly evaluated after being deployed for 6 months in the wild at the Chinese and Japanese Garden of Singapore. The quantitative data gathered throughout this study along with the obtained qualitative users' feedback, help to provide insights for the convenience of deploying new interactive furniture in smart cities, the adoption barriers of such physical systems, and the new opportunities that these street-level interactions can elicit.																	1868-5137	1868-5145															10.1007/s12652-019-01577-8		NOV 2019											
J								Multi-model LSTM-based convolutional neural networks for detection of apple diseases and pests	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Plant diseases and pests detection; Convolutional neural networks; Deep learning architectures; Deep features; LSTM	IDENTIFICATION; MODEL	In this paper, we proposed Multi-model LSTM-based Pre-trained Convolutional Neural Networks (MLP-CNNs) as an ensemble majority voting classifier for the detection of plant diseases and pests. The proposed hybrid model is based on the combination of LSTM network with pre-trained CNN models. Specifically, in transfer learning, we adopted deep feature extraction from various fully connected layers of these pre-trained deep models. AlexNet, GoogleNet and DenseNet201 models are used in this work for feature extraction. The extracted deep features are then fed into the LSTM layer in order to construct a robust hybrid model for apple disease and pest detection. Later, the output predictions of three LSTM layers determined the class labels of the input images by majority voting classifier. In addition, we use an automatic scheme for determining the best choice of the network parameters of the LSTM layer. The experiments are carried out using data consisting of real-time apple disease and pest images from Turkey and the accuracy rates are calculated for performance evaluation. The experimental results show that by using the proposed ensemble combination structure, the results are comparable to, or better than, the pre-trained deep architectures.																	1868-5137	1868-5145															10.1007/s12652-019-01591-w		NOV 2019											
J								Functional localization in the brain of a cynomolgus monkey based on spike pattern recognition with machine learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Functional localization; Spike; Pattern recognition; Machine learning; SVM; KNN	PARKINSONS-DISEASE; BETA OSCILLATIONS; LEAST-SQUARES; STIMULATION; CLASSIFICATION; DIAGNOSIS	Precise functional localization plays an important role in deep brain stimulation for treatment of Parkinson's disease (PD) to target the lesion of stimulation and observe spike morphology of the neural circuit for evaluating effect of stimulation. This research described a functional localization method in the brain of a cynomolgus monkey based on spike patterns recognition at different locations through machine learning algorithm. Through K-means algorithm cluster, spikes were sorted into different clusters, and the template of sorting was considered as spike pattern of each cluster. Four different spike patterns in cortex, three different spike patterns in white matter, and two different spike patterns in striatum were found through sorting in normal monkey, which were seen as features of different locations in normal monkey. While spike patterns found in PD monkey were totally different, including four different spike patterns in cortex, four different spike patterns in white matter, and five different spike patterns in striatum, considered as features of different locations in PD monkey. Cubic support vector machine (SVM) was used to train spike pattern recognition model for functional localization with accuracy of 100% in normal monkey, and the evaluation of trained model demonstrated reasonably excellent recognition accuracy of 99.5%. Weighted K-nearest neighbor (KNN) showed a better performance of accuracy (94.5%) of spike pattern recognition for functional localization in PD monkey than cubic SVM. In evaluation testing of the trained weighted KNN model, the accuracy reached to 96.1%. The results revealed that functional localization based on spike patterns recognition using machine learning algorithm would be an important tool in precise targeting and evaluating outcome not only for Parkinson's disease, but also for other major brain diseases.																	1868-5137	1868-5145															10.1007/s12652-019-01576-9		NOV 2019											
J								Formalizing GDPR Provisions in Reified I/O Logic: The DAPRECO Knowledge Base	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Deontic logic; Reification; Legal Informatics	COMPLEXITY; CIRCUMSCRIPTION; INTENTIONS; MANAGEMENT	The DAPRECO knowledge base is the main outcome of the interdisciplinary project bearing the same name (https://www.fnr.lu/projects/data-protection-regulation-compliance). It is a repository of rules written in LegalRuleML, an XML formalism designed to be a standard for representing the semantic and logical content of legal documents. The rules represent the provisions of the General Data Protection Regulation (GDPR), the new Regulation that is significantly affecting the digital market in the European Union and beyond. The DAPRECO knowledge base builds upon the Privacy Ontology (PrOnto) (Palmirani et al in Proceedings of the 7th international conference on electronic government and the information systems perspective: technology-enabled innovation for democracy, government and governance, 2018c), which provides a model for the legal concepts involved in the GDPR, by adding a further layer of constraints in the form of if-then rules, referring either to standard first order logic implications or to deontic statements. If-then rules are formalized in reified Input/Output logic (Robaldo and Sun in J Log Comput 7, 2017) and then codified in LegalRuleML. Reified Input/Output logic is an application of standard Input/Output logic for legal reasoning, in which Input/Output logic is combined with the reification-based approach in Hobbs and Gordon (A formal theory of commonsense psychology, how people think people think. Cambridge University Press, Cambridge, 2017). The DAPRECO knowledge base is then a case study for reified Input/Output logic, and it shows that the formalism indeed appears to be a good candidate to effectively formalize, via uniform and simple (flat) representations, complex linguistic/deontic phenomena that may be found in legal texts. To date, the DAPRECO knowledge base is the biggest knowledge base in LegalRuleML and Input/Output logic freely available online (https://github.com/dapreco/daprecokb/blob/master/gdpr/rioKB_GDPR.xml).																	0925-8531	1572-9583				DEC	2020	29	4					401	449		10.1007/s10849-019-09309-z		NOV 2019											
J								Elite artificial bees' colony algorithm to solve robot's fuzzy constrained routing problem	COMPUTATIONAL INTELLIGENCE										elite artificial bees' colony algorithm; mixed fuzzy numbers; robot route planning	SOLID TRANSPORTATION PROBLEM; OBJECTIVE SHORTEST-PATH; MOBILE ROBOT	One of the fundamental challenges of the robotics field is robot's movement. That is, why route planning is an eminent issue of robotics research and it is used to enhance autonomy of moving robots in complex environments. The objective of route planning problem is to find the shortest route without collide from initiation point to destination point so that the amount of energy consumption by robot would not exceed a predefined amount. Because neither the amount of energy consumption nor the robot's passed distance index cannot be measured precisely due to environmental conditions, and fuzzy data is used for modeling the problem and the problem would be called "Robot Fuzzy Constrained shortest Route" problem. The main contributions of this study are fivefold: (i) The mathematical model of fuzzy constrained shortest route problem (FCSRP) is formulated; (ii) An elite artificial bees' colony (EABC) algorithm is used to solve the robot's FSCRP; (iii) The proposed EABC algorithm is simulated with two fuzzy networks; (iv) The performance of the proposed approach is compared with the performance of genetic algorithm and particle swarm optimization algorithm; and (v) The results show the convergence speed of the EABC algorithm is higher than the existing algorithms.																	0824-7935	1467-8640				MAY	2020	36	2					659	681		10.1111/coin.12258		NOV 2019											
J								Evolutionary design model of passive filter circuit for practical application	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Evolutionary circuit design; Analog circuit synthesis; Differential evolution; Neighborhood model	DIFFERENTIAL EVOLUTION; ALGORITHMS; REPRESENTATION; ARCHITECTURE	Evolutionary circuit design is a promising way to study new circuit design methodologies, and the passive filter is the most basic circuit module widely existing in modern electronic systems. Focused on the basic and fatal criterion related to the filter circuit design, this paper presents a novel evolutionary design model of passive filter circuit. The proposed model includes a circuit representation method for passive filter circuit design based on circuit cells and the corresponding real encoding scheme, a fast fitness calculation method avoiding expensive SPICE simulations, and a simple and effective cell-based differential evolution algorithm. Experimental results show that the proposed model can quickly obtain filter circuits for challenging specifications. Under harsh design criteria, the design performance of the proposed model is not inferior to that of some advanced professional design techniques based on traditional design ideas.																	1389-2576	1573-7632				DEC	2020	21	4					571	604		10.1007/s10710-019-09369-x		NOV 2019											
J								Differential evolution algorithm with elite archive and mutation strategies collaboration	ARTIFICIAL INTELLIGENCE REVIEW										Differential evolution; Elite archive mechanism; Mutation strategies collaboration mechanism; Arrival flights scheduling	PARAMETER OPTIMIZATION; HARMONY SEARCH; PARTICLE SWARM; ADAPTATION; ENSEMBLE	This paper proposes a differential evolution algorithm with elite archive and mutation strategies collaboration (EASCDE), wherein two main improvements are presented. Firstly, an elite archive mechanism is introduced to make DE/rand/3 and DE/current-to-best/2 mutation strategies converge faster. Secondly, a mutation strategies collaboration mechanism is developed to tightly combine both strategies to balance global exploration and local exploitation. As a result, EASCDE can effectively keep population diversity in the early stage and significantly enhance convergence speed as well as solution quality in the later stage. The performance of EASCDE is verified by experimental analyses on the well-known test functions. The results demonstrate that EASCDE is superior to other compared competitors in terms of solution precision, convergence speed and stability. Moreover, EASCDE is also an efficient method in dealing with arrival flights scheduling problem.																	0269-2821	1573-7462				AUG	2020	53	6					4005	4050		10.1007/s10462-019-09786-5		NOV 2019											
J								MC/DC guided Test Sequence Prioritization using Firefly Algorithm	EVOLUTIONARY INTELLIGENCE										Test Sequence Prioritization; Firefly Optimization; CFG; MC; DC		Optimization of the regression testing process has been playing an important role in developing quality software. Still, it is difficult to achieve satisfactory results on the generation of non-redundant and optimized test sequences. There are many optimization techniques applied to regression testing. Firefly algorithm (FA) has gained its popularity as an optimization technique to provide better solutions in the areas of science and engineering. But, the original FA needs to have a better or modified objective function. This work uses FA with an improvised objective function to generate optimal test paths guided by "Modified Condition/Decision Coverage" (MC/DC) criteria in the form of a guided matrix. This matrix is built with MC/DC influence values that we obtained from the predicate nodes of "control flow graph" (CFG). This guided matrix also helps in measuring the fault-finding potential of a node. It also helps in routing fireflies to move between the nodes of a CFG. We have chosen MC/DC criteria, as it is the second strongest code coverage criteria due to its linear time complexity. Then, FA is used by defining an appropriate objective function to traverse the graph using fireflies. We obtain optimal test sequences after executing the FA. These test sequences are ranked by computing the mean brightness of nodes along a path and then prioritized based on their ranks. Our simulation and comparison are validated by experimenting on several moderate sized Java programs.																	1864-5909	1864-5917															10.1007/s12065-019-00322-6		NOV 2019											
J								Architecture for distributed query processing using the RDF data in cloud environment	EVOLUTIONARY INTELLIGENCE										RDF data; Cloud; Graph patterns; Queries; Triples	ENGINE	From past decade, the advancement in the field of RDF data management poses many challenges to researchers. Processing large volumes of RDF data is very difficult task in the cloud. The RDF data actually contains complex graphs along with large number of schemas. Distributing the RDF data with traditional approaches or partitioning them with conventional mechanism leads to faulty distribution as well as generated large number of join operations. To address the above issues, this paper developed architecture for distributed query processing using the adaptive hash partitioning approach along with hash join operation. This paper also developed an algorithm for executing the query by minimizing the joins. This paper presented an evaluation of the proposed model with other standard model. The experimental results proved that the proposed method had faster response time compared to the other standard models.																	1864-5909	1864-5917															10.1007/s12065-019-00315-5		NOV 2019											
J								Inductive general game playing	MACHINE LEARNING										Program induction; Program synthesis; Inductive logic programming; General game playing	ANSWER; COMPLEXITY; PROGRAMS; RULES	General game playing (GGP) is a framework for evaluating an agent's general intelligence across a wide range of tasks. In the GGP competition, an agent is given the rules of a game (described as a logic program) that it has never seen before. The task is for the agent to play the game, thus generating game traces. The winner of the GGP competition is the agent that gets the best total score over all the games. In this paper, we invert this task: a learner is given game traces and the task is to learn the rules that could produce the traces. This problem is central toinductive general game playing(IGGP). We introduce a technique that automatically generates IGGP tasks from GGP games. We introduce an IGGP dataset which contains traces from 50 diverse games, such asSudoku,Sokoban, andCheckers. We claim that IGGP is difficult for existing inductive logic programming (ILP) approaches. To support this claim, we evaluate existing ILP systems on our dataset. Our empirical results show that most of the games cannot be correctly learned by existing systems. The best performing system solves only 40% of the tasks perfectly. Our results suggest that IGGP poses many challenges to existing approaches. Furthermore, because we can automatically generate IGGP tasks from GGP games, our dataset will continue to grow with the GGP competition, as new games are added every year. We therefore think that the IGGP problem and dataset will be valuable for motivating and evaluating future research.																	0885-6125	1573-0565				JUL	2020	109	7			SI		1393	1434		10.1007/s10994-019-05843-w		NOV 2019											
J								A Convex Variational Model for Learning Convolutional Image Atoms from Incomplete Data	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Variational methods; Learning approaches; Inverse problems; Functional lifting; Convex relaxation; Convolutional Lasso; Machine learning; Texture reconstruction	INFIMAL CONVOLUTION; SPARSE; DECONVOLUTION; OPTIMIZATION	A variational model for learning convolutional image atoms from corrupted and/or incomplete data is introduced and analyzed both in function space and numerically. Building on lifting and relaxation strategies, the proposed approach is convex and allows for simultaneous image reconstruction and atom learning in a general, inverse problems context. Further, motivated by an improved numerical performance, also a semi-convex variant is included in the analysis and the experiments of the paper. For both settings, fundamental analytical properties allowing in particular to ensure well-posedness and stability results for inverse problems are proven in a continuous setting. Exploiting convexity, globally optimal solutions are further computed numerically for applications with incomplete, noisy and blurry data and numerical results are shown.																	0924-9907	1573-7683				APR	2020	62	3			SI		417	444		10.1007/s10851-019-00919-7		NOV 2019											
J								Efficient strategies to reduce power consumption in MANETs	PEERJ COMPUTER SCIENCE										DYMO; AODV; IEEE 802.11; IEEE 802.16; DCF; PS; DTIM		In current circumstances, where amelioration in technology is elevating, power optimization is of grave concern, whilst perceiving portable conditions. The focus is to design an efficient system with an aim to reduce power consumption and improve performance of other metrics. Heterogeneous wireless systems will command in the next-generation wireless networks with the aggregation of different remote access mechanisms. A node in MANET (Mobile Adhoc NETworks) while consuming significant amount of energy practices data transmission and data retrieval process whilst bonding with other neighboring nodes that are within its range. The proposed work implements User Specified energy model and DYMO (DYnamic Manet Ondemand) routing protocol. Further, additional features of IEEE 802.11 i.e., Power Saving Mode is employed. To obtain enhanced coverage at targeted areas, multi-hop relay strategy is taken into account, also to achieve a less power consuming network with a greater service life. Consequently, the efficiency of the devices is monitored by opting Residual Life Accurate battery model, by using different datasets of Duracell AA and AAA batteries. Simultaneously, battery model, energy model and DYMO (DYnamic Manet On-demand) are applied for IEEE 802.16 to get a comparative assessment of power consumption between IEEE 802.11 and IEEE 802.16. Results are generated for both the architectures i.e., 802.11 and 802.16 for metrics such as residual amount of energy for varying simulation time for all the nodes and for energy consumption in AODV (Ad Hoc On-Demand Distance Vector) and DYMO (DYnamic Manet Ondemand) routing protocol using Qualnet version 7.4.																	2376-5992					NOV 18	2019									e228	10.7717/peerj-cs.228													
J								A Sparse-Modeling Based Approach for Class Specific Feature Selection	PEERJ COMPUTER SCIENCE										Feature selection; Sparse coding; Bioinformatics; Dictionary learning; Ensemble learning	MOLECULAR CLASSIFICATION; MUTUAL INFORMATION; EXPRESSION; CARCINOMAS; CANCER	In this work, we propose a novel Feature Selection framework called Sparse-Modeling Based Approach for Class Specific Feature Selection (SMBA-CSFS), that simultaneously exploits the idea of Sparse Modeling and Class-Specific Feature Selection. Feature selection plays a key role in several fields (e.g., computational biology), making it possible to treat models with fewer variables which, in turn, are easier to explain, by providing valuable insights on the importance of their role, and likely speeding up the experimental validation. Unfortunately, also corroborated by the no free lunch theorems, none of the approaches in literature is the most apt to detect the optimal feature subset for building a final model, thus it still represents a challenge. The proposed feature selection procedure conceives a two-step approach: (a) a sparse modeling-based learning technique is first used to find the best subset of features, for each class of a training set; (b) the discovered feature subsets are then fed to a class-specific feature selection scheme, in order to assess the effectiveness of the selected features in classification tasks. To this end, an ensemble of classifiers is built, where each classifier is trained on its own feature subset discovered in the previous phase, and a proper decision rule is adopted to compute the ensemble responses. In order to evaluate the performance of the proposed method, extensive experiments have been performed on publicly available datasets, in particular belonging to the computational biology field where feature selection is indispensable: the acute lymphoblastic leukemia and acute myeloid leukemia, the human carcinomas, the human lung carcinomas, the diffuse large B-cell lymphoma, and the malignant glioma. SMBA-CSFS is able to identify/retrieve the most representative features that maximize the classification accuracy. With top 20 and 80 features, SMBA-CSFS exhibits a promising performance when compared to its competitors from literature, on all considered datasets, especially those with a higher number of features. Experiments show that the proposed approach may outperform the state-of-the-art methods when the number of features is high. For this reason, the introduced approach proposes itself for selection and classification of data with a large number of features and classes.																	2376-5992					NOV 18	2019									e237	10.7717/peerj-cs.237													
J								A unified approach for cluster-wise and general noise rejection approaches for k-means clustering	PEERJ COMPUTER SCIENCE										Clustering; k-means; Noise rejection; Rough set theory	C-MEANS ALGORITHM; ROUGH; FUZZY	Hard C-means (HCM; k-means) is one of the most widely used partitive clustering techniques. However, HCM is strongly affected by noise objects and cannot represent cluster overlap. To reduce the influence of noise objects, objects distant from cluster centers are rejected in some noise rejection approaches including general noise rejection (GNR) and cluster-wise noise rejection (CNR). Generalized rough C-means (GRCM) can deal with positive, negative, and boundary belonging of object to dusters by reference to rough set theory. GRCM realizes cluster overlap by the linear function threshold-based object-cluster assignment. In this study, as a unified approach for GNR and CNR in HCM, we propose linear function threshold-based C-means (LiFTCM) by relaxing GRCM. We show that the linear function threshold-based assignment in LiFTCM includes GNR, CNR, and their combinations as well as rough assignment of GRCM. The classification boundary is visualized so that the characteristics of LiFTCM in various parameter settings are clarified. Numerical experiments demonstrate that the combinations of rough clustering or the combinations of GNR and CNR realized by LiFTCM yield satisfactory results.																	2376-5992					NOV 18	2019									e238	10.7717/peerj-cs.238													
J								Improving rule-based classification using Harmony Search	PEERJ COMPUTER SCIENCE										Apriori algorithm; CBA algorithm; Harmony Search	ALGORITHM; ASSOCIATION; CLASSIFIERS	Classification and associative rule mining are two substantial areas in data mining. Some scientists attempt to integrate these two field called rule-based classifiers. Rule-based classifiers can play a very important role in applications such as fraud detection, medical diagnosis, etc. Numerous previous studies have shown that this type of classifier achieves a higher classification accuracy than traditional classification algorithms. However, they still suffer from a fundamental limitation. Many rule-based classifiers used various greedy techniques to prune the redundant rules that lead to missing some important rules. Another challenge that must be considered is related to the enormous set of mined rules that result in high processing overhead. The result of these approaches is that the final selected rules may not be the global best rules. These algorithms are not successful at exploiting search space effectively in order to select the best subset of candidate rules. We merged the Apriori algorithm, Harmony Search, and classification-based association rules (CBA) algorithm in order to build a rule-based classifier. We applied a modified version of the Apriori algorithm with multiple minimum support for extracting useful rules for each class in the dataset. Instead of using a large number of candidate rules, binary Harmony Search was utilized for selecting the best subset of rules that appropriate for building a classification model. We applied the proposed method on a seventeen benchmark dataset and compared its result with traditional association rule classification algorithms. The statistical results show that our proposed method outperformed other rule-based approaches.																	2376-5992					NOV 18	2019									e188	10.7717/peerj-cs.188													
J								Grey wolf assisted SIFT for improving copy move image forgery detection	EVOLUTIONARY INTELLIGENCE										Copy-move forgery detection; SIFT features; Keypoint threshold; Best-bin-first; GWO algorithm; RANSAC algorithm	PASSIVE DETECTION; ALGORITHM; DCT	Copy-move forgery is a general widespread type of digital image forgery, where a segment of an image is attached into a new portion of the similar image to hide or replicate the parts which are called forgered image. The forgered image appears original, as the objective region in spite of being forged, has attained the fundamental qualities of the similar image itself. The capability of the copy-move forgery detection (CMFD) technique is lacked due to some post-processing functions, like JPEG compression scaling, or rotation, etc. Therefore, this paper intends to develop a CMFD using Scale-invariant feature transform (SIFT), best-fin-first algorithm (BBF) and RANdom SAmple Consensus (RANSAC) directed by grey wolf optimization (GWO) algorithm. Initially, the keypoints are selected using SIFT principle, and BBF algorithm identifies the matched keypoints using keypoint threshold. Further, SIFT feature descriptor is determined, and the final extracted paired keypoints are given to RANSAC algorithm to remove all the mismatched keypoints. In this CMFD model, the parameters such as parameters keypoint threshold, maximum distance of inliers in RANSAC and distance threshold in SIFT features are optimized using GWO. The foremost purpose of this research work is maximizing the number of paired keypoints. Hence the proposed model is termed as GWO-based parameter optimization for CMFD (GWPO-CMD). The proposed model is compared over several other meta-heuristic-based keypoint threshold selections and proves its efficiency through diverse analysis.																	1864-5909	1864-5917															10.1007/s12065-019-00304-8		NOV 2019											
J								Deep learning methods in real-time image super-resolution: a survey	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image super-resolution; Real-time processing; Deep learning; Convolutional neural network; Generative adversarial network	HASHING-BASED APPROACH; SERVICE RECOMMENDATION; SUPER RESOLUTION; RECONSTRUCTION; NETWORK	Super-resolution is generally defined as a process to obtain high-resolution images form inputs of low-resolution observations, which has attracted quantity of attention from researchers of image-processing community. In this paper, we aim to analyze, compare, and contrast technical problems, methods, and the performance of super-resolution research, especially real-time super-resolution methods based on deep learning structures. Specifically, we first summarize fundamental problems, perform algorithm categorization, and analyze possible application scenarios that should be considered. Since increasing attention has been drawn in utilizing convolutional neural networks (CNN) or generative adversarial networks (GAN) to predict high-frequency details lost in low- resolution images, we provide a general overview on background technologies and pay special attention to super-resolution methods built on deep learning architectures for real-time super-resolution, which not only produce desirable reconstruction results, but also enlarge possible application scenarios of super resolution to systems like cell phones, drones, and embedding systems. Afterwards, benchmark datasets with descriptions are enumerated, and performance of most representative super-resolution approaches is provided to offer a fair and comparative view on performance of current approaches. Finally, we conclude the paper and suggest ways to improve usage of deep learning methods on real-time image super-resolution.																	1861-8200	1861-8219															10.1007/s11554-019-00925-3		NOV 2019											
J								Delayed labelling evaluation for data streams	DATA MINING AND KNOWLEDGE DISCOVERY										Stream mining; Delayed labels; Evaluation procedures; Classification	CLASSIFICATION	A large portion of the stream mining studies on classification rely on the availability of true labels immediately after making predictions. This approach is well exemplified by the test-then-train evaluation, where predictions immediately precede true label arrival. However, in many real scenarios, labels arrive with non-negligible latency. This raises the question of how to evaluate classifiers trained in such circumstances. This question is of particular importance when stream mining models are expected to refine their predictions between acquiring instance data and receiving its true label. In this work, we propose a novel evaluation methodology for data streams when verification latency takes place, namely continuous re-evaluation. It is applied to reference data streams and it is used to differentiate between stream mining techniques in terms of their ability to refine predictions based on newly arriving instances. Our study points out, discusses and shows empirically the importance of considering the delay of instance labels when evaluating classifiers for data streams.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1237	1266		10.1007/s10618-019-00654-y		NOV 2019											
J								The role of experts in the public perception of risk of artificial intelligence	AI & SOCIETY										Artificial intelligence; Social impacts of artificial intelligence; Risk; Risk perception; Experts	SOCIAL AMPLIFICATION; FUKUSHIMA	The goal of this paper is to describe the mechanism of the public perception of risk of artificial intelligence. For that we apply the social amplification of risk framework to the public perception of artificial intelligence using data collected from Twitter from 2007 to 2018. We analyzed when and how there appeared a significant representation of the association between risk and artificial intelligence in the public awareness of artificial intelligence. A significant finding is that the image of the risk of AI is mostly associated with existential risks that became popular after the fourth quarter of 2014. The source of that was the public positioning of experts who happen to be the real movers of the risk perception of AI so far instead of actual disasters. We analyze here how this kind of risk was amplified, its secondary effects, what are the varieties of risk unrelated to existential risk, and what is the dynamics of the experts in addressing their concerns to the audience of lay people.																	0951-5666	1435-5655				SEP	2020	35	3					663	673		10.1007/s00146-019-00924-9		NOV 2019											
J								Multilevel assessment of mental stress via network physiology paradigm using consumer wearable devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Stress assessment; Wearable devices; Network Physiology; Measurements; Classification; Machine learning		Mental stress is a physiological condition that has a strong negative impact on the quality of life, affecting both the physical and the mental health. For such a reason, accurate measurements of stress level can be helpful to provide mechanisms for prevention and treatment. This paper proposes a procedure for the classification of different mental stress levels by using physiological signals provided by low invasive wearable devices. 17 healthy volunteers participated in this study. Three different mental states were elicited in them: a resting condition, a stressful cognitive state, and a sustained attention task. The acquired physiological signals were: a one lead electrocardiogram (ECG), a respiratory signal, a blood volume pulse (BVP), and 14 channels of a 10-20 electroencephalogram (EEG). For all subjects, 59 time series of 300 samples each were structured by including the RR series, the respiratory series, the pulse arrival time (PAT) series, and the delta, theta, alpha, beta power series of the 14 EEG channels. Different classifiers were implemented to assess the mental stress level starting from a pool of 3481 features computed from the aforementioned physiological quantities, using the Network Physiology paradigm. The highest achieved accuracy was 84.6%, from logistic regression and random forest classifiers, cross validated by mean of leave-one-person-out analysis. A further analysis was carried out to evaluate the classification accuracy using only cardio-respiratory signals, since the latter are more suitable to be used in real-life scenarios. In this case, the highest achieved accuracy was 76.5% obtained by the random forest classifier.																	1868-5137	1868-5145															10.1007/s12652-019-01571-0		NOV 2019											
J								A random forest-based job shop rescheduling decision model with machine failures	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Job shop; Machine failure; Rescheduling decision; Random forest; Machine learning (ML); Artificial intelligence (AI)	HYBRID GENETIC ALGORITHM; SCHEDULING PROBLEM; REGRESSION; CLASSIFICATION; SEARCH	Machine failures are the common disturbances in production scheduling, whose appearances are generally random and uncertain. Rescheduling strategies have been proposed to deal with them. However, performances of these rescheduling strategies depend on status of machine failures, and there is no single strategy for every failure status. Hence, how to select the optimal strategy intelligently when a machine failure occurs becomes an important issue. Since the development of artificial intelligence (AI) and machine learning (ML) techniques, intelligent rescheduling has become possible. In this paper, we propose a new rescheduling decision model based on random forest, an effective machine learning method, to learn the optimal rescheduling strategy in different machine failures. We adopt a genetic algorithm (GA) to generate an initial scheduling scheme. Then we design simulation experiments to obtain data of different machine failures which could influence the initial scheme. In each machine failure, all rescheduling strategies are executed respectively and their performances are evaluated based on delay and deviation, then the best strategy is selected as a label. The random forest is trained based on these data samples with labels. Thus the internal mechanism between machine failures and rescheduling strategies can be learned. We conduct experiments to verify the effectiveness of this proposed method and the results show that accuracy can be as high as 97%. Moreover, compared with decision tree (DT) and support vector machine (SVM), the proposed method illustrates the best performance.																	1868-5137	1868-5145															10.1007/s12652-019-01574-x		NOV 2019											
J								A novel hybridized metaheuristic technique in enhancing the diagnosis of cross-sectional dent damaged offshore platform members	COMPUTATIONAL INTELLIGENCE										Offshore platform; free vibration; dent depth; dent angle; metaheuristic; differential evolution	TUBULAR MEMBERS; OPTIMIZATION; DESIGN; SYSTEM	Offshore jacket platforms are widely used for oil and gas extraction as well as transportation in shallow to moderate water depth. Tubular cross-sectional elements are used to construct offshore platforms. Tubular cross sections impart higher resistance against hydrodynamic forces and have high torsional rigidity. During operation, the members can be partially or fully damaged due to lateral impacts. The lateral impacts can be due to ship collisions or through the impact of falling objects. The impact forces can weaken some members that influence the overall performance of the platform. This demonstrates an urgent need to develop a framework that can accurately forecast dent depth as well as dent angle of the affected members. This study investigates the use of an adaptive metaheuristics algorithm to provide automatic detection of denting damage in an offshore structure. The damage information includes dent depth and the dent angle. A model is developed in combination with the percentage of the dent depth of the damaged member and is used to assess the performance of the method. It demonstrates that small changes in stiffness of individual damaged bracing members are detectable from measurements of global structural motion.																	0824-7935	1467-8640				FEB	2020	36	1					132	150		10.1111/coin.12247		NOV 2019											
J								Principles alone cannot guarantee ethical AI	NATURE MACHINE INTELLIGENCE											VALUES; BIOETHICS; MEDICINE; CODES	Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public-private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement. AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite this, Brent Mittelstadt highlights important differences between medical practice and AI development that suggest a principled approach may not work in the case of AI.																		2522-5839				NOV	2019	1	11					501	507		10.1038/s42256-019-0114-4													
J								Adversarial explanations for understanding image classification decisions and improved neural network robustness	NATURE MACHINE INTELLIGENCE												For sensitive problems, such as medical imaging or fraud detection, neural network (NN) adoption has been slow due to concerns about their reliability, leading to a number of algorithms for explaining their decisions. NNs have also been found to be vulnerable to a class of imperceptible attacks, called adversarial examples, which arbitrarily alter the output of the network. Here we demonstrate both that these attacks can invalidate previous attempts to explain the decisions of NNs, and that with very robust networks, the attacks themselves may be leveraged as explanations with greater fidelity to the model. We also show that the introduction of a novel regularization technique inspired by the Lipschitz constraint, alongside other proposed improvements including a half-Huber activation function, greatly improves the resistance of NNs to adversarial examples. On the ImageNet classification task, we demonstrate a network with an accuracy-robustness area (ARA) of 0.0053, an ARA 2.4 times greater than the previous state-of-the-art value. Improving the mechanisms by which NN decisions are understood is an important direction for both establishing trust in sensitive domains and learning more about the stimuli to which NNs respond. Deep neural networks can be led to misclassify an image when minute changes that are imperceptible to humans are introduced. While for some networks this ability can cast doubt on the reliability of the model, it also offers explainability for networks that use more robust regularization.																		2522-5839				NOV	2019	1	11					508	516		10.1038/s42256-019-0104-6													
J								Behavioural evidence for a transparency-efficiency tradeoff in human-machine cooperation	NATURE MACHINE INTELLIGENCE											EVOLUTION; PEOPLE	Recent advances in artificial intelligence and deep learning have made it possible for bots to pass as humans, as is the case with the recent Google Duplex-an automated voice assistant capable of generating realistic speech that can fool humans into thinking they are talking to another human. Such technologies have drawn sharp criticism due to their ethical implications, and have fueled a push towards transparency in human-machine interactions. Despite the legitimacy of these concerns, it remains unclear whether bots would compromise their efficiency by disclosing their true nature. Here, we conduct a behavioural experiment with participants playing a repeated prisoner's dilemma game with a human or a bot, after being given either true or false information about the nature of their associate. We find that bots do better than humans at inducing cooperation, but that disclosing their true nature negates this superior efficiency. Human participants do not recover from their prior bias against bots despite experiencing cooperative attitudes exhibited by bots over time. These results highlight the need to set standards for the efficiency cost we are willing to pay in order for machines to be transparent about their non-human nature. Algorithms and bots are capable of performing some behaviours at human or super-human levels. Humans, however, tend to trust algorithms less than they trust other humans. The authors find that bots do better than humans at inducing cooperation in certain human-machine interactions, but only if the bots do not disclose their true nature as artificial.																		2522-5839				NOV	2019	1	11					517	+		10.1038/s42256-019-0113-5													
J								Deep convolutional neural networks in the face of caricature	NATURE MACHINE INTELLIGENCE											RECOGNITION; MODELS; REPRESENTATIONS; DISTINCTIVENESS; RACE; IDENTIFICATION; FEATURES; TEXTURE; OBJECTS; SCALE	Real-world face recognition requires us to perceive the uniqueness of a face across variable images. Deep convolutional neural networks (DCNNs) accomplish this feat by generating robust face representations that can be analysed in a multidimensional 'face space'. We examined the organization of viewpoint, illumination, gender and identity in this space. We found that DCNNs create a highly organized face similarity structure in which identities and images coexist. Natural image variation is organized hierarchically, with face identity nested under gender, and illumination and viewpoint nested under identity. To examine identity, we caricatured faces and found that identification accuracy increased with the strength of identity information in a face, and caricature representations 'resembled' their veridical counterparts-mimicking human perception. DCNNs therefore offer a theoretical framework for reconciling decades of behavioural and neural results that emphasized either the image or the face in representations, without understanding how a neural code could seamlessly accommodate both. Human face recognition is robust to changes in viewpoint, illumination, facial expression and appearance. The authors investigated face recognition in deep convolutional neural networks by manipulating the strength of identity information in a face by caricaturing. They found that networks create a highly organized face similarity structure in which identities and images coexist.																		2522-5839				NOV	2019	1	11					522	529		10.1038/s42256-019-0111-7													
J								Human action recognition with a large-scale brain-inspired photonic computer	NATURE MACHINE INTELLIGENCE											FEATURES; SYSTEMS	The recognition of human actions in video streams is a challenging task in computer vision, with cardinal applications in brain-computer interfaces and surveillance, for example. Recently, deep learning has produced remarkable results, but it can be hard to use in practice, as its training requires large datasets and special-purpose and energy-consuming hardware. In this work, we propose a photonic hardware approach. Our experimental set-up comprises off-the-shelf components and implements an easy-to-train recurrent neural network with 16,384 nodes, scalable to hundreds of thousands of nodes. The system, based on the reservoir computing paradigm, is trained to recognize six human actions from the KTH video database using either raw frames as inputs or a set of features extracted with the histograms of an oriented gradients algorithm. We report a classification accuracy of 91.3%, comparable to state-of-the-art digital implementations, while promising a higher processing speed in comparison to the existing hardware approaches. Because of the massively parallel processing capabilities offered by photonic architectures, we anticipate that this work will pave the way towards simply reconfigurable and energy-efficient solutions for real-time video processing. Photonic computing devices have been proposed as a high-speed and energy-efficient approach to implementing neural networks. Using off-the-shelf components, Antonik et al. demonstrate a reservoir computer that recognizes different forms of human action from video streams using photonic neural networks.																		2522-5839				NOV	2019	1	11					530	537		10.1038/s42256-019-0110-8													
J								Human-level recognition of blast cells in acute myeloid leukaemia with convolutional neural networks	NATURE MACHINE INTELLIGENCE											MYELODYSPLASTIC SYNDROMES; INTEROBSERVER VARIANCE; CLASSIFICATION; DIAGNOSIS; CANCER; IMAGES	Reliable recognition of malignant white blood cells is a key step in the diagnosis of haematologic malignancies such as acute myeloid leukaemia. Microscopic morphological examination of blood cells is usually performed by trained human examiners, making the process tedious, time-consuming and hard to standardize. Here, we compile an annotated image dataset of over 18,000 white blood cells, use it to train a convolutional neural network for leukocyte classification and evaluate the network's performance by comparing to inter- and intra-expert variability. The network classifies the most important cell types with high accuracy. It also allows us to decide two clinically relevant questions with human-level performance: (1) if a given cell has blast character and (2) if it belongs to the cell types normally present in non-pathological blood smears. Our approach holds the potential to be used as a classification aid for examining much larger numbers of cells in a smear than can usually be done by a human expert. This will allow clinicians to recognize malignant cell populations with lower prevalence at an earlier stage of the disease. Deep learning is currently transforming digital pathology, helping to make more reliable and faster clinical diagnoses. A promising application is in the recognition of malignant white blood cells-an essential step for detecting acute myeloid leukaemia that is challenging even for trained human examiners. An annotated image dataset of over 18,000 white blood cells is compiled and used to train a convolutional neural network for leukocyte classification. The network classifies the most important cell types with high accuracy and can answer clinically relevant binary questions with human-level performance.																		2522-5839				NOV	2019	1	11					538	544		10.1038/s42256-019-0101-9													
J								Oversampling method using outlier detectable generative adversarial network	EXPERT SYSTEMS WITH APPLICATIONS										Class imbalance problem; Oversampling; Generative adversarial network; Outlier detection		A class imbalance problem occurs when a particular class of data is significantly more or less than another class of data. This problem is difficult to solve; however, solutions such as the oversampling method using synthetic minority oversampling technique (SMOTE) or conditional generative adversarial network (cGAN) have been suggested recently to solve this problem. In the case of SMOTE and their variations, it is possible to generate biased artificial data because it does not consider the entire data in the minority class. To overcome this problem, an oversampling method using cGAN has been proposed. However, such a method does not consider the majority class that affects the classification boundary. In particular, if there is an outlier in the majority class, the classification boundary may be biased. This paper presents an oversampling method using outlier detectable generative adversarial network (OD-GAN) to solve this problem. We use a discriminator, which is used only for training purposes in cGAN, as an outlier detector to quantify the difference between the distributions of the majority and minority classes. The discriminator can detect and remove outliers. This prevents the distortion of the classification boundary caused by outliers. The generator imitates the distribution of the minority class and generates artificial data to balance the dataset. We experiment with various datasets, oversampling techniques, and classifiers. The empirical results show that the performance of OD-GAN is better than those of other oversampling methods for imbalanced datasets with outliers. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 1	2019	133						1	8		10.1016/j.eswa.2019.0.5.006													
J								Impossibility results for belief contraction	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Belief change; Contraction; Eradication; Success postulates; Finite-based outcome	LOGIC	Three seemingly weak and plausible conditions on an operation of contraction on belief sets are shown to be logically incompatible: (1) there is at least one sentence that can be successfully removed by the operation, (2) both the original belief set and the outcome of the operation have finite representations, and (3) a non-tautologous sentence can be removed without loss of all its non-tautologous logical consequences.																	1012-2443	1573-7470				NOV	2019	87	3			SI		227	232		10.1007/s10472-019-09635-9													
J								Two AGM-style characterizations of model repair	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Belief revision; Model repair; Model checking; Representation theorem	REVISION; LOGIC; VERIFICATION; CHECKING; UPDATE	This work explores formal aspects of model repair, i.e., how to rationally modify Kripke models representing the behavior of a system in order to satisfy a desired property. We investigate the problem in the light of Alchourron, Gardenfors, and Makinson's work on belief revision. We propose two AGM-style characterizations of model repair: one based on belief sets and the other based on structural changes. In the first characterization, we define a set of rationality postulates over formulas with a close correspondence to those in the classical belief revision theory. We show that the proposed set of postulates fully characterizes the expected rationality of modifications in the model repair problem. In the second characterization, we propose a new set of rationality postulates based on structural modifications on models. These postulates have a close correspondence to the classical approach of model repair, while preserving the same rationality of the first characterization. We provide two representation results and the connection between them.																	1012-2443	1573-7470				NOV	2019	87	3			SI		233	257		10.1007/s10472-019-09656-4													
J								Probability, coherent belief and coherent belief changes	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Belief revision; Jeffrey conditionalization; Lockean Thesis; Coherence		This paper is about the statics and dynamics of belief states that are represented by pairs consisting of an agent's credences (represented by a subjective probability measure) and her categorical beliefs (represented by a set of possible worlds). Regarding the static side, we argue that the latter proposition should be coherent with respect to the probability measure and that its probability should reach a certain threshold value. On the dynamic side, we advocate Jeffrey conditionalisation as the principal mode of changing one's belief state. This updating method fits the idea of the Lockean Thesis better than plain Bayesian conditionalisation, and it affords a flexible method for adding and withdrawing categorical beliefs. We show that it fails to satisfy the traditional principles of Inclusion and Preservation for belief revision and the principle of Recovery for belief withdrawals, as well as the Levi and Harper identities. We take this to be a problem for the latter principles rather than for the idea of coherent belief change.																	1012-2443	1573-7470				NOV	2019	87	3			SI		259	291		10.1007/s10472-019-09649-3													
J								Practical reasoning using values: an argumentative approach based on a hierarchy of values	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Practical reasoning; Defeasible argumentation; Hierarchy of values		Values are at the heart of human decision-making. They are used to decide whether something or some state of affairs is good or not, and they are also used to address the moral dilemma of the right thing to do under given circumstances. Both uses are present in several everyday situations, from the design of a public policy to the negotiation of employee benefit packages. Both uses of values are specially relevant when one intends to design or validate that artificial intelligent systems behave in a morally correct way. In real life, the choice of policy components or the agreed upon benefit package are processes that involve argumentation. Likewise, the design and deployment of value-driven artificial entities may be well served by embedding practical reasoning capabilities in these entities or using argumentation for their design and certification processes. In this paper, we propose a formal framework to support the choice of actions of a value-driven agent and arrange them into plans that reflect the agent's preferences. The framework is based on defeasible argumentation. It presumes that agent values are partially ordered in a hierarchy that is used to resolve conflicts between incommensurable values.																	1012-2443	1573-7470				NOV	2019	87	3			SI		293	319		10.1007/s10472-019-09660-8													
