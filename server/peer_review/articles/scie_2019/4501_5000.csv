PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								An analysis ofRelaxedIK: an optimization-based framework for generating accurate and feasible robot arm motions	AUTONOMOUS ROBOTS										Inverse kinematics; Real-time motion planning; Collision avoidance	KINEMATIC CONTROL; MANIPULATORS	We present a real-time motion-synthesis method for robot manipulators, calledRelaxedIK, that is able to not only accurately match end-effector pose goals as done by traditional IK solvers, but also create smooth, feasible motions that avoid joint-space discontinuities, self-collisions, and kinematic singularities. To achieve these objectives on-the-fly, we cast the standard IK formulation as a weighted-sum non-linear optimization problem, such that motion goals in addition to end-effector pose matching can be encoded as terms in the sum. We present a normalization procedure such that our method is able to effectively make trade-offs to simultaneously reconcile many, and potentially competing, objectives. Using these trade-offs, our formulation allows features to berelaxedwhen in conflict with other features deemed more important at a given time. We compare performance against a state-of-the-art IK solver and a real-time motion-planning approach in several geometric and real-world tasks on seven robot platforms ranging from 5-DOF to 8-DOF. We show that our method achieves motions that effectively follow position and orientation end-effector goals without sacrificing motion feasibility, resulting in more successful execution of tasks compared to the baseline approaches. We also empirically evaluate how our solver performs with different optimization solvers, gradient calculation methods, and choice of loss function in the objective function.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1341	1358		10.1007/s10514-020-09918-9		AUG 2020											
J								A method for autonomous robotic manipulation through exploratory interactions with uncertain environments	AUTONOMOUS ROBOTS										Robotic manipulation; Interaction autonomy; Impedance control; Adaptive control	BODY IMPEDANCE CONTROL; STIFFNESS; FORCE	Expanding robot autonomy can deliver functional flexibility and enable fast deployment of robots in challenging and unstructured environments. In this direction, significant advances have been recently made in visual-perception driven autonomy, which is mainly due to the availability of rich sensory data-sets. However, current robots' physical interaction autonomy levels still remain at a basic level. Towards providing a systematic approach to this problem, this paper presents a new context-aware and adaptive method that allows a robotic platform to interact with unknown environments. In particular, a multi-axes self-tuning impedance controller is introduced to regulate quasi-static parameters of the robot based on previous experience in interacting with similar environments and the real-time sensory data. The proposed method is also capable of differentiating internal and external disruptions, and responding to them accordingly and appropriately. An agricultural experiment with different deformable material is presented to validate robot interaction autonomy improvements, and the capability of the proposed methodology in detecting and responding to unexpected events (e.g., faults).																	0929-5593	1573-7527				NOV	2020	44	8					1395	1410		10.1007/s10514-020-09933-w		AUG 2020											
J								The novel multi-swarm coyote optimization algorithm for automatic skin lesion segmentation	EVOLUTIONARY INTELLIGENCE										Coyote optimization algorithm; Multilevel thresholding; Global optimization; PH2 dataset		Coyote optimization algorithm (COA) is one of population-based swarm intelligence algorithms inspired by the swarming behavior of coyotes. However, COA showed its effectiveness in solving the global optimization problem, it suffers from premature convergence and stagnation in local optima, espicially in a complex space. In this paper, the multi-swarm topology is employed, where the population is divided into several sub-swarms. The performance of multi-swarm coyote optimization algorithm (MCOA) is evaluated on a set of benchmark functions provided in the IEEE CEC 2005 and IEEE CEC 2017 special sessions. Also, it is evaluated for solving multi-level thresholding problem, where 44 skin dermoscopic images obatined from PH2 benchmark dataset are used. The experimental results showed that employing mutli-swarm topology can significantly improve the population diversity and thus the exploration ability. Also, the results reveal that proposed MCOA has the advantages of remarkable stability and high accuracy compared with its classical version and other state-of-art meta-heuristic optimization algorithms. Additionally, a new skin lesion segmentation model based on MCOA is proposed as well. The results illustrate the effectiveness and efficiency of the proposed model and it can be further used for skin disease diagnosis and treatment planning.																	1864-5909	1864-5917															10.1007/s12065-020-00450-4		AUG 2020											
J								Multi-user energy efficient secured framework with dynamic resource allocation policy for mobile edge network computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet-of-things (IoT); Best fit factor (BFF); SJFP; eSFFDRR; Mobile-edge-Computing(MEC); Dynamic resource allocation; Storage and computation offloading	OPTIMIZATION; INTERNET	With an exploding use of Mobiles, smartphones and IoT device, mobile edge computing (MEC) emerged as technological boon in computing paradigm. The device offloads the computationally intensive task as well as task relevant to storage to the MEC cloud server to meet the requirement of service delay, extend the battery lifespan of mobile, and resolve the problem of limited mobile device resources. With this reference we propose novel framework architecture with security to perform offloading of high storage and computationally intensive task to MEC server with minimum energy consumption and delay. For dynamic resource allocation, we employ two scheduling policy one at mobile side i.e. SJFP and other at MEC server side i.e. eSFFDRR. Before to offload task, first AES encryption technique is employed to secure input parameter, and then compressed this encrypted data to secure task data and utilize more bandwidth. Our experimental result depicts that for high storage and computationally intensive task our proposed framework can save 85-87% processing time, 70% of memory utilization with minimum energy consumption. The experimental result also proved that our proposed work improves the performance of computationally intensive mobile application with reduced consumption of mobile device resources like computation time, memory utilization, CPU usage and energy consumption.																	1868-5137	1868-5145															10.1007/s12652-020-02407-y		AUG 2020											
J								Hierarchical severity grade classification of non-proliferative diabetic retinopathy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diabetic retinopathy; Gray; level co-occurrence matrix features; Statistical features; Support vector machine; k-nearest neighbour	FUNDUS IMAGES; VESSEL SEGMENTATION; MICROANEURYSMS; DIAGNOSIS; LEVEL	Curability of diabetic retinopathy (DR) abnormalities highly rely on regular monitoring, early-stage diagnosis and timely treatment. Detection and analysis of variation in eye images can help the patient to take the early action before progression of the disease. Vision loss can be effectively prevented by automated diagnostic system that assist the ophthalmologists who otherwise practice manual lesion detection processes which are tedious and time-consuming. This paper proposes a hierarchical severity level grading (HSG) system for the detection and classification of DR ailments. The retinal fundus images in the proposed HSG system are categorized as grade 0 (indicating Non-DR class) and DR severity grades 1, 2, 3 depending upon the number of anomalies; microaneurysms and haemorrhages in the fundus images. The challenge of retinal landmark segmentation, DR retinal discrimination and DR severity grading have been addressed in this work contributing to the novelty of the proposed approach. For non-DR and DR classification, the proposed system achieves an overall accuracy of 98.10% by SVM classifier and 100% by kNN classifier. Hierarchal discrimination into further grades of abnormalities resulted in accuracy values of 95.68% and 92.60% with SVM classifier using Gaussian kernel and, 97.90% and 95.30% employing fine kNN classifier. The HSG system demonstrates a clear improvement in accuracy with significantly less computational time comparative to the other state-of-the-art methods when applied to the MESSIDOR dataset. IDRiD dataset is also evaluated for performance validation of the proposed HSG system yielding a maximum of 94.00% classification accuracy using a kNN classifier with a computational time of 0.67 s.																	1868-5137	1868-5145															10.1007/s12652-020-02426-9		AUG 2020											
J								An effective vitiligo intelligent classification system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vitiligo; Cycle GAN; ADRD; Resnet50; Image classification	CONVOLUTIONAL NEURAL-NETWORKS; DEEP	Vitiligo is one of the most common skin diseases in the world. According to the World Health Organization (WHO), the number of people suffering from vitiligo is growing year by year and vitiligo becomes a worldwide problem. In order to helping doctors with vitiligo diagnosis, we propose a vitiligo artificial intelligence diagnosis system. It is able to generate vitiligo images in Wood Lamp with high resolution and classify these images with high precision. In our system, we employ Cycle-Consistent Adversarial Networks (Cycle GAN) to generate images in Wood Lamp. What's more, we use an advanced super resolution method, Attention-Aware DenseNet with Residual Deconvolution (ADRD), to improve the resolution of images. Finally, we obtain fantastic classification results with Resnet50. Our system is found to achieve the classification performance of 85.69% accuracy, which is increased by 9.32% compared with using Resnet50 to classify original images directly. The optimization and expansion of the system depend on the increase of data set and the improvement of system's modules.																	1868-5137	1868-5145															10.1007/s12652-020-02357-5		AUG 2020											
J								Topic flexible aspect based sentiment analysis using minimum spanning tree with Cuckoo search	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sentiment analysis (SA); Aspect based sentiment analysis; Aspect term extraction (ATE); Minimum spanning tree (MST); Random forest (RF); AdaBoost algorithm		A study by which the opinions, emotions, evaluations, appraisals, and sentiments of people towards different entities is expressed in the form of a text is called Sentiment Analysis (SA). The primary task of a sentiment analysis which is based on the aspects is the extraction of various aspects of the entities and the determination of the sentiments that correspond to the terms of aspects which are commented in the review document. Recently, there is a huge rise in interest to make an identification of various sentiments and aspects at the same time. Feature selection in terms of aspects of entity plays a crucial role in deciding the efficiency of the sentiment analysis; hence the Minimum Spanning Tree (MST) is used for feature selection.The MST has certain major advantages such as being computable quickly. The selection of optimal features to aid in better accuracy of classification is done through MST optimized with Cuckoo search algorithm. The features in sentiment analysis are classified using Random Forest (RF) and Ada Boost classifiers.The Random Forest (RF) is probably the most accurate among all algorithms of learning available today. The Ada Boost algorithm has a performance that is extremely good owing to its ability to be able to generate the expanding diversity. This was done in order to bring about an improvement in the final ensemble, as it contained several weak classifiers.																	1868-5137	1868-5145															10.1007/s12652-020-02416-x		AUG 2020											
J								An abstractive summary generation system for customer reviews and news article using deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recurrent Neural Network; Natural language processing; Recall vocabulary again; Tensor flow		The online customer reviews information available on the internet about any product consider as an essential information resource concerning customer's interest and their knowledge of the product. It is inscribed in the form of natural language and is unstructured data. To reduce the significant information in the form of a summary is vital to the firms that work on business intelligence. It will help in product recommendation and increase in customer understanding about the product. Therefore, there is much research for creating new methodologies to summarise the text in online customer reviews automatically. In this paper, RNN-Long short-term memory Tensor Flow model along with Recall-Vocabulary Again (RVA) and Copy mechanism has used for the task of generating summary in the form of term wise from the customer reviews and news article. The RNN, along with the RVA mechanism, has been trained through a feed-forward neural network with encoder-decoder to solve the general summarization. The method has validated for the efficiency of the Giga word and DUC dataset to minimize the problem of unknown words in a decoder and generate an accurate summary that contains more vital information.																	1868-5137	1868-5145															10.1007/s12652-020-02412-1		AUG 2020											
J								Symmetry in computer-aided music composition system with social network analysis and artificial neural network methods	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial neural network; Genetic algorithm; Computer-aided composition technology; Children's music creation system	GENETIC ALGORITHMS; MODEL	To reduce the cost of music creation, shorten the creation time, help composers create music more easily, and make the style of music works more easily accepted by more people, based on the principle of symmetry, network analysis and ANN (artificial neural network) algorithm are applied to design children's music creation system based on GA (genetic algorithm) and ANN. The system includes human-computer interaction technology, learning mechanism, and distributed architecture. To accurately describe music information and extract different types of music features, the Back-Propagation method is used to establish the music classification model. The system helps musicians to create music in real time, overcomes the contradiction between people's evaluation and music generation speed, and retains the advantages of traditional GA. The results show that, compared with other algorithms, the average accuracy of the computer-aided composition technology based on GA and ANN is 97.5%, the best performance in recall rate and synthesis is 94.35% and 93.134%, respectively, and the algorithm model is effective. Moreover, the simulation results of this algorithm show that it is helpful to improve the composition technology and help composers more easily create music, making the style of music works more acceptable to people. This study can provide theoretical basis and practical significance for the research of computer-aided music creation.																	1868-5137	1868-5145															10.1007/s12652-020-02436-7		AUG 2020											
J								Automatic classification of emotions in news articles through ensemble decision tree classification techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Emotion classification; News; Ensemble; Decision tree; Grafted C4; 5	RECOGNITION	Emotions form a major role in human life. As human interactions with online systems have increased drastically, emotion prediction from online text, which otherwise can be monotonous, would help to provide a better environment to the users. Identification of emotions from a normal text itself is very complicated while news text that does not explicitly convey emotions adds more intricacy to it. Data mining methods can be utilized in this context. In this work, the potential of decision tree classifiers in emotion classification is explored. The advocated methodology incorporates two segments towards emotion identification. The first segment deals with data preparation and involves dataset elicitation, translation, HTML tag removal, stop word elimination and stemming. The second segment that implements data mining takes the output of the first segment as its input and applies feature vector formulation, correlation based feature selection, building of bagged Grafted C4.5 learning model and performance evaluation. Based on the evolved classification rules, the emotions are categorized into joy, surprise, fear, sadness, disgust, neutral and mixed kind. Experiments have been conducted to analyse the effect of feature selection methods and ensemble methods in generating efficient rules. The accuracy is compared against eight other decision tree classifiers and also the support vector machine learning model. The proposed methodology achieves the maximum accuracy of 87.83% justifying its utilization in the real time applications.																	1868-5137	1868-5145															10.1007/s12652-020-02373-5		AUG 2020											
J								An Unsupervised Neural Network for Loop Detection in Underwater Visual SLAM	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot navigation and guidance; Underwater robotics; Loop detection; Deep learning; Neural network	PLACE RECOGNITION; CLOSURE; LOCALIZATION	Thispaper presents a Neural Network aimed at robust and fast visual loop detection in underwater environments. The proposal is based on an autoencoder architecture, in which the decoder part is being replaced by three fully connected layers. In order to help the proposed network to learn the features that define loop closings, two different global image descriptors to be targeted during training are proposed. Also, a method allowing unsupervised training is presented. The experiments, performed in coastal areas of Mallorca (Spain), show the validity of our proposal and compares it to previously existing methods, based on pre-engineered and learned descriptors.																	0921-0296	1573-0409															10.1007/s10846-020-01235-8		AUG 2020											
J								Representation of grossone-based arithmetic in simulink for scientific computing	SOFT COMPUTING										Infinity computer; Scientific computing; Numerical differentiation	HLA-BASED SIMULATIONS; NUMERICAL-METHODS; INFINITESIMALS; METHODOLOGY; WORKING	Numerical computing is a key part of the traditional computer architecture. Almost all traditional computers implement the IEEE 754-1985 binary floating point standard to represent and work with numbers. The architectural limitations of traditional computers make impossible to work with infinite and infinitesimal quantities numerically. This paper is dedicated to the Infinity Computer, a new kind of a supercomputer that allows one to perform numerical computations with finite, infinite, and infinitesimal numbers. The already available software simulator of the Infinity Computer is used in different research domains for solving important real-world problems, where precision represents a key aspect. However, the software simulator is not suitable for solving problems in control theory and dynamics, where visual programming tools like Simulink are used frequently. In this context, the paper presents an innovative solution that allows one to use the Infinity Computer arithmetic within the Simulink environment. It is shown that the proposed solution is user-friendly, general purpose, and domain independent.																	1432-7643	1433-7479															10.1007/s00500-020-05221-y		AUG 2020											
J								Automatic text classification using machine learning and optimization algorithms	SOFT COMPUTING										Text mining; Information retrieval; Document classification; Content analysis; Feature selection; Bio-inspired algorithms; PSO; ACO; ABC; FA; OTFS algorithm; Machine learning algorithms; NB; KNN; SVM; PNN; MLearn-ATC	DOCUMENTS	In the recent years, the volume of text documents in the form of digital way has grown up extremely in size. As significance, there is a need to be competent to automatically bring together and classify the documents based on their content. The main goal of text classification is to partition the unstructured set of documents into their respective categories based on its content. The main aim of this research work is to automatically classify the documents which are stored in the personal computer into their relevant categories. This work has two significant phases. In the first phase, the important features are selected for classification and the second phase is the classification of text documents. For selecting the optimal features, this research work proposes a new algorithm, optimization technique for feature selection (OTFS) algorithm. To estimate the proficiency of proposed feature selection algorithm, the OTFS algorithm was compared with the existing approaches artificial bee colony, firefly algorithm, ant colony optimization and particle swarm optimization. In the second phase, this research work proposed machine learning-based automatic text classification (MLearn-ATC) algorithm for text classification. In classification, the MLearn-ATC algorithm was compared with widely used classification techniques probabilistic neural network, support vector machine, K-nearest neighbor and Naive Bayes. From this, the output of first phase is used as the input for classification phase. The decisive results establish that the proposed algorithms achieve the better accuracy for optimizing the features and classifying the text documents based on their content.																	1432-7643	1433-7479															10.1007/s00500-020-05209-8		AUG 2020											
J								Whale optimization and sine-cosine optimization algorithms with cellular topology for parameter identification of chaotic systems and Schottky barrier diode models	SOFT COMPUTING										Cellular automata; Chaotic systems; Parameter identification; Schottky barrier diodes; Sine-cosine algorithm; Whale optimization	PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZER; DIFFERENTIAL EVOLUTION; SEARCH ALGORITHM; GLOBAL OPTIMIZATION; HARMONY SEARCH; EXTRACTION; DESIGN; BEHAVIOR	This research study aims to enhance the optimization accuracy of the two recently emerged metaheuristics of whale and sine-cosine optimizers by means of the balanced improvements in intensification and diversification phases of the algorithms provided by cellular automata (CA). Stagnation at the early phases of the iterations, which leads to entrapment in local optimum points in the search space, is one of the inherent drawbacks of the metaheuristic algorithms. As a favorable solution alternative to this problem, different types of cellular topologies are implemented into these two algorithms with a view to ameliorating their search mechanisms. Exploitation of the fertile areas in the search domain is maintained by the interaction between the topological neighbors, whereas the improved exploration is resulted from the smooth diffusion of the available population information among the structured neighbors. Numerical experiments have been carried out to assess the optimization performance of the proposed cellular-based algorithms. Optimization benchmark problems comprised of unimodal and multimodal test functions have been applied and numerical results have been compared with those found by some of the state-of-the-art literature optimizers including particle swarm optimization, differential evolution, artificial cooperative search and differential search. Cellular variants have been outperformed by the base algorithms for multimodal benchmark problems of Levy and Penalized1 functions. Then, the proposed cellular algorithms have been applied to two different parameter identification cases in order to test their efficiencies on real-world optimization problems. Extensive performance evaluations on different parameter optimization cases reveal that incorporating the CA concepts on these algorithms not only improves the optimization accuracy but also provides considerable robustness to acquired solutions.																	1432-7643	1433-7479															10.1007/s00500-020-05227-6		AUG 2020											
J								Axiomatic framework of fuzzy entropy and hesitancy entropy in fuzzy environment	SOFT COMPUTING										Fuzzy set; Intuitionist fuzzy set; Pythagorean fuzzy set; Fuzzy entropy; Hesitancy entropy; Overall entropy	SIMILARITY MEASURE; SETS; MEMBERSHIP; FUZZINESS	Entropy is a vital concept to measure uncertainties, in order to measure the uncertainties of fuzzy sets (FSs), intuitionist fuzzy sets (IFSs) and Pythagorean fuzzy sets (PFSs) more fully, in this paper, the axiomatic definition of fuzzy entropy of FSs is modified, the entropy measures of IFSs and PFSs are categorized as fuzzy entropy and hesitancy entropy, and the axiomatic definitions of these two entropy measures are also revised. Further, the axiomatic definitions of two overall entropies are given based on fuzzy entropy and hesitancy entropy, and the expressions of overall entropy of IFSs and PFSs are constructed by special functions. Then, it is shown that three existing overall entropy formulas can be constructed by three particular functions, and their rationality is proved. Finally, the effectiveness and feasibility of the proposed method and overall entropy are illustrated by an example and two comparative analyses.																	1432-7643	1433-7479															10.1007/s00500-020-05216-9		AUG 2020											
J								Focused random walk with probability distribution for SAT with long clauses	APPLIED INTELLIGENCE										Probability distribution; Satisfiability (SAT); Focused random walk (FRW); Stochastic local search (SLS)	LOCAL SEARCH; CONFIGURATION CHECKING; SCORING FUNCTIONS; ALGORITHM	Focused random walk (FRW) is one of the most influential paradigm of stochastic local search (SLS) algorithms for the propositional satisfiability (SAT) problem. Recently, an interestingprobability distribution(PD) strategy for variable selection was proposed and has been successfully used to improve SLS algorithms, resulting in state-of-the-art solvers. However, most solvers based on the PD strategy only usepolynomial function(PoF) to handle the exponential decay and are still unsatisfactory in dealing with medium and hugek-SAT instances at and near the phase transition. The present paper is focused on handling allk-SAT instances with long clauses. Firstly, an extensive empirical study of one state-of-the-art FRW solver WalkSATlm on a wide range of SAT problems is presented with the focus given on fitting the distribution of thebreakvalue of variable selected in each step, which turns out to be a Boltzmann function. Using theses case studies as a basis, we propose apseudo normal function(PNF) to fit the distribution of thebreakvalue of variable selected, which is actually a variation of the Boltzmann function. In addition, a newtie-breaking flipping(TBF) strategy is proposed to prevent the same variable from being flipped in consecutive steps. The PNF based PD strategy combined with the TBF strategy lead to a new variable selection heuristic named PNF-TBF. The PNF-TBF heuristic along with avariable allocation value(Vav) function are used to significantly improve ProbSAT, a state-of-the-art SLS solver, leads to a new FRW algorithm dubbed PNFSat, which achieves the state-of-the-art performance on a broad range of huge random 7-SAT instance near the phase transition as demonstrated via the extensive experimental studies. Some further improved versions on top of PNFSat are presented respectively, including PNFSat_alt, which achieves the state-of-the-art performance on the medium 7-SAT instances at the phase transition; PN&PoFSat, which achieves the state-of-the-art performance on a broad range of random 5-SAT benchmarks; as well as an integrated version of these three algorithms, named PDSat, which achieves the state-of-the-art performances on all huge and medium randomk-SAT instances with long clauses as demonstrated via the comparative studies using different benchmarks.																	0924-669X	1573-7497															10.1007/s10489-020-01768-3		AUG 2020											
J								Residual attention network using multi-channel dense connections for image super-resolution	APPLIED INTELLIGENCE										Image super-resolution; Deep convolutional neural networks; Multi-channel dense connections; Residual attention		In recent years, the methods based on deep convolutional neural networks (DCNN) have greatly promoted the development of image super-resolution (SR). However, deeper and wider SR networks are more difficult to train. For the SR task, the low-frequency information contained in low-resolution images is very important, and the neglect of exploring the feature information across channels hinders the representational capability of DCNN. To address these problems, we enhance the representational capability of DCNN by utilizing the interactive relationship among multiple channels. In this paper, a residual attention network using multi-channel dense connections (MCRAN) is proposed to improve the image super-resolution significantly. This method can make full use of multi-channel information for more effective feature expression and learning. In MCRAN, a multi-channel residual attention (MCRA) module is designed to coalesce the features of multiple different channels and the attention mechanism is applied to adjust the channel features adaptively. Accordingly, the channel features own more discriminative representation. In addition, the multi-source residual group (MSRG) structure is developed to construct a deeper network and simplify the training of network, which contains several long non-local skip connections (L-NLSC) to capture global low-frequency information in remote space. Besides, MSRG contains some short local-source skip connections (S-LSSC) to enhance the information interaction of local network. Extensive experimental evaluation on benchmark datasets on single image super-resolution proves the superiority of the proposed MCRAN.																	0924-669X	1573-7497															10.1007/s10489-020-01723-2		AUG 2020											
J								Implementation analysis of pixel-level image processing based on multiscale transforms	COMPUTATIONAL INTELLIGENCE										contourlet transform; curvelet transform; shearlet transform; wavelet transform	COMPLEX WAVELET TRANSFORM; SHEARLET TRANSFORM; CURVELET TRANSFORM; FUSION; CT; MR; SCHEME; MODEL	Image processing covers a wide range of processing techniques. Image Fusion is one of those technique which plays a vital role with medical images since different imaging methods provide different set of clinical information for diagnosis. Advances in technology provide us with plenty of imaging modalities. Image fusion is essential for a joint analysis of these multimodality images since each of these modalities provide unique and complementary characterization of the underlying anatomy and tissue microstructure. This paper analyzes the image fusion methods based on multiscale transforms and implements using wavelet, contourlet, curvelet, and shearlet transform. The results are compared.																	0824-7935	1467-8640															10.1111/coin.12384		AUG 2020											
J								A robust and accurate indoor localization system using deep auto-encoder combined with multi-feature fusion	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Indoor localization; Deep learning; Dimension reduction; Multi-feature fusion; Deep auto-encoder; Naive bayer classifier	SIGNALS	Many existing indoor localization systems use RSS as fingerprints, but RSS is a coarse-grained data, which not only fluctuates over time but also is not unique for a specific location due to rich multi-path effects and shadow fading in indoor environments. In order to further improve the localization accuracy, a robust and accurate indoor localization algorithm based on deep auto-encoder network combine with multi-feature fusion. To extract deep features hidden in CSI data and reduce the computational complexity of localization, an improved method is designed to achieve dimension reduction and feature extraction, which can avoid explicit information extraction of available CSI characteristics on the basis of effective representation of CSI fingerprint difference between different locations. Then, the fingerprint database is constructed by combination of RSS and CSI coding. Moreover, the Naive Bayer Classifier is adopted to improve the localization accuracy and stability. In the simulation experiment, the positioning effect of the proposed algorithm under different fingerprint libraries and the positioning effect under different positioning methods are mainly carried out. Experimental results show that the proposed method can effectively perform indoor positioning and has good practicability in the actual production environment.																	1868-5137	1868-5145															10.1007/s12652-020-02438-5		AUG 2020											
J								An integrated inbound logistics mode with intelligent scheduling of milk-run collection, drop and pull delivery and LNG vehicles	JOURNAL OF INTELLIGENT MANUFACTURING										Inbound logistics; Intelligent scheduling; Milk-run; Drop and Pull delivery; LNG vehicle	ALGORITHM; DECISION; PARTS	Although it provides a feasible inbound logistics solution for steady production and low inventory management, the Milk-run mode inevitably leads to a high transportation costs due to the features of small-batch and high-frequency delivery. In order to break through the defections of the existing inbound logistics mode, an integrated inbound logistics (IIL) mode with low-carbon and high efficiency is established. An intelligent scheduling method combines Milk-run collection with drop and pull delivery together. Moreover, the LNG vehicles are simultaneously used in the whole process. With AJ company's auto-parts inbound logistics as a case, the IIL mode is formulated with a mixed integer mathematical model. The genetic algorithm coded with Matlab is used to find the optimal solution. The results show that when compared with the original Milk-run mode, the IIL mode brings massive reductions in driving mileage, wait time and waste gas emission. It can make significant benefits in both economic and social sense. Therefore, it is entirely reasonable for management of industries to believe that the IIL mode will be a feasible and promising alternative for inbound logistics.																	0956-5515	1572-8145															10.1007/s10845-020-01637-3		AUG 2020											
J								Macroscopic numerical model of reinforced concrete shear walls based on material properties	JOURNAL OF INTELLIGENT MANUFACTURING										Reinforced concrete shear wall; Out-of-plane bending; Axial compression ratio; Shearing model; Collapse simulation	DESIGN	A macroscopic model evaluating shear behavior is necessary to simulate the failure process of shear walls under different axial compression ratios and different out-of-plane bending moments. Accordingly, based on the results of experiments and numerical simulations, a three-segment skeleton curve model was established to reflect the relationship between shear force and deformation when a reinforced concrete shear wall is subjected to axial and horizontal force. Corresponding hysteresis rules were proposed to obtain a macroscopic hysteresis model of the shear wall. Comparison between numerical and experimental results showed that the peak displacement, ductility, and hysteresis characteristics determined using the macroscopic model matched the experimental results well. The numerical results of the macroscopic shear model showed that the in-plane shear performance of a shear wall is almost unchanged when the out-of-plane displacement is very small, but if the displacement along the thickness direction increases, the in-plane shear bearing capacity and the deformation ability of a shear wall will notably decrease. The results can be used for structural design or collapse simulation.																	0956-5515	1572-8145															10.1007/s10845-020-01620-y		AUG 2020											
J								Brain Tumor Segmentation Using Deep Learning and Fuzzy K-Means Clustering for Magnetic Resonance Images	NEURAL PROCESSING LETTERS										Fuzzy K-means; Image denoising; Neural networks; Segmentation; Wiener filter	CLASSIFICATION; ALGORITHM; FUSION	The primary objective of this paper is to develop a methodology for brain tumor segmentation. Nowadays, brain tumor recognition and fragmentation is one among the pivotal procedure in surgical and medication planning arrangements. It is difficult to segment the tumor area from MRI images due to inaccessibility of edge and appropriately visible boundaries. In this paper, a combination of Artificial Neural Network and Fuzzy K-means algorithm has been presented to segment the tumor locale. It contains four phases, (1) Noise evacuation (2) Attribute extraction and selection (3) Classification and (4) Segmentation. Initially, the procured image is denoised utilizing wiener filter, and then the significant GLCM attributes are extricated from the images. Then Deep Learning based classification has been performed to classify the abnormal images from the normal images. Finally, it is processed through the Fuzzy K-Means algorithm to segment the tumor region separately. This proposed segmentation approach has been verified on BRATS dataset and produces the accuracy of 94%, sensitivity of 98% specificity of 99%, Jaccard index of 96%. The overall accuracy of this proposed technique has been improved by 8% when compared with K-Nearest Neighbor methodology.																	1370-4621	1573-773X															10.1007/s11063-020-10326-4		AUG 2020											
J								Algorithm for determining the mutual impact of nodes in weighted directed graphs	SOFT COMPUTING										Complex system; Weighted directed graph; Influence matrix; Accumulative impact; Rank distribution of nodes	COGNITIVE MAPS; CENTRALITY; SYSTEMS	We propose an algorithm for computing the influence matrix and rank distribution of nodes of a weighted directed graph by calculating the nodes' mutual impact. The algorithm of accumulative impact solves problems of dimension and computational complexity arising in the analysis of large complex systems. The algorithm calculates the mutual impact of each pair of vertices, making it possible to rank the nodes according to their importance within the system and to determine the most influential components. It produces results similar to those of the commonly used impulse method when applied to graphs that are impulse-stable in an impulse process, while overcoming the disadvantages of the impulse method in other situations. Results are always obtained regardless of impulse stability; they do not depend on the initial impulse, so that the initial values of the weights affect the calculation results. When elements in the adjacency matrix of the weighted directed graph are multiplied by a constant factor, scale invariance is not violated, and the full affect for each of the nodes scales proportionally. Several examples of analyses of weighted directed graphs, including one related to the practical problem of urban solid waste removal, are provided to demonstrate the advantages of the proposed algorithm.																	1432-7643	1433-7479															10.1007/s00500-020-05232-9		AUG 2020											
J								Reassessments of gross domestic product model for fractional derivatives with non-singular and singular kernels	SOFT COMPUTING										Gross domestic product; Atangana-Baleanu fractional derivative; Caputo-Fabrizio fractional derivative; Laplace transform; Modeling problem	DIFFERENTIAL-EQUATIONS	We study gross domestic product (GDP) model utilizing Atangana-Baleanu, Caputo-Fabrizio and Caputo fractional derivatives under the light of real data of the United Kingdom given by World Bank (World development indicators, 2018) between years 1972-2007. We obtain analytical solutions of fractional models by using Laplace transform. We compare the GDP results obtained for different fractional derivatives with real data by simulations and tables with statistical analysis showing the efficiency of fractional models to the integer-order counterpart employing error sum of squares and residual sum of squares.																	1432-7643	1433-7479															10.1007/s00500-020-05237-4		AUG 2020											
J								Multivariate Hidden Markov Models for disease progression	STATISTICAL ANALYSIS AND DATA MINING										copulas; disease progression; hidden Markov models; multivariate data	MAXIMUM-LIKELIHOOD ESTIMATION; PACKAGE	Disease progression models are a powerful tool for understanding the development of a disease, given some clinical measurements obtained from longitudinal events related to a sample of patients. These models are able to give some insights about the disease progression through the analysis of patients histories and can be also used to predict the future course of the disease for an individual. In particular, Hidden Markov Models are suitable for disease progression since they model the latent unobservable states of the disease. In this work, we propose a HMM where the outcome is multivariate and its components are not independent; to accomplish our aim, since we do not make any usual normality assumptions, we model the outcome using copulas. We first test the performance of our model in a simulation setting and show the validity of the method. Then, we study the course of Heart Failure, applying our model to an administrative dataset from Lombardia Region in Italy, showing how episodes of hospitalization can give information about the disease status of a patient.																	1932-1864	1932-1872				OCT	2020	13	5					499	507		10.1002/sam.11479		AUG 2020											
J								The formation and use of hierarchical cognitive maps in the brain: A neural network model	NETWORK-COMPUTATION IN NEURAL SYSTEMS										Neural Network models; motor control; model-based behaviour; neural development; self-organization; population coding	PREFRONTAL CORTEX; ORBITOFRONTAL CORTEX; DECISION-MAKING; ENCODE; NOISE; TASK; PATH; ABSTRACTION; COMPUTATION; BEHAVIOR	Many researchers have tried to model how environmental knowledge is learned by the brain and used in the form of cognitive maps. However, previous work was limited in various important ways: there was little consensus on how these cognitive maps were formed and represented, the planning mechanism was inherently limited to performing relatively simple tasks, and there was little consideration of how these mechanisms would scale up. This paper makes several significant advances. Firstly, the planning mechanism used by the majority of previous work propagates a decaying signal through the network to create a gradient that points towards the goal. However, this decaying signal limited the scale and complexity of tasks that can be solved in this manner. Here we propose several ways in which a network can can self-organize a novel planning mechanism that does not require decaying activity. We also extend this model with a hierarchical planning mechanism: a layer of cells that identify frequently-used sequences of actions and reuse them to significantly increase the efficiency of planning. We speculate that our results may explain the apparent ability of humans and animals to perform model-based planning on both small and large scales without a noticeable loss of efficiency.																	0954-898X	1361-6536															10.1080/0954898X.2020.1798531		AUG 2020											
J								IoT based home monitoring system with secure data storage by Keccak-Chaotic sequence in cloud server	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Video data encryption; IoT; Camera based sensing; Chaotic sequence	PROTOCOL; INTERNET; THREATS; DESIGN	Internet of things (IoT) has become an integral part of today's technological revolution, which enhances the people's quality of life. The IoT paradigm makes the world smarter and is employed in numerous real-time applications ranging from healthcare to vehicular networks. Surveillance systems are yet another important application of IoT and this work presents an IoT based home monitoring system, which deploys two ESP32 cameras for video sensing. The sensed data is not advisable to get stored as such, as the intruders may predict the usual events or frequent visitors to home. In order to ensure privacy, the cloud data storage must be secured and the purpose of this article is to ensure secure data storage, such that the data leakage can be prevented and the consistency is maintained. The data to be stored is secured by enforcing strict encryption scheme with keccak and chaotic sequence by considering the frames. The encrypted data is then stored in the cloud server. The performance of the proposed work is evaluated with respect to several performance measures and the work is proven to be efficient.																	1868-5137	1868-5145															10.1007/s12652-020-02424-x		AUG 2020											
J								Enhancing recommendation accuracy of item-based collaborative filtering using Bhattacharyya coefficient and most similar item	APPLIED INTELLIGENCE										Collaborative filtering; Similarity metrics; Prediction approaches; Sparsity	ALLEVIATE	The item-based collaborative filtering technique recommends an item to the user from the rating ofk-nearest items. Generally, a random value ofkis considered to find nearest neighbor from item-item similarity matrix. However, consideration of a random value forkintuitively is not a rational approach, as different items may have different value ofknearest neighbor. Sparsity in the data set is another challenge in collaborative filtering, as number of co-rated items' may be few or zero. Due to the above two reasons, collaborative filtering provides inaccurate recommendations, because the predicted rating may tend towards the Mean. The objective of the proposed work is to improve the accuracy by mitigating the above issues. Instead of using a random value ofk, we use the most similar neighbor for each target item so as to predict the target item, since findingkfor different target item is computationally expensive. Bhattacharyya Coefficient is used as a similarity measure to handle sparsity in the dataset. The performance of the proposed algorithm is tested the datasets of MovieLens and Film Trust, and experimental results reveal better prediction accuracy than the best of the prevalent prediction approaches exist in literature.																	0924-669X	1573-7497															10.1007/s10489-020-01775-4		AUG 2020											
J								Interval valuedm-polar fuzzy planar graph and its application	ARTIFICIAL INTELLIGENCE REVIEW										IVmPF graph; Strong edges of IVmPF graph; IVmPF planar graph; IVmPF faces; IVmPF dual graph	REPRESENTATION	In this article, a new idea of interval-valuedm-polar fuzzy (IVmPF) graph is introduced and investigated some of it's properties. Here, IVmPF multiset, interval-valuedm-polar fuzzy (IVmPF) multi graph are presented. IVmPF planar value of an interval-valuedm-polar fuzzy (IVmPF) planar graph along with degree of planarity value is also introduced to measure the planarity value of an interval-valuedm-polar fuzzy (IVmPF) graph. Some related terms like complete IVmPF graph, strong IVmPF graph, strong edges, faces of IVmPF planar graph are presented. In this paper, IVmPF dual graph is also described which is closely related to IVmPF planar graph. Some important properties are studied on IVmPF dual graph. Lastly, a real life application on IVmPF planar graph has been discussed to show its practicability.																	0269-2821	1573-7462															10.1007/s10462-020-09879-6		AUG 2020											
J								Smart contract for distributed energy trading in virtual power plants based on blockchain	COMPUTATIONAL INTELLIGENCE										blockchain; distributed energy resource; energy trading; smart contract; virtual power plant		The energy system is evolving from smart grid to energy Internet. Virtual power plant (VPP), as an important part of the energy Internet, plays an important role in the distributed energy generation and trading. In this article, a blockchain-based VPP transaction model is established for the future energy Internet driven by real-time electricity price. Then the smart contracts for distributed energy trading in VPPs using blockchain technology are proposed, and the key technological difficulties are analyzed and the solutions are given. Experiments show that the proposed model can reflect the supply and demand information in real time, so that two-way selection can be carried out under the condition of information symmetry when distributed energy is connected to the grid. If our method is applied, we can help distributed energy suppliers set electricity prices, reduce the trust cost and improve the energy trading efficiency. Also, we can help the distributed energy voluntarily participate in VPPs and joint maintain the system, then solve the problem of VPP's coordinated control and scheduling of distributed energy resources.																	0824-7935	1467-8640															10.1111/coin.12388		AUG 2020											
J								High-speed gaze detection using a single FPGA for driver assistance systems	JOURNAL OF REAL-TIME IMAGE PROCESSING										Gaze detection; ADAS; ESVIR; FPGA; FC neural networks	TRACKING; EYE	The performance of driver gaze detection by video-based eye-tracking often encounters problems in lowcomputing speed, high-power consumption, and installation space constraints inside the vehicle. In this paper, we present an eye-tracking system that uses a single field-programmable-gate-array chip to overcome the aforementioned problems. In the detection system, the image quality is 640x480 pixels with an 80 fps frame rate. Eye feature extraction is conducted using the enhanced semantics-based vague image representation approach. A succinct fully-connected neural network is then employed to classify various directions of sightline. Our experimental results exhibited a noticeable recognition speed at 0.52 mu s using a 100 MHz system clock and had an average detection rate of 92%.																	1861-8200	1861-8219															10.1007/s11554-020-01004-8		AUG 2020											
J								Predicting rice phenotypes with meta and multi-target learning	MACHINE LEARNING										Rice; Bioinformatics; Machine learning; Meta-learning; Multi-target learning	GENOMIC SELECTION; REGRESSION; KERNEL; GRADIENT; MODEL	The features in some machine learning datasets can naturally be divided into groups. This is the case with genomic data, where features can be grouped by chromosome. In many applications it is common for these groupings to be ignored, as interactions may exist between features belonging to different groups. However, including a group that does not influence a response introduces noise when fitting a model, leading to suboptimal predictive accuracy. Here we present two general frameworks for the generation and combination of meta-features when feature groupings are present. Furthermore, we make comparisons to multi-target learning, given that one is typically interested in predicting multiple phenotypes. We evaluated the frameworks and multi-target learning approaches on a genomic rice dataset where the regression task is to predict plant phenotype. Our results demonstrate that there are use cases for both the meta and multi-target approaches, given that overall, they significantly outperform the base case.																	0885-6125	1573-0565															10.1007/s10994-020-05881-9		AUG 2020											
J								Pore type identification in carbonate rocks using convolutional neural network based on acoustic logging data	NEURAL COMPUTING & APPLICATIONS										Pore type; CNN; Carbonate rocks; Acoustic properties; Time-frequency characteristic	OIL-FIELD; CLASSIFICATION; DECOMPOSITION; PERMEABILITY; PREDICTION; RESERVOIR; VELOCITY; EVENT; MEDIA; MODEL	Existing methods of well logging interpretation often contain uncertainties in the exploration and evaluation of carbonate reservoirs due to the complex pore types. Based on the time-frequency analysis of array acoustic logging data, the identification of pore types based on a convolutional neural network (CNN) was established. The continuous wavelet transform was first used to transform the 1-D acoustic wave data into 2-D time-frequency spectra as the input data of the CNN based on the pore aspect ratios. According to the acoustic logging data obtained from numerical simulations, a three-type (vug, interparticle-pores, and crack) prediction was established to validate the identification method. The noise-sensitivity analyses demonstrate that our method is stable for noise mixed signals. The CNN-based identification method was used to analyze the field acoustic logging data in a carbonate reservoir. According to the description of the pore structures from the core analysis, the formation pores were divided into two types (cracks and interparticle-pores). The accuracy of the method using field acoustic logging data can reach 90%. This work provides promising means for pore type identification from complex acoustic logging data by applying deep learning technologies, which can be easily extended into other similar neighboring carbonate reservoirs.																	0941-0643	1433-3058															10.1007/s00521-020-05246-2		AUG 2020											
J								SWAMPLAB	AI & SOCIETY										Arts; Sciences and technologies; Interconnection; Messiness; Change; Engagement; Social interactions; Humans and non-humans		'SWAMPLAB' is a strong case for intuitive insights through arts, sciences, and technologies to engage the self and establish meaningful social interactions including humans and non-humans. While zigzagging through processes of privatization, globalization, ecological, economic, social and political challenges, the power of such residencies or labs stems from the interplay with the local context and its habitants, in this case, nature reserve De Zegge, a 111 hectares swamp in the Northern part of Belgium. Mediation and participation are a core condition for the design of knowledge and engagement since without informed consent and participants understanding the nature of the process, no change is possible. The more individuals express and explore their knowledge through social encounters, narratives, and dialogs, the more crucial tacit knowledge can be shared. Making things disputable is another responsibility. In society at large, there are very few examples where people feel interconnected and yet at the same time can disagree. The notion of Swamplab is rather a call for wonderment, for not using each other as a stepping stone but setting up open dialogs, listening and reflecting together, playing around, tinkering, since messiness is allowed. Including non-humans, be they wolves, moles or avatars, and virtual agents, is key to cracking the individual resistance to change understandings, opinions, and behavior. The result may lead to deeper engagement and Buckminster Fuller's "comprehensive anticipatory design science" or what Bruno Latour calls a "progressive composition of the good common world".																	0951-5666	1435-5655															10.1007/s00146-020-01030-x		AUG 2020											
J								Auto-adaptive multi-scale Laplacian Pyramids for modeling non-uniform data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Laplacian Pyramids; Kernel methods; Overfitting; Multi-scale interpolation; Non-uniform data; Adaptive stopping	REGRESSION	Kernel-based techniques have become a common way for describing the local and global relationships of data samples that are generated in real-world processes. In this research, we focus on a multi-scale kernel based technique named Auto-adaptive Laplacian Pyramids (ALP). This method can be useful for function approximation and interpolation. ALP is an extension of the standard Laplacian Pyramids model that incorporates a modified Leave-One-Out Cross Validation procedure, which makes the method stable and automatic in terms of parameters selection without extra cost. This paper introduces a new algorithm that extends ALP to fit datasets that are non-uniformly distributed. In particular, the optimal stopping criterion will be point-dependent with respect to the local noise level and the sample rate. Experimental results over real datasets highlight the advantages of the proposed multi-scale technique for modeling and learning complex, high dimensional data.																	0952-1976	1873-6769				AUG	2020	93								103682	10.1016/j.engappai.2020.103682													
J								The PRISMA Hand I: A novel underactuated design and EMG/voice-based multimodal control	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE											PROSTHETIC HAND	The novel underactuated design of the PRISMA HAND I and its multimodal control based on integrated electromyographic signals and voice commands is presented. Due to limitations of EMG sensors and residual physiological signals of amputees, a smart solution utilizing a voice recognition module has been integrated into the EMG control to realize advanced intuitive commands and easy connection with the arm. Inspired by neuroscience studies on the human hand, two motor synergies have been implemented. The thumb adduction/abduction independent motion allows in-hand manipulation. The transmission system is based on whiffletree and pulleys for motion differentiation and on antagonistic elastic tendons. Using EMG and voice commands, the motors are moved together in a certain number of combination suitably and easily programmed using Arduino electronic prototyping. The use of a current sensor allows controlling the strength of the grasp to handle also fragile objects. The contributions of this work are: the bio-inspired design of kinematics and motion couplings by means of two motor synergies, the simple and efficient control interface allowing easy connection to the arm and intuitive use, finally, the cost reduction using economic hardware and mechanical components while preserving performance.																	0952-1976	1873-6769				AUG	2020	93								103698	10.1016/j.engappai.2020.103698													
J								Addressing Unequal Area Facility Layout Problems with the Coral Reef Optimization algorithm with Substrate Layers	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Unequal Area Facility Layout Problem; Coral Reefs Optimization; Facility Layout; Meta-heuristics; Bio-inspired algorithms	BAY STRUCTURE REPRESENTATION; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; FEATURE-SELECTION; DESIGN-PROBLEMS; SLICING TREE; TABU SEARCH; MODEL; SIMULATION; SPACE	The Unequal Area Facility Layout Problem (UA-FLP) is a relevant task in industrial manufacturing, in which the disposition of a number of facilities (or departments) in a manufacturing system must be obtained, under several optimization criteria and different constraints. The UA-FLP is a hard optimization problem, in which traditional optimization techniques do not obtain good results. Thus, it has been successfully tackled with different heuristics and meta-heuristics in the last years. In this work we address the UA-FLP with a multi-method ensemble approach, the Coral Reefs Optimization algorithm with Substrate Layers (CRO-SL). It is a novel multi-method evolutionary algorithm that encourages the evolution of several searching procedures at the same time over a single population. The CRO-SL has been previously applied to very difficult optimization problems, obtaining excellent performance. In this case, we adapt the CRO-SL to the UA-FLP, by means of increasing the diversity generation within the algorithm, which is helpful to improve the exploration of the searching space, avoiding to fall into local minima. Specifically, we propose to include several reproduction mechanisms (adapted to the UA-FLP) within each substrate of the algorithm, which will highly increase the diversity generation in the CRO-SL. An exhaustive experimental study of the CRO-SL performance in a large number of UA-FLP instances is carried out, including a comparison with the state-of-the-art algorithms for this problem. We will show the ability of the CRO-SL to reach or surpass the best-known solutions in most of the tested UA-FLP cases.																	0952-1976	1873-6769				AUG	2020	93								103697	10.1016/j.engappai.2020.103697													
J								Structural hole-based approach to control public opinion in a social network	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Structural holes; Opinion dynamics; Opinion evolution; Social network; Opinion control	DYNAMICS; PROPAGATION; MODELS; TRUST	Structural hole spanners play an important role in information diffusion. Compared with opinion leaders, structural hole spanners have better locations in social networks to expand the scope of information diffusion. In the past, researchers focused on evolution rules and opinion dynamics environments to monitor and even manage public opinion. In this study, we propose a novel structural-hole-based approach to control public opinion in social networks, hereinafter referred to as the SHCPO approach. We discuss the influence of both ordinary agents and structural hole spanners on opinion evolution using our improved Friedkin-Johnsen (FJ) model. Further, we analyze the evolution tendency of public opinion, which leads to the final consensus of public opinion, via the FJ model with ordinary agents in a community and structural hole spanners in joint communities. We reveal three kinds of connections between structural hole spanners and ordinary agents in joint communities. These comprise structural hole spanners connecting (1) two opinion leaders; (2) two ordinary agents; (3) one opinion leader and one ordinary agent. The three connections will lead to different opinion evolution conditions. According to the structural balance theory, we reconstruct the social network by changing the connections between structural hole spanners and agents in different communities. This guides the public opinion tendencies of joint communities towards the positive. Experimental results demonstrate beneficial effects of the SHCPO approach. We use three evaluation indicators to compare the SHCPO approach to five alternative methods. The percentage of positive opinions is used as an evaluation indicator. The SHCPO approach, compared with adding informed agents, add edges, the method from WWW and varying susceptibility to persuasion method, which guide the agent with a negative opinion towards positive opinion, has improved about 17%, 10%, 9%, 1%, respectively.																	0952-1976	1873-6769				AUG	2020	93								103690	10.1016/j.engappai.2020.103690													
J								A memory guided sine cosine algorithm for global optimization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Optimization; Population-based algorithms; Sine cosine algorithm; Exploration-exploitation	DIFFERENTIAL EVOLUTION; DESIGN	Real-world optimization problems demand an algorithm which properly explores the search space to find a good solution to the problem. The sine cosine algorithm (SCA) is a recently developed and efficient optimization algorithm, which performs searches using the trigonometric functions sine and cosine. These trigonometric functions help in exploring the search space to find an optimum. However, in some cases, SCA becomes trapped in a sub-optimal solution due to an inefficient balance between exploration and exploitation. Therefore, in the present work, a balanced and explorative search guidance is introduced in SCA for candidate solutions by proposing a novel algorithm called the memory guided sine cosine algorithm (MG-SCA). In MG-SCA, the number of guides is decreased with increase in the number of iterations to provide a sufficient balance between exploration and exploitation. The performance of the proposed MG-SCA is analysed on benchmark sets of classical test problems, IEEE CEC 2014 problems, and four well known engineering benchmark problems. The results on these applications demonstrate the competitive ability of the proposed algorithm as compared to other algorithms.																	0952-1976	1873-6769				AUG	2020	93								103718	10.1016/j.engappai.2020.103718													
J								A deep fusion model based on restricted Boltzmann machines for traffic accident duration prediction	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Accident management; Deep learning; Intelligent transportation systems; Temporal and spatial information; Traffic data fusion; Traffic prediction	INCIDENT CLEARANCE TIMES; NEURAL-NETWORK; INFLUENTIAL FACTORS; M5P TREE	Traffic accidents causing nonrecurrent congestion can decrease the capacity of highways and increase car emissions. Some models in previous studies have been built based on artificial intelligence or statistical theory because estimating the duration of an accident can aid traffic operation and management. However, only characteristics of traffic accidents were considered in most models; the spatial-temporal correlations of traffic flow were always ignored. In this study, a deep fusion model, which can simultaneously handle categorical and continuous variables, is proposed. The model considers not only the characteristics of traffic accidents but also the spatial-temporal correlations in traffic flow. In this model, a stacked restricted Boltzmann machine (RBM) is used to handle the categorical variables, a stacked Gaussian-Bernoulli RBM is used to handle the continuous variables, and a joint layer is used to fuse the extracted features. With extracted 1-80 data, the performance of the proposed model was evaluated and compared to some benchmark models. Furthermore, the target variable (duration) was divided into ten groups, and then the evaluation criteria of the models of each group were calculated. The results show that the novel model outperforms some previous models and that the fusion of different types of variables can improve prediction accuracy. In conclusion, the proposed model can fully mine nonlinear and complex patterns in traffic accident data and traffic flow data. The fusion of features is important to predict traffic accident durations.																	0952-1976	1873-6769				AUG	2020	93								103686	10.1016/j.engappai.2020.103686													
J								Affinity matrix with large eigenvalue gap for graph-based subspace clustering and semi-supervised classification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Affinity matrix; Subspace clustering; Semi-supervised classification; Low rank representation; Sparse representation	RECOGNITION; ALGORITHM; MOTION	In the graph-based learning method, the data graph or similarity matrix reveals the relationship between data, and reflects similar attributes within a class and differences between classes. Inspired by Davis-Kahan Theorem that the stability of matrix eigenvector space depends on its spectral distance (i.e. its eigenvalue gap), in this paper, we propose a global local affinity matrix model with low rank subspace sparse representation (GLAM-LRSR) based on global information of eigenvalue gap and local distance between samples. This method approximate the similarity matrix with ideally diagonal block structure from the perspective of maximizing the eigenvalue gap, and the local distance between data is utilized as a regular term to prevent the eigenvalue gap from being too large to ensure the efficacy of similarity matrix. We have shown that the combination of subspace (LRSR) partitioning method such as Sparse Subspace Clustering(SSC) and the similarity matrix constructed by GLAM can improve the accuracy of subspace clustering, and that the similarity matrix constructed by GLAM-LRSR can be successfully applied to graph-based semi-supervised classification task. Our experiments on synthetic data as well as the real-world datasets for face clustering, face recovery and motion segmentation have clearly demonstrate the significant advantages of GLAM-LRSR and its effectiveness.																	0952-1976	1873-6769				AUG	2020	93								103722	10.1016/j.engappai.2020.103722													
J								Enhanced fault diagnosis method using conditional Gaussian network for dynamic processes	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Bayesian networks; Conditional Gaussian networks; Dynamic process; Fault diagnosis	BAYESIAN NETWORK; CLASSIFIER; FRAMEWORK	Applying fault detection and diagnosis (FDD) technology to the process industry can help to detect faults in time and minimize their impact. The purpose of this study is to propose an enhanced fault diagnosis method under a Conditional Gaussian Network(CGN) efficient and suitable for dynamic processes fault monitoring. The key paths are as follows: first, a time series model is established for the process data and decomposed into time-dependent components and time-independent components; second, time-dependent components are discarded and time-independent components void of auto-correlation are considered instead of the original data to learn the CGN model. A numerical simulation case is used to illustrate the interest of our proposal. The effectiveness of the proposed method is further verified and compared on the Tennessee Eastman Process (TEP). The obtained results show that our method has high and better accuracies regarding the diagnosis of known and unknown faults in dynamic processes.																	0952-1976	1873-6769				AUG	2020	93								103704	10.1016/j.engappai.2020.103704													
J								Regenerative braking system modeling by fuzzy Q-Learning	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Regenerative braking system; Fuzzy systems; Batch reinforcement learning	PERFORMANCE; VALIDATION; STRATEGY	The regeneration factor, that expresses the ratio between the energy recovered to the battery during braking and the total braking energy, is difficult to be measured from independent instruments. In this paper, a reinforcement learning (RL) method is used to adjust and improve a fuzzy logic model for regenerative braking (FLmRB) for modeling Electric Vehicles' (EV) regenerative braking systems (RBSs). With the proposed approach, a specialist can infer the regeneration factor, by tuning the model for a specific EV using real data gathered from field tests, using as inputs, only variables measured from independent instruments, namely EV acceleration and jerk, and road inclination. The proposed approach was tested with real data sets of the Nissan Leaf EV. Twelve short-distance data sets in urban areas were collected to learn the regeneration factor, and two long-distance data sets in urban and sub-urban areas were used to validate the learned models. The results show that the learning method can successfully learn the regenerative braking factor improving the previously proposed FLmRB model approach which is based on manual design of the model.																	0952-1976	1873-6769				AUG	2020	93								103712	10.1016/j.engappai.2020.103712													
J								Selecting an airport ground access mode using novel fuzzy LBWA-WASPAS-H decision making model	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Ground access modes; Mode selection; Fuzzy sets; Multi-criteria decision making; Level Based Weight Assessment; WASPAS-H	CHOICE; COPRAS; NUMBERS	Airports are critical in ensuring a fast way of transporting people and goods. Choosing a reliable, fast and comfortable access mode to the airport is vital to ensure a seamless aviation system. The aim of this study is to select the best transport mode for Istanbul's newly constructed Istanbul Airport. One of the largest airports in the world with 150,000 passenger capacity per year, Istanbul Airport is located in the northern part of Istanbul, outside the city. However, the access to the new airport resulted in many controversies about the selection of the best mode. Underground metro, bus rapid transit (BRT), light rail transit (LRT) and premium bus services are put forward as alternative ground access modes. These alternatives are evaluated based on 4 main decision criteria including financial aspects, operating features, project characteristics and environmental sustainability, which are broken down into 14 sub-criteria. In this paper, the importance weights of the criteria are determined by novel fuzzy Level Based Weight Assessment (LBWA) which is capable of modelling human thinking. Afterwards, the traditional Weighted Aggregated Sum Product Assessment (WASPAS) method is enhanced by the integration of the fuzzy Weighted Heronian Mean (WHM) and fuzzy Weighted Geometric Heronian Mean (WGHM) functions. A hybrid fuzzy multi-criteria decision making method based on LBWA-WASPAS-H model is used to solve this ground access mode selection problem. The results show that an underground metro is the most optimal mode, followed by LRT, BRT, and premium bus services.																	0952-1976	1873-6769				AUG	2020	93								103703	10.1016/j.engappai.2020.103703													
J								Non-stationary Gaussian process regression applied in validation of vehicle dynamics models	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Validation; Vehicle; Confidence band; Gaussian process; Heteroscedastic; Non-stationary	CONFIDENCE BANDS	This work compares methods to compute confidence bands in a validation task of a vehicle single-track model. The confidence bands are computed from time series by naive method, Gaussian process regression and heteroscedastic and non-stationary Gaussian process regression. The simulation model considers the epistemic uncertainty of the vehicle mass parameter by Latin hypercube sampling. The validation procedure compares all stochastically simulated time series of the vehicle yaw rate with the confidence band of the reference data. The model is marked as valid if the yaw rate for each time step is within the confidence band of the reference data. The data was challenging due to noise and time-varying variance and smoothness. Due to required data pre-processing and the high sensitivity to noise in the reference data, the naive method has generated unusable confidence bands and cannot be recommended for similar validation tasks. Gaussian process regression solved the problem of noise sensitivity, but was not able to model the time-varying length scale of the reference data. Therefore, heteroscedastic and non-stationary Gaussian process regression is proposed to calculate accurate confidence bands of time-varying and noisy reference data for the validation of dynamic models by a confidence band approach.																	0952-1976	1873-6769				AUG	2020	93								103716	10.1016/j.engappai.2020.103716													
J								Non-parametric spatially constrained local prior for scene parsing on real-world data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Scene parsing; Image understanding; Image segmentation; Object classification; Artificial neural network	OBJECT RECOGNITION; IMAGE SEGMENTATION; FEATURES	Scene parsing aims to recognize the object category of every pixel in scene images, and it plays a central role in image content understanding and computer vision applications. However, accurate scene parsing from unconstrained real-world data is still a challenging task. In this paper, we present the non-parametric Spatially Constrained Local Prior (SCLP) for scene parsing on realistic data. For a given query image, the non-parametric SCLP is learnt by first retrieving a subset of most similar training images to the query image and then collecting prior information about object co-occurrence statistics between spatial image blocks and between adjacent superpixels from the retrieved subset. The SCLP is powerful in capturing both long- and short-range context about inter-object correlations in the query image and can be effectively integrated with traditional visual features to refine the classification results. Our experiments on the SIFT Flow and PASCAL-Context benchmark datasets show that the non-parametric SCLP used in conjunction with superpixel-level visual features achieves one of the top performance compared with state-of-the-art approaches.																	0952-1976	1873-6769				AUG	2020	93								103708	10.1016/j.engappai.2020.103708													
J								Survival analysis of failures based on Hawkes process with Weibull base intensity	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Survival analysis; Hawkes process; Conditional intensity function; Base intensity; Exponential distribution; Weibull distribution; Granger causality	RELIABILITY	In this paper, we construct a Hawkes process with time-varying base intensity to model the sequence of failure, i.e., failure events of the compressor station, and we combine survival analysis and point process model on various failure events of the compressor station based on Hawkes process. To our best knowledge, until now, nearly all relevant literature of the Hawkes point processes assumes that the base intensity of the conditional intensity function is time-invariant. This assumption is apparently too harsh to be verified. For example, in the practical application, including financial analysis, reliability analysis, survival analysis and social network analysis, the truth variation of the base intensity of the failure occurrence over time is not constant. The constant base intensity will not reflect the base intensity trend of the failure occurring over time. Thus, in order to solve this problem, in this paper, we propose a new time-varying base intensity, e.g. which is treated as obeying Weibull distribution. First, we introduce the base intensity into a Hawkes process that obeys the Weibull distribution, and then we propose an effective learning algorithm based on the maximum likelihood estimator. Experiments on the constant base intensity synthetic data, time-varying base intensity synthetic data, and real-world data show that our method can learn the triggering patterns of the Hawkes processes and the time-varying base intensity simultaneously and robustly. Experiments on real-world data also reveal the Granger causally of different types of failures and the base probability of failure varying over time. We put forward some suggestions for practical production based on the experimental results.																	0952-1976	1873-6769				AUG	2020	93								103709	10.1016/j.engappai.2020.103709													
J								G-Forest: An ensemble method for cost-sensitive feature selection in gene expression microarrays	ARTIFICIAL INTELLIGENCE IN MEDICINE										Feature selection; Cost-sensitive; Genetic algorithm; Random Forest; Microarray Gene expression; Silent diseases' diagnosis	CANCER CLASSIFICATION; ALGORITHM; MACHINE; HYBRID; FRAMEWORK	Microarray gene expression profiling has emerged as an efficient technique for cancer diagnosis, prognosis, and treatment. One of the major drawbacks of gene expression microarrays is the "curse of dimensionality", which hinders the usefulness of information in datasets and leads to computational instability. In recent years, feature selection techniques have emerged as effective tools to identify disease biomarkers to aid in medical screening and diagnosis. However, the existing feature selection techniques, first, do not suit the rare variance exists in genomic data; and second, do not consider the feature cost (i.e. gene cost). Because ignoring features' costs may result in high cost gene profiling, this study proposes a new algorithm, called G-Forest, for cost-sensitive feature selection in gene expression microarrays. G-Forest is an ensemble cost-sensitive feature selection algorithm that develops a population of biases for a Random Forest induction algorithm. The G-Forest embeds the feature cost in the feature selection process and allows for simultaneous selection of low-cost and most informative features. In particular, when constructing the initial population, the feature is randomly selected with a probability inversely proportional to its associated cost. The G-Forest was compared with multiple state-of-the-art algorithms. Experimental results showed the effectiveness and robustness of the G-Forest in selecting the least cost and most informative genes. The G-Forest improved accuracy up to 14 % and decreased costs up to 56 % - on average when compared with the other approaches tested in this article.																	0933-3657	1873-2860				AUG	2020	108								101941	10.1016/j.artmed.2020.101941													
J								Realistic hair simulator for skin lesion images: A novel benchemarking tool	ARTIFICIAL INTELLIGENCE IN MEDICINE										Melanoma; Generative adversarial networks; Synthetic; Computer-aided diagnosis; Dermatology	CLASSIFICATION; SEGMENTATION; MELANOMA	Automated skin lesion analysis is one of the trending fields that has gained attention among the dermatologists and health care practitioners. Skin lesion restoration is an essential pre-processing step for lesion enhancements for accurate automated analysis and diagnosis by both dermatologists and computer-aided diagnosis tools. Hair occlusion is one of the most popular artifacts in dermatoscopic images. It can negatively impact the skin lesions diagnosis by both dermatologists and automated computer diagnostic tools. Digital hair removal is a non-invasive method for image enhancement for decrease the hair-occlusion artifact in previously captured images. Several hair removal methods were proposed for skin delineation and removal without standardized benchmarking techniques. Manual annotation is one of the main challenges that hinder the validation of these proposed methods on a large number of images or against benchmarking datasets for comparison purposes. In the presented work, we propose a photo-realistic hair simulator based on context-aware image synthesis using image-to-image translation techniques via conditional adversarial generative networks for generation of different hair occlusions in skin images, along with ground-truth mask for hair location. Hair-occluded image is synthesized using the latent structure of any input hair-free image by deep encoding the input image into a latent vector of features. The locations of required hair are highlighted using white pixels on the input image. Then, these deep encoded features are used to reconstruct the synthetic highly realistic hair-occluded image. Besides, we explored using three loss functions including L 1 -norm, L 2 -norm and structural similarity index (SSIM) to maximize the image synthesis visual quality. For the evaluation of the generated samples, the t-SNE feature mapping and Bland-Altman test are used as visualization tools for the experimental results. The results show the superior performance of our proposed method compared to previous methods for hair synthesis with plausible colours and preserving the integrity of the lesion texture. The proposed method can be used to generate benchmarking datasets for comparing the performance of digital hair removal methods. The code is available online at https://github.com/attiamohammed/realhair.																	0933-3657	1873-2860				AUG	2020	108								101933	10.1016/j.artmed.2020.101933													
J								Identification of Alzheimer's disease based on wavelet transformation energy feature of the structural MRI image and NN classifier	ARTIFICIAL INTELLIGENCE IN MEDICINE										Alzheimer's disease; Image classification; Wavelet transformation; Feature extraction; Energy feature	PRINCIPAL COMPONENT ANALYSIS; ARTIFICIAL-INTELLIGENCE; FEATURE-SELECTION; DIAGNOSIS; MACHINE; AD; PREDICTION; PATTERNS; ATROPHY; FUSION	Alzheimer's disease (AD) is now difficult to be identified for clinicians, especially, at its prodromal stage, mild cognitive impairment (MCI), because of no obvious clinical symptom and few impacts on daily life at this phase. In addition, energy distribution differences of brain atrophies reflected in structural magnetic resonance imaging (sMRI) images between MCI patients and older healthy controls (HC) are minimal and subtle, which are difficult to be captured by the spatial analysis. In this study, we propose a novel method (namely AD-WTEF) to identify AD and MCI patients from HC subjects by extracting the wavelet transformation energy feature (WTEF) of the sMRI image. AD-WTEF firstly transforms each scan of the preprocessed sMRI image by wavelet to obtain its directional subbands with the same size at different transformation levels. And then, based on the anatomical automatic labeling (AAL) atlas, AD-WTEF constructs a new brain mask to segment the subbands at the same direction and transformation level into different energy regions of interest (EROIs). Thirdly, by averaging coefficients in an EROI, AD-WTEF gets an energy feature, following that energy features of different EROIs are connected to form an energy feature vector for describing the subbands at the same direction and transformation level. As a result, these energy feature vectors are further concatenated to be a WTEF of the sMRI image. Finally, the nearest neighbor (NN) classifier is selected and used for AD identification. Compared with other seven stateof-the-art methods, our AD-WTEF can effectively identify AD patients using the subtle energy distribution differences of sMRI images. Furthermore, experimental results indicate that our AD-WTEF can also find important brain ROIs related to AD.																	0933-3657	1873-2860				AUG	2020	108								101940	10.1016/j.artmed.2020.101940													
J								Handling imbalanced medical image data: A deep-learning-based one-class classification approach	ARTIFICIAL INTELLIGENCE IN MEDICINE										Medical image classification; Data imbalance; Deep learning; Image complexity	NOVELTY DETECTION; TUMOR-DETECTION	In clinical settings, a lot of medical image datasets suffer from the imbalance problem which hampers the detection of outliers (rare health care events), as most classification methods assume an equal occurrence of classes. In this way, identifying outliers in imbalanced datasets has become a crucial issue. To help address this challenge, one-class classification, which focuses on learning a model using samples from only a single given class, has attracted increasing attention. Previous one-class modeling usually uses feature mapping or feature fitting to enforce the feature learning process. However, these methods are limited for medical images which usually have complex features. In this paper, a novel method is proposed to enable deep learning models to optimally learn single-class-relevant inherent imaging features by leveraging the concept of imaging complexity. We investigate and compare the effects of simple but effective perturbing operations applied to images to capture imaging complexity and to enhance feature learning. Extensive experiments are performed on four clinical datasets to show that the proposed method outperforms four state-of-the-art methods.																	0933-3657	1873-2860				AUG	2020	108								101935	10.1016/j.artmed.2020.101935													
J								The natural language explanation algorithms for the lung cancer computer-aided diagnosis system	ARTIFICIAL INTELLIGENCE IN MEDICINE										Visual explanations; LIME; Lung cancer; Computer-aided diagnosis; Natural language; Classification; Explainable AI; Causability	NODULE DETECTION; BLACK-BOX; CT IMAGES; RADIOMICS	Two algorithms for explaining decisions of a lung cancer computer-aided diagnosis system are proposed. Their main peculiarity is that they produce explanations of diseases in the form of special sentences via natural language. The algorithms consist of two parts. The first part is a standard local post-hoc explanation model, for example, the well-known LIME, which is used for selecting important features from a special feature representation of the segmented lung suspicious objects. This part is identical for both algorithms. The second part is a model which aims to connect selected important features and to transform them to explanation sentences in natural language. This part is implemented differently for both algorithms. The training phase of the first algorithm uses a special vocabulary of simple phrases which produce sentences and their embeddings. The second algorithm significantly simplifies some parts of the first algorithm and reduces the explanation problem to a set of simple classifiers. The basic idea behind the improvement is to represent every simple phrase from vocabulary as a class of the "sparse" histograms. An implementation of the second algorithm is shown in detail.																	0933-3657	1873-2860				AUG	2020	108								101952	10.1016/j.artmed.2020.101952													
J								Automated ICD-10 code assignment of nonstandard diagnoses via a two-stage framework	ARTIFICIAL INTELLIGENCE IN MEDICINE										Automated ICD-10 code assignment; Two-stage framework; Text-matching	CLASSIFICATION	An electronic medical record (EMR) is a rich source of clinical information for medical studies. Each physician usually has his or her own way to describe a patient's diagnosis. This results in many different ways to describe the same disease, which produces a large number of informal nonstandard diagnoses in EMRs. The Tenth Revision of International Classification of Diseases (ICD-10) is a medical classification list of codes for diagnoses. Automated ICD-10 code assignment of the nonstandard diagnosis is an important way to improve the quality of the medical study. However, manual coding is expensive, time-consuming and inefficient. Moreover, terminology in the standard diagnostic library comprises approximately 23,000 subcategory (6-digit) codes. Classifying the entire set of subcategory codes is extremely challenging. ICD-10 codes in the standard diagnostic library are organized hierarchically, and each category code (3-digit) relates to several or dozens of subcategory (6-digit) codes. Based on the hierarchical structure of the ICD-10 code, we propose a two-stage ICD-10 code assignment framework, which examines the entire category codes (approximately 1900) and searches the subcategory codes under the specific category code. Furthermore, since medical coding datasets are plagued with a training data sparsity issue, we introduce more supervised information to overcome this issue. Compared with the method that searches within approximately 23,000 subcategory codes, our approach requires examination of a considerably reduced number of codes. Extensive experiments show that our framework can improve the performance of the automated code assignment.																	0933-3657	1873-2860				AUG	2020	108								101939	10.1016/j.artmed.2020.101939													
J								EBM plus : Advancing Evidence-Based Medicine via two level automatic identification of Populations, Interventions, Outcomes in medical literature	ARTIFICIAL INTELLIGENCE IN MEDICINE										Evidence Based Medicine; PICO; Machine learning; Neural networks; Natural Language Processing	SYSTEM; SPIDER; PICO	Evidence-Based Medicine (EBM) has been an important practice for medical practitioners. However, as the number of medical publications increases dramatically, it is becoming extremely difficult for medical experts to review all the contents available and make an informative treatment plan for their patients. A variety of frameworks, including the PICO framework which is named after its elements (Population, Intervention, Comparison, Outcome), have been developed to enable fine-grained searches, as the first step to faster decision making. In this work, we propose a novel entity recognition system that identifies PICO entities within medical publications and achieves state-of-the-art performance in the task. This is achieved by the combination of four 2D Convolutional Neural Networks (CNNs) for character feature extraction, and a Highway Residual connection to facilitate deep Neural Network architectures. We further introduce a PICO Statement classifier, that identifies sentences that not only contain all PICO entities but also answer questions stated in PICO. To facilitate this task we also introduce a high quality dataset, manually annotated by medical practitioners. With the combination of our proposed PICO Entity Recognizer and PICO Statement classifier we aim to advance EBM and enable its faster and more accurate practice.																	0933-3657	1873-2860				AUG	2020	108								101949	10.1016/j.artmed.2020.101949													
J								Indoor location identification of patients for directing virtual care: An AI approach using machine learning and knowledge-based methods	ARTIFICIAL INTELLIGENCE IN MEDICINE										Virtual care; Ambient sensors; Indoor localization; Machine learning; Semantic web; eHealth platform; Data fusion; Self-management; Ambient assisted living; Activities of daily living	ALGORITHMS; ADHERENCE; MODELS; SYSTEM	In a digitally enabled healthcare setting, we posit that an individual's current location is pivotal for supporting many virtual care services-such as tailoring educational content towards an individual's current location, and, hence, current stage in an acute care process; improving activity recognition for supporting self-management in a home-based setting; and guiding individuals with cognitive decline through daily activities in their home. However, unobtrusively estimating an individual's indoor location in real-world care settings is still a challenging problem. Moreover, the needs of location-specific care interventions go beyond absolute coordinates and require the individual's discrete semantic location; i.e., it is the concrete type of an individual's location (e.g., exam vs. waiting room; bathroom vs. kitchen) that will drive the tailoring of educational content or recognition of activities. We utilized Machine Learning methods to accurately identify an individual's discrete location, together with knowledge-based models and tools to supply the associated semantics of identified locations. We considered clustering solutions to improve localization accuracy at the expense of granularity; and investigate sensor fusion-based heuristics to rule out false location estimates. We present an AI-driven indoor localization approach that integrates both data-driven and knowledge-based processes and artifacts. We illustrate the application of our approach in two compelling healthcare use cases, and empirically validated our localization approach at the emergency unit of a large Canadian pediatric hospital.																	0933-3657	1873-2860				AUG	2020	108								101931	10.1016/j.artmed.2020.101931													
J								Coarse-to-fine classification for diabetic retinopathy grading using convolutional neural network	ARTIFICIAL INTELLIGENCE IN MEDICINE										Diabetic retinopathy grading; Coarse-to-fine classification; Convolutional neural networks; Fundus images	AUTOMATED DETECTION; ALGORITHM; THERAPY; SYSTEM	Diabetic retinopathy (DR) is the most common eye complication of diabetes and one of the leading causes of blindness and vision impairment. Automated and accurate DR grading is of great significance for the timely and effective treatment of fundus diseases. Current clinical methods remain subject to potential time-consumption and high-risk. In this paper, a hierarchically Coarse-to-fine network (CF-DRNet) is proposed as an automatic clinical tool to classify five stages of DR severity grades using convolutional neural networks (CNNs). The CF-DRNet conforms to the hierarchical characteristic of DR grading and effectively improves the classification performance of five-class DR grading, which consists of the following: (1) The Coarse Network performs two-class classification including No DR and DR, where the attention gate module highlights the salient lesion features and suppresses irrelevant background information. (2) The Fine Network is proposed to classify four stages of DR severity grades of the grade DR from the Coarse Network including mild, moderate, severe non-proliferative DR (NPDR) and proliferative DR (PDR). Experimental results show that proposed CF-DRNet out-performs some state-of-art methods in the publicly available IDRiD and Kaggle fundus image datasets. These results indicate our method enables an efficient and reliable DR grading diagnosis in clinic.																	0933-3657	1873-2860				AUG	2020	108								101936	10.1016/j.artmed.2020.101936													
J								Implementation of an ontological reasoning to support the guideline-based management of primary breast cancer patients in the DESIREE project	ARTIFICIAL INTELLIGENCE IN MEDICINE										Clinical decision support systems; Ontology; Clinical practice guidelines; Breast cancer	CLINICAL-PRACTICE GUIDELINES; DECISION-SUPPORT; IMPROVED SURVIVAL; DATA INTEGRATION; FRAMEWORK; MODEL; CARE; ADHERENCE; THERAPY; QUALITY	The DESIREE project has developed a platform offering several complementary therapeutic decision support modules to improve the quality of care for breast cancer patients. All modules are operating consistently with a common breast cancer knowledge model (BCKM) following the generic entity-attribute-value model. The BCKM is formalized as an ontology including both the data model to represent clinical patient information and the termino-ontological model to represent the application domain concepts. This ontological model is used to describe data semantics and to allow for reasoning at different levels of abstraction. We present the guideline-based decision support module (GL-DSS). Three breast cancer clinical practice guidelines have been formalized as decision rules including evidence levels, conformance levels, and two types of dependency, "refinement" and "complement", used to build complete care plans from the reconciliation of atomic recommendations. The system has been assessed on 138 decisions previously made without the system and re-played with the system after a washout period on simulated tumor boards (TBs) in three pilot sites. When TB clinicians changed their decision after using the GL-DSS, it was for a better decision than the decision made without the system in 75 % of the cases.																	0933-3657	1873-2860				AUG	2020	108								101922	10.1016/j.artmed.2020.101922													
J								Using topological data analysis and pseudo time series to infer temporal phenotypes from electronic health records	ARTIFICIAL INTELLIGENCE IN MEDICINE										Type 2 diabetes; Unsupervised machine learning; Longitudinal studies; Electronic phenotyping		Temporal phenotyping enables clinicians to better understand observable characteristics of a disease as it progresses. Modelling disease progression that captures interactions between phenotypes is inherently challenging. Temporal models that capture change in disease over time can identify the key features that characterize disease subtypes that underpin these trajectories. These models will enable clinicians to identify early warning signs of progression in specific sub-types and therefore to make informed decisions tailored to individual patients. In this paper, we explore two approaches to building temporal phenotypes based on the topology of data: topological data analysis and pseudo time-series. Using type 2 diabetes data, we show that the topological data analysis approach is able to identify disease trajectories and that pseudo time-series can infer a state space model characterized by transitions between hidden states that represent distinct temporal phenotypes. Both approaches highlight lipid profiles as key factors in distinguishing the phenotypes.																	0933-3657	1873-2860				AUG	2020	108								101930	10.1016/j.artmed.2020.101930													
J								Dealing with confounders and outliers in classification medical studies: The Autism Spectrum Disorders case study	ARTIFICIAL INTELLIGENCE IN MEDICINE										Confounders; Outliers; Autoencoder; Confounding Index; Machine learning; MRI; Autism Spectrum Disorders; Reproducibility	SUPPORT VECTOR MACHINE; INTELLECTUAL ABILITY; SEVERITY; THICKNESS	Machine learning (ML) approaches have been widely applied to medical data in order to find reliable classifiers to improve diagnosis and detect candidate biomarkers of a disease. However, as a powerful, multivariate, data-driven approach, ML can be misled by biases and outliers in the training set, finding sample-dependent classification patterns. This phenomenon often occurs in biomedical applications in which, due to the scarcity of the data, combined with their heterogeneous nature and complex acquisition process, outliers and biases are very common. In this work we present a new workflow for biomedical research based on ML approaches, that maximizes the generalizability of the classification. This workflow is based on the adoption of two data selection tools: an autoencoder to identify the outliers and the Confounding Index, to understand which characteristics of the sample can mislead classification. As a study-case we adopt the controversial research about extracting brain structural biomarkers of Autism Spectrum Disorders (ASD) from magnetic resonance images. A classifier trained on a dataset composed by 86 subjects, selected using this framework, obtained an area under the receiver operating characteristic curve of 0.79. The feature pattern identified by this classifier is still able to capture the mean differences between the ASD and Typically Developing Control classes on 1460 new subjects in the same age range of the training set, thus providing new insights on the brain characteristics of ASD. In this work, we show that the proposed workflow allows to find generalizable patterns even if the dataset is limited, while skipping the two mentioned steps and using a larger but not well designed training set would have produced a sample-dependent classifier.																	0933-3657	1873-2860				AUG	2020	108								101926	10.1016/j.artmed.2020.101926													
J								Rule-based automatic diagnosis of thyroid nodules from intraoperative frozen sections using deep learning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Thyroid nodule; Frozen section; Whole slide image; Deep learning; Rule-based protocol	NUCLEAR FEATURES; PATHOLOGY; EFFICIENT; CANCER	Frozen sections provide a basis for rapid intraoperative diagnosis that can guide surgery, but the diagnoses often challenge pathologists. Here we propose a rule-based system to differentiate thyroid nodules from intraoperative frozen sections using deep learning techniques. The proposed system consists of three components: (1) automatically locating tissue regions in the whole slide images (WSIs), (2) splitting located tissue regions into patches and classifying each patch into predefined categories using convolutional neural networks (CNN), and (3) integrating predictions of all patches to form the final diagnosis with a rule-based system. To be specific, we fine-tune the InceptionV3 model for thyroid patch classification by replacing the last fully connected layer with three outputs representing the patch's probabilities of being benign, uncertain, or malignant. Moreover, we design a rule-based protocol to integrate patches' predictions to form the final diagnosis, which provides interpretability for the proposed system. On 259 testing slides, the system correctly predicts 95.3% (61/64) of benign nodules and 96.7% (148/153) of malignant nodules, and classify 16.2% (42/259) slides as uncertain, including 19 benign and 16 malignant slides, which are a sufficiently small number to be manually examined by pathologists or fully processed through permanent sections. Besides, the system allows the localization of suspicious regions along with the diagnosis. A typical whole slide image, with 80, 000 x 60, 000 pixels, can be diagnosed within 1 min, thus satisfying the time requirement for intraoperative diagnosis. To the best of our knowledge, this is the first study to apply deep learning to diagnose thyroid nodules from intraoperative frozen sections. The code is released at https://github.com/PingjunChen/ThyroidRule.																	0933-3657	1873-2860				AUG	2020	108								101918	10.1016/j.artmed.2020.101918													
J								Continuous blood pressure measurement from one-channel electrocardiogram signal using deep-learning techniques	ARTIFICIAL INTELLIGENCE IN MEDICINE										Blood pressure; Residual network; Long short-term memory; ECG	MASKED HYPERTENSION; CUFF-LESS	Continuous blood pressure (BP) measurement is crucial for reliable and timely hypertension detection. State-of-the-art continuous BP measurement methods based on pulse transit time or multiple parameters require simultaneous electrocardiogram (ECG) and photoplethysmogram (PPG) signals. Compared with PPG signals, ECG signals are easy to collect using wearable devices. This study examined a novel continuous BP estimation approach using one-channel ECG signals for unobtrusive BP monitoring. A BP model is developed based on the fusion of a residual network and long short-term memory to obtain the spatial-temporal information of ECG signals. The public multiparameter intelligent monitoring waveform database, which contains ECG, PPG, and invasive BP data of patients in intensive care units, is used to develop and verify the model. Experimental results demonstrated that the proposed approach exhibited an estimation error of 0.07 +/- 7.77 mmHg for mean arterial pressure (MAP) and 0.01 +/- 6.29 for diastolic BP (DBP), which comply with the Association for the Advancement of Medical Instrumentation standard. According to the British Hypertension Society standards, the results achieved grade A for MAP and DBP estimation and grade B for systolic BP (SBP) estimation. Furthermore, we verified the model with an independent dataset for arrhythmia patients. The experimental results exhibited an estimation error of -0.22 +/- 5.82 mmHg, -0.57 +/- 4.39 mmHg, and -0.75 +/- 5.62 mmHg for SBP, MAP, and DBP measurements, respectively. These results indicate the feasibility of estimating BP by using a one-channel ECG signal, thus enabling continuous BP measurement for ubiquitous health care applications.																	0933-3657	1873-2860				AUG	2020	108								101919	10.1016/j.artmed.2020.101919													
J								Canada protocol: An ethical checklist for the use of artificial Intelligence in suicide prevention and mental health	ARTIFICIAL INTELLIGENCE IN MEDICINE										Artificial intelligence; Machine learning; Big Data; Ethics; Suicide; Mental Health; Prevention	DELPHI; CONSENSUS																		0933-3657	1873-2860				AUG	2020	108								101934	10.1016/j.artmed.2020.101934													
J								Temporal tree representation for similarity computation between medical patients	ARTIFICIAL INTELLIGENCE IN MEDICINE										Patient similarity; Temporal Tree	ELECTRONIC HEALTH RECORDS	Objective: The aim of this study is to compute similarities between patient records in an electronic health record (EHR). This is an important problem because the availability of effective methods for the computation of patient similarity would allow for assistance with and automation of tasks such as patients stratification, medical prognosis and cohort selection, and for unlocking the potential of medical analytics methods for healthcare intelligence. However, health data in EHRs presents many challenges that make the automatic computation of patient similarity difficult; these include: temporal aspects, multivariate, heterogeneous and irregular data, and data sparsity. Materials and methods: We propose a new method for EHR data representation called Temporal Tree: a temporal hierarchical representation which, based on temporal co-occurrence, preserves the compound information found at different levels in health data. In addition, this representation is augmented using the doc2vec embedding technique which here is exploited for patient similarity computation. We empirically investigate our proposed method, along with several state-of-the-art benchmarks, on a dataset of real world Intensive Care Unit (ICU) EHRs, for the task of identifying patients with a specific target diagnosis. Results: Our empirical results show that the Temporal Trees representation is significantly better than other traditional and state-of-the-art methods for representing patients and computing their similarities. Conclusion: Temporal trees capture the temporal relationships between medical, hierarchical data: this enables to effectively model the rich information provided within EHRs and thus the identification of similar patients.																	0933-3657	1873-2860				AUG	2020	108								101900	10.1016/j.artmed.2020.101900													
J								Domain expertise-agnostic feature selection for the analysis of breast cancer data	ARTIFICIAL INTELLIGENCE IN MEDICINE										Breast cancer; Clustering; Clustering performance evaluation; Dimensionality reduction; Feature selection; Proteomics; Unsupervised learning	CLASSIFICATION; ALGORITHMS	Progress in proteomics has enabled biologists to accurately measure the amount of protein in a tumor. This work is based on a breast cancer data set, result of the proteomics analysis of a cohort of tumors carried out at Karolinska Instituter. While evidence suggests that an anomaly in the protein content is related to the cancerous nature of tumors, the proteins that could be markers of cancer types and subtypes and the underlying interactions are not completely known. This work sheds light on the potential of the application of unsupervised learning in the analysis of the aforementioned data sets, namely in the detection of distinctive proteins for the identification of the cancer subtypes, in the absence of domain expertise. In the analyzed data set, the number of samples, or tumors, is significantly lower than the number of features, or proteins; consequently, the input data can be thought of as high-dimensional data. The use of high-dimensional data has already become widespread, and a great deal of effort has been put into high-dimensional data analysis by means of feature selection, but it is still largely based on prior specialist knowledge, which in this case is not complete. There is a growing need for unsupervised feature selection, which raises the issue of how to generate promising subsets of features among all the possible combinations, as well as how to evaluate the quality of these subsets in the absence of specialist knowledge. We hereby propose a new wrapper method for the generation and evaluation of subsets of features via spectral clustering and modularity, respectively. We conduct experiments to test the effectiveness of the new method in the analysis of the breast cancer data, in a domain expertise-agnostic context. Furthermore, we show that we can successfully augment our method by incorporating an external source of data on known protein complexes. Our approach reveals a large number of subsets of features that are better at clustering the samples than the state-of-the-art classification in terms of modularity and shows a potential to be useful for future proteomics research.																	0933-3657	1873-2860				AUG	2020	108								101928	10.1016/j.artmed.2020.101928													
J								A supervised machine learning-based methodology for analyzing dysregulation in splicing machinery: An application in cancer diagnosis	ARTIFICIAL INTELLIGENCE IN MEDICINE										Transcript-based analysis; Alternative Splicing; Feature weighting methods; Classification methods; Explaining classifier's predictions	FEATURE-SELECTION; EXPRESSION; CLASSIFICATION; MUTATIONS; SUBTYPES; IMPROVE; FAMILY	Deregulated splicing machinery components have shown to be associated with the development of several types of cancer and, therefore, the determination of such alterations can help the development of tumor-specific molecular targets for early prognosis and therapy. Determining such splicing components, however, is not a straightforward task mainly due to the heterogeneity of tumors, the variability across samples, and the fat-short characteristic of genomic datasets. In this work, a supervised machine learning-based methodology is proposed, allowing the determination of subsets of relevant splicing components that best discriminate samples. The methodology comprises three main phases: first, a ranking of features is determined by means of applying feature weighting algorithms that compute the importance of each splicing component; second, the best subset of features that allows the induction of an accurate classifier is determined by means of conducting an effective heuristic search; then the confidence over the induced classifier is assessed by means of explaining the individual predictions and its global behavior. At the end, an extensive experimental study was conducted on a large collection of transcript-based datasets, illustrating the utility and benefit of the proposed methodology for analyzing dysregulation in splicing machinery.																	0933-3657	1873-2860				AUG	2020	108								101950	10.1016/j.artmed.2020.101950													
J								Variable step dynamic threshold local binary pattern for classification of atrial fibrillation	ARTIFICIAL INTELLIGENCE IN MEDICINE										Local binary pattern; Dynamic threshold; Variable step; Atrial fibrillation; AFDB; MITDB	RATE-INDEPENDENT DETECTION; AUTOMATIC DETECTION	Objective: In this paper, we proposed new methods for feature extraction in machine learning-based classification of atrial fibrillation from ECG signal. Methods: Our proposed methods improved conventional 1-dimensional local binary pattern method in two ways. First, we proposed a dynamic threshold LBP code generation method for use with 1-dimensional signals, enabling the generated LBP codes to have a more detailed representation of the signal morphological pattern. Second, we introduced a variable step value into the LBP code generation algorithm to better cope with a high sampling frequency input signal without a downsampling process. The proposed methods do not employ computationally expensive processes such as filtering, wavelet transform, up/downsampling, or beat detection, and can be implemented using only simple addition, division, and compare operations. Results: Combining these two approaches, our proposed variable step dynamic threshold local binary pattern method achieved 99.11% sensitivity and 99.29% specificity when used as a feature generation algorithm in support vector machine classification of atrial fibrillation from MIT-BIH Atrial Fibrillation Database dataset. When applied on signals from MIT-BIH Arrhythmia Database, our proposed method achieved similarly good 99.38% sensitivity and 98.97% specificity. Conclusion: Our proposed methods achieved one of the best results among published works in atrial fibrillation classification using the same dataset while using less computationally expensive calculations, without significant performance degradation when applied on signals from multiple databases with different sampling frequencies.																	0933-3657	1873-2860				AUG	2020	108								101932	10.1016/j.artmed.2020.101932													
J								Constructing an Efficient Machine Learning Model for Tornado Prediction	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Machine learning; tornado prediction; superposition principle; data analysis	NEURAL-NETWORK; MANIPULATABILITY; CLASSIFIERS	Tornado prediction variables are analyzed using machine learning and decision analysis techniques. A model based on several choice procedures and the superposition principle is applied for different methods of data analysis. The constructed model has been tested on a database of tornadic events. It is shown that the tornado prediction model developed herein is more efficient than a previous set of machine learning models, opening the way to more accurate decisions.																	0219-6220	1793-6845				AUG	2020	19	5					1177	1187		10.1142/S0219622020500261													
J								An Invasive Weed Optimization-Based Fuzzy Decision-making Framework for Bridge Intervention Prioritization in Element and Network Levels	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Decision-making framework; bridge intervention; optimized fuzzy analytical network process; ground-penetrating radar; invasive weed optimization; fuzzy membership functions	ANALYTIC HIERARCHY PROCESS; RISK-ASSESSMENT; MANAGEMENT-SYSTEM; C-MEANS; CONSTRUCTION; SELECTION; MODEL; METHODOLOGY; INSPECTION; ALGORITHM	Recently, the number of deteriorating bridges has drastically increased. Furthermore, tight maintenance budgets are cut down, imposing escalating adverse implications on the safety of bridges. This state of affairs entails the development of decision support systems for the effective management of bridges within the allocated budget. As such, this study introduces an invasive weed optimization-based fuzzy decision-making framework designated for bridge intervention prioritization in both element and network levels. The proposed decision-making platform encompasses three main tiers. The first tier is an optimized fuzzy analytical network process model that aims at computing the weighting vector of the bridge defects, namely corrosion, delamination, cracking, spalling and scaling. In this model, a genetic algorithm optimization model is formulated to improve the consistencies of judgment matrices through circumventing the imprecisions encountered by the classical judgment assignment. The second tier encompasses establishing an integrated bridge deck condition assessment model capitalizing on ground-penetrating radar and inspection reports. In it, the severities of the bridge defects are demonstrated in the form of fuzzy membership functions to address the inherent uncertainties of inspection. Subsequently, a variable-length invasive weed optimization model is structured to automatically calibrate the fuzzy membership functions. The third model is designed for structuring a bridge maintenance decision-making strategy stepping on the integrated condition index. The capabilities of the proposed framework were validated through several levels of comparisons. For instance, it significantly outperformed some of the current condition assessment models. Additionally, it inferred that the thresholds separating the four categories of the integrated bridge deck condition index are 75.651, 67.769 and 60.318.																	0219-6220	1793-6845				AUG	2020	19	5					1189	1246		10.1142/S0219622020500273													
J								Multi-Biological Laboratory Examination Framework for the Prioritization of Patients with COVID-19 Based on Integrated AHP and Group VIKOR Methods	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										COVID-19; inflammatory marker; respiratory; prioritization; multi-criteria decision making; analytic hierarchy process; VIKOR	DECISION-MAKING; MULTICRITERIA ANALYSIS; TRACKING CHANNELS; FUZZY TOPSIS; OPEN ISSUES; BENCHMARKING; COUNT; CLASSIFICATION; OPTIMIZATION; PERFORMANCE	Coronavirus disease (COVID-19) pandemic has a tremendous effect on people's lives worldwide, and the number of infected patients increases daily. The healthcare sector is affected by a large number of patients with COVID-19, and a solution is urgently needed to avert the risk of deteriorating patients in terms of prioritizing patients based on their health conditions. Prioritization of patients with COVID-19 is a complex and multi-criteria decision-analysis (MCDA) problem due to (i) multiple biological laboratory examination criteria, (ii) criteria importance and (iii) trade-off amongst the criteria. This study presents a new multi-biological laboratory examination framework for prioritizing patients with COVID-19 on the basis of integrated MCDA methods. The experiment was conducted on the basis of three phases. In the first phase, patient datasets containing eight biological laboratory examination criteria for six patients with COVID-19 were derived and discussed. The outcome of this phase was used to propose a decision matrix on the basis of the intersection between "biological laboratory examination criteria" and "COVID-19 patients list". In the second phase, the analytic hierarchy process (AHP) method was utilized to set the subjective weights for the biological laboratory examination criteria by respiratory experts. In the last phase, the VIekriterijumsko KOmpromisno Rangiranje (VIKOR) method was adopted to prioritize patients in the context of individual and group decision making (GDM). Results showed that (1) the integration of AHP-VIKOR method based on individual and GDM contexts was effective for solving prioritization problems for patients with COVID-19, and (2) the prioritization results of patients with COVID-19 showed no variation in the internal and external VIKOR GDM contexts. The proposed multi-biological laboratory examination framework can differentiate between the mild and serious or critical condition of patients with COVID-19 by prioritizing them based on integrated AHP-VIKOR methods. In conclusion, medical sectors can use the proposed framework to differentiate the health conditions of infected patients and to assign appropriate care with prompt and effective treatment.																	0219-6220	1793-6845				AUG	2020	19	5					1247	1269		10.1142/S0219622020500285													
J								Novel Stable Approach with Probability Distribution for Multi-Criteria Decision-Making Problems of Multi-Valued Neutrosophic Sets	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multi-valued neutrosophic sets (MVNSs); multi-criteria decision-making (MCDM); probability distribution; dependent degree	AGGREGATION OPERATORS; FUZZY; MATRIX	In this paper, a novel approach and framework based on interval-dependent degree and probability distribution for multi-criteria decision-making problems with multi-valued neutrosophic sets (MVNSs) is proposed. First, a simplified dependent function and distribution function are given and integrated into a concise formula, which is called the interval-dependent function and contains interval computing and probability distribution information in an interval. Then a transformation operator is defined and it is shown how to convert MVNSs into an interval set. Subsequently, the interval-dependent function with the probability distribution of MVNSs is deduced. Finally, an example and comparative analysis are provided to verify the feasibility and effectiveness of the proposed method. In addition, uncertainty analysis, which reflects the dynamic change of the ranking result with decision-makers' preferences, is performed by setting different distribution functions, which increases the reliability and accuracy of the proposed method.																	0219-6220	1793-6845				AUG	2020	19	5					1271	1292		10.1142/S0219622020500339													
J								An Integrated Decision-Making Framework to Appraise Water Losses in Municipal Water Systems	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Decision support; fuzzy set; Monte Carlo simulation; sensitivity analysis; sustainability	ANALYTIC HIERARCHY PROCESS; FUZZY SYNTHETIC EVALUATION; LOSS MANAGEMENT; MODEL; AHP; FAILURE; RISK; MULTICRITERIA; SELECTION; METHODOLOGY	To mitigate the acute water shortage problems, water utilities are combating to find potential solutions. Water losses management in Water Supply Networks (WSNs) is amongst the prominent solutions. This work intends to develop a decision support framework to diagnose the criticality of WSNs according to an associated Water Loss Risk Index (WLRI) at pipe and zone levels. It utilized the Fuzzy Analytic Hierarchy Process (FAHP) to evaluate the influential factors that contribute to water losses. The Fuzzy Synthetic Evaluation Technique (FSET) has been utilized to assess WLRI at pipe level and Ordered Weighted Averaging (OWA) operator has been used to aggregate the individual WLRI values for each zone. The framework is extended by incorporating Monte Carlo simulation model to generate the final ranking of zones. The outcomes of this simulation showed fair stability in terms of ranking the scrutinized zones. The integration of this framework in water losses management practices and planning policies of water utilities has a large potential in improving water supply services and the performance of WSNs.																	0219-6220	1793-6845				AUG	2020	19	5					1293	1326		10.1142/S0219622020500297													
J								Integrating Risk into Project Control Using Bayesian Networks	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Project management; project control; risk analysis; Bayesian Networks; earned value management	EARNED VALUE MANAGEMENT; SCHEDULE CONTROL; COST; PREDICTION; CONSTRUCTION; ALGORITHMS; FRAMEWORK; INFERENCE; FOOTBALL; MODEL	Projects are, by definition, risky and uncertain ventures. Therefore, the performance and risk of major projects should be carefully controlled in order to increase their probability of success. Quantitative project control techniques assist project managers in detecting problems, thus responding to them early on, by comparing the baseline plan with the project progress. However, project risk and uncertainty are rarely considered by these techniques. This paper proposes a project control framework that integrates the project uncertainty and associated risk factors into project control. Our framework is based on earned value management (EVM), which is an effective and widely used quantitative project control technique. The framework uses hybrid Bayesian Networks (BNs) to enhance EVM with the ability to compute the uncertainty associated with its parameters and risk factors. The framework can be applied to projects from different domains, and we illustrate its use with a simple example and a case study of a construction project.																	0219-6220	1793-6845				AUG	2020	19	5					1327	1352		10.1142/S0219622020500315													
J								An Improved MABAC Group Decision-Making Method Using Regret Theory and Likelihood in Probability Multi-Valued Neutrosophic Sets	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Probability multi-valued neutrosophic sets; likelihood; multi-attributive border approximation area comparison; regret theory; multi-attribute group decision-making	AGGREGATION OPERATORS; SIMILARITY MEASURE; FUZZY-SETS; SELECTION; NUMBERS; WEIGHT; CHOICE	Probability multi-valued neutrosophic set (PMVNS) is a preferable tool to capture the preference and hesitancy of decision makers (DMs) and to depict inconsistent and ambiguous information. In this paper, we improve the multi-attributive border approximation area comparison (MABAC) method under the PMVNS environment and establish a three-phase multi-attribute group decision-making (MAGDM) method. Firstly, some concepts of PMVNS, traditional MABAC method and regret theory (RT) are reviewed. Then, the similarity measure for PMVNSs is defined and utilized to derive the important degree of DMs, and the likelihood of preference relations expressed by the probability multi-valued neutrosophic numbers (PMVNNs) is first presented and employed to replace the distance deviation in traditional MABAC method. Furthermore, a novel MAGDM method where the performance of alternatives is expressed by the PMVNN is established by combining the likelihood-based MABAC method and RT which considered given DMs' behavior psychology. Finally, a case study is implemented to demonstrate the feasibility and applicability of our proposed approach.																	0219-6220	1793-6845				AUG	2020	19	5					1353	1387		10.1142/S0219622020500303													
J								ReS-Algorithm for Converting Normalized Values of Cost Criteria Into Benefit Criteria in MCDM Tasks	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multicriteria decision-making; normalization of multidimensional data; normalization values of the cost criteria; reverse sorting (ReS) algorithm for converting normalized values of the cost criteria in to the benefit criteria	FUZZY AHP; TOPSIS METHOD; SELECTION; PERFORMANCE; VIKOR	A review of modern methods of data normalization in the tasks of multicriteria decision-making and multidimensional classification is presented. The invariant properties of linear normalization methods are determined. Two basic principles of normalization of multidimensional data are defined: preservation of dispositions of natural and normalized values on the measurement scale and the absence of a displacement in the areas of normalized values of various criteria relative to each other. A method is proposed for converting normalized values of cost criteria to profit criteria based on the reverse sorting algorithm (ReS-algorithm). The ReS-algorithm preserves the dispositions of the natural and normalized values of the attributes of the alternatives and eliminates the displacement the areas of normalized values of the cost criteria relative to the profit criteria, which ensures the equality of the contributions of various criteria to the performance indicator of the alternatives.																	0219-6220	1793-6845				AUG	2020	19	5					1389	1423		10.1142/S0219622020500327													
J								Application of the Reed-Solomon Algorithm as a Remote Sensing Data Fusion Tool for Land Use Studies	ALGORITHMS										Reed-Solomon algorithm; data fusion; operationalization; challenges; remote sensing; land use; land cover	MULTISENSOR IMAGE FUSION; SPATIAL-RESOLUTION; CLASSIFICATION; REGION; TRANSFORM; SUPPORT	The Reed-Solomon algorithm is well known in different fields of computer science. The novelty of this study lies in the different interpretation of the algorithm itself and its scope of application for remote sensing, especially at the preparatory stage, i.e., data fusion. A short review of the attempts to use different data fusion approaches in geospatial technologies explains the possible usage of the algorithm. The rationale behind its application for data fusion is to include all possible information from all acquired spectral bands, assuming that complete composite information in the form of one compound image will improve both the quality of visualization and some aspects of further quantitative and qualitative analyses. The concept arose from an empirical, heuristic combination of geographic information systems (GIS), map algebra, and two-dimensional cellular automata. The challenges are related to handling big quantitative data sets and the awareness that these numbers are in fact descriptors of a real-world multidimensional view. An empirical case study makes it easier to understand the operationalization of the Reed-Solomon algorithm for land use studies.																		1999-4893				AUG	2020	13	8							188	10.3390/a13080188													
J								Extended Z-MABAC Method Based on Regret Theory and Directed Distance for Regional Circular Economy Development Program Selection With Z-Information	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Euclidean distance; Decision making; Probability distribution; Computational complexity; Reliability theory; Current measurement; Circular economy (CE); fuzzy cut-set theory; regret theory; standardized Euclidean distance; Z-number	DECISION-MAKING; Z-NUMBERS; MODEL; BACKORDER; INVENTORY	Decision makers (DMs) have different cognitive levels in practical experience, information reserve, and thinking ability. Thus, decision information is often not completely reliable. As a tool that can effectively represent information reliability, Z-number has been studied by many scholars in recent years. Current research on Z-number assumes that differences in various parts of a Z-number can complement one another. However, in many cases, the preference of DMs for each part is difficult to determine, or DMs believe that the differences in various parts cannot be complementary. Therefore, to solve such decision problems, this paper attempts to extend the traditional MABAC method to the Z-information environment by introducing the directed distance and regret theory. The proposed method simultaneously considers the randomness and fuzziness of Z-number. An example about regional circular economy development program selection is provided to illustrate the feasibility of the proposed method. Results show that the proposed method can solve complex decision problems rationally and effectively, and it has broad application prospects.																	1063-6706	1941-0034				AUG	2020	28	8					1851	1863		10.1109/TFUZZ.2019.2923948													
J								Squeeze-and-Excitation Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Squeeze-and-excitation; image representations; attention; convolutional neural networks	VISUAL-ATTENTION; MODEL	The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of similar to 25 percent. Models and code are available at https://github.com/hujie-frank/SENet.																	0162-8828	1939-3539				AUG. 1	2020	42	8					2011	2023		10.1109/TPAMI.2019.2913372													
J								Hybrid optimization with cryptography encryption for medical image security in Internet of Things	NEURAL COMPUTING & APPLICATIONS										IoT; Medical images; Cloud; Encryption; Decryption; Optimization; PSO; Grasshopper optimization; ECC	ENABLING TECHNOLOGIES; VISUAL CRYPTOGRAPHY; HEALTH-CARE; BIG DATA; CLOUD; MODEL; SERVICES; CREATION; SCHEME; IOT	The development of the Internet of Things (IoT) is predicted to change the healthcare industry and might lead to the rise of the Internet of Medical Things. The IoT revolution is surpassing the present-day human services with promising mechanical, financial, and social prospects. This paper investigated the security of medical images in IoT by utilizing an innovative cryptographic model with optimization strategies. For the most part, the patient data are stored as a cloud server in the hospital due to which the security is vital. So another framework is required for the secure transmission and effective storage of medical images interleaved with patient information. For increasing the security level of encryption and decryption process, the optimal key will be chosen using hybrid swarm optimization, i.e., grasshopper optimization and particle swarm optimization in elliptic curve cryptography. In view of this method, the medical images are secured in IoT framework. From this execution, the results are compared and contrasted, whereas a diverse encryption algorithm with its optimization methods from the literature is identified with the most extreme peak signal-to-noise ratio values, i.e., 59.45 dB and structural similarity index as 1.																	0941-0643	1433-3058				AUG	2020	32	15					10979	10993		10.1007/s00521-018-3801-x													
J								Secure image encryption scheme based on fractals key with Fibonacci series and discrete dynamical system	NEURAL COMPUTING & APPLICATIONS										Fractals; Cryptography; Fibonacci; Chaotic maps; Brute force attack	CHAOTIC SYSTEM; CRYPTANALYSIS; PERMUTATION	In this article, our aim is to design a new and efficient digital information confidentiality mechanism. We have offered an encryption scheme which is based on fractals and multiple chaotic iterative maps in order to add more confusion and diffusion capability. Due to randomness nature and unique repetitive pattern of fractal increases key space to hundreds of bits and enhance security level of proposed cryptosystem. The projected algorithm is authenticated by utilizing security performance analyses. The security performances elucidate that our suggested technique is quite competent for digital image encryption.																	0941-0643	1433-3058				AUG	2020	32	15					11837	11857		10.1007/s00521-019-04667-y													
J								A NARX Model Reference Adaptive Control Scheme: Improved Disturbance Rejection Fractional-Order PID Control of an Experimental Magnetic Levitation System	ALGORITHMS										multi-loop model reference control; PID controllers; FOPID retuning control; disturbance rejection control	IMPLEMENTATION; STABILITY; DYNAMICS; DESIGN	Real control systems require robust control performance to deal with unpredictable and altering operating conditions of real-world systems. Improvement of disturbance rejection control performance should be considered as one of the essential control objectives in practical control system design tasks. This study presents a multi-loop Model Reference Adaptive Control (MRAC) scheme that leverages a nonlinear autoregressive neural network with external inputs (NARX) model in as the reference model. Authors observed that the performance of multi-loop MRAC-fractional-order proportional integral derivative (FOPID) control with MIT rule largely depends on the capability of the reference model to represent leading closed-loop dynamics of the experimental ML system. As such, the NARX model is used to represent disturbance-free dynamical behavior of PID control loop. It is remarkable that the obtained reference model is independent of the tuning of other control loops in the control system. The multi-loop MRAC-FOPID control structure detects impacts of disturbance incidents on control performance of the closed-loop FOPID control system and adapts the response of the FOPID control system to reduce the negative effects of the additive input disturbance. This multi-loop control structure deploys two specialized control loops: an inner loop, which is the closed-loop FOPID control system for stability and set-point control, and an outer loop, which involves a NARX reference model and an MIT rule to increase the adaptation ability of the system. Thus, the two-loop MRAC structure allows improvement of disturbance rejection performance without deteriorating precise set-point control and stability characteristics of the FOPID control loop. This is an important benefit of this control structure. To demonstrate disturbance rejection performance improvements of the proposed multi-loop MRAC-FOPID control with NARX model, an experimental study is conducted for disturbance rejection control of magnetic levitation test setup in the laboratory. Simulation and experimental results indicate an improvement of disturbance rejection performance.																		1999-4893				AUG	2020	13	8							201	10.3390/a13080201													
J								Graph Planarity by Replacing Cliques with Paths	ALGORITHMS										planar graphs; k-planarity; NP-hardness; polynomial time reduction; cliques; paths	NODETRIX	This paper introduces and studies the following beyond-planarity problem, which we call h-CLIQUE2PATH PLANARITY. Let G be a simple topological graph whose vertices are partitioned into subsets of size at most h, each inducing a clique. h-CLIQUE2PATH PLANARITY asks whether it is possible to obtain a planar subgraph of G by removing edges from each clique so that the subgraph induced by each subset is a path. We investigate the complexity of this problem in relation to k-planarity. In particular, we prove that h-CLIQUE2PATH PLANARITY is NP-complete even when h = 4 and G is a simple 3-plane graph, while it can be solved in linear time when G is a simple 1-plane graph, for any value of h. Our results contribute to the growing fields of hybrid planarity and of graph drawing beyond planarity.																		1999-4893				AUG	2020	13	8							194	10.3390/a13080194													
J								Towards Cognitive Recommender Systems	ALGORITHMS										recommender systems; cognitive technology; deep learning; knowledge lakes	USER MODELS; PERSONALIZATION; INFORMATION; FRAMEWORK; AGENT	Intelligence is the ability to learn from experience and use domain experts' knowledge to adapt to new situations. In this context, an intelligent Recommender System should be able to learn from domain experts' knowledge and experience, as it is vital to know the domain that the items will be recommended. Traditionally, Recommender Systems have been recognized as playlist generators for video/music services (e.g., Netflix and Spotify), e-commerce product recommenders (e.g., Amazon and eBay), or social content recommenders (e.g., Facebook and Twitter). However, Recommender Systems in modern enterprises are highly data-/knowledge-driven and may rely on users' cognitive aspects such as personality, behavior, and attitude. In this paper, we survey and summarize previously published studies on Recommender Systems to help readers understand our method's contributions to the field in this context. We discuss the current limitations of the state of the art approaches in Recommender Systems and the need for our new approach: A vision and a general framework for a new type of data-driven, knowledge-driven, and cognition-driven Recommender Systems, namely,Cognitive Recommender Systems. Cognitive Recommender Systems will be the new type of intelligent Recommender Systems that understand the user's preferences, detect changes in user preferences over time, predict user's unknown favorites, and explore adaptive mechanisms to enable intelligent actions within the compound and changing environments. We present a motivating scenario in banking and argue that existing Recommender Systems: (i) do not use domain experts' knowledge to adapt to new situations; (ii) may not be able to predict the ratings or preferences a customer would give to a product (e.g., loan, deposit, or trust service); and (iii) do not support data capture and analytics around customers' cognitive activities and use it to provide intelligent and time-aware recommendations.																		1999-4893				AUG	2020	13	8							176	10.3390/a13080176													
J								On the Optimal Calculation of the Rice Coding Parameter	ALGORITHMS										data compression; Internet of Things; low-power data compression; Micromole; Rice encoding; wireless sensor networks	DATA-COMPRESSION; TRANSFORM; ALGORITHM; CODES	In this article, we design and evaluate several algorithms for the computation of the optimal Rice coding parameter. We conjecture that the optimal Rice coding parameter can be bounded and verify this conjecture through numerical experiments using real data. We also describe algorithms that partition the input sequence of data into sub-sequences, such that if each sub-sequence is coded with a different Rice parameter, the overall code length is minimised. An algorithm for finding the optimal partitioning solution for Rice codes is proposed, as well as fast heuristics, based on the understanding of the problem trade-offs.																		1999-4893				AUG	2020	13	8							181	10.3390/a13080181													
J								Scalable Block Preconditioners for Linearized Navier-Stokes Equations at High Reynolds Number	ALGORITHMS										scalable preconditioners; Navier-Stokes equations; GMRES method; low-rank updates; multigrid	SCHUR COMPLEMENT PRECONDITIONERS; MIXED CONSTRAINT PRECONDITIONERS; SADDLE-POINT PROBLEMS; TRIANGULAR PRECONDITIONERS; CONVERGENCE ANALYSIS	We review a number of preconditioners for the advection-diffusion operator and for the Schur complement matrix, which, in turn, constitute the building blocks for Constraint and Triangular Preconditioners to accelerate the iterative solution of the discretized and linearized Navier-Stokes equations. An intensive numerical testing is performed onto the driven cavity problem with low values of the viscosity coefficient. We devise an efficient multigrid preconditioner for the advection-diffusion matrix, which, combined with the commuted BFBt Schur complement approximation, and inserted in a2x2block preconditioner, provides convergence of the Generalized Minimal Residual (GMRES) method in a number of iteration independent of the meshsize for the lowest values of the viscosity parameter. The low-rank acceleration of such preconditioner is also investigated, showing its great potential.																		1999-4893				AUG	2020	13	8							199	10.3390/a13080199													
J								A Brief Survey of Fixed-Parameter Parallelism	ALGORITHMS										fixed-parameter parallelism; parameterized complexity; parallel computing	ALGORITHMS	This paper provides an overview of the field of parameterized parallel complexity by surveying previous work in addition to presenting a few new observations and exploring potential new directions. In particular, we present a general view of how knownFPTtechniques, such as bounded search trees, color coding, kernelization, and iterative compression, can be modified to produce fixed-parameter parallel algorithms.																		1999-4893				AUG	2020	13	8							197	10.3390/a13080197													
J								Methodology for Analyzing the Traditional Algorithms Performance of User Reviews Using Machine Learning Techniques	ALGORITHMS										machine learning; preprocessing; semantic analysis; text mining; term frequency/inverse document frequency (TF/IDF); scraping; Google Play Store		Android-based applications are widely used by almost everyone around the globe. Due to the availability of the Internet almost everywhere at no charge, almost half of the globe is engaged with social networking, social media surfing, messaging, browsing and plugins. In the Google Play Store, which is one of the most popular Internet application stores, users are encouraged to download thousands of applications and various types of software. In this research study, we have scraped thousands of user reviews and the ratings of different applications. We scraped 148 application reviews from 14 different categories. A total of 506,259 reviews were accumulated and assessed. Based on the semantics of reviews of the applications, the results of the reviews were classified negative, positive or neutral. In this research, different machine-learning algorithms such as logistic regression, random forest and naive Bayes were tuned and tested. We also evaluated the outcome of term frequency (TF) and inverse document frequency (IDF), measured different parameters such as accuracy, precision, recall and F1 score (F1) and present the results in the form of a bar graph. In conclusion, we compared the outcome of each algorithm and found that logistic regression is one of the best algorithms for the review-analysis of the Google Play Store from an accuracy perspective. Furthermore, we were able to prove and demonstrate that logistic regression is better in terms of speed, rate of accuracy, recall and F1 perspective. This conclusion was achieved after preprocessing a number of data values from these data sets.																		1999-4893				AUG	2020	13	8							202	10.3390/a13080202													
J								Local-Topology-Based Scaling for Distance Preserving Dimension Reduction Method to Improve Classification of Biomedical Data-Sets	ALGORITHMS										dimension reduction; distance preserving; local topology; multidimensional scaling (MDS)		Dimension reduction is often used for several procedures of analysis of high dimensional biomedical data-sets such as classification or outlier detection. To improve the performance of such data-mining steps, preserving both distance information and local topology among data-points could be more useful than giving priority to visualization in low dimension. Therefore, we introduce topology-preserving distance scaling (TPDS) to augment a dimension reduction method meant to reproduce distance information in a higher dimension. Our approach involves distance inflation to preserve local topology to avoid collapse during distance preservation-based optimization. Applying TPDS on diverse biomedical data-sets revealed that besides providing better visualization than typical distance preserving methods, TPDS leads to better classification of data points in reduced dimension. For data-sets with outliers, the approach of TPDS also proves to be useful, even for purely distance-preserving method for achieving better convergence.																		1999-4893				AUG	2020	13	8							192	10.3390/a13080192													
J								The Model Order Reduction Method as an Effective Way to Implement GPC Controller for Multidimensional Objects	ALGORITHMS										high order model; model predictive control (MPC); generalized predictive control (GPC); numerical problems	GENERALIZED PREDICTIVE CONTROL; SYSTEMS	The paper addresses issues associated with implementing GPC controllers in systems with multiple input signals. Depending on the method of identification, the resulting models may be of a high order and when applied to a control/regulation law, may result in numerical errors due to the limitations of representing values in double-precision floating point numbers. This phenomenon is to be avoided, because even if the model is correct, the resulting numerical errors will lead to poor control performance. An effective way to identify, and at the same time eliminate, this unfavorable feature is to reduce the model order. A method of model order reduction is presented in this paper that effectively mitigates these issues. In this paper, the Generalized Predictive Control (GPC) algorithm is presented, followed by a discussion of the conditions that result in high order models. Examples are included where the discussed problem is demonstrated along with the subsequent results after the reduction. The obtained results and formulated conclusions are valuable for industry practitioners who implement a predictive control in industry.																		1999-4893				AUG	2020	13	8							178	10.3390/a13080178													
J								Constructing Reliable Computing Environments on Top of Amazon EC2 Spot Instances	ALGORITHMS										cloud computing; amazon EC2 (Elastic Compute Cloud); spot instances; reliability; scheduling; workflow applications	CLOUD; SERVICE; MIGRATION; WORKFLOWS; BUDGET; TIME	Cloud provider Amazon Elastic Compute Cloud (EC2) gives access to resources in the form of virtual servers, also known as instances. EC2 spot instances (SIs) offer spare computational capacity at steep discounts compared to reliable and fixed price on-demand instances. The drawback, however, is that the delay in acquiring spots can be incredible high. Moreover, SIs may not always be available as they can be reclaimed by EC2 at any given time, with a two-minute interruption notice. In this paper, we propose a multi-workflow scheduling algorithm, allied with a container migration-based mechanism, to dynamically construct and readjust virtual clusters on top of non-reserved EC2 pricing model instances. Our solution leverages recent findings on performance and behavior characteristics of EC2 spots. We conducted simulations by submitting real-life workflow applications, constrained by user-defined deadline and budget quality of service (QoS) parameters. The results indicate that our solution improves the rate of completed tasks by almost 20%, and the rate of completed workflows by at least 30%, compared with other state-of-the-art algorithms, for a worse-case scenario.																		1999-4893				AUG	2020	13	8							187	10.3390/a13080187													
J								Deep Learning-Enabled Semantic Inference of Individual Building Damage Magnitude from Satellite Images	ALGORITHMS										computer vision; artificial intelligence; disaster management; remote sensing; damage magnitude; satellite imaging; buildings		Natural disasters are phenomena that can occur in any part of the world. They can cause massive amounts of destruction and leave entire cities in great need of assistance. The ability to quickly and accurately deliver aid to impacted areas is crucial toward not only saving time and money, but, most importantly, lives. We present a deep learning-based computer vision model to semantically infer the magnitude of damage to individual buildings after natural disasters using pre- and post-disaster satellite images. This model helps alleviate a major bottleneck in disaster management decision support by automating the analysis of the magnitude of damage to buildings post-disaster. In this paper, we will show our methods and results for how we were able to obtain a better performance than existing models, especially in moderate to significant magnitudes of damage, along with ablation studies to show our methods and results for the importance and impact of different training parameters in deep learning for satellite imagery. We were able to obtain an overall F1 score of 0.868 with our methods.																		1999-4893				AUG	2020	13	8							195	10.3390/a13080195													
J								Cross-Camera Erased Feature Learning for Unsupervised Person Re-Identification	ALGORITHMS										unsupervised; cross camera; feature discriminative learning; person re-identification	NETWORK	Most supervised person re-identification methods show their excellent performance, but using labeled datasets is very expensive, which limits its application in practical scenarios. To solve the scalability problem, we propose a Cross-camera Erased Feature Learning (CEFL) framework for unsupervised person re-identification that learns discriminative features from image appearances without manual annotations, where both of the cross-camera global image appearance and the local details are explored. Specifically, for the global appearance, in order to bridge the gap between images with the same identities under different cameras, we generate style-transferred images. The network is trained to classify the original images, the style-transferred images and the negative samples. To learn the partial details of the images, we generate erased images and train the network to pull the similar erased images together and push the dissimilar ones away. In addition, we joint learn the discriminative global and local information to learn a more robust model. Global and erased features are used together in feature learning which are successful conjunction of BFENet. A large number of experiments show the superiority of CEFL in unsupervised pedestrian re-identification.																		1999-4893				AUG	2020	13	8							193	10.3390/a13080193													
J								A checklist for safe robot swarms	NATURE MACHINE INTELLIGENCE											FAULT-TOLERANCE	As robot swarms move from the laboratory to real-world applications, a routine checklist of questions could help ensure their safe operation.																		2522-5839				AUG	2020	2	8					420	422		10.1038/s42256-020-0213-2													
J								Origami-inspired miniature manipulator for teleoperated microsurgery	NATURE MACHINE INTELLIGENCE											SURGICAL ROBOT; MECHANISM; DIAMETERS; FORCE	Robot-assisted microsurgery promises high stability and accuracy for instance in eye- or neurosurgery applications. A new miniature robotics device, based on an origami-inspired design, can make complex 3D motions and reaches a precision of around 26 micrometres. The use of a structure with a remote fixed point around which a mechanism can rotate is called remote centre of motion (RCM). The technique is widely used in minimally invasive surgery to avoid excess force on the incision site during the robot's motion. Here we describe the design, fabrication and characterization of an origami-inspired miniature RCM manipulator for teleoperated microsurgery (the mini-RCM has mass 2.4 g and size 50 mm x 70 mm x 50 mm), which is actuated by three independently controlled linear actuators with concomitant sensing (each mini-LA has mass 0.41 g and size 28 mm x 7 mm x 3.6 mm). The mini-RCM has a payload capacity of approximately 27 mN and attains a positional precision of 26.4 mu m. We demonstrate its potential utility as a precise tool for teleoperated microsurgery by performing 0.5-mm-square tracing and micro-cannulation teleoperated microsurgical procedures under a microscope. Teleoperation using the mini-RCM reduced the deviation from the desired trajectory by 68% compared to manual operation. In addition, the mini-RCM allows gravity compensation and back drivability for safety. Its compact, simple structure facilitates manufacture.																		2522-5839				AUG	2020	2	8					437	+		10.1038/s42256-020-0203-4													
J								A unified framework for integrative study of heterogeneous gene regulatory mechanisms	NATURE MACHINE INTELLIGENCE											HUMAN GENOME; ARCHITECTURE; ANNOTATION; REGIONS	Gene expression is regulated by a variety of mechanisms, which have been difficult to study in a unified way. The authors propose a flexible framework that can integrate different types of data for studying their joint effects on gene expression. The framework uses a general network representation for data integration, metapaths for inputting prior knowledge of gene regulatory mechanisms, and embedding techniques for capturing complex structures in the data. Gene expression is regulated by a large variety of mechanisms. Previous studies attempting to model the quantitative relationships between gene expression levels and regulatory mechanisms have considered only one or a few mechanisms at a time, which cannot provide a full picture of the complex interactions among different mechanisms. This was partially due to the heterogeneity of the mechanisms, which involve different types of biological objects and data representations, making it hard to study them in a unified way. Here, we describe a flexible framework that can integrate very different types of data for studying their joint effects on gene expression. In this framework, domain knowledge is represented by metapaths, while the manifestations of their effects in actual data are summarized by an embedding of the biological objects in a latent space. We demonstrate the use of our framework in integrating several diverse types of data that are related to gene expression in different ways, including DNA contacts in three-dimensional genome architecture, protein-protein interactions, genomic neighbourhoods and broad chromatin accessibility domains. The modelling results reveal that these several types of data are able to model gene expression fairly well individually, but even better when integrated.																		2522-5839				AUG	2020	2	8					447	456		10.1038/s42256-020-0205-2													
J								Minimal-uncertainty prediction of general drug-likeness based on Bayesian neural networks	NATURE MACHINE INTELLIGENCE											CHEMISTRY; OPTIMIZATION	Triaging unpromising lead molecules early in the drug discovery process is essential for accelerating its pace while avoiding the costs of unwarranted biological and clinical testing. Accordingly, medicinal chemists have been trying for decades to develop metrics-ranging from heuristic measures to machine-learning models-that could rapidly distinguish potential drugs from small molecules that lack drug-like features. However, none of these metrics has gained universal acceptance and the very idea of 'drug-likeness' has recently been put into question. Here, we evaluate drug-likeness using different sets of descriptors and different state-of-the-art classifiers, reaching an out-of-sample accuracy of 87-88%. Remarkably, because these individual classifiers yield different Bayesian error distributions, their combination and selection of minimal-variance predictions can increase the accuracy of distinguishing drug-like from non-drug-like molecules to 93%. Because total variance is comparable with its aleatoric contribution reflecting irreducible error inherent to the dataset (as opposed to the epistemic contribution due to the model itself), this level of accuracy is probably the upper limit achievable with the currently known collection of drugs. When designing new drugs, there are countless ways to create molecules, yet only a few interact with biological targets. Beker and colleagues provide here a graph neural network based metric for drug-likeness that can guide the search.																		2522-5839				AUG	2020	2	8					457	+		10.1038/s42256-020-0209-y													
J								Elucidation of DNA methylation onN(6)-adenine with deep learning	NATURE MACHINE INTELLIGENCE											N-6-ADENINE; N-6-METHYLADENINE; VARIANTS	Research on DNA methylation onN(6)-adenine (6mA) in eukaryotes has received much recent attention. Recent studies have generated a large amount of 6mA genomic data, yet the role of DNA 6mA in eukaryotes remains elusive, or even controversial. We argue that the sparsity of DNA 6mA in eukaryotes, the limitations of current biotechnologies for 6mA detection and the sophistication of the 6mA regulatory mechanism together pose great challenges for elucidation of DNA 6mA. To exploit existing 6mA genomic data and address this challenge, here we develop a deep-learning-based algorithm for predicting potential DNA 6mA sites de novo from sequence at single-nucleotide resolution, with application to three representative model organisms,Arabidopsis thaliana,Drosophila melanogasterandEscherichia coli. Extensive experiments demonstrate the accuracy of our algorithm and its superior performance compared with conventionalk-mer-based approaches. Furthermore, our saliency maps-based context analysis protocol reveals interestingcis-regulatory patterns around the 6mA sites that are missed by conventional motif analysis. Our proposed analytical tools and findings will help to elucidate the regulatory mechanisms of 6mA and benefit the in-depth exploration of their functional effects. Finally, we offer a complete catalogue of potential 6mA sites based on in silico whole-genome prediction. The role of DNA methylation onN(6)-adenine (6mA) in eukaryotes is a challenging research problem. Tan et al. develop a deep-learning-based algorithm to predict 6mA sites from sequences at single-nucleotide resolution, and apply the method to three representative model organisms. The method is further developed to visualize regulatory patterns around 6mA sites.																		2522-5839				AUG	2020	2	8					466	+		10.1038/s42256-020-0211-4													
J								Making deep neural networks right for the right scientific reasons by interacting with their explanations	NATURE MACHINE INTELLIGENCE											MACHINE; SENSORS	Deep learning approaches can show excellent performance but still have limited practical use if they learn to predict based on confounding factors in a dataset, for instance text labels in the corner of images. By using an explanatory interactive learning approach, with a human expert in the loop during training, it becomes possible to avoid predictions based on confounding factors. Deep neural networks have demonstrated excellent performances in many real-world applications. Unfortunately, they may show Clever Hans-like behaviour (making use of confounding factors within datasets) to achieve high performance. In this work we introduce the novel learning setting of explanatory interactive learning and illustrate its benefits on a plant phenotyping research task. Explanatory interactive learning adds the scientist into the training loop, who interactively revises the original model by providing feedback on its explanations. Our experimental results demonstrate that explanatory interactive learning can help to avoid Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust in the underlying model.																		2522-5839				AUG	2020	2	8					476	+		10.1038/s42256-020-0212-3													
J								Fixed-time synchronization of delayed Cohen-Grossberg neural networks based on a novel sliding mode	NEURAL NETWORKS										Cohen-Grossberg neural networks; Fixed-time synchronization; Sliding-mode surface; Discontinuous activations	FINITE-TIME; PROJECTIVE SYNCHRONIZATION; STABILIZATION; STABILITY; SYSTEMS; DESIGN	This paper has discussed fixed-time synchronization of discontinuous Cohen-Grossberg neural networks with time-varying delays and matched disturbances based on sliding mode control technology. First, a novel sliding-mode surface is established. And, the dynamics on the sliding-mode surface can be achieved in the fixed time by employing the Gudermannian function. Then, considering the effect of delay, two different control schemes are introduced to ensure the fixed time reachability of the sliding mode. In addition, some useful criteria are given out for fixed-time synchronization of neural networks, and the setting time is formulated in a straightforward way. Finally, some examples and simulations are presented to verify the validity of the proposed results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						1	12		10.1016/j.neunet.2020.04.020													
J								Common stochastic inputs induce neuronal transient synchronization with partial reset	NEURAL NETWORKS										Neural encoding; Stein's model; Poisson process; Synchronization; Common inputs; Partial reset	DELAYED NEURAL-NETWORKS; LAG SYNCHRONIZATION; MODELS; SYSTEMS; INTEGRATION; DYNAMICS; NOISE	Neuronal synchronization plays important roles in information encoding and transmission in the brain. Mathematical models of neurons have been widely used to simulate synchronization behavior and analyze its mechanisms. Common stochastic inputs are considered to be effective in facilitating synchronization. However, the mechanisms of how partial reset affects neuronal synchronization are still not well understood. In this paper, the synchronization of Stein's model neurons with partial reset is studied. The differences in synchronization mechanisms between neurons with full reset and those with partial reset are analyzed, and the findings lead to the novel concept of transient synchronization. Furthermore, it is proven analytically that due to common stochastic inputs, Stein's model neurons with different initial membrane potentials and partial reset achieve transient synchronization with probability 1. Additionally, a systematic numerical analysis is performed to explore the similarities and differences between full reset and partial reset regarding model parameters, synchronization time, and desynchronization behavior. Thus, partial reset is a powerful and flexible tool that facilitates neuronal synchronization while reserving the possibility of desynchronization. Our analysis also provides an alternative approach to analyze neurons of the integrate-and-fire family and a theoretical complement implying possible information encoding mechanisms in the brain. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						13	21		10.1016/j.neunet.2020.04.019													
J								Performance metrics for online seizure prediction	NEURAL NETWORKS										Online seizure prediction; iEEG signal; Lead seizure; Prediction horizon; Prediction period; Sensitivity	SPECTRAL POWER; EPILEPSY; SYSTEM; LONG; SVM	Many recent studies on online seizure prediction from iEEG signal describe various prediction algorithms and their prediction performance. In contrast, this paper focuses on proper specification of system parameters, such as prediction period, prediction horizon and data-driven characterization of lead seizures. Whereas prediction performance clearly depends on these system parameters many researchers simply set the values of these parameters in an ad hoc manner. Our paper investigates the effect of these system parameters on online prediction performance, using both synthetic and real life data sets. Therefore, meaningful comparison of methods/algorithms (for online seizure prediction) should consider proper specification of system parameters. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						22	32		10.1016/j.neunet.2020.04.022													
J								Theory of adaptive SVD regularization for deep neural networks	NEURAL NETWORKS										Deep networks; Adaptive regularization; Overfitting; Singular values decomposition; Matrix decomposition		Deep networks can learn complex problems, however, they suffer from overfitting. To solve this problem, regularization methods have been proposed that are not adaptable to the dynamic changes in the training process. With a different approach, this paper presents a regularization method based on the Singular Value Decomposition (SVD) that adjusts the learning model adaptively. To this end, the overfitting can be evaluated by condition numbers of the synaptic matrices. When the overfitting is high, the matrices are substituted with their SVD approximations. Some theoretical results are derived to show the performance of this regularization method. It is proved that SVD approximation cannot solve overfitting after several iterations. Thus, a new Tikhonov term is added to the loss function to converge the synaptic weights to the SVD approximation of the best-found results. Following this approach, an Adaptive SVD Regularization (ASR) is proposed to adjust the learning model with respect to the dynamic training characteristics. ASR results are visualized to show how ASR overcomes overfitting. The different configurations of Convolutional Neural Networks (CNN) are implemented with different augmentation schemes to compare ASR with state-of-the-art regularization methods. The results show that on MNIST, F-MNIST, SVHN, CIFAR-10 and CIFAR-100, the accuracies of ASR are 99.4%, 95.7%, 97.1%, 93.2% and 55.6%, respectively. Although ASR improves the overfitting and validation loss, its elapsed time is not significantly greater than the learning without regularization. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						33	46		10.1016/j.neunet.2020.04.021													
J								Automated classification of cells into multiple classes in epithelial tissue of oral squamous cell carcinoma using transfer learning and convolutional neural network	NEURAL NETWORKS										Oral squamous cell carcinoma; Deep learning; Convolution neural network; Transfer learning; Biopsy	CANCER; PATTERN; INDIA	The analysis of tissue of a tumor in the oral cavity is essential for the pathologist to ascertain its grading. Recent studies using biopsy images reveal computer-aided diagnosis for oral sub-mucous fibrosis (OSF) carried out using machine learning algorithms, but no research has yet been outlined for multi-class grading of oral squamous cell carcinoma (OSCC). Pertinently, with the advent of deep learning in digital imaging and computational aid in the diagnosis, multi-class classification of OSCC biopsy images can help in timely and effective prognosis and multi-modal treatment protocols for oral cancer patients, thus reducing the operational workload of pathologists while enhancing management of the disease. With this motivation, this study attempts to classify OSCC into its four classes as per the Broder's system of histological grading. The study is conducted on oral biopsy images applying two methods: (i) through the application of transfer learning using pre-trained deep convolutional neural network (CNN) wherein four candidate pre-trained models, namely Alexnet, VGG-16, VGG-19 and Resnet-50, were chosen to find the most suitable model for our classification problem, and (ii) by a proposed CNN model. Although the highest classification accuracy of 92.15% is achieved by Resnet-50 model, the experimental findings highlight that the proposed CNN model outperformed the transfer learning approaches displaying accuracy of 97.5%. It can be concluded that the proposed CNN based multi-class grading method of OSCC could be used for diagnosis of patients with OSCC. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						47	60		10.1016/j.neunet.2020.05.003													
J								Robust image classification against adversarial attacks using elastic similarity measures between edge count sequences	NEURAL NETWORKS										Adversarial machine learning; Deep neural networks; Time series analysis; Computer vision		Due to their unprecedented capacity to learn patterns from raw data, deep neural networks have become the de facto modeling choice to address complex machine learning tasks. However, recent works have emphasized the vulnerability of deep neural networks when being fed with intelligently manipulated adversarial data instances tailored to confuse the model. In order to overcome this issue, a major effort has been made to find methods capable of making deep learning models robust against adversarial inputs. This work presents a new perspective for improving the robustness of deep neural networks in image classification. In computer vision scenarios, adversarial images are crafted by manipulating legitimate inputs so that the target classifier is eventually fooled, but the manipulation is not visually distinguishable by an external observer. The reason for the imperceptibility of the attack is that the human visual system fails to detect minor variations in color space, but excels at detecting anomalies in geometric shapes. We capitalize on this fact by extracting color gradient features from input images at multiple sensitivity levels to detect possible manipulations. We resort to a deep neural classifier to predict the category of unseen images, whereas a discrimination model analyzes the extracted color gradient features with time series techniques to determine the legitimacy of input images. The performance of our method is assessed over experiments comprising state-of-the-art techniques for crafting adversarial attacks. Results corroborate the increased robustness of the classifier when using our discrimination module, yielding drastically reduced success rates of adversarial attacks that operate on the whole image rather than on localized regions or around the existing shapes of the image. Future research is outlined towards improving the detection accuracy of the proposed method for more general attack strategies. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						61	72		10.1016/j.neunet.2020.04.030													
J								Deep learning from label proportions with labeled samples	NEURAL NETWORKS										Learning from label proportions (LLP); Convolutional neural networks (convNets); Multi-class problem; Random sampling		Learning from label proportions (LLP), where the training data is in form of bags, and only the proportions of classes in each bag are available, has attracted wide interest in machine learning community. In general, most LLP algorithms adopt random sampling to obtain the proportional information of different categories, which correspondingly obtains some labeled samples in each bag. However, LLP training process always fails to leverage these labeled samples, which may contain essential data distribution information. To address this issue, in this paper, we propose end-to-end LLP solver based on convolutional neural networks (ConvNets), called LLP with labeled samples (LLP-LS). First, we reshape the cross entropy loss in ConvNets, so that it can combine the proportional information and labeled samples in each bag. Second, in order to comply with the training data in a bag manner, ADAM based on batch is employed to train LLP-LS. Hence, the batch size in training process is in accordance with the bag size. Compared with up-to-date methods on multi-class problem, our algorithm can obtain the state-of-the-art on several image datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						73	81		10.1016/j.neunet.2020.04.026													
J								BPGAN: Bidirectional CT-to-MRI prediction using multi-generative multi-adversarial nets with spectral normalization and localization	NEURAL NETWORKS										Bidirectional prediction; Cross modality; Generative adversarial nets; Pathological invariance; Spectral normalization	CONVOLUTIONAL NEURAL-NETWORK; ATTENUATION CORRECTION; IMAGE; PET/MRI	Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) are widely used detection technology in screening, diagnosis, and image-guided therapy for both clinical and research. However, CT imposes ionizing radiation to patients during acquisition. Compared to CT, MRI is much safer and does not involve any radiations, but it is more expensive and has prolonged acquisition time. Therefore, it is necessary to estimate one modal image from another given modal image of the same subject for the case of radiotherapy planning. Considering that there is currently no bidirectional prediction model between MRI and CT images, we propose a bidirectional prediction by using multi-generative multi-adversarial nets (BPGAN) for the prediction of any modal from another modal image in paired and unpaired fashion. In BPGAN, two nonlinear maps are learned by projecting same pathological features from one domain to another with cycle consistency strategy. Technologically, pathological prior information is introduced to constrain the feature generation to attack the potential risk of pathological variance, and edge retention metric is adopted to preserve geometrically distortion and anatomical structure. Algorithmically, spectral normalization is designed to control the performance of discriminator and to make predictor learn better and faster, and the localization is proposed to impose regularizer on predictor to reduce generalization error. Experimental results show that BPGAN generates better predictions than recently state-of-the-art methods. Specifically, BPGAN achieves average increment of MAE 33.2% and 37.4%, and SSIM 24.5% and 44.6% on two baseline datasets than comparisons. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						82	96		10.1016/j.neunet.2020.05.001													
J								Deep Multi-Critic Network for accelerating Policy Learning in multi-agent environments	NEURAL NETWORKS										Deep Multi-Critic Network; Policy Learning; Football player analysis	GO	Humans live among other humans, not in isolation. Therefore, the ability to learn and behave in multi-agent environments is essential for any autonomous system that intends to interact with people. Due to the presence of multiple simultaneous learners in a multi-agent learning environment, the Markov assumption used for single-agent environments is not tenable, necessitating the development of new Policy Learning algorithms. Recent Actor-Critic algorithms proposed for multi-agent environments, such as Multi-Agent Deep Deterministic Policy Gradients and Counterfactual Multi-Agent Policy Gradients, find a way to use the same mathematical framework as single agent environments by augmenting the Critic with extra information. However, this extra information can slow down the learning process and afflict the Critic with Curse of Dimensionality. To combat this, we propose a novel Deep Neural Network configuration called Deep Multi-Critic Network. This architecture works by taking a weighted sum over the outputs of multiple critic networks of varying complexity and size. The configuration was tested on data collected from a real-world multi-agent environment. The results illustrate that by using Deep Multi-Critic Network, less data is needed to reach the same level of performance as when not using the configuration. This suggests that as the configuration learns faster from less data, then the Critic may be able to learn Q-values faster, accelerating Actor training as well. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0893-6080	1879-2782				AUG	2020	128						97	106		10.1016/j.neunet.2020.04.023													
J								Multi-projection of unequal dimension optimal transport theory for Generative Adversary Networks	NEURAL NETWORKS										Generative adversarial networks; Unequal dimension; Multi-projection; Optimal transport		As a major step forward in machine learning, generative adversarial networks (GANs) employ the Wasserstein distance as a metric between the generative distribution and target data distribution, and thus can be viewed as optimal transport (OT) problems to reflect the underlying geometry of the probability distribution. However, the unequal dimensions between the source random distribution and the target data, result in often instability in the training processes, and lack of diversity in the generative images. To resolve the challenges, we propose here a multiple-projection approach, to project the source and target probability measures into multiple different low-dimensional subspaces. Moreover, we show that the original problem can be transformed into a variant multi-marginal OT problem, and we provide the explicit properties of the solutions. In addition, we employ parameterized approximation for the objective, and study the corresponding differentiability and convergence properties, ensuring that the problem can indeed be computed. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						107	125		10.1016/j.neunet.2020.04.029													
J								A unified robust framework for multi-view feature extraction with L2,1-norm constraint	NEURAL NETWORKS										Multi-view; Feature extraction; L21-norm; Robust feature extraction framework	CANONICAL CORRELATION-ANALYSIS; EIGENFACES	Multi-view feature extraction methods mainly focus on exploiting the consistency and complementary information between multi-view samples, and most of the current methods apply the F-norm or L2-norm as the metric, which are sensitive to the outliers or noises. In this paper, based on L2,1-norm, we propose a unified robust feature extraction framework, which includes four special multi-view feature extraction methods, and extends the state-of-art methods to a more generalized form. The proposed methods are less sensitive to outliers or noises. An efficient iterative algorithm is designed to solve L2,1-norm based methods. Comprehensive analyses, such as convergence analysis, rotational invariance analysis and relationship between our methods and previous F-norm based methods illustrate the effectiveness of our proposed methods. Experiments on two artificial datasets and six real datasets demonstrate that the proposed L2,1-norm based methods have better performance than the related methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						126	141		10.1016/j.neunet.2020.04.024													
J								Training memristor-based multilayer neuromorphic networks with SGD, momentum and adaptive learning rates	NEURAL NETWORKS										Memristor; Neural network; Adaptive learning rate	NEURAL-NETWORKS; COMPUTING SYSTEM; SYNCHRONIZATION; IMPLEMENTATION; CONVERGENCE; FPGA	Neural networks implemented with traditional hardware face inherent limitation of memory latency. Specifically, the processing units like GPUs, FPGAs, and customized ASICs, must wait for inputs to read from memory and outputs to write back. This motivates memristor-based neuromorphic computing in which the memory units (i.e., memristors) have computing capabilities. However, training a memristorbased neural network is difficult since memristors work differently from CMOS hardware. This paper proposes a new training approach that enables prevailing neural network training techniques to be applied for memristor-based neuromorphic networks. Particularly, we introduce momentum and adaptive learning rate to the circuit training, both of which are proven methods that significantly accelerate the convergence of neural network parameters. Furthermore, we show that this circuit can be used for neural networks with arbitrary numbers of layers, neurons, and parameters. Simulation results on four classification tasks demonstrate that the proposed circuit achieves both high accuracy and fast speed. Compared with the SGD-based training circuit, on the WBC data set, the training speed of our circuit is increased by 37.2% while the accuracy is only reduced by 0.77%. On the MNIST data set, the new circuit even leads to improved accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						142	149		10.1016/j.neunet.2020.04.025													
J								Synchronization criteria for quaternion-valued coupled neural networks with impulses	NEURAL NETWORKS										Quaternion-valued; Coupled neural network; Synchronization; Impulses	COMPLEX DYNAMICAL NETWORKS; TIME-VARYING DELAYS; EXPONENTIAL SYNCHRONIZATION; GLOBAL SYNCHRONIZATION; STABILITY; ARRAY; CHAOS	We consider the global exponential synchronization of a category of quaternion-valued coupled neural networks (QVCNNs) with impulses in this article. It makes up for the gap of coupled neural networks with impulses in quaternion. On account of the product of two quaternions cannot be exchanged under normal circumstances, for convenience, we isolate the QVCNN into four real-valued coupled neural networks (RVCNNs) which are converted into an augmented system by defining a new augmented vector. By leveraging a distinctive Lyapunov-Krasovskii function and some matrix inequalities, several sufficient conditions for the global exponential synchronization of the system are attained. Ultimately, two examples are used to prove the validity of the theories in this paper. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						150	157		10.1016/j.neunet.2020.04.027													
J								Impulsive synchronization of coupled delayed neural networks with actuator saturation and its application to image encryption	NEURAL NETWORKS										Coupled neural networks; Impulsive control; Exponential synchronization; Actuator saturation; Time-varying delays; Image encryption	NONLINEAR-SYSTEMS; STABILITY; OPTIMIZATION; SUBJECT	The actuator of any physical control systems is constrained by amplitude and energy, which causes the control systems to be inevitably affected by actuator saturation. In this paper, impulsive synchronization of coupled delayed neural networks with actuator saturation is presented. A new controller is designed to introduce actuator saturation term into impulsive controller. Based on sector nonlinearity model approach, impulsive controls with actuator saturation and with partial actuator saturation are studied, respectively, and some effective sufficient conditions are obtained. Numerical simulation is presented to verify the validity of the theoretical analysis results. Finally, the impulsive synchronization is applied to image encryption. The experimental results show that the proposed image encryption system has high security properties. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						158	171		10.1016/j.neunet.2020.05.016													
J								Sequential vessel segmentation via deep channel attention network	NEURAL NETWORKS										X-ray coronary angiography; Deep learning; Vessel segmentation temporal-spatial features; Channel attention blocks; Class imbalance	CORONARY-ARTERY SEGMENTATION; BLOOD-VESSELS; ACTIVE CONTOUR; IMAGES; ENHANCEMENT; REGRESSION; TRACKING; RIDGE	Accurately segmenting contrast-filled vessels from X-ray coronary angiography (XCA) image sequence is an essential step for the diagnosis and therapy of coronary artery disease. However, developing automatic vessel segmentation is particularly challenging due to the overlapping structures, low contrast and the presence of complex and dynamic background artifacts in XCA images. This paper develops a novel encoder-decoder deep network architecture which exploits the several contextual frames of 2D+t sequential images in a sliding window centered at current frame to segment 2D vessel masks from the current frame. The architecture is equipped with temporal-spatial feature extraction in encoder stage, feature fusion in skip connection layers and channel attention mechanism in decoder stage. In the encoder stage, a series of 3D convolutional layers are employed to hierarchically extract temporal-spatial features. Skip connection layers subsequently fuse the temporal-spatial feature maps and deliver them to the corresponding decoder stages. To efficiently discriminate vessel features from the complex and noisy backgrounds in the XCA images, the decoder stage effectively utilizes channel attention blocks to refine the intermediate feature maps from skip connection layers for subsequently decoding the refined features in 2D ways to produce the segmented vessel masks. Furthermore, Dice loss function is implemented to train the proposed deep network in order to tackle the class imbalance problem in the XCA data due to the wide distribution of complex background artifacts. Extensive experiments by comparing our method with other state-of-the-art algorithms demonstrate the proposed method's superior performance over other methods in terms of the quantitative metrics and visual validation. To facilitate the reproductive research in XCA community, we publicly release our dataset and source codes at https://github.com/Binjie-Qin/SVS-net. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						172	187		10.1016/j.neunet.2020.05.005													
J								Fast Haar Transforms for Graph Neural Networks	NEURAL NETWORKS										Graph Neural Networks; Haar basis; Graph convolution; Fast Haar Transforms; Geometric deep learning; Graph Laplacian		Graph Neural Networks (GNNs) have become a topic of intense research recently due to their powerful capability in high-dimensional classification and regression tasks for graph-structured data. However, as GNNs typically define the graph convolution by the orthonormal basis for the graph Laplacian, they suffer from high computational cost when the graph size is large. This paper introduces a Haar basis, which is a sparse and localized orthonormal system for a coarse-grained chain on the graph. The graph convolution under Haar basis, called Haar convolution, can be defined accordingly for GNNs. The sparsity and locality of the Haar basis allow Fast Haar Transforms (FHTs) on the graph, by which one then achieves a fast evaluation of Haar convolution between graph data and filters. We conduct experiments on GNNs equipped with Haar convolution, which demonstrates state-of-the-art results on graph-based regression and node classification tasks. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						188	198		10.1016/j.neunet.2020.04.028													
J								Analog neuron hierarchy	NEURAL NETWORKS										Recurrent neural network; Analog neuron hierarchy; Deterministic context-free language; Turing machine; Chomsky hierarchy	COMPUTATIONAL POWER; NETWORKS; COMPLEXITY	In order to refine the analysis of the computational power of discrete-time recurrent neural networks (NNs) between the binary-state NNs which are equivalent to finite automata (level 3 in the Chomsky hierarchy), and the analog-state NNs with rational weights which are Turing-complete (Chomsky level 0), we study an intermediate model alpha ANN of a binary-state NN that is extended with alpha > 0 extra analog-state neurons. For rational weights, we establish an analog neuron hierarchy 0ANNs subset of 1ANNs subset of 2ANNs subset of 3ANNs and separate its first two levels. In particular, 0ANNs coincide with the binary-state NNs (Chomsky level 3) being a proper subset of 1ANNs which accept at most context-sensitive languages (Chomsky level 1) including some non-context-free ones (above Chomsky level 2). We prove that the deterministic (context-free) language L-h= {0(n)1(n) vertical bar n > 1} cannot be recognized by any 1ANN even with real weights. In contrast, we show that deterministic pushdown automata accepting deterministic languages can be simulated by 2ANNs with rational weights, which thus constitute a proper superset of 1ANNs. Finally, we prove that the analog neuron hierarchy collapses to 3ANNs by showing that any Turing machine can be simulated by a 3ANN having rational weights, with linear-time overhead. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						199	215		10.1016/j.neunet.2020.05.006													
J								Special T-Net: Nested encoder-decoder architecture for the main vessel segmentation in coronary angiography	NEURAL NETWORKS										Convolutional neural network; Main vessel segmentation; Coronary angiography; Encoder and decoder		In this paper, we proposed nested encoder-decoder architecture named T-Net. T-Net consists of several small encoder-decoders for each block constituting convolutional network. T-Net overcomes the limitation that U-Net can only have a single set of the concatenate layer between encoder and decoder block. To be more precise, the U-Net symmetrically forms the concatenate layers, so the low-level feature of the encoder is connected to the latter part of the decoder, and the high-level feature is connected to the beginning of the decoder. T-Net arranges the pooling and up-sampling appropriately during the encoding process, and likewise during the decoding process so that feature maps of various sizes are obtained in a single block. As a result, all features from the low-level to the high-level extracted from the encoder are delivered from the beginning of the decoder to predict a more accurate mask. We evaluated T-Net for the problem of segmenting three main vessels in coronary angiography images. The experiment consisted of a comparison of U-Net and T-Nets under the same conditions, and an optimized T-Net for the main vessel segmentation. As a result, T-Net recorded a Dice Similarity Coefficient score (DSC) of 83.77%, 10.69% higher than that of U-Net, and the optimized T-Net recorded a DSC of 88.97% which was 15.89% higher than that of U-Net. In addition, we visualized the weight activation of the convolutional layer of T-Net and U-Net to show that T-Net actually predicts the mask from earlier decoders. Therefore, we expect that T-Net can be effectively applied to other similar medical image segmentation problems. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						216	233		10.1016/j.neunet.2020.05.002													
J								Embedding and approximation theorems for echo state networks	NEURAL NETWORKS										Reservoir computing; Lorenz equations; Dynamical system; Delay embedding; Persistent homology; Recurrent neural networks	TOPOLOGY; MACHINE	Echo State Networks (ESNs) are a class of single-layer recurrent neural networks that have enjoyed recent attention. In this paper we prove that a suitable ESN, trained on a series of measurements of an invertible dynamical system, induces a C1 map from the dynamical system's phase space to the ESN's reservoir space. We call this the Echo State Map. We then prove that the Echo State Map is generically an embedding with positive probability. Under additional mild assumptions, we further conjecture that the Echo State Map is almost surely an embedding. For sufficiently large, and specially structured, but still randomly generated ESNs, we prove that there exists a linear readout layer that allows the ESN to predict the next observation of a dynamical system arbitrarily well. Consequently, if the dynamical system under observation is structurally stable then the trained ESN will exhibit dynamics that are topologically conjugate to the future behaviour of the observed dynamical system. Our theoretical results connect the theory of ESNs to the delay-embedding literature for dynamical systems, and are supported by numerical evidence from simulations of the traditional Lorenz equations. The simulations confirm that, from a one dimensional observation function, an ESN can accurately infer a range of geometric and topological features of the dynamics such as the eigenvalues of equilibrium points, Lyapunov exponents and homology groups. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						234	247		10.1016/j.neunet.2020.05.013													
J								Graph transform learning	NEURAL NETWORKS										Transform learning; Graphical model; Clustering; Signal processing	ALGORITHM	Transform learning is a new representation learning framework where we learn an operator/transform that analyses the data to generate the coefficient/representation. We propose a variant of it called the graph transform learning; in this we explicitly account for the correlation in the dataset in terms of graph Laplacian. We will give two variants; in the first one the graph is computed from the data and fixed during the operation. In the second, the graph is learnt iteratively from the data during operation. The first technique will be applied for clustering, and the second one for solving inverse problems. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						248	253		10.1016/j.neunet.2020.05.020													
J								Generalized Guerra's interpolation schemes for dense associative neural networks	NEURAL NETWORKS										Associative neural networks; Statistical mechanics; PDE-theory; Hebbian learning; Pattern recognition	STATISTICAL-MECHANICS; HOPFIELD MODEL; GIBBS-STATES; PATTERNS; REGIME	In this work we develop analytical techniques to investigate a broad class of associative neural networks set in the high-storage regime. These techniques translate the original statistical-mechanical problem into an analytical-mechanical one which implies solving a set of partial differential equations, rather than tackling the canonical probabilistic route. We test the method on the classical Hopfield model - where the cost function includes only two-body interactions (i.e., quadratic terms) - and on the "relativistic"Hopfield model - where the (expansion of the) cost function includes p-body (i.e., of degree p) contributions. Under the replica symmetric assumption, we paint the phase diagrams of these models by obtaining the explicit expression of their free energy as a function of the model parameters (i.e., noise level and memory storage). Further, since for non-pairwise models ergodicity breaking is non necessarily a critical phenomenon, we develop a fluctuation analysis and find that criticality is preserved in the relativistic model. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						254	267		10.1016/j.neunet.2020.05.009													
J								Accurate and efficient sequential ensemble learning for highly imbalanced multi-class data	NEURAL NETWORKS										Sequential ensemble learning; Multi-class classification; Highly imbalanced data	CLASSIFICATION; MACHINE; PERFORMANCE	Multi-class classification for highly imbalanced data is a challenging task in which multiple issues must be resolved simultaneously, including (i) accuracy on classifying highly imbalanced multi-class data; (ii) training efficiency for large data; and (iii) sensitivity to high imbalance ratio (IR). In this paper, a novel sequential ensemble learning (SEL) framework is designed to simultaneously resolve these issues. SEL framework provides a significant property over traditional AdaBoost, in which the majority samples can be divided into multiple small and disjoint subsets for training multiple weak learners without compromising accuracy (while AdaBoost cannot). To ensure the class balance and majority-disjoint property of subsets, a learning strategy called balanced and majority-disjoint subsets division (BMSD) is developed. Unfortunately it is difficult to derive a general learner combination method (LCM) for any kind of weak learner. In this work, LCM is specifically designed for extreme learning machine, called LCM-ELM. The proposed SEL framework with BMSD and LCM-ELM has been compared with state-of-the-art methods over 16 benchmark datasets. In the experiments, under highly imbalanced multi-class data (IR up to 14K; data size up to 493K), (i) the proposed works improve the performance in different measures including G-mean, macro-F, micro-F, MAUC; (ii) training time is significantly reduced. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						268	278		10.1016/j.neunet.2020.05.010													
J								Uni-image: Universal image construction for robust neural model	NEURAL NETWORKS										Adversarial machine learning; Uni-Image Procedure; Defense technique; Semantic adversarial example; Image classification		Deep neural networks have shown high performance in prediction, but they are defenseless when they predict on adversarial examples which are generated by adversarial attack techniques. In image classification, those attack techniques usually perturb the pixel of an image to fool the deep neural networks. To improve the robustness of the neural networks, many researchers have introduced several defense techniques against those attack techniques. To the best of our knowledge, adversarial training is one of the most effective defense techniques against the adversarial examples. However, the defense technique could fail against a semantic adversarial image that performs arbitrary perturbation to fool the neural networks, where the modified image semantically represents the same object as the original image. Against this background, we propose a novel defense technique, Uni-Image Procedure (UIP) method. UIP generates a universal-image (uni-image) from a given image, which can be a clean image or a perturbed image by some attacks. The generated uni-image preserves its own characteristics (i.e. color) regardless of the transformations of the original image. Note that those transformations include inverting the pixel value of an image, modifying the saturation, hue, and value of an image, etc. Our experimental results using several benchmark datasets show that our method not only defends well known adversarial attacks and semantic adversarial attack but also boosts the robustness of the neural network. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						279	287		10.1016/j.neunet.2020.05.018													
J								Generalized norm for existence, uniqueness and stability of Hopfield neural networks with discrete and distributed delays	NEURAL NETWORKS										Hopfield neural networks; Exponential stability; xi-norm; Discrete-distributed delays	GLOBAL EXPONENTIAL STABILITY; SYSTEMS	In this paper, the existence, uniqueness and stability criteria of solutions for Hopfield neural networks with discrete and distributed delays (DDD HNNs) are investigated by the definitions of three kinds of generalized norm (xi-norm). A general DDD HNN model is firstly introduced, where the discrete delays tau(pq)(t) are asynchronous time-varying delays. Then, {xi, 1}-norm, {xi, 2}-norm and {xi, infinity}-norm are successively used to derive the existence, uniqueness and stability criteria of solutions for the DDD HNNs. In the proof of theorems, special functions and assumptions are given to deal with discrete and distributed delays. Furthermore, a corollary is concluded for the existence and stability criteria of solutions. The methods given in this paper can also be used to study the synchronization and mu-stability of different DDD NNs. Finally, two numerical examples and their simulation figures are given to illustrate the effectiveness of these results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						288	293		10.1016/j.neunet.2020.05.014													
J								Cross-modality paired-images generation and augmentation for RGB-infrared person re-identification	NEURAL NETWORKS										Person re-identification; Cross-modality; Feature disentanglement; Image generation; Adversarial learning	NETWORKS	RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. Considering no correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing marginal distribution divergence between the entire RGB and IR sets. However, this set-level alignment strategy may lead to misalignment of some instances, which limit the performance for RGB-IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged features. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Third, our method learns a latent manifold space. In the space, we can random sample and generate lots of images of unseen classes. Training with those images, the learned identity feature space is more smooth can generalize better when test. Finally, extensive experimental results on two standard benchmarks demonstrate that the proposed model favorably against state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						294	304		10.1016/j.neunet.2020.05.008													
J								Sparsity through evolutionary pruning prevents neuronal networks from overfitting	NEURAL NETWORKS										Evolution; Artificial neural networks; Maze task; Evolutionary algorithm; Overfitting; Biological plausibility	PATH-INTEGRATION; DESERT ANTS; CATAGLYPHIS; CONNECTOME; MICROGLIA; FEATURES; SYSTEM; EDGE; MAP	Modern Machine learning techniques take advantage of the exponentially rising calculation power in new generation processor units. Thus, the number of parameters which are trained to solve complex tasks was highly increased over the last decades. However, still the networks fail - in contrast to our brain - to develop general intelligence in the sense of being able to solve several complex tasks with only one network architecture. This could be the case because the brain is not a randomly initialized neural network, which has to be trained from scratch by simply investing a lot of calculation power, but has from birth some fixed hierarchical structure. To make progress in decoding the structural basis of biological neural networks we here chose a bottom-up approach, where we evolutionarily trained small neural networks in performing a maze task. This simple maze task requires dynamic decision making with delayed rewards. We were able to show that during the evolutionary optimization random severance of connections leads to better generalization performance of the networks compared to fully connected networks. We conclude that sparsity is a central property of neural networks and should be considered for modern Machine learning approaches. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0893-6080	1879-2782				AUG	2020	128						305	312		10.1016/j.neunet.2020.05.007													
J								Approximation rates for neural networks with general activation functions	NEURAL NETWORKS										Approximation theory; Stratified sampling; Neural networks	FEEDFORWARD NETWORKS; BOUNDS	We prove some new results concerning the approximation rate of neural networks with general activation functions. Our first result concerns the rate of approximation of a two layer neural network with a polynomially-decaying non-sigmoidal activation function. We extend the dimension independent approximation rates previously obtained to this new class of activation functions. Our second result gives a weaker, but still dimension independent, approximation rate for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activation function. Finally, we show that a stratified sampling approach can be used to improve the approximation rate for polynomially decaying activation functions under mild additional assumptions. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						313	321		10.1016/j.neunet.2020.05.019													
J								Regularized least squares locality preserving projections with applications to image recognition	NEURAL NETWORKS										Locality preserving projection; Dimensionality reduction; Small-sample-size problem; Regularized least squares	DIMENSIONALITY REDUCTION; DISCRIMINANT-ANALYSIS; ALGORITHM; EIGENFACES; EIGENMAPS; ARNOLDI	Locality preserving projection (LPP), as a well-known technique for dimensionality reduction, is designed to preserve the local structure of the original samples which usually lie on a low-dimensional manifold in the real world. However, it suffers from the undersampled or small-sample-size problem, when the dimension of the features is larger than the number of samples which causes the corresponding generalized eigenvalue problem to be ill-posed. To address this problem, we show that LPP is equivalent to a multivariate linear regression under a mild condition, and establish the connection between LPP and a least squares problem with multiple columns on the right-hand side. Based on the developed connection, we propose two regularized least squares methods for solving LPP. Experimental results on real-world databases illustrate the performance of our methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						322	330		10.1016/j.neunet.2020.05.023													
J								Real-time multiple spatiotemporal action localization and prediction approach using deep learning	NEURAL NETWORKS										Deep learning; Action localization; Action prediction; Spatiotemporal; YOLO network; Optical flow		Detecting the locations of multiple actions in videos and classifying them in real-time are challenging problems termed "action localization and prediction" problem. Convolutional neural networks (ConvNets) have achieved great success for action localization and prediction in still images. A major advance occurred when the AlexNet architecture was introduced in the ImageNet competition. ConvNets have since achieved state-of-the-art performances across a wide variety of machine vision tasks, including object detection, image segmentation, image classification, facial recognition, human pose estimation, and tracking. However, few works exist that address action localization and prediction in videos. The current action localization research primarily focuses on the classification of temporally trimmed videos in which only one action occurs per frame. Moreover, nearly all the current approaches work only offline and are too slow to be useful in real-world environments. In this work, we propose a fast and accurate deep-learning approach to perform real-time action localization and prediction. The proposed approach uses convolutional neural networks to localize multiple actions and predict their classes in real time. This approach starts by using appearance and motion detection networks (known as "you only look once" (YOLO) networks) to localize and classify actions from RGB frames and optical flow frames using a two-stream model. We then propose a fusion step that increases the localization accuracy of the proposed approach. Moreover, we generate an action tube based on frame level detection. The frame by frame processing introduces an early action detection and prediction with top performance in terms of detection speed and precision. The experimental results demonstrate this superiority of our proposed approach in terms of both processing time and accuracy compared to recent offline and online action localization and prediction approaches on the challenging UCF-101-24 and J-HMDB-21 benchmarks. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						331	344		10.1016/j.neunet.2020.05.017													
J								Progressive learning: A deep learning framework for continual learning	NEURAL NETWORKS										Continual learning; Computer vision; Deep learning; Machine learning; Neural networks; Speech recognition	RECOGNITION	Continual learning is the ability of a learning system to solve new tasks by utilizing previously acquired knowledge from learning and performing prior tasks without having significant adverse effects on the acquired prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Progressive learning is a deep learning framework for continual learning that comprises three procedures: curriculum, progression, and pruning. The curriculum procedure is used to actively select a task to learn from a set of candidate tasks. The progression procedure is used to grow the capacity of the model by adding new parameters that leverage parameters learned in prior tasks, while learning from data available for the new task at hand, without being susceptible to catastrophic forgetting. The pruning procedure is used to counteract the growth in the number of parameters as further tasks are learned, as well as to mitigate negative forward transfer, in which prior knowledge unrelated to the task at hand may interfere and worsen performance. Progressive learning is evaluated on a number of supervised classification tasks in the image recognition and speech recognition domains to demonstrate its advantages compared with baseline methods. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using a smaller number of dedicated parameters. y(C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				AUG	2020	128						345	357		10.1016/j.neunet.2020.05.011													
J								Iterative Algorithm for Solving Scalar Fractional Differential Equations with Riemann-Liouville Derivative and Supremum	ALGORITHMS										Riemann-Liouville fractional derivative; supremum; approximate solutions		The initial value problem for a special type of scalar nonlinear fractional differential equation with a Riemann-Liouville fractional derivative is studied. The main characteristic of the equation is the presence of the supremum of the unknown function over a previous time interval. This type of equation is difficult to be solved explicitly and we need approximate methods for its solving. In this paper, initially, mild lower and mild upper solutions are defined. Then, based on these definitions and the application of the monotone-iterative technique, we present an algorithm for constructing two types of successive approximations. Both sequences are monotonically convergent from above and from below, respectively, to the mild solutions of the given problem. The suggested iterative scheme is applied to particular problems to illustrate its application.																		1999-4893				AUG	2020	13	8							184	10.3390/a13080184													
J								Machine Learning-Guided Dual Heuristics and New Lower Bounds for the Refueling and Maintenance Planning Problem of Nuclear Power Plants	ALGORITHMS										operations research; mixed integer programming; stochastic optimization; dual bounds; dual heuristics; hybrid heuristics; matheuristics; machine learning; EURO/ROADEF Challenge 2010; maintenance planning; nuclear power plants; power generation	UNIT COMMITMENT PROBLEM; METAHEURISTICS; AGGREGATION; ALGORITHM	This paper studies the hybridization of Mixed Integer Programming (MIP) with dual heuristics and machine learning techniques, to provide dual bounds for a large scale optimization problem from an industrial application. The case study is the EURO/ROADEF Challenge 2010, to optimize the refueling and maintenance planning of nuclear power plants. Several MIP relaxations are presented to provide dual bounds computing smaller MIPs than the original problem. It is proven how to get dual bounds with scenario decomposition in the different 2-stage programming MILP formulations, with a selection of scenario guided by machine learning techniques. Several sets of dual bounds are computable, improving significantly the former best dual bounds of the literature and justifying the quality of the best primal solution known.																		1999-4893				AUG	2020	13	8							185	10.3390/a13080185													
J								Pavement Defect Segmentation in Orthoframes with a Pipeline of Three Convolutional Neural Networks	ALGORITHMS										pavement distress; digital image; Deep Learning; transfer learning; Convolutional Neural Network; active learning		In the manuscript, the issue of detecting and segmenting out pavement defects on highway roads is addressed. Specifically, computer vision (CV) methods are developed and applied to the problem based on deep learning of convolutional neural networks (ConvNets). A novel neural network structure is considered, based on a pipeline of three ConvNets and endowed with the capacity for context awareness, which improves grid-based search for defects on orthoframes by considering the surrounding image content-an approach, which essentially draws inspiration from how humans tend to solve the task of image segmentation. Also, methods for assessing the quality of segmentation are discussed. The contribution also describes the complete procedure of working with pavement defects in an industrial setting, involving the workcycle of defect annotation, ConvNet training and validation. The results of ConvNet evaluation provided in the paper hint at a successful implementation of the proposed technique.																		1999-4893				AUG	2020	13	8							198	10.3390/a13080198													
J								Two-Component Bayesian Hierarchical Models for Cost-Benefit Analysis of Traffic Barrier Crash Count	ALGORITHMS										Bayesian hierarchical; machine learning; zero inflated model; hurdle model; cost-benefit analysis	REGRESSION; INFERENCE	Road departure crashes tend to be hazardous, especially in rural areas like Wyoming. Traffic barriers could be installed to mitigate the severity of those crashes. However, the severity of traffic barriers crashes still persists. Besides various drivers and environmental characteristics, the roadways and barrier geometric characteristics play a critical role in the severity of barrier crashes. The Wyoming department of transportation (WYDOT) has initiated a project to identify and optimize the heights of those barriers that are below the design standard, while prioritizing them based on the monetary benefit. This is to optimize first barriers that need an immediate attention, considering the limited budget, and then all other barriers being under design. In order to account for both aspects of frequency and severity of crashes, equivalent property damage only (EPDO) was considered. The data of this type besides having an over-dispersion, exhibits excess amounts of zeroes. Thus, a two-component model was employed to provide a flexible way of addressing this problem. Beside this technique, one-component hierarchical modeling approach was considered for a comparison purpose. This paper presents an empirical cost-benefit analysis based on Bayesian hierarchical machine learning techniques. After identifying the best model in terms of the performance, deviance information criterion (DIC), the results were converted into an equation, and the equation was used for a purpose of machine learning technique. An automated method generated cost based on barriers' current conditions, and then based on optimized barrier heights. The empirical analysis showed that cost-sensitive modeling and machine learning technique deployment could be used as an effective way for cost-benefit analysis. That could be achieved through measuring the associated costs of barriers' enhancements, added benefits over years and consequently, barrier prioritization due to lack of available budget. A comprehensive discussion across the two-component models, zero-inflated and hurdle, is included in the manuscript.																		1999-4893				AUG	2020	13	8							179	10.3390/a13080179													
J								On a Nonsmooth Gauss-Newton Algorithms for Solving Nonlinear Complementarity Problems	ALGORITHMS										Gauss-Newton method; nonsmooth equations; nonsmooth optimization; nonlinear complementarity problem; B-differential; superlinear convergence; global convergence	CONVERGENCE ANALYSIS; EQUATIONS	In this paper, we propose a new version of the generalized damped Gauss-Newton method for solving nonlinear complementarity problems based on the transformation to the nonsmooth equation, which is equivalent to some unconstrained optimization problem. The B-differential plays the role of the derivative. We present two types of algorithms (usual and inexact), which have superlinear and global convergence for semismooth cases. These results can be applied to efficiently find all solutions of the nonlinear complementarity problems under some mild assumptions. The results of the numerical tests are attached as a complement of the theoretical considerations.																		1999-4893				AUG	2020	13	8							190	10.3390/a13080190													
J								Node Placement Optimization of Wireless Sensor Networks Using Multi-Objective Adaptive Degressive Ary Number Encoded Genetic Algorithm	ALGORITHMS										wireless sensor networks; node placement; a fast non-dominated sorted genetic algorithm (NSGA2); degressive ary number; adaptive	DEPLOYMENT; COVERAGE	The wireless sensor network (WSN) has the advantages of low cost, high monitoring accuracy, good fault tolerance, remote monitoring and convenient maintenance. It has been widely used in various fields. In the WSN, the placement of node sensors has a great impact on its coverage, energy consumption and some other factors. In order to improve the convergence speed of a node placement optimization algorithm, the encoding method is improved in this paper. The degressive ary number encoding is further extended to a multi-objective optimization problem. Furthermore, the adaptive changing rule of ary number is proposed by analyzing the experimental results of theN-ary number encoded algorithm. Then a multi-objective optimization algorithm adopting the adaptive degressive ary number encoding method has been used in optimizing the node placement in WSN. The experiments show that the proposed adaptive degressive ary number encoded algorithm can improve both the optimization effect and search efficiency when solving the node placement problem.																		1999-4893				AUG	2020	13	8							189	10.3390/a13080189													
J								PSO for Fuzzy Clustering of Multi-view Relational Data	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Cluster analysis; multi-view relational data; fuzzy clustering; PSO	C-MEANS; SWARM	Particle Swarm Optimization (PSO) is a population-based meta-heuristic known for its simplicity, being successfully used in clustering task with interesting performance. Clustering of multi-view data sets has received increasing attention since it explores multiple sources or views of data sets aiming at improving clustering accuracy. Previous studies mainly focused on PSO-based clustering of single-view vector data, neither single- nor multi-view PSO-based clustering of relational received proper attention. This paper introduces a PSO-based approach to the fuzzy clustering of multi-view relational data, which can cluster data sets described by several dissimilarity matrices, each of them representing a particular view. In this work, ten fitness functions were considered, in which eight of them were adapted to deal with multi-view relational data and to consider the relevance weights of views. These fitness functions were compared to evaluate which best fit to cluster multi-view relational data. The performance and usefulness of the proposed approach, in comparison with previous single- and multi-view relational fuzzy clustering algorithms, are illustrated with several multi-view data sets. The Adjusted Rand Index (ARI) and F-measure were used to assess the quality of fuzzy partitions provided by clustering algorithms. The results have shown that the proposed methods significantly outperformed the compared algorithms in the majority of cases.																	0218-0014	1793-6381				AUG	2020	34	9							2050022	10.1142/S0218001420500226													
J								Practical Considerations on Nonparametric Methods for Estimating Intrinsic Dimensions of Nonlinear Data Structures	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Intrinsic dimension; projection technique; correlation dimension; charting manifold; maximum likelihood estimation; fractal dimension	PRINCIPAL COMPONENTS; FAULT-DIAGNOSIS; NUMBER; LIMITATIONS; DISTANCE; PCA	This paper develops readily applicable methods for estimating the intrinsic dimension of multi-variate datasets. The proposed methods, which make use of theoretical properties of the empirical distribution functions of (pairwise or pointwise) distances, build on the existing concepts of (i) correlation dimensions and (ii) charting manifolds that are contrasted with (iii) a maximum likelihood technique and (iv) other recently proposed geometric methods including MiND and IDEA. This comparison relies on application studies involving simulated examples, a recorded dataset from a glucose processing facility, as well as several benchmark datasets available from the literature. The performance of the proposed techniques is generally in line with other dimension estimators, specifically noting that the correlation dimension variants perform favorably to the maximum likelihood method in terms of accuracy and computational efficiency.																	0218-0014	1793-6381				AUG	2020	34	9							2058010	10.1142/S0218001420580100													
J								Semantic Analysis in Soccer Videos Using Support Vector Machine	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Audio segmentation; event detection; video semantic analysis; soccer video analysis; support vector machine; shoot boundary detection; video semantic analysis	EVENT DETECTION; BAYESIAN NETWORK; ANNOTATION; FRAMEWORK	A tremendous increase in the video content uploaded on the internet has made it necessary for auto-recognition of videos in order to analyze, moderate or categorize certain content that can be accessed easily later on. Video analysis requires the study of proficient methodologies at the semantic level in order to address the issues such as occlusions, changes in illumination, noise, etc. This paper is aimed at the analysis of the soccer videos and semantic processing as an application in the video semantic analysis field. This study proposes a framework for automatically generating and annotating the highlights from a soccer video. The proposed framework identifies the interesting clips containing possible scenes of interest, such as goals, penalty kicks, etc. by parsing and processing the audio/video components. The framework analyzes, separates and annotates the individual scenes inside the video clips and saves using kernel support vector machine. The results show that semantic analysis of videos using kernel support vector machines is a reliable method to separate and annotate events of interest in a soccer game.																	0218-0014	1793-6381				AUG	2020	34	9							2055018	10.1142/S0218001420550186													
J								Human Behavior Recognition Based on Motion Data Analysis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										KGA algorithm; fall detection; threshold-based; three-axis accelerometer		The development of sensor technologies and smart devices has made it possible to realize real-time data acquisition of human beings. Human behavior monitoring is the process of obtaining activity information with wearables and computer technology. In this paper, we design a data preprocessing method based on the data collected by a single three-axis accelerometer. We first use Butterworth filter as low-pass filtering to remove the noise. Then, we propose a KGA algorithm to remove abnormal data and smooth them at the same time. This method uses genetic algorithm to optimize the parameters of Kalman filter. After that, we use a threshold-based method to identify falls that are harmful to the elderly. The key point of this method is to distinguish falls from people's daily activities. According to the characteristics of human falls, we extract eigenvalues that can effectively distinguish daily activities from falls. In addition, we use cross-validation to determine the threshold of the method. The results show that in the analysis of 11 kinds of human daily activities and 15 types of falls, our method can distinguish 15 types of falls. The recognition recall rate in our method reaches 99.1%.																	0218-0014	1793-6381				AUG	2020	34	9							2056005	10.1142/S0218001420560054													
J								A Knowledge-Based System for Questionnaires Evaluation of Digital Piano Collective Course for Preschool Education Major in Normal Universities	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Knowledge-based system; preschool education; digital piano; digital piano collective course; piano teaching	EXPERT-SYSTEM	In order to improve the teaching level of digital piano collective course for preschool education major in normal universities and provide high-quality music education for children, a knowledge-based system for questionnaires evaluation of digital piano collective course for preschool education major in normal universities is proposed. The system can collect sufficient data and information, the exact orientation of piano learning, the exact choice of teaching mode by questionnaires. A comprehensive analysis of the students' characteristics, teachers' post skills and the orientation of digital piano syllabus in the preschool collective course teaching of digital piano in normal universities is made. Moreover, some countermeasures for perfecting digital piano teaching mode are proposed. As a result, it promotes the level of piano teaching in preschool education major, realizes the goal of arousing students' enthusiasm for study and reduces teachers' pressure.																	0218-0014	1793-6381				AUG	2020	34	9							2059032	10.1142/S0218001420590326													
J								A Method of Interest Degree Mining Based on Behavior Data Analysis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Numerical analysis; user social media application preferences; big data; interest degree mining analysis	SYSTEM	Based on big data, this paper starts from the behavior data of users on social media, and studies and explores the core issues of user modeling under personalized services. Focusing on the goal of user interest modeling, this paper proposes corresponding improvement measures for the existing interest model, which has great difference in interest description among different users and it is difficult to find the user interest change in time. For the above problems, this paper takes user-generated content and user behavior information as the analysis object, and uses natural language processing, knowledge warehouse, data fusion and other methods and techniques to numerically analyze user interest mining based on text mining and multi-source data fusion. We propose a user interest label space mapping method to avoid data sparse problem caused by too many dimensions in interest analysis. At the same time, we propose a method to extract and blend the long-term and short-term interests, and realize the comprehensive evaluation of interests. In the analysis of the big data phase, the user preference social property application preference value law, it is expected to achieve user Internet social media application preference data mining from the perspective of big data.																	0218-0014	1793-6381				AUG	2020	34	9							2059030	10.1142/S0218001420590302													
J								Analysis of Geomagnetic Signals Based on Wavelet Transform and Fractal Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Earthquake; geomagnetic field; signal processing; wavelet transform; multiresolution analysis; fractal analysis	EARTHQUAKE	In this paper, in order to figure out the variations before an earthquake and extract abnormal signals related to it, the geomagnetic three component Z, H, F minute values of 15 geomagnetic stations within 600 km of the epicentral distance before the MS 6.6 Minxian-Zhangxian earthquake in Gansu were analyzed. The following are the results. (1) After the fractal analysis was used directly, only three geomagnetic stations in 15 geomagnetic stations showed synchronous anomalous signals; (2) After the method of this paper was used, 9 of the 15 geomagnetic stations (including the three stations in the first point) extracted two synchronous anomalous signals, and six of the nine geomagnetic stations presented additional synchronous anomalous signals. (3) Of the three abnormal signals extracted, one had a medium-term effect and two had short-term effects. (4) The anomalous duration of the Z component of nine geomagnetic stations was longer than that of H and F. And as the epicentral distance increased, duration decreased. While the proposed method could not clearly indicate the exact relationship between the anomalous signals and the earthquake, it was proved that the signals extracted are effective and well-correlative to the earthquake.																	0218-0014	1793-6381				AUG	2020	34	9							2058011	10.1142/S0218001420580112													
J								A New Date-Balanced Method Based on Adaptive Asymmetric and Diversity Regularization in Person Re-Identification	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Adaptive asymmetric; diversity regularization; metric learning; person re-identification; data imbalance		Person re-identification (person re-ID) is a challenging task which aims at spotting same persons among disjoint camera views. It has certainly generated a lot of attention in the field of computer vision, but it remains a challenging task due to the complexity of person appearances from different camera views. To solve this challenging problem, many excellent methods have been proposed, especially metric learning-based algorithms. However, most of them suffer from the problem of data imbalance. To solve this problem, in the paper we proposed a new data-balanced method and named it Enhanced Metric Learning (EML) based on adaptive asymmetric and diversity regularization for person re-ID. Metric learning is important for person re-ID because it can eliminate the negative effects caused by camera differences to a certain extent. But most metric learning approaches often neglect the problem of data imbalance caused by too many negative samples but few positive samples. And they often treat all negative samples the same as positive ones, which can lead to the loss of important information. Our approach pays different attention to the positive samples and negative ones. Firstly, we classified negative samples into three groups adaptively, and then paid different attention to them using adaptive asymmetric strategy. By treating samples differently, the proposed method can better exploit the discriminative information between positive and negative samples. Furthermore, we also proposed to impose a diversity regularizer to avoid over-fitting when the training sets are small or medium-sized. Finally, we designed a series of experiments on four challenging databases (VIPeR, PRID450S, CIJEK01 and GRID), to compare with some excellent metric learning methods. Experimental results show that the rank-1 matching rate of the proposed method has outperformed the state-of-the-art by 3.64%, 4.2%, 3.13% and 2.83% on the four databases, respectively.																	0218-0014	1793-6381				AUG	2020	34	9							2056004	10.1142/S0218001420560042													
J								Key Algorithms of Video Target Detection and Recognition in Intelligent Transportation Systems	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Target detection; target recognition; intelligent traffic; adaptive fuzzy estimation		With the popularization of video detection and recognition systems and the advancement of video image processing technology, the application research of intelligent transportation systems based on computer vision technology has received more and more attention. It comprehensively utilizes image processing, pattern recognition, artificial intelligence and other technologies. It also involves processing and analyzing the video image sequence collected by the detection system, intelligently understanding the video content and making processing, and dealing with various problems such as accident information judgment, pedestrian and vehicle classification, traffic flow parameter detection, and moving target tracking. It promotes intelligent transportation systems to be more intelligent and practical, and provides comprehensive, real-time traffic status information for traffic management and control. Therefore, the research on the method of traffic information detection based on computer vision has important theoretical and practical significance. The detection and recognition of video targets is an important research direction in the field of intelligent transportation and computer vision. However, due to the background complexity, illumination changes, target occlusion and other factors in the detection and recognition environment, the application still faces many difficulties, and the robustness and accuracy of detection and recognition need to be further improved. In this paper, several key problems in video object detection and recognition are studied, including accurate segmentation of target and background, shadow in complex scenes; accurate classification of extracted foreground targets; and target recognition in complex background. In response to these problems, this paper proposes a corresponding solution.																	0218-0014	1793-6381				AUG	2020	34	9							2055016	10.1142/S0218001420550162													
J								Enhancing Machine Learning Aptitude Using Significant Cluster Identification for Augmented Image Refining	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Clustering; dictionary learning; homomorphic filter; orthogonal matching pursuit; sparse modeling; speckle noise	SIGNAL RECOVERY; DICTIONARY; FRAMEWORK; SINGLE; MODEL	Enhancing the image to remove noise, preserving the useful features and edges are the most important tasks in image analysis. In this paper, Significant Cluster Identification for Maximum Edge Preservation (SCI-MEP), which works in parallel with clustering algorithms and improved efficiency of the machine learning aptitude, is proposed. Affinity propagation (AP) is a base method to obtain clusters from a learnt dictionary, with an adaptive window selection, which are then refined using SCI-MEP to preserve the semantic components of the image. Since only the significant clusters are worked upon, the computational time drastically reduces. The flexibility of SCI-MEP allows it to be integrated with any clustering algorithm to improve its efficiency. The method is tested and verified to remove Gaussian noise, rain noise and speckle noise from images. Our results have shown that SCI-MEP considerably optimizes the existing algorithms in terms of performance evaluation metrics.																	0218-0014	1793-6381				AUG	2020	34	9							2051009	10.1142/S021800142051009X													
J								A Light-Weight Change Detection Method Using YCbCr-Based Texture Consensus Model	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Motion detection; background subtraction; sample-consensus	BACKGROUND SUBTRACTION; MOTION DETECTION; MIXTURE; SEGMENTATION; ALGORITHMS; ENSEMBLE	Background subtraction is a prerequisite and often the very first step employed in several high-level and real-time computer vision applications. Several parametric and non-parametric change detection algorithms employing multiple feature spaces have been proposed to date but none has proven to be robust against all challenges that can possibly be posed in a complex real-time environment. Amongst the varied challenges posed, illumination variations, shadows, dynamic backgrounds, camouflaged and bootstrapping artifacts are some of the well-known problems. This paper presents a light-weight hybrid change detection algorithm that integrates a novel combination of RGB color space and conditional YCbCr-based XCS-LBP texture descriptors (YXCS-LBP) into a modified pixel-based background model. The conditional employment of light-weight YXCS-LBP texture features with the modified Visual background extractor (ViBe) aiming at reduction in false positives, produces outperforming results without incurring much memory and computational cost. The random and time-subsampled update strategy employed with the proposed classification procedure ensures the efficient suppression of shadows and bootstrapping artifacts along with the complete retention of long-term static objects in the foreground masks. Comprehensive performance analysis of the proposed technique on publicly available Change Detection dataset (2014 CDnet dataset) demonstrates the superiority of the proposed technique over different state-of-the-art-methods against varied challenges.																	0218-0014	1793-6381				AUG	2020	34	9							2050023	10.1142/S0218001420500238													
J								SRDT: A Novel Robust RGB-D Tracker Based on Siamese Region Proposal Network and Depth Information	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										RGB-D tracking; RGB-D; Siamese Network; Region Proposal Network; occlusion handler		Visual tracking is still a challenging fundamental task in the field of computer vision, especially in complex scenes such as long-term occlusion, nonrigid deformation and fast movement. In this paper, we presented an RGB-D tracker based on the Siamese Region Proposal Network and Depth Information. First, Siamese Network with shared parameters was constructed to perform feature extraction on the target patch and search area. Second, Region Proposal Network was constructed to estimate the target position in the RGB channels. At the same time, the depth information in the RGB-D video was used to determine the target occlusion state and fine-tune the target position. Finally, the tracker used depth information to achieve occlusion recovery when the target was fully occluded. The experimental result shows that the method has better performance in tracking accuracy and tracking speed on the large-scale Princeton RGB-D Tracking Benchmark (PTB) dataset.																	0218-0014	1793-6381				AUG	2020	34	9							2054023	10.1142/S0218001420540233													
J								An Experimental Analysis Method of Visual Performance on the Error Factors of Digital Information Interface	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Information omission; error factors; visual performance; information searching; physiological reactions	EYE-MOVEMENTS; MODEL; SEARCH; DESIGN	As per global accident statistics, human error accounts for over 85% of accidents. Therefore, human error analysis of cognitive behavior of operators can be the key to solving information interface design problems of digital smart task monitoring interfaces. This paper proposes an analytical method based on psychological experiments introduced into task monitoring interfaces to study reactions to error factors. It uses psychological techniques to conduct experiments which evoke physiological reactions to various error factors under different sub-interfaces of the monitoring system and sub-task environments. The behavioral and eye tracking data demonstrate the association between the error factors and the information interface. Error factors arising in visual search are directly related to the layout of the task-interface, the information proximity-position, and the information features-volume. Our method opens up new approaches for design optimizations of visual information interfaces and introduces novel concepts for the introduction of interface design via error factor analysis.																	0218-0014	1793-6381				AUG	2020	34	9							2055019	10.1142/S0218001420550198													
J								Estimate of Head Posture Based on Coordinate Transformation with MP-MTM-LSTM Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										MP-MTM-LSTM network; fatigue monitoring; coordinate transformation; deep learning	NEURAL-NETWORK	Fatigue monitoring can effectively reduce or even avoid traffic accidents. Head posture estimation is one of the focuses in the field of fatigue monitoring. In this paper, according to the coordinate rotation transformation and neural network theory, a method for predicting the change of head posture with sight-line coordinates is proposed. First, the coordinate rotation transformation theory is used to replace the head posture change amount with the coordinate change amount, and the first-order difference value of the sight-line point coordinate is obtained by the difference method. Then, under the unified Cartesian coordinate system, the MP-MTM-LSTM neural network is established with the input information of first-order difference value and the output information of coordinate change amount. The innovation of this method is that the Cartesian coordinate change is employed instead of the Euler angle transformation. In the model verification phase, the true value of the head pose is collected by the posture meter. The experimental results show that the absolute error between the predicted value and the true value estimated by the new method is less than 15%. In the field of fatigue monitoring, the proposed method can estimate the amount of head posture change effectively, which is suitable for the case where the head center point is not fixed.																	0218-0014	1793-6381				AUG	2020	34	9							2059031	10.1142/S0218001420590314													
J								An Advanced User Intent Model Based On User Learning Process	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										User intent model; irrelevant feedback; tentative click; user learning process; query expansion	QUERY EXPANSION; RELEVANCE FEEDBACK; FRAMEWORK	User intent analysis is a continuous research hotspot in the field of query expansion. However, the big amount of irrelevant feedbacks in search log has negatively impacted the precision of user intent model. By observing the log, it can be found that tentative click is a major source of irrelevant feedback. It is also observed that a kind of new feedback information can be extracted from the log to recognize the characteristics of tentative clicks. With this new feedback information, this paper proposes an advanced user intent model and applies it into query expansion. Experiment results show that the model can effectively decrease the negative impact of irrelevant feedbacks that belong to tentative clicks and increase the precision of query expansion, especially for those informational queries.																	0218-0014	1793-6381				AUG	2020	34	9							2050024	10.1142/S021800142050024X													
J								Mask R-CNN Method for Dashboard Feature Extraction in Eye Tracking	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Line of sight; eye tracking; Mask R-CNN; information extraction		The traditional information extraction technology of dashboard is easily affected by external factors, and the robustness is poor. To improve the safety of the pilot's performance on the dashboard, this paper proposes a way for extracting the dashboard feature information in eye tracking, which acquires the line of sight point in simulated dashboard. It then uses the Mask R-CNN method to detect the gaze area and then extracts the target feature information. Finally, it fuses two sets of data to get the result of the pilot who extracts the target gaze area in the scene. Experiment results show that the method of new dashboard information extraction proposed in this paper has a better accuracy.																	0218-0014	1793-6381				AUG	2020	34	9							2055017	10.1142/S0218001420550174													
J								Adaptive Reconstruction of Imperfectly Observed Monotone Functions, with Applications to Uncertainty Quantification	ALGORITHMS										adaptive approximation; isotonic regression; optimisation under uncertainty; uncertainty quantification; aerodynamic design	OPTIMIZATION	Motivated by the desire to numerically calculate rigorous upper and lower bounds on deviation probabilities over large classes of probability distributions, we present an adaptive algorithm for the reconstruction of increasing real-valued functions. While this problem is similar to the classical statistical problem of isotonic regression, the optimisation setting alters several characteristics of the problem and opens natural algorithmic possibilities. We present our algorithm, establish sufficient conditions for convergence of the reconstruction to the ground truth, and apply the method to synthetic test cases and a real-world example of uncertainty quantification for aerodynamic design.																		1999-4893				AUG	2020	13	8							196	10.3390/a13080196													
J								Adaptive Metrics for Adaptive Samples	ALGORITHMS										surface reconstruction; homology inference; adaptive sampling; topological data analysis	NOISY	We generalize the local-feature size definition of adaptive sampling used in surface reconstruction to relate it to an alternative metric on Euclidean space. In the new metric, adaptive samples become uniform samples, making it simpler both to give adaptive sampling versions of homological inference results and to prove topological guarantees using the critical points theory of distance functions. This ultimately leads to an algorithm for homology inference from samples whose spacing depends on their distance to a discrete representation of the complement space.																		1999-4893				AUG	2020	13	8							200	10.3390/a13080200													
J								Faster Algorithms for Mining Shortest-Path Distances from Massive Time-Evolving Graphs	ALGORITHMS										large graph mining; algorithm engineering; experimental algorithmics; time-evolving data; big data processing	QUERIES	Computing shortest-path distances is a fundamental primitive in the context of graph data mining, since this kind of information is essential in a broad range of prominent applications, which include social network analysis, data routing, web search optimization, database design and route planning. Standard algorithms for shortest paths (e.g., Dijkstra's) do not scale well with the graph size, as they take more than a second or huge memory overheads to answer a singlequery on the distancefor large-scale graph datasets. Hence, they are not suited to mine distances from big graphs, which are becoming the norm in most modern application contexts. Therefore, to achieve faster query answering, smarter and more scalable methods have been designed, the most effective of them based on precomputing and querying a compact representation of the transitive closure of the input graph, called the 2-hop-coverlabeling. To use such approaches in realistictime-evolvingscenarios, when the managed graph undergoes topological modifications over time, specificdynamic algorithms, carefully updating the labeling as the graph evolves, have been introduced. In fact, recomputing from scratch the 2-hop-coverstructure every time the graph changes is not an option, as it induces unsustainable time overheads. While the state-of-the-art dynamic algorithm to update a 2-hop-coverlabeling againstincrementalmodifications (insertions of arcs/vertices, arc weights decreases) offers very fast update times, the only known solution fordecrementalmodifications (deletions of arcs/vertices, arc weights increases) is still far from being considered practical, as it requires up to tens of seconds of processing per update in several prominent classes of real-world inputs, as experimentation shows. In this paper, we introduce a new dynamic algorithm to update 2-hop-coverlabelings against decremental changes. We prove its correctness, formally analyze its worst-case performance, and assess its effectiveness through an experimental evaluation employing both real-world and synthetic inputs. Our results show that it improves, by up to several orders of magnitude, upon average update times of the only existing decremental algorithm, thus representing a step forward towards real-time distance mining in general, massive time-evolving graphs.																		1999-4893				AUG	2020	13	8							191	10.3390/a13080191													
J								Trajectory Clustering and k-NN for Robust Privacy Preserving k-NN Query Processing in GeoSpark	ALGORITHMS										algorithmic data management; big spatial data management; GeoSpark; k-NN Queries; privacy preserving; trajectories	NEAREST-NEIGHBOR QUERY; MOVING-OBJECTS; BIG DATA	Privacy Preserving and Anonymity have gained significant concern from the big data perspective. We have the view that the forthcoming frameworks and theories will establish several solutions for privacy protection. Thek-anonymity is considered a key solution that has been widely employed to prevent data re-identifcation and concerns us in the context of this work. Data modeling has also gained significant attention from the big data perspective. It is believed that the advancing distributed environments will provide users with several solutions for efficient spatio-temporal data management. GeoSpark will be utilized in the current work as it is a key solution that has been widely employed for spatial data. Specifically, it works on the top of Apache Spark, the main framework leveraged from the research community and organizations for big data transformation, processing and visualization. To this end, we focused on trajectory data representation so as to be applicable to the GeoSpark environment, and a GeoSpark-based approach is designed for the efficient management of real spatio-temporal data. Th next step is to gain deeper understanding of the data through the application ofknearest neighbor (k-NN) queries either using indexing methods or otherwise. Thek-anonymity set computation, which is the main component for privacy preservation evaluation and the main issue of our previous works, is evaluated in the GeoSpark environment. More to the point, the focus here is on the time cost ofk-anonymity set computation along with vulnerability measurement. The extracted results are presented into tables and figures for visual inspection.																		1999-4893				AUG	2020	13	8							182	10.3390/a13080182													
J								Sphere Fitting with Applications to Machine Tracking	ALGORITHMS										sphere fitting; coresets; sampling methodologies; geometric approximation algorithms	CIRCLE DETECTOR; ALGORITHM	We suggest a provable and practical approximation algorithm for fitting a set P of n points in R-d to a sphere. Here, a sphere is represented by its center x is an element of R-d and radius r > 0. The goal is to minimize the sum Sigma(p is an element of P) vertical bar parallel to of distances to the points up to a multiplicative factor of 1 +/- epsilon, for a given constant epsilon > 0, over every such r and x. Our main technical result is a data summarization of the input set, called coreset, that approximates the above sum of distances on the original (big) set P for every sphere. Then, an accurate sphere can be extracted quickly via an inefficient exhaustive search from the small coreset. Most articles focus mainly on sphere identification (e.g., circles in 2D image) rather than finding the exact match (in the sense of extent measures), and do not provide approximation guarantees. We implement our algorithm and provide extensive experimental results on both synthetic and real-world data. We then combine our algorithm in a mechanical pressure control system whose main bottleneck is tracking a falling ball. Full open source is also provided.																		1999-4893				AUG	2020	13	8							177	10.3390/a13080177													
J								Influence Maximization with Priority in Online Social Networks	ALGORITHMS										social networks; influence maximization with priority; optimization; approximation algorithm	COMPETITIVE INFLUENCE	The Influence Maximization (IM) problem, which finds a set ofknodes (calledseedset) in a social network to initiate the influence spread so that the number of influenced nodes after propagation process is maximized, is an important problem in information propagation and social network analysis. However, previous studies ignored the constraint of priority that led to inefficient seed collections. In some real situations, companies or organizations often prioritize influencing potential users during their influence diffusion campaigns. With a new approach to these existing works, we propose a new problem calledInfluence Maximization with Priority(IMP) which finds out a set seed ofknodes in a social network to be able to influence the largest number of nodes subject to the influence spread to a specific set of nodesU(calledpriority set) at least a given thresholdTin this paper. We show that the problem is NP-hard under well-knownICmodel. To find the solution, we propose two efficient algorithms, calledIntegrated Greedy(IG) andIntegrated Greedy Sampling(IGS) with provable theoretical guarantees. IG provides a(1 - (1 - 1/k)(t))-approximation solution with t is an outcome of algorithm and t >= 1. The worst-case approximation ratio is obtained when t = 1 and it is equal to 1/k. In addition, IGS is an efficient randomized approximation algorithm based on sampling method that provides a(1 - (1 - 1/k)(t) - epsilon)-approximation solution with probability at least 1 - delta with epsilon > 0, delta is an element of(0,1) as input parameters of the problem. We conduct extensive experiments on various real networks to compare our IGS algorithm to the state-of-the-art algorithms in IM problem. The results indicate that our algorithm provides better solutions interns of influence on the priority sets when approximately give twice to ten times higher than threshold T while running time, memory usage and the influence spread also give considerable results compared to the others.																		1999-4893				AUG	2020	13	8							183	10.3390/a13080183													
J								A Predictive Analysis on Emerging Technology Utilization in Industrialized Construction in the United States and China	ALGORITHMS										industrialized construction; simulation; construction modeling; decision-making; predictive analysis; logistic regression	INDUSTRY 4.0; INFRASTRUCTURE; CHALLENGES; DESIGN; SYSTEM; MODEL; BIM	Considering the increasing use of emerging technologies in industrialized construction in recent years, the primary objective of this article is to develop and validate predictive models to predict the emerging technology utilization level of industrialized construction industry practitioners. Our preliminary research results indicate that the company background and personal career profiles can significantly affect practitioners' technology utilization level. Thus, our prediction model is based on four variables: company size, company type, working experience, and working position. The United States and China are selected as the case studies to validate the prediction model. First, a well-designed questionnaire survey is distributed to the industrialized construction industry practitioners from the two countries, which leads to 81 and 99 valid responses separately. Then, ordinal logistic regression is used to develop a set of models to predict the practitioners' utilization level of the four main technology types. Finally, the external test dataset consisting of 16 cases indicates the prediction models have a high accuracy. The results also reflect some differences of the technology utilization status in the industrialized construction industry between the United States and China. The major contribution of this research is offering an efficient and accurate method to predict practitioners' technology utilization level in industrialized construction. Significantly, the models are believed to have a wide application in promoting the emerging technologies in the actual industrialized construction.																		1999-4893				AUG	2020	13	8							180	10.3390/a13080180													
J								De-ghosting in High Dynamic Range Imaging Based on Intensity Scaling Cue	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										image sequence analysis; image fusion; image reconstruction; image motion analysis; image quality	QUALITY ASSESSMENT; REMOVAL; IMAGES; SCENES	A High Dynamic Range (HDR) image produced from a sequence of low dynamic range (LDR) images can contain motion artefacts (ghosting) if the scene contains moving objects. Conventional de-ghosting methods first detect moving objects in the scene, and then either remove those moving objects totally or reconstruct them. However, these methods are computationally expensive. This paper proposes a de-ghosting method that does not require explicit detection of moving regions. First, the ratio between camera exposure times of a target image and a reference image, which is called the intensity scaling factor in this paper, is computed. Since the information about camera exposure time is not available always, we propose a novel method to estimate the intensity scaling factor from non-saturated and non-moving pixel. Then, the estimated scaling factor is used as a cue to label every pixel in the target image as either static or moving pixel. Finally, the values of moving pixels are corrected with their expected values which can be estimated from the intensity scaling factor. Experimental results show that the proposed method generates more accurate ghost-free HDR images the existing state of the art methods.																	1582-7445	1844-7600				AUG	2020	20	3					3	10		10.4316/AECE.2020.03001													
J								A Real Time Simulator of a Phase Shifted Converter for High Frequency Applications	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										real-time systems; closed loop systems; field programmable gate arrays; high level synthesis; DC-DC power converters	THE-LOOP SIMULATION; BATTERY CHARGER; SYSTEMS	This paper presents a switched function FPGA-based Real Time Simulator (RTS) of a synchronous Phase Shifted (PS) converter. The design methods developed contribute to improving the accuracy, the portability, to lowering the cost and the resource demand of RTS models, enabling them to be easily deployed both in hardware in the loop (HIL) simulations, but also in error detection or health monitoring systems where these properties are essential. The research work carried out demonstrates the importance of reducing the simulation time step for avoiding false limit cycling behavior and obtaining an accurate closed loop response of the RTS. The very small time step (20 ns), not achievable with commercial real time simulation tools, helped in accurately modeling the time and frequency response of the converter for switching frequencies of 200 kHz (tested) and above. Although applied to a particular type of DC-DC converter, the methods presented can be used to successfully model a wide range of Switched Mode Power Supply (SMPS) topologies. An innovative hardware platform that enables running the real time simulation model in parallel with the reference converter and facilitates a comparative analysis that proves the fidelity of the RTS of the PS converter was also developed.																	1582-7445	1844-7600				AUG	2020	20	3					11	22		10.4316/AECE.2020.03002													
J								An Artificial Immune System Approach for a Multi-Compartment Queuing Model for Improving Medical Resources and Inpatient Bed Occupancy in Pandemics	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										artificial intelligence; evolutionary computation; hospitals; optimization; queueing analysis	GENETIC ALGORITHM; BLACK-HOLE; OPTIMIZATION; COST	In the context of the Covid-19 pandemic the pressure that is put on the medical systems is increasing exponentially. Healthcare systems resources are in general scarce, and hence they require policies that ensure the optimal usage of beds and utilization costs. The aim of this study is to explore how artificial immune system approaches for a multiqueuing model may aid the hospital managers improve their resources. The proposed system outlines the route of Covid-19 patients in the intensive care unit (ICU), the compartmental model proposes a reasonable composition of the ICU, considering the queuing parameters, while the artificial immune system optimizes the needed resources (beds plus associated costs). The methodology was demonstrated through a simulation based on real data collected from official sources.																	1582-7445	1844-7600				AUG	2020	20	3					23	30		10.4316/AECE.2020.03003													
J								A Digital Signal Amplification Device for Microelectrode Arrays based on Stochastic Resonance	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										multielectrode; signal; stochastic; resonance; amplifier	INFORMATION-TRANSMISSION; NOISE; SYSTEM; OUTPUT	In this work, an experimental study was carried out about the construction of an amplification equipment based on the phenomenon of stochastic resonance (SR), which was initially thought to detect spikes and bursts from human and animal neuronal tissue, both in vitro (from microelectrode array, MEA) and in vivo, from electrodes in the cerebral cortex of mammals. The implemented equipment was called CADSR (Computer-Aided Digital Stochastic Resonator) and brings as innovation the fact of being controlled and monitored by the computer, through a graphical interlace that allows an automatic tuning, making it possible to obtain the optimum level of noise to maintain SR in real-time. Experimental results show that for electrical signals from multi-electrode arrays with amplitude below 25 microvolts, the amplification system using stochastic resonance is better than conventional amplifier systems, which use operational amplifiers in linear configurations.																	1582-7445	1844-7600				AUG	2020	20	3					31	40		10.4316/AECE.2020.03004													
J								Shannon Energy Application for Detection of ECG R-peak using Bandpass Filter and Stockwell Transform Methods	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										biomedical signal processing; spectral analysis; electrocardiography; detection algorithms; signal processing algorithms	HILBERT TRANSFORM; QRS; ALGORITHM	Shannon energy-based algorithm has been implemented in peak detection method of various physiological signals including electrocardiogram, which is used to enhance significant peaks for accurate peak detection. Two significant methods of R-peak detection that apply Shannon energy are identified. However, direct comparison cannot be made due to the differences in database used, number of beat analysed, frequency range selected, and signal processing technique applied. This paper aimed to properly evaluate the performance of Shannon eneru-based algorithms for R-peak detection on two methods of bandpass filter and Stockwell transform. Simple enveloping technique using moving average filter is proposed, and a threshold is set to localize R-peak at a selected frequency range of 7-15 Hz. Performance of both methods were then evaluated using all 48 data from MIT-BIH Arrhythmia database. Result showed that both methods are equivalently useful in reducing P and T waves interference and produced similar output of Shannon energy envelope. However, Shannon energy application on bancipass filter offered 99.71% sensitivity, 99.80 % positive predictivity and 99.52% accuracy, slightly better than that of the Stockwell transform method that only produced 99.65% sensitivity, 99.68% positive predictivity and 99.33% accuracy.																	1582-7445	1844-7600				AUG	2020	20	3					41	48		10.4316/AECE.2020.03005													
J								Effective Wavelet Algorithm for an Automatic Detection of Atrial Fibrillation	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										adaptive algorithms; biomedical signal processing; performance evaluation; threshold; wavelet	PHYSIONET	Cardiac anomalies are usually marked through irregular cardiac cycles. Atrial fibrillation is given through a rapid beating of the atria, announcing a possible heart failure or stroke. Electrocardiograms are an efficient way of supervising the electric activity of the heart. We have developed an effective, simple to implement automatic detection algorithm for identifying changes of the cardiac rhythm. The algorithm is based on wavelets and an enhanced time domain thresholding procedure. We take into account a variation of the electrocardiogram's amplitudes, to avoid loss of clinical features. The interval between beats is computed and provided for a reliable diagnosis. The results are validated both with objective evaluation criteria and displayed graphically, assisting the medical diagnosis procedure.																	1582-7445	1844-7600				AUG	2020	20	3					49	56		10.4316/AECE.2020.03006													
J								Diagnosis of Alzheimer's Disease from Brain Magnetic Resonance Imaging Images using Deep Learning Algorithms	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										artificial intelligence; artificial neural network; image classification; machine learning; medical diagnosis	PREDICTION	Alzheimer's disease is one amongst the progressive disorder that cruelly affects the brain cells. It causes the death of nerve cells and tissue loss in brain. It usually tends to start slowly and aggravates overtime. The symptoms of Alzheimer's disease vary from person to person depending on the severity of the unhealthiness. It exhibits behavioral symptoms such as communication impairments, memory loss, taking a longer time to complete usual activities, and change in attitude and behavior. if the problem worsens over time, then it cannot be cured. Hence it should he identified at the earlier stage itself and treat the patient to lead a normal life on their own. Deep learning algorithms exhibit marvelous performance over conventional machine learning algorithms in identifying the complex patterns in the large volumes of high-dimensional medical imaging data. Hence, recently significant attention has been paid to apply deep learning for medical diagnosis. In this research, Deep Convolution Neural Network (DCNN) and VGG-16 inspired CNN (VCNN) models have been built to classify the different stages of Alzheimer's Disease from the Magnetic Resonance Imaging(MRI) images. Experiments are carried out on an ADNI dataset and the results obtained show that the proposed models achieved excellent accuracy.																	1582-7445	1844-7600				AUG	2020	20	3					57	64		10.4316/AECE.2020.03007													
J								Two Types of Fuzzy Logic Controllers for the Speed Control of the Doubly-Fed Induction Machine	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										induction motors; fuzzy logic; fuzzy control; velocity control; regulators; stability	SLIDING-MODE CONTROLLER; FIELD-ORIENTED CONTROL; INTERVAL TYPE-2; ALGORITHMS; SYSTEMS; DESIGN	The paper presents two fuzzy logic control algorithms: type-1 and type-2. These two nonlinear techniques are used for adjust the speed control with a direct stator flux orientation control of a doubly fed induction motor. The effectiveness of the proposed control strategy is evaluated under different operating conditions such as of reference speed and for load torque step changes at nominal parameters and in the presence of parameter variation (stator resistance, rotor resistance and moment of inertia). The results of the simulation of the doubly fed induction motor velocity control have shown that fuzzy type-2 ensures better dynamic performances with respect to fuzzy type-1 control, even by parametric variations and external disturbances.																	1582-7445	1844-7600				AUG	2020	20	3					65	74		10.4316/AECE.2020.03008													
J								Influence of Different Pole Head Shapes on Motor Performance in Switched Reluctance Motors	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										acoustic noise; inductance curve; radial force; switched reluctance motor; torque ripple	ACOUSTIC NOISE; OPTIMAL-DESIGN; TORQUE; RECONSTRUCTION	The main reasons of the vibrations occurring in the stator of Switched Reluctance Motors (SRM) are the radial forces and they cause acoustic noise. This has an adverse effect on the performance of SRM. The aim of this study is to reduce radial forces by giving different geometric shapes to the pole heads and to investigate the effect of these pole shapes on the motor performance of Sint In this study, the radial forces of four different SRMs having generally the same dimensions but different pole head shapes are calculated and compared with each other. In addition, the effects of different pole head shapes on the inductance curve and the torque ripple are investigated. To calculate the radial forces, torque and inductance values, ANSYS software is used which uses finite element method (FEM). After reshaping the pole heads, rotor position is changed with the increments of 1 degree from the unaligned to aligned position and the radial forces, torque and inductance values are calculated for each incremental position. According to the results, radial force is reduced about 19.03% at the rated current as compared to a standard SRM. However, torque ripple is observed to increase by about 3.29%.																	1582-7445	1844-7600				AUG	2020	20	3					75	82		10.4316/AECE.2020.03009													
J								Edge-Preserving Filtering and Fuzzy Image Enhancement in Depth Images captured by RealSense Cameras in Robotic Applications	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										filtering algorithms; fuzzy logic; image enhancement; robots; stereo vision	FUZZIFIED GABOR FILTER; RECONSTRUCTION	This paper presents both the use of depth cameras in robotic applications and effects of post-processing on the captured depth images. The performance of depth cameras and post-processing image enhancement are evaluated with the aim to improve the depth-based object detection. First, the edge-preserving exponential moving average (EMA) filter and the fuzzy contrast enhancement procedures are briefly introduced. Then, the use of depth cameras with post-processing methods is shown in the example of painting robots. The use of the stereo depth camera is essential in robotic applications, since it constitutes the initial steps in a series of robotic operations, where the goal is to both detect and extract obstacles on walls that are not intended to he painted.																	1582-7445	1844-7600				AUG	2020	20	3					83	92		10.4316/AECE.2020.03010													
J								Model-Based Dynamic Fractional-Order Sliding Mode Controller Design for Performance Analysis and Control of a Coupled Tank Liquid-Level System	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										fractional calculus; level control; nonlinear control systems; process control; sliding mode control	IMPLEMENTATION	In this paper, a model-based dynamic fractional-order sliding mode controller (FOSMC) is designed and implemented to a coupled tank experimental setup for controlling the liquid level. First, a model-based dynamic sliding-mode controller is designed by using the dynamic equations of a vertically positioned coupled tank system. Then, the sliding surface of the sliding-mode controller is defined in fractional order so that the designed controller can make better water level tracking. The liquid level control of the system is realized in two different steps. In the first step, the water level of the upper tank is controlled by a pump and in this application the bottom tank is not considered. In the second step, the water level of the bottom tank is controlled with upper tank's output water. In addition, a model-based dynamic sliding mode controller (SMC) is also applied to the system to show the performance of the proposed controller in terms of robustness to disturbances, reference tracking and error elimination capability. Experimental results show that the proposed controller reduces the reference tracking error by 3.68% and 10.17% for the upper tank and 17.07% for the, bottom tank when compared to the SMC, and the control signal contains more chattering than the SMC.																	1582-7445	1844-7600				AUG	2020	20	3					93	100		10.4316/AECE.2020.03011													
J								Intuitionistic Fuzzy Partial Logistic Regression Model Using Ridge Methodology	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Goodness-of-fit measure; intuitionistic fuzzy number; intuitionistic fuzzy partial logistic regression; kernel method; ridge estimation	ESTIMATORS	This paper applies a ridge estimation approach in an existing partial logistic regression model with exact predictors, intuitionistic fuzzy responses, intuitionistic fuzzy coefficients and intuitionistic fuzzy smooth function to improve an existing intuitionistic fuzzy partial logistic regression model in the presence of multicollinearity. For this purpose, ridge methodology is also involved to estimate the parametric intuitionistic fuzzy coefficients and nonparametric intuitionistic fuzzy smooth function. Some common goodness-of-fit criteria are also used to examine the performance of the proposed regression model. The potential application of the proposed method are illustrated and compared with the intuitionistic partial logistic regression model through two numerical examples. The results clearly indicate the proposed ridge method is quite efficient in model's performances when there is multicollinearity among the predictors.																	0218-4885	1793-6411				AUG	2020	28	4					527	543		10.1142/S0218488520500221													
J								Project Time-Cost-Quality Trade-off Problem: A Novel Approach Based on Fuzzy Decision Making	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Project scheduling; time-cost-quality trade-off; multi-objective programming; fuzzy decision making; preferred solution	DISCRETE-TIME; OPTIMIZATION; MANAGEMENT; ALGORITHM; MODEL	Trade-off problems concentrate on balancing the main parameters of a project as completion time, total cost and quality of activities. In this study, the problem of project time-cost-quality trade-off is formulated and solved from a new standpoint. For this purpose, completion time and crash cost of project are illustrated as fuzzy goals, also the dependency of implementing time of each activity and its execution-quality is described by a fuzzy number. The overall quality of the project execution is defined as the minimum execution-quality of the project activities that should be maximized. Based on some real assumptions, a three-objective programming problem associated with the time-costquality trade-off problem is formulated; then with the aim of identifying a fair and appropriate trade-off, the research problem is reformulated as a single objective linear programming by utilizing a fuzzy decision-making methodology. Generating a final preferred solution, rather than a set of Pareto optimal solutions, and having a reasonable interpretation are two most important advantages of the proposed approach. To explain the practical performance of the proposed models and approach, a time-cost-quality trade-off problem for a project with real data is solved and analyzed.																	0218-4885	1793-6411				AUG	2020	28	4					545	567		10.1142/S0218488520500233													
J								A Weighted-Logic Representation of C-Revising Ordinal Conditional Functions	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Uncertainty distributions; ordinal conditional functions; possibility theory; belief revision; weighted knowledge bases	BELIEF REVISION	The problem of belief change is considered as a major issue in managing the dynamics of an information system. It consists in modifying an uncertainty distribution, representing agents' beliefs, in the light of a new information. In this paper, we focus on the so-called multiple iterated belief revision or C-revision, proposed for conditioning or revising uncertain distributions under uncertain inputs. Uncertainty distributions are represented in terms of ordinal conditional functions. We will use prioritized or weighted knowledge bases as a compact representation of uncertainty distributions. The input information leading to a revision of an uncertainty distribution is also represented by a set of consistent weighted formulas. This paper shows that C-revision, defined at a semantic level using ordinal conditional functions, has a very natural representation using weighted knowledge bases. We propose simple syntactic methods for revising weighted knowledge bases, that are semantically meaningful in the frameworks of possibility theory and ordinal conditional functions. In particular, we show that the space complexity of the proposed syntactic C-revision is linear with respect to the size of initial weighted knowledge bases.																	0218-4885	1793-6411				AUG	2020	28	4					569	589		10.1142/S0218488520500245													
J								A New Method for Ranking Interval Type-2 Fuzzy Numbers Based on Mellin Transform	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										interval type-2 fuzzy numbers; Mellin transform; distance measure; possibility degree; ranking	DECISION-MAKING METHOD; MEAN-VALUE; SETS; FUZZISTICS; DISTANCE	Interval type-2 fuzzy sets provide us with additional degrees of freedom to represent the uncertainty and the fuzziness of the real word than traditional type-1 fuzzy sets. Interval type-2 fuzzy numbers ranking has an important role in the decision making analysis. In this paper, the probatilistic mean value and variance of interval type-2 fuzzy numbers are proposed based on the Mellin transform for type-1 fuzzy numbers. The interval type-2 fuzzy number with the higher mean is ranked higher. If the mean values are equal the one with the smaller variance is judged higher rank. On this basis, some new distance measures and possibility degree formula are proposed to comparing interval type-2 fuzzy numbers based on their Mellin mean value and variance. Some benchmarking numerical examples are given, and some interpretation issues are explained.																	0218-4885	1793-6411				AUG	2020	28	4					591	611		10.1142/S0218488520500257													
J								Covering Problem for Solutions of Max-Archimedean Bipolar Fuzzy Relation Equations	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Bipolar fuzzy relation equations; max-Archimedean composition; covering problem; irredundant coverings	LINEAR OPTIMIZATION; OBJECTIVE FUNCTION; RESOLUTION; SUBJECT	This paper discusses the resolution of max-Archimedean bipolar fuzzy relation equations. In the literature, many methods have been proposed based on 0-1 integer programming problem or reduction methods for the optimization with bipolar fuzzy relation equations. A new concept based on the idea of covering and the notions of leading, non-leading variables are introduced in the present paper for finding the solutions of max-Archimedean bipolar fuzzy relation equations. It is shown that the problem of finding the complete solution set of the system of max-Archimedean bipolar fuzzy relation equations is equivalent to solving a covering problem and the solutions of such equations correspond to irredundant coverings of the covering problem. The proposed method is illustrated with some examples.																	0218-4885	1793-6411				AUG	2020	28	4					613	634		10.1142/S0218488520500269													
J								Vectorized Kernel-Based Fuzzy C-Means: a Method to Apply KFCM on Crisp and Non-Crisp Numbers	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Clustering; kernel methods; crisp; symbolic interval and fuzzy numbers; microarray gene expression data	CLUSTERING APPROACH; SEGMENTATION; ALGORITHMS; MECHANISMS; MODEL	Kernel methods are a class of algorithms for pattern analysis to robust them to noise, overlaps, outliers and also unequal sized clusters. In this paper, kernel-based fuzzy c-means (KFCM) method is extended to apply KFCM on any crisp and non-crisp input numbers only in a single structure. The proposed vectorized KFCM (VKFM) algorithm maps the input (crisp or non-crisp) features to crisp ones and applies the KFCM (with prototypes in feature space) on them. Finally the resulted crisp prototypes in the mapped space are influenced by an inverse mapping to obtain the prototypes' (centers') parameters in the input features space. The performance of the proposed method has been compared with the conventional FCM and KFCM and other new methods, to show its effectiveness in clustering of gene expression data and segmentation of land-cover using satellite images Simulation results show good accuracy of proposed method in compare to other methods.																	0218-4885	1793-6411				AUG	2020	28	4					635	659		10.1142/S0218488520500270													
J								On the Use of m-Probability-Estimation and Imprecise Probabilities in the Naive Bayes Classifier	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Supervised learning; naive Bayes; m-estimate; m-probability-estimation; imprecise probabilities; noisy data	CREDAL-C4.5	Within the field of supervised classification, the naive Bayes (NB) classifier is a very simple and fast classification method that obtains good results, being even comparable with much more complex models. It has been proved that the NB model is strongly dependent on the estimation of conditional probabilities. In the literature, it had been shown that the classical and Laplace estimations of probabilities have some drawbacks and it was proposed a NB model that takes into account the a priori probabilities in order to estimate the conditional probabilities, which was called m-probability-estimation. With a very scarce experimentation, this approximation based on m-probability-estimation demonstrated to provide better results than NB with classical and Laplace estimations of probabilities. In this research, a new naive Bayes variation is proposed, which is based on the m-probability-estimation version and takes into account imprecise probabilities in order to calculate the a priori probabilities. An exhaustive experimental research is carried out, with a large number of data sets and different levels of class noise. From this experimentation, we can conclude that the proposed NB model and the m-probabilityestimation approach provide better results than NB with classical and Laplace estimation of probabilities. It will be also shown that the proposed NB implies an improvement over the m-probability-estimation model, especially when there is some class noise.																	0218-4885	1793-6411				AUG	2020	28	4					661	682		10.1142/S0218488520500282													
J								Multi-Level Fine-Scaled Sentiment Sensing with Ambivalence Handling	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Ambivalence sentiment handling; emotion sensing; multi-level fine-scaled sentiment analysis; sentiment strength level; social media analysis	EMOTION; EXPLORATION; KNOWLEDGE	Social media represent a rich source of information, such as critiques, feedback, and other opinions posted online by Internet users. Such information is typically a good reflection of users' sentiments and attitudes towards various services, topics, or products. Sentiment analysis has become an increasingly important natural language processing (NLP) task to help users make sense of what is happening in the Internet blogosphere and it can be useful for companies as well as public organizations. However, most existing sentiment analysis techniques are only able to analyze data at the aggregate level, merely providing a binary classification (positive vs. negative), and are not able to generate finer characterizations of sentiments as well as emotions involved. This paper describes a new opinion analysis scheme, i.e., a multi-level fine-scaled sentiment sensing with ambivalence handling. The ambivalence handler is presented in detail along with the strength-level tune parameters for analyzing the strength and the fine-scale of both positive or negative sentiments. It is capable of drilling deeper into text in order to reveal multi-level fine-scaled sentiments as well as different types of emotions.																	0218-4885	1793-6411				AUG	2020	28	4					683	697		10.1142/S0218488520500294													
J								Neural Networks as Classification Mechanisms of Complex Human Activities	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Human activity recognition; human motion; neural networks; deep learning for time series; classification of complex activities	HUMAN ACTION RECOGNITION; JOINTS; POSE	Within this paper, we present two neural nets for view-independent complex human activity recognition (HAR) from video frames. For our study here, we reduce the number of frames produced by a video sequence given that we can identify activities from a sparsely sampled sequence of body poses, and, at the same time, we are able to reduce the processing complexity and response while hardly affecting the accuracy, precision, and recall. To do so, we use a formal framework to ensure the quality of data collection and data preprocessing. We utilize neural networks for the classification of single and complex body activities. More specifically, we consider the sequence of body poses as a time-series problem given that they can provide state-of-the-art results on challenging recognition tasks with little data engineering. Deep Learning in the form of Convolutional Neural Network (CNN), Long Short-Term Neural Network (LSTM), and a one-dimensional Convolutional Neural Network Long Short-Term Memory model (CNN-LSTM) are used as benchmarks to classify the activity.																	0218-2130	1793-6349				AUG	2020	29	5							2050011	10.1142/S0218213020500116													
J								Traffic Sign Recognition Using a Synthetic Data Training Approach	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Traffic sign recognition; synthetic data; convolutional neural network	DEEP NEURAL-NETWORK; CLASSIFICATION; SYSTEMS	Traffic Sign Recognition (TSR) is a crucial component in many automotive applications, such as driver assistance, sign maintenance, and vehicle autonomy. In this paper, we present an efficient approach to training a machine learning-based TSR solution. In our choice of recognition method, we have opted for convolutional neural networks, which have demonstrated best-in-class performance in previous works on TSR. One of the challenges related to training deep neural networks is the requirement for a large amount of training data. To circumvent the tedious process of acquiring and manually labelling real data, we investigate the use of synthetically generated images. Our networks, trained on only synthetic data, are capable of recognising traffic signs in challenging real-world footage. The classification results achieved on the GTSRB benchmark are seen to outperform existing state-of-the-art solutions.																	0218-2130	1793-6349				AUG	2020	29	5							2050013	10.1142/S021821302050013X													
J								Smart Cities - Detecting Humans in Regions of Disasters: Synergy of Drones, Micro-robots in Underground Tunnels	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Ground bio-inspired micro-robots; path planning; tunnel reconstruction; synergistic collaboration		The pieces of information that are being collected from regions of disaster is critical as the rapid deployment of the first responders rely on them. Another critical part of that deployment is the acquisition of different types of information (visual, sounds, and others). Even with that information the rescuing teams still face the difficult task of rescuing humans under debris. Some of the constraints that make this task harder are the wrecked building's unknown structure, time limitation, the difficulty to collect information under the debris and more. An important issue is the accurate collection of information beneath destroyed structures and the 3D representation of the space and the correct location of the human subject under the debris. This paper deals with the design and the capabilities of a ground bio-inspired micro-robot, called Tzitziki, capable to select visual and audio information beneath destroyed buildings and locate human subjects in areas (like deep underground cavities), the reconstruction of the underground cavities and the synergistic collaboration of drones, micro-robots and human first responders. Illustrative examples are provided proving the concept.																	0218-2130	1793-6349				AUG	2020	29	5							2050006	10.1142/S0218213020500062													
J								Auxiliary Dictionary of Diversity Learning for Face Recognition with a Single Sample Per Person	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Single sample; virtual face images; auxiliary dictionary of diversity; patch	REPRESENTATION; ROBUST	Face recognition for a single sample per person is challenging due to the lack of sufficient sample information. However, using generic training set to learn an auxiliary dictionary is an effective way to alleviate this problem. Considering generic training sample of diversity, we proposed an algorithm of auxiliary dictionary of diversity learning (ADDL). We first produced virtual face images by mirror images, square block occlusion and grey transform, and then learned an auxiliary dictionary of diversity using a designed objective function. Considering patch-based method can reduce the influence of variations, we seek extended sparse representation with l(2)-minimization for each probe patch. Experimental results in the CMUPIE, Extended Yale B and LFW datasets demonstrate that ADDL performs better than other related algorithms.																	0218-2130	1793-6349				AUG	2020	29	5							2050015	10.1142/S0218213020500153													
J								Application of Improved Artificial Intelligence with Runner-Root Meta-Heuristic Algorithm for Dairy Products Industry: A Case Study	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Artificial intelligence; runner-root algorithm; gray wolf optimization; invasive weed optimization; time series neural network	OPTIMIZATION	As the dairy products have a short consumption period, the accurate prediction of their demand is very important for the dairy industry. Accordingly, this research specifically addresses the prediction of dairy product demand (DPD). The main contribution of this research is to provide an integrated framework based on statistical tests, time-series prediction and artificial intelligence with the runner-root algorithm (RRA) as a novel meta-heuristic algorithm to obtain the best prediction of DPD in Iran. First, a series of economic and social indicators that seemed to be effective in the demand for dairy products are identified and the ineffective indices are eliminated. Next, the artificial intelligence tools including MLP, ANFIS, and LSTM are implemented and improved with the help of RRA. The designed hybrid methods are implemented by using data from 2013 to 2017 of the Iran diary industry. This novel algorithm is compared to gray wolf optimization, invasive weed optimization, and particle swarm optimization. The results show that the proposed MLP-RRA has the most ability to improve by using meta-heuristic algorithms. The coefficient of determination is 98.19%. Moreover, in each artificial intelligence tools, RRA causes better results than the other tested algorithms. The highly accurate results confirm that the proposed hybrid methods based on the RRA algorithm are able to improve the prediction of demand for various products.																	0218-2130	1793-6349				AUG	2020	29	5							2050008	10.1142/S0218213020500086													
J								Deep Learning Based Sentiment Analysis in a Code-Mixed English-Hindi and English-Bengali Social Media Corpus	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Code-switching; recurrent neural networks; convolutional neural networks		Sentiment analysis is a circumstantial analysis of text, identifying the social sentiment to better understand the source material. The article addresses sentiment analysis of an English-Hindi and English-Bengali code-mixed textual corpus collected from social media. Code-mixing is an amalgamation of multiple languages, which previously mainly was associated with spoken language. However, social media users also deploy it to communicate in ways that tend to be somewhat casual. The coarse nature of social media text poses challenges for many language processing applications. Here, the focus is on the low predictive nature of traditional machine learners when compared to Deep Learning counterparts, including the contextual language representation model BERT (Bidirectional Encoder Representations from Transformers), on the task of extracting user sentiment from code-mixed texts. Three deep learners (a BiLSTM CNN, a Double BiLSTM and an Attention-based model) attained accuracy 20-60% greater than traditional approaches on code-mixed data, and were for comparison also tested on monolingual English data.																	0218-2130	1793-6349				AUG	2020	29	5							2050014	10.1142/S0218213020500141													
J								Systematic Construction of Neural Forms for Solving Partial Differential Equations Inside Rectangular Domains, Subject to Initial, Boundary and Interface Conditions	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Interface conditions; neural forms; neural networks; partial differential equations	NETWORK METHODS	A systematic approach is developed for constructing proper trial solutions to Partial Differential Equations (PDEs) of up to second order, using neural forms that satisfy prescribed initial, boundary and interface conditions. The spatial domain considered is of the rectangular hyper-box type. On each face either Dirichlet or Neumann conditions may apply. Robin conditions may be accommodated as well. Interface conditions that induce discontinuities, have not been treated to date in the relevant neural network literature. As an illustration a common problem of heat conduction through a system of two rods in thermal contact is considered.																	0218-2130	1793-6349				AUG	2020	29	5							2050009	10.1142/S0218213020500098													
J								Variance Counterbalancing for Stochastic Large-scale Learning	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Large-scale training; neural networks; stochastic gradient descent; variance reduction		Stochastic Gradient Descent (SGD) is perhaps the most frequently used method for large scale training. A common example is training a neural network over a large data set, which amounts to minimizing the corresponding mean squared error (MSE). Since the convergence of SGD is rather slow, acceleration techniques based on the notion of "Mini-Batches" have been developed. All of them however, mimicking SGD, impose diminishing step-sizes as a means to inhibit large variations in the MSE objective. In this article, we introduce random sets of mini-batches instead of individual minibatches. We employ an objective function that minimizes the average MSE and its variance over these sets, eliminating so the need for the systematic step size reduction. This approach permits the use of state-of-the-art optimization methods, far more efficient than the gradient descent, and yields a significant performance enhancement.																	0218-2130	1793-6349				AUG	2020	29	5							2050010	10.1142/S0218213020500104													
J								Estimation of Personalized Heterogeneous Treatment Effects Using Concatenation and Augmentation of Feature Vectors	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Treatment effect; meta-learner; regression; treatment; control; simulation	INFERENCE	A new meta-algorithm for estimating the conditional average treatment effects is proposed in the paper. The basic idea behind the algorithm is to consider a new dataset consisting of feature vectors produced by means of concatenation of examples from control and treatment groups, which are close to each other. Outcomes of new data are defined as the difference between outcomes of the corresponding examples comprising new feature vectors. The second idea is based on the assumption that the number of controls is rather large and the control outcome function is precisely determined. This assumption allows us to augment treatments by generating feature vectors which are closed to available treatments. The outcome regression function constructed on the augmented set of concatenated feature vectors can be viewed as an estimator of the conditional average treatment effects. A simple modification of the Co-learner based on the random subspace method or the feature bagging is also proposed. Various numerical simulation experiments illustrate the proposed algorithm and show its outperformance in comparison with the well-known T-learner and X-learner for several types of the control and treatment outcome functions.																	0218-2130	1793-6349				AUG	2020	29	5							2050005	10.1142/S0218213020500050													
J								A supervised learning approach for heading detection	EXPERT SYSTEMS										heading detection; machine learning; supervised learning algorithm; text segmentation	FEATURE SPACE THEORY; MACHINE; DOCUMENTS; ACCURACY; HTML; AUC	As the popularity of the portable document format (PDF) file format increases, research that facilitates PDF text analysis or extraction is necessary. Heading detection is a crucial component of PDF-based text classification processes. This research involves training a supervised learning model to detect headings by systematically testing and selecting classifier features usingrecursive feature elimination. Results indicate that decision tree is the best classifier with an accuracy of 95.83%, sensitivity of 0.981, and a specificity of 0.946. This research into heading detection contributes to the field of PDF-based text extraction and can be applied to the automation of large scale PDF text analysis in a variety of professional and policy-based contexts.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12520	10.1111/exsy.12520													
J								A hybrid data envelopment analysis and multi-attribute decision making approach to sustainability assessment	EXPERT SYSTEMS										CRITIC method; cross-efficiency; data envelopment analysis; selective measure; sustainability	SELECTING PERFORMANCE-MEASURES; DETERMINING ATTRIBUTE WEIGHTS; CROSS-EFFICIENCY; DEA; MODEL	The concept of sustainability consists of three main dimensions: environmental, techno-economic, and social. Measuring the sustainability status of a system or technology is a significant challenge, especially when it needs to consider a large number of attributes in each dimension of sustainability. In this study, we first propose a hybrid approach, involving data envelopment analysis (DEA) and a multi-attribute decision making (MADM) methodologies, for computing an index for each dimension of sustainability, and then we define the overall sustainability index as the mean of the three measured indexes. Towards this end, we define new concepts ofefficiency and cross-efficiency of order(p, q)wherepandqare the number of inputs and outputs, respectively. For a given(p, q), we address the problem of finding efficiency of order(p, q)by developing a novel DEA-based selecting method. Finally, we define the sustainability index as a weighted sum of all possible cross-efficiencies of order(p, q). Form a computational viewpoint, the proposed selecting model significantly decreases the computational burden in comparison with the successive solving of traditional DEA models. A case study of the electricity-generation technologies in the United Kingdom is taken as a real-world example to illustrate the potential application of our method.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12347	10.1111/exsy.12347													
J								A novel completeness definition of event logs and corresponding generation algorithm	EXPERT SYSTEMS										big data analytics; business process management; event log completeness; log generation; testing	BIG DATA ANALYTICS; PROCESS MODELS; LOCAL COMPLETENESS; PETRI NETS	As the promotion of technologies and applications of Big Data, the research of business process management (BPM) has gradually deepened to consider the impacts and challenges of big business data on existing BPM technologies. Recently, parallel business process mining (e.g. discovering business models from business visual data, integrating runtime business data with interactive business process monitoring visualisation systems and summarising and visualising historical business data for further analysis, etc.) and multi-perspective business data analytics (e.g. pattern detecting, decision-making and process behaviour predicting, etc.) have been intensively studied considering the steep increase in business data size and type. However, comprehensive and in-depth testing is needed to ensure their quality. Testing based solely on existing business processes and their system logs is far from sufficient. Large-scale randomly generated models and corresponding complete logs should be used in testing. To test parallel algorithms for discovering process models, different log completeness and generation algorithms were proposed. However, they suffer from either state space explosion or non-full-covering task dependencies problem. Besides, most existing generation algorithms rely on random executing strategy, which leads to low and unstable efficiency. In this paper, we propose a novel log completeness type, that is, #TAR completeness, as well as its generation algorithm. The experimental results based on a series of randomly generated process models show that the #TAR complete logs outperform the state-of-the-art ones with lower capacity, fuller dependencies covering and higher generating efficiency.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12529	10.1111/exsy.12529													
J								Measuring environmental sustainability performance of freight transportation seaports inChina:A data envelopment analysisapproach based on the closest targets	EXPERT SYSTEMS										closest targets; data envelopment analysis; environmental performance; freight transportation	MODELING UNDESIRABLE FACTORS; SLACKS-BASED MEASURE; EFFICIENCY ANALYSIS; DEA; DISTANCE; FRONTIER; DISPOSABILITY; ENERGY; CHINA; WEAK	Because of China's rapid economic development, its freight transportation system has grown to become one of China's high-pollution-emission sectors. However, there are few studies that pay close attention to measuring and improving the environmental performance of China's freight transportation system, especially in regard to seaports. In this paper, data envelopment analysis (DEA) is applied to measure the environmental performance of freight transportation seaports. In addition, we also provide benchmarking information to point the way to improving environmental performance effectively. Our proposed DEA model is based on the closest targets, which satisfies the strong monotonicity and can yield the most relevant solution for the inefficient seaports. An empirical study of 21 of China's primary freight transportation seaports shows that most of them have relatively good environmental performance. Among the five coastal port groups, the Bohai-rim port group had the best environmental performance, whereas the Pearl River port group had the worst. Our data show significant differences between the best and worst performances, indicating that more measures should be taken to balance and coordinate the development between the five coastal port groups.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12334	10.1111/exsy.12334													
J								Cross-efficiency evaluation method based on the conservative point of view	EXPERT SYSTEMS										cross-evaluation; efficiency; DEA; efficiency; flexible manufacturing systems	DATA ENVELOPMENT ANALYSIS; EMISSION PERMITS; DEA MODELS; ALLOCATION; RANKING; AGGREGATION; SELECTION	Traditional cross-efficiency evaluation models have ignored the problem that large differences may exist among cross-efficiencies, which may make decision making units (DMUs) unwilling to accept cross-efficiency evaluation results. Aimed at solving this problem, this paper proposes an altruism cross-efficiency model based on the conservative point of view. Compared with other cross-efficiency evaluation models, the proposed model mainly exhibits the following advantages. First, the proposed model no longer guarantees the DMU's self-evaluation efficiency, allowing the self-evaluation efficiency to adaptively change. Therefore, the peer-evaluation process is flexible and adaptable to the actual situation. Second, the proposed model abandons the traditional max-average secondary objective function and proposes to use a max-min objective function. Thus, the proposed model can maximize the peer-efficiency of the worst performing peer-DMU, achieving the effects of reducing the gaps among cross-efficiencies. Third, the cross-efficiency evaluation model is based on the conservative point of view, which helps the DMU to distinguish potential competitors among peer-DMUs. Lastly, to solve the non-linear model proposed in this paper, the algorithm is designed to describe how to solve the model. The case of a flexible manufacturing system is used to show the appropriateness of the suggested model and algorithm.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12336	10.1111/exsy.12336													
J								The global system-ranking efficiency model and calculating examples with consideration of the nonhomogeneity of decision-making units	EXPERT SYSTEMS										data envelopment analysis; nonhomogeneity; ranking; system-DEA	DATA ENVELOPMENT ANALYSIS; INDUSTRIAL ENERGY EFFICIENCY; DEA MODELS; CHINA; EMISSIONS; PROTOCOLS; PITFALLS; INPUT	Data envelopment analyses have been widely used to evaluate the relative efficiency of decision-making units (DMUs). However, the traditional data envelopment analysis model has not considered the problem of DMUs' nonhomogeneity. If nonhomogeneous DMUs are evaluated under the same production frontier, conclusions may not be precise. For example, some DMUs' input redundancy and output deficit cannot be adjusted as per planning results, which may lead to mistakes in management. This paper loosens the assumption of DMU homogeneity and builds a global system-ranking efficiency model based on existing literature, which divides the problem of DMUs' nonhomogeneity into external nonhomogeneity and internal homogeneity. Data have been collected from 114 listed enterprises in China's solar power industry, and the analysis results indicate that this paper's model is stable and reliable and can be used as a reference for production managers.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12272	10.1111/exsy.12272													
J								Novel recommendation system based on long-term composition for adaptive web services	COMPUTATIONAL INTELLIGENCE										adaptive web service; long-term composed services; recommended systems; structured system services	ISSUES	In the era of digital web services, composition of features on the fly is inevitable. The Long-term Composed Service (LCS) entertains the composition of features to any extent, since it has an open-ended lifetime. In the proposed research work, we have intended to provide service support to run the business toward a long time commitment. Structure-based recommended system for LCSs (RS-LCSs) is proposed, where user queries and recent updation/requirements are considered for exhibiting the response through the system. In the proposed system, business has been regulated according to the time constraints. We have tested our proposed system on the standard benchmark dataset and quantitative metrics show our proposed method has performed well against the compared methods. The forecasting of business has been done through our model to address the recent queries and new requirements issues to provide an adaptive web service for the business development.																	0824-7935	1467-8640				AUG	2020	36	3					1063	1077		10.1111/coin.12309													
J								Water-body segmentation from satellite images using Kapur's entropy-based thresholding method	COMPUTATIONAL INTELLIGENCE										entropy-based; image segmentation; morphological operations; satellite imagery; threshold	CLASSIFICATION; EXTRACTION; ALGORITHM; NETWORKS	Water body segmentation helps in extracting water bodies like lake, pond, river, and reservoir from high resolution satellite images. This also helps in discovering new water bodies. But, extraction of water bodies from satellite images is much complicated, mainly due to the severe disparity in size, shape, and appearance of the water bodies. In this article, Kapur's entropy-based thresholding method is proposed for the segmentation of water bodies from Very High Resolution (VHR) satellite images. The dataset used in this article is AIRS (Aerial Imagery for Roof Segmentation) dataset, with VHR satellite images, from which only the images with water bodies are considered. Experimental results show that the proposed method yields better segmentation performance with an overall accuracy of 98.43% and Structural Similarity Index rate of 0.9712.																	0824-7935	1467-8640				AUG	2020	36	3					1242	1260		10.1111/coin.12339													
J								Fault diagnosis in wireless sensor network using negative selection algorithm and support vector machine	COMPUTATIONAL INTELLIGENCE										artificial immune system; fault diagnosis; negative selection algorithm; support vector machine; wireless sensor networks	IMMUNE-SYSTEM; CLASSIFICATION	In this article, an improved negative selection algorithm (INSA) has been proposed to identify faulty sensor nodes in wireless sensor network (WSN) and then the faults are classified into soft permanent, soft intermittent, and soft transient fault using the support vector machine technique. The performance metrics such as fault detection accuracy, false alarm rate, false positive rate, diagnosis latency (DL), energy consumption, fault classification accuracy (FCA), and false classification rate (FCR) are used to evaluate the performance of the proposed INSA. The simulation result shows that the INSA gives better result as compared to the existing algorithms in terms of performance metrics. The fault classification performance is measured by FCA and FCR. It has also seen that the proposed algorithm gives less DL and consumes less energy than that of existing algorithms proposed by Mohapatra et al, Zhang et al, and Panda et al for WSN.																	0824-7935	1467-8640				AUG	2020	36	3					1374	1393		10.1111/coin.12380													
J								Application of Quantile Graphs to the Automated Analysis of EEG Signals	NEURAL PROCESSING LETTERS										Electroencephalographic time series; Epilepsy; Complex networks; Quantile graphs; Network measures	SEIZURE DETECTION; EIGENVECTOR METHODS; TRANSFORM; EPILEPSY	Epilepsy is classified as a chronic neurological disorder of the brain and affects approximately 2% of the world population. This disorder leads to a reduction in people's productivity and imposes restrictions on their daily lives. Studies of epilepsy often rely on electroencephalogram (EEG) signals to provide information on the behavior of the brain during seizures. Recently, a map from a time series to a network has been proposed and that is based on the concept of transition probabilities; the series results in a so-called "quantile graph" (QG). Here, this map, which is also called the QG method, is applied for the automatic detection of normal, pre-ictal (preceding a seizure), and ictal (occurring during a seizure) conditions from recorded EEG signals. Our main goal is to illustrate how the differences in dynamics in the EEG signals are reflected in the topology of the corresponding QGs. Based on various network metrics, namely, the clustering coefficient, the shortest path length, the mean jump length, the modularity and the betweenness centrality, our results show that the QG method is able to detect differences in dynamical properties of brain electrical activity from different extracranial and intracranial recording regions and from different physiological and pathological brain states.																	1370-4621	1573-773X				AUG	2020	52	1			SI		5	20		10.1007/s11063-018-9936-z													
J								Fuzzy Time Series Models Using Pliant- and Asymptotically Pliant Arithmetic-Based Inference	NEURAL PROCESSING LETTERS										Fuzzy time series models; Fuzzy arithmetic; Pliant numbers; Asymptotically pliant numbers	FORECASTING-MODEL; NEURAL-NETWORKS; ENROLLMENTS; OPTIMIZATION; INTERVALS; LENGTH	The fuzzy time series modeling techniques proposed in this study are based on a fuzzy inference method in which the fuzzy output is either a so-called pliant or quasi pliant (q-pliant) number. The novelty of the introduced inference method lies in the fact that its fuzzy output is obtained by fuzzy arithmetic operations; namely, via weighted aggregation of pliant numbers or q-pliant numbers, which are the consequents of the fuzzy rules. These fuzzy inference systems are called the pliant arithmetic-based fuzzy inference system (PAFIS) and the quasi pliant arithmetic-based fuzzy inference system (QPAFIS). The advantage of the defuzzification methods of these two systems is twofold. On the one hand, they do not require any numerical integration to generate the crisp output, on the other hand, they run in a constant time. Here, it is discussed how the pliant arithmetic-based fuzzy time series and the quasi pliant arithmetic-based fuzzy time series models can be established by utilizing the PAFIS and QPAFIS methods. Lastly, the modeling capabilities of the introduced methods are also examined on some real-life time series, and the forecasting results are compared with those of some well-known and recent time series forecasting methods. Based on the experimental results, our methods may be viewed as novel viable time series modeling techniques.																	1370-4621	1573-773X				AUG	2020	52	1			SI		21	55		10.1007/s11063-018-9927-0													
J								Ordinal Multi-class Architecture for Predicting Wind Power Ramp Events Based on Reservoir Computing	NEURAL PROCESSING LETTERS										Wind power ramp events; Reservoir computing; Over-sampling; Reanalysis data; Ordinal classification; Kernel mapping	NEURAL-NETWORKS; SPEED; MACHINE; MODELS	Wind power ramp events (WPREs) are strong increases or decreases of wind speed in a short period of time. Predicting WPREs in wind farms is of vital importance given that they can produce damages in the turbines, and, in any case, they suddenly affect the wind farm production. In contrast to previous binary definitions of the prediction problem (ramp vs non-ramp), a three-class prediction model is used in this paper, proposing a novel discretization function, able to detect the nature of WPREs: negative ramp, non-ramp and positive ramp events. Moreover, the natural order of these labels is exploited to obtain better results in the prediction of these events. The independent variables used for prediction include, in this case, past wind speed values and meteorological data obtained from physical models (reanalysis data). Reanalysis will be also used for recovering missing data from the measuring stations in the wind farm. The proposed prediction methodology is based on Reservoir Computing and an over-sampling process for alleviating the high degree of unbalance in the dataset (non-ramp events are much more frequent than ramps). Three elements are combined in the prediction method: a recurrent neural network layer, a nonlinear kernel mapping and an ordinal logistic regression,to exploit the information provided by the order of the classes). Preprocessing is based on a variation of the standard synthetic minority over-sampling technique, which is applied to the reservoir activations (since the direct application over the input variables would damage its temporal structure). The performance of the method is analysed by comparing it against other state-of-the-art classifiers, such as Support Vector Machines, nominal logistic regression, an autoregressive ordinal neural network, or the use of leaky integrator neurons instead of the standard sigmoidal units. From the results obtained, the benefits of the kernel mapping and the ordinal model are clear, and, in general, the performance obtained with the Reservoir Computing approach is shown to be very robust in the detection of ramps.																	1370-4621	1573-773X				AUG	2020	52	1			SI		57	74		10.1007/s11063-018-9922-5													
J								A Multi-resolution Approximation for Time Series	NEURAL PROCESSING LETTERS										Time series; Multi-resolution representation; Classification; Discord discovery	REPRESENTATION	Time series is a common and well-known way for describing temporal data. However, most of the state-of-the-art techniques for analysing time series have focused on generating a representation for a single level of resolution. For analysing of a time series at several levels of resolutions, one would require to compute different representations, one for each resolution level. We introduce a multi-resolution representation for time series based on local trends and mean values. We require the level of resolution as parameter, but it can be automatically computed if we consider the maximum resolution of the time series. Our technique represents a time series using trend-value pairs on each segment belonging to a resolution level. To provide a useful representation for data mining tasks, we also propose dissimilarity measures and a symbolic representation based on the SAX technique for efficient similarity search using a multi-resolution indexing scheme. We evaluate our method for classification and discord discovery tasks over a diversity of data domains, achieving a better performance in terms of efficiency and effectiveness compared with some of the best-known classic techniques. Indeed, for some of the experiments, the time series mining algorithms using our multi-resolution representation were an order of magnitude faster, in terms of distance computations, than the state of the art.																	1370-4621	1573-773X				AUG	2020	52	1			SI		75	96		10.1007/s11063-018-9929-y													
J								Machine Learning Nowcasting of PV Energy Using Satellite Data	NEURAL PROCESSING LETTERS										Photovoltaic energy; Nowcasting; EUMETSAT; Support vector regression; Lasso; Clear sky models	POWER	Satellite-measured radiances are obviously of great interest for photovoltaic (PV) energy prediction. In this work we will use them together with clear sky irradiance estimates for the nowcasting of PV energy productions over peninsular Spain. We will feed them directly into two linear Machine Learning models, Lasso and linear Support Vector Regression (SVR), and two highly non-linear ones, Deep Neural Networks (in particular, Multilayer Perceptrons, MLPs) and Gaussian SVRs. We shall also use a simple clear sky-based persistence model for benchmarking purposes. We consider prediction horizons of up to 6 h, with Gaussian SVR being statistically better than the other models at each horizon, since its errors increase slowly with time (with an average of 1.92% for the first three horizons and of 2.89% for the last three). MLPs performance is close to that of the Gaussian SVR for the longer horizons (with an average of 3.1%) but less so at the initial ones (average of 2.26%), being nevertheless significantly better than the linear models. As it could be expected, linear models give weaker results (in the initial horizons, Lasso and linear SVR have already an error of 3.21% and 3.46%, respectively), but we will take advantage of the spatial sparsity provided by Lasso to try to identify the concrete areas with a larger influence on PV energy nowcasts.																	1370-4621	1573-773X				AUG	2020	52	1			SI		97	115		10.1007/s11063-018-09969-1													
J								An Evaluation of Equity Premium Prediction Using Multiple Kernel Learning with Financial Features	NEURAL PROCESSING LETTERS										Support vector classification; Support vector regression; Financial time series; Multiple kernel learning; Kernel functions for time series	STOCK; RETURNS	This paper introduces and extensively explores a forecasting procedure based on multivariate dynamic kernels to re-examine-under a non-linear, kernel methods framework-the experimental tests reported by Welch and Goyal (Rev Financ Stud 21(4):1455-1508,2008) showing that several variables proposed in the finance literature are of no use as exogenous information to predict the equity premium under linear regressions. For this new approach to equity premium forecasting, kernel functions for time series are used with multiple kernel learning (MKL) in order to represent the relative importance of each of the variables. We find that, in general, the predictive capabilities of the MKL models do not improve consistently with the use of some or all of the variables, nor does the predictability by single kernels, as determined by different resampling procedures that we implement and compare. This fact tends to corroborate the instability already observed by Welch and Goyal for the predictive power of exogenous variables, now in a non-linear modelling framework.																	1370-4621	1573-773X				AUG	2020	52	1			SI		117	134		10.1007/s11063-018-09971-7													
J								Time Series, Spectral Densities and Robust Functional Clustering	NEURAL PROCESSING LETTERS										Time series clustering; Robust clustering; Robust functional data clustering; Spectral analysis	K-MEANS; ALGORITHM; PROPOSAL; MODEL	In this work, a robust clustering algorithm for stationary time series is proposed. The algorithm is based on the use of estimated spectral densities, which are considered as functional data, as the basic characteristic of stationary time series for clustering purposes. A robust algorithm for functional data is then applied to the set of spectral densities. Trimming techniques and restrictions on the scatter within groups reduce the effect of noise in the data and help to prevent the identification of spurious clusters. The procedure is tested in a simulation study and is also applied to a real data set.																	1370-4621	1573-773X				AUG	2020	52	1			SI		135	152		10.1007/s11063-018-9926-1													
J								Fast and Memory-Efficient Import Vector Domain Description	NEURAL PROCESSING LETTERS										One-class learning; Import vector domain description; Nystrom method; Kernel methods	SUPPORT	One-class learning is a classical and hard computational intelligence task. In the literature, there are some effective and powerful solutions to address the problem. There are examples in the kernel machines realm, Support Vector Domain Description, and the recently proposed Import Vector Domain Description (IVDD), which directly delivers the sample probability of belonging to the class. Here, we propose and discuss two optimization techniques for IVDD to significantly improve the memory footprint and consequently to scale to datasets that are larger than the original formulation. We propose two strategies. First, we propose using random features to approximate the gaussian kernel together with a primal optimization algorithm. Second, we propose a Nystrom-like approximation of the functional together with a fast converging and accurate self-consistent algorithm. In particular, we replace the a posteriori sparsity of the original optimization method of IVDD by randomly selecting a priori landmark samples in the dataset. We find this second approximation to be superior. Compared to the original IVDD with the RBF kernel, it achieves high accuracy, is much faster, and grants huge memory savings.																	1370-4621	1573-773X				AUG	2020	52	1			SI		511	524		10.1007/s11063-020-10243-6													
J								Global Exponential Stability of Hybrid Non-autonomous Neural Networks with Markovian Switching	NEURAL PROCESSING LETTERS										Global exponential stability; Hybrid non-autonomous neural networks (HNNNs); Pulse delay; Markovian switching; Halanay inequality	TIME-VARYING DELAYS; SYNCHRONIZATION; PARAMETERS	This paper discusses the global exponential stability for a class of hybrid non-autonomous neural networks (HNNNs) with Markovian switching, which includes the factors of time delays and impulse disturbance. A novel Halanay inequality with cross terms is established by using stochastic analysis technique. Some sufficiency criteria for the global exponential stability of the HNNNs with Markovian switching are derived by the Halanay inequality and some mathematical analysis methods. The results obtained have better fault tolerance and redundancy under certain accuracy than the existing results in the literature. Finally, numerical experiments are provided to illustrate our theoretical results.																	1370-4621	1573-773X				AUG	2020	52	1			SI		525	543		10.1007/s11063-020-10262-3													
J								Adaptive Sampled-Data Observer Design for a Class of Nonlinear Systems with Unknown Hysteresis	NEURAL PROCESSING LETTERS										PI hysteresis; Adaptive sampled-data observer; RBFNNs; Nonlinear systems; UUB	ORDER CHAOTIC SYSTEMS; FRACTIONAL-ORDER; SYNCHRONIZATION	In this paper, a novel adaptive sampled-data observer design is studied for a class of nonlinear systems with unknown Prandtl-Ishlinskii hysteresis and unknown unmatched disturbances based on radial basis function neural networks (RBFNNs). To begin with, we investigate a sampled-data nonlinear system and present sufficient conditions such that the sampled-data nonlinear system is ultimately uniformly bounded (UUB). Then, an adaptive sampled-data observer is designed to estimate the unknown states of the nonlinear system. The unknown hysteresis and the unknown disturbances are approximated by RBFNNs. We also give the learning laws of the weights of RBFNNs, and prove that the estimation errors of the states and the weights are UUB, based on the obtained sufficient conditions and a special constructing Lyapunov-Krasovskii function. Finally, the effectiveness of the proposed design method is verified by numerical simulations.																	1370-4621	1573-773X				AUG	2020	52	1			SI		561	579		10.1007/s11063-020-10275-y													
J								Latent-MVCNN: 3D Shape Recognition Using Multiple Views from Pre-defined or Random Viewpoints	NEURAL PROCESSING LETTERS										3D shape recognition; 3D shape classification; 3D shape retrieval	RETRIEVAL; CLASSIFICATION; MODEL	The Multi-view Convolution Neural Network (MVCNN) has achieved considerable success in 3D shape recognition. However, 3D shape recognition using view-images from random viewpoints has not been yet exploited in depth. In addition, 3D shape recognition using a small number of view-images remains difficult. To tackle these challenges, we developed a novel Multi-view Convolution Neural Network, "Latent-MVCNN" (LMVCNN), that recognizes 3D shapes using multiple view-images from pre-defined or random viewpoints. The LMVCNN consists of three types of sub Convolution Neural Networks. For each view-image, the first type of CNN outputs multiple category probability distributions and the second type of CNN outputs a latent vector to help the first type of CNN choose the decent distribution. The third type of CNN outputs the transition probabilities from the category probability distributions of one view to the category probability distributions of another view, which further helps the LMVCNN to find the decent category probability distributions for each pair of view-images. The three CNNs cooperate with each other to the obtain satisfactory classification scores. Our experimental results show that the LMVCNN achieves competitive performance in 3D shape recognition on ModelNet10 and ModelNet40 for both the pre-defined and the random viewpoints and exhibits promising performance when the number of view-images is quite small.																	1370-4621	1573-773X				AUG	2020	52	1			SI		581	602		10.1007/s11063-020-10268-x													
J								Authority updating: An expert authority evaluation algorithm considering post-evaluation and power indices in social networks	EXPERT SYSTEMS										authority; expert finding; post-evaluation; power indices; social network	GROUP DECISION-MAKING; KNOWLEDGE; CENTRALITY; RANKING; INTERVENTION; INDIVIDUALS; HEURISTICS; SELECTION; ONLINE; FORUMS	In group assessment, the focus is on finding high-authority experts to improve the reliability of assessment results. In this study, we propose an authority updating algorithm while considering the power and judgement reliability of an expert on the basis of social networks and post-evaluations. A network power index is established and used to reflect the power of an expert while considering social networks. The measurement of the judgement reliability of an expert considers the post-evaluation of the objects selected by experts, thereby more scientifically reflecting the reliability of experts. The analysis shows the following: although the social-network structure influences the authority of experts, the influence weakens when the assessment group is a highly or even fully connected group; the network effect may increase the authority of some experts and reduce that of others, and it will weaken as the network connectivity increases; moreover, the judgement reliability and authority of an expert while considering post-evaluation can encourage him/her to make fair assessments and strive to reduce his/her motivation and cognitive biases.																	0266-4720	1468-0394														e12605	10.1111/exsy.12605		AUG 2020											
J								Fast and robust visual tracking with hard balanced focal loss and guided domain adaption	IMAGE AND VISION COMPUTING										Visual tracking; Siamese network; Domain adaptation; Hard balanced focal loss	OBJECT TRACKING; NETWORKS	Recently, Siamese networks based trackers have shown excellent performance in accuracy and speed. However, previous studies treat all training samples equally and use the general feature space without adapting to the specific video during tracking. These trackers ignore the class data imbalance during training and the feature space difference between the generic domain and the current tracking target domain, which limits the robustness of trackers. In this paper, we propose an algorithm for learning a discriminative and self-adaptive feature representation, in order to achieve accurate and robust tracking. During the off-line training stage, a hard balanced focal loss function is utilized to solve the positive-negative samples imbalance and the hard-easy negative samples imbalance. During the tracking phase, an off-line trained guided domain adaptation module is embedded into the Siamese networks, which can quickly transfer the feature space from the general domain to the current video domain by adjusting the search branch channel weights. Our networks are trained in an end-to-end manner and without online updating. Our tracker runs at 130 FPS while achieving favorable performance against the state-of-the-art methods on OTB-2013, OTB-2015, VOT-2016, VOT-2017, GOT-10 K and TC-128 benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103929	10.1016/j.imavis.2020.103929													
J								Viewpoint constrained and unconstrained Cricket stroke localization from untrimmed videos	IMAGE AND VISION COMPUTING										Cricket stroke; Action localization; TIoU; C3D; RNN; GRU	RECOGNITION	In this work, we create two new video datasets for the task of temporal Cricket stroke extraction. The two datasets, namely, the Highlights dataset (with approx. 117K frames) and the Generic dataset (with approx. 1.93M frames), comprise of Cricket telecast videos collected from available online sources and down-sampled to 360x640 at 25FPS. These untrimmed videos have been manually annotated with temporal Cricket strokes considering viewpoint invariance assumption. We construct two learning based localization pipelines which are dependent (Constrained) and independent (Unconstrained) of our viewpoint labeling assumption. The Unconstrained pipeline finetunes a pretrained C3D model with GRU training in disconnected and connected modes, while our Constrained pipeline uses boundary detection with first frame classification for generating the temporal localizations. Two post-processing steps, of filtering and boundary correction, are also discussed which help in improving the overall accuracy values. A modified evaluation metric, Weighted Mean TIoU, for single category temporal localization problem is also presented and compared with the evaluations of the standard rnAP metric (threshold >= 0.5) on the created dataset. The best weighted mean TIoU of our method was 0.9376 and 0.7145 on the Highlights and Generic test partitions, respectively. Moreover, we compare our baseline method with 3D Segment CNNs and Temporal Recurrent Networks (TRNs) which have state of art results on THUMOS 2014 dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103944	10.1016/j.imavis.2020.103944													
J								CrossFusion net: Deep 3D object detection based on RGB images and point clouds in autonomous driving	IMAGE AND VISION COMPUTING										Deep learning; 3D object detection; Data fusion; Autonomous driving		In recent years, accurate 3D detection plays an important role in a lot of applications. Autonomous driving, for instance, is one of typical representatives. This paper aims to design an accurate 3D detector that takes both LiDAR point clouds and RGB images as inputs according to the fact that both LiDAR and camera have their own merits. A deep novel end-to-end two-stream learnable architecture, CrossFusion Net, is designed to exploit features from both LiDAR point clouds as well as RGB images through a hierarchical fusion structure. Specifically, CrossFusion Net utilizes bird's eye view (BEV) of point clouds through projection. Besides, these two feature maps of different streams are fused through the newly introduced CrossFusion(CF) layer. The proposed CF layer transforms feature maps of one stream to another based on the spatial relationship between the BEV and RGB images. Additionally, we apply attention mechanism on the transformed feature map and the original one to automatically decide the importance of the two feature maps from the two sensors. Experiments on the challenging KITTI car 3D detection benchmark and BEV detection benchmark show that the presented approach outperforms the other state-of-the-art methods in average precision(AP), specifically, as well as outperforms UberATG-ContFuse [3] of 8% AP in moderate 3D car detection. Furthermore, the proposed network learns an effective representation in perception of circumstances via RGB feature maps and BEV feature maps. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103955	10.1016/j.imavis.2020.103955													
J								Monocular depth estimation with SPN loss	IMAGE AND VISION COMPUTING										Depth estimation; Monocular depth estimation		Understanding the 3D space is crucial for autonomous vehicles for planning and navigation. Traditionally autonomous vehicles use LiDAR sensor to 3D map its environment. LiDAR sensor data are often noisy and sparse making it not fully reliable for real-time applications like autonomous driving, thus redundant such sensors are used for the purpose. The array of cameras in an autonomous vehide purposed for detection and tracking can be reused for depth estimation as well. In this paper, an unsupervised monocular depth estimation approach for autonomous vehicles which can be used as redundant depth estimators replacing multiple LiDAR sensors. Here, a deep learning based method is used with a multiscale encoder-decoder network to estimate depth. Target view among the stereo pairs is reconstructed by inverse warping the source view using geometric camera projection. The network is guided by the stereo positive-negative(SPN) loss which minimizes the loss between reconstructed view and corresponding stereo ground truth and, also maximizes the loss between reconstructed views and corresponding opposite stereo ground truth. The proposed approach shows state of the art accuracy in autonomous driving dataset KITTI. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103934	10.1016/j.imavis.2020.103934													
J								Joint detection and tracking in videos with identification features	IMAGE AND VISION COMPUTING										Detection; Multi-object tracking; Re-identification; Online; Tracking by detection		Recent works have shown that combining object detection and tracking tasks, in the case of video data, results in higher performance for both tasks, but they require a high frame-rate as a strict requirement for performance. This assumption is often violated in real-world applications, when models run on embedded devices, often at only a few frames per second. Videos at low frame-rate suffer from large object displacements. Here re-identification features may support to match large-displaced object detections, but current joint detection and re-identification formulations degrade the detector performance, as these two are contrasting tasks. In the real-world application having separate detector and re-id models is often not feasible, as both the memory and runtime effectively double. Towards robust long-term tracking applicable to reduced-computational-power devices, we propose the first joint optimization of detection, tracking and re-identification features for videos. Notably, our joint optimization maintains the detector performance, a typical multi-task challenge. At inference time, we leverage detections for tracking (tracking-by-detection) when the objects are visible, detectable and slowly moving in the image. We leverage instead re-identification features to match objects which disappeared (e.g. due to occlusion) for several frames or were not tracked due to fast motion (or low-frame-rate videos). Our proposed method reaches the state-of-the-art on MOT, it ranks 1st in the UA-DETRAC'18 tracking challenge among online trackers, and 3rd overall. (C) 2020 Elsevier B.V. All tights reserved.																	0262-8856	1872-8138				AUG	2020	100								103932	10.1016/j.imavis.2020.103932													
J								Development of an embedded road boundary detection system based on deep learning	IMAGE AND VISION COMPUTING										Deep learning; Embedded system; Hyperbola model; Lane detection; Particle filter	LANE DETECTION; TRACKING; FILTERS; MODEL	The ability to sense the surrounding environment is an important developing technology in the field of automated vehicles. Lane line detection could determine a vehicle's travelable area. An embedded road boundary detection system based on deep learning was developed in this study. The system can detect structured and unstructured roads in a variety of situations. To obtain an image with clear lane markings, a convolution autoencoder with the characteristics of noise reduction and reconstruction was used to remove all objects in the images except lane markings. Then, the feature points of the lane line were extracted, and the lane line was fitted with a hyperbolic model. Finally, a particle filter was used for lane tracking. The road boundary detection system was implemented on the NVIDIA Jetson TX2 platform. Three different situations, day, night, and rainy day were selected to demonstrate the performance of the proposed algorithm. Additionally, to deal with structured roads, some special scenes, such as shadows, tunnels, degenerate lane markings, and blocked lane markings, were considered. According to the experimental results, the accuracy of the proposed lane detection system for structured and unstructured roads was 90.02%. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103935	10.1016/j.imavis.2020.103935													
J								Gender based face aging with cycle-consistent adversarial networks	IMAGE AND VISION COMPUTING										Face aging; Age estimation; Image-to-Image translation; Age group classification; Cycle-Consistent Generative Adversarial Networks; Subjective test	AGE ESTIMATION	Face aging is a task which referred to image synthesis, and the challenge comes from the training dataset, most existing face aging works require paired face images which is difficult to collect. Face images with various ages of the same person can be considered as unpaired images which come from different domains. The degree of aging effect can be influenced by age, gender, race and some other factors. In this paper, we are committed to studying the impact of gender on face aging problem, which involves the processing and modeling of face images. It has been proved that Generative Adversarial Networks(GANs) is competitive in realistic image synthesis, and many works employed Cycle-Consistent Adversarial Networks(CycleGANs) have shown the high performance in unpaired image-to-image translation. To overcome current difficulties and improve the performance of existing models on the face aging tasks, we proposed an innovative Gender-based training method using CycleGAN by pairwise training CycleGAN over several age groups which are grouped by age and gender. We build a constraint model based on gender discrimination to better simulate the expected aging effect of face images. To evaluate our works using subjective method, we have set up a quantitative evaluation mechanism with participants involved in. Compared with other similar subjective evaluation methods, our method is more objective in the demonstration of experimental results. The experimental results show that our method has a more realistic and excellent performance compared to those using CycleGAN for face age synthesis directly. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103945	10.1016/j.imavis.2020.103945													
J								Cross-Correlated Attention Networks for Person Re-Identification	IMAGE AND VISION COMPUTING										Attention; Feature extraction; Cross correlation; Person Re-Identification; Surveillance		Deep neural networks need to make robust inference in the presence of occlusion, background clutter, pose and viewpoint variations -to name a few- when the task of person re-identification is considered. Attention mechanisms have recently proven to be successful in handling the aforementioned challenges to some degree. However previous designs fail to capture inherent inter-dependencies between the attended features; leading to restricted interactions between the attention blocks. In this paper, we propose a new attention module called Cross-Correlated Attention (CCA); which aims to overcome such limitations by maximizing the information gain between different attended regions. Moreover, we also propose a novel deep network that makes use of different attention mechanisms to learn robust and discriminative representations of person images. The resulting model is called the Cross-Correlated Attention Network (COIN). Extensive experiments demonstrate that the CCAN comfortably outperforms current state-of-the-art algorithms by a tangible margin. Modeling the inherentspatial relations between different attended regions within the deep architecture. joint end-to-end cross correlated attention and representational learning. State-of-the-art results in terms of mAP and Rank-1 accuracies across several challenging datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				AUG	2020	100								103931	10.1016/j.imavis.2020.103931													
J								JSNet: A simulation network of JPEG lossy compression and restoration for robust image watermarking against JPEG attack	COMPUTER VISION AND IMAGE UNDERSTANDING										Image compression; JPEG; Convolutional neural network; Robust digital watermarking		Deep learning-based watermarking methods have achieved a better performance in capacity and invisibility than some traditional methods. However, their robustness against JPEG lossy compression attack is still to be improved. To enhance the robustness and construct an end-to-end method, it is urgent to simulate the JPEG lossy compression by a neural network and then introduce it into the deep learning-based watermarking methods. In this paper, a JPEG simulation network JSNet is proposed to reappear the whole procedure of the JPEG lossy compression and restoration except entropy encoding as realistically as possible. The steps of sampling, DCT, and quantization are modeled by the max-pooling layer, convolution layer, and 3D noise-mask, respectively. The proposed JSNet can simulate JPEG lossy compression with any quality factors. To verify the proposed JSNet in improving the robustness against JPEG compression attack, a CNN-based robust watermarking network (CRWNet) is proposed as an application example. The end-to-end CRWNet contains three subnetworks, i.e., embedding subnetwork, JSNet, and extraction subnetwork. Here. the JSNet is regarded as an attack module in the pipeline. Experimental results on two publicly available datasets (ImageNet, BossBase) demonstrate that: (a) the proposed JSNet can well simulate JPEG lossy compression under any quality factors with small Root mean square error (RMSE) values; (b) the proposed CRWNet considering JSNet has achieved an average 30.6 percent advantage over the basic model without consideration of JSNet.																	1077-3142	1090-235X				AUG	2020	197								103015	10.1016/j.cviu.2020.103015													
J								An attention recurrent model for human cooperation detection	COMPUTER VISION AND IMAGE UNDERSTANDING												User cooperative behaviour is mandatory and valuable to warranty data acquisition quality in forensic biometrics. In the present paper, we consider human cooperative behaviour in front of wearable security cameras. Moreover, we propose a human cooperation detection pipeline based on deep learning. Recently, recurrent neural networks (RNN) have shown remarkable performance on several tasks such as image captioning, video analysis, or natural language processing. Our proposal describes an RNN architecture with the aim at detecting whether a human is exhibiting an adversarial behaviour by trying to avoid the camera. This data is obtained by analysing the noise patterns of human movement. More specifically, we are not only providing an extensive analysis on the proposed pipeline considering different configurations and a wide variety of RNN types, but also an ensemble of the generated models to outperform each single model. The experiment has been carried out using videos captured from a mobile device camera (GOTCHA Dataset) and the obtained results have demonstrated the robustness of the proposed method.																	1077-3142	1090-235X				AUG	2020	197								102991	10.1016/j.cviu.2020.102991													
J								High-speed multi-person pose estimation with deep feature transfer	COMPUTER VISION AND IMAGE UNDERSTANDING										Human pose estimation; Feature transfer; Deep learning		Recent advancements in deep learning have significantly improved the accuracy of multi-person pose estimation from RGB images. However, these deep learning methods typically rely on a large number of deep refinement modules to refine the features of body joints and limbs, which hugely reduce the run-time speed and therefore limit the application domain. In this paper, we propose a feature transfer framework to capture the concurrent correlations between body joint and limb features. The concurrent correlations of these features form a complementary structural relationship, which mutually strengthens the network's inferences and reduces the needs of refinement modules. The transfer sub-network is implemented with multiple convolutional layers, and is merged with the body part detection network to form an end-to-end system. The transfer relationship is automatically learned from ground-truth data instead of being manually encoded, resulting in a more general and efficient design. The proposed framework is validated on the multiple popular multi-person pose estimation benchmarks - MPII, COCO 2018 and PoseTrack 2017 and 2018. Experimental results show that our method not only significantly increases the inference speed to 73.8 frame per second (FPS), but also attains comparable state-of-the-art performance.																	1077-3142	1090-235X				AUG	2020	197								103010	10.1016/j.cviu.2020.103010													
J								Visual BMI estimation from face images using a label distribution based method	COMPUTER VISION AND IMAGE UNDERSTANDING										Visual BMI estimation; Label distribution based method; Two-stage learning framework	BODY-MASS INDEX; AGE ESTIMATION; RISK	Body mass index (BMI) analysis from face images is an interesting and challenging topic in machine learning and computer vision. Recent research shows that facial adiposity is associated with BMI prediction. In this work, we investigate the problem of visual BMI estimation from face images by a two-stage learning framework. BMI-related facial features are learned from the first stage. Then a label distribution based BMI estimator is learned by an optimization procedure that is implemented by projecting the features and assigned labels to a new domain which maximizing the correlation between them. Two label assignment strategies are analyzed for modeling the single BMI value as a discrete probability distribution over a range of BMIs. Extensive experiments are conducted on FIW-BMI, Morph II and VIP_attribute datasets. The experimental results show that the two-stage learning framework improves the performance step by step. More importantly, the proposed BMI estimator efficiently reduces the error. It outperforms regression based methods, two label distribution methods and two deep learning methods in most cases.																	1077-3142	1090-235X				AUG	2020	197								102985	10.1016/j.cviu.2020.102985													
J								Joint identification-verification for person re-identification: A four stream deep learning approach with improved quartet loss function	COMPUTER VISION AND IMAGE UNDERSTANDING										Person re-identification; Improved quartet loss; Quartet loss; Triplet loss; Verification; Identification		A deep four-stream convolutional neural network (CNN) is proposed for person re-identification (re-ID) to overcome the poor generalisation of the traditional triplet loss function. Specifically, the proposed method is a four-stream network, taking four input images where two images are from the same identity and the other two are from different identities. The network uses dual identification and verification losses in a single framework to minimise the intra-class distance while maximising the inter-class distance. Extensive experiments illustrate the state-of-the-art performance of the proposed approach on seven challenging person re-ID datasets: VIPeR, CUHK03, CUHK01, PRID2011, i-LIDS, Market-1501, and DukeMTMC-reID. In addition, we build a five-stream network and a four-stream network with an alternate formulation of positive and negative pairs to further explore the performance of the proposed four-stream network. We also demonstrate promising performance when training and testing sets are from different domains, highlighting the real-world applicability of the approach.																	1077-3142	1090-235X				AUG	2020	197								102989	10.1016/j.cviu.2020.102989													
J								Infrared and visible image fusion via gradientlet filter	COMPUTER VISION AND IMAGE UNDERSTANDING										Image fusion; Fuzzy gradient threshold function; Gradientlet filter; Saliency map; Infrared	MULTISCALE TRANSFORM; CONTOURLET TRANSFORM; FRAMEWORK	In this paper, we propose an image filter based on fuzzy gradient threshold function and global optimization, termed as gradientlet filter, from the perspective of luminance and gradient separation. It can remove small gradient textures and noise while maintaining the overall brightness and edge gradients of an image. Based on gradientlet filter and image saliency, we further put forward a new method for infrared and visible image fusion, which can overcome the challenges of low contrast, edge blurring and noise existing in traditional fused images. First, the gradientlet filter is used to decompose source images into approximate layers and residual layers, where the former reflects the overall brightness of source images without edge blurring and noise, and the latter reflects the small gradient texture and noise of source images. Second, according to the characteristics of the approximate and residual layers, we propose contrast and gradient saliency maps and construct corresponding weight matrices. Finally, the fused image is obtained by fusion and reconstruction based on previously obtained sub-images and weight matrices. Extensive experiments on publicly available databases demonstrate the advantages of our method over state-of-the-art methods in terms of maintaining image contrast, improving target saliency, preventing edge blurring, and reducing noise.																	1077-3142	1090-235X				AUG	2020	197								103016	10.1016/j.cviu.2020.103016													
J								A minimum barrier distance for multivariate images with applications	COMPUTER VISION AND IMAGE UNDERSTANDING										Vectorial Dahu pseudo-distance; Minimum barrier distance; Visual saliency; Object segmentation; Mathematical morphology; Tree of shapes	SALIENT OBJECT DETECTION; SEGMENTATION; COMPUTATION; TREE	Distance transforms and the saliency maps they induce are widely used in image processing, computer vision, and pattern recognition. The minimum barrier distance (MBD) has proved to provide accurate results in this context. Recently, Geraud et al. have presented a fast-to-compute alternative definition of this distance, called the Dahu pseudo-distance. This distance is efficient, powerful, and have many important applications. However, it is restricted to grayscale images. In this article we revisit this pseudo-distance. First, we offer an extension to multivariate image. We call this extension the vectorial Dahu pseudo-distance. We provide an efficient way to compute it. This new version is not only able to deal with color images but also multi-spectral and multimodal ones. Besides, through our benchmarks, we demonstrate how robust and competitive the vectorial Dahu pseudo-distance is, compared to other MB-based distances. This shows that this distance is promising for salient object detection, shortest path finding, and object segmentation. Secondly, we combine the Dahu pseudo-distance with the geodesic distance to take into account spatial information from the image. This combination of distances provides efficient results in many applications such as segmentation of thin elements or path finding in images.																	1077-3142	1090-235X				AUG	2020	197								102993	10.1016/j.cviu.2020.102993													
J								Self-supervised on-line cumulative learning from video streams	COMPUTER VISION AND IMAGE UNDERSTANDING										Incremental Learning; Cumulative Learning; Memory Based Learning; Multiple Object Tracking; Face Recognition; Lifelong Learning; Long Term Object Tracking	OBJECT TRACKING; MULTITARGET; CAMERA	We present a novel online self-supervised method for face identity learning from video streams. The method exploits deep face feature descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative descriptor matching solution based on Reverse Nearest Neighbor and a memory based cumulative learning strategy that discards redundant descriptors while time progresses. This allows building a comprehensive and cumulative representation of all the past visual information observed so far. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information.																	1077-3142	1090-235X				AUG	2020	197								102983	10.1016/j.cviu.2020.102983													
J								Scalable learning for bridging the species gap in image-based plant phenotyping	COMPUTER VISION AND IMAGE UNDERSTANDING											SEGMENTATION	The traditional paradigm of applying deep learning - collect, annotate and train on data - is not applicable to image-based plant phenotyping. Data collection involves the growth of many physical samples, imaging them at multiple growth stages and finally manually annotating each image. This process is error-prone, expensive, time consuming and often requires specialised equipment. Almost 400,000 different plant species exist across the world. Each varying greatly in appearance, geometry and structure, a species gap exists between the domain of each plant species. The performance of a model is not generalisable and may not transfer to images of an unseen plant species. With the costs of data collection and number of plant species, it is not tractable to apply deep learning to the automation of plant phenotyping measurements. Hence, training using synthetic data is effective as the cost of data collection and annotation is free. We investigate the use of synthetic data for image-based plant phenotyping. Our conclusions and released data are applicable to the measurement of phenotypic traits including plant area, leaf count, leaf area and shape. In this paper, we validate our proposed approach on leaf instance segmentation for the measurement of leaf area. We study multiple synthetic data training regimes using Mask-RCNN when few or no annotated real data is available. We also present UPGen: a Universal Plant Generator for bridging the species gap. UPGen leverages domain randomisation to produce widely distributed data samples and models stochastic biological variation. A model trained on our synthetic dataset traverses the domain and species gaps. In validating UPGen, the relationship between different data parameters and their effects on leaf segmentation performance is investigated. Imitating a plant phenotyping facility processing a new plant species, our methods outperform standard practices, such as transfer learning from publicly available plant data, by 26.6% and 51.46% on two unseen plant species respectively. We benchmark UPGen by using it to compete in the CVPPP Leaf Segmentation Challenge. Generalising across multiple plant species, our method achieves state-of-the-art performance scoring a mean of 88% across A1-4 test datasets. Our synthetic dataset and pretrained model are available at https://csiro-robotics.github.io/UPGen-Webpage/.																	1077-3142	1090-235X				AUG	2020	197								103009	10.1016/j.cviu.2020.103009													
J								Learning lightweight Multi-Scale Feedback Residual network for single image super-resolution	COMPUTER VISION AND IMAGE UNDERSTANDING										Image super-resolution; CNNs; Residual learning; RNN; Feedback		In the past years, convolutional neural networks (CNNs) have demonstrated great success for single image super-resolution (SISR). However, existing CNNs for SISR generally have two limitations: (1) the network depth is very deep, which not only weakens the information flow from bottom to top but also has a heavy model capacity; (2) the network architectures are often feed-forward, which prevent the previous layers capturing the useful information from the following layers, limiting the feature learning capability. To address these issues, this paper presents a lightweight Multi-scale Feedback Residual network for SISR. Specifically, we design a lightweight feedback-based recurrent neural network (FRNN) tailored to SISR. The FRNN is consists of a series of recursive Densely-Connected Blocks (DCBs) with the Low-Resolution (LR) image features and the output of the former DCB as inputs. Each DCB adaptively fuses multi-level features from the side-output intermediate feature maps to generate a powerful feature representation. Meanwhile, the DCB cascades a set of Multi-scale Residual Blocks (MRBs), each of which has an enlarged field of view to fully capture multi-scale context information. Moreover, the MRB has a novel Multi-Kernel Fusion Block (MKFB) design, which can dynamically adjust the receptive field size of the output feature representation based on the multi-scale inputs. The whole network of our MFRSR is lightweight with only similar to 4.5M parameters, but achieves favorable performance on five benchmark datasets compared to the state-of-the-art methods in terms of PSNR and SSIM.																	1077-3142	1090-235X				AUG	2020	197								103005	10.1016/j.cviu.2020.103005													
J								Hyperspectral image restoration via CNN denoiser prior regularized low-rank tensor recovery	COMPUTER VISION AND IMAGE UNDERSTANDING										Hyperspectral image (HSI); Low-rank tensor decomposition; Plug-and-play; Deep prior; Restoration	SPARSE	Hyperspectral images (HSIs) are widely used in various tasks such as mineral detection and food safety. However, during the imaging process, they are often contaminated by various noises. In this paper for HSIs restoration tasks, we firstly investigate the advantages of traditional physical restoration models and the denoising convolutional neural networks (CNN). For the physical prior of HSIs, a Tucker decomposition based low-rank tensor approximation can fully explore the global correlations in both the spatial and spectral domains. And for the implicit prior, a CNN based method can represent the prior which cannot be designed by mathematical theory tools. Then, we combine the advantages of the two methods to introduce the HSI restoration CNN with the low-rank tensor approximation based regularization in the flexible and extensible plug-and-play framework. The proposed model can be quickly solved using the alternating direction method of multipliers method. Experiments with simulated data and real data show that, compared with competitive methods, the proposed method achieves better HSI restoration results in various quantitative evaluation indicators.																	1077-3142	1090-235X				AUG	2020	197								103004	10.1016/j.cviu.2020.103004													
J								Adversarial examples for replay attacks against CNN-based face recognition with anti-spoofing capability	COMPUTER VISION AND IMAGE UNDERSTANDING										Adversarial examples; Anti-spoofing; Physical domain adversarial examples; Presentation attack; Face authentication		In the race of arms between attackers, trying to build more and more realistic face replay attacks, and defenders, deploying spoof detection modules with ever-increasing capabilities, CNN-based methods have shown outstanding detection performance thus raising the bar for the construction of realistic replay attacks against face-based authentication systems. Rather than trying to rebroadcast even more realistic faces, we show that attackers can successfully fool a face authentication system equipped with a deep learning spoof detection module, by exploiting the vulnerabilities of CNNs to adversarial perturbations. We first show that mounting such an attack is not a trivial task due to the unique features of spoofing detection modules. Then, we propose a method to craft adversarial images that can be successfully exploited to build an effective replay attack. Experiments conducted on the REPLAY-MOBILE database demonstrate that our attacked images achieve good performance against a face recognition system equipped with CNN-based anti-spoofing, in that they are able to pass the face detection, spoof detection and face recognition modules of the authentication chain.																	1077-3142	1090-235X				AUG	2020	197								102988	10.1016/j.cviu.2020.102988													
J								Pyramid Channel-based Feature Attention Network for image dehazing	COMPUTER VISION AND IMAGE UNDERSTANDING										Image dehazing; Deep neural network; Channel attention	MODEL	Traditional deep learning-based image dehazing methods usually use the high-level features (which contain more semantic information) to remove haze in the input image, while ignoring the low-level features (which contain more detail information). In this paper, a Pyramid Channel-based Feature Attention Network (PCFAN) is proposed for single image dehazing, which leverages complementarity among different level features in a pyramid manner with channel attention mechanism. PCFAN consists of three modules: a three-scale feature extraction module, a pyramid channel-based feature attention module (PCFA), and an image reconstruction module. The three-scale feature extraction module simultaneously captures the low-level spatial structural features and the high-level contextual features in different scales. The PCFA module utilizes the feature pyramid and the channel attention mechanism, which effectively extracts interdependent channel maps and selectively aggregates the more important features in a pyramid manner for image dehazing. The image reconstruction module is used to reconstruct features to recover a clear image. Meanwhile, a loss function that combines a mean square error loss part and an edge loss part is employed in PCFAN, which can better preserve image details. Experimental results demonstrate that the proposed PCFAN outperforms existing state-of-the-art algorithms on standard benchmark datasets in terms of accuracy, efficiency, and visual effect. The code will be made publicly available.																	1077-3142	1090-235X				AUG	2020	197								103003	10.1016/j.cviu.2020.103003													
J								Middle-Level Features for the Explanation of Classification Systems by Sparse Dictionary Methods	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										XAI and explainable artificial intelligence; machine learning; sparse coding	CONVOLUTIONAL NEURAL-NETWORKS; DAMAGE DETECTION; ALGORITHMS; PREDICTION	Machine learning (ML) systems are affected by a pervasive lack of transparency. The eXplainable Artificial Intelligence (XAI) research area addresses this problem and the related issue of explaining the behavior of ML systems in terms that are understandable to human beings. In many explanation of XAI approaches, the output of ML systems are explained in terms of low-level features of their inputs. However, these approaches leave a substantive explanatory burden with human users, insofar as the latter are required to map low-level properties into more salient and readily understandable parts of the input. To alleviate this cognitive burden, an alternative model-agnostic framework is proposed here. This framework is instantiated to address explanation problems in the context of ML image classification systems, without relying on pixel relevance maps and other low-level features of the input. More specifically, one obtains sets of middle-level properties of classification inputs that are perceptually salient by applying sparse dictionary learning techniques. These middle-level properties are used as building blocks for explanations of image classifications. The achieved explanations are parsimonious, for their reliance on a limited set of middle-level image properties. And they can be contrastive, because the set of middle-level image properties can be used to explain why the system advanced the proposed classification over other antagonist classifications. In view of its model-agnostic character, the proposed framework is adaptable to a variety of other ML systems and explanation problems.																	0129-0657	1793-6462				AUG	2020	30	8							2050040	10.1142/S0129065720500409													
J								How the Cerebellum and Prefrontal Cortex Cooperate During Trace Eyeblinking Conditioning	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Granular time-sensitivity; spiking neural networks; system-level neuroscience; eye blinking conditioning; prefrontal cortex	SPIKING NEURAL-NETWORKS; MOTOR CORTEX; HIPPOCAMPAL-LESIONS; DOPAMINE RELEASE; BASAL GANGLIA; DELAY; PLASTICITY; MODEL; MECHANISMS; REORGANIZATION	Several data have demonstrated that during the widely used experimental paradigm for studying associative learning, trace eye blinking conditioning (TEBC), there is a strong interaction between cerebellum and medial prefrontal cortex (mPFC). Despite this evidence, the neural mechanisms underlying this interaction are still not clear. Here, we propose a neurophysiologically plausible computational model to address this issue. The model is constrained on the basis of two critical anatomo-physiological features: (i) the cerebello-cortical organization through two circuits, respectively, targeting M1 and mPFC; (ii) the different timing in the plasticity mechanisms of these parallel circuits produced by the granule cells time sensitivity according to which different subpopulations are active at different moments during conditioned stimuli. The computer simulations run with the model suggest that these features are critical to understand how the cooperation between cerebellum and mPFC supports motor areas during TEBC. In particular, a greater trace interval produces greater plasticity changes at the slow path synapses involving mPFC with respect to plasticity changes at the fast path involving M1. As a consequence, the greater is the trace interval, the stronger is the mPFC involvement. The model has been validated by reproducing data collected through recent real mice experiments.																	0129-0657	1793-6462				AUG	2020	30	8							2050041	10.1142/S0129065720500410													
J								Study of the Functional Brain Connectivity and Lower-Limb Motor Imagery Performance After Transcranial Direct Current Stimulation	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										BCI; Motor Imagery; PDC; tDCS	COHERENCE; CORTEX; TDCS; EEG; SYNCHRONIZATION; CLASSIFICATION; EXECUTION; DIAGNOSIS; SELECTION; RECOVERY	The use of transcranial direct current stimulation (tDCS) has been related to the improvement of motor and learning tasks. The current research studies the effects of an asymmetric tDCS setup over brain connectivity, when the subject is performing a motor imagery (MI) task during five consecutive days. A brain-computer interface (BCI) based on electroencephalography is simulated in offline analysis to study the effect that tDCS has over different electrode configurations for the BCI. This way, the BCI performance is used as a validation index of the effect of the tDCS setup by the analysis of the classifier accuracy of the experimental sessions. In addition, the relationship between the brain connectivity and the BCI accuracy performance is analyzed. Results indicate that tDCS group, in comparison to the placebo sham group, shows a higher significant number of connectivity interactions in the motor electrodes during MI tasks and an increasing BCI accuracy over the days. However, the asymmetric tDCS setup does not improve the BCI performance of the electrodes in the intended hemisphere.																	0129-0657	1793-6462				AUG	2020	30	8							2050038	10.1142/S0129065720500380													
J								Multiscaled Neural Autoregressive Distributed Lag: A New Empirical Mode Decomposition Model for Nonlinear Time Series Forecasting	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Feedforward neural networks; empirical mode decomposition; autoregressive distributed lag; bitcoin-oil nexus	RATE PASS-THROUGH; WAVELET TRANSFORM; NETWORK MODEL; METHODOLOGY; PREDICTION; MUSIC	Forecasting has always been the cornerstone of machine learning and statistics. Despite the great evolution of the time series theory, forecasters are still in the hunt for better models to make more accurate decisions. The huge advances in neural networks over the last years has led to the emergence of a new generation of effective models replacing classic econometric models. It is in this direction that we propose, in this paper, a new multiscaled Feedforward Neural Network (FNN), with the aim of forecasting multivariate time series. This new model, called Empirical Mode Decomposition (EMD)-based Neural ARDL, is inspired from the well-known Autoregressive Distributed Lag (ARDL) model being our proposal founded upon the concepts of nonlinearity, EMD-multiresolution and neural networks. These features give the model the ability to effectively capture many nonlinear patterns like the ones often present in econophysical time series, such as nonlinear trends, seasonal effects, long-range dependency, etc. The proposed algorithm can be summarized into the following four basic tasks: (i) EMD breaking-down multivariate time series into different resolution levels, (ii) feeding EMD components from the same levels into a number of feedforward neural ARDL models, (iii) from one level to the next, extrapolating the component corresponding to the response variable (scalar output) a number of steps ahead, and finally, (iv) recombining level-by-level forecasts into a single output. An optimal learning scheme is rigorously designed for efficiently training the new proposed architecture. The approach is finally tested and compared to a number of powerful benchmark models, where experiments are conducted on real-world data.																	0129-0657	1793-6462				AUG	2020	30	8							2050039	10.1142/S0129065720500392													
J								Improved Overlap-based Undersampling for Imbalanced Dataset Classification with Application to Epilepsy and Parkinson's Disease	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Class overlap; imbalanced data; undersampling; classification; adaptive threshold; fuzzy C-means; epilepsy; Parkinson's disease	SAMPLING METHOD; SMOTE	Classification of imbalanced datasets has attracted substantial research interest over the past decades. Imbalanced datasets are common in several domains such as health, finance, security and others. A wide range of solutions to handle imbalanced datasets focus mainly on the class distribution problem and aim at providing more balanced datasets by means of resampling. However, existing literature shows that class overlap has a higher negative impact on the learning process than class distribution. In this paper, we propose overlap-based undersampling methods for maximizing the visibility of the minority class instances in the overlapping region. This is achieved by the use of soft clustering and the elimination threshold that is adaptable to the overlap degree to identify and eliminate negative instances in the overlapping region. For more accurate clustering and detection of overlapped negative instances, the presence of the minority class at the borderline areas is emphasized by means of oversampling. Extensive experiments using simulated and real-world datasets covering a wide range of imbalance and overlap scenarios including extreme cases were carried out. Results show significant improvement in sensitivity and competitive performance with well-established and state-of-the-art methods.																	0129-0657	1793-6462				AUG	2020	30	8							2050043	10.1142/S0129065720500434													
J								Organization incentive driven by modeling of the co-opetition behavior in agent-based complex network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Organizational incentive; Employee psychologies; Game theory; Decision-making behavior		Different from a interaction model of traditional organizational incentive and individual behaviors, the research incorporates the concept of "cognitive bias" into a decision making model of employees' co-opetition behaviors under organizational reform of joint-stock commercial banks based on theories of social dynamics, and establishes a model of employees' co-opetition behaviors under different organizational incentives as well as variation characteristics of employees' co-opetition behaviors. Research results show that in organizational change of commercial banks, incentive measures and communication modes shall be treated in differential manners according to specific stages, so that balanced distribution of resources and effect maximization can be achieved. At the initial stage of reform, strengthening of core employee management shall be emphasized and its diffusion effects shall be utilized positively. In this way, progress of organizational reform can be promoted from the microscopic layer.																	1868-5137	1868-5145				AUG	2020	11	8			SI		3305	3313		10.1007/s12652-019-01517-6													
J								Multi-objective scheduling strategy for scientific workflows in cloud environment: A Firefly-based approach	APPLIED SOFT COMPUTING										Cloud computing; Workflow scheduling; Firefly algorithm; Multi-objective optimization; QoS; Resource utilization	ALGORITHM; OPTIMIZATION; EFFICIENT; AWARE; COST	Cloud computing is a distributed computing paradigm, that provides infrastructure and services to the users using the pay-as-you-use billing model. With the increasing demands and diversity of the scientific workflows, the cloud providers face a fundamental issue of resource provisioning and load balancing. Although, the workflow scheduling in the cloud environment is extensively studied, however, most of the strategies ignore to consider the multiple conflicting objectives of the workflows for scheduling and resource provisioning. To address the above-mentioned issues, in the paper, we introduce a new workflow scheduling strategy using the Firefly algorithm (FA) by considering multiple conflicting objectives including workload of cloud servers, makespan, resource utilization, and reliability. The main purpose of the FA is to find a suitable cloud server for each workflow that can meet its requirements while balancing the loads and resource utilization of the cloud servers. In addition, a rule-based approach is designed to assign the tasks on the suitable VM instances for minimizing the makespan of the workflow while meeting the deadline. The proposed scheduling strategy is evaluated over Google cluster traces using various simulation runs. The control parameters of the FA are also thoroughly investigated for better performance. Through the experimental analysis, we prove that the proposed strategy performs better than the state-of-the-art-algorithms in terms of different Quality-of-Service (QoS) parameters including makespan, reliability, resource utilization and loads of the cloud servers. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106411	10.1016/j.asoc.2020.106411													
J								Performance risk assessment in public-private partnership projects based on adaptive fuzzy cognitive map	APPLIED SOFT COMPUTING										Risk assessment; Structural equation model; Fuzzy cognitive map; Public-private partnership; What-if scenario analysis	CRITICAL SUCCESS FACTORS; ANALYTIC HIERARCHY PROCESS; BAYESIAN BELIEF NETWORK; ROOT CAUSE ANALYSIS; PPP PROJECTS; PROCESS MANAGEMENT; TIME-SERIES; COMPLEXITY; ALLOCATION; FRAMEWORK	High complexity exists underlying public-private partnership (PPP) projects due to their huge scale, large investment, and long-term relationships among various participants, leading to difficulty in managing PPP project performance risk. A robust model that integrates the structural equation model (SEM) and fuzzy cognitive map (FCM) is proposed to perceive and assess the performance risk in PPP projects. SEM is used to learn causal relationships among critical factors representing PPP project performance from the data given. Based on the well-verified SEM, an adaptive FCM model consisting of 14 observed variables and 5 latent variables is built. The proposed approach is capable of performing predictive, diagnostic, and hybrid analysis in various scenarios. Results indicate that variables, including project characteristics (A), project participants (B), project input (C), and project progress (D), all display positive correlations with the target performance (T). Particularly, variables C and D are identified to be more sensitive in ensuring the project satisfactory performance than variables A and B. The optimal risk mitigation strategy can be discovered when the project performance is under an unsatisfactory level. It is found that upgrading the variable with a higher priority would be more efficient to improve the target performance than the variable with a lower priority, which is helpful in both generic and specific situations. The novelty of this research lies in the development of an adaptive FCM model that is capable of learning casual relationships from observed data and assessing risk subjected to uncertainty, subjectivity, and interdependence. The developed model can be used to provide insights into a better understanding of risk mitigation strategies through what-if scenario analysis, enabling to enhance the likelihood of success in PPP projects. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106413	10.1016/j.asoc.2020.106413													
J								Feature selection based on improved binary global harmony search for data classification	APPLIED SOFT COMPUTING										Feature selection; Population-based optimization; Binary harmony search; Data classification	PARTICLE SWARM OPTIMIZATION; BRAIN STORM OPTIMIZATION; ARTIFICIAL BEE COLONY; GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION; HYBRID APPROACH; FUZZY ARTMAP; MODEL; MACHINE; SYSTEM	Harmony search (HS) is an effective meta-heuristic algorithm inspired by the music improvisation process, where musicians search for a pleasing harmony by adjusting their instruments' pitches. The HS algorithm and its variants have been widely used to solve binary and continuous optimization problems. In this paper, we propose an improved binary global harmony search algorithm, called IBGHS, to undertake feature selection problems. A modified improvisation step is introduced to enhance the global search ability and increase the convergence speed of the algorithm. In addition, the K-nearest neighbor (KNN) is used as an underlying learning model to evaluate the effectiveness of the selected feature subsets. The experimental results on eighteen benchmark problems indicate that the proposed IBGHS algorithm is able to produce comparable results as compared with other state-of-the-art population-based methods such as genetic algorithm (GA), particle swarm optimization (PSO), antlion optimizer (ALO), novel global harmony search (NGHS) and whale optimization algorithm (WOA) in solving feature selection problems. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106402	10.1016/j.asoc.2020.106402													
J								Predicting next day direction of stock price movement using machine learning methods with persistent homology: Evidence from Kuala Lumpur Stock Exchange	APPLIED SOFT COMPUTING										Stock price movement prediction; Machine learning methods; Persistent homology; Technical indicators	TOPOLOGICAL DATA-ANALYSIS; MODE DECOMPOSITION; NETWORK; INTELLIGENCE; INDICATORS; RETURNS; MARKETS	Predicting direction of stock price movement is notably important to provide a better guidance to assist market participants in making their investment decisions. This study presents a hybrid method combining machine learning methods with persistent homology to improve the prediction performance. Three stock prices namely Kuala Lumpur Composite Index, Kuala Lumpur Stock Exchange Industrial and Kuala Lumpur Stock Exchange Technology sampled from Kuala Lumpur Stock Exchange are selected for experimental evaluation. In particular, persistent homology was applied to obtain a new and useful input vectors of invariant topological features from returns of these stock prices for further classification task using machine learning methods such as logistic regression, artificial neural network, support vector machine and random forest to predict the next day movement direction of Kuala Lumpur Composite Index. For comparative analysis, we compare the proposed method with others, where the machine learning methods are applied independently on stock returns and also on technical indicators respectively. By using the average of prediction performances and pairwise model comparison method, these two evaluation measures revealed that machine learning methods with persistent homology produced better prediction performance. Our results also demonstrated that the combination of support vector machine with persistent homology generates the best outcome. In general, a combination of machine learning methods with persistent homology is an emerging and promising alternative tool for predicting direction of stock price movement. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106422	10.1016/j.asoc.2020.106422													
J								Optimized fuzzy self-tuning PID controller design based on Tribe-DE optimization algorithm and rule weight adjustment method for load frequency control of interconnected multi-area power systems	APPLIED SOFT COMPUTING										Fuzzy self-tuning PID controller; Load frequency control; Rule weight adjustment method; Tribe-DE optimization algorithm	AUTOMATIC-GENERATION CONTROL; HARMONY SEARCH ALGORITHM; DIFFERENTIAL EVOLUTION; PREDICTIVE CONTROL; AGC	The reliable load frequency control (LFC) is addressed as one of the most important services in the modern electric power system operation and planning. Since the power systems have various structural and non-structural uncertainties, using control methods with fixed parameters may not yield the optimal performance of the system. Thus, in this research, a novel fuzzy PID controller is introduced to LFC for interconnected multi-area power systems in parametric uncertainties as well as external disturbances existence. The proposed algorithm adjusts the scaling factors and the modal parameters of input and output membership functions as well as the weights of fuzzy PID controller rule values. According to the transient response of the area control error (ACE) variable partitioning, the fuzzy rule weight values are obtained. Furthermore, in order to enhance the quality of the response, the scaling factors and the modal parameters of the input and output membership functions of fuzzy PID controllers are optimized by Tribe-DE (TDE) algorithm. The method is examined on the two and three area interconnected power systems at several conditions and an Integral of Time multiplied Absolute Error (ITAE) less than 0.0108 as well as an absolute maximum undershoot less than 0.0210 Hz for Delta f(1) regulation are obtained in the two area interconnected power system. Good transient behavior, disturbance rejection capability and insensitivity to parameter changes are advantages of the proposed controller. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106424	10.1016/j.asoc.2020.106424													
J								SVM kernel based on particle swarm optimized vector and Bayesian optimized SVM in atmospheric particulate matter forecasting	APPLIED SOFT COMPUTING										Artificial neural networks; Environmental pollutant; Particle swarm optimization; Bayesian optimization; Support vector machines	ARTIFICIAL-INTELLIGENCE; PM10 CONCENTRATIONS; INFORMATION-SYSTEM; METEOROLOGICAL CONDITIONS; URBAN AREA; PREDICTION; REGRESSION; ALGORITHM; MODEL; POLLUTION	The application of Artificial Intelligence (AI) has been upgraded in many scientific fields the last years, with the development of new artificial intelligence-based technologies and techniques. Considering that in the literature there is a very limited number of studies proposing and testing new SVM kernels in regression problems, this research introduces a novel SVM Kernel by incorporating a transformed particle swarm optimized ANN weight vector in a Bayesian optimized SVM kernel in a time series problem for predicting the atmospheric pollutant factor Particulate Matter 10 (PM10). The proposed model introduces a new SVM kernel that illustrates an increased forecasting accuracy compared to the conventional optimized ANN and SVM models according to the experimental results. The findings of the proposed methodology illustrate that the new proposed SVM Kernel can be utilized as an improved forecasting technique. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106410	10.1016/j.asoc.2020.106410													
J								Multi-attribute dynamic two-sided matching method of talent sharing market in incomplete preference ordinal environment	APPLIED SOFT COMPUTING										Multi-attribute decision making; Two-sided matching; Incomplete preference ordinal information; Priority; Talent sharing	PARTICIPATION; INNOVATION; IDEAS	With the development of the mobile Internet, big data and cloud computing, talent sharing has emerged and developed in the labor market. As a new economy mode of the Internet, the talent sharing can match surplus labor with relevant needs companies and achieve the maximum interests for two sides with the help of the network platform. In this paper, we deeply investigate the matching problem of talent sharing in incomplete preference ordinal environment. Unlike the traditional twosided matching method, which directly ignores the incomplete preference ordinal, we firstly fill the incomplete preference ordinal by using the collaborative filtering algorithm. In the two-sided matching of talent sharing, we pay full attention to the individual differences of the seekers and the solvers. Because of individual differences, the seekers and the solvers have different preferences for different decision attributes, i.e., the attribute priority matrices. At the same time, considering the psychological expectations of the seekers and the solvers, we construct the satisfaction degree matrices based on the prospect theory. Given the constraints on the attribute priority and the satisfaction degree, the bi-objective optimization model for multi-stage dynamic decision-making is established. Moreover, with the aid of dynamic decision-making process, we can obtain more matches when meeting the psychological expectations of the matching subjects. Finally, a case study of the talent sharing platform Upwork is given to illustrate the validity of our proposed method. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106427	10.1016/j.asoc.2020.106427													
J								A new fuzzy strategy for size and topology optimization of truss structures	APPLIED SOFT COMPUTING										Topology and size optimization; Fuzzy logic; Metaheuristic algorithms	PARTICLE SWARM OPTIMIZATION; DISCRETE BAR AREAS; GLOBAL OPTIMIZATION; SIZING OPTIMIZATION; SEARCH ALGORITHM; DESIGN; SHAPE; CONSTRAINTS	Most of the metaheuristic optimization techniques are the general optimization tools with the ability of solving various class of problems. However, in the case of more complex engineering problems, adding extra customizing procedure(s) can improve their performance. In this regard, the current study deals with introducing a new auxiliary fuzzy decision mechanism to enhance these methods' capability on handling the structural size and topology optimization problems. The proposed mechanism aims to reduce the complexity level of the structural size and topology optimization problems by converting their complex search spaces into simpler fuzzy domains. Introduced fuzzy mechanism, during the optimization process, permanently monitors the population updating process and emphasizes either size or topology search behavior of each agent. Proposed mechanism evaluates agents via two predefined concepts so-called Normalized Objective Function (NOFi) and Normalized Members Density (NMDi). Since the presented fuzzy strategy designed as an independent regulator module, it can be integrated with different optimization algorithms. In the current work, it is combined with the Interactive Search Algorithm (ISA) optimization method and the compound method is named as Fuzzy Tuned Interactive Search Algorithm (FTISA). Eventually, its performance is comparatively assessed on solving a number of structural size and topology optimization problems with dynamic and static constraints. Achieved results demonstrate that the introduced fuzzy strategy not only significantly enhances the computational cost of the process but also improves the accuracy of the solutions and stability of the algorithm. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106412	10.1016/j.asoc.2020.106412													
J								Robust unsupervised dimensionality reduction based on feature clustering for single-cell imaging data	APPLIED SOFT COMPUTING										Feature clustering; Unsupervised dimensionality reduction; Single-cell data; Feature selection; Imaging data	SUPPORT VECTOR MACHINES; FEATURE-SELECTION; CLASSIFICATION; MICROSCOPY; STRATEGIES; CANCER	Biological data, and in particular imaging data, have experienced an exponential growth in terms of volume and complexity in the last few years, raising new challenges in the field of machine learning. Unsupervised problems are of particular relevance, as the generation of labels for the data is often labor-intensive, expensive or simply not possible. However, interpretability of the data and the results is key to extract new valuable knowledge from the large-scale datasets that are studied. This highlights the necessity of adequate unsupervised dimensionality reduction techniques that can lower the computational workload necessary to process the dataset, while at the same time providing information on its structure. This paper describes a framework that brings together previous proposals on unsupervised feature clustering, with the goal of providing a scalable, interpretable and robust dimensionality reduction on single-cell imaging data. The framework integrates several inter-feature dissimilarity measures, clustering algorithms, quality criteria to select the best feature clustering, and dimensionality reduction methods that are built on the clustering. For each of these components, several approaches proposed in previous works have been tested and evaluated on three use cases coming from two different imaging datasets, highlighting the best-performing components. Affinity clustering is applied for feature clustering for the first time. The results were validated using statistical tests, showing that many of the combinations tested lowered the complexity of the datasets while maintaining or improving the accuracy yielded by classifiers applied on them. The analysis highlighted affinity clustering as the best algorithm for feature clustering, with median differences of up to 8.9% and 0.9% in accuracy with respect to FSFS and hierarchical clustering. Representation entropy obtained a median difference of 13.0% and 0.8% with respect to class separability and silhouette index, respectively, as a robust unsupervised criterion to select the cluster set. Dissimilarities based on Pearson's correlation performed slightly better than the alternatives, with a median improvement of 2.8% with respect to the cosine distance. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106421	10.1016/j.asoc.2020.106421													
J								Neuro-fuzzy system dynamics technique for modeling construction systems	APPLIED SOFT COMPUTING										Construction modeling; Hybrid technique; Fuzzy logic; Neuro-fuzzy systems; System dynamics	GENETIC ALGORITHM; INFERENCE SYSTEM; NETWORK; LOGIC; PRODUCTIVITY; VALIDATION	The performance of construction systems (e.g., activities, operations, projects) is commonly measured using different indicators, such as productivity or production rate. The accurate prediction of performance, which is an important concern of construction researchers and practitioners, requires effective techniques for construction modeling. However, the complexity of construction systems creates three challenges for construction modeling: (1) construction systems are affected by numerous interacting factors, (2) the factors that affect construction systems often exhibit both probabilistic and nonprobabilistic uncertainty, and (3) construction systems are dynamic. Fuzzy system dynamics (FSD) is a simulation technique that can be used for modeling construction systems with the potential to address these three challenges. However, the application of FSD in construction is still limited due to its low accuracy for modeling the non-linear, complex, and highly-dimensional relationships that exist between the system variables. Currently, these system relationships are most often defined in FSD by linear regression, due its computational simplicity. This paper introduces a new hybrid technique - neuro-fuzzy system dynamics (N-FSD) - by integrating FSD with hybrid neuro-fuzzy systems. In N-FSD, hybrid neuro-fuzzy systems are used to define the non-linear, complex and high-dimensional relationships between the system variables, which improves the accuracy of FSD models in construction applications. The applicability of the N-FSD technique is tested through a construction case study by modeling the production rate of earthmoving operations. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106400	10.1016/j.asoc.2020.106400													
J								Using Deep Learning for price prediction by exploiting stationary limit order book features	APPLIED SOFT COMPUTING										Limit order book; Stationary features; Price forecasting; Deep Learning	NEURAL-NETWORK; MODEL	The recent surge in Deep Learning (DL) research of the past decade has successfully provided solution to many difficult problems. The field of Quantitative analysis has been slowly adapting the new methods to its problems, but due to problems such as the non-stationary nature of financial data, significant challenges must be overcome before DL is fully utilized. In this work a new method to construct stationary features is proposed such that allows DL models to be applied effectively. These features are thoroughly tested on the task of predicting mid price movements of the Limit Order Book. Several DL models are evaluated such as recurrent Long Short Term Memory (LSTM) networks and Convolutional Neural Networks (CNN). Finally a novel model that combines the ability of the CNN to extract useful features and the ability of LSTMs' to analyse time series, is proposed and evaluated. The combined model is able to outperform the individual LSTM and CNN models in the prediction horizons that are tested. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106401	10.1016/j.asoc.2020.106401													
J								Fuzzy-Model-Based Output Feedback Sliding-Mode Control for Discrete-Time Uncertain Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Output feedback; Nonlinear systems; Lyapunov methods; Fuzzy systems; Silicon; Sliding mode control; Stability analysis; Convex optimization; fuzzy control; nonlinear systems; output feedback; sliding-mode control (SMC)	H-INFINITY CONTROL; VARIABLE-STRUCTURE CONTROL; STABILITY ANALYSIS; CONTROL DESIGN; STABILIZATION	This paper addresses the output feedback sliding-mode control (SMC) problem for discrete-time uncertain nonlinear systems through Takagi-Sugeno fuzzy dynamic models. Combining with the sliding surface, a descriptor system is constructed to characterize the sliding motion dynamics. Sufficient conditions for asymptotic stability analysis of the sliding motion are attained by the piecewise quadratic Lyapunov functions within a convex optimization setup. Two SMC design approaches are proposed to ensure the finite-time convergence of the sliding surface. Two simulation examples are presented to show the effectiveness of the proposed approaches.																	1063-6706	1941-0034				AUG	2020	28	8					1519	1530		10.1109/TFUZZ.2019.2917127													
J								Asynchronous Filtering for Discrete-Time Switched T-S Fuzzy Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Asynchronous switching; discrete-time switched T-S fuzzy system; H-infinity filtering; mode-dependent hybrid dwell time	H-INFINITY; EXPONENTIAL STABILITY; STABILIZATION; GAIN	In this paper, asynchronous H-infinity filtering is studied for discrete-time switched Takagi-Sugeno (T-S) fuzzy systems. New Lyapunov functions are constructed for switched systems with asynchronous switching between subsystems and candidate filters. These improved Lyapunov functions are dependent on the mode of filters instead of subsystems, which are more consistent with the switching mechanism of switched systems with asynchronous switching. Meanwhile, they are quasi-time-dependent, which are effective in achieving more general results than traditional time-independent Lyapunov functions. Based on the new Lyapunov functions, a stability and l(2)-gain criterion is deduced for switched T-S fuzzy systems. Then, switched fuzzy filters are designed to guarantee that the filtering error system is globally uniformly asymptotically stable and has a prescribed H-infinity performance. At last, an illustrative example is provided to demonstrate the potentials and advantages of the proposed filtering scheme.																	1063-6706	1941-0034				AUG	2020	28	8					1531	1541		10.1109/TFUZZ.2019.2917667													
J								Generic Evolving Self-Organizing Neuro-Fuzzy Control of Bio-Inspired Unmanned Aerial Vehicles	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy neural networks; Fuzzy control; Artificial neural networks; Fuzzy logic; Uncertainty; Stability analysis; Lyapunov methods; Evolving; generic; model-free; neuro-fuzzy; self-constructing; sliding mode control (SMC)	SLIDING MODE CONTROL; TRACKING CONTROL; ONLINE IDENTIFICATION; LEARNING ALGORITHM; NETWORK; ROBOT; DESIGN; SYSTEM	In recent times, with the incremental demand for fully autonomous systems, research interests are observed in learning machine-based intelligent, self-organizing, and evolving controllers. In this paper, a new evolving and self-organizing controller, namely generic-controller (G-controller), is proposed. The G-controller works in a fully online mode with minor expert domain knowledge. It is developed by incorporating the sliding mode control (SMC) theory with an advanced incremental learning machine, namely generic evolving neuro-fuzzy inference system. The controller starts operating from scratch with an empty set of fuzzy rule, and therefore, no offline training is required. To cope with the changing dynamic characteristics of the plant, the controller can add or prune the rules on demand. Control law and adaptation laws for the consequent parameters are derived from the SMC algorithm to establish a stable closed-loop system, where the stability of the G-controller is guaranteed by using the Lyapunov function. The uniform asymptotic convergence of tracking error to zero is witnessed through the implication of an auxiliary robustifying control term. In addition, the implementation of the multivariate Gaussian function helps the controller to handle the nonaxis parallel data from the plant and consequently, enhances the robustness against uncertainties and environmental perturbations. Finally, the controller's performance has been evaluated by observing the tracking performance in controlling simulated plants of unmanned aerial vehicle, namely bio-inspired flapping wing micro air vehicle and hexacopter for a variety of trajectories.																	1063-6706	1941-0034				AUG	2020	28	8					1542	1556		10.1109/TFUZZ.2019.2917808													
J								Distribution Information Based Intuitionistic Fuzzy Clustering for Infrared Ship Segmentation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Distribution information; infrared (IR) images; intuitionistic fuzzy c-means (IFCM); ship segmentation	ALGORITHM; OPTIMIZATION	This paper presents a distribution information based intuitionistic fuzzy clustering method for infrared ship segmentation. The algorithm could effectively suppress the influences of nontarget objects with high intensity and intensity inhomogeneity in the infrared ship images. There are mainly two improvements in this paper. First, it proposes a fuzzy clustering algorithm incorporating global distribution information of ship targets in the form of the Gaussian model. The spatial information, along with intensity, is used to exert different effects on different classes. Second, an intuitionistic fuzzy clustering way is incorporated into the process of ship segmentation, which combines the intensity distribution information of the local region. The intuitionistic fuzzy distance and local intensity distribution information would help in solving the problem of intensity inhomogeneity and blurring edges. Experiment results on the dataset containing 200 infrared ship images indicate the superiority of the proposed method compared with other state-of-the-art methods.																	1063-6706	1941-0034				AUG	2020	28	8					1557	1571		10.1109/TFUZZ.2019.2917809													
J								On the Conjunction of Possibility Measures	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Measurement uncertainty; Uncertainty; Possibility theory; Fuzzy sets; Linguistics; Indexes; Extraterrestrial measurements; Aggregation; conjunction compatible; fuzzy measure; possibility theory; uncertainty		In this paper, we review the idea of fuzzy measures and look at some properties of these measures. We describe a procedure for aggregating multiple fuzzy measures. We discuss the use of fuzzy measures for modeling information about uncertain variables. We carefully look at the properties of possibility measures. We look at the issue of conjuncting possibility measures.																	1063-6706	1941-0034				AUG	2020	28	8					1572	1574		10.1109/TFUZZ.2019.2917813													
J								On the Topological Structure of KM Fuzzy Metric Spaces and Normed Spaces	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Extraterrestrial measurements; Probabilistic logic; Fuzzy sets; Information science; Topology; Fuzzy metric space; fuzzy normed space; locally convex; quasi-metric family; quasi-norm family	FIXED-POINT THEOREMS; MAPPINGS; DOMAIN	In this paper, some relations among the axioms of KM fuzzy metric spaces and KM fuzzy normed spaces are discussed, and some characterizations concerning the additional axiom are given. Also, a quasi-metric family and a quasi-norm family corresponding, respectively, to the KM fuzzy metric spaces and the KM fuzzy normed spaces are introduced and studied. Using their properties, the different topological structures of KM fuzzy metric spaces and KM fuzzy normed spaces with variable t-norm are described, and some new properties on these spaces are obtained. As their special cases, the topological structures and properties concerning GV fuzzy metric spaces and GV fuzzy normed spaces are presented.																	1063-6706	1941-0034				AUG	2020	28	8					1575	1584		10.1109/TFUZZ.2019.2917858													
J								A Novel Z-Network Model Based on Bayesian Network and Z-Number	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Bayesian network (BN); dependence assessment; Z-network; Z-network reasoning; Z-number; Z-valuation uncertainty reasoning	DEPENDENCE ASSESSMENT; FUZZY; UNCERTAINTY; INFERENCE	Z-number is an effective model to describe uncertainty in the real world. Under the condition that uncertainty reasoning is an important issue to process information, how to achieve Z-valuation uncertainty reasoning is a problem. As a Z-number involves both fuzzy and probabilistic uncertainty, main difficulty in the problem to be solved is accomplishing both two uncertainties' reasoning. In this paper, a novel Z-network model and its associated reasoning algorithm are proposed to overcome the difficulty. Structure of the proposed Z-network that contains three basic structures is directed acyclic graph, and this is similarly with Bayesian network (BN). Process of reasoning algorithm involves two parts: first, Bayesian reasoning is applied to establish an optimization model for probabilistic uncertainty reasoning in a Z-number; second, the arithmetic approach of discrete Z-number on if-then rule and maximum entropy approach are proposed for fuzzy uncertainty reasoning. Z-network is essentially an extended model on the basis of BN and properties of a Z-number for Z-valuation uncertainty reasoning. In application, a novel framework of dependence assessment in human reliability analysis is proposed based on Z-network, and a case study demonstrates its effectiveness.																	1063-6706	1941-0034				AUG	2020	28	8					1585	1599		10.1109/TFUZZ.2019.2918999													
J								Static Output Feedback Control of Switched Nonlinear Systems With Actuator Faults	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Switches; Actuators; Fuzzy systems; Reliability; Output feedback; Closed loop systems; Actuator faults; fuzzy systems; reliable control; switched systems; static output feedback (SOF) control	MARKOV JUMP SYSTEMS; H-INFINITY CONTROL; TIME; ALGORITHM; DELAY	This paper is focused on the static output feedback (SOF) control problem for a class of switched nonlinear systems with actuator faults. By means of the Takagi-Sugeno fuzzy model, the switched nonlinear plant is described by a family of switched fuzzy systems. Considering transmission failures may occur between controller and actuator, a reliable SOF controller against actuator faults is designed. Sufficient conditions are developed to guarantee the existence of the reliable SOF controller. Furthermore, an iterative algorithm is designed to determine the controller gains, which avoids the conservatism brought by the traditional singular value decomposition method. To validate the effectiveness of the proposed approach, a numerical example is exploited and simulation results are also presented.																	1063-6706	1941-0034				AUG	2020	28	8					1600	1609		10.1109/TFUZZ.2019.2917177													
J								Adaptive Fuzzy Containment Control for Multiple Uncertain Euler-Lagrange Systems With an Event-Based Observer	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy logic; Multi-agent systems; Vehicle dynamics; Uncertainty; Observers; Trajectory; Adaptive fuzzy control; containment control; event-triggered control; Euler-Lagrange (EL) systems	MULTIAGENT SYSTEMS; CONSENSUS; LEADERLESS; NETWORK; DESIGN	This paper considers the containment control problem for multiple Euler-Lagrange systems with unknown nonlinear dynamics, where the dynamics of the leaders are different from those of the followers. In addition, some followers cannot obtain information of the leaders owing to the limited communication range. We first adopt an event-based observer to estimate a trajectory inside the convex hull spanned by states of the leaders, in which continuous communication can be avoided as well. Then, we further utilize the fuzzy logic systems to approximate the unknown nonlinear dynamics and propose an adaptive control scheme. Under the proposed scheme, we can ensure that the states of the followers can converge to the convex hull formed by these states of the leaders. Finally, a simulation example is given to validate the effectiveness of the proposed control scheme.																	1063-6706	1941-0034				AUG	2020	28	8					1610	1619		10.1109/TFUZZ.2019.2919484													
J								Fuzzy Approximation-Based Adaptive Control of Nonlinear Uncertain State Constrained Systems With Time-Varying Delays	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Delays; Time-varying systems; Adaptive control; Delay effects; Artificial neural networks; Nonlinear systems; Adaptive fuzzy control; Barrier Lyapunov functions; full state constraints; Lyapunov-Krasovskii Functionals; time-varying delays	BARRIER LYAPUNOV FUNCTIONS; OUTPUT-FEEDBACK CONTROL; DYNAMIC SURFACE CONTROL; TRACKING CONTROL; NEURAL-NETWORK; OBSERVER; DESIGN	In this paper, a novel adaptive fuzzy tracking control strategy is developed for nonlinear time-varying delayed systems with full state constraints. State constraints and time delays are normally found in various real-life plants, which are two important factors for degrading system performance significantly. In the framework of adaptive control, the effects of state constraints and time-varying delays are removed simultaneously. The integral Barrier Lyapunov functionals (IBLFs) are applied to achieve full-state-constraint satisfactions and remove the need of the transformed error constraints in previous BLFs. The unknown time-varying delays are completely compensated by introducing the separation technique and Lyapunov-Krasovskii functionals (LKFs). The unknown functions existing in systems are approximated by employing fuzzy logic systems (FLSs). With the help of less-adjustable parameters, only one parameter is needed to be adjusted online in each step of control design. The novel strategy can guarantee that a satisfactory tracking performance is achieved and the signals existing in the closed-loop system are bounded. Finally, by presenting simulation results, the efficiency of the proposed approach is revealed.																	1063-6706	1941-0034				AUG	2020	28	8					1620	1630		10.1109/TFUZZ.2019.2919490													
J								On Elliptical Possibility Distributions	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Probability distribution; Covariance matrices; Probability density function; Possibility theory; Optical imaging; Satellites; Probabilistic logic; Change detection; elliptical probability distributions; Hellinger divergence; joint possibility distributions; Kullback-Leibler divergence; multivariate possibility distributions		This paper aims to propose two main contributions in the field of multivariate data analysis through the possibility theory. The first proposition is the definition of a generalized family of multivariate elliptical possibility distributions. These distributions have been derived from a consistent probability-possibility transformation over the family of so-called elliptical probability distributions. The second contribution proposed by this paper is the definition of two divergence measures between possibilistic distributions. We prove that a symmetric version of the Kullback-Leibler divergence guarantees all divergence properties when related to the space of possibility distributions. We further derive analytical expressions of the latter divergence and of the Hellinger divergence for certain possibility distributions pertaining to the elliptical family proposed, especially the normal multivariate possibility divergence in two dimensions. Finally, this paper provides an illustration of the developed possibilistic tools in an application of bi-band change detection between optical satellite images.																	1063-6706	1941-0034				AUG	2020	28	8					1631	1639		10.1109/TFUZZ.2019.2920803													
J								Supereigenvalue Problem to Addition-Min Fuzzy Matrix With Application in P2P File Sharing System	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Eigenvalues and eigenfunctions; Peer-to-peer computing; Programming; Linear matrix inequalities; Algebra; Computational complexity; Addition-min composition; constrained supereigenvalue; fuzzy matrix; fuzzy relation inequality; peer-to-peer (P2P) network; supereigenvalue	OBJECTIVE FUNCTION SUBJECT; RELATION INEQUALITIES CONSTRAINTS; EIGENSPACE STRUCTURE; PROGRAMMING SUBJECT; PERRON ROOT; MAX; ALGORITHM; EIGENVECTORS; OPTIMIZATION; COMPLEXITY	In this paper, supereigenvalue and constrained supereigenvalue problems of an addition-min fuzzy matrix are investigated. In a peer-to-peer file sharing system, the download requirement of the terminals could be described by a constant constraint system with addition-min fuzzy relation inequalities. Without considering the constant constraint system, we first study the supereigenvalue problem. While considering the constant constraint system, the so-called constrained supereigenvalue is further proposed and investigated. A constrained supereigenvalue represents the ratio of the data-download quality to the data-sending quality under a constant constraint. In order to arouse the enthusiasm of the terminals to share their local file data, we aim to maximize the constrained supereigenvalue. A nonlinear programming approach is developed to find the unique maximum constrained supereigenvalue, with some illustrative examples.																	1063-6706	1941-0034				AUG	2020	28	8					1640	1651		10.1109/TFUZZ.2019.2920806													
J								Adaptive Fuzzy Control for High-Order Nonlinear Time-Delay Systems With Full-State Constraints and Input Saturation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Nonlinear systems; Fuzzy logic; Adaptive systems; Fuzzy control; Lyapunov methods; Trajectory; Delays; Adaptive fuzzy control; barrier Lyapunov function (BLF); full-state constraints; high-order nonlinear time-delay systems; input saturation	OUTPUT-FEEDBACK STABILIZATION; NEURAL TRACKING CONTROL; HOMOGENEOUS DOMINATION	This paper investigates adaptive fuzzy tracking control for high-order nonlinear time-delay systems with full-state constraints and input saturation. By adopting fuzzy approximation technique, frequently used growth assumptions imposed on unknown system nonlinearities are removed. High-order barrier Lyapunov functions are employed to prevent the transgression of full-state constraints. The auxiliary subsystem and the Nussbaum gain technique are utilized to analyze the effect of input saturation. Appropriate Lyapunov-Krasovskii functionals are constructed to compensate the adverse effect caused by the unknown time-delay. Novel feasibility conditions are formulated as sufficient conditions for the existence of proposed control. It is rigorously proved that all the closed-loop signals are bounded in the semi-global sense, error signals converge to small bounded compact sets, full-state constraints, and input saturation are not violated, and the arguments of unknown nonlinearities are constrained within a compact set, on which fuzzy approximation is valid. Eventually, the theoretical result is confirmed by two simulation examples.																	1063-6706	1941-0034				AUG	2020	28	8					1652	1663		10.1109/TFUZZ.2019.2920808													
J								Conditional Distributivity Equation for Uninorms With Continuous Underlying Operators	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Seminars; Indexes; Mathematical model; Fuzzy sets; Fuzzy logic; Expert systems; Neural networks; Conditional distributivity; continuous t-conorm; continuous t-norm; continuous underlying operators; uninorm	FUZZY RULE CONFIGURATION; IMPORTATION; LAW	Our investigations are motivated by distributive logical connectives and their generalizations used in fuzzy set theory and focused also by Klement in the Linz Seminar 2000 closing session. This paper is mainly devoted to solving the functional equations of conditional distributivity of U-1 over U-2, where both U-1 and U-2 are uninorms with continuous underlying operators. Then, the almost complete characterization of such a pair (U-1, U-2) of uninorms is given except for only a small fraction of the unit square. Next, we give the complete characterization of the previous pair under some appropriate restriction, that is, the second operator U-2 is an element of U-max boolean OR U-min. Finally, we also construct a counterexample to illustrate that the obtained results are necessary but not sufficient for general cases with unlimited condition.																	1063-6706	1941-0034				AUG	2020	28	8					1664	1678		10.1109/TFUZZ.2019.2920809													
J								A Novel Approach to Observer-Based Fault Estimation and Fault-Tolerant Controller Design for T-S Fuzzy Systems With Multiple Time Delays	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Iron; Actuators; Fuzzy systems; Delays; Observers; Time-varying systems; Fault tolerance; Fault estimation (FE); fuzzy Lyapunov-Krasovskii functional; fault-tolerant control (FTC); multiple time-varying delays; Takagi-Sugeno (T-S) fuzzy systems	DEPENDENT STABILITY ANALYSIS; VARYING DELAY; ROBUST STABILITY; DYNAMIC-SYSTEMS; UNKNOWN INPUTS; STABILIZATION; ACTUATOR; ACCOMMODATION	In this paper, fault estimation (FE) and fault-tolerant control (FTC) are investigated for Takagi-Sugeno (T-S) fuzzy systems with multiple time-varying delays as well as actuator and sensor faults. Local nonlinear models and external disturbances are also considered. Inspired by the information from the (k - 1)th induction FE, a novel observer is addressed to establish the kth error dynamics. The k-step induction FE observer can weaken the effect from input disturbances caused by the derivatives of actuator faults and can perform better FE subject to sensor and actuator faults and multiple time delays simultaneously. Compared with the existing results, the proposed observer can realistically better show the sizes and shapes of the actuator and sensor faults. In addition, according to online information from the k-step FE, an active dynamic output feedback fault-tolerant controller is proposed to make the closed-loop fuzzy system asymptotically stable. Moreover, the fuzzy Lyapunov-Krasovskii functional is developed by introducing some free-weighting matrices such that the delay dependent sufficient conditions are given in the form of a set of linear matrix inequalities (LMIs) with less conservatism for the existence of observer and fault-tolerant controller. At last, two simulation examples are given to prove the advantages and effectiveness of the approach given in this paper.																	1063-6706	1941-0034				AUG	2020	28	8					1679	1693		10.1109/TFUZZ.2019.2921258													
J								Controllability of k-Valued Fuzzy Cognitive Maps	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Controllability; Fuzzy cognitive maps; Cognition; Knowledge based systems; Sufficient conditions; Neural networks; Controllability; dynamics; fuzzy cognitive maps (FCMs); inference system	BOOLEAN NETWORKS; DYNAMICS; DESIGN	Fuzzy cognitive maps (FCMs) as a kind of knowledge-based tools are widely applied to model complex dynamical systems using causal relations. Besides the representation and reasoning of systems behaviors, how to control the given systems into a desirable target by causal objects established by FCMs is also an open problem. Although, so far, there are some existing works about the applications of FCMs on the control-related problems, it is still a lack of the theoretical analysis in this domain. In this paper, the controllability of k-valued FCMs is studied. To improve the universality of models, a temporal extension of generalized FCMs is implemented. By means of semitensor product, the algebraic representation of k-valued FCMs with controls is established and a generalized formula of control-depending network transition matrices is achieved. A necessary and sufficient condition is proved to determine the control-depending fixed points of k-valued FCMs with temporalization. By utilizing three kinds of controls, the controllability of the discrete FCMs is discussed, respectively. The reachability condition of a specific target state from a given initial state at time s is studied, and the reachable set along with the corresponding reachable probability are also provided by analytic formula. Results provide a way to make FCMs evolving into the designed states by controls, which can further conduct the behaviors of the modeled systems in reality. Examples are shown to demonstrate the effectiveness and feasibility of the proposed scheme.																	1063-6706	1941-0034				AUG	2020	28	8					1694	1707		10.1109/TFUZZ.2019.2921263													
J								Reliable Event-Triggered Asynchronous Extended Passive Control for Semi-Markov Jump Fuzzy Systems and Its Application	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Event-triggered mechanism (ETM); mixed H-infinity and passive asynchronous control; semi-Markov jump systems; Takagi-Sugeno (T-S) fuzzy systems	SLIDING MODE CONTROL; H-INFINITY CONTROL; LINEAR-SYSTEMS; TRACKING CONTROL; DELAY SYSTEMS; TIME; STABILITY; STABILIZATION; DESIGN	This paper is concerned with the reliable extended passive control problem for semi-Markov jump Takagi-Sugeno (T-S) fuzzy systems based on an event-triggered mechanism (ETM). An asynchronous approach is utilized to cope with the fuzzy-basis-dependent membership functions in networked fuzzy systems. For fear of the waste of communication resources, an ETM to screen the transmitted data is adopted for T-S fuzzy semi-Markov jump systems under consideration. Owing to the use of Lyapunov stability theory and some novel integral inequalities based on auxiliary functions, some sufficient conditions are obtained to guarantee that the resulting closed-loop system not only meets a prescribed mixed H-infinity and passive performance but also is stochastically stable. Then, by using a skillful matrix decoupling approach, the gains of the fuzzy controller may be expressed specifically. Lastly, the superiority and availability of the developed design method are extensively described and validated via its application to a truck control model.																	1063-6706	1941-0034				AUG	2020	28	8					1708	1722		10.1109/TFUZZ.2019.2921264													
J								Stability Analysis and Estimation of Domain of Attraction for Positive Polynomial Fuzzy Systems With Input Saturation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Stability analysis; Direction-of-arrival estimation; Control systems; Estimation; Nonlinear systems; Thermal stability; Fuzzy systems; Domain of attraction (DOA); imperfect premise matching (IPC) concept; improved Taylor-series-membership-functions-dependent (ITSMFD) method; input saturation; positive polynomial fuzzy model based (PPFMB) control systems	MODEL-BASED CONTROL; DISCRETE-TIME-SYSTEMS; STABILIZATION ANALYSIS; ACTUATOR SATURATION; CONTROLLER-DESIGN; TRACKING CONTROL; LINEAR-SYSTEMS; SUBJECT; DISTURBANCE; DELAY	In this paper, the stability and positivity of positive polynomial fuzzy model based (PPFMB) control system are investigated, in which the positive polynomial fuzzy model and positive polynomial fuzzy controller are allowed to have different premise membership functions from each other. These mismatched premise membership functions can increase the flexibility of controller design; however, it will lead to the conservative results when the stability is analyzed based on the Lyapunov stability theory. To relax the positivity/stability conditions, the improved Taylor-series-membership-functions-dependent (ITSMFD) method is introduced by introducing the sample points information of Taylor-series approximate membership functions, local error information and boundary information of substate space of premise variables into the stability/positivity conditions. Meanwhile, the ITSMFD method is extended to the PPFMB control system with input saturation to relax the estimation of domain of attraction. Finally, simulation examples are presented to verify the feasibility of this method.																	1063-6706	1941-0034				AUG	2020	28	8					1723	1736		10.1109/TFUZZ.2019.2921267													
J								Adaptive Fuzzy Dynamic Surface Control of Nonlinear Constrained Systems With Unknown Virtual Control Coefficients	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy logic; Adaptive systems; Lyapunov methods; Backstepping; Explosions; Complexity theory; Adaptive dynamic surface control; dead zone; full state constraints; unknown virtual control coefficients	NEURAL-CONTROL; EXPONENTIAL SYNCHRONIZATION; TRACKING CONTROL; DELAY SYSTEMS; STABILIZATION; DESIGN; INPUT	This paper studies the problem of adaptive fuzzy dynamic surface control (DSC) of nonstrict-feedback nonlinear systems subject to unknown virtual control coefficients, dead zone, and full state constraints. The Nussbaum gain technique is used to overcome the difficulty caused by the unknown virtual control coefficients. By utilizing the information of tan-type barrier Lyapunov function, the requirement of full state constraints is successfully achieved. In addition, to handle the problem of "explosion of complexity" resulted from backstepping itself, a DSC approach using the sliding mode differentiator is introduced. Then, based on backstepping control, we develop a new adaptive fuzzy DSC strategy, which ensures that all state constrains are not violated via designing parameters appropriately. Meanwhile, other signals existing in the closed-loop system are bounded. Finally, comparative results are provided to illustrate the effectiveness of the proposed approach.																	1063-6706	1941-0034				AUG	2020	28	8					1737	1747		10.1109/TFUZZ.2019.2921277													
J								Coupling Mechanistic Approaches and Fuzzy Logic to Model and Simulate Complex Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Mathematical model; Computational modeling; Fuzzy logic; Biological system modeling; Complex systems; Data models; Analytical models; Complex systems; fuzzy networks; hybrid modeling; systems simulation	QUANTITATIVE SIMULATION; STOCHASTIC SIMULATION; RAS/CAMP/PKA PATHWAY; SIGNAL-TRANSDUCTION; NETWORKS; MSN2	Several mathematical formalisms can be exploited to model complex systems, in order to capture different features of their dynamic behavior and leverage any available quantitative or qualitative data. Correspondingly, either quantitative models or qualitative models can be defined; bridging the gap between these two worlds would allow us to simultaneously exploit the peculiar advantages provided by each modeling approach. However, to date, the attempts in this direction have been limited to specific fields of research. In this paper, we propose a novel, general-purpose computational framework, named Fuzzy-mechanistic modeling of compleX systems (FuzzX), for the analysis of hybrid models consisting of a quantitative (or mechanistic) module and a qualitative module that can reciprocally control each other's dynamic behavior through a common interface. FuzzX takes advantage of precise quantitative information about the system through the definition and simulation of the mechanistic module. At the same time, it describes the behavior of components and their interactions that are not known in full details, by exploiting fuzzy logic for the definition of the qualitative module. We applied FuzzX for the analysis of a hybrid model of a complex biochemical system, characterized by the presence of positive and negative feedback regulations. We show that FuzzX is able to correctly reproduce known emergent behaviors of this system in normal and perturbed conditions. We envision that FuzzX could be employed to analyze any kind of complex system when quantitative information is limited, as well as to extend existing mechanistic models with fuzzy modules to describe those components and interactions of the system that are not fully characterized.																	1063-6706	1941-0034				AUG	2020	28	8					1748	1759		10.1109/TFUZZ.2019.2921517													
J								Filtering in Gaussian Linear Systems With Fuzzy Switches	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Markov processes; Hidden Markov models; Atmospheric modeling; Optical switches; Filtering; Linear systems; Fast filtering; fuzzy switching linear model; triplet Markov models	RANDOM-FIELDS; MARKOV	This paper presents recent results on conditionally Gaussian observed Markov switching models by incorporating fuzzy switches in the model, instead of hard ones. This kind of generalization is of interest for applications involving continuous switching regimes, such as tracking an object using cameras in intermittent sunlight and shadow conditions. The filter developed hereby is recursive, optimal, and exact, up to an approximation of integrals according to some fuzzy measures. Experiences on simulated and on real data-dealing with outdoor air temperature and power consumption of a building-confirm the accuracy and effectiveness of the proposed filter compared with the hard filter with "crisp" switches.																	1063-6706	1941-0034				AUG	2020	28	8					1760	1770		10.1109/TFUZZ.2019.2921944													
J								Probabilistic Forecasting With Fuzzy Time Series	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Forecasting; Time series analysis; Uncertainty; Probabilistic logic; Predictive models; Computational modeling; Stochastic processes; Forecast uncertainty; fuzzy systems; fuzzy time series (FTS); probabilistic forecasting; time series analysis	IMPERIALIST COMPETITIVE ALGORITHM; HYBRID MODEL; PREDICTION; SETS; INTELLIGENCE; ENROLLMENTS; SELECTION; TUTORIAL; RULES; TESTS	In recent years, the demand for developing low computational cost methods to deal with uncertainties in forecasting has been increased. Probabilistic forecasting is a class of forecasting in which the method provides intervals or probability distributions as outcomes of its forecasting. The aim of this paper is, therefore, proposing a new forecasting approach based on fuzzy time series (FTS) that takes advantage of fuzzy and stochastic patterns on data and is capable to deal with point, interval, and distribution forecasts. The method proposed was empirically tested with typical financial time series, and the results were compared with other standard FTS and statistical methods. The results show that the proposed method obtained accurate results and outperformed standard FTS methods. The proposed method also combines versatility, scalability, and low computational cost, making it useful on a wide range of application scenarios.																	1063-6706	1941-0034				AUG	2020	28	8					1771	1784		10.1109/TFUZZ.2019.2922152													
J								On the Relationship Between Similarity Measures and Thresholds of Statistical Significance in the Context of Comparing Fuzzy Sets	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy sets; Standards; Pattern recognition; Image recognition; Risk analysis; Computational modeling; Testing; Fuzzy sets; similarity measure; statistical analysis	RISK ANALYSIS; ENTROPY; DISTANCE	Comparing fuzzy sets by computing their similarity is common, with a large set of measures of similarity available. However, while commonplace in the computational intelligence community, the application and results of similarity measures are less common in the wider scientific context, where statistical approaches are the standard for comparing distributions. This is challenging, as it means that developments around similarity measures arising from the fuzzy community are inaccessible to the wider scientific community and that the fuzzy community fails to take advantage of a strong statistical understanding, which may be applicable to comparing (fuzzy membership) functions. In this paper, we commence a body of work on systematically relating the outputs of similarity measures to the notion of statistically significant difference; that is, how (dis)similar do two fuzzy sets need to be for them to be statistically different? We explain that in this context, it is useful to initially focus on dis-similarity, rather than similarity, as the former aligns directly with the widely used concept of statistical difference. We propose two methods of applying statistical tests to the outputs of fuzzy dissimilarity measures to determine significant difference. We show how the proposed work provides deeper insight into the behavior and possible interpretation of degrees of dis-similarity and, consequently, similarity, and how the interpretation differs with respect to context (e.g., the complexity of the fuzzy sets).																	1063-6706	1941-0034				AUG	2020	28	8					1785	1798		10.1109/TFUZZ.2019.2922161													
J								A Novel Group Decision-Making Method for Interval-Valued Intuitionistic Multiplicative Preference Relations	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Consensus; consistency; goal programming model; group decision-making (GDM); interval-valued intuitionistic multiplicative preference relation (IVIMPR)	CONSISTENCY ANALYSIS; MODELS; COMPATIBILITY; AGGREGATION; RANKING; WEIGHTS	In this paper, we propose a consistency and consensus-based method to solve the group decision-making (GDM) problems within the framework of interval-valued intuitionistic multiplicative preference relations (IVIMPRs). First, we introduce a similarity measure that expresses the similarity between two decision makers (DMs). Then, a similarity-based consensus index is offered to evaluate closeness between individuals' judgments. Based on the consensus index, the concept of acceptable consensus for IVIMPRs is presented. In the sequel, a consistency and consensus improvement model that aims at retaining original opinions of the DMs is introduced to make consistency and consensus of IVIMPRs acceptable. Moreover, the DMs' comprehensive weights are obtained by combining their subjective weights and objective weights. Afterward, to derive priority weights of alternatives, a programming model is established and solved by three approaches considering DMs' different risk attitudes. Finally, a GDM method with IVIMPRs is offered, and its application to select the partners is offered.																	1063-6706	1941-0034				AUG	2020	28	8					1799	1814		10.1109/TFUZZ.2019.2922917													
J								Mean Square Stabilization for Sampled-Data T-S Fuzzy Systems With Random Packet Dropout	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Stability analysis; Packet loss; Fuzzy systems; Symmetric matrices; Numerical stability; Control systems; Markovian packet dropout; mean square stabilization; sampled-data controller; Takagi-Sugeno (T-S) fuzzy system	NETWORKED CONTROL-SYSTEMS; INPUT-DELAY APPROACH; NONLINEAR-SYSTEMS; FEEDBACK STABILIZATION; DESIGN; CONTROLLERS; MODELS	This paper investigates the problem of mean square stabilization for sampled-data Takagi-Sugeno (T-S) fuzzy networked control systems with random packet dropouts. The packet dropout process is modeled as a discrete-time Markovian chain with complete or incomplete accessible transition probability matrix. Based on the Lyapunov function approach, sufficient conditions are derived for the mean square stability. The corresponding sampled-data fuzzy controller design algorithm formulated in terms of linear matrix inequalities is also given based upon the stability conditions. The numerical examples and simulations are provided to illustrate the usefulness of the developed theory.																	1063-6706	1941-0034				AUG	2020	28	8					1815	1824		10.1109/TFUZZ.2019.2923377													
J								Adaptive Fuzzy Output-Feedback Control for Switched Nonlinear Systems With Stable and Unstable Unmodeled Dynamics	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Switches; Nonlinear dynamical systems; Adaptive systems; Fuzzy control; Lyapunov methods; Adaptive fuzzy control; multiple Lyapunov function (MLF); switched nonlinear system; stable and unstable modes; small-gain; unmodeled dynamics	SMALL-GAIN APPROACH; LYAPUNOV FUNCTIONS; STABILITY ANALYSIS; TRACKING CONTROL; NEURAL-CONTROL; DESIGN; STABILIZATION; OBSERVER; APPROXIMATION; ALGORITHM	Dynamic uncertainty is a potential factor destabilizing the closed-loop system. This paper aims at constructing an adaptive fuzzy output-feedback control scheme for a class of switched nonlinear systems interconnected with unmodeled dynamics. In the investigated model, the x-system is interconnected with the unmodeled z-dynamics. Two types of unmodeled dynamics (i.e., all modes are stable and some modes are unstable) are considered in this paper. Separate switched state observer and adaptive fuzzy controller are designed for each mode of the x-system. In our control scheme, only two adaptive parameters are required to update online. With the help of multiple Lyapunov function, two lemmas are proposed to guide us how to determine the input-too-utput gains of the x-system and the unmodeled dynamics when the influence of switching is considered. By using the small-gain approach, the closed-loop switched nonlinear system is guaranteed to be of input-to-state practically stablility (ISpS). With the concept of ISpS, we prove that the closed-loop system's output is convergent to a small neighborhood of zero, and all the signals in the closed-loop system are bounded. Finally, three simulations are also given to illustrate the effectiveness of our main result.																	1063-6706	1941-0034				AUG	2020	28	8					1825	1839		10.1109/TFUZZ.2019.2922165													
J								Fuzzy-Rough Set Bireducts for Data Reduction	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Rough sets; Tools; Uncertainty; Feature extraction; Noise measurement; Training data; Dimensionality reduction; Bireducts; feature selection (FS); fuzzy-rough sets; instance selection		Data reduction is an important step that helps ease the computational intractability for learning techniques when data are large. This is particularly true for the huge datasets that have become commonplace in recent times. The main problem facing both data preprocessors and learning techniques is that data are expanding both in terms of dimensionality and also in terms of the number of data instances. Approaches based on fuzzy-rough sets offer many advantages for both feature selection and classification, particularly for real-valued and noisy data; however, the majority of recent approaches tend to address the task of data reduction in terms of either dimensionality or training data size in isolation. This paper demonstrates how the notion of fuzzy-rough bireducts can be used for the simultaneous reduction of data size and dimensionality. It also shows how bireducts and, therefore, reduced subtables of data can be used not only as a preprocessing tool but also for the learning of compact and robust classifiers. Furthermore, the ideas can also be extended to the unsupervised domain when dealing with unlabeled data. Experimental evaluation of various techniques demonstrate that high levels of simultaneous reduction of both dimensionality and data size can be achieved whilst maintaining robust performance.																	1063-6706	1941-0034				AUG	2020	28	8					1840	1850		10.1109/TFUZZ.2019.2921935													
J								A Neuro-Fuzzy Model for Online Optimal Tuning of PID Controllers in Industrial System Applications to the Mining Sector	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Tuning; Adaptation models; Discharges (electric); Solids; Transfer functions; Neural networks; Automobiles; Artificial neural network (NN); computational intelligence (CI); fuzzy system; online tuning; proportional; derivative and integral (PID) controllers; steady-state error	OUTPUT-FEEDBACK CONTROL; DESIGN; FUTURE; IMPLEMENTATION; ALGORITHM	This paper develops a model for optimal and online tuning of PID controllers and evaluates its performance. The proposed model is based on computational intelligence approaches and can be used in industrial plant operating processes. The proposed tuning model is called PID-neuro-fuzzy model, a formulation based on a structured artificial neural network and fuzzy rules. The neural network is used to perform an optimal adjustment of PID controller gains to ensure the operating point of the system required to reduce the time of accommodation and the steady-state error. A fuzzy system is incorporated into a real-time gain-scheduling scheme to compensate for the possible variations in plant parameters. The performance of the proposed model is evaluated based on its ability to deal with uncertainties and disturbances in the process. The efficiency of the model is investigated through computational simulations in five plants of two industrial processes in the mining sector: 1) solid bulk unloading processes by car dumper and 2) solid bulk resumption process by bucket wheel reclaimers. In conclusion, the main advantage of the proposed model is its adaptability in relation to variations in plant parameters during the operational process.																	1063-6706	1941-0034				AUG	2020	28	8					1864	1877		10.1109/TFUZZ.2019.2923963													
J								A Unified View of Different Axiomatic Measures Defined on L-Fuzzy Sets	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Atanassov intuitionistic fuzzy sets (AIF); interval-valued fuzzy sets (IVF); L-fuzzy sets	SIMILARITY MEASURES; DISTANCE MEASURE; INCLUSION; ENTROPY	The recent literature contains a multitude of extensions of (axiomatic) notions from the context of ordinary fuzzy sets to more general contexts. Using the language of lattices, we provide a general and compact formulation encompassing a large number of those notions and their potential extensions to even more complex frameworks. The new formulation offers a unifying perspective of the different measures and operations between (generalized) fuzzy sets, and has a potential impact on the simplification of the redundant mathematical proofs concerning the formal relations between the different notions and the properties of certain particular constructive definitions.																	1063-6706	1941-0034				AUG	2020	28	8					1878	1886		10.1109/TFUZZ.2019.2923958													
J								A Lyapunov Analysis for Mamdani Type Fuzzy-Based Sliding Mode Control	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Sliding mode control; Stability criteria; Fuzzy logic; Asymptotic stability; Lyapunov methods; Fuzzy control; Lyapunov stability; sliding mode control	STABILITY ANALYSIS; DESIGN	We derive sufficient conditions to guarantee asymptotic stability for fuzzy-based sliding mode controllers applied to single-input single-output nonlinear systems, made in the framework of the Lyapunov functions. Such conditions also preserve the well-recognized features of both sliding mode control and Mamdani-type fuzzy control such as robustness and uncertainties treatment. In addition, the rules for tuning the cores of membership functions are also found. Real-time experiments, made on a one-link pendulum, are performed to illustrate and demonstrate the theoretical results.																	1063-6706	1941-0034				AUG	2020	28	8					1887	1895		10.1109/TFUZZ.2019.2923167													
J								Learning Deep Landmarks for Imbalanced Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Deep learning; Neural networks; Data models; Feature extraction; Hidden Markov models; Task analysis; Learning systems; Classification; deep learning; imbalanced learning	SMOTE	We introduce a deep imbalanced learning framework called learning DEep Landmarks in laTent spAce (DELTA). Our work is inspired by the shallow imbalanced learning approaches to rebalance imbalanced samples before feeding them to train a discriminative classifier. Our DELTA advances existing works by introducing the new concept of rebalancing samples in a deeply transformed latent space, where latent points exhibit several desired properties including compactness and separability. In general, DELTA simultaneously conducts feature learning, sample rebalancing, and discriminative learning in a joint, end-to-end framework. The framework is readily integrated with other sophisticated learning concepts including latent points oversampling and ensemble learning. More importantly, DELTA offers the possibility to conduct imbalanced learning with the assistancy of structured feature extractor. We verify the effectiveness of DELTA not only on several benchmark data sets but also on more challenging real-world tasks including click-through-rate (CTR) prediction, multi-class cell type classification, and sentiment analysis with sequential inputs.																	2162-237X	2162-2388				AUG	2020	31	8					2691	2704		10.1109/TNNLS.2019.2927647													
J								Learning Multi-Level Density Maps for Crowd Counting	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Cameras; Head; Estimation; Feature extraction; Switches; Training; Adaptation models; Balanced loss (BL); convolutional neural network (CNN); crowd counting; multi-level density maps	FEATURES; TRACKING; MULTIPLE; HUMANS; DEEP	People in crowd scenes often exhibit the characteristic of imbalanced distribution. On the one hand, people size varies largely due to the camera perspective. People far away from the camera look smaller and are likely to occlude each other, whereas people near to the camera look larger and are relatively sparse. On the other hand, the number of people also varies greatly in the same or different scenes. This article aims to develop a novel model that can accurately estimate the crowd count from a given scene with imbalanced people distribution. To this end, we have proposed an effective multi-level convolutional neural network (MLCNN) architecture that first adaptively learns multi-level density maps and then fuses them to predict the final output. Density map of each level focuses on dealing with people of certain sizes. As a result, the fusion of multi-level density maps is able to tackle the large variation in people size. In addition, we introduce a new loss function named balanced loss (BL) to impose relatively BL feedback during training, which helps further improve the performance of the proposed network. Furthermore, we introduce a new data set including 1111 images with a total of 49 061 head annotations. MLCNN is easy to train with only one end-to-end training stage. Experimental results demonstrate that our MLCNN achieves state-of-the-art performance. In particular, our MLCNN reaches a mean absolute error (MAE) of 242.4 on the UCF_CC_50 data set, which is 37.2 lower than the second-best result.																	2162-237X	2162-2388				AUG	2020	31	8					2705	2715		10.1109/TNNLS.2019.2933920													
J								Objective Video Quality Assessment Combining Transfer Learning With CNN	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Measurement; Feature extraction; Spatiotemporal phenomena; Video recording; Quality assessment; Databases; Nonlinear distortion; Convolutional neural network (CNN); imbalanced data; transfer learning; video quality assessment (VQA)	VISUAL-ATTENTION; SIMILARITY	Nowadays, video quality assessment (VQA) is essential to video compression technology applied to video transmission and storage. However, small-scale video quality databases with imbalanced samples and low-level feature representations for distorted videos impede the development of VQA methods. In this paper, we propose a full-reference (FR) VQA metric integrating transfer learning with a convolutional neural network (CNN). First, we imitate the feature-based transfer learning framework to transfer the distorted images as the related domain, which enriches the distorted samples. Second, to extract high-level spatiotemporal features of the distorted videos, a six-layer CNN with the acknowledged learning ability is pretrained and finetuned by the common features of the distorted image blocks (IBs) and video blocks (VBs), respectively. Notably, the labels of the distorted IBs and VBs are predicted by the classic FR metrics. Finally, based on saliency maps and the entropy function, we conduct a pooling stage to obtain the quality scores of the distorted videos by weighting the block-level scores predicted by the trained CNN. In particular, we introduce a preprocessing and a postprocessing to reduce the impact of inaccurate labels predicted by the FR-VQA metric. Due to feature learning in the proposed framework, two kinds of experimental schemes including train-test iterative procedures on one database and tests on one database with training other databases are carried out. The experimental results demonstrate that the proposed method has high expansibility and is on a par with some state-of-the-art VQA metrics on two widely used VQA databases with various compression distortions.																	2162-237X	2162-2388				AUG	2020	31	8					2716	2730		10.1109/TNNLS.2018.2890310													
J								RecSys-DAN: Discriminative Adversarial Networks for Cross-Domain Recommender Systems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Generative adversarial networks; Gallium nitride; Recommender systems; Learning systems; Task analysis; Generators; Neural networks; Adversarial learning; domain adaptation; imbalanced data; neural networks; recommender systems (RSs)		Data sparsity and data imbalance are practical and challenging issues in cross-domain recommender systems (RSs). This paper addresses those problems by leveraging the concepts which derive from representation learning, adversarial learning, and transfer learning (particularly, domain adaptation). Although various transfer learning methods have shown promising performance in this context, our proposed novel method RecSys-DAN focuses on alleviating the cross-domain and within-domain data sparsity and data imbalance and learns transferable latent representations for users, items, and their interactions. Different from the existing approaches, the proposed method transfers the latent representations from a source domain to a target domain in an adversarial way. The mapping functions in the target domain are learned by playing a min-max game with an adversarial loss, aiming to generate domain indistinguishable representations for a discriminator. Four neural architectural instances of ResSys-DAN are proposed and explored. Empirical results on real-world Amazon data show that, even without using labeled data (i.e., ratings) in the target domain, RecSys-DAN achieves competitive performance as compared to the state-of-the-art supervised methods. More importantly, RecSys-DAN is highly flexible to both unimodal and multimodal scenarios, and thus it is more robust to the cold-start recommendation which is difficult for the previous methods.																	2162-237X	2162-2388				AUG	2020	31	8					2731	2740		10.1109/TNNLS.2019.2907430													
J								Siamese Neural Networks for User Identity Linkage Through Web Browsing	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Internet; Artificial neural networks; Data models; Learning systems; Couplings; Cost-sensitive classification; loss function; Siamese neural networks (NNs); user identity linkage (UIL)	ONLINE BEHAVIOR; CLASSIFICATION; DOMAINS	Linking online identities of users among countless heterogeneous network services on the Internet can provide an explicit digital representation of users, which can benefit both research and industry. In recent years, user identity linkage (UIL) through the Internet has become an emerging task with great potential and many challenges. Existing works mainly focus on online social networks that consider inconsistent profiles, content, and networks as features or use sparse location-based data sets to link the online behaviors of a real person. To extend the UIL problem to a general scenario, we try to link the web-browsing behaviors of users, which can help to distinguish specific users from others, such as children or malicious users. More specifically, we propose a Siamese neural network (NN) architecture-based UIL (SAUIL) model that learns and compares the highest-level feature representation of input web-browsing behaviors with deep NNs. Although the number of matching and nonmatching pairs for the UIL problem is highly imbalanced, previous studies have not considered imbalanced UIL data sets. Therefore, we further address the imbalanced learning issue by proposing cost-sensitive SAUIL (C-SAUIL) model, which assumes higher costs for misclassifying the minority class. In the experiments, the proposed model is robust and exhibits a good performance on very large, real-world data sets collected from different regions with distinct characteristics.																	2162-237X	2162-2388				AUG	2020	31	8					2741	2751		10.1109/TNNLS.2019.2929575													
J								Deep Least Squares Fisher Discriminant Analysis	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Covariance matrices; Kernel; Training; Proposals; Linear discriminant analysis; Neural networks; Encoding; Deep neural networks (DNNs); Fisher discriminant analysis (FDA); kernel discriminant analysis; nonlinear classifiers	FEATURE-EXTRACTION; ALGORITHM; NETWORKS	While being one of the first and most elegant tools for dimensionality reduction, Fisher linear discriminant analysis (FLDA) is not currently considered among the top methods for feature extraction or classification. In this paper, we will review two recent approaches to FLDA, namely, least squares Fisher discriminant analysis (LSFDA) and regularized kernel FDA (RKFDA) and propose deep FDA (DFDA), a straightforward nonlinear extension of LSFDA that takes advantage of the recent advances on deep neural networks. We will compare the performance of RKFDA and DFDA on a large number of two-class and multiclass problems, many of them involving class-imbalanced data sets and some having quite large sample sizes; we will use, for this, the areas under the receiver operating characteristics (ROCs) curve of the classifiers considered. As we shall see, the classification performance of both methods is often very similar and particularly good on imbalanced problems, but building DFDA models is considerably much faster than doing so for RKFDA, particularly in problems with quite large sample sizes.																	2162-237X	2162-2388				AUG	2020	31	8					2752	2763		10.1109/TNNLS.2019.2906302													
J								Adaptive Chunk-Based Dynamic Weighted Majority for Imbalanced Data Streams With Concept Drift	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Thermal stability; Learning systems; Technological innovation; Detectors; Twitter; Bagging; Predictive models; Concept drift; ensemble methods; imbalance learning; online learning	ENSEMBLE; MACHINE	One of the most challenging problems in the field of online learning is concept drift, which deeply influences the classification stability of streaming data. If the data stream is imbalanced, it is even more difficult to detect concept drifts and make an online learner adapt to them. Ensemble algorithms have been found effective for the classification of streaming data with concept drift, whereby an individual classifier is built for each incoming data chunk and its associated weight is adjusted to manage the drift. However, it is difficult to adjust the weights to achieve a balance between the stability and adaptability of the ensemble classifiers. In addition, when the data stream is imbalanced, the use of a size-fixed chunk to build a single classifier can create further problems; the data chunk may contain too few or even no minority class samples (i.e., only majority class samples). A classifier built on such a chunk is unstable in the ensemble. In this article, we propose a chunk-based incremental learning method called adaptive chunk-based dynamic weighted majority (ACDWM) to deal with imbalanced streaming data containing concept drift. ACDWM utilizes an ensemble framework by dynamically weighting the individual classifiers according to their classification performance on the current data chunk. The chunk size is adaptively selected by statistical hypothesis tests to access whether the classifier built on the current data chunk is sufficiently stable. ACDWM has four advantages compared with the existing methods as follows: 1) it can maintain stability when processing nondrifted streams and rapidly adapt to the new concept; 2) it is entirely incremental, i.e., no previous data need to be stored; 3) it stores a limited number of classifiers to ensure high efficiency; and 4) it adaptively selects the chunk size in the concept drift environment. Experiments on both synthetic and real data sets containing concept drift show that ACDWM outperforms both state-of-the-art chunk-based and online methods.																	2162-237X	2162-2388				AUG	2020	31	8					2764	2778		10.1109/TNNLS.2019.2951814													
J								Discriminative Fast Hierarchical Learning for Multiclass Image Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Visualization; Training; Support vector machines; Vegetation; Deep learning; Correlation; Task analysis; Hierarchical learning; imbalanced learning; multitask SVM classifier; stochastic gradient descent (SGD)	CONCEPT ONTOLOGY; RECOGNITION; EFFICIENT	In this article, a discriminative fast hierarchical learning algorithm is developed for supporting multiclass image classification, where a visual tree is seamlessly integrated with multitask learning to achieve fast training of the tree classifier hierarchically (i.e., a set of structural node classifiers over the visual tree). By partitioning a large number of categories hierarchically in a coarse-to-fine fashion, a visual tree is first constructed and further used to handle data imbalance and identify the interrelated learning tasks automatically (e.g., the tasks for learning the node classifiers for the sibling child nodes under the same parent node are strongly interrelated), and a multitask SVM classifier is trained for each nonleaf node to achieve more effective separation of its sibling child nodes at the next level of the visual tree. Both the internode visual similarities and the interlevel visual correlations are utilized to train more discriminative multitask SVM classifiers and control the interlevel error propagation effectively, and a stochastic gradient descent (SGD) algorithm is developed for learning such multitask SVM classifiers with higher efficiency. Our experimental results have demonstrated that our fast hierarchical learning algorithm can achieve very competitive results on both the classification accuracy rates and the computational efficiency.																	2162-237X	2162-2388				AUG	2020	31	8					2779	2790		10.1109/TNNLS.2019.2948881													
J								Fast Matrix Factorization With Nonuniform Weights on Missing Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Learning systems; Optimization; Task analysis; Matrix decomposition; Predictive models; Complexity theory; Elementwise alternating least squares (eALS); matrix factorization (MF); missing data; recommendation system		Matrix factorization (MF) has been widely used to discover the low-rank structure and to predict the missing entries of data matrix. In many real-world learning systems, the data matrix can be very high dimensional but sparse. This poses an imbalanced learning problem since the scale of missing entries is usually much larger than that of the observed entries, but they cannot be ignored due to the valuable negative signal. For efficiency concern, existing work typically applies a uniform weight on missing entries to allow a fast learning algorithm. However, this simplification will decrease modeling fidelity, resulting in suboptimal performance for downstream applications. In this paper, we weight the missing data nonuniformly, and more generically, we allow any weighting strategy on the missing data. To address the efficiency challenge, we propose a fast learning method, for which the time complexity is determined by the number of observed entries in the data matrix rather than the matrix size. The key idea is twofold: 1) we apply truncated singular value decomposition on the weight matrix to get a more compact representation of the weights and 2) we learn MF parameters with elementwise alternating least squares (eALS) and memorize the key intermediate variables to avoid repeating computations that are unnecessary. We conduct extensive experiments on two recommendation benchmarks, demonstrating the correctness, efficiency, and effectiveness of our fast eALS method.																	2162-237X	2162-2388				AUG	2020	31	8					2791	2804		10.1109/TNNLS.2018.2890117													
J								Iterative Privileged Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Support vector machines; Data models; Predictive models; Context modeling; Training; Learning systems; Image recognition; Gradient boosted trees; learning using privileged information (LUPI)	INFORMATION	While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.																	2162-237X	2162-2388				AUG	2020	31	8					2805	2817		10.1109/TNNLS.2018.2889906													
J								Radial-Based Oversampling for Multiclass Imbalanced Data Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Learning systems; Taxonomy; Machine learning; Task analysis; Clustering algorithms; Proposals; Imbalanced data; machine learning; multiclass imbalance; oversampling	OVER-SAMPLING TECHNIQUE; MINORITY CLASS; DATA-SETS; SMOTE	Learning from imbalanced data is among the most popular topics in the contemporary machine learning. However, the vast majority of attention in this field is given to binary problems, while their much more difficult multiclass counterparts are relatively unexplored. Handling data sets with multiple skewed classes poses various challenges and calls for a better understanding of the relationship among classes. In this paper, we propose multiclass radial-based oversampling (MC-RBO), a novel data-sampling algorithm dedicated to multiclass problems. The main novelty of our method lies in using potential functions for generating artificial instances. We take into account information coming from all of the classes, contrary to existing multiclass oversampling approaches that use only minority class characteristics. The process of artificial instance generation is guided by exploring areas where the value of the mutual class distribution is very small. This way, we ensure a smart oversampling procedure that can cope with difficult data distributions and alleviate the shortcomings of existing methods. The usefulness of the MC-RBO algorithm is evaluated on the basis of extensive experimental study and backed-up with a thorough statistical analysis. Obtained results show that by taking into account information coming from all of the classes and conducting a smart oversampling, we can significantly improve the process of learning from multiclass imbalanced data.																	2162-237X	2162-2388				AUG	2020	31	8					2818	2831		10.1109/TNNLS.2019.2913673													
J								Self-Paced Balance Learning for Clinical Skin Disease Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Diseases; Skin; Complexity theory; Training; Task analysis; Image recognition; Learning systems; Class imbalance; clinical skin disease recognition; complexity level; self-paced balance learning (SPBL)	CONVOLUTIONAL NEURAL-NETWORKS; SUPPORT VECTOR MACHINES; CLASSIFICATION; SMOTE; RULE	Class imbalance is a challenging problem in many classification tasks. It induces biased classification results for minority classes that contain less training samples than others. Most existing approaches aim to remedy the imbalanced number of instances among categories by resampling the majority and minority classes accordingly. However, the imbalanced level of difficulty of recognizing different categories is also crucial, especially for distinguishing samples with many classes. For example, in the task of clinical skin disease recognition, several rare diseases have a small number of training samples, but they are easy to diagnose because of their distinct visual properties. On the other hand, some common skin diseases, e.g., eczema, are hard to recognize due to the lack of special symptoms. To address this problem, we propose a self-paced balance learning (SPBL) algorithm in this paper. Specifically, we introduce a comprehensive metric termed the complexity of image category that is a combination of both sample number and recognition difficulty. First, the complexity is initialized using the model of the first pace, where the pace indicates one iteration in the self-paced learning paradigm. We then assign each class a penalty weight that is larger for more complex categories and smaller for easier ones, after which the curriculum is reconstructed by rearranging the training samples. Consequently, the model can iteratively learn discriminative representations via balancing the complexity in each pace. Experimental results on the SD-198 and SD-260 benchmark data sets demonstrate that the proposed SPBL algorithm performs favorably against the state-of-the-art methods. We also demonstrate the effectiveness of the SPBL algorithm's generalization capacity on various tasks, such as indoor scene image recognition and object classification.																	2162-237X	2162-2388				AUG	2020	31	8					2832	2846		10.1109/TNNLS.2019.2917524													
J								Sparse Supervised Representation-Based Classifier for Uncontrolled and Imbalanced Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Dictionaries; Collaboration; Face recognition; Linear programming; Prototypes; Classification algorithms; Data driven; face recognition; imbalanced classification; spare supervised representation-based classifier (SSRC); sparse representation-based classification (SRC)	FACE RECOGNITION	The sparse representation-based classification (SRC) has been utilized in many applications and is an effective algorithm in machine learning. However, the performance of SRC highly depends on the data distribution. Some existing works proved that SRC could not obtain satisfactory results on uncontrolled data sets. Except the uncontrolled data sets, SRC cannot deal with imbalanced classification either. In this paper, we proposed a model named sparse supervised representation classifier (SSRC) to solve the above-mentioned issues. The SSRC involves the class label information during the test sample representation phase to deal with the uncontrolled data sets. In SSRC, each class has the opportunity to linearly represent the test sample in its subspace, which can decrease the influences of the uncontrolled data distribution. In order to classify imbalanced data sets, a class weight learning model is proposed and added to SSRC. Each class weight is learned from its corresponding training samples. The experimental results based on the AR face database (uncontrolled) and 15 KEEL data sets (imbalanced) with an imbalanced rate ranging from 1.48 to 61.18 prove SSRC can effectively classify uncontrolled and imbalanced data sets.																	2162-237X	2162-2388				AUG	2020	31	8					2847	2856		10.1109/TNNLS.2018.2884444													
J								Deep Neural Architectures for Highly Imbalanced Data in Bioinformatics	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neurons; Self-organizing feature maps; Bioinformatics; Training; Genomics; Computer architecture; Task analysis; Bioinformatics; deep neural architectures; high-class imbalance; precursor microRNA (pre-miRNA) classification	PRE-MIRNAS; PREDICTION; IDENTIFICATION; CLASSIFICATION; MICRORNAS; TOOLS	In the postgenome era, many problems in bioinformatics have arisen due to the generation of large amounts of imbalanced data. In particular, the computational classification of precursor microRNA (pre-miRNA) involves a high imbalance in the classes. For this task, a classifier is trained to identify RNA sequences having the highest chance of being miRNA precursors. The big issue is that well-known pre-miRNAs are usually just a few in comparison to the hundreds of thousands of candidate sequences in a genome, which results in highly imbalanced data. This imbalance has a strong influence on most standard classifiers and, if not properly addressed, the classifier is not able to work properly in a real-life scenario. This work provides a comparative assessment of recent deep neural architectures for dealing with the large imbalanced data issue in the classification of pre-miRNAs. We present and analyze recent architectures in a benchmark framework with genomes of animals and plants, with increasing imbalance ratios up to 1:2000. We also propose a new graphical way for comparing classifiers performance in the context of high-class imbalance. The comparative results obtained show that, at a very high imbalance, deep belief neural networks can provide the best performance.																	2162-237X	2162-2388				AUG	2020	31	8					2857	2867		10.1109/TNNLS.2019.2914471													
J								On the Dynamics of Classification Measures for Imbalanced and Streaming Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data visualization; Atmospheric measurements; Particle measurements; Histograms; Task analysis; Size measurement; Sensitivity; Class imbalance; classification measures; concept drift; data streams; measure gradients; measure histograms		As each imbalanced classification problem comes with its own set of challenges, the measure used to evaluate classifiers must be individually selected. To help researchers make this decision in an informed manner, experimental and theoretical investigations compare general properties of measures. However, existing studies do not analyze changes in measure behavior imposed by different imbalance ratios. Moreover, several characteristics of imbalanced data streams, such as the effect of dynamically changing class proportions, have not been thoroughly investigated from the perspective of different metrics. In this paper, we study measure dynamics by analyzing changes of measure values, distributions, and gradients with diverging class proportions. For this purpose, we visualize measure probability mass functions and gradients. In addition, we put forward a histogram-based normalization method that provides a unified, probabilistic interpretation of any measure over data sets with different class distributions. The results of analyzing eight popular classification measures show that the effect class proportions have on each measure is different and should be taken into account when evaluating classifiers. Apart from highlighting imbalance-related properties of each measure, our study shows a direct connection between class ratio changes and certain types of concept drift, which could be influential in designing new types of classifiers and drift detectors for imbalanced data streams.																	2162-237X	2162-2388				AUG	2020	31	8					2868	2878		10.1109/TNNLS.2019.2899061													
J								Nonpooling Convolutional Neural Network Forecasting for Seasonal Time Series With Trends	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Market research; Artificial neural networks; Time series analysis; Data models; Correlation; Neurons; Mathematical model; Automatic learning; convolutional layer; neural network (NN); pooling layer; time series		This article focuses on a problem important to automatic machine learning: the automatic processing of a nonpreprocessed time series. The convolutional neural network (CNN) is one of the most popular neural network (NN) algorithms for pattern recognition. Seasonal time series with trends are the most common data sets used in forecasting. Both the convolutional layer and the pooling layer of a CNN can be used to extract important features and patterns that reflect the seasonality, trends, and time lag correlation coefficients in the data. The ability to identify such features and patterns makes CNN a good candidate algorithm for analyzing seasonal time-series data with trends. This article reports our experimental findings using a fully connected NN (FNN), a nonpooling CNN (NPCNN), and a CNN to study both simulated and real time-series data with seasonality and trends. We found that convolutional layers tend to improve the performance, while pooling layers tend to introduce too many negative effects. Therefore, we recommend using an NPCNN when processing seasonal time-series data with trends. Moreover, we suggest using the Adam optimizer and selecting either a rectified linear unit (ReLU) function or a linear activation function. Using an NN to analyze seasonal time series with trends has become popular in the NN community. This article provides an approach for building a network that fits time-series data with seasonality and trends automatically.																	2162-237X	2162-2388				AUG	2020	31	8					2879	2888		10.1109/TNNLS.2019.2934110													
J								SRGC-Nets: Sparse Repeated Group Convolutional Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convolutional neural networks (CNNs); deep learning; group convolution; sparse repeated group convolution (SRGC); SRGC neural networks (SRGC-Nets)	PRUNING ALGORITHM	Group convolution is widely used in many mobile networks to remove the filter's redundancy from the channel extent. In order to further reduce the redundancy of group convolution, this article proposes a novel repeated group convolutional (RGC) kernel, which has M primary groups, and each primary group includes N tiny groups. In every primary group, the same convolutional kernel is repeated in all the tiny groups. The RGC filter is the first kernel to remove the redundancy . from group extent. Based on RGC, a sparse RGC (SRGC) kernel is also introduced in this article, and its corresponding network is called SRGC neural networks (SRGC-Net). The SRGC kernel is the summation of RGC kernel and pointwise group convolutional (PGC) kernel. The number of PGC's groups is M. Accordingly, in each primary group, besides the center locations in all channels, the values of parameters located in other N-1 tiny groups are all zero. Therefore, SRGC can significantly reduce the parameters. Moreover, it can also effectively retrieve spatial and channel-difference features by utilizing RGC and PGC to preserve the richness of produced features. Comparative experiments were performed on the benchmark classification data sets. Compared with the traditional popular networks, SRGC-Nets can perform better with timely reducing the model size and computational complexity. Furthermore, it can also achieve better performances than other latest state-of-the-art mobile networks on most of the databases and effectively decrease the test and training runtime.																	2162-237X	2162-2388				AUG	2020	31	8					2889	2902		10.1109/TNNLS.2019.2933665													
J								Homophily Preserving Community Detection	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Topology; Network topology; Symmetric matrices; Social networking (online); Data mining; Graphical models; Learning systems; Community detection; graph clustering; node homophily; nonnegative matrix factorization (NMF)	NETWORKS; FACTORIZATION	As a fundamental problem in social network analysis, community detection has recently attracted wide attention, accompanied by the output of numerous community detection methods. However, most existing methods are developed by only exploiting link topology, without taking node homophily (i.e., node similarity) into consideration. Thus, much useful information that can be utilized to improve the quality of detected communities is ignored. To overcome this limitation, we propose a new community detection approach based on nonnegative matrix factorization (NMF), namely, homophily preserving NMF (HPNMF), which models not only link topology but also node homophily of networks. As such, HPNMF is able to better reflect the inherent properties of community structure. In order to capture node homophily from scratch, we provide three similarity measurements that naturally reveal the association relationships between nodes. We further present an efficient learning algorithm with convergence guarantee to solve the proposed model. Finally, extensive experiments are conducted, and the results demonstrate that HPNMF has strong ability to outperform the state-of-the-art baseline methods.																	2162-237X	2162-2388				AUG	2020	31	8					2903	2915		10.1109/TNNLS.2019.2933850													
J								Evolutionary Compression of Deep Neural Networks for Biomedical Image Segmentation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Biomedical imaging; Image segmentation; Image coding; Biological system modeling; Task analysis; Computer architecture; Biological neural networks; Biomedical image segmentation; deep neural networks (DNNs); evolutionary algorithm (EA); multiobjective optimization	BLOOD-VESSEL SEGMENTATION; RETINAL IMAGES; ALGORITHM; CLASSIFICATION; LEVEL	Biomedical image segmentation is lately dominated by deep neural networks (DNNs) due to their surpassing expert-level performance. However, the existing DNN models for biomedical image segmentation are generally highly parameterized, which severely impede their deployment on real-time platforms and portable devices. To tackle this difficulty, we propose an evolutionary compression method (ECDNN) to automatically discover efficient DNN architectures for biomedical image segmentation. Different from the existing studies, ECDNN can optimize network loss and number of parameters simultaneously during the evolution, and search for a set of Pareto-optimal solutions in a single run, which is useful for quantifying the tradeoff in satisfying different objectives, and flexible for compressing DNN when preference information is uncertain. In particular, a set of novel genetic operators is proposed for automatically identifying less important filters over the whole network. Moreover, a pruning operator is designed for eliminating convolutional filters from layers involved in feature map concatenation, which is commonly adopted in DNN architectures for capturing multi-level features from biomedical images. Experiments carried out on compressing DNN for retinal vessel and neuronal membrane segmentation tasks show that ECDNN can not only improve the performance without any retraining but also discover efficient network architectures that well maintain the performance. The superiority of the proposed method is further validated by comparison with the state-of-the-art methods.																	2162-237X	2162-2388				AUG	2020	31	8					2916	2929		10.1109/TNNLS.2019.2933879													
J								Scalable Distributed Filtering for a Class of Discrete-Time Complex Networks Over Time-Varying Topology	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Complex networks; Topology; Couplings; Covariance matrices; Heuristic algorithms; Scalability; Complex networks; distributed filtering; error boundedness; monotonicity; recursive algorithm; time-varying topology	STATE ESTIMATION; DYNAMIC-MODELS; SYSTEMS; SYNCHRONIZATION; IDENTIFICATION; SATURATIONS; STABILITY; SCALE	This article is concerned with the distributed filtering problem for a class of discrete complex networks over time-varying topology described by a sequence of variables. In the developed scalable filtering algorithm, only the local information and the information from the neighboring nodes are used. As such, the proposed filter can be implemented in a truly distributed manner at each node, and it is no longer necessary to have a certain center node collecting information from all the nodes. The aim of the addressed filtering problem is to design a time-varying filter for each node such that an upper bound of the filtering error covariance is ensured and the desired filter gain is then calculated by minimizing the obtained upper bound. The filter is established by solving two sets of recursive matrix equations, and thus, the algorithm is suitable for online application. Sufficient conditions are provided under which the filtering error is exponentially bounded in mean square. The monotonicity of the filtering error with respect to the coupling strength is discussed as well. Finally, an illustrative example is presented to demonstrate the feasibility and effectiveness of our distributed filtering strategy.																	2162-237X	2162-2388				AUG	2020	31	8					2930	2941		10.1109/TNNLS.2019.2934131													
J								Barrier Function-Based Adaptive Control for Uncertain Strict-Feedback Systems Within Predefined Neural Network Approximation Sets	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Artificial neural networks; Lyapunov methods; Adaptive control; Nonlinear systems; Learning systems; Stability analysis; Adaptive control; barrier function (BF); global stability; neural networks (NNs); strict-feedback systems	DYNAMIC SURFACE CONTROL; NONLINEAR-SYSTEMS; BACKSTEPPING CONTROL; TRACKING CONTROL; ROBUST; IDENTIFICATION	In this article, a globally stable adaptive control strategy for uncertain strict-feedback systems is proposed within predefined neural network (NN) approximation sets, despite the presence of unknown system nonlinearities. In contrast to the conventional adaptive NN control results in the literature, a primary benefit of the developed approach is that the barrier Lyapunov function is employed to predefine the compact set for maintaining the validity of NN approximation at each step, thus accomplishing the global boundedness of all the closed-loop signals. Simulation results are performed to clarify the effectiveness of the proposed methodology.																	2162-237X	2162-2388				AUG	2020	31	8					2942	2954		10.1109/TNNLS.2019.2934403													
J								Synchronization and Consensus in Networks of Linear Fractional-Order Multi-Agent Systems via Sampled-Data Control	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Synchronization; Couplings; Multi-agent systems; Laplace equations; Protocols; Control systems; Autonomous agents; Consensus; fractional-order; multi-agent systems; sampled-data control; synchronization	LEADER-FOLLOWING CONSENSUS; GLOBAL ASYMPTOTICAL PERIODICITY; DELAYED NEURAL-NETWORKS; SUFFICIENT CONDITIONS; COOPERATIVE CONTROL; O(T(-ALPHA)) STABILITY; CONTAINMENT CONTROL; ARRAY	This article addresses synchronization and consensus problems in networks of linear fractional-order multi-agent systems (LFOMAS) via sampled-data control. First, under very mild assumptions, the necessary and sufficient conditions are obtained for achieving synchronization in networks of LFOMAS. Second, the results of synchronization are applied to solve some consensus problems in networks of LFOMAS. In the obtained results, the coupling matrix does not have to be a Laplacian matrix, its off-diagonal elements do not have to be nonnegative, and its row-sum can be nonzero. Finally, the validity of the theoretical results is verified by three simulation examples.																	2162-237X	2162-2388				AUG	2020	31	8					2955	2964		10.1109/TNNLS.2019.2934648													
J								A Double-Variational Bayesian Framework in Random Fourier Features for Indefinite Kernels	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Kernel; Bayes methods; Approximation algorithms; Inference algorithms; Acceleration; Learning systems; Gaussian mixture model; Indefinite kernel; kernel approximation; random Fourier features (RFFs); variational inference	INFERENCE; SVM	Random Fourier features (RFFs) have been successfully employed to kernel approximation in large-scale situations. The rationale behind RFF relies on Bochner's theorem, but the condition is too strict and excludes many widely used kernels, e.g., dot-product kernels (violates the shift-invariant condition) and indefinite kernels [violates the positive definite (PD) condition]. In this article, we present a unified RFF framework for indefinite kernel approximation in the reproducing kernel Krein spaces (RKKSs). Besides, our model is also suited to approximate a dot-product kernel on the unit sphere, as it can be transformed into a shift-invariant but indefinite kernel. By the Kolmogorov decomposition scheme, an indefinite kernel in RKKS can be decomposed into the difference of two unknown PD kernels. The spectral distribution of each underlying PD kernel can be formulated as a nonparametric Bayesian Gaussian mixtures model. Based on this, we propose a double-infinite Gaussian mixture model in RFF by placing the Dirichlet process prior. It takes full advantage of high flexibility on the number of components and has the capability of approximating indefinite kernels on a wide scale. In model inference, we develop a non-conjugate variational algorithm with a sub-sampling scheme for the posterior inference. It allows for the non-conjugate case in our model and is quite efficient due to the sub-sampling strategy. Experimental results on several large classification data sets demonstrate the effectiveness of our nonparametric Bayesian model for indefinite kernel approximation when compared to other representative random feature-based methods.																	2162-237X	2162-2388				AUG	2020	31	8					2965	2979		10.1109/TNNLS.2019.2934729													
J								New Varying-Parameter ZNN Models With Finite-Time Convergence and Noise Suppression for Time-Varying Matrix Moore-Penrose Inversion	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convergence; Neural networks; Mathematical model; Noise reduction; Robustness; Real-time systems; Robots; Finite-time convergence; Moore-Penrose inverse; noise suppression; robustness; varying-parameter zeroing neural network (VPZNN)	RECURRENT NEURAL-NETWORKS; GENERALIZED INVERSE; NONLINEAR-SYSTEMS; DESIGN; OPTIMIZATION	This article aims to solve the Moore-Penrose inverse of time-varying full-rank matrices in the presence of various noises in real time. For this purpose, two varying-parameter zeroing neural networks (VPZNNs) are proposed. Specifically, VPZNN-R and VPZNN-L models, which are based on a new design formula, are designed to solve the right and left Moore-Penrose inversion problems of time-varying full-rank matrices, respectively. The two VPZNN models are activated by two novel varying-parameter nonlinear activation functions. Detailed theoretical derivations are presented to show the desired finite-time convergence and outstanding robustness of the proposed VPZNN models under various kinds of noises. In addition, existing neural models, such as the original ZNN (OZNN) and the integration-enhanced ZNN (IEZNN), are compared with the VPZNN models. Simulation observations verify the advantages of the VPZNN models over the OZNN and IEZNN models in terms of convergence and robustness. The potential of the VPZNN models for robotic applications is then illustrated by an example of robot path tracking.																	2162-237X	2162-2388				AUG	2020	31	8					2980	2992		10.1109/TNNLS.2019.2934734													
J								A Semismooth Newton Algorithm for High-Dimensional Nonconvex Sparse Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convergence; minimax concave penalty (MCP); semismooth newton (SSN); smoothly clipped absolute deviation (SCAD); warm start	NONCONCAVE PENALIZED LIKELIHOOD; COORDINATE DESCENT ALGORITHMS; VARIABLE SELECTION; MODEL SELECTION; ADAPTIVE LASSO; REGRESSION; OPTIMIZATION; CONVERGENCE; SHRINKAGE	The smoothly clipped absolute deviation (SCAD) and the minimax concave penalty (MCP)-penalized regression models are two important and widely used nonconvex sparse learning tools that can handle variable selection and parameter estimation simultaneously and thus have potential applications in various fields, such as mining biological data in high-throughput biomedical studies. Theoretically, these two models enjoy the oracle property even in the high-dimensional settings, where the number of predictors p may be much larger than the number of observations n. However, numerically, it is quite challenging to develop fast and stable algorithms due to their nonconvexity and nonsmoothness. In this article, we develop a fast algorithm for SCAD- and MCP-penalized learning problems. First, we show that the global minimizers of both models are roots of the non-smooth equations. Then, a semismooth Newton (SSN) algorithm is employed to solve the equations. We prove that the SSN algorithm converges locally and superlinearly to the Karush-Kuhn-Tucker (KKT) points. The computational complexity analysis shows that the cost of the SSN algorithm per iteration is O(np). Combined with the warm-start technique, the SSN algorithm can be very efficient and accurate. Simulation studies and a real data example suggest that our SSN algorithm, with comparable solution accuracy with the coordinate descent (CD) and the difference of convex (DC) proximal Newton algorithms, is more computationally efficient.																	2162-237X	2162-2388				AUG	2020	31	8					2993	3006		10.1109/TNNLS.2019.2935001													
J								Sparse Manifold-Regularized Neural Networks for Polarimetric SAR Terrain Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Manifolds; Covariance matrices; Deep learning; Sociology; Learning systems; Deep learning; manifold regularization; polarimetric synthetic aperture radar (PolSAR); sparse filtering	IMAGE CLASSIFICATION; FEATURE-EXTRACTION; FACE; REPRESENTATION	In this article, a new deep neural network based on sparse filtering and manifold regularization (DSMR) is proposed for feature extraction and classification of polarimetric synthetic aperture radar (PolSAR) data. DSMR uses a novel deep neural network (DNN) to automatically learn features from raw SAR data. During preprocessing, the spatial information between pixels on PolSAR images is exploited to weight each data sample. Then, in the pretraining and fine-tuning, DSMR uses the population sparsity and the lifetime sparsity (dual sparsity) to learn the global features and preserves the local structure of data by neighborhood-based manifold regularization. The dual sparsity only needs to tune a few parameters, and the manifold regularization cuts down the number of training samples. Experimental results on synthesized and real PolSAR data sets from different SAR systems show that DSMR can improve classification accuracy compared with conventional DNNs, even for data sets with a large angle of incidence.																	2162-237X	2162-2388				AUG	2020	31	8					3007	3016		10.1109/TNNLS.2019.2935027													
J								Weighted Broad Learning System and Its Application in Nonlinear Industrial Process Modeling	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Learning systems; Training; Heuristic algorithms; Neural networks; Automation; Process control; Control engineering; Broad learning system (BLS); incremental learning algorithm; noise and outliers; weighted penalty factor	RESTRICTED BOLTZMANN MACHINE; OUTLIER DETECTION; ROBUST; REGRESSION	Broad learning system (BLS) is a novel neural network with effective and efficient learning ability. BLS has attracted increasing attention from many scholars owing to its excellent performance. This article proposes a weighted BLS (WBLS) based on BLS to tackle the noise and outliers in an industrial process. WBLS provides a unified framework for easily using different methods of calculating the weighted penalty factor. Using the weighted penalty factor to constrain the contribution of each sample to modeling, the normal and abnormal samples were allocated higher and lower weights to increase and decrease their contributions, respectively. Hence, the WBLS can eliminate the bad effect of noise and outliers on the modeling. The weighted ridge regression algorithm is used to compute the algorithm solution. Weighted incremental learning algorithms are also developed using the weighted penalty factor to tackle the noise and outliers in the additional samples and quickly increase nodes or samples without retraining. The proposed weighted incremental learning algorithms provide a unified framework for using different methods of computing weights. We test the feasibility of the proposed algorithms on some public data sets and a real-world application. Experiment results show that our method has better generalization and robustness.																	2162-237X	2162-2388				AUG	2020	31	8					3017	3031		10.1109/TNNLS.2019.2935033													
J								Siamese Dilated Inception Hashing With Intra-Group Correlation Enhancement for Image Retrieval	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Correlation; Hash functions; Semantics; Training; Approximation algorithms; Convolution; Image retrieval; Category-level semantics; deep hashing; image retrieval; multi-scale contextual information	QUANTIZATION; CODES	For large-scale image retrieval, hashing has been extensively explored in approximate nearest neighbor search methods due to its low storage and high computational efficiency. With the development of deep learning, deep hashing methods have made great progress in image retrieval. Most existing deep hashing methods cannot fully consider the intra-group correlation of hash codes, which leads to the correlation decrease problem of similar hash codes and ultimately affects the retrieval results. In this article, we propose an end-to-end siamese dilated inception hashing (SDIH) method that takes full advantage of multi-scale contextual information and category-level semantics to enhance the intra-group correlation of hash codes for hash codes learning. First, a novel siamese inception dilated network architecture is presented to generate hash codes with the intra-group correlation enhancement by exploiting multi-scale contextual information and category-level semantics simultaneously. Second, we propose a new regularized term, which can force the continuous values to approximate discrete values in hash codes learning and eventually reduces the discrepancy between the Hamming distance and the Euclidean distance. Finally, experimental results in five public data sets demonstrate that SDIH can outperform other state-of-the-art hashing algorithms.																	2162-237X	2162-2388				AUG	2020	31	8					3032	3046		10.1109/TNNLS.2019.2935118													
J								Graph Edge Convolutional Neural Networks for Skeleton-Based Action Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convolution; Skeleton; Feature extraction; Solid modeling; Dynamics; Pose estimation; Data models; Action recognition; graph convolutional neural networks (CNNs); skeletal data		Body joints, directly obtained from a pose estimation model, have proven effective for action recognition. Existing works focus on analyzing the dynamics of human joints. However, except joints, humans also explore motions of limbs for understanding actions. Given this observation, we investigate the dynamics of human limbs for skeleton-based action recognition. Specifically, we represent an edge in a graph of a human skeleton by integrating its spatial neighboring edges (for encoding the cooperation between different limbs) and its temporal neighboring edges (for achieving the consistency of movements in an action). Based on this new edge representation, we devise a graph edge convolutional neural network (CNN). Considering the complementarity between graph node convolution and edge convolution, we further construct two hybrid networks by introducing different shared intermediate layers to integrate graph node and edge CNNs. Our contributions are twofold, graph edge convolution and hybrid networks for integrating the proposed edge convolution and the conventional node convolution. Experimental results on the Kinetics and NTU-RGB+D data sets demonstrate that our graph edge convolution is effective at capturing the characteristics of actions and that our graph edge CNN significantly outperforms the existing state-of-the-art skeleton-based action recognition methods.																	2162-237X	2162-2388				AUG	2020	31	8					3047	3060		10.1109/TNNLS.2019.2935173													
J								Multi-Atlas Segmentation of Anatomical Brain Structures Using Hierarchical Hypergraph Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Image segmentation; Biomedical imaging; Hippocampus; Brainstem; Neuroimaging; Brainstem nuclei; context features; hippocampus; hypergraph learning; multi-atlas segmentation (MAS)	GRAY-MATTER; MRI; BIRTH; REGISTRATION; HIPPOCAMPUS; IMAGES; SIZE	Accurate segmentation of anatomical brain structures is crucial for many neuroimaging applications, e.g., early brain development studies and the study of imaging biomarkers of neurodegenerative diseases. Although multi-atlas segmentation (MAS) has achieved many successes in the medical imaging area, this approach encounters limitations in segmenting anatomical structures associated with poor image contrast. To address this issue, we propose a new MAS method that uses a hypergraph learning framework to model the complex subject-within and subject-to-atlas image voxel relationships and propagate the label on the atlas image to the target subject image. To alleviate the low-image contrast issue, we propose two strategies equipped with our hypergraph learning framework. First, we use a hierarchical strategy that exploits high-level context features for hypergraph construction. Because the context features are computed on the tentatively estimated probability maps, we can ultimately turn the hypergraph learning into a hierarchical model. Second, instead of only propagating the labels from the atlas images to the target subject image, we use a dynamic label propagation strategy that can gradually use increasing reliably identified labels from the subject image to aid in predicting the labels on the difficult-to-label subject image voxels. Compared with the state-of-the-art label fusion methods, our results show that the hierarchical hypergraph learning framework can substantially improve the robustness and accuracy in the segmentation of anatomical brain structures with low image contrast from magnetic resonance (MR) images.																	2162-237X	2162-2388				AUG	2020	31	8					3061	3072		10.1109/TNNLS.2019.2935184													
J								Unsupervised Domain Adaptation With Adversarial Residual Transform Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Adaptation models; Neural networks; Transforms; Training; Task analysis; Gallium nitride; Adversarial neural networks; residual connections; transfer learning; unsupervised domain adaptation (DA)		Domain adaptation (DA) is widely used in learning problems lacking labels. Recent studies show that deep adversarial DA models can make markable improvements in performance, which include symmetric and asymmetric architectures. However, the former has poor generalization ability, whereas the latter is very hard to train. In this article, we propose a novel adversarial DA method named adversarial residual transform networks (ARTNs) to improve the generalization ability, which directly transforms the source features into the space of target features. In this model, residual connections are used to share features and adversarial loss is reconstructed, thus making the model more generalized and easier to train. Moreover, a special regularization term is added to the loss function to alleviate a vanishing gradient problem, which enables its training process stable. A series of experiments based on Amazon review data set, digits data sets, and Office-31 image data sets are conducted to show that the proposed ARTN can be comparable with the methods of the state of the art.																	2162-237X	2162-2388				AUG	2020	31	8					3073	3086		10.1109/TNNLS.2019.2935384													
J								Efficient Implementation of Second-Order Stochastic Approximation Algorithms in High-Dimensional Problems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Modified-Newton method; Newton method; quasi-Newton method; simultaneous perturbation stochastic approximation (SPSA); stochastic optimization; symmetric indefinite factorization		Stochastic approximation (SA) algorithms have been widely applied in minimization problems when the loss functions and/or the gradient information are only accessible through noisy evaluations. Stochastic gradient (SG) descent-a first-order algorithm and a workhorse of much machine learning-is perhaps the most famous form of SA. Among all SA algorithms, the second-order simultaneous perturbation stochastic approximation (2SPSA) and the second-order stochastic gradient (2SG) are particularly efficient in handling high-dimensional problems, covering both gradient-free and gradient-based scenarios. However, due to the necessary matrix operations, the per-iteration floating-point-operations (FLOPs) cost of the standard 2SPSA/2SG is O(p(3)), where p is the dimension of the underlying parameter. Note that the O(p(3)) FLOPs cost is distinct from the classical SPSA-based per-iteration O(1) cost in terms of the number of noisy function evaluations. In this work, we propose a technique to efficiently implement the 2SPSA/2SG algorithms via the symmetric indefinite matrix factorization and show that the FLOPs cost is reduced from O(p(3)) to O(p(2)). The formal almost sure convergence and rate of convergence for the newly proposed approach are directly inherited from the standard 2SPSA/2SG. The improvement in efficiency and numerical stability is demonstrated in two numerical studies.																	2162-237X	2162-2388				AUG	2020	31	8					3087	3099		10.1109/TNNLS.2019.2935455													
J								Probability Density Rank-Based Quantization for Convex Universal Learning Machines	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convex universal learning machines (CULMs); kernel method; regression; vector quantization	NYSTROM METHOD; KERNEL; ALGORITHM; APPROXIMATION	The distributions of input data are very important for learning machines, such as the convex universal learning machines (CULMs). The CULMs are a family of universal learning machines with convex optimization. However, the computational complexity is a crucial problem in CULMs, because the dimension of the nonlinear mapping layer (the hidden layer) of the CULMs is usually rather large in complex system modeling. In this article, we propose an efficient quantization method called Probability density Rank-based Quantization (PRQ) to decrease the computational complexity of CULMs. The PRQ ranks the data according to the estimated probability densities and then selects a subset whose elements are equally spaced in the ranked data sequence. We apply the PRQ to kernel ridge regression (KRR) and random Fourier feature recursive least squares (RFF-RLS), which are two typical algorithms of CULMs. The proposed method not only keeps the similarity of data distribution between the code book and data set but also reduces the computational cost by using the kd-tree. Meanwhile, for a given data set, the method yields deterministic quantization results, and it can also exclude the outliers and avoid too many borders in the code book. This brings great convenience to practical applications of the CULMs. The proposed PRQ is evaluated on several real-world benchmark data sets. Experimental results show satisfactory performance of PRQ compared with some state-of-the-art methods.																	2162-237X	2162-2388				AUG	2020	31	8					3100	3113		10.1109/TNNLS.2019.2935502													
J								Energy-Efficient LSTM Networks for Online Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Additives; Computational complexity; Logic gates; Computer architecture; Convergence; ef-operator; exponentiated gradient (EG); gradient descent; long short-term memory (LSTM); matrix factorization	NEURAL-NETWORK; EXPONENTIATED GRADIENT; ALGORITHM; DESCENT	We investigate variable-length data regression in an online setting and introduce an energy-efficient regression structure build on long short-term memory (LSTM) networks. For this structure, we also introduce highly effective online training algorithms. We first provide a generic LSTM-based regression structure for variable-length input sequences. To reduce the complexity of this structure, we then replace the regular multiplication operations with an energy-efficient operator, i.e., the ef-operator. To further reduce the complexity, we apply factorizations to the weight matrices in the LSTM network so that the total number of parameters to be trained is significantly reduced. We then introduce online training algorithms based on the stochastic gradient descent (SGD) and exponentiated gradient (EG) algorithms to learn the parameters of the introduced network. Thus, we obtain highly efficient and effective online learning algorithms based on the LSTM network. Thanks to our generic approach, we also provide and simulate an energy-efficient gated recurrent unit (GRU) network in our experiments. Through an extensive set of experiments, we illustrate significant performance gains and complexity reductions achieved by the introduced algorithms with respect to the conventional methods.																	2162-237X	2162-2388				AUG	2020	31	8					3114	3126		10.1109/TNNLS.2019.2935796													
J								Unsupervised Anomaly Detection With LSTM Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Anomaly detection; Time series analysis; Training; Hidden Markov models; Support vector machines; Computer architecture; Prediction algorithms; Anomaly detection; gated recurrent unit (GRU); long short-term memory (LSTM); support vector data description (SVDD); support vector machines (SVMs)	NOVELTY DETECTION; SUPPORT; FRAMEWORK; MACHINE	We investigate anomaly detection in an unsupervised framework and introduce long short-term memory (LSTM) neural network-based algorithms. In particular, given variable length data sequences, we first pass these sequences through our LSTM-based structure and obtain fixed-length sequences. We then find a decision function for our anomaly detectors based on the one-class support vector machines (OC-SVMs) and support vector data description (SVDD) algorithms. As the first time in the literature, we jointly train and optimize the parameters of the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective gradient and quadratic programming-based training methods. To apply the gradient-based training method, we modify the original objective criteria of the OC-SVM and SVDD algorithms, where we prove the convergence of the modified objective criteria to the original criteria. We also provide extensions of our unsupervised formulation to the semisupervised and fully supervised frameworks. Thus, we obtain anomaly detection algorithms that can process variable length data sequences while providing high performance, especially for time series data. Our approach is generic so that we also apply this approach to the gated recurrent unit (GRU) architecture by directly replacing our LSTM-based structure with the GRU-based structure. In our experiments, we illustrate significant performance gains achieved by our algorithms with respect to the conventional methods.																	2162-237X	2162-2388				AUG	2020	31	8					3127	3141		10.1109/TNNLS.2019.2935975													
J								Development of a framework for modeling preference times in triathlon	NEURAL COMPUTING & APPLICATIONS										Datasets; Data mining; Prediction; Sport competition; Framework	PARTICLE SWARM OPTIMIZATION	Preference time in a triathlon denotes the time that is planned to be achieved by an athlete in a particular competition. Usually, the preference time is calculated some days, weeks, or even months before the competition. Mostly, trainers calculate the proposed preference time according to the current form, body performances of athletes, psychological abilities and their health state. They also take course specifications into account in order to make their proposal as exact as possible. However, until recently, this prediction was performed manually. This paper presents an automatic framework for modeling preference times based on previous results of athletes on a particular racecourse and particle swarm optimization. Indeed, the framework observed the problem as optimization, where the goal is to find such preference time that is as much as possible correlated with past data. Practical experiments with different scenarios reveal that the proposed solution is promising.																	0941-0643	1433-3058				AUG	2020	32	15					10833	10846		10.1007/s00521-018-3632-9													
J								An approach to provide dynamic, illustrative, video-based guidance within a goal-driven smart home	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart home; Guidance; Video-processing; Metadata generation; Activities of daily living; Ontology	RETRIEVAL	The global population is aging in a never-before seen way, introducing an increasing ageing-related cognitive ailments, such as dementia. This aging is coupled with a reduction in the global support ratio, reducing the availability of formal and informal support and therefore capacity to care for those suffering these aging related ailments. Assistive Smart Homes (SH) are a promising form of technology enabling assistance with activities of daily living, providing support of suffers of cognitive ailments and increasing their independence and quality of life. Traditional SH systems have deficiencies that have been partially addressed by through goal-driven SH systems. Goal-driven SHs incorporate flexible activity models, goals, which partially address some of these issues. Goals may be combined to provide assistance with dynamic and variable activities. This paradigm-shift, however, introduces the need to provide dynamic assistance within such SHs. This study presents a novel approach to achieve this through video based content analysis and a mechanism to facilitate matching analysed videos to dynamic activities/goals. The mechanism behind this approach is detailed and followed by the presentation of an evaluation where showing promising results were shown.																	1868-5137	1868-5145				AUG	2020	11	8					3045	3056		10.1007/s12652-016-0421-0													
J								Semantic event fusion of computer vision and ambient sensor data for activity recognition to support dementia care	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activity recognition; Computer vision; Sensors; Ambient intelligence; Semantic web; Ontologies; Rules; Dementia	ELDERLY-PEOPLE; SEGMENTATION; LOCALIZATION	Although many Ambient Intelligence frameworks either address heterogeneous ambient sensing or computer vision techniques, very limited work integrates both techniques in the scope of activity recognition in pervasive environments. This paper presents such a framework that integrates both a computer vision component and heterogeneous sensors with unanimous semantic representation and interpretation, while it also addresses challenges for realistic applications, such as fast, efficient image analysis and ontology-based temporal interpretation models. The framework is validated through an application in clinical dementia assessment yielding positive results and fruitful conclusions for the proposed semantic fusion of vision and sensor observations.																	1868-5137	1868-5145				AUG	2020	11	8					3057	3072		10.1007/s12652-016-0437-5													
J								Ontology-based sensor fusion activity recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											AMBIENT; HEALTH	This paper investigates the fusion of wearable and ambient sensors for recognizing activities of daily living in a smart home setting using ontology. The proposed approach exploits the advantages of both types of sensing to resolve uncertainties due to missing sensor data. The resulting system is able to infer activities which cannot be inferred with the single type of sensing only. The methodology of ontological modeling the wearable and ambient sensors and the fusion of contexts captured from the sensors, as well as corresponding activity is investigated and described. The proposed system is compared with a system that uses ambient sensors without wearable sensor on the internally collected and publicly available datasets. The results of the experiments show that the proposed system is more robust in handling uncertainties. It is also more capable of inferring additional information about activities, which is not possible with environment sensing only, with overall recognition accuracy of 91.5 and 90% on internal and public datasets, respectively.																	1868-5137	1868-5145				AUG	2020	11	8					3073	3087		10.1007/s12652-017-0668-0													
J								Dense descriptor for visual tracking and robust update model strategy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Visual tracking; Dense SIFT; RANSAC	OBJECT TRACKING; CUE INTEGRATION; MULTIPLE; GRADIENTS; PEOPLE; COLOR	Context analysis is a research field that is attracting growing interest in recent years, especially due to the encouraging results carried out by the semantic-based approach. Anyway, semantic strategies entail the use of trackers capable to show robustness to long-term occlusions, viewpoint changes and identity swap that represent the main problem of many tracking-by-detection solutions. This paper proposes a robust tracking-by-detection framework based on dense SIFT descriptors in combination with an ad-hoc target appearance model update able to overtake the discussed issues. The obtained performances show how our tracker competes with state-of-the-art results and manages occlusions, clutter, changes of scale, rotation and appearance, better than competing tracking methods.																	1868-5137	1868-5145				AUG	2020	11	8					3089	3099		10.1007/s12652-017-0461-0													
J								Learning multi-path CNN for mural deterioration detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mural deterioration detection; Multi-input; max-fusion; multi-path CNN; Dense CRF		Mural deterioration easily destroys valuable paintings and must be monitored frequently for preventive protection. Deterioration detection in mural images is often manually labeled and is a preprocessing step for mural protection and restoration. Many deterioration forms are commonly invisible with only one lighting condition because mural deterioration is caused by changes in the material and plaster layer. This study addresses mural deterioration detection through a multi-path convolutional neural network (CNN), which takes images of a scene with multiple lightings as inputs and generates a binary map that indicates deterioration regions. We design an eight-path CNN in which seven paths are utilized for basic feature extraction from lighted images, and the remaining path is responsible for cross feature fusion. This mechanism enables our method to not only identify suitable features for different lightings but also utilize these features collaboratively through cross feature fusion. Furthermore, we build two realistic mural deterioration datasets of real-world mural deterioration and briquettes that simulate the cave deterioration. Extensive experiments verify the effectiveness and efficiency of our method.																	1868-5137	1868-5145				AUG	2020	11	8					3101	3108		10.1007/s12652-017-0656-4													
J								Secure system based on UAV and BLE for improving SAR missions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Drone; Unmanned aerial vehicle; Physical web; Search and rescue; Security		This work describes an integrated solution to face a civil security problem in the area of search and rescue of missing people. This proposal is based on the use of emerging technologies such as unmanned aerial vehicles, also known as drones, and the use of simulated beacons on smartphones. In particular, in the presented tool, drones fly synchronously in a specific area so that each drone uses on-board sensors to scan and detect any signal emitted by bluetooth low energy beacons from smartphones of missing people. This technique allows getting the GPS position of any detected missing person. This work also includes some security issues related to possible attacks focused on the perimeter and physical security.																	1868-5137	1868-5145				AUG	2020	11	8					3109	3120		10.1007/s12652-017-0603-4													
J								Intelligent agent for real-world applications on robotic edutainment and humanized co-learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent agent; Dynamic assessment; Humanized co-learning; Robot edutainment; Brain-computer-interface		Dynamic assessment with an intelligent agent can differentiate the capabilities and proficiency of students. It can therefore be advocated as an interactive approach to conduct assessments on students in learning systems. Facebook AI Research proposed ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. They also developed Darkforest, which displays the competence and skills of high-level amateur Go players. To enable these open-source AI bots to assist humans at different levels in learning Go, this paper proposes an intelligent agent for real-world applications in robotic edutainment and humanized co-learning. To achieve this, we successfully constructed an OpenGo Darkforest (OGD) cloud platform using these AI bots and further combined the brain computer interface with the OGD cloud platform to observe the relationship between the brainwaves and win rates of human Go players. The intelligent agent also converted human brainwaves into physiological indices and reflected these in the robot to express human feelings or emotions in real-time. For future educational applications, this paper also presents intelligent robot teachers learning together with students in Taiwan and Japan. More than 200 students have been co-learning with intelligent robot teachers in Tainan, Kaohsiung, Taipei, and Tokyo from 2018 to 2019. The learning performance and feedback from students and teachers has been extremely positive, especially from remedial students.																	1868-5137	1868-5145				AUG	2020	11	8					3121	3139		10.1007/s12652-019-01454-4													
J								An efficient data packet iteration and transmission algorithm in opportunistic social networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Opportunistic social networks; Efficient data packet; Transmission; Iteration	MOBILE HEALTH	Effective data transmission is a key technology in researching opportunistic networks. Increased data packet transmission among nodes can easily cause the death of nodes, especially in social networks environment. However, effective packet transmission is seldom discussed in existing algorithms in opportunistic networks research. In this study, an efficient data packet iteration and transmission (EDPIT) algorithm, which selects data packets via iteration, is proposed to save energy and overhead during transmission. The effective transmission among nodes in this algorithm improves the transmission rate of data packets. With satisfactory results from simulation and comparison with some existing algorithms, the EDPIT algorithm is found to not only reduce energy consumption but also improve the delivery ratio and overhead in opportunistic social networks.																	1868-5137	1868-5145				AUG	2020	11	8					3141	3153		10.1007/s12652-019-01480-2													
J								An energy-aware multi-sensor geo-fog paradigm for mission critical applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy; Fog computing; Heuristic search; K* algorithm; Geospatial; Wireless sensor network	SENSOR CLOUD; INTERNET; FRAMEWORK; HEALTH; THINGS	Sensor cloud is an integral component for smart computing infrastructure. Cloud servers are largely used to store and process sensor data. For mission critical applications use of only wireless sensor network results in provisioning of service in a small area and the use of a long distant remote cloud servers increase delay that degrades the Quality of Service. Further, geospatial information differs over regions. Thus storing and processing the data of all regions inside the cloud data centres may not be efficient with respect to response time (latency), energy consumption etc., which are crucial factors for mission critical applications. To overcome these limitations, we propose multi-sensor geo-fog paradigm. We consider defense sector in our work as mission critical application. For energy optimized services with minimal delay fog computing has been used, where the intermediate devices process the data. The proposed paradigm will offer fast and energy-efficient processing of defense related sensor and geospatial data. A mathematical model of the paradigm is developed. The sensor and geospatial data processing and analysis take place inside the fog device. If abnormality is detected in the data or emergency situation occurs, then shortest path to the victim region is determined using intelligent K* heuristic search algorithm. The simulation results demonstrate that the proposed fog based network scenario reduces energy consumption, average jitter and average delay by 12-15%, 10-14% and 9-11% respectively than the cloud based network. The simulation results demonstrate that saving about 20% of resources increases the performance for priority user whereas the resource availability for the normal users is not compromised.																	1868-5137	1868-5145				AUG	2020	11	8					3155	3173		10.1007/s12652-019-01481-1													
J								Neutrosophic goal programming strategy for multi-level multi-objective linear programming problem	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-level multi-objective programming; Neutrosophic number; Interval programming; Goal programming	ROUTING METHOD; ALGORITHM; INTERNET; NUMBER; KIND	Neutrosophic set theory plays an important role in dealing with the impreciseness and inconsistency in data encountered in solving real life problems. This article aims to present a novel goal programming based strategy which will be helpful to solve Multi-Level Multi-Objective Linear Programming Problem (MLMOLPP) with parameters as neutrosophic numbers (NNs). Difficulty in decision making arises due to the presence of multiple decision makers (DMs) and impreciseness in information. Here each level DM has multiple linear objective functions with parameters considered as NNs which are represented in the formc+dI wherecanddare considered real numbers and the symbolIdenotes indeterminacy. The constraints are also linear with the parameters as NNs. Firstly the NNs are changed into intervals and the problem turns into a multi-level multi-objective linear programming problem considering interval parameters. Then interval programming technique is employed to obtain the target interval of each objective function. In order to avoid decision deadlock which may arise in hierarchical (multi-level) problem, a possible relaxation is imposed by each level DM on the decision variables under his/her control. Finally a goal programming strategy is presented to solve the MLMOLPP with interval parameters. The method presented in this paper facilitates to solve MLMOLPP with multiple conflicting objectives in an uncertain environment represented through NNs of the formc+dI where indeterminacyIplays a pivotal role. Lastly, a mathematical example is solved to show the novelty and applicability of the developed strategy.																	1868-5137	1868-5145				AUG	2020	11	8					3175	3186		10.1007/s12652-019-01482-0													
J								Towards emotion recognition from contextual information using machine learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Emotion recognition; Context-aware applications; Smart devices; Affective computing	DECISION-MAKING; NEGATIVE EMOTIONS; RESPONSES; ANXIETY; SYSTEMS; MODELS	Emotions influence cognitive processes that underlie human behavior. Whereas experiencing negative emotions may lead to develop psychological disorders, experiencing positive emotions may improve creative thinking and promote cooperative behavior. The importance of human emotions has led to the development of automatic emotion recognition systems based on analysis of speech waveforms, facial expressions, and physiological signals as well as text data mining. However, emotions are associated with a context (in which emotions are actually experienced), hence, this work focuses on emotion recognition from contextual information. In this paper, we present a study aimed to assess the feasibility of automatically recognizing emotions from individuals' contexts. In this study, 32 participants provided information using a mobile application about their emotions and the context (e.g., companions, activities, and locations) in which these emotions were experienced. We used machine learning techniques to buildindividualmodels,generalmodels, andgender-specificmodels to automatically recognize emotions of participants. The empirical results show that individuals' emotions are highly related to their context and that automatic recognition of emotions in real-world situations is feasible by using contextual data.																	1868-5137	1868-5145				AUG	2020	11	8					3187	3207		10.1007/s12652-019-01485-x													
J								Energy consumption balancing in multi-interface networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-interface networks; Energy optimization; NP-completeness; Optimal algorithms		In heterogeneous networks, devices can communicate by means of multiple interfaces. By choosing which interfaces to activate (switch-on) at each device, several connections might be established. A connection is established when the devices at its endpoints share at least one active interface. Interfaces are associated with a cost defining the percentage of energy consumed to switch-on the corresponding interface. In this paper, we consider the case where each device is limited to activate at most a fixed numberpof its available interfaces in order to accomplish the required task. In particular, we consider the so-called Coverage problem. Given a networkG=(V,E) nodesVrepresent devices, edgesErepresent connections that can be established. The aim is to activate at mostpinterfaces at each node in order to establish all the connections defined byE. Parameterpimplies a sort of balanced consumption among devices so that none of them suffers-in terms of consumed energy-for being exploited in the network more than others. We provide aNP-completeness proof for the feasibility of the problem even considering the basic case ofp=2and unitary costs for all the interfaces. That is, each interface costs the same as all the others. Then we provide optimal algorithms that solve the problem in polynomial time for different graph topologies and general costs associated to the interfaces.																	1868-5137	1868-5145				AUG	2020	11	8					3209	3219		10.1007/s12652-019-01486-w													
J								Designing a robust demand-differentiated platelet supply chain network under disruption and uncertainty	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Platelet supply chain; Multi-product; Multi-objective programming; Robust optimization; Disruption	POSSIBILISTIC PROGRAMMING APPROACH; OPTIMIZATION MODEL; BLOOD; ALLOCATION; PLASMA	Managing platelets supply chain network has proved challenging. Besides stochastic demand, the high perishability of platelets and the diversity of their demands make the management more intricate. As a motivation to conduct this paper, we investigate a real-world case study facing a variety of platelet demands to satisfy. Being the first-ever study, we contribute a practical method for the efficient design and planning of a multiple platelet-derived products supply chain network. As the most perishable blood product, platelets need to be maintained fresh. Thus, we suggest a bi-objective model make a tradeoff relationship between the network costs and platelets' freshness. Further, we account for two realistic features that the products are categorized into three main types with respect to their application and lifetime, and hospitals are prioritized based on their specialty and the population of patients they cover. To cope with the uncertainty and objective multiplicity, we develop a mixed approach. The network robustness under uncertainty is controlled by a robust method and the Pareto solutions of the conflicting objectives are obtained via an interactive approach. Further, we take into account real-world scenarios that the network facilities may face disruptions and utilize a robust scenario-based approach to deal with the disruption scenarios. The results demonstrate that although simultaneous demand fluctuation and disruption increase both logistics costs and delivery time, the proposed model is capable of achieving robust solutions that a little increase in the logistic costs obtains a considerable reduction in the level of relative regret. Further, the network will benefit from a favorable saving in the logistic costs only by a little increase in the storage time.																	1868-5137	1868-5145				AUG	2020	11	8					3231	3258		10.1007/s12652-019-01501-0													
J								Uncertain random quadratic bottleneck assignment problem	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Simulation; Uncertain random variable; Assignment problem; Quadratic bottleneck assignment problem		Uncertain random expected value simulation is one of the most important techniques to solve uncertain random optimization problems where random variables coexist with uncertain variables. A general simulation algorithm is designed to estimate the uncertain random expected value. Furthermore, an uncertain random quadratic bottleneck assignment problem is proposed and an uncertain random expected value model is presented. An algorithm is designed to find a lower bound of the uncertain random quadratic bottleneck assignment problem.																	1868-5137	1868-5145				AUG	2020	11	8					3259	3264		10.1007/s12652-019-01510-z													
J								A new fog based security strategy (FBS2) for reliable image transmission	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cryptography; Steganography; Fog; LFSR; YCbCr color space	ALGORITHM; INTERNET	Fog computing has become a fast intermediate between the cloud and the internet of things (IOT) devices. So, the communication channel between the fog and IOT devices must be secured against attacks. Both Cryptography and steganography are well-known techniques for data security. Each of them has its own advantages and strength points, hence, combining Cryptography and steganography in a hybrid system certainly promotes the system's level of security. Although several hybrid security strategies have been proposed, they suffer from many drawbacks especially when they are applied in fog environments. Fog computing requires not only high security level but also a real time transmission. Hence, recent hybrid security strategies cannot be applied directly in fog environments as they suffer from long processing time. This paper introduces a new Fog Based Security Strategy (FBS2). FBS(2)is a hybrid security strategy as it combines Cryptography and steganography techniques. The proposed Cryptography Technique (PCT) is divided into two phases, namely; confusion and diffusion. The former scrambles the secret image pixels, while the latter changes the pixels values using new confusion and diffusion methodologies respectively. Confusion divides the secret images into closed loops of pixels, then rearrange the pixels into straight paths, while diffusion employs a Programmable Linear Feedback Shift Register (PLFSR). On the other hand, the Proposed Steganography Technique is implemented using the discrete wavelet packet transform. Embedding process depends on a new matching procedure, which is based on the most significant bits of both the encrypted secret image and the corresponding employed cover image pixels. Least significant bits (LSBs) are used as indicators to the matching process. At first an intermediate code is derived from the matching process, then, it is encrypted to represent the LSBs of the stego-image. Experimental results have shown that the proposed FBS(2)outperforms the previous counterparts in terms of efficiency, security, and processing time, which proves its suitability for application on the fog.																	1868-5137	1868-5145				AUG	2020	11	8					3265	3303		10.1007/s12652-019-01512-x													
J								A Jacobian approach for calculating the Lyapunov exponents of short time series using support vector regression	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Chaos theory; Lyapunov exponents; Time series; Support vector regression	CHAOS; MACHINES; SPECTRUM	In order to characterize a system and to analyze the predictability of the time series under investigation, the detection of chaos and fractal behavior in experimental data is essential. In this work, support vector regression with two different kernel types namely, the linear kernel and sigmoid kernel, has been utilized for the calculation of the Lyapunov exponents of the given time series. The developed technique for the estimation of Lyapunov exponents has been validated with the help of time series generated from well known chaotic maps and also by comparing the Lyapunov exponents obtained using Rosenstein method. The results of this work reveal that the proposed technique is capable of producing accurate positive exponents for all the considered chaotic maps.																	1868-5137	1868-5145				AUG	2020	11	8					3329	3335		10.1007/s12652-019-01525-6													
J								Energy load time-series forecast using decomposition and autoencoder integrated memory network	APPLIED SOFT COMPUTING										Load demand forecasting; Seasonal analysis; Long short term memory network; Variational Mode Decomposition	EMPIRICAL MODE DECOMPOSITION; ELECTRICITY CONSUMPTION; DEMAND; PREDICTION; REGRESSION; SYSTEM; GARCH	With the increasing population and rising living standard, the demand for energy and materials have increased to a greater extent. The accurate estimation of increasing electricity demand is prerequisite for strategies planning, improving revenue, reducing power wastage and stable operation of the energy demand management system. Recent advancements in the field of electricity load forecasting provide powerful tools to capture non-linear energy demand trends and outperform conventional load prediction models. However, the existing demand prediction models suffer from some significant shortcomings that need to be addressed for improved prediction accuracy. In this context, the current research work proposes a deep learning based hybrid approach which firstly implements Variational Mode Decomposition (VMD) and Autoencoder models to extract meaningful sub-signals/features from the data. Subsequently, a Long Short-term Memory (LSTM) network model is trained for each sub-signal to forecast electricity demand by utilizing historical, seasonal and timestamp data dependencies. The support for incorporating seasonal and timestamp information to LSTM model is provided through the agglomerative clustering algorithm. Furthermore, an error variance modelling strategy is also employed to enhance the prediction accuracy of the proposed approach. The experiments are conducted on electricity consumption dataset of Himachal Pradesh, India. Performance assessment of the proposed approach is made by comparing prediction results with Support Vector Regression (SVR), Recurrent Neural Network (RNN), Deep Belief Network (DBN) and EMD+LSTM models. The experimental results demonstrate that the proposed model outperforms other state-of-the-art demand forecasting models and has the lowest MAPE (3.04%). (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106390	10.1016/j.asoc.2020.106390													
J								Ensemble-learning based neural networks for novelty detection in multi-class systems	APPLIED SOFT COMPUTING										Novelty detection; Neural networks; Ensemble-learning; Posterior class probability; Confidence intervals	REGRESSION; CLASSIFIER; LASSO; RIDGE	In most real-world systems or processes, determining the complete set of classes during the training phase is generally impossible. There is a high chance that novelties or abnormal data can appear in future phases which might severely affect the performance of the machine learning system. Novelty detection is of great importance in many critical systems and domains, such as business intelligence, process monitoring, information security, clinical decision support etc. Most of the available methods for novelty detection use a one-class classification (OCC) criterion, i.e. treating multiple known classes as a single "Normal" class, whose aim is to distinguish data samples between "Normal'' and "Not Normal'' classes. In this paper, the problem of novelty detection in multi-class systems is addressed through ensemble based learning of neural networks (EBNN), capable of both detecting novelties and classifying the known normal samples in future datasets. Moreover, the model is analogous to the semisupervised learning system as it is trained using only the available normal classes. Evaluation of the proposed model (EBNN) on UCI machine learning datasets showed that the model not only outperforms other models in detecting novelties but also has a better multi-class classification accuracy for known normal classes. The proposed model implements a novel activation function in its framework and differs from the commonly available novelty detection models in three aspects. First, the model is much simpler to implement and does not need any initial assumptions about the model. Second, the model does not require any novel or abnormal data during training phase (semi-supervised learning). Third, it can be used as a two in one system to detect novelties and at the same time to classify data based on known classes. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106396	10.1016/j.asoc.2020.106396													
J								An artificial bee colony algorithm with adaptive heterogeneous competition for global optimization problems	APPLIED SOFT COMPUTING										Global optimization problems; Artificial bee colony algorithm; Adaptive strategy; Heterogeneous searching	PARTICLE SWARM OPTIMIZATION; SEARCH; EFFICIENT; STRATEGY	Artificial bee colony (ABC) algorithm is an efficient bio-inspired optimizer proposed recently. Though it has gained great popularity, ABC suffers from its slow convergence and poor generalization on various problem landscapes. To address the issues, an augmented ABC with adaptive heterogeneous competition (ABC-AHC) is proposed in this study. In ABC-AHC, two bee swarms with each conducting heterogeneous but complementary capabilities are implemented to improve the search capabilities on various problem spaces. To dynamically adjust the search behaviors, an adaptive mechanism is developed to trigger the competition and migration between the bee swarms. Comparative studies are conducted for parameter tuning and the heterogeneous searching (HST). Existing algorithms including ABC variants and non-ABC variants are adopted to validate the performance of ABC-AHC. Numerical comparisons are conducted on 30D and 100D benchmark functions, CEC 2014 test function, random function and the real-world problems. Numerical results demonstrate that the proposed strategies significantly enhance ABC's search capability and convergence speed on the various benchmark functions. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106391	10.1016/j.asoc.2020.106391													
J								Mathematical model and bee algorithms for mixed-model assembly line balancing problem with physical human-robot collaboration	APPLIED SOFT COMPUTING										Mixed-model assembly line; Human-robot collaboration; Bee algorithm; Artificial bee colony algorithm; Mixed-integer linear programming	KRILL HERD ALGORITHM; PARALLEL WORKSTATIONS; OPTIMIZATION ALGORITHM; COLONY; FORMULATION; DESIGN	The collaboration of human workers and robots draws increasing attention from the manufacturing enterprises to embrace the Industry 4.0 paradigm in a competitive way. Motivated by the requirements of collaboration between human workers and robots in assembly lines, this study investigates the mixed-model assembly line balancing (MMALB) problem with the collaboration between human workers and robots. A mixed-integer linear programming (MILP) model is formulated to tackle the small-size problems optimally to minimize the sum of cycle times of models. Also, bee algorithm (BA) and artificial bee colony (ABC) algorithm are implemented and improved to solve the large-size problems due to the NP-hardness of this problem. The proposed BA algorithm utilizes a new employed bee phase to accelerate the evolution of the swarm and new scout phase to escape from being trapped into local optima and produce a high-quality and diverse population. The developed ABC proposes a new onlooker phase to accelerate the evolution of the whole swarm by removing the poor-quality solutions, new scout phase to achieve high-quality solutions while preserving the diversity of the swarm, and local search to enhance exploitation capacity. Computational study on a set of generated instances indicates that the improvements enhance the BA and ABC algorithm by a significant margin, and the proposed BA and ABC algorithm achieve competing performance in comparison with nine other algorithms, including the late acceptance hill-climbing algorithm, simulated annealing algorithm, genetic algorithm, particle swarm optimization algorithm, discrete cuckoo search algorithm, the original bee algorithm, and three artificial bee colony algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106394	10.1016/j.asoc.2020.106394													
J								Solution of an EPQ model for imperfect production process under game and neutrosophic fuzzy approach	APPLIED SOFT COMPUTING										EPQ model; Imperfect items; Rework; Deterioration; Backlogging; Game theory; Neutrosophic offset; Sine-cut; Optimization	ECONOMIC PRODUCTION QUANTITY; INVENTORY MODEL; MATRIX GAMES; QUALITY; REWORK; SYSTEM	This article deals with an Economic Production Quantity (EPQ) deteriorating inventory model for non-random uncertain environment. It includes rework process, screening of imperfect items and partial backlogging. The items are partially serviceable, because at the time of production some items are found to be defective which cannot be recoverable or serviceable. At first, we develop a cost minimization problem under several assumptions related to imperfect items and rework process under certain linear constraints. We solve the crisp model (primal nonlinear problem) first, and then we convert this model into equivalent game problem taking the help of the theories related to strong and weak duality theorem. However, this game problem consists of the Lagrangian function that correspond a nonlinear objective function subject to some linear constraints. The main objective of the study is to develop a solution procedure of the problem associated to an imperfect process where all unit cost components might increase or decrease neutrosophically. Thus, according to the experiences gained by the decision maker (DM) we fuzzify all cost components as sub-neutrosophic offset. To defuzzify the model we have utilized the sine cuts of neutrosophic fuzzy numbers followed by a solution procedure developed in solving the matrix game exclusively. To validate the model, a numerical example is studied then we have compared the optimal results among the original problem, the equivalent game problem and the game problem under neutrosophic environment explicitly. Our findings reveal that under negative alpha-cuts the value of the objective function assumes lower and higher values. Finally, sensitivity analysis, graphical illustrations, conclusions and scope of future works have been discussed. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106397	10.1016/j.asoc.2020.106397													
J								A bi-objective home healthcare routing and scheduling problem considering patients' satisfaction in a fuzzy environment	APPLIED SOFT COMPUTING										Home care services; Optimization; Fuzzy environment; Patients' satisfaction; Metaheuristics	SIMULTANEOUS DELIVERY; ALGORITHM; OPTIMIZATION; ASSIGNMENT; SEARCH; TRAVEL; SOLVE; MODEL; TIMES	Home care services are an alternative answer to hospitalization, and play an important role in reducing the healthcare costs for governments and healthcare practitioners. To find a valid plan for these services, an optimization problem called the home healthcare routing and scheduling problem is motivated to perform the logistics of the home care services. Although most studies mainly focus on minimizing the total cost of logistics activities, no study, as far as we know, has treated the patients' satisfaction as an objective function under uncertainty. To make this problem more practical, this study proposes a bi-objective optimization methodology to model a multi-period and multi-depot home healthcare routing and scheduling problem in a fuzzy environment. With regards to a group of uncertain parameters such as the time of travel and services as well as patients' satisfaction, a fuzzy approach named as the Jimenez's method, is also utilized. To address the proposed home healthcare problem, new and well-established metaheuristics are obtained. Although the social engineering optimizer (SEO) has been applied to several optimization problems, it has not yet been applied in the healthcare routing and scheduling area. Another innovation is to develop a new modified multi-objective version of SEO by using an adaptive memory strategy, so-called AMSEO. Finally, a comprehensive discussion is provided by comparing the algorithms based on multi-objective metrics and sensitivity analyses. The practicality and efficiency of the AMSEO in this context lends weight to the development and application of the approach more broadly. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106385	10.1016/j.asoc.2020.106385													
J								A new game-theoretical multi-objective evolutionary approach for cash-in-transit vehicle routing problem with time windows (A Real life Case)	APPLIED SOFT COMPUTING										Vehicle routing problem with time window; Cash-in-transit; Risk minimization; Game theory; Hybrid genetic algorithm	VARIABLE NEIGHBORHOOD SEARCH; HAZARDOUS MATERIALS; GENETIC ALGORITHMS; OPTIMIZATION ALGORITHM; RISK; TRANSPORTATION; SYSTEM; MODEL; NETWORK; PRODUCT	Cash transfer from a central treasury to bank branches, which is with high security, is one of the crucial processes in the banking system. In this paper, a new multi-objective game theory-based model is developed to increase the security of cash-in-transit. For this purpose and in order to reduce the transportation costs, a bi-objective vehicle routing problem with time window is developed where the risk of transfers (including armed robbers attack and theft) and the distance traveled by vehicles are minimized. In order to better estimate the robber's performance, the probability of robber's ambush is calculated by the game theory approach, in such a way that a two-player, zero-sum game is played between the robber and the cash carrier. The probability of theft success is also estimated in the proposed approach through a multiple-criteria decision-making and in order to be further representative of real-life situations. A periodic review is also added to the proposed model to increase the cash transport security in which the previously used links would enjoy less chance of choosing in the current period. Moreover, a new multi-objective hybrid genetic algorithm incorporated with a number of new heuristics and operators is developed to tackle the proposed model. The efficiency and effectiveness of the algorithm are examined through several standard data sets, and the results indicate the effectiveness of the proposed solution algorithm. The wide applicability of our proposed approach in real-life situations is examined with a real case study as well. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106378	10.1016/j.asoc.2020.106378													
J								Extended TODIM method for CCUS storage site selection under probabilistic hesitant fuzzy environment	APPLIED SOFT COMPUTING										CCUS; Storage site selection; TODIM; Choquet integral; lambda-fuzzy measure	GROUP DECISION-MAKING; GEOLOGICAL MEDIA; SUITABLE AREAS; CO2 STORAGE; SEQUESTRATION; FRAMEWORK; RANKING	Carbon capture, utilization and storage (CCUS) technologies are effective for urgently dealing with climate change and reducing carbon dioxide (CO2). The storage of CO2 in deep strata often leads to CO2 leakage due to geological and engineering reasons, which has a huge impact on humans and ecology. CO2 storage site selection can be regarded as a multi-criteria decision-making (MCDM) problem. Decision makers do not always show completely rational and may have the preference of bounded rational behavior, which may affect the selection of CO2 storage site. At the same time, criteria interaction is an interesting issue in multi-criteria decision-making. In this paper, we develop an extended novel TODIM method based on lambda-fuzzy measure and Choquet integral to select the CO2 storage site using evaluation information given by decision makers which can take the form of a probabilistic hesitant fuzzy set, whereby lambda-fuzzy measure and Choquet integral are used to calculate the weights of criteria. The hamming distance measure between two probabilistic hesitant fuzzy element (P-HFE) is calculated and the gain and loss matrices for every criterion are obtained. Further, the overall values of all alternatives can be calculated to get the ranking order of CO2 storage site. Decision makers can select a suitable CO2 storage site according to ranking results. Finally, a CO2 storage site selection example is used to describe the effectiveness of the proposed procedures. The sensitivity analysis also explores the influence of the loss aversion coefficient and the change of criteria weights on decision results. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106381	10.1016/j.asoc.2020.106381													
J								Constructing grey prediction models using grey relational analysis and neural networks for magnesium material demand forecasting	APPLIED SOFT COMPUTING										Neural network; Grey prediction; Artificial intelligence; Electronics industry; Environmental protection	FUZZY TIME-SERIES; ENERGY-CONSUMPTION; ALGORITHM	In terms of environmental protection, magnesium is a lightweight material that has been widely used to manufacture components for electronics. By forecasting the demand for magnesium materials, we can evaluate its prospects in the related industries. Grey prediction is appropriate for this study, because there is limited available data on the demand for magnesium, and it does not coincide with the statistical assumptions. Therefore, this study applies the GM(1,1) model, which is the most frequently used grey prediction model, to forecast the demand for magnesium materials. To improve the accuracy of predictions with the GM(1,1) model, its residual modification was established by the neural network. In particular, this study used grey relational analysis to estimate the weight of each sample that was required to avoid unreasonably treating each sample with equal importance in the traditional grey prediction. The forecasting ability of the proposed grey residual modification models was verified using real data regarding the demand for magnesium materials. The results showed that the proposed prediction model performed well compared with the other prediction models considered. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106398	10.1016/j.asoc.2020.106398													
J								Automating detection and localization of myocardial infarction using shallow and end-to-end deep neural networks	APPLIED SOFT COMPUTING										Myocardial infarction; ECG signal processing; Deep residual learning; Artificial neural networks; Discrete wavelet transform; Principal Component Analysis	BASE-LINE WANDER; WAVELET TRANSFORM; ECG SIGNALS; CLASSIFICATION; ALGORITHM; MYOGLOBIN; DIAGNOSIS; FEATURES; PATTERN	Myocardial infarction (MI), also known as a heart attack, is one of the common cardiac disorders caused by prolonged myocardial ischemia. For MI patients, specifying the exact location of a heart muscle suffering from blood shortage or stoppage is of crucial importance. Automatic localization systems can support physicians for better decisions in emergency situations. Using 12-lead electrocardiogram, in this paper, two MI detection and localization methods are proposed with classic and end-to-end deep machine learning techniques. For the feature extraction phase, the classic approach performs a Discrete Wavelet Transform (DWT) and Principal Component Analysis (PCA) on the pre-processed signals followed by a shallow neural network (NN) for the classification phase. However, in the end-to-end residual deep learning technique, a Convolutional Neural Network (CNN) is directly employed on the pre-processed input signals. For specifying the infarcted region of myocardium, 6 classes of subdiagnosis are considered. Proposed models are verified with the Physikalisch-Technische Bundesanstalt (PTB) dataset, where the data of each patient is first grouped and then carefully partitioned to training, validation, and test datasets. The results of K-fold cross-validation indicate that the general model achieves over 98% accuracy for both MI detection and localization with fewer number of feature sets compared to previous studies. Moreover, the end-to-end CNN model shows superior performance by achieving perfect results. Thus, with the larger size of CNN models, one may choose a perfect system that requires larger memory compared to another system that requires less computational power and accepts nearly 2% of false positives and/or false negatives. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106383	10.1016/j.asoc.2020.106383													
J								Unsupervised phase learning and extraction from quasiperiodic multidimensional time-series data	APPLIED SOFT COMPUTING										Phase; Unsupervised learning; Neural network; Signal processing		Automatic phase extraction is commonly done by digital markers or handcrafted feature detection that are designed specifically for one particular type of signal. A few more generalized ways are to apply Hilbert transform or complex wavelet convolution. However, they are limited to single-dimensional signals, and they require the input window to cover multiple signal periods to extract a smooth phase sequence. In this work, we propose a learning-based phase extraction method for multi-dimensional signals, consuming only unlabeled signal examples for training. A neural network architecture is designed to map a window of signal directly to a phase value under a key constraint that encourages two consecutive phase values to progress forward. The concept has been generalized to a broad range of applications with an adjustable input window, a flexible phase progression penalty design, and better training stability. The proposed method consistently outperforms the complex wavelet convolution in three synthetic signals by reducing the phase extraction error by 63% while shortening the size of the observation window by over 90%. An experiment with an augmented electrocardiogram (ECG) signal also produces a clean phase sequence with an input window that is over thirty times smaller. A qualitative test on eleven-dimensional kinematic signals from three repetitive upper limb movements has shown that the method can also capture the fluctuation in phase progression. With the flexibility, the labelless training, the accuracy, and the short observation window, this technique has a potential to be used in a variety of applications that need instantaneous phase extraction. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106386	10.1016/j.asoc.2020.106386													
J								Cognitive fuzzy sets for decision making	APPLIED SOFT COMPUTING										Fuzzy sets; Cognitive fuzzy set; Multiple criteria decision making; Interactive decision making; Hospital internal supply chain	NETWORK PROCESS ANP; LINMAP METHOD; MULTIDIMENSIONAL-ANALYSIS; INFORMATION; PERFORMANCE; CONSENSUS; MODEL; ALLOCATION; OPERATORS	The intuitionistic fuzzy set (IFS) is useful in information expression but the cognitive overlap of people might cause the situation where the membership degree plus non-membership degree is greater than 1. The linear programming techniques for multidimensional analysis of preference (LINMAP) method is a well-known decision-making approach but little research focused on the interactive multiple criteria decision making (MCDM) problems within the LINMAP framework. This paper aims to propose a cognitive fuzzy set (CFS) to overcome the drawbacks of IFSs, and then develop an interactive cognitive fuzzy LINMAP (CF-LINMAP) method. To achieve these goals, the joint degree, which is caused by the cognitive overlap, of an IFS is interpreted, and then the concept of the CFS is proposed. We define the distance measure and comparison of CFSs, based on which, the cognitive fuzzy consistency and inconsistency indexes are calculated. The irrational index, which represents the irrational degree of experts, is introduced to improve the consistency and inconsistency indexes at each decision-making stage. Afterwards, an interactive CF-LINMAP method is proposed to deal with MCDM problems. An illustration concerning the hospital internal supply chain selection is given to show the applicability of the interactive CF-LINMAP method. Finally, the sensitivity analysis is done to demonstrate the reliability of the results. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106374	10.1016/j.asoc.2020.106374													
J								Crisscross differential evolution algorithm for constrained hydrothermal scheduling	APPLIED SOFT COMPUTING										Crisscross differential evolution; Fuzzy set theory; Hydrothermal scheduling; Multi-objective optimization problem; Opposition-based learning	GRAVITATIONAL SEARCH ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; DISPATCH; HYBRID; COORDINATION; SYSTEM	This paper proposes a novel chaotic-crisscross differential evolution (CCDE) algorithm to realize an optimal generation schedule of multi-chain short-term hydrothermal system over 24 hours' time-horizon in a multi-objective framework, considering conflicting economic-environmental aspects of thermal units. The equality constraints of active power balance and the amount of available water are independently handled using variable elimination method. However, the statistical uncertainties called residues arise due to infringements of equality constraints while adjusting the violated dependent variables within their boundaries. These residues are fuzzy quantified within their prescribed bounds, and are embedded as objectives to be optimized. An interactive unified fuzzy satisfying function is aimed to solve the conflict of three objectives. The global solution accuracy and convergence rate of stochastic algorithms are significantly affected by parameter-tuning, exploration and exploitation strategies. The proposed algorithm integrates dual crisscross mechanism orthogonally with chaotically tuned DE to balance exploration and exploitation. Information collected about non-dominated solutions from search space is processed using opposition-based learning for better accuracy of global solution in three-dimensional objective function hyperspace. The numerical results show improvement in unified satisfying objective function and convergence performance metrics over the existing methods. The competence of the proposed algorithm is confirmed through illustrations on benchmark functions and is substantiated through statistical significance tests. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				AUG	2020	93								106393	10.1016/j.asoc.2020.106393													
J								Coupled application of deep learning model and quantile regression for travel time and its interval estimation using data in different dimensions	APPLIED SOFT COMPUTING										Deep learning; Intelligent transportation system; Restricted Boltzmann machine; Short-term prediction	PREDICTION; NETWORK; UNCERTAINTY; SERIES	The rapid development of sensing and computing methods and their application to transportation engineering in recent years provide us data support to traffic flow prediction. However, the travel time prediction is still a complex and difficult task in the intelligent transportation system because of its nonlinear and nonstationary characteristics. In this study, a hybrid model coupling the deep learning model and the quantile regression (QR) has been proposed to achieve the deterministic and probabilistic travel time prediction. To consider multiple correlations of the traffic flow, a spatial-temporal state-space matrix has been developed. Then, a novel deep belief network stacked by several Gaussian Bernoulli Restricted Boltzmann Machine (GBRBM) to extract important features and a regression layer to finish the prediction were developed. Moreover, to strengthen the reliability of results, the QR was applied to generate a prediction interval. Using real-world data sets, the proposed hybrid model was evaluated and contrasted with several benchmark models. The results show the deep learning model outperform the shallow learning model. The prediction interval providing by QR is better than that provided by the traditional method. It indicates that our proposed hybrid model can obtain a more perfect and reliable prediction for travel time which is meaningful to the advanced traveler information system. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106387	10.1016/j.asoc.2020.106387													
J								Local probabilistic model for Bayesian classification: A generalized local classification model	APPLIED SOFT COMPUTING										Bayesian decision; Classification; Probabilistic model; Local learning; Probability estimation	NAIVE BAYES; CLASSIFIERS; INFERENCE; ALGORITHM; AVERAGE	In Bayesian classification, it is important to establish a probability distribution model, e.g., a Gaussian distribution for each class for probability estimation. Most of the previous methods modeled the probability distribution in the whole sample space. However, real-world problems are usually too complex to model in the whole sample space; some fundamental assumptions are required to simplify the global model, for example, the class conditional independence assumption for naive Bayesian classification. In this paper, with the insight that the distribution in a local sample space should be simpler than that in the whole sample space, a local probabilistic model established for a local region is expected much simpler and can relax the fundamental assumptions that may not be true in the whole sample space. Based on these advantages we propose establishing local probabilistic models for probability estimation in Bayesian classification. In addition, a Bayesian classifier adopting a local probabilistic model can even be viewed as a generalized local classification model; by tuning the size of the local region and the corresponding local model assumption, a fitting model can be established for a particular classification problem. The experimental results on several real-world datasets demonstrate the effectiveness of local probabilistic models for Bayesian classification. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106379	10.1016/j.asoc.2020.106379													
J								A fuzzy type-2 fault detection methodology to minimize false alarm rate in induction motor monitoring applications	APPLIED SOFT COMPUTING										Interval type-2 fuzzy system; Faults diagnosis; Uncertainties analysis; Industrial monitoring	LOGIC SYSTEMS; IDENTIFICATION	Automatic routines for Fault Detection and Diagnosis (FDD) are very important in industrial monitoring systems. However, false alarms potentially occur. High false alarm rates may lead to outages and consequent losses in the production process. To address such problem, a new FDD strategy based on type-2 fuzzy systems is proposed herein to minimize the false alarm rate. By applying system identification techniques, parametric models are estimated in order to represent the operation of the system under several levels of fault severity. The test system is a detailed dynamic nonlinear model of induction motor drive. The faults considered were partial short-circuit in stator winding coils. A performance comparison was made by implementing the monitoring system with both a type-2 fuzzy system interval and a type-1 fuzzy system. The results obtained thereby showed the improved performance and robustness of type-2 fuzzy system-based monitoring system, which outdoes the performance obtained by a type-1 fuzzy system. Furthermore, the performance of the proposed type-2 fuzzy system-based monitoring system may be further improved by using a Genetic Algorithm for tuning the parameters of the fuzzy type-2 system. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				AUG	2020	93								106373	10.1016/j.asoc.2020.106373													
J								FUZZ-EQ: A data equalizer for boosting the discrimination power of fuzzy classifiers	APPLIED SOFT COMPUTING										Fuzzy partitioning; Preprocessing; Fuzzy rule-based classification systems; Fuzzy decision trees; Probability integral transform; Quantile function	ALGORITHMS	The definition of linguistic terms is a critical part of the construction of any fuzzy classifier. Fuzzy partitioning methods (FPMs) range from simple uniform partitioning to sophisticated optimization algorithms. In this paper we present FUZZ-EQ, a preprocessing algorithm that facilitates the construc-tion of meaningful fuzzy partitions regardless of the FPM used. The proposed approach is radically different from any existing FPM: instead of adjusting the fuzzy sets to the training data, FUZZ-EQ adjusts the training data to a hypothetical uniform partition before applying any FPM. To do so, the original data distribution is transformed into a uniform distribution by applying the probability integral transform. FUZZ-EQ allows FPMs to provide classifiers with more granularity on high density regions, increasing the overall discrimination capability. Additionally, we describe the procedure to reverse this transformation and recover the interpretability of linguistic terms. To assess the effectiveness of our proposal, we conducted an extensive empirical study consisting of 41 classification tasks and 9 fuzzy classifiers with different FPMs, rule induction algorithms, and rule structures. We also tested the scalability of FUZZ-EQ in Big Data classification problems such as HIGGS, with 11 million examples. Experimental results reveal that FUZZ-EQ significantly boosted the classification performance of those classifiers using the same linguistic terms for all rules, including state-of-the-art classifiers such as FARC-HD or IVTURS. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106399	10.1016/j.asoc.2020.106399													
J								Fuzzy finite-time stable compensation control for a building structural vibration system with actuator failures	APPLIED SOFT COMPUTING										Building structural vibration; Finite-time stability; Fuzzy adaptive control; Actuator failure	SWITCHED NONLINEAR-SYSTEMS; ACTIVE CONTROL; STABILIZATION; STABILITY	This paper investigates finite-time stability vibration control for a building structure system in the presence of actuator failure. The dynamic compensation approach for seismic waves improves the control performance by considering such waves as an unknown nonlinear item in the system. In the control design, this item is approximated via the adaptive fuzzy control method. In addition, it is considered that actuators can fail and therefore lessen, the effectiveness of the suppression of building structure vibration. An adaptive failure compensation method is proposed to address this problem. Moreover, to rapidly suppress vibration, a finite-time active vibration controller is designed in combination with a failure compensation method. Under the proposed control strategy, the building structure system with uncertain actuator failures can be guaranteed to be stable in finite time. Finally, examples of different failure cases are presented to demonstrate the anti-seismic effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106372	10.1016/j.asoc.2020.106372													
J								Evolutionary Black-Box Topology Optimization: Challenges and Promises	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Topology; Evolutionary computation; Space exploration; Gradient methods; Linear programming; Genetic algorithms; CADCAM; design automation; design optimization; evolutionary computation; large scale optimization; product design; topology optimization; topology	MULTIOBJECTIVE GENETIC ALGORITHM; CONSTRUCTIVE SOLID GEOMETRY; LEVEL-SET METHOD; FREQUENCY-SELECTIVE SURFACES; PARTICLE SWARM OPTIMIZATION; COMPLIANT MECHANISMS; CONSTRAINED OPTIMIZATION; DESIGN OPTIMIZATION; STRUCTURAL DESIGN; PERFORMANCE ENHANCEMENT	Black-box topology optimization (BBTO) uses evolutionary algorithms and other soft computing techniques to generate near-optimal topologies of mechanical structures. Although evolutionary algorithms are widely used to compensate the limited applicability of conventional gradient optimization techniques, methods based on BBTO have been criticized due to numerous drawbacks. In this article, we discuss topology optimization as a black-box optimization problem. We review the main BBTO methods, discuss their challenges and present approaches to relax them. Dealing with those challenges effectively can lead to wider applicability of topology optimization, as well as the ability to tackle industrial, highly constrained, nonlinear, many-objective, and multimodal problems. Consequently, future research in this area may open the door for innovating new applications in science and engineering that may go beyond solving classical optimization problems of mechanical structures. Furthermore, algorithms designed for BBTO can be added to existing software toolboxes and packages of topology optimization.																	1089-778X	1941-0026				AUG	2020	24	4					613	633		10.1109/TEVC.2019.2954411													
J								A Survey of Weight Vector Adjustment Methods for Decomposition-Based Multiobjective Evolutionary Algorithms	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Sociology; Pareto optimization; Evolutionary computation; Computer science; Shape; Decomposition-based MOEA; multiobjective evolutionary algorithms based on decomposition (MOEA; D); multiobjective evolutionary algorithms; weight vector adjustment	NONDOMINATED SORTING APPROACH; GENETIC LOCAL SEARCH; OPTIMIZATION PROBLEMS; MOEA/D; PREFERENCE; PERFORMANCE; DESIGN; SELECTION; VERSION	Multiobjective evolutionary algorithms based on decomposition (MOEA/D) have attracted tremendous attention and achieved great success in the fields of optimization and decision-making. MOEA/Ds work by decomposing the target multiobjective optimization problem (MOP) into multiple single-objective subproblems based on a set of weight vectors. The subproblems are solved cooperatively in an evolutionary algorithm framework. Since weight vectors define the search directions and, to a certain extent, the distribution of the final solution set, the configuration of weight vectors is pivotal to the success of MOEA/Ds. The most straightforward method is to use predefined and uniformly distributed weight vectors. However, it usually leads to the deteriorated performance of MOEA/Ds on solving MOPs with irregular Pareto fronts. To deal with this issue, many weight vector adjustment methods have been proposed by periodically adjusting the weight vectors in a random, predefined, or adaptive way. This article focuses on weight vector adjustment on a simplex and presents a comprehensive survey of these weight vector adjustment methods covering the weight vector adaptation strategies, theoretical analyses, benchmark test problems, and applications. The current limitations, new challenges, and future directions of weight vector adjustment are also discussed.																	1089-778X	1941-0026				AUG	2020	24	4					634	649		10.1109/TEVC.2020.2978158													
J								Self-Adaptation in Nonelitist Evolutionary Algorithms on Discrete Problems With Unknown Structure	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Evolutionary algorithm (EA); level-based analysis; runtime analysis; self-adaptation	BLACK-BOX COMPLEXITY; LEVEL-BASED ANALYSIS; MUTATION-RATES; SEARCH	A key challenge to make effective use of evolutionary algorithms (EAs) is to choose appropriate settings for their parameters. However, the appropriate parameter setting generally depends on the structure of the optimization problem, which is often unknown to the user. Nondeterministic parameter control mechanisms adjust parameters using information obtained from the evolutionary process. Self-adaptation-where parameter settings are encoded in the chromosomes of individuals and evolve through mutation and crossover-is a popular parameter control mechanism in evolutionary strategies. However, there is little theoretical evidence that self-adaptation is effective, and self-adaptation has largely been ignored by the discrete evolutionary computation community. Here, we show through a theoretical runtime analysis that a nonelitist, discrete EA which self-adapts its mutation rate not only outperforms EAs which use static mutation rates on LEADINGONES(k) but also improves asymptotically on an EA using a state-of-the-art control mechanism. The structure of this problem depends on a parameter k, which is a priori unknown to the algorithm, and which is needed to appropriately set a fixed mutation rate. The self-adaptive EA achieves the same asymptotic runtime as if this parameter was known to the algorithm beforehand, which is an asymptotic speedup for this problem compared to all other EAs previously studied. An experimental study of how the mutation-rates evolve show that they respond adequately to a diverse range of problem structures. These results suggest that self-adaptation should be adopted more broadly as a parameter control mechanism in discrete, nonelitist EAs.																	1089-778X	1941-0026				AUG	2020	24	4					650	663		10.1109/TEVC.2020.2985450													
J								Efficacy of the Metropolis Algorithm for the Minimum-Weight Codeword Problem Using Codeword and Generator Search Spaces	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Combinatorial optimization; Markov chain; metropolis algorithm; minimum-weight codeword; mixing time analysis; randomized local search heuristics	OPTIMIZATION; DISTANCE	This article studies the efficacy of the Metropolis algorithm for the minimum-weight codeword problem. The input is a linear code C given by its generator matrix and our task is to compute a nonzero codeword in the code C of least weight. In particular, we study the Metropolis algorithm on two possible search spaces for the problem: 1) the codeword space and 2) the generator space. The former is the space of all codewords of the input code and is the most natural one to use and hence has been used in previous work on this problem. The latter is the space of all generator matrices of the input code and is studied for the first time in this article. In this article, we show that for an appropriately chosen temperature parameter the Metropolis algorithm mixes rapidly when either of the search spaces mentioned above are used. Experimentally, we demonstrate that the Metropolis algorithm performs favorably when compared to previous attempts. When using the generator space, the Metropolis algorithm is able to outperform the previous algorithms in most of the cases. We have also provided both theoretical and experimental justification to show why the generator space is a worthwhile search space to use for this problem.																	1089-778X	1941-0026				AUG	2020	24	4					664	678		10.1109/TEVC.2020.2980111													
J								A Data-Driven Parallel Scheduling Approach for Multiple Agile Earth Observation Satellites	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Satellites; Task analysis; Scheduling; Processor scheduling; Predictive models; Earth; Adaptation models; Agile earth observation satellite (EOS) scheduling; cooperative neuro-evolution of augmenting topologies (C-NEAT); data-driven; probability prediction model; task assignment strategy	SEARCH ALGORITHM; MULTISATELLITE	To address the large-scale and time-consuming multiple agile earth observation satellite (multi-AEOS) scheduling problems, this article proposes a data-driven parallel scheduling approach, which is composed of a probability prediction model, a task assignment strategy, and a parallel scheduling manner. In this approach, given the historical data of satellite scheduling, a prediction model is trained based on the cooperative neuro-evolution of augmenting topologies (C-NEAT) to predict the probabilities that a task will be fulfilled by different satellites. Driven by the probability prediction model, an assignment strategy is adopted for dividing the multi-AEOS scheduling problem into several single-AEOS scheduling subproblems, which can adaptively assign each task to the satellite with the highest predicted probability and greatly decrease the problem size. In a parallel manner, the single-AEOS scheduling subproblems are optimized, respectively, leading to an acceleration in the optimization efficiency of the original problem. Computational experiments indicate that the proposed approach presents better overall performance than other state-of-the-art methods within a very limited scheduling time. As the two main components of the proposed approach, the prediction model based on C-NEAT and the task assignment strategy also outperform other models with traditional training algorithms and inadaptive assignment strategies, respectively.																	1089-778X	1941-0026				AUG	2020	24	4					679	693		10.1109/TEVC.2019.2934148													
J								Distributed Pareto Optimization for Large-Scale Noisy Subset Selection	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Linear programming; Noise measurement; Approximation algorithms; Optimization; Greedy algorithms; Additive noise; Distributed algorithms; Distributed algorithms; experimental studies; large-scale; multiobjective evolutionary algorithms (MOEAs); noise; Pareto optimization; subset selection; theoretical analyses	EVOLUTIONARY OPTIMIZATION; GAUSSIAN-PROCESSES; ALGORITHMS; DESIGN	Subset selection, aiming to select the best subset from a ground set with respect to some objective function, is a fundamental problem with applications in many areas, such as combinatorial optimization, machine learning, data mining, computer vision, information retrieval, etc. Along with the development of data collection and storage, the size of the ground set grows larger. Furthermore, in many subset selection applications, the objective function evaluation is subject to noise. We thus study the large-scale noisy subset selection problem in this paper. The recently proposed DPOSS algorithm based on multiobjective evolutionary optimization is a powerful distributed solver for large-scale subset selection. Its performance, however, has been only validated in the noise-free environment. In this paper, we first prove its approximation guarantee under two common noise models, i.e., multiplicative noise and additive noise, disclosing that the presence of noise degrades the performance of DPOSS largely. Next, we propose a new distributed multiobjective evolutionary algorithm called DPONSS for large-scale noisy subset selection. We prove that the approximation guarantee of DPONSS under noise is significantly better than that of DPOSS. We also conduct experiments on the application of sparse regression, where the objective evaluation is often estimated using a sample data, bringing noise. The results on various real-world data sets, whose size can reach millions, clearly show the excellent performance of DPONSS.																	1089-778X	1941-0026				AUG	2020	24	4					694	707		10.1109/TEVC.2019.2929555													
J								Distributed Individuals for Multiple Peaks: A Novel Differential Evolution for Multimodal Optimization Problems	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Sociology; Statistics; Optimization; Space exploration; Refining; Aerospace electronics; Learning systems; Differential evolution (DE); distributed individuals DE (DIDE); lifetime mechanism; multimodal optimization	MULTIOBJECTIVE OPTIMIZATION; ALGORITHM; SEARCH	Locating more peaks and refining the solution accuracy on the found peaks are two challenging issues in solving multimodal optimization problems (MMOPs). To deal with these two challenges, a distributed individuals differential evolution (DIDE) algorithm is proposed in this article based on a distributed individuals for multiple peaks (DIMP) framework and two novel mechanisms. First, the DIMP framework provides sufficient diversity by letting each individual act as a distributed unit to track a peak. Based on the DIMP framework, each individual uses a virtual population controlled by an adaptive range adjustment strategy to explore the search space sufficiently for locating a peak and then gradually approach it. Second, the two novel mechanisms named lifetime mechanism and elite learning mechanism (ELM) cooperate with the DIMP framework. The lifetime mechanism is inspired by the natural phenomenon that every organism will gradually age and has a limited lifespan. When an individual runs out of its lifetime and also has good fitness, it is regarded as an elite solution and will be added to an archive. Then the individual restarts a new lifetime, so as to bring further diversity to locate more peaks. The ELM is proposed to refine the accuracy of those elite solutions in the archive, being efficient in dealing with the solution accuracy issue on the found peaks. The experimental results on 20 multimodal benchmark test functions show that the proposed DIDE algorithm has generally better or competitive performance compared with the state-of-the-art multimodal optimization algorithms.																	1089-778X	1941-0026				AUG	2020	24	4					708	719		10.1109/TEVC.2019.2944180													
J								A Framework to Handle Multimodal Multiobjective Optimization in Decomposition-Based Evolutionary Algorithms	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Pareto optimization; Indexes; Evolutionary computation; Spatial diversity; Decision making; Benchmark testing; Decomposition-based evolutionary algorithms; multimodal multiobjective optimization; reference vector-based evolutionary algorithms; solution space diversity	OMNI-OPTIMIZER; DIVERSITY; SELECTION; MOEA/D; BENCHMARKING; PERFORMANCE; 2-ARCHIVE; BEHAVIOR; SINGLE; EMOA	Multimodal multiobjective optimization is to locate (almost) equivalent Pareto optimal solutions as many as possible. While decomposition-based evolutionary algorithms have good performance for multiobjective optimization, they are likely to perform poorly for multimodal multiobjective optimization due to the lack of mechanisms to maintain the solution space diversity. To address this issue, this article proposes a framework to improve the performance of decomposition-based evolutionary algorithms for multimodal multiobjective optimization. Our framework is based on three operations: 1) assignment; 2) deletion; and 3) addition operations. One or more individuals can be assigned to the same subproblem to handle multiple equivalent solutions. In each iteration, a child is assigned to a subproblem based on its objective vector, i.e., its location in the objective space. The child is compared with its neighbors in the solution space assigned to the same subproblem. The performance of improved versions of six decomposition-based evolutionary algorithms by our framework is evaluated on various test problems regarding the number of objectives, decision variables, and equivalent Pareto optimal solution sets. Results show that the improved versions perform clearly better than their original algorithms.																	1089-778X	1941-0026				AUG	2020	24	4					720	734		10.1109/TEVC.2019.2949841													
J								Surrogate-Assisted Robust Optimization of Large-Scale Networks Based on Graph Embedding	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Robustness; Computational modeling; Germanium; Computational efficiency; Optimization methods; Task analysis; Complex networks; evolutionary algorithm (EA); graph embedding (GE); robustness; surrogate-assisted optimization	EVOLUTIONARY; ATTACKS; MODEL	Robust optimization of complex networks has attracted much attention in recent years. Although existing methods have been successful in achieving promising results, the computational cost for robust optimization tasks is extremely high, which prevents them from being further applied to large-scale networks. Thus, computationally efficient robust optimization methods are in high demand. This article proposes a low-cost method for estimating the robustness of networks with the help of graph embedding techniques and surrogate models. An evolutionary algorithm is then developed to find large-scale robust networks by combining the surrogate-assisted low-cost robustness estimator with the time-consuming real robustness measure by means of a model management strategy. The experimental results on different kinds of synthetic and real networks demonstrate the highly competitive search ability of the proposed algorithm. In addition, the algorithm is able to save up to 80% of the computation time for enhancing the robustness of large-scale networks compared with the state-of-the-art methods.																	1089-778X	1941-0026				AUG	2020	24	4					735	749		10.1109/TEVC.2019.2950935													
J								Novel Interactive Preference-Based Multiobjective Evolutionary Optimization for Bolt Supporting Networks	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Fasteners; Optimization; Rocks; Stability analysis; Tunneling; Bolt supporting network; interaction; multiobjective evolutionary optimization; preference; surrogate model	GENETIC ALGORITHMS	Previous methods of designing a bolt supporting network, which depend on engineering experiences, seek optimal bolt supporting schemes in terms of supporting quality. The supporting cost and time, however, have not been considered, which restricts their applications in real-world situations. We formulate the problem of designing a bolt supporting network as a three-objective optimization model by simultaneously considering such indicators as quality, economy, and efficiency. Especially, two surrogate models are constructed by support vector regression for roof-to-floor convergence and the two-sided displacement, respectively, so as to rapidly evaluate supporting quality during optimization. To solve the formulated model, a novel interactive preference-based multiobjective evolutionary algorithm is proposed. The highlight of generic methods which interactively articulate preferences is to systematically manage the regions of interest by three steps, that is, "partitioning-updating-tracking" in accordance with the cognition process of human. The preference regions of a decision-maker (DM) are first articulated and employed to narrow down the feasible objective space before the evolution in terms of nadir point, not the commonly used ideal point. Then, the DM's preferences are tracked by dynamically updating these preference regions based on satisfactory candidates during the evolution. Finally, individuals in the population are evaluated based on the preference regions. We apply the proposed model and algorithm to design the bolt supporting network of a practical roadway. The experimental results show that the proposed method can generate an optimal bolt supporting scheme with a good balance between supporting quality and the other demands, besides speeding up its convergence.																	1089-778X	1941-0026				AUG	2020	24	4					750	764		10.1109/TEVC.2019.2951217													
J								MUMI: Multitask Module Identification for Biological Networks	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Task analysis; Optimization; Multitasking; Search problems; Evolution (biology); Evolutionary computation; Active module; community detection; multifactorial evolution	EVOLUTIONARY MULTITASKING; COMMUNITY STRUCTURE; DISCOVERY; ALGORITHM	Identifying modules from biological networks is important since modules reveal essential mechanisms and dynamic processes in biological systems. Existing algorithms focus on identifying either active modules or topological modules (communities), which represent dynamic and topological units in the network, respectively. However, high-level biological phenomena, e.g., functions are emergent properties from the interplay between network topology and dynamics. Therefore, to fully explain the mechanisms underlying the high-level biological phenomena, it is important to identify the overlaps between communities and active modules, which indicate the topological units with significant changes of dynamics. However, despite the importance, there are no existing methods to do so. In this article, we propose the multitask module identification (MUMI) algorithm to detect the overlaps between active modules and communities simultaneously. The experimental results show that our method provides new insights into biological mechanisms by combining information from active modules and communities. By formulating the problem as a multitasking learning problem which searches for these two types of modules simultaneously, the algorithm can exploit their latent complementarities to obtain better search performance in terms of accuracy and convergence. Our MATLAB implementation of MUMI is available at https://github.com/WeiqiChen/Mumi-multitask-module-identification.																	1089-778X	1941-0026				AUG	2020	24	4					765	776		10.1109/TEVC.2019.2952220													
J								Paradoxes in Numerical Comparison of Optimization Algorithms	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Optimization; Benchmark testing; Mathematical model; Numerical models; Machine learning algorithms; Measurement; Convergence; Cycle ranking; evolutionary computation; machine learning; numerical comparison; optimization	PARTICLE SWARM OPTIMIZATION; DERIVATIVE-FREE OPTIMIZATION; GLOBAL OPTIMIZATION; SOFTWARE	Numerical comparison is often key to verifying the performance of optimization algorithms, especially, global optimization algorithms. However, studies have so far neglected issues concerning comparison strategies necessary to rank optimization algorithms properly. To fill this gap for the first time, we combine voting theory and numerical comparison research areas, which have been disjoint so far, and thus extend the results of the former to the latter for optimization algorithms. In particular, we investigate compatibility issues arising from comparing two and more than two algorithms, termed "C2" and "C2+" in this article, respectively. Through defining and modeling "C2" and "C2+" mathematically, it is uncovered and illustrated that numerical comparison can be incompatible. Further, two possible paradoxes, namely, "cycle ranking" and "survival of the nonfittest," are discovered and analyzed rigorously. The occurrence probabilities of these two paradoxes are also calculated under the no-free-lunch assumption, which shows the first justifiable use of the impartial culture assumption from voting theory, providing a point of reference to the frequency of the paradoxes occurring. It is also shown that significant influence on these probabilities comes from the number of algorithms and the number of optimization problems studied in the comparison. Further, various limiting probabilities when the number of optimization problems goes to infinity are also derived and characterized. The results would help guide benchmarking and developing optimization and machine learning algorithms.																	1089-778X	1941-0026				AUG	2020	24	4					777	791		10.1109/TEVC.2019.2955110													
J								A Novel Evolutionary Algorithm for Dynamic Constrained Multiobjective Optimization Problems	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Optical fibers; Sociology; Statistics; Heuristic algorithms; Linear programming; Optimization; Convergence; Change response; dynamic constrained multiobjective optimization; population selection; test problems	GENETIC ALGORITHMS; STRATEGY; ENVIRONMENTS; IMMIGRANTS; OBJECTIVES; TIME	To promote research on dynamic constrained multiobjective optimization, we first propose a group of generic test problems with challenging characteristics, including different modes of the true Pareto front (e.g., convexity-concavity and connectedness-disconnectedness) and the changing feasible region. Subsequently, motivated by the challenges presented by dynamism and constraints, we design a dynamic constrained multiobjective optimization algorithm with a nondominated solution selection operator, a mating selection strategy, a population selection operator, a change detection method, and a change response strategy. The designed nondominated solution selection operator can obtain a nondominated population with diversity when the environment changes. The mating selection strategy and population selection operator can adaptively handle infeasible solutions. If a change is detected, the proposed change response strategy reuses some portion of the old solutions in combination with randomly generated solutions to reinitialize the population, and a steady-state update method is designed to improve the retained previous solutions. The experimental results show that the proposed test problems can be used to clearly distinguish the performance of algorithms, and that the proposed algorithm is very competitive for solving dynamic constrained multiobjective optimization problems in comparison with state-of-the-art algorithms.																	1089-778X	1941-0026				AUG	2020	24	4					792	806		10.1109/TEVC.2019.2958075													
J								Optimal Sizing of Recycling Folded Cascode Amplifier for Low Frequency Applications Using New Hybrid Swarm Intelligence-Based Technique	APPLIED ARTIFICIAL INTELLIGENCE											PARTICLE SWARM; OPTIMIZATION; ALGORITHM	A new efficient design approach for sizing a high performance analog amplifier circuit namely the Recycling Folded Cascode (RFC) amplifier is presented. A RFC amplifier is an enhanced version of the conventional folded cascode amplifier and achieves better slew rate, gain, bandwidth, offset etc. for same area and power budget. Low frequency amplifiers such as biomedical or neural have a demanding requirement of low area, low power and low noise apart from meeting other optimal design specifications which have inherent trade-off amongst themselves. As a result, manual sizing becomes a computationally inefficient approach. Thus, swarm based optimization techniques have been employed to efficiently determine the optimal sizing for the RFC amplifier such that the area is minimized while meeting all the optimal design specifications considering the constraints. A new hybrid whale particle swarm optimization (HWPSO) algorithm is employed which takes advantage of the good qualities of both the whale algorithm and the PSO algorithm to optimize the area with less computational complexity. Simulations and statistical analysis have been performed and comparisons with other state of art algorithms reveals that HWPSO-based approach achieves a minimum circuit area of 21 mu m(2)with a mean Friedman's statistical rank of 2.05 while meeting optimal design specifications for low frequency systems. Finally, validation with circuit design tool Cadence Virtuoso is done and pre as well as post layout analysis have been performed which further illustrated a close agreement with algorithmic results.																	0883-9514	1087-6545				NOV 9	2020	34	13					994	1010		10.1080/08839514.2020.1795786		AUG 2020											
J								Energy-aware whale optimization algorithm for real-time task scheduling in multiprocessor systems	APPLIED SOFT COMPUTING										Multiprocessor systems; Task scheduling; Energy; DVFS; Whale optimization algorithm	GENETIC ALGORITHM; PERFORMANCE	The growth of Multiprocessing Systems (MPS) has become a necessity for dealing with complex tasks and speeding up their execution. Increasing the number of processing cores on a single chip produces a vast processing power, but the biggest obstacle is the energy generated from these cores. The traditional techniques guarantee to get the optimal schedule, but they are costly in terms of time and memory storage. In this paper, we propose an Improved Whale Algorithm (IWA) to allocate the dependent tasks in MPS with two objectives minimizing the energy consumption and the makespan. The processing cores are assumed to support Dynamic Voltage and Frequency Scaling (DVFS) as an effective technique to reduce energy. The allocation of tasks in MPS is an NP-hard problem. Inadequate scheduling of tasks can result in consuming energy. Also, the failure to complete the tasks before their predetermined deadlines is a critical issue for real-time applications. We consider three different sources of energy coming from the communication, idle, and active states of the processing cores. We use an initialization procedure to produce a population of candidate schedules that respects the precedence among tasks. In IWA, we employ two different discretization methods to map the continuous values into discrete ones. Two specialized crossover operations are adopted to boost the quality of the candidate schedules while respecting the dependencies among the tasks. IWA implements the Load Balancing Improvement (LBI) strategy to alleviate the load of tasks from heavy cores. LBI plays a vital role in decreasing the static energy consumption. We compare IWA with the other algorithms, and the results show the superiority of IWA. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106349	10.1016/j.asoc.2020.106349													
J								Efficient design of wideband digital fractional order differentiators and integrators using multi-verse optimizer	APPLIED SOFT COMPUTING										Digital fractional order differentiator; Digital fractional order integrator; Particle Swarm Optimization; Multi-verse optimizer; Genetic Algorithm; Metaheuristic; Optimization; L-1-norm	SERIES EXPANSION; FIR; DOMAIN; POWER; APPROXIMATIONS	In this paper, a novel method is proposed based on combining L-1-norm optimally criterion with a recently-proposed metaheuristic called multi-verse optimizer (MVO) to design 2nd-4th order stable, minimum phase and wideband infinite impulse response (IIR) digital fractional order differentiators (DFODs) for the fractional order differentiators (FODs) of one-half, one-third and one-fourth order. To confirm the superiority of the proposed approach, we conduct comparisons of the MVO-based designs with the real-coded genetic algorithm (RCGA) and particle swarm optimization (PSO)-based designs in terms of accuracy, robustness, consistency, and efficiency. The transfer functions of the proposed designs are inverted to obtain new models of digital fractional order integrators (DFOIs) of the same order. A comparative study of the frequency responses of the proposed digital fractional order differentiators and integrators with the ones of the existing models is then conducted. The results demonstrate that the proposed designs yield the optimal magnitude responses in terms of absolute magnitude error (AME) with flat response profiles. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106340	10.1016/j.asoc.2020.106340													
J								Developing two heuristic algorithms with metaheuristic algorithms to improve solutions of optimization problems with soft and hard constraints: An application to nurse rostering problems	APPLIED SOFT COMPUTING										Nurse rostering problem; Decision tree; Greedy search algorithm; Bat algorithm; Particle swarm optimization	PARTICLE SWARM OPTIMIZATION; VEHICLE-ROUTING PROBLEM; FEATURE-SELECTION; SCHEDULING PROBLEM; PROGRAMMING-MODEL; BEE COLONY; TIME; CLASSIFICATION; RESIDENTS; SERVICE	Many researchers have studied optimization problems with soft and hard constraints, such as school timetabling, nurse rostering, vehicle routing with soft time window, and job/machine scheduling. Nurse rostering problem (NRP) is the research problem in this paper. This study proposes two heuristic algorithms, which are the decision tree method and the greedy search algorithm, to integrate with metaheuristic algorithms in order to generate better initial solutions in less time and to improve solutions' quality. This research examines the algorithms' performance based on two scenarios and two metaheuristic algorithms: bat algorithm (BA) and particle swarm optimization (PSO). For the two scenarios, BA (or PSO) with the decision tree method outperforms BA (or PSO) without the decision tree method, and BA (or PSO) with the greedy search algorithm outperforms BA (or PSO) without the greedy search algorithm. Furthermore, the results show that BA (or PSO) with the decision tree method and the greedy search algorithm can generate better initial solutions in less time and improve solutions' quality. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106336	10.1016/j.asoc.2020.106336													
J								A full migration BBO algorithm with enhanced population quality bounds for multimodal biomedical image registration	APPLIED SOFT COMPUTING										Biogeography-based Optimization; Swarm intelligence; Image registration; Medical imaging; Nature-inspired algorithm	SEGMENTATION APPROACH; OPTIMIZATION; INTENSITY; ROBUST; EFFICIENT	Medical images acquired from different modalities give rise to many practical problems in image registration. Intensity-based registration techniques have been increasingly used in multimodal image registration; these techniques integrate different images that have shared content into a single representation, by transformation. The estimation of the optimal transformation requires the optimization of a similarity metric between the images. Recently, many optimization methods have been proposed that focus on the development of the optimization component. However, there is still room for large amounts of improvement, from both an efficiency point of view and a quality perspective. In this paper we present a new Biogeography-based Optimization (BBO) algorithm, the Biogeography-based Optimization algorithm with Elite Learning (BBO-EL), for multimodal medical image registration. First, we propose a hybrid full migration operator in which each individual has the chance to perform the migration operation and the whole population has the chance to expand the search space. In this way, the search ability of the BBO algorithm is enhanced and matches well the characteristics of multimodal medical image registration. In addition, considering that the quality of some individuals could be deteriorated as caused by the migration operation, we propose an undo operator on the deteriorated individuals. Thus, the lower bound of the whole population's quality can be maintained at a higher level. Furthermore, in the original BBO algorithm, a number of good individuals might be not involved in the migration operation, and we present an elite learning operator that is based on social comparison theory to improve the upper bound of the whole population's quality. Therefore, after improving both the lower bound and the upper bound of the whole population's quality, the accuracy and the convergence speed of the multimodal medical registration can be greatly enhanced. The BBO-EL has been tested in many experiments on benchmark datasets include six kind of different modality images, from up to eighteen different patients, which can make up 54 multimodal registration scenarios. The BBO-EL obtained 30 best performance scenarios while the state-of-the-art algorithm obtained 21 scenarios. The results demonstrated that BBO-EL outperforms the state-of-the-art algorithm in most cases for practical problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106335	10.1016/j.asoc.2020.106335													
J								FBI inspired meta-optimization	APPLIED SOFT COMPUTING										Forensic-based investigation algorithm; Metaheuristic optimization; Swarm intelligence and evolutionary computation; Benchmark functions; Construction engineering and project management	LEARNING-BASED OPTIMIZATION; DIFFERENTIAL EVOLUTION; FIREFLY ALGORITHM; KRILL HERD; SEARCH	This study developed a novel optimization algorithm, called Forensic-Based Investigation (FBI), inspired by the suspect investigation-location-pursuit process that is used by police officers. Although numerous unwieldy optimization algorithms hamper their usability by requiring predefined operating parameters, FBI is a user-friendly algorithm that does not require predefined operating parameters. The performance of parameter-free FBI was validated using four experiments: (1) The robustness and efficiency of FBI were compared with those of 12 representations of the top leading metaphors by using 50 renowned multidimensional benchmark problems. The result indicated that FBI remarkably outperformed all other algorithms. (2) FBI was applied to solve a resource-constrained scheduling problem associated with a highway construction project. The experiment demonstrated that FBI yielded the shortest schedule with a success rate of 100%, indicating its stability and robustness. (3) FBI was utilized to solve 30 benchmark functions that were most recently presented at the IEEE Congress on Evolutionary Computation (CEC) competition on bound-constrained problems. Its performance was compared with those of the three winners in CEC to validate its effectiveness. (4) FBI solved high-dimensional problems, by increasing the number of dimensions of benchmark functions to 1000. FBI is efficient because it requires a relatively short computational time for solving problems, it reaches the optimal solution more rapidly than other algorithms, and it efficaciously solves high-dimensional problems. Given that the experiments demonstrated FBI's robustness, efficiency, stability, and user-friendliness, FBI is promising for solving various complex problems. Finally, this study provided the scientific community with a metaheuristic optimization platform for graphically and logically manipulating optimization algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106339	10.1016/j.asoc.2020.106339													
J								A remaining useful life prediction method with long-short term feature processing for aircraft engines	APPLIED SOFT COMPUTING										Remaining useful life prediction; Aircraft engine; Differential technique; Fibonacci window feature extension; CatBoost	PROGNOSTICS; SYSTEMS	As one of the key components of aircraft, any failure of the engine can lead to serious accidents. The reliability and safety can be guaranteed by predicting the remaining useful life of the aircraft engine. The data-driven approaches are suitable for predicting the remaining useful life of the aircraft engine, but they generally suffer from the following challenges: (i) how to capture the real degradation trend of the engine; (ii) how to efficiently and fully utilize the temporal correlation between the sensor data; (iii) how to handle highly nonlinear data. In order to address these challenges, an effective data-driven remaining useful life prediction method is proposed in this paper. Firstly, a long-term differential technique is proposed to extract forward differential features, which fully reflects the actual degradation trend in the entire lifetime. Then, the Fibonacci window is proposed for short-term feature extension, which makes full use of the temporal correlation of the historical data and effectively reduces the extra computational load. Finally, the CatBoost algorithm is used to predict the remaining useful life on the highly nonlinear data, and superior prediction performance is obtained. In order to verify the effectiveness of the proposed method, the experiments are carried out on the aircraft engine dataset provided by NASA. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106344	10.1016/j.asoc.2020.106344													
J								Point and interval forecasting for wind speed based on linear component extraction	APPLIED SOFT COMPUTING										Wind speed forecasting; Probability interval prediction; Deterministic point prediction; Hybrid system; Component extraction	EMPIRICAL MODE DECOMPOSITION; SINGULAR SPECTRUM ANALYSIS; FUZZY NEURAL-NETWORK; MULTIOBJECTIVE OPTIMIZATION; PREDICTION INTERVALS; POWER-GENERATION; HYBRID ARIMA; MULTISTEP; ANN; COMBINATION	As a renewable energy, wind power attracts more and more attention. However, the intermittence and randomness of wind speed make the utilization of wind power a challenging task. Therefore, it is essential to improve the ability of wind speed forecasting. This paper proposes a new hybrid system for wind speed forecasting, which includes three modules: data pre-processing module, deterministic point prediction module and probability interval prediction module. Empirical mode decomposition (EMD) and singular spectrum analysis (SSA) are conducted to extract the linear component of the initial wind speed series in data pre-processing module, autoregressive integrated moving average model (ARIMA) and back propagation neural network (BPNN) are employed to produce the prediction points (PPs) of the initial data in deterministic point prediction module, and ARIMA and improved First Order Markov Chain (IFOMC) model are provided to make the uncertainty analysis of wind speed in probability interval prediction module. A case study is selected to test the performance of the new system. The simulation results demonstrate that the new system can achieve better precision and higher efficiency than several benchmark methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106350	10.1016/j.asoc.2020.106350													
J								Inception v3 based cervical cell classification combined with artificially extracted features	APPLIED SOFT COMPUTING										Inception v3; Artificial feature extraction; Transfer learning; Medical image processing; Cervical cancer disease diagnosis	PAP-SMEAR IMAGES; DEEP; SEGMENTATION; NUCLEI	Traditional cell classification methods generally extract multiple features of the cell manually. Moreover, the simple use of artificial feature extraction methods has low universality. For example, it is unsuitable for cervical cell recognition because of the complexity of the cervical cell texture and the large individual differences between cells. Using the convolutional neural network classification method is a good way to solve this problem. However, although the cell features can be extracted automatically, the cervical cell domain knowledge will be lost, and the corresponding features of different cell types will be missing; hence, the classification effect is not sufficiently accurate. Aiming at addressing the limitations of the two mentioned classification methods, this paper proposes a cell classification algorithm that combines Inception v3 and artificial features, which effectively improves the accuracy of cervical cell recognition. In addition, to address the under-fitting problem and carry out effective deep learning training with a relatively small amount of medical data, this paper inherits the strong learning ability from transfer learning, and achieves accurate and effective cervical cell image classification based on the Herlev dataset. Using this method, an accuracy of more than 98% is achieved, providing an effective framework for computer aided diagnosis of cervical cancer. The proposed algorithm has good universality, low complexity, and high accuracy, rendering it suitable for further extension and application to the classification of other types of cancer cells. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106311	10.1016/j.asoc.2020.106311													
J								Composite Monte Carlo decision making under high uncertainty of novel coronavirus epidemic using hybridized deep learning and fuzzy rule induction	APPLIED SOFT COMPUTING										Monte Carlo simulation; Decision support; COVID-19; 2019-nCoV; Coronavirus		In the advent of the novel coronavirus epidemic since December 2019, governments and authorities have been struggling to make critical decisions under high uncertainty at their best efforts. In computer science, this represents a typical problem of machine learning over incomplete or limited data in early epidemic Composite Monte-Carlo (CMC) simulation is a forecasting method which extrapolates available data which are broken down from multiple correlated/casual micro-data sources into many possible future outcomes by drawing random samples from some probability distributions. For instance, the overall trend and propagation of the infested cases in China are influenced by the temporal-spatial data of the nearby cities around the Wuhan city (where the virus is originated from), in terms of the population density, travel mobility, medical resources such as hospital beds and the timeliness of quarantine control in each city etc. Hence a CMC is reliable only up to the closeness of the underlying statistical distribution of a CMC, that is supposed to represent the behaviour of the future events, and the correctness of the composite data relationships. In this paper, a case study of using CMC that is enhanced by deep learning network and fuzzy rule induction for gaining better stochastic insights about the epidemic development is experimented. Instead of applying simplistic and uniform assumptions for a MC which is a common practice, a deep learning-based CMC is used in conjunction of fuzzy rule induction techniques. As a result, decision makers are benefited from a better fitted MC outputs complemented by min-max rules that foretell about the extreme ranges of future possibilities with respect to the epidemic. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106282	10.1016/j.asoc.2020.106282													
J								LSTSVM classifier with enhanced features from pre-trained functional link network	APPLIED SOFT COMPUTING										LSTSVM; Neural networks; RVFL	SUPPORT VECTOR MACHINE	In this paper, we propose an improved model for the classification problems. We use least squares twin support vector machines (LSTSVM) and pre-trained functional link to enhance the feature space. LSTSVM algorithm is used in many real world classification problems as it has lower computational complexity and solves system of linear equations instead of solving quadratic programming problems (QPPs). Since neural network models provide implicit feature representation and is one of the reasons for the success of neural networks. Here, we propose a model wherein the input feature space is enhanced by the pre-trained functional link network. Weights are generated by LSTSVM, and a non-linear function is applied on the product between input features and the weights to get the enhanced features. These features are concatenated with the input features to get the extended feature space. Final classification is done by LSTSVM based on these extended features. Numerical experiments and statistical tests conducted show that the proposed model outperforms the baseline methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106305	10.1016/j.asoc.2020.106305													
J								Dynamic differential annealed optimization: New metaheuristic optimization algorithm for engineering applications	APPLIED SOFT COMPUTING										Dynamic differential annealed optimization; Optimization algorithms; Computational intelligence; Path planning; Engineering design optimization	PARTICLE SWARM OPTIMIZATION; NATURE-INSPIRED ALGORITHM; EVOLUTION STRATEGY; GENETIC ALGORITHM; DESIGN; SEARCH; STEELS; GA	This work proposes a novel optimization algorithm which can be used to solve a wide range of mathematical optimization problems where the global minimum or maximum is required. The new algorithm is based on random search and classical simulated annealing algorithm (it mimics the modern process of producing high-quality steel) and is designated dynamic differential annealed optimization (DDAO). The proposed algorithm was benchmarked for 51 test functions. The dynamic differential annealed optimization algorithm has been compared to a large number of highly cited optimization algorithms. Over numerical tests, DDAO has outperformed some of these algorithms in many cases and shown high performance. Constrained path planning and spring design problem were selected as a practical engineering optimization problem. DDAO converged to the global minimum of problems efficiently, and for spring design problem DDAO has found the best feasible solution than what is found by many algorithms. (C) 2020 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				AUG	2020	93								106392	10.1016/j.asoc.2020.106392													
J								A framework based on (probabilistic) soft logic and neural network for NLP	APPLIED SOFT COMPUTING										Neural networks; Sentiment classification; Part-of-speech tagging; Soft logic; Probabilistic soft logic		Deep neural networks have emerged as a flexible framework that achieved state-of-the-art performance in many NLP applications such as machine translation, named entity recognition, sentiment analysis, and part-of-speech tagging. The main advantage of these neural models is their ability to learn useful representations without hand-engineering features. While this success, these models still suffer from the interpretability issue. More recently, probabilistic soft logic (PSL) is a promising framework based on first-order logic that achieves interesting results in both computer vision and NLP by capturing semantic relationships between entities. Moreover, unifying knowledge-driven modeling approaches and data-driven approaches is a promising framework that will have an exciting impact on structured prediction problems. In this paper, we developed NeuralGLogic a generalization framework of the previous model proposed by Huet al. (2016) that combines deep neural networks with logic rules built either using Soft Logic (SL) or Probabilistic Soft Logic (PSL). Furthermore, we evaluate our framework on different neural network architectures applied to two NLP tasks: sentiment classification and part-of-speech tagging. Experimental results showed that we were able to improve the results over the baselines and outperformed all the previous state-of-the-art systems emphasizing the utility of both SL and PSL rules in reducing the uninterpretability of the neural models thus validating our intuition. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106232	10.1016/j.asoc.2020.106232													
J								Introducing clustering based population in Binary Gravitational Search Algorithm for Feature Selection	APPLIED SOFT COMPUTING										Gravitational search algorithm; Feature selection; Initial population clustering; UCI dataset	PARTICLE SWARM OPTIMIZATION; CLASSIFICATION; HYBRID	Feature Selection (FS) is an important aspect of knowledge extraction as it helps to reduce dimensionality of data. Among the numerous FS algorithms proposed over the years, Gravitational Search Algorithm (GSA) is a popular one which has been applied to various domains. However, GSA suffers from the problem of pre-mature convergence which affects exploration leading to performance degradation. To aid exploration, in the present work, we use a clustering technique in order to make the initial population distributed over the entire feature space and to increase the inclusion of features which are more promising. The proposed method is named Clustering based Population in Binary GSA (CPBGSA). To assess the performance of our proposed model, 20 standard UCI datasets are used, and the results are compared with some contemporary methods. It is observed that CPBGSA outperforms other methods in 12 out of 20 cases in terms of average classification accuracy. The relevant codes of the entire CPBGSA model can be found in the provided link: https://github.com/ManosijGhosh/Clustering-based-Population-in-Binary-GSA. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106341	10.1016/j.asoc.2020.106341													
J								A memory-based Grey Wolf Optimizer for global optimization tasks	APPLIED SOFT COMPUTING										Optimization; Swarm intelligence; Grey Wolf Optimizer; Exploration and exploitation	MOTH-FLAME OPTIMIZATION; POWER DISPATCH; ALGORITHM; EVOLUTION; SCHEME	Grey Wolf Optimizer (GWO) is a new nature-inspired metaheuristic algorithm based on the leadership and social behaviour of grey wolves in nature. It has shown potential to solve several real-life applications, but still for some complex optimization tasks, it may face the problem of getting trapped at local optima and premature convergence. Therefore, in this study, to prevent from these drawbacks and to get a more stable sense of balance between exploitation and exploration, a new modified GWO called memory-based Grey Wolf Optimizer (mGWO) is proposed. In the mGWO, the search mechanism of the wolves is modified based on the personal best history of each individual wolves, crossover and greedy selection. These strategies help to enhance the global exploration, local exploitation and an appropriate balance between them during the search procedure. To investigate the effectiveness of the proposed mGWO, it has been tested on standard and complex benchmarks given in IEEE CEC 2014 and IEEE CEC 2017. Furthermore, some real engineering design problems and multilevel thresholding problem are also solved using the mGWO. The results analysis and its comparison with other algorithms demonstrate the better search-efficiency, solution accuracy and convergence rate of the proposed mGWO in performing the global optimization tasks. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106367	10.1016/j.asoc.2020.106367													
J								Discrete evolutionary multi-objective optimization for energy-efficient blocking flow shop scheduling with setup time	APPLIED SOFT COMPUTING										Blocking flow shop; Energy consumption; Multi-objective evolutionary optimization; Self-adaptive	FLEXIBLE JOB-SHOP; WATER-WAVE OPTIMIZATION; BEE COLONY ALGORITHM; TRANSPORTATION; CONSUMPTION; MAKESPAN	Sustainable scheduling problems have been attracted great attention from researchers. For the flow shop scheduling problems, researches mainly focus on reducing economic costs, and the energy consumption has not yet been well studied up to date especially in the blocking flow shop scheduling problem. Thus, we construct a multi-objective optimization model of the blocking flow shop scheduling problem with makespan and energy consumption criteria. Then a discrete evolutionary multi-objective optimization (DEMO) algorithm is proposed. The three contributions of DEMO are as follows. First, a variable single-objective heuristic is proposed to initialize the population. Second, the self-adaptive exploitation evolution and self-adaptive exploration evolution operators are proposed respectively to obtain high quality solutions. Third, a penalty-based boundary interstation based on the local search, called by PBI-based-local search, is designed to further improve the exploitation capability of the algorithm. Simulation results show that DEMO outperforms the three state-of-the-art algorithms with respect to hypervolume, coverage rate and distance metrics. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106343	10.1016/j.asoc.2020.106343													
J								Cultural coalitions detection approach using GPU based on hybrid Bat and Cultural Algorithms	APPLIED SOFT COMPUTING										GPU; Parallel approaches; Hybrid approaches; Cultural algorithm; Bat optimization; Culture; Coalitions; Social agents		Currently, robot technology plays a crucial role in our modern life. Its great favor over humanity was not limited to the industrial domain, but also on the social one. Social robots become equipped with an artificial culture that allows them to interact with humans. Although the great importance of this technology, some misdeeds can create a real catastrophe. Coalition creation is one of its most dangerous troubles, where a subset of robots cooperates to impose pernicious decisions. There are efficient coalitions detection methods, but their exponential computation time makes their use limited only to small data. This paper is an extended version of Kechid and Drias (2019) presented at the IEA/AIE 2019 conference. In this paper, we propose a Cultural coalitions detection approach using GPU based on hybrid Bat and Cultural Algorithms. Unlike the existing literature, We view the problem of finding coalitions as an optimization problem to get relevant solutions in a significantly reduced amount of time. The proposed approach can increase the population diversity and improve the searching ability for an optimal exploration-exploitation balance. Also, it can launch several cultural bats in GPU to make real parallelism. Experimental results on several datasets show that the proposed method will considerably reduce the runtime. These datasets represent the result of artificial cultural agents playing the colored trails (CT) game. Concerning the creation of profiles, we use real datasets generated based on the World Values survey. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106368	10.1016/j.asoc.2020.106368													
J								Optimal driving based trip planning of electric vehicles using evolutionary algorithms: A driving assistance system	APPLIED SOFT COMPUTING										Driving assistance system; Optimal driving; Electric vehicle; Multi-objective optimization; Evolutionary algorithm	MULTIOBJECTIVE GENETIC ALGORITHM; ENERGY-CONSUMPTION; ACCELERATION; RANGE; PREDICTION; IMPACT; SPEED	The existing driving assistance systems (DAS) are not capable to manage the electric vehicle (EV) problems namely insufficiency of charging stations and inadequate range. A novel DAS is presented here to extend the range and overcome other EV drawbacks by suggesting the driver an optimal driving strategy (ODS) continuously throughout trip performing. ODS is decided by solving a multi-objective optimization problem (MOOP), subsequently adopting a multi-criterion decision making technique. Implementation of the DAS in real application requires both better optimization results and low computational time. A study was carried out to investigate the DAS performance with four contending evolutionary algorithms (EAs), NSGAII (a non-dominated sorting multi-objective genetic algorithm), PESA (Pareto envelope-based selection algorithm), PAES (Pareto archived evolution strategy), and SPEA 2 (Strength Pareto evolutionary algorithm). After an initial investigation of EA performances based on different matrices, NSGAII and PESA were found to be most suitable. The natures of decision variables in the Pareto-optimal solutions were analyzed. After an extensive analysis based on different micro-trip structures, it was found that without considering the computational time, PESA solutions possess better convergence and diversity properties than NSGAII solutions. Various approaches were adopted to minimize DAS computation time considering both NSGAII and PESA without significantly compromising the solution's optimality. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106361	10.1016/j.asoc.2020.106361													
J								Modified two-phase fuzzy goal programming integrated with IF-TOPSIS for green supplier selection	APPLIED SOFT COMPUTING										Green supplier selection; Intuitionistic fuzzy TOPSIS; Two-phase fuzzy goal programming; Multi-objective programming	GROUP DECISION-MAKING; MULTIOBJECTIVE MATHEMATICAL-MODEL; ORDER ALLOCATION PROBLEM; ANALYTIC NETWORK PROCESS; MULTI CRITERIA APPROACH; PERFORMANCE EVALUATION; QUANTITY DISCOUNTS; VENDOR SELECTION; CHAIN; MCDM	The environmental consciousness of society and globally competitive market have considerably increased thanks to the scientific studies, media, governmental and non-governmental organizations. In this regard, environmental factors have been considered within the supplier selection process which is a major decision point in supply chains. Hence, in addition to the optimization of the traditional criteria, green criteria have also started to take its place in the supplier selection problem. In this study, an integrated methodology including the Intuitionistic Fuzzy Technique for Order Preference by Similarity to Ideal Solution (IF-TOPSIS) and a modified two-phase fuzzy goal programming model are proposed to better address this selection problem in a multi-item/multi-supplier/multi-period environment. The detailed steps are explicitly provided within the proposed methodology. In this respect, the criteria importance weights are determined via IF-TOPSIS which enables the opportunity to handle the vagueness within the evaluation process of decision-makers. Afterward, the obtained importance weights are used in the modified two-phase fuzzy goal programming model for selecting the best suppliers. An application in the air filter industry is performed to demonstrate the validation of the proposed methodology. Consequently, the proposed methodology successfully provides the best selection of suppliers by satisfying both classic and green criteria. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106371	10.1016/j.asoc.2020.106371													
J								Online advertising assignment problem without free disposal	APPLIED SOFT COMPUTING										Online bipartite matching; Online algorithms; Deterministic algorithm; Stochastic programming; Benders decomposition	ALGORITHM	This paper presents an online advertising assignment problem that generalizes the online version of the bipartite matching problem. Specifically, it focuses on the Display Ads problem, which is a generalization of the edge-weighted and capacitated matching problem. The display ads problem has been studied alongside the property of free disposal, in which an advertisement is allowed to be matched more times than its capacity. Although the problem with free disposal is tractable, the problem situation might be restricted and challenging to apply to other types of problems. The objective of this research on the display ads problem is to maximize the total weight of matched edges while considering a strict capacity constraint. This paper analyzes two online input orders (adversarial and probabilistic orders) to the problem. For the adversarial order, we design deterministic algorithms with worst-case guarantees and prove the competitive ratios of them. Upper bounds for the problem are also proposed. For the probabilistic order, stochastic online algorithms, consisting of scenario-based stochastic programming and Benders decomposition, are presented. We conduct numerical experiments of the stochastic online algorithm in two probabilistic order models (known IID and random permutation). (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106370	10.1016/j.asoc.2020.106370													
J								A Quantum-Inspired Self-Supervised Network model for automatic segmentation of brain MR images	APPLIED SOFT COMPUTING										Quantum computing; Medical image segmentation; Fully Convolutional Neural Network; QIBDS Net; U-Net	NEURAL-NETWORK	The classical self-supervised neural network architectures suffer from slow convergence problem and incorporation of quantum computing in classical self-supervised networks is a potential solution towards it. In this article, a fully self-supervised novel quantum-inspired neural network model referred to as Quantum-Inspired Self-Supervised Network (QIS-Net) is proposed and tailored for fully automatic segmentation of brain MR images to obviate the challenges faced by deeply supervised Convolutional Neural Network (CNN) architectures. The proposed QIS-Net architecture is composed of three layers of quantum neuron (input, intermediate and output) expressed as qbits. The intermediate and output layers of the QIS-Net architecture are inter-linked through bi-directional propagation of quantum states, wherein the image pixel intensities (quantum bits) are self-organized in between these two layers without any external supervision or training. Quantum observation allows to obtain the true output once the superimposed quantum states interact with the external environment. The proposed self-supervised quantum-inspired network model has been tailored for and tested on Dynamic Susceptibility Contrast (DSC) brain MR images from Nature data sets for detecting complete tumor and reported promising accuracy and reasonable dice similarity scores in comparison with the unsupervised Fuzzy C-Means clustering, self-trained QIBDS Net, Opti-QIBDS Net, deeply supervised U-Net and Fully Convolutional Neural Networks (FCNNs). (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106348	10.1016/j.asoc.2020.106348													
J								Security analysis on dummy based side-channel countermeasures-Case study: AES with dummy and shuffling	APPLIED SOFT COMPUTING										Side-channel analysis; Simple power analysis; Hiding countermeasure; BCDC	HIGHER-ORDER MASKING; POWER ANALYSIS	Side-channel analysis is a serious type of attack that can break mathematically secure cryptographic algorithms. Many studies have designed countermeasures against side-channel analysis, such as masking and hiding schemes. Frequently, designers employ combined countermeasures that use both a first-order masking scheme and a hiding scheme to provide sufficient security and efficiency. Random insertion of dummy operations scheme, which is one of the hiding schemes, randomly changes the execution time of the operation to be attacked by inserting dummy operations. However, if the dummy operations can be distinguished from real ones, attackers could extract secret information with lower complexity than the intended attack complexity with the designer inserting the dummy operations. In this paper, we present a novel vulnerability that can enable dummy and real operations to be distinguished for various implementation methods using C language on the XMEGA128D4 microprocessor. This novel vulnerability occurs regardless of the four methods of implementation of dummy operations and the compile levels. We also present a new countermeasure against this vulnerability and demonstrate the security through practical experimentation. (C) 2020 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				AUG	2020	93								106352	10.1016/j.asoc.2020.106352													
J								Efficient hybrid local search heuristics for solving the travelling thief problem	APPLIED SOFT COMPUTING										Combinatorial optimization; Interdependence; Travelling thief problem; Local search; Bit-flip	LIN-KERNIGHAN; OPTIMIZATION; ALGORITHM; ANOVA	Real-world problems often consist of several interdependent subproblems. The degree of interaction of the subproblems is associated with the complexity of the problem and solving each subproblem optimally not ensure the optimal solution of the overall problem. The Travelling Thief Problem (TTP) integrates two well-known combinatorial optimization problems namely the classical Travelling Salesman Problem (TSP) and the 0-1 Knapsack Problem (KP). TTP was introduced to represent the complication of a real-world combinatorial optimization problem. The goal of this problem is to provide a tour to a thief over all the cities and a picking plan that determines which item should be taken from which city to achieve the maximum benefits. The KP component of the TTP is more efficient as compared to the TSP component for optimization. Our proposed method mainly focuses on constructing a picking plan for a near-optimal tour generated by Chained Lin-Kernighan Heuristic (CLKH). In this picking plan, items are picked up according to their scoring value which is calculated by our proposed formulation. Additionally, bit-flip is used for a better solution that can give a more profitable picking plan. The experimental results suggest that our proposed approach can meet or beat current state-of-the-art methods for a large number of TTP instances. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				AUG	2020	93								106284	10.1016/j.asoc.2020.106284													
J								Short-term photovoltaic power generation forecasting based on random forest feature selection and CEEMD: A case study	APPLIED SOFT COMPUTING										PV power generation forecasting; Random forest; Similar days discrimination; Complementary ensemble empirical mode decomposition; Improved back propagation neural network	EMPIRICAL MODE DECOMPOSITION; SOLAR; PREDICTION; UNCERTAINTY	To mitigate solar curtailment caused by large-scale development of photovoltaic (PV) power generation, accurate forecasting of PV power generation is important. A hybrid forecasting model was constructed that combines random forest (RF), improved grey ideal value approximation (IGIVA), complementary ensemble empirical mode decomposition (CEEMD), the particle swarm optimization algorithm based on dynamic inertia factor (DIFPSO), and backpropagation neural network (BPNN), called RF-CEEMD-DIFPSO-BPNN. PV power generation is affected by many factors. The RF method is used to calculate the importance degree and rank the factors, then eliminate the less important factors. Then, the importance degree calculated by RF is transferred as the weight values to the IGIVA model to screen the similar days of different weather types to improve the data quality of the training sets. Then, the original power sequence is decomposed into intrinsic mode functions (IMFs) at different frequencies and a residual component by CEEMD to weaken the fluctuation of the original sequence. We empirically analyzed a PV power plant to verify the effectiveness of the hybrid model, which proved that the RF-CEEMD-DIFPSO-BPNN is a promising approach in terms of PV power generation forecasting. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106389	10.1016/j.asoc.2020.106389													
J								Optimal generation scheduling of pumped storage hydro-thermal system with wind energy sources	APPLIED SOFT COMPUTING										Optimal generation scheduling; Modified crisscross particle swarm optimization; Pumped storage unit; Wind energy	PARTICLE SWARM OPTIMIZATION; POWER-SYSTEM; ALGORITHM; HYDRO; HYBRID; SOLAR; PSO; OPERATION; DISPATCH; BATTERY	In this article, the pumped storage hydrothermal system with wind energy sources (PSHTS-WES) has been modelled. The generation scheduling problem consists of mixed decision variables. In order to search for an optimum generation schedule for the PSHTS-WES system, a solution methodology has been proposed. In the proposed solution methodology, a modified crisscross PSO (MCPSO) technique has been proposed to deal with continuous decision variables adaptively. Further, an improved binary PSO (BPSO) technique has been implemented to search the binary decision variables. The proposed methodology has been implemented for three different test systems. Test system-I and II are the hydrothermal system (HTS) and PSHTS, respectively. The test system-III is a coordinated PSHTS-WES energy source. The achieved results have been compared with the other state-of-art algorithms. It has been found that the proposed solution methodology is able to search better results with the least standard deviation and mean computational time. The presence of a pumped storage unit has reduced the total thermal power generation by 221.23 MW, and the total cost obtained has been reduced by 2.42% for test system-II. The impact of WES is evident from the results of test system-III, which illustrated that WES able to reduce the total thermal power generation by 4528.85 MW and optimal cost by 19.86%. The robustness of the proposed solution methodology has been verified by implementing the two-sample t-test. The analysis from the numerical results verifies that the proposed methodology can achieve a better solution in comparison with the established techniques. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106345	10.1016/j.asoc.2020.106345													
J								Game Theory Based Pixel Approximation for Remote Sensing Imagery	APPLIED SOFT COMPUTING										Game theory; Rough set theory; Granulation; Clustering; Mixed pixels; Silhouette coefficient; Calinski-Harabasz Index; Davies Bouldin index	MIXED PIXELS; CLASSIFICATION; SELECTION; PURE	Classification of remote sensing images faces several challenges due to mixed pixels. Such pixels that are wrongly classified are called mixed pixels. There is uncertainty about the class label of mixed pixels as they represent the average energy emitted from different objects present within their spatial extent. Rough set theory can address the vagueness in data. Therefore, in this work rough set concept is used to identify the mixed pixels. In a multi-player environment, the Game theory is a science of making rational decisions. We propose a game theory-based approach to approximate the mixed pixels to lower approximations of a class. For pixel approximation, we have applied the spatial information of neighbouring pixels. Experiment for the implementation of the proposed approach is carried on six Landsat 5 Thematic Mapper images with a different region of interest. These datasets vary in size and have different dominant classes. The impact of the proposed method on the quality of clusters is studied with the help of cluster quality parameters. The results of the percentage of approximation demonstrate the significant number of mixed pixels approximated to one of the class. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106365	10.1016/j.asoc.2020.106365													
J								Multi-objective meta-heuristic optimization in intelligent control: A survey on the controller tuning problem	APPLIED SOFT COMPUTING										Meta-heuristics; Multi-objective optimization; Controller tuning; Intelligent control	ORDER PID CONTROLLER; CONSTRAINT-HANDLING TECHNIQUES; GENETIC ALGORITHM; EVOLUTIONARY ALGORITHMS; DECISION-MAKING; DIFFERENTIAL EVOLUTION; CONTROL-SYSTEMS; NSGA-II; DESIGN; PERFORMANCE	Multi-objective optimization has been adopted in many engineering problems where a set of requirements must be met to generate successful applications. Among them, there are the tuning problems from control engineering, which are focused on the correct setting of the controller parameters to properly govern complex dynamic systems to satisfy desired behaviors such as high accuracy, efficient energy consumption, low cost, among others. These requirements are stated in a multi-objective optimization problem to find the most suitable controller parameters. Nevertheless, these parameters are tough to find because of the conflicting control performance requirements (i.e., a requirement cannot be met without harming the others). Hence, the use of techniques from computational intelligence and soft computing is necessary to solve multi-objective problems and handle the trade-offs among control performance objectives. Meta-heuristics have shown to obtain outstanding results when solving complex multi-objective problems at a reasonable computational cost. In this survey, the literature related to the use of multi-objective meta-heuristics in intelligent control focused on the controller tuning problem is reviewed and discussed. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106342	10.1016/j.asoc.2020.106342													
J								Parameter-free fuzzy histogram equalisation with illumination preserving characteristics dedicated for contrast enhancement of magnetic resonance images	APPLIED SOFT COMPUTING										Contrast enhancement; Fuzzy sets; Histogram equalisation; Magnetic resonance imaging; Pre-processing	SEGMENTATION; ERROR	Low-field MRI scanners do not offer sufficient image contrast. Hence, offline algorithms for improving image contrast are often needed. Even though modified versions of Histogram Equalisation (HE) are extensively used on panoramic images, they have serious limitations. Most of such modified algorithms have multiple operational parameters which need to be tuned manually. Parameter-free modifications lag in terms of illumination-preserving features. To address these issues, a novel formulation of Parameter-free Fuzzy Histogram Equalisation (PFHE) algorithm with good illuminationpreserving characteristics, dedicated for contrast enhancement of MRI is introduced in this paper. In PFHE, a Homogeneity Fuzzy Sub-set (HFS) and its fuzzy complement, termed as Texture Fuzzy Sub-set (TFS) are computed based on the fuzzy similarity of the pixels in the input image with their eight-connected neighbours. Following this, an approximate output is estimated by applying a transformation similar to the histogram equalisation on the Fuzzy Textural Histogram (FTH) derived from TFS. The final output is computed as a nonlinear combination of the approximate output and the input image. The fuzzy weighting vectors used in the nonlinear combination are derived from the HFS. Both Qualitative and quantitative evaluations reveal that the PFHE is superior to Bi-Histogram Equalisation (BHE), Weighted Threshold Histogram Equalisation (WTHE), Contrast Limited Adaptive Histogram Equalisation (CLAHE), Non-parametric Modified Histogram Equalisation (NMHE), Exposurebased Sub-Image Histogram Equalisation (ESIHE), Median-Mean Based Sub-Image-Clipped Histogram Equalisation (MMSICHE) and Dominant Orientation-based Texture Histogram Equalisation (DOTHE), in terms of ability to preserve diagnostically significant features in the MR image. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106364	10.1016/j.asoc.2020.106364													
J								Multi-scale channel importance sorting and spatial attention mechanism for retinal vessels segmentation	APPLIED SOFT COMPUTING										Multi-scale; Asymmetric cascade convolution; Channel independence; Spatial attention mechanism; Retinal vessels	BLOOD-VESSELS; SUPERVISED METHOD	Retinal Vessels segmentation is an important procedure for detecting and diagnosing a variety of pathological diseases. However, the inherent complex properties around the disc make it challenging to improve the segmenting accuracy of capillaries and the retinal blood vessels at the ends. In this paper, we proposed a multi-scale channel importance sorting and important spatial information positioning (MSCS) encoder-decoder for segmentation in Retinal Vessels. Firstly, the fully convolutional encoder-decoder is formed to implement a series of linear and non-linear transformation and achieve end-to-end segmentation tasks. Then, the channel importance sorting module is employed to suppress useless feature responses during the process of encoding and to identify effective channels, whose information is utilized to recognize capillaries and the retinal vessels at the ends. Finally, in the decoding stage, the spatial attention mechanism module is designed to extract the positioning information of multi-scale feature maps. The spatial information of retinal vessels is collected to better locate the position of the vessels. In addition, aiming at taking fully advantage of the network, the multi-scale asymmetric cascade convolution module is proposed to reduce the parameters of the model and increase the operation rate. Experimental results on DRIVE, STARE datasets indicate that the proposed method outperforms other state-of-the-art strategies. This system, as demonstrated, can greatly decrease false positive rate of the blood vessels at the ends and enhance the sharpness of retinal vessels. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106353	10.1016/j.asoc.2020.106353													
J								A novel RK4-Hopfield Neural Network for Power Flow Analysis of power system	APPLIED SOFT COMPUTING										Power Flow Analysis (PFA); Modified Hopfield Neural Network (MHNN); Newton Raphson (NR) method; Iwamoto; Euler; 4th order Runge-Kutta (RK4) method	LOAD-FLOW; COMPUTATION; TIME; CONVERGENCE; ALGORITHM; MODEL	This paper presents a novel Runge-Kutta (RK4) based modified hopfield neural network (MHNN) for solving a set of non-linear transcendental power flow equations of power system. The proffered method is a Lyapunov based energy function approach to minimize real and reactive power mismatches of the system. A set of non-linear differential equations derived from energy function, describing the dynamical behavior of HNN is framed for solving Power Flow equations. These dynamic equations of the network are solved by RK4 method to deduce the unknown variables of the system. The feasibility of proposed method is tested on 5-bus, IEEE 14-bus, 39-bus and 57-bus test system. The analytical equation describing the behavior of MHNN is coded in MATLAB software. The results obtained reveal that the suggested method gives accurate solution and reduces the computational complexity than conventional Newton Raphson (NR) method. The sensitivity analysis is also tested for change in R/X ratio of the system, initial conditions and loading of the system. The proposed method is robust for above specified changes and involves less computational effort. To prove the applicability and consistency of projected method, IEEE 118-bus system has been tested. The power flow solutions found through proffered method are compared with solutions obtained from numerical approaches in order to validate the proposed approach. Moreover, the stability of the system is studied in Lyapunov sense of notion which assures converged solution of proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106346	10.1016/j.asoc.2020.106346													
J								Food package suggestion system based on multi-objective optimization: A case study on a real-world restaurant	APPLIED SOFT COMPUTING										Multi-objective optimization; Real-time service; Mobile application; Data mining	ALGORITHM; ONTOLOGY; MOEA/D	Ordering dishes in a restaurant is a significant task, which determines not only the customers' dining experience, but also the restaurant's reputation. However, assisting customers in ordering a satisfying food package (FP), i.e., a combination of dishes, remains a challenge. First, local restaurants usually have very limited information about their customers, except the number of customers and their budget. Thus, suggesting FPs that satisfy their budget as well as surprise their palate is very difficult. Second, as a real-world function, FPs are required to be generated in real time while addressing several realistic issues such as dynamic dish inventories. In this study, we first extract knowledge from the history of orders of a restaurant, such as correlations among dishes, to formulate the FP suggestion as a multi-objective optimization problem. Thereafter, we propose a knowledge-based multi-objective evolutionary algorithm (k-MOEA) to tackle the problem and generate the suggested FPs. In addition, we develop an intelligent dish-ordering system (iOrdering), including several designed online and offline mechanisms to meet the real-time requirements of the FP suggestion services. Finally, the effectiveness of the k-MOEA is evaluated quantitatively by comparing it with three categories of baselines. Moreover, we have deployed the iOrdering system in a hot pot restaurant chain, and a real-world experiment demonstrates the advanced user experience of the devised system, including more than 77% acceptance rate of the suggested dishes and 66% saving of ordering time. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106369	10.1016/j.asoc.2020.106369													
J								A novel hybrid feature selection method based on dynamic feature importance	APPLIED SOFT COMPUTING										Feature selection; Dynamic feature importance; Hybrid method; Feature selection framework	MAXIMAL INFORMATION COEFFICIENT; MUTUAL INFORMATION; GENE SELECTION; CLASSIFICATION; OPTIMIZATION; PREDICTION; FRAMEWORK; RELEVANCE; FILTER	Feature selection aims to eliminate unimportant and redundant features or to select effective and interacting features. It is a challenging task to accurately measure the relationships of candidate features, the selected features and categories in the selection process, especially for high-dimensional and small-sample-size data. To this end, a new measure named Dynamic Feature Importance (DFI) is proposed, as well as its corresponding feature selection algorithm named Dynamic Feature Importance based Feature Selection (DFIFS). In order to obtain higher classification accuracy with smaller number of features, a newly Modified-Dynamic Feature Importance based Feature Selection (M-DFIFS) algorithm is developed by combining DFIFS with classical filters. Based on experiments with 14 public high-dimensional datasets, the lately M-DFIFS algorithm shows significantly better performance than five typical filter algorithms in terms of their average accuracy with acceptable computing time. When using random forest as the classifier, M-DFIFS brings a great advantage in the number decrease of selected features. Hence the new feature selection framework "Filter + DFIFS'' is verified very effective to solve problems of obtaining high accuracy with a few features. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106337	10.1016/j.asoc.2020.106337													
J								Water-Energy-Food nexus evaluation with a social network group decision making approach based on hesitant fuzzy preference relations	APPLIED SOFT COMPUTING										Water-Energy-Food; Social network group decision making; Trust relationship; Self-confidence; Hesitant fuzzy preference relations	MISSING VALUES ESTIMATION; CONSENSUS MODEL; ADDITIVE CONSISTENCY; INFERENCE SYSTEM; CLASSIFICATION; AGGREGATION; FEEDBACK; ASSESSMENTS; ALGORITHMS	With the rapid increase and development of global population and economic, the Water-Energy-Food (WEF) nexus evaluation which is related to the sustainable development of human has become a hotspot. Whereas, the study of the WEF nexus evaluation from the perspectives of social network group decision making (SNGDM) is still a challenge. Hence, this paper aims to develop a trust-based SNGDM approach with hesitant fuzzy preference relations to the WEF nexus evaluation. In the proposed model, a new fuzzy adjacency matrix and trust score matrix based on expert's self-confidence are defined to imply experts' trust relationship and trust score, respectively. To improve the reliability of the final decision(s), an iterative algorithm is presented to improve the consistency of experts' evaluations. Subsequently, the individual evaluation can be aggregated into a group one by using the trust score induced ordered weighted averaging operator while the trust scores of experts are the induced factors. Additionally, an algorithm is utilized to achieve a high level consensus in SNGDM of the WEF nexus evaluation. Finally, some comparison analyses and discussions show the feasibility and validity of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106363	10.1016/j.asoc.2020.106363													
J								Data-driven prognosis method using hybrid deep recurrent neural network	APPLIED SOFT COMPUTING										Long short-term memory; Prognostics; Recurrent neural network; Remaining useful life prediction	REMAINING USEFUL LIFE; MAINTENANCE; DIAGNOSIS; PREDICTION	Prognostics and health management (PHM) has attracted increasing attention in modern manufacturing systems to achieve accurate predictive maintenance that reduces production downtime and enhances system safety. Remaining useful life (RUL) prediction plays a crucial role in PHM by providing direct evidence for a cost-effective maintenance decision. With the advances in sensing and communication technologies, data-driven approaches have achieved remarkable progress in machine prognostics. This paper develops a novel data-driven approach to precisely estimate the remaining useful life of machines using a hybrid deep recurrent neural network (RNN). The long short-term memory (LSTM) layers and classical neural networks are combined in the deep structure to capture the temporal information from the sequential data. The sequential sensory data from multiple sensors data can be fused and directly used as input of the model. The extraction of handcrafted features that relies heavily on prior knowledge and domain expertise as required by traditional approaches is avoided. The dropout technique and decaying learning rate are adopted in the training process of the hybrid deep RNN structure to increase the learning efficiency. A comprehensive experimental study on a widely used prognosis dataset is carried out to show the outstanding effectiveness and superior performance of the proposed approach in RUL prediction. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106351	10.1016/j.asoc.2020.106351													
J								A probabilistic linguistic-based deviation method for multi-expert qualitative decision making with aspirations	APPLIED SOFT COMPUTING										Probabilistic linguistic term set; Multi-expert multi-criteria decision making; Distance measure; Aspiration levels; Deviation model	TERM SETS; PROGRAMMING METHOD; PREFERENCE; MODEL; PERFORMANCE; PROMETHEE	Probabilistic linguistic term set (PLTS) is a popular tool for modeling complex linguistic perceptions of decision-makers (DMs) and has gained successful applications in the field of multi-expert multicriteria decision making (MEMCDM). In many probabilistic linguistic decision-making situations DMs are usually aspiration oriented in which the utilities of DMs do not depend on the absolute level of criteria values, but on the degree to which the criteria values match their aspirations levels. However, the aspirations of DMs are not considered in existing probabilistic linguistic decision making methods. One contribution of this paper is to introduce five probabilistic linguistic-based aspiration utility functions to take DMs' aspirations into account. These functions can well describe the utility variation of DMs under different preference structures with the aspiration levels. Afterwards, the probabilistic linguistic-based indicator is defined to take into account the criteria weights represented by PLTSs, which greatly facilitates DMs to provide the weights of criteria, comparing with the use of crisp numbers. As the second contribution, we build a probabilistic linguistic-based deviation model to identify the decision results in MEMCDM. This model can achieve the goal that the decision results for group opinions are consistent with that for the individual DM's opinions to the greatest extent. On the other hand, we also show with some counterexamples that the existing probabilistic linguistic distance measures are unreasonable. We present an improving distance measure for PLTSs and show its desirable properties. The biggest advantage of the developed distance measure is that it not only considers the deviation between the proportion information of linguistic terms but also takes into account linguistic terms themselves in PLTS. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				AUG	2020	93								106362	10.1016/j.asoc.2020.106362													
J								Evolutionary computation for solving search-based data analytics problems	ARTIFICIAL INTELLIGENCE REVIEW										Unified swarm intelligence; Data analytics; Evolutionary algorithms; Search-based data analytics; Swarm intelligence	PARTICLE SWARM OPTIMIZER; MULTIOBJECTIVE OPTIMIZATION; INTELLIGENCE; BIOINFORMATICS; ALGORITHM; CLASSIFICATION; SELECTION; METAHEURISTICS; OPPORTUNITIES; SEGMENTATION	Automatic extracting of knowledge from massive data samples, i.e., big data analytics (BDA), has emerged as a vital task in almost all scientific research fields. The BDA problems are rather difficult to solve due to their large-scale, high-dimensional, and dynamic properties, while the problems with small data are usually hard to handle due to insufficient data samples and incomplete information. Such difficulties lead to the search-based data analytics problem, where a data analysis task is modeled as a complex, dynamic, and computationally expensive optimization problem and then solved by using an iterative algorithm. In this paper, we intend to present an extensive and in-depth discussion on the utilizing of evolutionary computation (EC) based optimization methods [including evolutionary algorithms (EAs) and swarm intelligence (SI)] for solving search-based data analysis problems. Then, as an example for illustration, we provide a comprehensive review of the applications of state-of-the-art EC methods for different types of data mining problems in bioinformatics. Here, the detailed analysis and discussion are conducted on three types of data samples, which include sequences data, network data, and image data. Finally, we survey the challenges faced by EC methods and the trend for future directions. Based on the applications of EC methods for search-based data analysis problems involving inexact and uncertain information, the insights of data analytics are able to understand better, and more efficient algorithms could be designed to solve real-world complex BDA problems.																	0269-2821	1573-7462															10.1007/s10462-020-09882-x		AUG 2020											
J								Major advancements in kernel function approximation	ARTIFICIAL INTELLIGENCE REVIEW										Kernel; Approximation; Explicit feature maps; Kernel function; Theoretical guarantees; Random Fourier features; Classification	RANDOM FEATURES; MATRIX; FREQUENT	Kernel based methods have become popular in a wide variety of machine learning tasks. They rely on the computation of kernel functions, which implicitly transform the data in its input space to data in a very high dimensional space. Efficient application of these functions have been subject to study in the last 10 years. The main focus was on improving the scalability of kernel based methods. In this regard, kernel function approximation using explicit feature maps have emerged as a substitute for traditional kernel based methods. Over the years, various advancements from the theoretical perspective have been made to explicit kernel maps, especially to the method of random Fourier features (RFF), which is the main focus of our work. In this work, the major developments in the theory of kernel function approximation are reviewed in a systematic manner and the practical applications are discussed. Furthermore, we identify the shortcomings of the current research, and discuss possible avenues for future work.																	0269-2821	1573-7462															10.1007/s10462-020-09880-z		AUG 2020											
J								An interval fuzzy number-based fuzzy collaborative forecasting approach for DRAM yield forecasting	COMPLEX & INTELLIGENT SYSTEMS										Fuzzy collaborative forecasting; Interval fuzzy number; Mixed binary nonlinear programming	COLONY OPTIMIZATION ALGORITHM; INTELLIGENCE APPROACH; INFERENCE SYSTEM; UNIT COST; TIME; PERFORMANCE; MODEL	Most existing fuzzy collaborative forecasting (FCF) methods adopt type-1 fuzzy numbers to represent fuzzy forecasts. FCF methods based on interval-valued fuzzy numbers (IFNs) are not widely used. However, the inner and outer sections of an IFN-based fuzzy forecast provide meaning information that serves different managerial purposes, which is a desirable feature for a FCF method. This study proposed an IFN-based FCF approach. Unlike existing IFN-based fuzzy association rules or fuzzy inference systems, the IFN-based FCF approach ensures that all actual values fall within the corresponding fuzzy forecasts. In addition, the IFN-based FCF approach optimizes the forecasting precision and accuracy with the outer and inner sections of the aggregation result, respectively. Based on the experimental results, the proposed FCF-II approach surpassed existing methods in forecasting the yield of a dynamic random access memory product.																	2199-4536	2198-6053															10.1007/s40747-020-00179-8		AUG 2020											
J								Recognition and location of typical automotive parts based on the RGB-D camera	COMPLEX & INTELLIGENT SYSTEMS										RGB-D camera; Recognition and location; SIFT algorithm; Automatic assembly line		Aiming at the problem that the accuracy of multi-part automatic assembly line sorting is not high, a set of machine vision-based recognition and positioning system is designed with KINECT as the RGB-D camera. The internal and external parameters of the RGB-D camera were calibrated using MATLAB; taking the automobile tire as the target part, because it is better for the system and more accurate, the feature invariant feature transformation (SIFT) algorithm is used to extract and match the feature points of the target part. The depth-based image obtains the spatial position parameters of the target part, thereby calculating the three-dimensional coordinates of the target part, and realizes the recognition and positioning functions of the system. The experimental results show that the visual positioning effectiveness is 96% in the unstructured indoor environment, and the system has good robustness and real-time performance.																	2199-4536	2198-6053															10.1007/s40747-020-00182-z		AUG 2020											
J								Serial and parallel memetic algorithms for the bounded diameter minimum spanning tree problem	EXPERT SYSTEMS										bounded diameter; memetic; meta-heuristic; minimum spanning tree; NP-hard	OPTIMIZATION; HEURISTICS	Given a connected, weighted, undirected graphG= (V,E) and an integerD >= 2, the bounded diameter minimum spanning tree (BDMST) problem seeks a spanning tree of minimum cost, whose diameter is no greater thanD. The problem is known to be NP-hard, and finds application in various domains such as information retrieval, wireless sensor networks and distributed mutual exclusion. This article presents a two-phase memetic algorithm for the BDMST problem that combines a specialized recombination operator (proposed in this work) with good heuristics in order to more effectively direct the exploration of the search space into regions containing better solutions. A parallel meta-heuristic is also proposed and shown to obtain very good speedups - super-linear, in several cases - vis-a-vis the serial memetic algorithm. To the best of the authors' knowledge, this is the first parallel meta-heuristic proposed for the BDMST problem, and potentially paves the way for handling much larger problems than reported in the literature. Some observations and theorems are presented in order to provide the underlying framework for the proposed algorithms. Further, the proposed memetic algorithm and parallel meta-heuristic are shown, in the course of several computational experiments, to in general obtain superior solution quality with much lesser computational effort in comparison to the best known meta-heuristics in the literature over a wide range of benchmark instances.																	0266-4720	1468-0394															10.1111/exsy.12610		AUG 2020											
J								Kinematic Analysis of an Under-actuated, Closed-loop Front-end Assembly of a Dragline Manipulator	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Dragline mining manipulator; underactuated closed-loop mechanism; generalized speeds; Baumgarte's stabilization technique (BST); feedforward displacement		Dragline excavators are closed-loop mining manipulators that operate using a rigid multilink framework and rope and rigging system, which constitute its front-end assembly. The arrangements of dragline front-end assembly provide the necessary motion of the dragline bucket within its operating radius. The assembly resembles a five-link closed kinematic chain that has two independent generalized coordinates of drag and hoist ropes and one dependent generalized coordinate of dump rope. Previous models failed to represent the actual closed loop of dragline front-end assembly, nor did they describe the maneuverability of dragline ropes under imposed geometric constraints. Therefore, a three degrees of freedom kinematic model of the dragline front-end is developed using the concept of generalized speeds. It contains all relevant configuration and kinematic constraint conditions to perform complete digging and swinging cycles. The model also uses three inputs of hoist and drag ropes linear and a rotational displacement of swinging along their trajectories. The inverse kinematics is resolved using a feedforward displacement algorithm coupled with the Newton-Raphson method to accurately estimate the trajectories of the ropes. The trajectories are solved only during the digging phase and the singularity was eliminated using Baumgarte's stabilization technique (BST), with appropriate inequality constraint equations. It is shown that the feedforward displacement algorithm can produce accurate trajectories without the need to manually solve the inverse kinematics from the geometry. The research findings are well in agreement with the dragline real operational limits and they contribute to the efficiency and the reduction in machine downtime due to better control strategies of the dragline cycles.																	1476-8186	1751-8520				AUG	2020	17	4					527	538		10.1007/s11633-019-1217-4													
J								Automatic "Ground Truth" Annotation and Industrial Workpiece Dataset Generation for Deep Learning	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Deep learning; dataset generation; automatic annotation; neural networks; industrial workpiece dataset	DATABASE; OBJECT	In industry, it is becoming common to detect and recognize industrial workpieces using deep learning methods. In this field, the lack of datasets is a big problem, and collecting and annotating datasets in this field is very labor intensive. The researchers need to perform dataset annotation if a dataset is generated by themselves. It is also one of the restrictive factors that the current method based on deep learning cannot expand well. At present, there are very few workpiece datasets for industrial fields, and the existing datasets are generated from ideal workpiece computer aided design (CAD) models, for which few actual workpiece images were collected and utilized. We propose an automatic industrial workpiece dataset generation method and an automatic ground truth annotation method. Included in our methods are three algorithms that we proposed: a point cloud based spatial plane segmentation algorithm to segment the workpieces in the real scene and to obtain the annotation information of the workpieces in the images captured in the real scene; a random multiple workpiece generation algorithm to generate abundant composition datasets with random rotation workpiece angles and positions; and a tangent vector based contour tracking and completion algorithm to get improved contour images. With our procedures, annotation information can be obtained using the algorithms proposed in this paper. Upon completion of the annotation process, a json format file is generated. Faster R-CNN (Faster R-convolutional neural network), SSD (single shot multibox detector) and YOLO (you only look once: unified, real-time object detection) are trained using the datasets proposed in this paper. The experimental results show the effectiveness and integrity of this dataset generation and annotation method.																	1476-8186	1751-8520				AUG	2020	17	4					539	550		10.1007/s11633-020-1221-8													
J								Automatic Classification of Cardiac Arrhythmias Based on Hybrid Features and Decision Tree Algorithm	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Electrocardiogram (ECG); cardiac arrhythmias; empirical mode decomposition (EMD); variational mode decomposition (VMD); hybrid features; decision tree classifier	FEATURE-EXTRACTION; FEATURE-SELECTION; ECG; SYSTEM; TIME	Accurate classification of cardiac arrhythmias is a crucial task because of the non-stationary nature of electrocardiogram (ECG) signals. In a life-threatening situation, an automated system is necessary for early detection of beat abnormalities in order to reduce the mortality rate. In this paper, we propose an automatic classification system of ECG beats based on the multi-domain features derived from the ECG signals. The experimental study was evaluated on ECG signals obtained from the MIT-BIH Arrhythmia Database. The feature set comprises eight empirical mode decomposition (EMD) based features, three features from variational mode decomposition (VMD) and four features from RR intervals. In total, 15 features are ranked according to a ranker search approach and then used as input to the support vector machine (SVM) and C4.5 decision tree classifiers for classifying six types of arrhythmia beats. The proposed method achieved best result in C4.5 decision tree classier with an accuracy of 98.89% compared to cubic-SVM classifier which achieved an accuracy of 95.35% only. Besides accuracy measures, all other parameters such as sensitivity (Se), specificity (Sp) and precision rates of 95.68%, 99.28% and 95.8% was achieved better in C4.5 classifier. Also the computational time of 0.65 s with an error rate of 0.11 was achieved which is very less compared to SVM. The multi-domain based features with decision tree classifier obtained the best results in classifying cardiac arrhythmias hence the system could be used efficiently in clinical practices.																	1476-8186	1751-8520				AUG	2020	17	4					551	561		10.1007/s11633-019-1219-2													
J								Remote Sensing Image Registration Based on Improved KAZE and BRIEF Descriptor	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Remote sensing image; image registration; composite nonlinear diffusion filter; binary code string; multi-scale pyramid space	EFFICIENT; SIFT; ALGORITHM; LOCALITY	Remote sensing image registration is still a challenging task owing to the significant influence of nonlinear differences between remote sensing images. To solve this problem, this paper proposes a novel approach with regard to feature-based remote sensing image registration. There are two key contributions: 1) we bring forward an improved strategy of composite nonlinear diffusion filtering according to the scale factors in multi-scale space and 2) we design a gradually decreasing resolution of multi-scale pyramid space. And a binary code string is served as feature descriptors to improve matching efficiency. Extensive experiments of different categories of remote image datasets on feature extraction and feature registration are performed. The experimental results demonstrate the superiority of our proposed scheme compared with other classical algorithms in terms of correct matching ratio, accuracy and computation efficiency.																	1476-8186	1751-8520				AUG	2020	17	4					588	598		10.1007/s11633-019-1218-3													
J								Optimal design of fuzzy-AGC based on PSO & RCGA to improve dynamic stability of interconnected multi area power systems	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Power system dynamic stability; fuzzy logic automatic generation control (FLAGC); particle swarm optimization (PSO); real coded genetic algorithm (RCGA); simultaneous coordination scheme	LOAD-FREQUENCY CONTROL; ROBUST POLE-PLACEMENT; CONTROLLER-DESIGN; COORDINATED DESIGN; OPTIMIZATION; OSCILLATIONS; LOCATION	Quickly getting back the synchronism of a disturbed interconnected multi area power system due to variations in loading condition is recognized as prominent issue related to automatic generation control (AGC). In this regard, AGC system based on fuzzy logic, i.e., so-called FLAGC can introduce an effectual performance to suppress the dynamic oscillations of tie-line power exchanges and frequency in multi-area interconnected power system. Apart from that, simultaneous coordination scheme based on particle swarm optimization (PSO) along with real coded genetic algorithm (RCGA) is suggested to coordinate FLAGCs of the all areas. To clarify the high efficiency of aforementioned strategy, two different interconnected multi area power systems, i.e., three-area hydro-thermal power system and five-area thermal power system have been taken into account for relevant studies. The potency of this strategy has been thoroughly dealt with by considering the step load perturbation (SLP) in both the under study power systems. To sum up, the simulation results have plainly revealed dynamic performance of FLAGC as compared with conventional AGC (CAGC) in each power system in order to damp out the power system oscillations.																	1476-8186	1751-8520				AUG	2020	17	4					599	609		10.1007/s11633-017-1064-0													
J								Composite control of nonlinear singularly perturbed systems via approximate feedback linearization	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Approximate feedback linearization (AFL); composite control; nonlinear singularly perturbed system; order reduction; decomposition	MAXIMAL STABILITY BOUNDS; PERTURBATIONS	This article is devoted to the problem of composite control design for continuous nonlinear singularly perturbed (SP) system using approximate feedback linearization (AFL) method. The essence of AFL method lies in the feedback linearization only of a certain part of the original nonlinear system. According to AFL approach, we suggest to solve feedback linearization problems for continuous nonlinear SP system by reducing it to two feedback linearization problems for slow and fast subsystems separately. The resulting AFL control is constructed in the form of asymptotic composition (composite control). Standard procedure for the composite control design consists of the following steps: 1) system decomposition, 2) solution of control problem for fast subsystem, 3) solution of control problem for slow subsystem, 4) construction of the resulting control in the form of the composition of slow and fast controls. The main difficulty during system decomposition is associated with dynamics separation condition for nonlinear SP system. To overcome this, we propose to change the sequence of the design procedure: 1) solving the control problem for fast state variables part, 2) system decomposition, 3) solving the control problem for slow state variables part, 4) construction of the resulting composite control. By this way, fast feedback linearizing control is chosen so that the dynamics separation condition would be met and the fast subsystem would be stabilizable. The application of the proposed approach is illustrated through several examples.																	1476-8186	1751-8520				AUG	2020	17	4					610	620		10.1007/s11633-017-1076-9													
J								View Transfer on Human Skeleton Pose: Automatically Disentangle the View-Variant and View-Invariant Information for Pose Representation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION										Representation learning; Human skeleton pose; View transfer; Unsupervised action recognition	ACTION RECOGNITION	Learning a good pose representation is significant for many applications, such as human pose estimation and action recognition. However, the representations learned by most approaches are not intrinsic and their transferability in different datasets and different tasks is limited. In this paper, we introduce a method to learn a versatile representation, which is capable of recovering unseen corrupted skeletons, being applied to the human action recognition, and transferring pose from one view to another view without knowing the relationships of cameras. To this end, a sequential bidirectional recursive network (SeBiReNet) is proposed for modeling kinematic dependency between skeleton joints. Utilizing the SeBiReNet as the core module, a denoising autoencoder is designed to learn intrinsic pose features through the task of recovering corrupted skeletons. Instead of only extracting the view-invariant feature as many other methods, we disentangle the view-invariant feature from the view-variant feature in the latent space and use them together as a representation of the human pose. For a better feature disentanglement, an adversarial augmentation strategy is proposed and applied to the denoising autoencoder. Disentanglement of view-variant and view-invariant features enables us to realize view transfer on 3D poses. Extensive experiments on different datasets and different tasks verify the effectiveness and versatility of the learned representation.																	0920-5691	1573-1405															10.1007/s11263-020-01354-7		AUG 2020											
J								Automated breast cancer detection using hybrid extreme learning machine classifier	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mammogram; Classification; ELM; Fruit fly; Optimization; Accuracy; CAD	NEURAL-NETWORK; MAMMOGRAMS; MASSES	Breast cancer has been identified as one of the major diseases that have led to the death of women in recent decades. Mammograms are extensively used by physicians to diagnose breast cancer. The selection of appropriate image enhancement, segmentation, feature extraction, feature selection and prediction algorithm plays an essential role in precise cancer diagnosis on mammograms and remains as a major task in the research field. Classification methods predict the class label for unlabeled dataset based on its proximity to the learnt pattern. The selected features obtained after feature selection are classified using an extreme learning machines (ELM) to three classes with the classes being normal, benign and malignant. Low generalisation performance is the problem which happens due to the ill-conditioned output matrix of the hidden layer of the classifier. The optimisation algorithms would resolve these issues because of their global searching ability. This paper proposes ELM with Fruitfly Optimisation Algorithm (ELM-FOA) to tune the input weight to obtain optimum output at the ELM's hidden node to obtain the solution analytically. The testing sensitivity and precision of ELM-FOA are 97.5% and 100% respectively. The developed method can detect the calcifications and tumours with 99.04% accuracy. The optimal selection of preprocessing and segmentation algorithms, features from multiple feature filters and the efficient classifier algorithm meliorate the performance of the approach.																	1868-5137	1868-5145															10.1007/s12652-020-02359-3		AUG 2020											
J								Architectural framework and simulation of quantum key optimization techniques in healthcare networks for data security	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Healthcare networks; Key generation; Three node network; Ring node network and optimal rate key; Quantum key distribution	MANAGEMENT; MULTICAST; EFFICIENT	Healthcare systems are heterogeneous by nature, where each device is running on different architectures, platform and operating system. This heterogeneity affects the communication performance in-terms of delay and security threats. An optimized quantum key management technique has been proposed for the purpose of managing the keys with little overhead, which enhances the health care information security by reducing the threats. And also the communication with the key authority is simulated with the quantum channel. The generated key is distributed via a dedicated quantum channel in terms of quantum bits which decreases the eavesdropping rate, transmission error, and leakage to maximum extent which improves the security further. Healthcare User group and content server communicate with the key server via a quantum channel sending them photons. Then they discuss results using a public channel. After getting an encryption key from the key server via the quantum channel, the content server can encrypt their healthcare content and send them by any public channel to the healthcare user groups. This proposed research work investigates group secret key generation problems for different types of Healthcare networks and also addresses the quantum key distribution which enhances the key security in healthcare networks. The analysis shows that the two algorithms yield optimal group key rates in healthcare networks. Numerical results are also provided to validate the performance of the proposed key generation, optimization and quantum distribution. The key generation, optimization and healthcare content encryption and decryption using those keys enhances the security of patient data and quantum simulation shows that about 90% of eavesdropping rate is reduced in healthcare network.																	1868-5137	1868-5145															10.1007/s12652-020-02393-1		AUG 2020											
J								Image compression using optimized wavelet filter derived from grey wolf algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Grey wolf optimization (GWO); Discrete wavelet transform (DWT); Evolved Wavelet filters	GENETIC ALGORITHM	This investigation practices a grey wolf optimization algorithm to optimize the filter coefficients of the two-dimensional wavelet transform to increase the quality of the decompressed image with lossy compression. The reason for the decrease in image quality in wavelet-based compression is nothing more than a quantization error. Our approach reduces the effect of this quantization error by using an adaptive filtering technique with optimization of the wavelet filter coefficients. Experimental results showed that this optimized filter outperforms existing wavelet filters.																	1868-5137	1868-5145															10.1007/s12652-020-02290-7		AUG 2020											
J								An optimized distributed secure routing protocol using dynamic rate aware classified key for improving network security in wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WMN; Secure routing; Classified key distributional scheme; Rate aware routing; SRM; QoS		To transmit information over the industrial network today, you need data in a secure way to make it a high security root. Routing security issues in the wireless web network have been well studied. The problem of security in routing in wireless mesh networks (WMN) has been well studied. There exist numerous techniques to resolve this issue but differ and suffer to achieve higher security performance in WMN. To resolve this issue, a dynamic rate aware classified key distributional secure routing (DRCKDS) is proposed. In this approach, the sensor nodes maintain various factors related to the neighbor like energy, transmission involvement, rate of success and so on. According to this, available routes are identified to reach the destination from the source. For each route identified, the method computes the secure route measure (SRM). According to the SRM value, an optimal route is selected for the transmission. Similarly, the method generates different secret keys and distributes them through the transmission route selected. The same key has been used to encrypt the data and forward the packet through the route selected. The method improves the security performance and improves the quality of service of WMN.																	1868-5137	1868-5145															10.1007/s12652-020-02392-2		AUG 2020											
J								Study on various control strategies of plate type heat exchanger for non-Newtonian fluids	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Plate type heat exchanger; Model Predictive Controller; PID controller	MODEL-PREDICTIVE CONTROL; DESIGN	Heat exchangers are expedients used to transfer of heat energy from one medium to other while preventing the two from mixing. Heat exchangers are hired in a selection of domestic, commercial, and industrial applications, such as air conditioning, power production, refrigeration, process industry, and manufacturing industry. A larger variation in the temperature affects property of non-Newtonian fluids, thus affects the uniform heat transfer process. This paper is primarily to deal the response analysis of different controllers that is regulating the outlet temperature of fluid inside the PHE is proposed. The cold fluid flow is highly influenced on the hot water temperature, the research work mainly focuses on controlling cold fluid flow. In order to attain the precise model of the system, a mathematical model by first principle methods and process history based methods were used. A PI, PID and Model Predictive Controller (MPC) are known for its better control action of the system. The controller's performance analysed such as PI, PID and MPC controller to adjust the temperature of hot fluid outlet to a required setpoint. Performance criteria analysis value for various controllers are PI, PID and MPC. It is understood that MPC controller results lower value of performance measures compared to other controllers, this implies that it could track the setpoint by rejecting the external disturbances with less settling time and moderate oscillation. The transient performance and the error criteria of the controllers are investigated and the best controller to regulate the temperature is found out. It is found out that the MPC controller outperforms PI and PID controller from the results of simulation.																	1868-5137	1868-5145															10.1007/s12652-020-02401-4		AUG 2020											
J								Protecting user profile based on attribute-based encryption using multilevel access security by restricting unauthorization in the cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data clouds; Cloud security; Industry security standard; Privacy preservation; ABE; User profiles; MATW	SERVICE	Data security in centralized storage needs advancement in privacy standards because of all of the cloud. The data security in the cloud has been well provided by security industries like cloud network security, data center security, distributed security and soon, also their exist numerous techniques to preserve the privacy of cloud users. The earlier methods enforce user privacy by restricting the malicious access from various users. Data authentication is provable access to keep privacy among other standards. However, the privacy of cloud users has been breached on several occasions. To improve cloud security and enforce efficient privacy preservation, a multi-level micro access restriction algorithm has been presented in this paper. The cloud data has been indexed in multiple levels, the data present in each level has been restricted using the profile and set if encryption standards. The user request has been evaluated for its trusted access according to the access grant present in profile data. Similarly, the cloud data has been encrypted with the user key and the key belongs to the data owner. The method estimates micro access trust weight (MATW), which has been used to restrict the user from malicious access and to preserve user privacy. The method improves the performance of cloud security and introduces higher privacy preservation accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02400-5		AUG 2020											
J								ICE-Based Refinement Type Discovery for Higher-Order Functional Programs	JOURNAL OF AUTOMATED REASONING										Higher-order program verification; Machine learning; Formal verification; Refinement types	VERIFICATION; INVARIANTS; INFERENCE	We propose a method for automatically finding refinement types of higher-order function programs. Our method is an extension of theIceframework of Garg et al. for finding invariants. In addition to the usual positive and negative samples in machine learning, theirIceframework usesimplication constraints, which consist of pairs (x, y) such that ifxsatisfies an invariant, so doesy. From these constraints,Iceinfersinductiveinvariants effectively. We observe that the implication constraints in the originalIceframework are not suitable for finding invariants of recursive functions with multiple function calls. We thus generalize the implication constraints to those of the form({x(1),...,x(k)},y), which eans that if all of x(1),...x(k) satisfy an invariant, so doesy. We extend their algorithms for inferring likely invariants from samples, verifying the inferred invariants, and generating new samples. We have implemented our method and confirmed its effectiveness through experiments.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1393	1418		10.1007/s10817-020-09571-y		AUG 2020											
J								Real-time chinese traffic warning signs recognition based on cascade and CNN	JOURNAL OF REAL-TIME IMAGE PROCESSING										TSR; Object detection; Cascade; CNN		Warning signs are of great significance to traffic safety. In this paper, a real-time recognition method for Chinese Traffic Warning Signs (CTWS) is proposed. CTWS are all triangles with yellow background, black border and black pattern. Their similarity is conducive to the localization task of object detection but adverse to the classification task of object detection. After analyzing the characteristics of these signs, real-time recognition for CTWS is carried out by employing Cascade classifier and Convolutional Neural Network (CNN). A Cascade classifier with 9 layers is trained with local binary patterns to locate the CTWS in frames. And a 10-layer CNN model is built to determine the specific category of the signs located by the Cascade classifier. We evaluate the method on CCTSDB-based dataset and GTSDB, and experiments show that the proposed method can perform accurate recognition at an average speed of 81.79fps without GPU. Since the proposed method only needs to call CNN that requires vast computing power in a small number of frames containing CTWS while performing real-time recognition, it can effectively save the valuable on-board computing resources compared with other object detection algorithms that is purely based on CNN such as YOLOv3, YOLOv3-tiny and Faster R-CNN.																	1861-8200	1861-8219															10.1007/s11554-020-01003-9		AUG 2020											
J								Accountability and Control Over Autonomous Weapon Systems: A Framework for Comprehensive Human Oversight	MINDS AND MACHINES										Autonomous Weapon Systems; Responsibility; Accountability; Accountability gap; Meaningful human control; Human oversight; Comprehensive human oversight framework	RESPONSIBILITY	Accountability and responsibility are key concepts in the academic and societal debate on Autonomous Weapon Systems, but these notions are often used as high-level overarching constructs and are not operationalised to be useful in practice. "Meaningful Human Control" is often mentioned as a requirement for the deployment of Autonomous Weapon Systems, but a common definition of what this notion means in practice, and a clear understanding of its relation with responsibility and accountability is also lacking. In this paper, we present a definition of these concepts and describe the relations between accountability, responsibility, control and oversight in order to show how these notions are distinct but also connected. We focus on accountability as a particular form of responsibility-the obligation to explain one's action to a forum-and we present three ways in which the introduction of Autonomous Weapon Systems may create "accountability gaps". We propose a Framework for Comprehensive Human Oversight based on an engineering, socio-technical and governance perspective on control. Our main claim is that combining the control mechanisms at technical, socio-technical and governance levels will lead to comprehensive human oversight over Autonomous Weapon Systems which may ensure solid controllability and accountability for the behaviour of Autonomous Weapon Systems. Finally, we give an overview of the military control instruments that are currently used in the Netherlands and show the applicability of the comprehensive human oversight Framework to Autonomous Weapon Systems. Our analysis reveals two main gaps in the current control mechanisms as applied to Autonomous Weapon Systems. We have identified three first options as future work for the design of a control mechanism, one in the technological layer, one in the socio-technical layer and one the governance layer, in order to achieve comprehensive human oversight and ensure accountability over Autonomous Weapon Systems.																	0924-6495	1572-8641															10.1007/s11023-020-09532-9		AUG 2020											
J								Subspace data stream clustering with global and local weighting models	NEURAL COMPUTING & APPLICATIONS										Soft subspace clustering; Growing neural gas; Data stream; Entropy weighting system	GROWING NEURAL GAS; ALGORITHM	Subspace clustering discovers clusters embedded in multiple, overlapping subspaces of high dimensional data. It has been successfully applied in many domains. Data streams are ordered and potentially infinite sequences of data points created by a typically non-stationary data generating process. Clustering this type of data requires some restrictions in time and memory. In this paper, we propose the S2G-Stream algorithm based on growing neural gas and soft subspace clustering. We introduce two types of entropy weighting for both features and blocks, and also two weighting models (local and global). Experiments on public datasets demonstrated the ability of S2G-Stream to detect relevant features and blocks and to provide the best partitioning of the data.																	0941-0643	1433-3058															10.1007/s00521-020-05184-z		AUG 2020											
J								Classification of olive leaf diseases using deep convolutional neural networks	NEURAL COMPUTING & APPLICATIONS										Deep learning; Convolutional neural networks; Olive plant disease; Transfer learning; Data augmentation	AGRICULTURE; PLANTS; YIELD; SPOT	In recent years, there have been significant achievements in object classification with various techniques using several deep learning architectures. These architectures are now also used for classification and detection of many plant diseases. Olives are important plant species which are grown in certain regions of the world. The disease types that affect the olive plants vary on the region where it is grown. This study presents a data set consisting of 3400 olive leaves samples which also includes healthy leaves so as to detect Aculus olearius and Olive peacock spot diseases, which are common olive plant diseases in Turkey. This experimental study used transfer learning methods on VGG16 and VGG19 architectures, as well as on our proposed CNN architecture. Effects of data augmentation on performance were one aim of this research. In the experimental studies which applied data augmentation the highest success value in trained models was 95%, whereas in the experiments without data augmentation the highest value was 88%. Another subject of this research is the Adam, AdaGrad, Stochastic gradient descent and RMS Prop optimization algorithms' effect on the network's performance. As a result of the conducted experiments, Adam and SGD optimization algorithms were generally observed to generate superior results.																	0941-0643	1433-3058															10.1007/s00521-020-05235-5		AUG 2020											
J								Feature Selection Method Based on Differential Correlation Information Entropy	NEURAL PROCESSING LETTERS										Differential correlation information entropy; mRMR; Classification; Feature selection	MUTUAL INFORMATION; CLASSIFICATION	Feature selection is one of the major aspects of pattern classification systems. In previous studies, Ding and Peng recognized the importance of feature selection and proposed a minimum redundancy feature selection method to minimize redundant features for sequential selection in microarray gene expression data. However, since the minimum redundancy feature selection method is used mainly to measure the dependency between random variables of mutual information, the results cannot be optimal without consideration of global feature selection. Therefore, based on the framework of minimum redundancy-maximum correlation, this paper introduces entropy to measure global feature selection and proposes a new feature subset evaluation method, differential correlation information entropy. In our function, different bivariate correlation metrics are selected. Then, the feature selection is completed through sequence forward search. Two different classification models are used on eleven standard data sets of the UCI machine learning knowledge base to compare various comparison algorithms, such as mRMR, reliefF and feature selection method with joint maximal information entropy, with our method. The experimental results show that feature selection based on our proposed method is obviously superior to that of other models.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1339	1358		10.1007/s11063-020-10307-7		AUG 2020											
J								Exponential Synchronization of Complex-Valued Neural Networks Via Average Impulsive Interval Strategy	NEURAL PROCESSING LETTERS										Exponential synchronization; Impulsive effects; Time-varying delay; Distributed delays	DELAYED DYNAMICAL NETWORKS; TIME-VARYING DELAYS; MIXED DELAYS; STABILITY; DISCRETE; NODES	In this paper, the issue of the exponential synchronization for complex-valued neural networks with both discrete and distributed delays is investigated by applying impulsive control protocol. Based on the Lyapunov-Krasovskii function, average impulsive interval as well as the comparison principle, some simple verifiable sufficient criteria are established to guarantee the exponential synchronization between the master and the slave systems. Meanwhile, through the serious analysis of the networks systems, the exponential convergence rate can be specified. Additionally, a numerical example is finally given to illustrate the effectiveness of the proposed theoretical results.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1377	1394		10.1007/s11063-020-10309-5		AUG 2020											
J								Content-Based Bipartite User-Image Correlation for Image Recommendation	NEURAL PROCESSING LETTERS										Bipartite graph; Visual correlation; Personalized recommendation; Social multimedia network		The popularity of online social curation networks takes benefits from its convenience to retrieve, collect, sort and share multimedia contents among users. With increasing content and user intent gap, effective recommendation becomes highly desirable for its further development. In this paper, we propose a content-based bipartite graph for image recommendation in social curation networks. Bipartite graph employs given sparse user-image interactions to infer user-image correlation for recommendation. Beside given user-image interactions, the user interacted visual content also reveals valuable user preferences. Visual content is embedded into the bipartite graph to extend the correlation density and the recommendation scope simultaneously. Furthermore, the content similarity is employed for recommendation reranking to improve the visual quality of recommended images. Experimental results demonstrate that the proposed method enhances the recommendation ability of the bipartite graph effectively.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1445	1459		10.1007/s11063-020-10317-5		AUG 2020											
J								Canonical Correlation Analysis Based Hyper Basis Feedforward Neural Network Classification for Urban Sustainability	NEURAL PROCESSING LETTERS										Air pollution; Big data; Canonical correlation analysis; Gaussian activation function; Hyper basis feedforward neural network; Pollutants features; Urban sustainability		People give more importance concerning the overall quality of the modernized ecosystem. The pollution of air is one of the significant problems to be resolved as it restricted the ecological transformation of the modernized ecosystem. Therefore, it is fundamental to evaluate the implication of these ecological issues to enhance the urban ecosystem. This vital purpose of this research is to propose a canonical correlation analysis based hyper basis feedforward neural network classification (CCA-HBFNNC) model for evaluating sustainable urban environmental quality. The CCA-HBFNNC model initially acquires a large size of U.S. air pollution dataset as input. Then, a canonical correlative analysis based feature selection algorithm is applied in the CCA-HBFNNC model to select the key pollutant features, which bear fundamental implications to the modernize air pollution to maintain the level of urban sustainability. After the feature selection process, the CCA-HBFNNC model applies the HYPER BASIS FEEDFORWARD NEURAL NETWORK CLASSIFICATION (HBFNNC) algorithm in order to classify input air data based on chosen pollutants features. During the classification process, the HBFNNC algorithm used three critical layers namely hidden, output and input layers for efficiently categorizing each input data as higher or lower pollution level with higher accuracy. If the level of air pollution on the urban environment is higher, finally CCA-HBFNNC model significantly reduces the pollution level. In this way, the CCA-HBFNNC model attains improved urban sustainability levels when compared to sophisticated operation. An experimental evaluation of the CCA-HBFNNC model is determined in terms of CCA-HBFNNC model, time complexity and false-positive rate in consideration of the diversified number of air data retrieved from the big data sets. An investigational result shows that the proposed CCA-HBFNNC model can increases the sustainability level and minimizes the time complexity of urban development when contrasted with contemporary works.																	1370-4621	1573-773X															10.1007/s11063-020-10327-3		AUG 2020											
J								A fast graph-based data classification method with applications to 3D sensory data in the form of point clouds	PATTERN RECOGNITION LETTERS										Data classification 1; Graph-based setting; Optimization; Auction dynamics; Sensory data	REGULARIZATION; FRAMEWORK	Data classification, where the goal is to divide data into predefined classes, is a fundamental problem in machine learning with many applications, including the classification of 3D sensory data. In this paper, we present a data classification method which can be applied to both semi-supervised and unsupervised learning tasks. The algorithm is derived by unifying complementary region-based and edge-based approaches; a gradient flow of the optimization energy is performed using modified auction dynamics. In addition to being unconditionally stable and efficient, the method is equipped with several properties allowing it to perform accurately even with small labeled training sets, often with considerably fewer labeled training elements compared to competing methods; this is an important advantage due to the scarcity of labeled training data. Some of the properties are: the embedding of data into a weighted similarity graph, the in-depth construction of the weights using, e.g., geometric information, the use of a combination of region-based and edge-based techniques, the incorporation of class size information and integration of random fluctuations. The effectiveness of the method is demonstrated by experiments on classification of 3D point clouds; the algorithm classifies a point cloud of more than a million points in 1-2 min. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						1	7		10.1016/j.patrec.2020.06.005													
J								A new approach for classification skin lesion based on transfer learning, deep learning, and IoT system	PATTERN RECOGNITION LETTERS										Skin lesion; Transfer learning; CNN; Internet of things	MELANOMA	Melanoma skin cancer is one of the most common diseases in the world. It is essential to diagnose melanoma at an early stage. Visual inspection during the medical examination of skin lesions is not a simple task, as there is a similarity between lesions. Also, medical experience and disposition can result in inaccurate diagnoses. Technologies such as the Internet of Things (IoT) have helped to create effective health systems. Doctors can use them anywhere, with the guarantee that more people can be diagnosed without prejudice to subjective factors. Transfer Learning and Deep Learning are increasingly significant in the clinical diagnosis of different diseases. This work proposes the use of Transfer Learning and Deep Learning in an IoT system to assist doctors in the diagnosis of common skin lesions, typical nevi, and melanoma. This work uses Convolutional Neural Networks (CNNs) as resource extractors. The CNN models used were: Visual Geometry Group (VGG), Inception, Residual Networks (ResNet), Inception-ResNet, Extreme Inception (Xception), MobileNet, Dense Convolutional Network (DenseNet), and Neural Architecture Search Network (NASNet). For the classification of injuries, the Bayes, Support Vector Machines (SVM), Random Forest (RF), Perceptron Multilayer (MLP), and the K-Nearest Neighbors (KNN) classifiers are used. This study used two datasets: the first provided by the International Skin Imaging Collaboration (ISIC) at the International Biomedical Imaging Symposium (ISBI); the second is PH2. For ISBI-ISIC, this study examined lesions between nevi and melanomas. In PH2, this work analyzed the diagnosis based on lesions of common nevus, atypical nevi, and melanomas. The DenseNet201 extraction model, combined with the KNN classifier achieved an accuracy of 96.805% for the ISBI-ISIC dataset and 93.167% for the PH2. Thus, an approach focused on the IoT system is reliable and efficient for doctors who assist in the diagnosis of skin lesions. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						8	15		10.1016/j.patrec.2020.05.019													
J								Adaptive learning of minority class prior to minority oversampling	PATTERN RECOGNITION LETTERS										Class imbalance; Relative neighborhood graph; Minority set estimation; Oversampling	CLASSIFICATION; SMOTE	The minority oversampling techniques have substantiated their appropriateness and utility in the domain of class-imbalance learning. However, this does not affirm the true class of the synthetic minority points. In this work, Adaptive Learning of Minority Class prior to Minority Oversampling (ALMCMO), we work towards bridging this gap by estimating the minority set before oversampling the synthetic points. We estimate a varying and adaptive volume of minority space around the minority points. We aim to guarantee the class-memberships of the synthetic minority points by sampling them from the estimated minority spaces. In our empirical study, we have used six comparing methods, 23 datasets and two classifiers. The results indicate the certain superiority of the proposed method over the six competing schemes. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						16	24		10.1016/j.patrec.2020.05.020													
J								Two-stage multi-modal MR images fusion method based on Parametric Logarithmic Image Processing (PLIP) Model	PATTERN RECOGNITION LETTERS										MRI; HVS; PLIP	CONTOURLET TRANSFORM; WAVELET; FRAMEWORK	MRI is one of the most compliant technique that is used for the screening of Brain Tumor. MRI can be acquired in four available modalities which are MR-T1, MR-T2, MR-PD and MR-Gad; among these MR-T2 comprises of most of the detailed information of the tumorous tissues. However, the accuracy and reliability of the diagnosis may be affected due to lack of sufficient details in each modality (as different MRI modalities highlight different set of tissues). Therefore, MR Image(s) fusion is essential to obtain a more illustrative image containing the requisite complementary details of each modality. For this purpose, multi-modal fusion of MR-T2 with MR-T1, MR-PD and MR-Gad have been dealt in this work using the proposed fusion method. This paper presents a two-stage fusion method using Stationary Wavelet Transform (SWT) in combination with Parameterized Logarithmic Image Processing (PLIP) model. At Stage-I of sub-band decomposition: the first level SWT coefficients contain large amount of noise thus suppressing the necessary edge information. This aspect has been resolved at Stage-II by employing second level SWT decomposition along with Principal Component Analysis (PCA). The fusion coefficients from both the stages are finally fused using PLIP operators (prior to reconstruction). The obtained results are compared qualitatively as well as quantitatively using fusion metrics like Entropy, Fusion Factor, Standard Deviation and Edge Strength. Noteworthy visual response is obtained with PLIP fusion model in coherence with Human Visual System (HVS) characteristics. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						25	30		10.1016/j.patrec.2020.05.027													
J								Vector score alpha integration for classifier late fusion	PATTERN RECOGNITION LETTERS												Alpha integration is a family of integrators that encompasses many classic fusion operators (e.g., mean, product, minimum, maximum) as particular cases. This paper proposes vector score integration (VSI), a new alpha integration method for late fusion of multiple classifiers considering the joint effect of all the classes of the multi-class problem. Theoretical derivations to optimize the parameters of VSI for achieving the minimum probability of error are provided. VSI was applied to two classification tasks using electroencephalographic signals. The first task was the automatic stage classification of a neuropsychological test performed by epileptic subjects and the second one was the classification of sleep stages from apnea patients. Four single classifiers (linear and quadratic discriminant analysis, naive Bayes, and random forest) and three competitive fusion methods were estimated for comparison: mean, majority voting, and separated score integration (SSI). SSI is based on alpha integration, but unlike the proposed method, it considers the scores from each class in isolation, not accounting for possible dependencies among scores corresponding to different classes. VSI was able to optimally combine the results from all the single classifiers, in terms of accuracy and kappa coefficient, and outperformed the results of the other fusion methods in both applications. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						48	55		10.1016/j.patrec.2020.05.014													
J								Heuristic algorithms for diversity-aware balanced multi-way number partitioning	PATTERN RECOGNITION LETTERS										Artificial intelligence; Number partitioning; Heuristic algorithms; Balanced multi-way number partitioning	DESIGN	Number partitioning is a classic problem in artificial intelligence. And balanced multi-way number partitioning problem (BMNP) aims to partition a set of numbers into multiple subsets, such that (1) each subset contains the same number of numbers and (2) the subset sums are equal. The BMNP problem has various applications in real world scenarios, including task allocation, CPU scheduling, file placement in data center, multi-source data processing, etc. In this paper, we consider the problem of diversity-aware balanced multi-way number partitioning (DBMNP). DBMNP differs from BMNP, in that each number is associated with a type attribute. In addition to the two goals of BMNP, DBMNP also requires that the types of numbers in each subset are as diversified as possible. To solve the problem, we propose three heuristic algorithms to minimize the difference between subset sums and at the same time maximize diversify of each subset. Extensive experiments are conducted to evaluate the effectiveness of our proposed algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						56	62		10.1016/j.patrec.2020.05.022													
J								Feature selection and learning for graphlet kernel	PATTERN RECOGNITION LETTERS										Structural characterization of families of graphs; Small world graphs; complex networks; Graph algorithms	EDIT DISTANCE	Graph-based representations have been used with considerable success in Bioinformatics. One of the challenging problems with graph-based representation is that of estimating the similarity between two input graphs. Graph kernels are an answer to this problem that aim at bridging the gap between a vectorised representation and a structured representation. However, existing graph kernels suffer from one of two problems. They are either computationally very expensive or have low classification accuracy. In this paper we present a method that can be used to improve the accuracy and efficiency of one of the most popular graph kernels, i.e., graphlet kernel. The main idea behind graphlet kernel is to use a graphlet frequency vector as a feature vector. We propose a framework that can be used to select a subset of features that can be used to estimate the similarity between graphs. We show that the proposed method not only increases the efficiency of the resulting kernel but also increases the classification accuracy. We enrich the feature vector by identifying a set of higher-order graphlets that can be efficiently computed. We also show that different datasets from bioinformatics domain share common graphlets. Therefore the set of features learned from one bioinformatics dataset can also be used to classify graphs in another bioinformatics dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						63	70		10.1016/j.patrec.2020.05.023													
J								On the performance of Matthews correlation coefficient (MCC) for imbalanced dataset	PATTERN RECOGNITION LETTERS										Matthews correlation coefficient; Classification accuracy measurement; Performance evaluation; Imbalanced dataset		The Matthews Correlation Coefficient (MCC) is one of the popular measurements for classification accuracy. It has been generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The study of this paper finds that this is not true. MCC deteriorates seriously when the dataset in classification are imbalanced. Experiment results and analysis show that MCC is not suitable for classification accuracy measurement on imbalanced datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						71	80		10.1016/j.patrec.2020.03.030													
J								Simplified long short-term memory model for robust and fast prediction	PATTERN RECOGNITION LETTERS										Long short-term memory; Recurrent neural network; Robustness; Simplified model		Long short-term memory(LSTM) is an effective solution to time sequence prediction. Considering the data perturbations, in this letter, a variant model of LSTM is proposed to achieve robustness of prediction. Specifically, data processing procedure in the recurrent unit of proposed model is reformulated, the gates are controlled by only one variable, and the variable is the sum of long-term memory and the current input. Due to the simplified two-gate structure of proposed model, the speed of prediction is improved as well. The experiments on three datasets verify that the proposed model with simplified structure has higher robustness and shorter running time than the traditional LSTM model. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				AUG	2020	136						81	86		10.1016/j.patrec.2020.05.033													
J								A benchmark dataset for real-time detection of icons in mobile apps and a small-scale feature module	PATTERN RECOGNITION LETTERS										Icon detection; Small-scale feature module; IconYOLO; Passthrough; MobileIcon		Icons are widely used as a UI component in mobile applications. However, it is considerably difficult to detect icons because of their small size. Existing rapid object detection frameworks, such as YOLO and SSD, cannot perform well on small objects, such as icons, because of the multiple downsampling layers of convolutional networks and prediction grids. This paper summarizes our recent efforts toward the goal of small-scale icon detection. First, we describe the tiny icon collection (named MobileIcon), which contains 32,670 images spanning 20 different icon categories. This database allows us to systematically study icon detection in mobile apps and to establish a benchmark for icon recognition. We also explore a way to detect icons in real-time called IconYOLO, which detects icons at the native resolution of App snapshots. We introduce a small-scale feature module based on a modified passthrough operation to fuse deep semantic information with shallow detail features. Different from the original passthrough operation, our structure employs more upsamples and DBL units for feature maps fusing. The vanishing gradient issue can be addressed by residual units. Finally, icons can be detected on a single-scale grid rather than a multiscale grid in YOLOv3. Compared with YOLOv3, IconYOLO performs 9.63% better on the MobileIcon dataset. To further verify the generalization of our network, an experiment on the traditional PASCAL VOC dataset is performed. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						87	93		10.1016/j.patrec.2020.04.037													
J								Receiver operating characteristic curves with an indeterminacy zone	PATTERN RECOGNITION LETTERS										Indeterminacy in classification; Receiver operating characteristic (ROC)	ROC CURVE; AREA	This work extends Receiver Operating Characteristic (ROC) curve to the situation where some cases, falling in an intermediate "indeterminacy zone" of the predictor, are not classified. It addresses two challenges: definition of sensitivity and specificity bounds for this case; and summarization of the large number of possibilities arising from different choices of indeterminacy zones. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						94	100		10.1016/j.patrec.2020.04.035													
J								Palm vein recognition based on competitive coding scheme using multi-scale local binary pattern with ant colony optimization	PATTERN RECOGNITION LETTERS										Biometrics; Palm vein recognition; Competitive coding; MLBP; Ant colony optimization		Among the various biometric traits that can be extracted from the hand, the palm vein structure that represents a reliable and secure source for identifying and/or verifying the identity of a person. Several recognition methods were proposed in the literature exploiting this modality; among them, the attractive approaches based on a competitive coding. Aiming to further improve the performance of these approaches, this paper presents a novel palm vein recognition method for personal authentication and identification based on a competitive coding scheme using Multi-scale local binary pattern (MLBP) with Ant colony optimization (ACO). ACO allows to override potential blocking points related to image quality or contrast problems that can be encountered with images from the Near infrared spectral band. Thepre-processed images will be then sorted with a competitive coding scheme using MLBP; where the final image will be composed of the winning code from the different MLBP images. The matching process for making-decision is then performed using Kullback-Leibler divergence and Jaccard distance. The experimental results obtained on MS-PolyU database has shown that the proposed method achieves improved performances for both identification and verification modes up to 99.64% in terms of CIR for the identification and 0.0 0078% in terms of EER for the verification; and also outperforms the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						101	110		10.1016/j.patrec.2020.05.030													
J								How distance metrics influence missing data imputation with k-nearest neighbours	PATTERN RECOGNITION LETTERS										Missing Data; Data Imputation; k-nearest neighbours; Distance Functions; Heterogeneous Data; Imbalanced Data	SURVIVAL PREDICTION	In missing data contexts, k-nearest neighbours imputation has proven beneficial since it takes advantage of the similarity between patterns to replace missing values. When dealing with heterogeneous data, researchers traditionally apply the HEOM distance, that handles continuous, nominal and missing data. Although other heterogeneous distances have been proposed, they have not yet been investigated and compared for k-nearest neighbours imputation. In this work, we study the effect of several heterogeneous distances on k-nearest neighbours imputation on a large benchmark of publicly-available datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						111	119		10.1016/j.patrec.2020.05.032													
J								Multi-task learning for natural language processing in the 2020s: Where are we going?	PATTERN RECOGNITION LETTERS										Multi-task learning; Task relationship; Natural language processing		Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						120	126		10.1016/j.patrec.2020.05.031													
J								An attention-based row-column encoder-decoder model for text recognition in Japanese historical documents	PATTERN RECOGNITION LETTERS										Attention mechanism; Sequence-to-sequence; Row-column BLSTM; Residual connection; Historical documents; Multiple text lines recognition	NEURAL-NETWORK; CHARACTER	This paper presents an attention-based row-column encoder-decoder (ARCED) model for recognizing an input image of multiple text lines from Japanese historical documents without explicit segmentation of lines. The recognition system has three main parts: a feature extractor, a row-column encoder, and a decoder. We introduce a row-column BLSTM in the encoder and a residual LSTM network in the decoder. The whole system is trained end-to-end by a standard cross-entropy loss function, requiring only document images and their ground-truth text. We experimentally evaluate the performance of ARCED on the dataset of Japanese historical documents: Kana-PRMU. The results of the experiments show that ARCED outperforms the state-of-the-art recognition methods on the dataset. Furthermore, we demonstrate that the row-column BLSTM in the encoder and the residual LSTM in the decoder improves the performance of the encoder-decoder model for the recognition of Japanese historical document. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						134	141		10.1016/j.patrec.2020.05.026													
J								A computer vision system for automatic cherry beans detection on coffee trees	PATTERN RECOGNITION LETTERS										Coffee production; Caturra; Bourbon; Castillo; Noise reduction; Segmentation; Morphological transformations	CLASSIFICATION; INTELLIGENCE; AGRICULTURE; CROPS	Coffee production estimation is an essential task for coffee farmers in terms of money investment and planning time. In Colombia, the traditional methodology to estimate the total amount of cherry coffee beans is through direct measurements in the field; leave out the cherry beans collected of coffee production (destructive sampling). The cherry coffee dropped in this process cannot be harvest by the producer. In this sense, we found several shortcomings in this methodology as counting errors in the sampling process, insufficient coffee bean samples, significant expenses of costs and time, and coffee beans losses. To handle these issues, we propose a classic Computer Vision (CV) approach to detect cherry beans in coffee trees. This approach substitutes the destructive counting method as a first step to estimate coffee production. To evaluate the CV proposed, seven coffee farmers counted the number of cherry beans on 600 images of coffee trees (castillo, bourbon, and caturra varieties) by human visual perception (ground truth). From evaluations of coffee farmers, we computed statistical measures like precision, recall and, F1-score. The CV system achieved the best results for bourbon coffee trees with 0.594 of precision; 0.669 of total relevant cherry beans correctly classified. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						142	153		10.1016/j.patrec.2020.05.034													
J								Ensembling complex network 'perspectives' for mild cognitive impairment detection with artificial neural networks	PATTERN RECOGNITION LETTERS										Decision support systems; Mild cognitive impairment; Diffusion-weighted imaging; Complex networks; Artificial neural networks	ALZHEIMERS-DISEASE; CONNECTIVITY; CLASSIFICATION; TRACTOGRAPHY; DEMENTIA	In this paper, we propose a novel method for mild cognitive impairment detection based on jointly exploiting the complex network and the neural network paradigm. In particular, the method is based on ensembling different brain structural "perspectives" with artificial neural networks. On one hand, these perspectives are obtained with complex network measures tailored to describe the altered brain connectivity. In turn, the brain reconstruction is obtained by combining diffusion-weighted imaging (DWI) data to tractography algorithms. On the other hand, artificial neural networks provide a means to learn a mapping from topological properties of the brain to the presence or absence of cognitive decline. The effectiveness of the method is studied on a well-known benchmark data set in order to evaluate if it can provide an automatic tool to support the early disease diagnosis. Also, the effects of balancing issues are investigated to further assess the reliability of the complex network approach to DWI data. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						168	174		10.1016/j.patrec.2020.06.001													
J								SceneAdapt: Scene-based domain adaptation for semantic segmentation using adversarial learning	PATTERN RECOGNITION LETTERS										Semantic segmentation; Domain adaptation; Scene adaptation; Adversarial learning		Semantic segmentation methods have achieved outstanding performance thanks to deep learning. Nevertheless, when such algorithms are deployed to new contexts not seen during training, it is necessary to collect and label scene-specific data in order to adapt them to the new domain using fine-tuning. This process is required whenever an already installed camera is moved or a new camera is introduced in a camera network due to the different scene layouts induced by the different viewpoints. To limit the amount of additional training data to be collected, it would be ideal to train a semantic segmentation method using labeled data already available and only unlabeled data coming from the new camera. We formalize this problem as a domain adaptation task and introduce a novel dataset of urban scenes with the related semantic labels. As a first approach to address this challenging task, we propose SceneAdapt, a method for scene adaptation of semantic segmentation algorithms based on adversarial learning. Experiments and comparisons with state-of-the-art approaches to domain adaptation highlight that promising performance can be achieved using adversarial learning both when the two scenes have different but points of view, and when they comprise images of completely different scenes. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						175	182		10.1016/j.patrec.2020.06.002													
J								Temporal logistic neural Bag-of-Features for financial time series forecasting leveraging limit order book data	PATTERN RECOGNITION LETTERS										Limit order book data; Bag-of-Features; Time-series forecasting	MACHINE	Time series forecasting is a crucial component of many important applications, ranging from forecasting the stock markets to energy load prediction. The high-dimensionality, velocity and variety of the data collected in many of these applications pose significant and unique challenges that must be carefully addressed for each of them. In this work, a novel Temporal Logistic Neural Bag-of-Features approach, that can be used to tackle these challenges, is proposed. The proposed method can be effectively combined with deep neural networks, leading to powerful deep learning models for time series analysis. However, combining existing BoF formulations with deep feature extractors pose significant challenges: the distribution of the input features is not stationary, tuning the hyper-parameters of the model can be especially difficult and the normalizations involved in the BoF model can cause significant instabilities during the training process. The proposed method is capable of overcoming these limitations by a employing a novel adaptive scaling mechanism and replacing the classical Gaussian-based density estimation involved in the regular BoF model with a logistic kernel. The effectiveness of the proposed approach is demonstrated using extensive experiments on a large-scale limit order book dataset that consists of more than 4 million limit orders. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						183	189		10.1016/j.patrec.2020.06.006													
J								Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost	PATTERN RECOGNITION LETTERS										Imbalanced classification; XGBoost; Python package	ALGORITHMS; FRAMEWORK; TREES	The paper presents Imbalance-XGBoost, a Python package that combines the powerful XGBoost software with weighted and focal losses to tackle binary label-imbalanced classification tasks. Though a small-scale program in terms of size, the package is, to the best of our knowledge, the first of its kind which provides an integrated implementation for the two loss functions on XGBoost and brings a general-purpose extension to XGBoost for label-imbalanced scenarios. In this paper, the design and usage of the package are discussed and illustrated with examples. Furthermore, as the first- and second-order derivatives of the loss functions are essential for the implementations, the algebraic derivation is discussed and it can be deemed as a separate contribution. The performances of the methods implemented in the package are extensively evaluated on Parkinson's disease classification dataset, and multiple competitive performances are presented with the ROC and Precision-Recall (PR) curves. To further assert the superiority of the methods, the performances on four other benchmark datasets from the UCI machine learning repository are additionally reported. Given the scalable nature of XGBoost, the package has great potentials to be broadly applied to real-life binary classification tasks, which are usually of large-scale and label-imbalanced. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						190	197		10.1016/j.patrec.2020.05.035													
J								Positive-unlabeled learning for open set domain adaptation	PATTERN RECOGNITION LETTERS										Computer vision; Deep learning; Image classification; Domain adaptation; Open set recognition; Positive-Unlabelled learning		Open Set Domain Adaptation (OSDA) focuses on bridging the domain gap between a labeled source domain and an unlabeled target domain, while also rejecting target classes that are not present in the source as unknown. The challenges of this task are closely related to those of Positive-Unlabeled (PU) learning where it is essential to discriminate between positive (known) and negative (unknown) class samples in the unlabeled target data. With this newly discovered connection, we leverage the theoretical framework of PU learning for OSDA and, at the same time, we extend PU learning to tackle uneven data distributions. Our method combines domain adversarial learning with a new non-negative risk estimator for PU learning based on self-supervised sample reconstruction. With experiments on digit recognition and object classification, we validate our risk estimator and demonstrate that our approach allows reducing the domain gap without suffering from negative transfer. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						198	204		10.1016/j.patrec.2020.06.003													
J								SaHAN: Scale-aware hierarchical attention network for scene text recognition	PATTERN RECOGNITION LETTERS										Scene text recognition; Character scale-variation problem; Multi-scale features; Hierarchical attention decoder		Scene text recognition has become a research hotspot owing to its abundant semantic information and various applications. Recent methods of scene text recognition usually focus on handling shape distortion, attention drift, or background noise, ignoring that text recognition encounters character scale-variation problem. To address this issue, in this paper, we propose a new scale-aware hierarchical attention network (SaHAN) for scene text recognition. Inspired by feature pyramid network, we exploit the inherent pyramidal structure of a deep convolutional network to retain multi-scale features for flexible receptive fields. Then, we construct a hierarchical attention decoder that performs the attention mechanism twice on multi-scale features to collect the most fine-grained information for prediction. The SaHAN is trained in a weak supervision way, requiring only images and corresponding text labels. Extensive experiments on seven benchmarks reveal that SaHAN achieves state-of-the-art performance. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						205	211		10.1016/j.patrec.2020.06.009													
J								A text-based visual context modulation neural model for multimodal machine translation	PATTERN RECOGNITION LETTERS										Deep learning; Machine translation; Multimodality		We introduce a novel multimodal machine translation model that integrates image features modulated by its caption. Generally, images contain vastly more information rather than just their description. Furthermore, in multimodal machine translation task, feature maps are commonly extracted from pre-trained network for objects. Therefore, it is not appropriate to utilize these feature map directly. To extract the visual features associated with the text, we design a modulation network based on the textual information from the encoder and visual information from the pretrained CNN. However, because multimodal translation data is scarce, using overly complicated models could result in poor performance. For simplicity, we apply a feature-wise multiplicative transformation. Therefore, our model is a modular trainable network embedded in the architecture in existing multimodal translation models. We verified our model by conducting experiments on the Transformer model with the Multi30k dataset and evaluating translation quality using the BLEU and METEOR metrics. In general, our model was an improvements over a text-based model and other existing models. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						212	218		10.1016/j.patrec.2020.06.010													
J								A neural model for text localization, transcription and named entity recognition in full pages	PATTERN RECOGNITION LETTERS										Document image analysis; Information extraction; Text detection; Handwritten text recognition; Named entity recognition; Deep neural networks; Multi-task learning		In the last years, the consolidation of deep neural network architectures for information extraction in document images has brought big improvements in the performance of each of the tasks involved in this process, consisting of text localization, transcription, and named entity recognition. However, this process is traditionally performed with separate methods for each task. In this work we propose an end-to-end model that combines a one stage object detection network with branches for the recognition of text and named entities respectively in a way that shared features can be learned simultaneously from the training error of each of the tasks. By doing so the model jointly performs handwritten text detection, transcription, and named entity recognition at page level with a single feed forward step. We exhaustively evaluate our approach on different datasets, discussing its advantages and limitations compared to sequential approaches. The results show that the model is capable of benefiting from shared features by simultaneously solving interdependent tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						219	227		10.1016/j.patrec.2020.05.001													
J								Discriminative block-diagonal covariance descriptors for image set classification	PATTERN RECOGNITION LETTERS										Covariance descriptor; SPD matrix; Riemannian manifold; Image set classification	FACE RECOGNITION; FEATURE-SELECTION	Image set classification has recently received much attention due to its various applications in pattern recognition and computer vision. To compare and match image sets, the major challenges are to devise an effective and efficient representation and to define a measure of similarity between image sets. In this paper, we propose a method for representing image sets based on block-diagonal Covariance Descriptors (CovDs). In particular, the proposed image set representation is in the form of non-singular covariance matrices, also known as Symmetric Positive Definite (SPD) matrices, that lie on Riemannian manifold. By dividing each image of an image set into square blocks of the same size, we compute the corresponding block CovDs instead of the global one. Taking the relative discriminative power of these block CovDs into account, a block-diagonal SPD matrix can be constructed to achieve a better discriminative capability. We extend the proposed approach to work with bidirectional CovDs and achieve a further boost in performance. The resulting block-diagonal SPD matrices combined with Riemannian metrics are shown to provide a powerful basis for image set classification. We perform an extensive evaluation on four datasets for several image set classification tasks. The experimental results demonstrate the effectiveness and efficiency of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						230	236		10.1016/j.patrec.2020.05.018													
J								Uncertainty-aware integration of local and flat classifiers for food recognition	PATTERN RECOGNITION LETTERS										CNNs; Deep learning; Epistemic uncertainty; Image classification; Food recognition		Food image recognition has recently attracted the attention of many researchers, due to the challenging problem it poses, the ease collection of food images, and its numerous applications to health and leisure. In real applications, it is necessary to analyze and recognize thousands of different foods. For this purpose, we propose a novel prediction scheme based on a class hierarchy that considers local classifiers, in addition to a flat classifier. In order to make a decision about which approach to use, we define different criteria that take into account both the analysis of the Epistemic Uncertainty estimated from the 'children' classifiers and the prediction from the 'parent' classifier. We evaluate our proposal using three Uncertainty estimation methods, tested on two public food datasets. The results show that the proposed method reduces parent-child error propagation in hierarchical schemes and improves classification results compared to the single flat classifier, meanwhile maintains good performance regardless the Uncertainty estimation method chosen. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						237	243		10.1016/j.patrec.2020.06.013													
J								Data augmentation method for improving the accuracy of human pose estimation with cropped images	PATTERN RECOGNITION LETTERS										Data augmentation; Human pose estimation; Keypoint detection		Neural networks have improved the accuracy of human pose estimation from a single RGB image. However, such estimation remains difficult, especially when the human body is only partially visible due to a limited field of view of the camera or occlusions. In this paper, we introduce a data augmentation method called body-cropping augmentation (BCA), which generalizes the dataset for effective training in human pose estimation. This technique includes the policies of data generation and the training strategy using the augmented data. The experiments with the COCO val2017 dataset with ground-truth bounding boxes show BCA consistently enhances accuracies of state-of-the-art neural networks by an average of 1.08% without any modification to the network architecture. Moreover, the proposed BCA technique effectively reduces the false negatives of localizing keypoints, especially in an input image with a few visible keypoints. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						244	250		10.1016/j.patrec.2020.06.015													
J								Adversarial joint domain adaptation of asymmetric feature mapping based on least squares distance	PATTERN RECOGNITION LETTERS										Joint domain adaptation; Adversarial learning; Asymmetric feature mapping; Conditional distribution alignment		Joint domain adaptation aims to learn a high-quality classifier for an unlabeled dataset with the help of auxiliary data. Most methods reduce domain shifts through some carefully designed distance measures. Adversarial learning, which is rarely used for joint domain adaptation, can learn more transferable features while avoiding explicit distance measures. However, it usually suffers from a gradient vanishing problem during the training process. In order to solve the above problems, we propose a novel adversarial joint domain adaptation method, namely Asymmetric Feature mapping based on Least Squares Distance (AFLSD), which consists of asymmetric marginal distribution alignment and conditional distribution alignment. The asymmetric feature mapping, which can get closer features with more flexible parameters, is optimized by the least squares distance to reduce the gradient vanishing problem. The results of classification and other comparative experiments show that AFLSD is superior to the most advanced domain adaptation methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						251	256		10.1016/j.patrec.2020.06.007													
J								OccGAN: Semantic image augmentation for driving scenes	PATTERN RECOGNITION LETTERS										Occlusion; GAN; Semantic; Augmentation; Cityscapes		Difficult images with complicated environments and occlusion have significant impacts on the performance of algorithms. They obey the long-tail distribution in the widely used datasets, which results in rare samples being overwhelmed during training. This paper presents a new approach to generate plausible occluded images with annotation as a kind of data augmentation with scenes semantics. To achieve this task, we proposed the Occlusion-based Generative Adversarial Network (OccGAN) structure, which consists of a Rationality Module and an Authenticity Module. The Rationality Module generated preliminary occluded samples under the guidance of prior semantic knowledge. And the Authenticity Module is a generative adversarial structure to ensure the reality of the produced images. Qualitative results of the visualization process are given to verify the ablation study. Experiments on the semantic segmentation task indicate that several state-of-the-art algorithms combined with our OccGAN such as DRN, Deeplabv3+, PSPNet and ResNet-38, have boosts on IoU class scores and IoU category scores successfully. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						257	263		10.1016/j.patrec.2020.06.011													
J								Adaptive machine learning strategies for network calibration of IoT smart air quality monitoring devices	PATTERN RECOGNITION LETTERS											MICROSENSORS; OLFACTION	Air Quality Multi-sensors Systems (AQMS) are IoT devices based on low cost chemical microsensors array that recently have showed capable to provide relatively accurate air pollutant quantitative estimations. Their availability permits to deploy pervasive Air Quality Monitoring (AQM) networks that will solve the geographical sparseness issue that affect the current network of AQ Regulatory Monitoring Systems (AQRMS). Unfortunately their accuracy have shown limited in long term field deployments due to negative influence of several technological issues including sensors poisoning or ageing, non target gas interference, lack of fabrication repeatability, etc. Seasonal changes in probability distribution of priors, observables and hidden context variables (i.e. non observable interferents) challenge field data driven calibration models which short to mid term performances recently rose to the attention of Urban authorithies and monitoring agencies. In this work, we address this non stationary framework with adaptive learning strategies in order to prolong the validity of multisensors calibration models enabling continuous learning. Relevant parameters influence in different network and node-to-node recalibration scenario is analyzed. Results are hence useful for pervasive deployment aimed to permanent high resolution AQ mapping in urban scenarios as well as for the use of AQMS as AQRMS backup systems providing data when AQRMS data are unavailable due to faults or scheduled mainteinance. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						264	271		10.1016/j.patrec.2020.04.032													
J								Using an autoencoder in the design of an anomaly detector for smart manufacturing	PATTERN RECOGNITION LETTERS										Fault detection; Anomaly detection; Smart manufacturing; Smart industry; Interpretable machine learning; Autoencoder; Anomaly discriminator		According to the smart manufacturing paradigm, the analysis of assets' time series with a machine learning approach can effectively prevent unplanned production downtimes by detecting assets' anomalous operational conditions. To support smart manufacturing operators with no data science background, we propose an anomaly detection approach based on deep learning and aimed at providing a manageable machine learning pipeline and easy to interpret outcome. To do so we combine (i) an autoencoder, a deep neural network able to produce an anomaly score for each provided time series, and (ii) a discriminator based on a general heuristics, to automatically discern anomalies from regular instances. We prove the convenience of the proposed approach by comparing its performances against isolation forest with different case studies addressing industrial laundry assets' power consumption and bearing vibrations. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						272	278		10.1016/j.patrec.2020.06.008													
J								Multilabel naive Bayes classification considering label dependence	PATTERN RECOGNITION LETTERS										Multilabel classifier; Naive Bayes classification; Label dependence		Multilabel classification is the task of assigning relevant labels to an instance, and it has received considerable attention in recent years. This task can be performed by extending a single-label classifier, such as the naive Bayes classifier, to utilize the useful relations among labels for achieving better multilabel classification accuracy. However, the conventional multilabel naive Bayes classifier treats each label independently and hence neglects the relations among labels, resulting in degenerated accuracy. We propose a new multilabel naive Bayes classifier that considers the relations or dependence among labels. Experimental results show that the proposed method outperforms conventional multilabel classifiers. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						279	285		10.1016/j.patrec.2020.06.021													
J								Deep learning strategies for foetal electrocardiogram signal synthesis	PATTERN RECOGNITION LETTERS										Deep learning; Foetal electrocardiogram; Convolutional neural network; Deep belief neural network; Back propagation neural network	NETWORKS	One of the most difficult tasks for the physicians is to acquire a quality foetal electrocardiogram (fECG) to analyze, manage and plan according to the condition of the foetus in the womb. Hence the foetal electrocardiogram signal is not preferred to execute the analysis to monitor the Foetal condition. Other traditional methods are being used to access the foetal condition. The foetal electrocardiogram signal can be acquired either by using invasive or non-invasive techniques. Since the invasive technique is harmful for the foetus, non-invasive technique is mostly adopted. The foetal electrocardiogram signal can be acquired only after twenty five weeks the foetus is developed in the womb, which is referred as the Antepartum period. This article portrays the use of Deep learning techniques for non-invasive foetal electrocardiogram signal synthesis using artificial intelligent techniques. Convolutional neural network (CNN), Deep belief neural networks (BNN) and Back propagation Neural Network (BPNN) have been utilized and tested for the proposal. The outcomes and performance are compared with reference to the synthesized high quality foetal electrocardiogram signal. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						286	292		10.1016/j.patrec.2020.06.016													
J								Two-stage human hair segmentation in the wild using deep shape prior	PATTERN RECOGNITION LETTERS										Humain hair segmentation; Shape prior; Spatial attention		Human hair is a crucial biometric characteristic with rich color and texture information. In this paper, we propose a novel hair segmentation approach integrating a deep shape prior into a carefully designed two-stage Fully Convolutional Neural Network (FCNN) pipeline. First, we utilize a FCNN with an Atrous Spatial Pyramid Pooling (ASPP) module to train a human hair shape prior based on a specific distance transform. In the second stage, we combine the hair shape prior and the original image to form the input of a symmetric encoder-decoder FCNN with a border refinement module to get the final hair segmentation output. Both quantitative and qualitative results show that our method achieves state-of-the-art performance on the LFW-Part and Figaro1k datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						293	300		10.1016/j.patrec.2020.06.014													
J								From neighbors to strengths - the k-strongest strengths (kSS) classification algorithm	PATTERN RECOGNITION LETTERS										Classification problems; k-Nearest neighbor; Gravitational force		In this study we introduce the k-Strongest Strengths (kSS) Classification Algorithm, a novel approach for classification problems based on the well-known k-Nearest Neighbor (kNN) classifier. The proposed kSS method is motivated by an analogy to the Law of Universal Gravitation. The novelty of kSS resides in that instead of only using the neighbors' labels to classify an unseen object it uses gravitation forces or strengths exerted by training objects. To incorporate this Newtonian concept into kSS, mass to training objects needs to be assigned. Following this idea we propose novel mass functions that exploit object's topology properties within 11 data sets, which comprise binary and multi-class problems that present high and low imbalance ratio. Experiments show that kSS obtains an average f 1 score of 0.87, outperforming other popular Machine Learning methods such as Artificial Neural Networks (0.80), Decision Trees (0.75), other kNN variants (0.79), naive Bayes (0.72) and Support Vector Machines (0.82). All these differences being statistically significant. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						301	308		10.1016/j.patrec.2020.06.020													
J								Three-dimensional reconstruction of CT image features based on multi-threaded deep learning calculation	PATTERN RECOGNITION LETTERS										Fuzzy clustering; CT image; Feature region; Deep learning; Multithreading; 3D reconstruction	3D RECONSTRUCTION; TRANSFORM; FIELD	Traditional technology uses serial processing method in CT image feature extraction. It is prone to loss of image data, which causes problems such as ring distortion of the reconstructed image and long reconstruction time. Therefore, a three-dimensional (3D) reconstruction algorithm for CT image features based on multi-threaded deep learning calculation is designed. Feature image textures are segmented using parameters such as gray, mean, and variance. Fusion calculates co-occurrence logarithm of the gray level with segmented sub-block and its pixels. Co-occurrence probability of the sub-block gray levels is obtained, followed by getting optimal feature volume data, which is stored in 3D texture and 1D texture for interpolation calculation.The optimal feature volume data is stored in 3D and 1D texture for interpolation calculation. Rotation matrix is stored in the global storage space of CUDA, which is used for multi-threaded calculations. After completing multi-threaded batch CT image hardware configuration and algorithm flow settings, the thread's index and bounding box is calculated. 3D reconstruction of CT image features is achieved by model accumulation. The experimental simulation proves that local detail information loss is small after reconstruction of the proposed method. Its reconstruction time is short and has good applicability. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						309	315		10.1016/j.patrec.2020.04.033													
J								Understanding trained CNNs by indexing neuron selectivity	PATTERN RECOGNITION LETTERS										Convolutional neural networks; Visualization of CNNs; Neuron selectivity; CNNs Understanding; Feature visualization		The impressive performance of Convolutional Neural Networks (CNNs) when solving different vision problems is shadowed by their black-box nature and our consequent lack of understanding of the representations they build and how these representations are organized. To help understanding these issues, we propose to describe the activity of individual neurons by their Neuron Feature visualization and quantify their inherent selectivity with two specific properties. We explore selectivity indexes for: an image feature (color); and an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer Conv4 or class selective neurons such as dog-face neurons in layer Conv5 in VGG-M, and establishes a methodology to derive other selectivity properties. Indexing on neuron selectivity can statistically draw how features and classes are represented through layers in a moment when the size of trained nets is growing and automatic tools to index neurons can be helpful. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						318	325		10.1016/j.patrec.2019.10.013													
J								Cascade of encoder-decoder CNNs with learned coordinates regressor for robust facial landmarks detection	PATTERN RECOGNITION LETTERS										Face alignment; Facial landmark detection; Cascaded shape regression; Heatmap regression		Convolutional Neural Nets (CNNs) have become the reference technology for many computer vision problems. Although CNNs for facial landmark detection are very robust, they still lack accuracy when processing images acquired in unrestricted conditions. In this paper we investigate the use of a cascade of Neural Net regressors to increase the accuracy of the estimated facial landmarks. To this end we append two encoder-decoder CNNs with the same architecture. The first net produces a set of heatmaps with a rough estimation of landmark locations. The second, trained with synthetically generated occlusions, refines the location of ambiguous and occluded landmarks. Finally, a densely connected layer with shared weights among all heatmaps, accurately regresses the landmark coordinates. The proposed approach achieves state-of-the-art results in 300W, COFW and WFLW that are widely considered the most challenging public data sets. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						326	332		10.1016/j.patrec.2019.10.012													
J								A Multi-Scale Model based on the Long Short-Term Memory for day ahead hourly wind speed forecasting	PATTERN RECOGNITION LETTERS										Artificial neural networks; Multi-scale; Long Short-Term Memory; Wind speed forecasting	RECURRENT NEURAL-NETWORKS; MULTISTEP; DECOMPOSITION	Crucial to wind energy penetration in electrical systems is the precise forecasting of wind speed, which turns into accurate future wind power estimates. Current trends in wind speed forecasting involve using Recurrent Neural Networks to model complex temporal dynamics in the time-series. These networks, however, have problems learning long temporal dependencies in the data. To address this issue, we devise a Multi-scale Model Based on the Long Short-Term Memory for the day-ahead hourly wind speed forecasting task. Our model uses dense layers to build sub-sequences of different timescales which are used as input for multiple Long Short-Term Memory Networks (LSTM), which model each temporal scale and integrate their information accordingly. An experiment with altered wind speed data shows that our proposal is better able to learn long term dependencies than the stacked LSTM. Furthermore, results on four wind speed datasets of varying length from northern Chile reveal that our approach outperforms several models in terms of MAE and RMSE. Training times also exhibit that adding depth to the model does not increase computational times substantially, making it a more efficient approach than the stacked LSTM. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				AUG	2020	136						333	340		10.1016/j.patrec.2019.10.011													
J								A GP-based ensemble classification framework for time-changing streams of intrusion detection data	SOFT COMPUTING										Data streams; Ensemble learning; Genetic programming; Intrusion detection; Cybersecurity	STATISTICAL COMPARISONS; CLASSIFIERS	Intrusion detection tools have largely benefitted from the usage of supervised classification methods developed in the field of data mining. However, the data produced by modern system/network logs pose many problems, such as the streaming and non-stationary nature of such data, their volume and velocity, and the presence of imbalanced classes. Classifier ensembles look a valid solution for this scenario, owing to their flexibility and scalability. In particular, data-driven schemes for combining the predictions of multiple classifiers have been shown superior to traditional fixed aggregation criteria (e.g., predictions' averaging and weighted voting). In intrusion detection settings, however, such schemes must be devised in an efficient way, since (part of) the ensemble may need to be re-trained frequently. A novel ensemble-based framework is proposed here for the online intrusion detection, where the ensemble is updated through an incremental stream-oriented learning scheme, correspondingly to the detection of concept drifts. Differently from mainstream ensemble-based approaches in the field, our proposal relies on deriving, though an efficient genetic programming (GP) method, an expressive kind of combiner function defined in terms of (non-trainable) aggregation functions. This approach is supported by a system architecture, which integrates different kinds of functionalities, ranging from the drift detection, to the induction and replacement of base classifiers, up to the distributed computation of GP-based combiners. Experiments on both artificial and real-life datasets confirmed the validity of the approach.																	1432-7643	1433-7479															10.1007/s00500-020-05200-3		AUG 2020											
J								Multi-valued picture fuzzy soft sets and their applications in group decision-making problems	SOFT COMPUTING										Multi-valued picture fuzzy sets; Multi-valued picture fuzzy soft sets; Multi-attribute group decision-making problems	AGGREGATION OPERATORS; INFORMATION; MODELS; SYSTEM	Soft set theory initiated by Molodtsov in 1999 has been emerging as a generic mathematical tool for dealing with uncertainty. A noticeable progress is found concerning the practical use of soft set in decision-making problems. The purpose of this manuscript is to explore the novel of multi-valued picture fuzzy set (MPFS) and multi-valued picture fuzzy soft set (MPFSS) which are the generalizations of the notions of picture fuzzy soft set (PFSS) and multi-fuzzy soft set (MFSS). This notion can be used to express fuzzy information in more general and effective way. In particular, some basic operations such as union, intersection, complement and product of the proposed MPFSS are developed, and their properties are investigated. Furthermore, some aggregation operators corresponding to the proposed MPFSSs are called multi-picture fuzzy soft weighted averaging, multi-picture fuzzy soft ordered weighted averaging and multi-picture soft hybrid weighted averaging operators for a collections of MPFSSs are also developed. Moreover, based on these operators, we presented a new method to deal with the multi-attribute group decision-making problems under the multi-valued picture fuzzy soft environment. Finally, we used some practical examples to illustrate the validity and superiority of the proposed method by comparing with other existing methods. The graphical interpretation of the explored approaches is also utilized with future directions.																	1432-7643	1433-7479															10.1007/s00500-020-05116-y		AUG 2020											
J								Partial label learning based on label distributions and error-correcting output codes	SOFT COMPUTING										Partial label learning; Weakly supervised learning; ECOC; Prior information	MULTICLASS; CLASSIFICATION; ALGORITHM; DESIGN; ECOC; MACHINE	Partial label learning (PLL) is a class of weak supervision learning problems in which each data sample has a candidate set of labels, among which only one label is correct. In this paper, a new PLL algorithm with prior information of the label distribution based on ECOC (PL-PIE) is proposed. PL-PIE utilizes the ECOC framework to decompose the problem into multiple binary problems. Different from the instability of the existing random dichotomy, the proposal exploits the prior information of label distribution to generate positive and negative classes with stable performance. Extensive experimental results demonstrate that the proposed PL-PIE algorithm has highly competitive performance compared to the state-of-the-art PLL algorithms.																	1432-7643	1433-7479															10.1007/s00500-020-05203-0		AUG 2020											
J								First-Order Automated Reasoning with Theories: When Deduction Modulo Theory Meets Practice	JOURNAL OF AUTOMATED REASONING										Automated deduction; Deduction modulo theory; First-order logic; Rewriting; Automated reasoning systems	SEMANTIC CUT-ELIMINATION; COMPLETENESS; PROVER; LOGIC; CALCULUS; PROGRAM; PROOFS; ZENON	We discuss the practical results obtained by the first generation of automated theorem provers based on Deduction modulo theory. In particular, we demonstrate the concrete improvements such a framework can bring to first-order theorem provers with the introduction of a rewrite feature. Deduction modulo theory is an extension of predicate calculus with rewriting both on terms and propositions. It is well suited for proof search in theories because it turns many axioms into rewrite rules. We introduce two automated reasoning systems that have been built to extend other provers with Deduction modulo theory. The first one is Zenon Modulo, a tableau-based tool able to deal with polymorphic first-order logic with equality, while the second one is iProverModulo, a resolution-based system dealing with first-order logic with equality. We also provide some experimental results run on benchmarks that show the beneficial impact of the extension on these two tools and their underlying proof search methods. Finally, we describe the two backends of these systems to the Dedukti universal proof checker, which also relies on Deduction modulo theory, and which allows us to verify the proofs produced by these tools.																	0168-7433	1573-0670				AUG	2020	64	6					1001	1050		10.1007/s10817-019-09533-z													
J								A Library for Formalization of Linear Error-Correcting Codes	JOURNAL OF AUTOMATED REASONING										Coq; Mathematical components; Error-correcting codes	PARITY-CHECK CODES	Error-correcting codes add redundancy to transmitted data to ensure reliable communication over noisy channels. Since they form the foundations of digital communication, their correctness is a matter of concern. To enable trustful verification of linear error-correcting codes, we have been carrying out a systematic formalization in the Coq proof-assistant. This formalization includes the material that one can expect of a university class on the topic: the formalization of well-known codes (Hamming, Reed-Solomon, Bose-Chaudhuri-Hocquenghem) and also a glimpse at modern coding theory. We demonstrate the usefulness of our formalization by extracting a verified decoder for low-density parity-check codes based on the sum-product algorithm. To achieve this formalization, we needed to develop a number of libraries on top of Coq's Mathematical Components. Special care was taken to make them as reusable as possible so as to help implementers and researchers dealing with error-correcting codes in the future.																	0168-7433	1573-0670				AUG	2020	64	6					1123	1164		10.1007/s10817-019-09538-8													
J								Vagueness visualization in building models across different design stages	ADVANCED ENGINEERING INFORMATICS										Building Information Modeling (BIM); Level of Development (LOD); Multi-LOD; Uncertainty visualization; Vagueness visualization; Information uncertainty; Meta-model	UNCERTAINTY VISUALIZATION; ENERGY PERFORMANCE; SPECIFICATION; ARCHITECTURE; MANAGEMENT; KNOWLEDGE	The iterative and developing nature of designing a building involves the specification and handling of vague, imprecise, and incomplete information. A crucial factor for mitigating the impact of these uncertainties on the decision-making process is to effectively quantify and communicate them among the project stakeholders. The interactive visualization of 3D building models provides great support for evaluating building designs. However, the currently available visualization methods of the available authoring tools do not incorporate the potential uncertainties associated with the geometric and semantic information of building elements. Currently, building models appear precise and certain, even in the early design stages, which can lead to false assumptions and model evaluations, affecting the decisions made throughout the design stages. Hence, this paper presents a set of visualization approaches, including intrinsic, extrinsic, animation, and walkthroughs, that have been developed to present the uncertainties associated with the building elements' information. The efficiency of the approaches developed in this study was evaluated through an online survey and interviews. More specifically, the approaches were compared in terms of intuitiveness, applicability, and acceptance. The evaluation results positively indicated the participants' ability to understand the amount and impact of the uncertainties on the design by using the developed approaches.																	1474-0346	1873-5320				AUG	2020	45								101107	10.1016/j.aei.2020.101107													
J								CLOI-NET: Class segmentation of industrial facilities' point cloud datasets	ADVANCED ENGINEERING INFORMATICS										Class segmentation; Industrial facilities; Point cloud processing; CLOI	LASER SCANNER INTENSITY; OBJECT RECOGNITION; 3D RECONSTRUCTION; INCIDENCE ANGLE; MODEL; PIPELINES; FEATURES; AREA	Shape segmentation from point cloud data is a core step of the digital twinning process for industrial facilities. However, it is also a very labor intensive step, which counteracts the perceived value of the resulting model. The state-of-the-art method for automating cylinder detection can detect cylinders with 62% precision and 70% recall, while other shapes must then be segmented manually and shape segmentation is not achieved. This performance is promising, but it is far from drastically eliminating the manual labor cost. We argue that the use of class segmentation deep learning algorithms has the theoretical potential to perform better in terms of per point accuracy and less manual segmentation time needed. However, such algorithms could not be used so far due to the lack of a pre-trained dataset of laser scanned industrial shapes as well as the lack of appropriate geometric features in order to learn these shapes. In this paper, we tackle both problems in three steps. First, we parse the industrial point cloud through a novel class segmentation solution (CLOI-NET) that consists of an optimized PointNET + + based deep learning network and post-processing algorithms that enforce stronger contextual relationships per point. We then allow the user to choose the optimal manual annotation of a test facility by means of active learning to further improve the results. We achieve the first step by clustering points in meaningful spatial 3D windows based on their location. Then, we apply a class segmentation deep network, and output a probability distribution of all label categories per point and improve the predicted labels by enforcing post-processing rules. We finally optimize the results by finding the optimal amount of data to be used for training experiments. We validate our method on the largest richly annotated dataset of the most important to model industrial shapes (CLOD and yield 82% average accuracy per point, 95.6% average AUC among all classes and estimated 70% labor hour savings in class segmentation. This proves that it is the first to automatically segment industrial point cloud shapes with no prior knowledge at commercially viable performance and is the foundation for efficient industrial shape modeling in cluttered point clouds.																	1474-0346	1873-5320				AUG	2020	45								101121	10.1016/j.aei.2020.101121													
J								Anomaly detection of defects on concrete structures with the convolutional autoencoder	ADVANCED ENGINEERING INFORMATICS										Anomaly detection; Unsupervised learning; Convolutional autoencoder; Concrete structure; Cracking; Spalling	CRACK DETECTION; DAMAGE DETECTION	This paper reports the application of deep learning for implementing the anomaly detection of defects on concrete structures, so as to facilitate the visual inspection of civil infrastructure. A convolutional autoencoder was trained as a reconstruction-based model, with the defect-free images, to rapidly and reliably detect defects from the large volume of image datasets. This training process was in the unsupervised mode, with no label needed, thereby requiring no prior knowledge and saving an enormous amount of time for label preparation. The built anomaly detector favors minimizing the reconstruction errors of defect-free images, which renders high reconstruction errors of defects, in turn, detecting the location of defects. The assessment shows that the proposed anomaly detection technique is robust and adaptable to defects on wide ranges of scales. Comparison was also made with the segmentation results produced by other automatic classical methods, revealing that the results made by the anomaly map outperform other segmentation methods, in terms of precision, recall, F-1 measure and F-2 measure, without severe under- and over-segmentation. Further, instead of merely being a binary map, each pixel of the anomaly map is represented by the anomaly score, which acts as a risk indicator for alerting inspectors, wherever defects on concrete structures are detected.																	1474-0346	1873-5320				AUG	2020	45								101105	10.1016/j.aei.2020.101105													
J								A global supply chain risk management framework: An application of text-mining to identify region-specific supply chain risks	ADVANCED ENGINEERING INFORMATICS										Global supply chain; Risk management; Data analytics; Text mining; Sentiment analysis; Google News	BIG DATA; DECISION-MAKING; DATA SCIENCE; SENTIMENT; SELECTION; ONTOLOGY; DESIGN; ALLOCATION; MODEL	Nowadays global supply chains enable companies to enhance competitive advantages, increase manufacturing flexibility and reduce costs through a broader selection of suppliers. Despite these benefits, however, insufficient understanding of uncertain regional differences and changes often increases risks in supply chain operations and even leads to a complete disruption of a supply chain. This paper addresses this issue by proposing a text-mining based global supply chain risk management framework involving two phases. First, the extant literature about global supply chain risks was collected and analyzed using a text-based approaches, including term frequency, correlation, and bi-gram analysis. The results of these analyses revealed whether the term-related content is important in the studied literature, and correlated topic model clustering further assisted in defining potential supply chain risk factors. A risk categorization (hierarchy) containing a total of seven global supply chain risk types and underlying risk factors was developed based on the results. In the second phase, utilizing these risk factors, sentiment analysis was conducted on online news articles, selected according to the specific type of risk, to recognize the pattern of risk variation. The risk hierarchy and sentiment analysis results can improve the understanding of regional global supply chain risks and provide guidance in supplier selection.																	1474-0346	1873-5320				AUG	2020	45								101053	10.1016/j.aei.2020.101053													
J								Integrating a constraint-based optimization approach into the design of oil & gas structures	ADVANCED ENGINEERING INFORMATICS										Constraint satisfaction problem; Design optimization; Steel structures; Engineer-to-order; Oil & gas	PRODUCT CONFIGURATION; ORDER; ENGINEER; SPACE	Currently, design optimization is widely applied in civil and mechanical engineering. Optimization strategies are used to enhance the product performance and reduce the cost, lead time and environmental impacts related to the product lifecycle. In this context, evolutionary algorithms are used for determining the optimum solution in engineering problems. The design of complex products, such as those that are engineered to order, often requires the study of subproblems. Modularization is a common practice to reduce the complexity; however, the configuration practices are difficult to be applied in engineered to order products. As a solution, the integration of the optimization tools and model-based simulations is proposed to manage the complexity. However, even when a commercial software is available to support the parameter optimization, there may exist a lack of design tools that can be integrated with the product structure of an engineered to order product. This paper describes a design optimization approach that integrates a Constraint Satisfaction Problem (CSP) tool with model-based simulations in a collaborative design context. A platform tool is developed using the .NET and MiniZinc languages. The case study is focused on the design optimization of a 700-ton steel structure. In particular, the optimization analysis considers the mechanical behavior, weight, and cost reduction.																	1474-0346	1873-5320				AUG	2020	45								101129	10.1016/j.aei.2020.101129													
J								A field implementation of linear prediction for leak-monitoring in water distribution networks	ADVANCED ENGINEERING INFORMATICS										Leak detection; Water distribution networks; Linear prediction; Acoustic signals	ACOUSTIC-EMISSION; SENSOR; MODEL; PIPES; NOISE	Water distribution networks (WDNs) are complex systems that are subjected to stresses due to a number of hydraulic and environmental loads. As a result, system leaks remain an unavoidable reality. Leaks which are not large enough to become visible at the street level can often go undetected for prolonged periods of time; the presence of smaller leaks can be concealed in system variability. The current paper addresses the problem of leak-detection and localization in WDNs, using a data-driven methodology which utilizes linear prediction (LP) theory. LP has a relatively simple mathematical formulation and has been shown in laboratory studies to effectively capture leak-induced signatures in fluid-filled pipes. In this paper, the performance of LP for leak-detection is verified, using field data in an operational WDN. In addition, a two-part localization approach is proposed which utilizes LP pre-processed data, in tandem with the traditional cross-correlation approach. Results of the field study show that the proposed method is able to perform both leak-detection and localization in full-scale systems using relatively short time signal lengths. This is advantageous in continuous monitoring situations as this minimizes data transmission requirements, which are one of the main impediments to full-scale implementation and deployment of leak-detection technology. In addition to the analysis results, a novel hydrant-mounted data-acquisition system is proposed, along with its unique hardware and software capabilities.																	1474-0346	1873-5320				AUG	2020	45								101103	10.1016/j.aei.2020.101103													
J								Design entropy theory: A new design methodology for smart PSS development	ADVANCED ENGINEERING INFORMATICS										Design theory; Smart product-service systems; Information theory; Design entropy; Value co-creation; User experience	PRODUCT-SERVICE-SYSTEMS; FRAMEWORK; INNOVATION	Smart product-service systems (Smart PSS), as an emerging digital servitization paradigm, leverages smart, connected products and their generated services as a solution bundle to meet individual customer needs. Owing to the advanced information and communication technologies, Smart PSS development differs from the existing product and/or service design mainly in three aspects: (1) closed-loop design/redesign iteration; (2) value co-creation in the context; and (3) design with context-awareness. These unique characteristics bring up new design challenges, and to the authors' best knowledge, none of the existing design theories can address them well. Aiming to fill this gap, a novel design methodology for the Smart PSS development is proposed based on the information theory, where both the system and stakeholders can be regarded as the information containers. Hence, the closed-loop design/redesign iteration can be treated as the dynamic change of information and entropy in a balanced ecosystem. Meanwhile, the value co-creation process is considered as the exchange of accumulated information via the container. Lastly, the design context-awareness represents the process of reducing entropy. As a novel prescriptive design theory, it follows Shannon's information theory to determine the best design/redesign solutions by considering the three characteristics integrally. It is hoped that the proposed design entropy theory can largely facilitate today's Smart PSS development with better performance and user satisfaction.																	1474-0346	1873-5320				AUG	2020	45								101124	10.1016/j.aei.2020.101124													
J								Automated segmentation of RGB-D images into a comprehensive set of building components using deep learning	ADVANCED ENGINEERING INFORMATICS										Building information modeling; Semantic segmentation; Deep learning; Class balancing; RGB-D; 3DFacilities	POINT CLOUDS; RECONSTRUCTION; MODELS; BIM; INTERIORS; ELEMENTS; OBJECT	Building information modeling (BIM) has a semantic scope that encompasses all building systems, e.g. architectural, structural, mechanical, electrical, and plumbing. Automated, comprehensive digital modeling of buildings will require methods for semantic segmentation of images and 3D reconstructions capable of recognizing all building component classes. However, prior building component recognition methods have had limited semantic coverage and are not easily combined or scaled. Here we show that a deep neural network can semantically segment RGB-D (i.e. color and depth) images into 13 building component classes simultaneously despite the use of a small training dataset with only 1490 object instances. For this task, the method achieves an average intersection over union (IoU) of 0.5. The dataset was designed using a common building taxonomy to ensure comprehensive semantic coverage and was collected from a diversity of buildings to ensure infra-class diversity. As a consequence of its semantic scope, it was necessary to perform pre-segmentation and 3D to 2D projection as leverage for dataset annotation. In creating our deep learning pipeline, we found that transfer learning, class balancing, and prevention of overfitting effectively overcame the dataset's borderline adequate class representation. Our results demonstrate how the semantic coverage of a building component recognition method can be scaled to include a larger diversity of building systems. We anticipate our method to be a starting point for broadening the scope of the semantic segmentation methods involved in digital modeling of buildings.																	1474-0346	1873-5320				AUG	2020	45								101131	10.1016/j.aei.2020.101131													
J								Structured authoring for AR-based communication to enhance efficiency in remote diagnosis for complex equipment	ADVANCED ENGINEERING INFORMATICS										Augmented reality; Maintenance; Diagnosis; Remote collaboration	AUGMENTED-REALITY; VIRTUAL ENVIRONMENTS; DESIGN; COLLABORATION; MAINTENANCE; INFORMATION; TECHNOLOGY; INDUSTRY	Remote diagnosis procedures are prone to communication errors due to varying levels of experience and knowledge between expert maintainers and technicians. These result in inefficiencies that delay the diagnosis process. The aim of the paper is to develop a Structured-Message Authoring framework for Augmented Reality (AR) Remote Communication (SMAARRC) and to evaluate its ability to enhance the efficiency of remote diagnosis services. The framework proposes a message structure and automatic AR content creation rules for it that enable data capture and sharing within a remote context. Laboratory experiments present an average time reduction of 56% for remote calls while maintaining same quality compared to traditional remote communication methods (phone calls and emails). Remote experts feedback evidence the usability and feasibility of this framework to work in real-life conditions.																	1474-0346	1873-5320				AUG	2020	45								101096	10.1016/j.aei.2020.101096													
J								A research agenda for augmented and virtual reality in architecture, engineering and construction	ADVANCED ENGINEERING INFORMATICS										Augmented reality; Virtual reality; Construction; Immersive technologies; Mixed reality; Visualisation	BUILDING INFORMATION; SITUATION AWARENESS; USER-INTERFACE; THINGS IOT; SAFETY; VISUALIZATION; MANAGEMENT; BIM; SIMULATION; FRAMEWORK	This paper presents a study on the usage landscape of augmented reality (AR) and virtual reality (VR) in the architecture, engineering and construction sectors, and proposes a research agenda to address the existing gaps in required capabilities. A series of exploratory workshops and questionnaires were conducted with the participation of 54 experts from 36 organisations from industry and academia. Based on the data collected from the workshops, six AR and VR use-cases were defined: stakeholder engagement, design support, design review, construction support, operations and management support, and training. Three main research categories for a future research agenda have been proposed, i.e.: (i) engineering-grade devices, which encompasses research that enables robust devices that can be used in practice, e.g. the rough and complex conditions of construction sites; (ii) workflow and data management; to effectively manage data and processes required by AR and VR technologies; and (iii) new capabilities; which includes new research required that will add new features that are necessary for the specific construction industry demands. This study provides essential information for practitioners to inform adoption decisions. To researchers, it provides a research road map to inform their future research efforts. This is a foundational study that formalises and categorises the existing usage of AR and VR in the construction industry and provides a roadmap to guide future research efforts.																	1474-0346	1873-5320				AUG	2020	45								101122	10.1016/j.aei.2020.101122													
J								Fault-tolerant control of variable speed limits for freeway work zone using likelihood estimation	ADVANCED ENGINEERING INFORMATICS										Freeway work zone; Variable speed limits; Fault-tolerant control; Likelihood estimation	LOOP DETECTOR DATA; MISSING DATA; WAVES; MODEL	Freeway work zone with lane closure can lead to disruption to local traffic and cause significant impacts on mobility, safety and environmental sustainability. To mitigate traffic congestion near work zone area, many variable speed limits (VSL) control approaches have been developed. However, VSL control system, as a critical transportation management system, is prone to the occurrence of traffic sensor faults. Faulty sensors can cause great deviations of traffic measurements and system degradation. Therefore, this study aims to develop a fault-tolerant VSL control strategy for freeway work zone with the consideration of the mainline sensor fault and ramp sensor fault. To analyze the traffic dynamics near work zone area, a traffic flow model has been built first. Then a sliding mode controller in the previous study has been utilized for VSL control. In addition to the traffic states estimated by a Kalman filter, two observers have been developed to provide analytical redundancy of traffic states estimation. By comparing the logarithm of the likelihood estimations from the Kalman filter and two observers, a fault diagnosis scheme has been designed to detect and identify the faults of mainline sensors and ramp sensors. Then the VSL controller can be reconfigured accordingly in case of sensor faults. The proposed system is implemented and evaluated under a realistic freeway work zone environment using traffic simulator SUMO. The results demonstrate that the developed system can accurately detect and identify the sensor faults in real time. Consistent improvements of mobility, safety and sustainability are also achieved under fault-free and sensor faults scenarios.																	1474-0346	1873-5320				AUG	2020	45								101133	10.1016/j.aei.2020.101133													
J								Failure mode classification and bearing capacity prediction for reinforced concrete columns based on ensemble machine learning algorithm	ADVANCED ENGINEERING INFORMATICS										Machine learning; Ensemble learning; Adaptive boosting; Classification; Regression; Reinforced concrete; Column; Flexure; Shear	SUPPORT VECTOR MACHINE; SELF-COMPACTING CONCRETE; COMPRESSION-FIELD-THEORY; SOFTENED TRUSS MODEL; SHEAR-STRENGTH; OPTIMIZATION; ADABOOST.RT; PERFORMANCE; METAMODEL; MEMBERS	Failure mode (FM) and bearing capacity of reinforced concrete (RC) columns are key concerns in structural design and/or performance assessment procedures. The failure types, i.e., flexure, shear, or mix of the above two, will greatly affect the capacity and ductility of the structure. Meanwhile, the design methodologies for structures of different failure types will be totally different. Therefore, developing efficient and reliable methods to identify the FM and predict the corresponding capacity is of special importance for structural design/assessment management. In this paper, an intelligent approach is presented for FM classification and bearing capacity prediction of RC columns based on the ensemble machine learning techniques. The most typical ensemble learning method, adaptive boosting (AdaBoost) algorithm, is adopted for both classification and regression (prediction) problems. Totally 254 cyclic loading tests of RC columns are collected. The geometric dimensions, reinforcing details, material properties are set as the input variables, while the failure types (for classification problem) and peak capacity forces (for regression problem) are set as the output variables. The results indicate that the model generated by the AdaBoost learning algorithm has a very high accuracy for both FM classification (accuracy = 0.96) and capacity prediction (R-2 = 0.98). Different learning algorithms are also compared and the results show that ensemble learning (especially AdaBoost) has better performance than single learning. In addition, the bearing capacity predicted by the AdaBoost is also compared to that by the empirical formulas provided by the design codes, which shows an obvious superior of the proposed method. In summary, the machine learning technique, especially the ensemble learning, can provide an alternate to the conventional mechanics-driven models in structural design in this big data time.																	1474-0346	1873-5320				AUG	2020	45								101126	10.1016/j.aei.2020.101126													
J								An immersive virtual reality serious game to enhance earthquake behavioral responses and post-earthquake evacuation preparedness in buildings	ADVANCED ENGINEERING INFORMATICS										Immersive virtual reality; Serious games; Earthquake emergencies; Evacuation training	SELF-EFFICACY; SIMULATION; EDUCATION; VS.	Enhancing the earthquake behavioral responses and post-earthquake evacuation preparedness of building occupants is beneficial to increasing their chances of survival and reducing casualties after the mainshock of an earthquake. Traditionally, training approaches such as seminars, posters, videos or drills are applied to enhance preparedness. However, they are not highly engaging and have limited sensory capabilities to mimic life-threatening scenarios for the purpose of training potential participants. Immersive Virtual Reality (IVR) and Serious Games (SG) as innovative digital technologies can be used to create training tools to overcome these limitations. In this study, we propose an IVR SG-based training system to improve earthquake behavioral responses and post-earthquake evacuation preparedness. Auckland City Hospital was chosen as a case study to test our IVR SG training system. A set of training objectives based on best evacuation practice has been identified and embedded into several training scenarios of the IVR SG. Hospital staff (healthcare and administrative professionals) and visitors were recruited as participants to be exposed to these training scenarios. Participants' preparedness has been measured along two dimensions: 1) Knowledge about best evacuation practice; 2) Self-efficacy in dealing with earthquake emergencies. Assessment results showed that there was a significant knowledge and self-efficacy increase after the training. In addition, participants acknowledged that it was easy, helpful, and engaging to learn best evacuation practice knowledge through the IVR SG training system.																	1474-0346	1873-5320				AUG	2020	45								101118	10.1016/j.aei.2020.101118													
J								Do you need a blockchain in construction? Use case categories and decision framework for DLT design options	ADVANCED ENGINEERING INFORMATICS										Construction industry; Construction automation; Blockchain; Distributed Ledger Technology (DLT); Smart contracts	TECHNOLOGY	Blockchain and other forms of Distributed Ledger Technology (DLT) provide an opportunity to integrate digital information, management, and contracts to increase trust and collaboration within the construction industry. DLT enables direct peer-to-peer transactions of value across a distributed network by providing an immutable and transparent record of these transactions. Furthermore, there is potential for business process optimization and automation on the transaction level through the use of smart contracts, which are code protocols deployed on supported DLT systems. However, DLT research in the construction industry remains at a theoretical level; there have been few implementation case studies to date. One potential reason for this is a knowledge gap between use-case ideas and the DLT technical system implementation. This paper aims to reduce this gap by (1) reviewing and categorizing proposed DLT use cases in construction literature, (2) providing an overview of DLT and its design options, (3) proposing an integrated framework to match DLT design options with desired characteristics of a use case, and (4) analysing the use cases using the new framework. Together, the use case categories and proposed decision framework can guide future implementers toward more connected and structured thinking between the technological properties of DLT and use cases in construction.																	1474-0346	1873-5320				AUG	2020	45								101094	10.1016/j.aei.2020.101094													
J								A rule-based method for automated surrogate model selection	ADVANCED ENGINEERING INFORMATICS										Surrogate modeling; Automated surrogate model selection; Rule-based method; Metamodeling	METAMODELING TECHNIQUES; ENGINEERING DESIGN; CROSS-VALIDATION; SUPPLY CHAIN; OPTIMIZATION; ENERGY; INFORMATION; MANAGEMENT; AUSTENITE; FRAMEWORK	Surrogate models have been widely used in engineering design because of their capability to approximate computationally complex engineering systems. In practice, the choice of surrogate models is extremely important since there are many types of surrogate models, and they also have different hyper-parameters. Traditional manual selection approaches are very time-consuming and cannot be generalized. To address these challenges, an evolutionary algorithm (EA)-based approaches are proposed and studied. However, they lack interpretability and are computationally expensive. To address these gaps, we create a rule-based method for an automatic surrogate model selection called AutoSM. The drastic increase in the selection pace by pre-screening of surrogate model types based on selection rule extraction is the scientific contribution of our proposed method. First, an interpretable decision tree is built to map four critical features, including problem scale, noise, size of sample and nonlinearity, to the types of surrogate model and select the promising surrogate model; then, a genetic algorithm (GA) is used to find the appropriate hyper-parameters for each selected surrogate model. The AutoSM is tested with three theoretical problems and two engineering problems, including a hot rod rolling and a blowpipe design problem. According to the empirical results, using the proposed AutoSM, we can find the promising surrogate model and associated hyper-parameter in 9 times less than other automatic selection approaches such as concurrent surrogate model selection (COSMOS) while maintaining the same accuracy and robustness in surrogate model selection. Besides, the proposed AutoSM, unlike previous EA-based automatic surrogate model selection methods, is not a black box and is interpretable.																	1474-0346	1873-5320				AUG	2020	45								101123	10.1016/j.aei.2020.101123													
J								Beyond BCI-Validating a wireless, consumer-grade EEG headset against a medical-grade system for evaluating EEG effects of a test anxiety intervention in school	ADVANCED ENGINEERING INFORMATICS										Attention; Interference; Deep breathing; EEG; Intervention; Test anxiety	STATE ANXIETY; PERFORMANCE; CHILDREN; OSCILLATIONS; ADOLESCENTS; EFFICIENCY; RESPONSES; ALPHA	Educational neuroscience is an emerging interdisciplinary field. However, the use of neuroimaging techniques and tools such as electroencephalography (EEG) in school-based interventions and research is limited, largely due to the high costs and physical constraints of conventional research- or medical-grade equipment. Neuroimaging and electrophysiological data can provide useful evidence to validate the efficacy of interventions. The present study explores the utility of lightweight, affordable, and easy-to-set-up EEG systems for use in school-based research with children. Specifically, we examine the effects of a deep-breathing-for-test-anxiety intervention on brain electrical activity during a flanker distractor interference task in eleven-year olds, comparing the pattern of results observed using a consumer-grade EEG system (Emotiv EPOC +) against that obtained using a medical-grade EEG system (Neurostyle). Behavioral, EEG, and respiratory data was obtained from Primary 5 students (N = 45; M-age = 10.88, SD = 0.33), split into Emotiv and Neurostyle groups. The aim of the study was two-fold: to examine the effects of deep breathing on neurophysiological and behavioral correlates of inhibitory control of attention in children, and to understand the affordances and limitations of the Emotiv EPOC + system for school-based research with children. Results from power spectral analyses suggest that deep breathing may enhance attentional control on a neural level by modulating brain electrical activity on several frequencies. Despite limitations, the consumer-grade EEG system appears to be capable of detecting some degree of power spectral differences associated with intervention effects.																	1474-0346	1873-5320				AUG	2020	45								101106	10.1016/j.aei.2020.101106													
J								Customer requirement-driven design method and computer-aided design system for supporting service innovation conceptualization handling	ADVANCED ENGINEERING INFORMATICS										Requirement-driven design; Computer-aided service design; Abductive logic; Service innovation	PRODUCT-SERVICE; TRIZ; QFD; SELECTION; THINKING	How to create value to meet customers' requirements by effectively using advanced digital technology (DT) for digital transformation is an implementation challenge for new and future scenarios of current contexts. Valid digital transformation issues need to be considered to deliver novel values with sustainability concerns. Attempting to effectively achieve a successful service system design in an industrial context, a holistic customer requirement-driven service design method with abductive logic was proposed to drive self-service productivity enhancement. Laddering theory, ontology-based design knowledge hierarchy (DKH), theory of inventive problem solving (TRIZ) and quality function deployment (QFD) technique are adopted in terms of effective design knowledge handling in innovation conceptualization with a traceable path and abductive logic linking from customer concerns, business context and technology digitization. Then, a comprehensive case study of an empirical smart meal-ordering service system for the digital transformation of canteen processes was illustrated to verify this approach.																	1474-0346	1873-5320				AUG	2020	45								101117	10.1016/j.aei.2020.101117													
J								An integrated online pick-to-sort order batching approach for managing frequent arrivals of B2B e-commerce orders under both fixed and variable time-window batching	ADVANCED ENGINEERING INFORMATICS										E-commerce order handling; Online order batching; Real time operational decision making; Artificial intelligence; Pick-to-sort systems; Warehouse management	SYSTEM; CLASSIFICATION; STRATEGY	The emergence of e-commerce marketplace, especially the business-to-business (B2B) e-commerce sector, has created a vast market opportunity for retailers and logistics service providers (LSPs). However, facing the structural changes in B2B logistics orders, without any transformation of logistics operating process in distribution centres, LSPs are experiencing serious challenges in handling B2B e-commerce orders. In order for LSPs to grasp the market pie of the logistics and distribution sector of the e-commerce business through improving the core capability of handling B2B e-commerce order in their distribution centres, this paper proposes a novel approach for LSPs to systematically execute B2B order pre-processing so that the B2B e-commerce orders are better managed. An intelligent B2B order handling system (IOHS) is developed for efficient management of discrete, frequently arrived B2B e-commerce orders. By applying the "warehouse postponement" concept through the development of IOHS integrating cloud database management, fuzzy logic, and genetic algorithm approach, B2B order grouping solutions are effectively generated. A case study is conducted in a Hong Kong-based logistics service provider who handles outsourced B2B e-commerce orders, indicating the feasibility of the proposed solution to facilitate e-order fulfilment under both fixed and variable time-window batching settings in distribution centres. A significant improvement in terms of the throughput rate of handling B2B e-commerce order in the distribution centre is found, thereby increasing the satisfaction of buyers and sellers throughout the order handling and delivery process, as well as facilitating LSPs' capability to manage a larger number of orders received from B2B e-commerce platform, given the greater product exposure of sellers in today's B2B e-business.																	1474-0346	1873-5320				AUG	2020	45								101125	10.1016/j.aei.2020.101125													
J								Real-time smart video surveillance to manage safety: A case study of a transport mega-project	ADVANCED ENGINEERING INFORMATICS										Computer vision; Construction safety; Video surveillance; Stuck-by accident	DEEP LEARNING APPROACH; NEURAL-NETWORKS; CONSTRUCTION; EQUIPMENT; WORKERS; SYSTEM; MODEL; EXCAVATORS; FALLS	There is a tendency for accidents and even fatalities to arise when people enter hazardous work areas during the construction of projects in urban areas. A limited amount of research has been devoted to developing vision-based proximity warning systems that can determine when people enter a hazardous area automatically. Such systems, however, are unable to identify specific hazards and the status of a piece of plant (e.g., excavator) in real-time. In this paper, we address this limitation and develop a real-time smart video surveillance system that can detect people and the status of plant (i.e. moving or stationary) in a hazardous area. The application of this approach is demonstrated during the construction of a mega-project, the Wuhan Rail Transit System in China. We reveal that our combination of computer vision and deep learning can accurately recognize people in a hazardous work area in real-time during the construction of transport projects. Our developed systems can provide instant feedback concerning unsafe behavior and thus enable appropriate actions to be put in place to prevent their re-occurrence.																	1474-0346	1873-5320				AUG	2020	45								101100	10.1016/j.aei.2020.101100													
J								New hybrid data mining model for credit scoring based on feature selection algorithm and ensemble classifiers	ADVANCED ENGINEERING INFORMATICS										Credit scoring; Data mining; Ensemble classifier; Feature selection; Hybrid model		The aim of this paper is to propose a new hybrid data mining model based on combination of various feature selection and ensemble learning classification algorithms, in order to support decision making process. The model is built through several stages. In the first stage, initial dataset is preprocessed and apart of applying different preprocessing techniques, we paid a great attention to the feature selection. Five different feature selection algorithms were applied and their results, based on ROC and accuracy measures of logistic regression algorithm, were combined based on different voting types. We also proposed a new voting method, called if-any, that outperformed all other voting methods, as well as a single feature selection algorithm's results. In the next stage, a four different classification algorithms, including generalized linear model, support vector machine, naive Bayes and decision tree, were performed based on dataset obtained in the feature selection process. These classifiers were combined in eight different ensemble models using soft voting method. Using the real dataset, the experimental results show that hybrid model that is based on features selected by if-any voting method and ensemble GLM + DT model performs the highest performance and outperforms all other ensemble and single classifier models.																	1474-0346	1873-5320				AUG	2020	45								101130	10.1016/j.aei.2020.101130													
J								A new integrated approach for engineering characteristic prioritization in quality function deployment	ADVANCED ENGINEERING INFORMATICS										Product development; Quality function deployment; Picture fuzzy linguistic set; EDAS method; Product-service system	LINGUISTIC TERM SETS; AGGREGATION OPERATORS; FUZZY; SUSTAINABILITY; RANKING; MODEL; QFD	As a customer-driven quality improvement tool, quality function deployment (QFD) can convert customer requirements (CRs) into appropriate engineering characteristics (ECs) in product design and development. However, the conventional QFD method has been criticized for a variety of drawbacks, which limit its efficiency and potential applications. In this study, a new QFD approach integrating picture fuzzy linguistic sets (PFLSs) and the evaluation based on distance from average solution (EDAS) method is proposed for the determination of ranking order of ECs. The PFLSs are utilized to express the judgements of experts on the relationships among CRs and ECs. Then, the EDAS method is extended under picture fuzzy linguistic environment for the prioritization of the ECs identified in QFD. Moreover, a combined weighing method based on technique for order of preference by similarity to ideal solution (TOPSIS) and maximum entropy theory is established to calculate the weights of experts objectively. Finally, a product-service system design is provided to illustrate the effectiveness of the proposed QFD approach. The result shows that the manufacturer should pay more attention to "Meantime before failure", "Warning feature" and "Quality of product manual". Feedback from domain experts indicates that the integrated approach being proposed in this paper is more suitable for assessing and prioritizing ECs in QFD.																	1474-0346	1873-5320				AUG	2020	45								101099	10.1016/j.aei.2020.101099													
J								Multi-material structural topology optimization with decision making of stiffness design criteria	ADVANCED ENGINEERING INFORMATICS										Computational design optimization, design decision-making; Multi-material topology optimization; Material combination selection; Multi-material design	AUTOMATED DESIGN; SELECTION; REINFORCEMENT; COMPONENTS; MODEL; STEP	A new methodology for making design decisions of structures using multi-material optimum topology information is presented. Multi-material analysis contributes significant applications to enhance the bearing capacity and performance of structures. A method that chooses an appropriate material combination satisfying design stiffness requirement economically is currently needed. An alternative method of making design-decision is to utilize a multi-material topology optimization (MMTO) approach. This study provides a new computational design optimization procedure as a guideline to find the optimal multi-material design by considering structure strain energy and material cost. The MMTO problem is analyzed using an alternative active-phase approach. The procedure consists of three design steps. First, steel grid configurations and composite with material properties are defined as a given structure for automatic design decision-making (DDM). And then design criteria of the steel composites structure is given to be limited strain energy by designers and engineers. Second, topology changes in the automatic distribution of multi-steel materials combination and volume control of each material during optimization procedures are achieved and at the same time, their converged minimal strain energy is produced for each material combination. And third, the strain energy and material cost which is computed based on the material ratio in the combinations are used as design decision parameters. A study in constructional steel composites to produce optimal and economical multi-material designs demonstrates the efficiency of the present DDM methodology.																	1474-0346	1873-5320				AUG	2020	45								101098	10.1016/j.aei.2020.101098													
J								Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing	ADVANCED ENGINEERING INFORMATICS										Edge Cloud Computing; Machine Learning; Quality inspection; Quality prediction; Manufacturing	ARTIFICIAL NEURAL-NETWORKS; VIRTUAL METROLOGY SYSTEM; SOFT SENSOR; DESIGN; IMPROVEMENT; ARCHITECTURES; DIAGNOSIS; INTERNET; THINGS	The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks. In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.																	1474-0346	1873-5320				AUG	2020	45								101101	10.1016/j.aei.2020.101101													
J								Intelligent trademark similarity analysis of image, spelling, and phonetic features using machine learning methodologies	ADVANCED ENGINEERING INFORMATICS										Convolutional neural network; Siamese neural network; Trademark similarity assessment; Trademark (TM) infringement; Vector space model	RETRIEVAL; PATTERN	The rapid development of consumer products with short life spans, along with fast, global e-commerce and e-marketing distribution of products and services requires greater due diligence to protect intangible assets such as brands and corporate logos which can easily be copied or distributed through grey channels and internet sales sites. Trademarks (TMs) are government registered intellectual property rights (IPRs) used to legally protect a companies' identities and brand equity. The rapid growth of global trademark (TM) registrations and the number of TM infringement cases pose a great challenge for TM owners to detect infringement and take action to protect TMs, consumer trust, and market share. This research develops advanced TM similarity assessment models using machine learning (ML) approaches. Litigation principles over similarity follow US TM laws which are consistent with global TM protection convention under the World Intellectual Property Organization (WIPO). This research covers the similarity analysis of TM spelling, pronunciation, and images, which are most likely to cause TM confusion among customers. The research focuses on deploying machine learning for natural language (spelling and phonetic features) and image similarity analyses. The vector space modeling algorithms are trained and verified for the similarity analysis of TM wordings in both spelling and pronunciation. The convolutional neural network and Siamese neural network models are trained and verified for TM image similarity comparison. The training and testing sets consist of 250,000 and 20,000 different image pairs respectively. This research provides a significant contribution toward implementing intelligent and automated IPR protection. The system solution supports users (companies, TM attorneys, or IP officers) to identify similar registered TMs before registering new TMs ensuring uniqueness to avoid infringement disputes. The solution also supports automatic screening of online content to detect potential infringement of TM images and wording for effective global IPR protection.																	1474-0346	1873-5320				AUG	2020	45								101120	10.1016/j.aei.2020.101120													
J								Prediction of product design decision Making: An investigation of eye movements and EEG features	ADVANCED ENGINEERING INFORMATICS										Eye movements; EEG; Design decision making; Multimodal fusion	TRACKING; MANAGEMENT; KNOWLEDGE; USERS	Design decision making is happened in every design node and iteration, and the expert decision-making bias and personal preference will ultimately affect the success or failure of the product reaching the market. In this paper, we try to predict the design decision making by investigating the relations between design decision making and subjects' eye movements and Electroencephalogram(EEG) response. Four different methods were applied and compared to classify the different EEG features and two methods were used for EEG feature selection to correspond the design decision making results. In this study, the authors applied a multimodal fusion strategy for design decision making recognition where the authors used eye tracking and EEG response data as input dataset. According to the experiment results, the performance of the fusion strategy combined with EEG signals and eye movement characteristics is well in fitting the expert decision making results. The multimodal fusion combining eye tracking data and EEG has a strong potential to be a new design decision method to guide the design practice and provide supportive and objective data to reduce the effects of subjectivity, one-sidedness and superficiality in decision making. These results show that it is possible to create a classifier based on features extracted from eye movements and EEG response for the design decision making behaviour.																	1474-0346	1873-5320				AUG	2020	45								101095	10.1016/j.aei.2020.101095													
J								Evolutionary design framework for Smart PSS: Service engineering approach	ADVANCED ENGINEERING INFORMATICS										Service engineering; Smart PSS; Digitalization; Evolutionary design framework	OF-THE-ART; LIFE-CYCLE; SYSTEMS; INNOVATION; CREATION; CAD	Digital technology is transforming industry, economy, and society. Under this digitalization trend, a new concept of product and service offerings, Smart PSS has been introduced. While the Smart PSS concept aiming at datadriven innovation attracts more attentions, there are two major challenges in designing Smart PSS. First, available data from digital technologies are actually limited by technical, financial, and social reasons. Second, the context of applying Smart PSS dynamically changes, which makes designed Smart PSS unfit to the target situation. To overcome these challenges, we propose an evolutionary design framework for Smart PSS. Our approach is based on the service engineering research, which has been contributing to the design and development of PSS. The proposed framework consists of three conceptual spaces and three cycles. The main features of the framework were two design cycles; in-system and ex-system design cycle. In-system design cycle is a process of creating and applying human knowledge for making the most use of data collected by digital technologies. Ex-system design cycle is to adapt to dynamic changes in contexts of the applied Smart PSS. We examine how the proposed framework works for designing a Smart PSS, with a case study about the digitalization of a restaurant business.																	1474-0346	1873-5320				AUG	2020	45								101119	10.1016/j.aei.2020.101119													
J								A knowledge-based process planning framework for wire arc additive manufacturing	ADVANCED ENGINEERING INFORMATICS										Process planning; Directed energy deposition; Knowledge-based engineering; Design for additive manufacturing; Computer-aided design	COMPONENTS; MODEL	Wire arc additive manufacturing (WAAM) provides a rapid and cost-effective solution for fabricating low-to-medium complexity and medium-to-large size metal parts. In WAAM, process settings are well-recognized as fundamental factors that determine the performance of the fabricated parts such as geometry accuracy and microstructure. However, decision-making on process variables for WAAM still heavily relies on knowledge from domain experts. For achieving reliable and automated production, process planning systems that can capture, store, and reuse knowledge are needed. This study proposes a process planning framework by integrating a WAAM knowledge base together with our in-house developed computer-aided tools. The knowledge base is construed with a data-knowledge-service structure to incorporate various data and knowledge including metamodels and planning rules. Process configurations are generated from the knowledge base and then used as inputs to computer-aided tools. Moreover, the process planning system also supports the early-stage design of products in the context of design for additive manufacturing. The proposed framework is demonstrated in a digital workflow of fabricating industrial-grade components with overhang features.																	1474-0346	1873-5320				AUG	2020	45								101135	10.1016/j.aei.2020.101135													
J								Deep learning-based classification of work-related physical load levels in construction	ADVANCED ENGINEERING INFORMATICS										Ergonomics; Work-related musculoskeletal disorders; Physical loads; Long-short term memory; Deep learning; Wearable inertial measurement units	MUSCULOSKELETAL DISORDERS; SPATIOTEMPORAL PARAMETERS; BIDIRECTIONAL LSTM; PHONEME CLASSIFICATION; BIOMECHANICAL ANALYSIS; AUTOMATED DETECTION; GAIT ANALYSIS; RISK; RECOGNITION; SYSTEM	Work-related musculoskeletal disorders (WMSDs) are the leading cause of the nonfatal injuries for construction workers, and a worker's overexertion is a major source of such WMSDs. Pushing, pulling, and carrying movements-which are all activities largely associated with physical loads-account for 35% of WMSDs. However, most previous studies have focused on the identification of non-ergonomic postures, and there has been limited effort expended on measuring a worker's exposures to the physical loads caused by materials or tools during construction tasks. With the advantage of using a wearable inertial measurement sensor to monitor a worker's bodily movements, this study investigates the feasibility of identifying various physical loading conditions by analyzing a worker's lower body movements. In the experiment with laboratory settings, workers performed a load carrying task by moving concrete bricks. A bidirectional long short-term memory algorithm is employed to classify physical load levels; this approach achieved 74.6 to 98.6% accuracy and 0.59 to 0.99 F-score in classification. The results demonstrate the feasibility of the proposed approach in identifying the states of physical loads. The findings of this study contribute to the literature on classifying ergonomically at-risk workers and on preventing WMSDs in high physical demand occupations, thereby helping enhance the health and safety of the construction workplace.																	1474-0346	1873-5320				AUG	2020	45								101104	10.1016/j.aei.2020.101104													
J								Reinforcement learning based optimizer for improvement of predicting tunneling-induced ground responses	ADVANCED ENGINEERING INFORMATICS										Tunnel; Ground response; Reinforcement learning; Extreme learning machine; Optimization	ARTIFICIAL NEURAL-NETWORKS; SURFACE SETTLEMENTS; DEFORMATION; SENSITIVITY; MODEL; ANN; MACHINE	Prediction of ground responses is important for improving performance of tunneling. This study proposes a novel reinforcement learning (RL) based optimizer with the integration of deep-Q network (DQN) and particle swarm optimization (PSO). Such optimizer is used to improve the extreme learning machine (ELM) based tunneling-induced settlement prediction model. Herein, DQN-PSO optimizer is used to optimize the weights and biases of ELM. Based on the prescribed states, actions, rewards, rules and objective functions, DQN-PSO optimizer evaluates the rewards of actions at each step, thereby guides particles which action should be conducted and when should take this action. Such hybrid model is applied in a practical tunnel project. Regarding the search of global best weights and biases of ELM, the results indicate the DQN-PSO optimizer obviously outperforms conventional metaheuristic optimization algorithms with higher accuracy and lower computational cost. Meanwhile, this model can identify relationships among influential factors and ground responses through self-practicing. The ultimate model can be expressed with an explicit formulation and used to predict tunneling-induced ground response in real time, facilitating its application in engineering practice.																	1474-0346	1873-5320				AUG	2020	45								101097	10.1016/j.aei.2020.101097													
J								Performance of a Steady-State Visual Evoked Potential and Eye Gaze Hybrid Brain-Computer Interface on Participants With and Without a Brain Injury	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Electroencephalography; Visualization; Navigation; Brain injuries; Electrodes; Assistive technology; Collaboration; Brain-computer interface (BCI); brain injury (BI); data fusion; eye tracking; virtual environment	BCI; REAL	The brain-computer interface (BCI) and the tracking of eye gaze provide modalities for human-machine communication and control. In this article, we provide the evaluation of a collaborative BCI and eye gaze approach, known as a hybrid BCI. The combined inputs interact with a virtual environment to provide actuation according to a four-way menu system. The following two approaches are evaluated: first, steady-state visual evoked potential (SSVEP) BCI with on-screen stimulation; second, hybrid BCI, which combined eye gaze and SSVEP for navigation and selection. A study comprises participants without known brain injury (non-BI, N = 30) and participants with known brain injury (BI, N = 14). A total of 29 out of 30 non-BI participants can successfully control the hybrid BCI, while nine out of the 14 BI participants are able to achieve control, as evidenced by task completion. The hybrid BCI provides a mean accuracy of 99.84% in the cohort of non-BI participants and 99.14% in the cohort of BI participants. Information transfer rates are 24.41 bpm in non-BI participants and 15.87 bpm in BI participants. The research goal is to quantify usage of SSVEP and ET approaches in cohorts of non-BI and BI participants. The hybrid is the preferred interaction modality for most participants for both cohorts. When compared to non-BI participants, it is encouraging that nine out of 14 participants with known BI can use the hBCI technology with equivalent accuracy and efficiency, albeit with slower transfer rates.																	2168-2291	2168-2305				AUG.	2020	50	4					277	286		10.1109/THMS.2020.2983661													
J								A Usability Study of Low-Cost Wireless Brain-Computer Interface for Cursor Control Using Online Linear Model	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Electroencephalography; Brain modeling; Training; Usability; Kinematics; Controllability; Headphones; Brain-computer interface (BCI); cursor control; confounding variables; electroencephalogram (EEG); imagined body kinematics; usability	EEG; BCI; ARM; SYSTEM; P300	Computer cursor control using electroencephalogram (EEG) signals is a common and well-studied brain-computer interface (BCI). The emphasis of the literature has been primarily on evaluation of the objective measures of assistive BCIs such as accuracy of the neural decoder whereas the subjective measures such as user's satisfaction play an essential role for the overall success of a BCI. As far as we know, the BCI literature lacks a comprehensive evaluation of the usability of the mind-controlled computer cursor in terms of decoder efficiency (accuracy), user experience, and relevant confounding variables concerning the platform for the public use. To fill this gap, we conducted a 2-D EEG-based cursor control experiment among 28 healthy participants. The computer cursor velocity was controlled by the imagery of hand movement using a paradigm presented in the literature named imagined body kinematics with a low-cost wireless EEG headset. In this article, we evaluated the usability of the platform for different objective and subjective measures while we investigated the extent to which the training phase may influence the ultimate BCI outcome. We conducted pre- and post-BCI experiment interview questionnaires to evaluate the usability. Analyzing the questionnaires and the testing phase outcome shows a positive correlation between the individuals' ability of visualization and their level of mental controllability of the cursor. Despite individual differences, analyzing training data shows the significance of electrooculogram on the predictability of the linear model. The results of this work may provide useful insights towards designing a personalized user-centered assistive BCI.																	2168-2291	2168-2305				AUG.	2020	50	4					287	297		10.1109/THMS.2020.2983848													
J								Cross-Modality Matching for Evaluating User Experience of Emerging Mobile EEG Technology	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Electroencephalography; Mood; Headphones; Force; User experience; Usability; Electrodes; Dry sensors; electroencephalography (EEG); psychophysical methods; usability testing and evaluation; wearable devices		Emerging technology for brain-state monitoring offers the possibility to conduct measurements outside the laboratory. However, user-experience research is lacking. In this article, we present and test an approach for determining the development of user experience in the course of time using the so-called cross-modality matching (CMM). We conducted experiments with 24 subjects and evaluated seven mobile electroencephalography (EEG) devices. Using the CMM method, we registered the headset pressure of the EEG devices and subject's mood. We are able to identify a correlation between headset pressure and mood and to observe time trends. Subjects rated the heaviest, pin-based device as less comfortable in the course of time. The gel-based EEG cap is the most comfortable device regarding its long-time properties. The CMM approach for user-experience evaluation of new EEG technologies is direct, rapid, and easy to perform. This fact creates new opportunities for future studies in the field of user experience and human factors.																	2168-2291	2168-2305				AUG.	2020	50	4					298	305		10.1109/THMS.2020.2989380													
J								An Ego-Vision System for Discovering Human Joint Attention	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Videos; Task analysis; Visualization; Cameras; Image segmentation; Proposals; Noise measurement; Dataset; egocentric vision; graphical model; joint attention	CO-SEGMENTATION; GAZE	Joint attention often happens during social interactions, in which individuals share focus on the same object. This article proposes an egocentric vision-based system (ego-vision system) that aims to discover the objects looked at jointly by a group of persons engaged in interactive activities. The proposed system relies on a collection of wearable eye-tracking cameras that provide an egocentric view of the interaction scenes as well as points-of-gaze measurement of each participant. Technically in our system, we develop a hierarchical conditional random field (CRF) based graphical model that can temporally localize joint attention periods and spatially segment objects of joint attention. By solving these two coupled tasks together in an iterative optimization procedure, we show that human joint attention can be reliably discovered from videos even with cluttered background and noisy gaze measurement. A new dataset of joint attention is collected and annotated for evaluating the two tasks of joint attention where two to four persons are involved. Experimental results demonstrate that our approach achieves state-of-the-art performance on both tasks of spatial segmentation and temporal localization of joint attention.																	2168-2291	2168-2305				AUG.	2020	50	4					306	316		10.1109/THMS.2020.2965429													
J								Statistical Modeling of Visual Attention of Junior and Senior Anesthesiologists During the Induction of General Anesthesia in Real and Simulated Cases	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Visualization; Task analysis; Data models; Monitoring; Anesthesia; Predictive models; Mathematical model; Attention allocation; anesthesiology; salience; effort; expectancy; value (SEEV) model; simulation	SITUATION AWARENESS; EYE-TRACKING; PERFORMANCE; STRATEGIES	In visually rich working environments, it is important for operators to distribute their visual attention in an optimal fashion in order to operate safely. Computational models can provide a systematic method of investigating the attention distribution of humans. In this article, we reanalyze eye tracking data from anesthesiologists when inducing general anesthesia to test whether the so-called expectancy value version of the salience, effort, expectancy, value (SEEV) model can accommodate the visual attention distribution of anesthesiologists, and to investigate the effect of case (real versus simulated cases) and experience (junior versus senior) on the expectancy value model fit. The overall model fit is good (predicted-observed percentage dwell time correlation of 0.810, R-2 = 0.656). We observe that the model fit is better in simulated cases than real ones. In addition, the model fit is good for junior anesthesiologists independent of the case, but that there is an even better model fit in simulated cases than in real ones for senior anesthesiologists (case x experience interaction). Overall, the expectancy value model can be validated. However, at least within the context of anesthesiology, the full SEEV model may be needed to capture the large and distractive visual work environment of anesthesiologists. From a practical point of view, previous research suggested that anesthesiologists pay more attention to monitoring in simulated cases. However, the SEEV analysis suggests that anesthesiologists do not pay extra attention to monitoring equipment in simulated cases, but may not be able to pay enough attention to monitoring equipment in real cases.																	2168-2291	2168-2305				AUG.	2020	50	4					317	326		10.1109/THMS.2020.2983817													
J								Development and Validation of a Scenario-Based Drilling Simulator for Training and Evaluating Human Factors	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Training; Industries; Physics; Human factors; Personnel; Safety; Drilling; education; simulation-based learning (SBL); simulator; training		Drilling and completing an oil/gas well is a time-sensitive and high-value operation, in which environment/system parameters change in unseen, unpredictable environments. Safety issues arise at every stage. Drilling principles can be taught using traditional methods, but safety and event response are difficult to teach in such formats. Here, in this article, we integrate a hardware-in-the-loop simulator, downhole physics, and auxiliary touchscreen interfaces (similar to a rig's add-on equipment) to develop a realistic, real-time drilling simulator for well control operation training. Realistic operational data are supplied to the simulator representative of downhole operations, including unplanned well events. The well plan accounts for drilling parameter changes, the pore-pressure fracture-gradient drilling window, mud weights, etc., which occur in response to the unplanned events. The developed simulator is used for hands-on training, human factor studies, model verification, and evaluating new auxiliary equipment and/or operational procedures. A critical research objective was evaluating the accuracy/realism of the developed system. To do so, eight petroleum engineering students and 11 certified drillers were trained and asked to complete a comprehensive (>6 h) drilling operation. System accuracy was measured by comparing how new versus experienced operators learned to operate the simulator, execute mission-critical tasks, and respond to unplanned events. The results validate the realism of the developed simulator and scenarios, since personnel with prior drilling experience took significantly less time to master the system.																	2168-2291	2168-2305				AUG.	2020	50	4					327	336		10.1109/THMS.2020.2969014													
J								A Comparative Evaluation of a Virtual Reality Table and a HoloLens-Based Augmented Reality System for Anatomy Training	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Training; Cadaver; TV; Augmented reality; Biomedical optical imaging; Hardware; Anatomy; augmented reality; comparative study; Microsoft HoloLens; training; virtual reality (VR); VR table	REHABILITATION; INTERVENTIONS; SIMULATION	Anatomy training with real cadavers poses many practical problems for which new training and educational solutions have been developed making use of technologies based on real-time 3-D graphics. Although virtual reality (VR) and augmented reality (AR) have been previously used in the medical field, it is not easy to select the right 3-D technology or setup for each particular problem. For this reason, this article presents a comprehensive comparative study with 82 participants between two different 3-D interactive setups: an optical-based AR setup, implemented with a Microsoft HoloLens device, and a semi-immersive setup based on a VR Table. Both setups are tested using an anatomy training software application. Our primary hypothesis is that there would be statistically significant differences between the use of the AR application and the use of the VR Table. Our secondary hypothesis is that user preference and recommendation for the VR setup would be higher than for the HoloLens-based system. After completing two different tasks with both setups, the participants filled two questionnaires about the use of the anatomy training application. Three objective measures are also recorded (time, number of movements, and a score). The results of the experiments show that more than two-thirds of the users prefer, recommend, and find more useful the VR setup. The results also show that there are statistically significant differences in the use of both systems in favor of the VR Table.																	2168-2291	2168-2305				AUG.	2020	50	4					337	348		10.1109/THMS.2020.2984746													
J								The Effect of Room Complexity on Physical Object Selection Performance in 3-D Mobile User Interfaces	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Complexity theory; Task analysis; Navigation; User interfaces; Virtual environments; Performance evaluation; Control systems; Environment complexity; human-computer interaction (HCI); physical object selection; smart office; 3-D user interface (UI)	VIRTUAL ENVIRONMENTS	An important challenge in smart environments is how to manipulate the smart objects. Although mobile applications are typically used for controlling a smart environment, no previous study has evaluated the users performance in manipulating smart objects under different environmental complexities. This article presents an experimental comparison between three different selection techniques 3-D, 2-D, and physical user interfaces (UIs). We evaluate these techniques across two levels of environment complexity measuring 51 participants timing data and errors. Our results indicate that the 3-D UI is superior for task completion time and error, and the 2-D UI is not a better solution than the physical UI when the environment is not complex. The results also show the importance of considering the environment complexity in choosing the proper UI.																	2168-2291	2168-2305				AUG.	2020	50	4					349	357		10.1109/THMS.2020.2984750													
J								Deep learning-guided estimation of attenuation correction factors from time-of-flight PET emission data	MEDICAL IMAGE ANALYSIS										PET/CT; Attenuation correction; Machine learning; Deep learning; Quantification	CONVOLUTIONAL NEURAL-NETWORK; PSEUDO-CT IMAGES; ZERO-ECHO-TIME; GENERATION; HEAD	Purpose: Attenuation correction (AC) is essential for quantitative PET imaging. In the absence of concurrent CT scanning, for instance on hybrid PET/MRI systems or dedicated brain PET scanners, an accurate approach for synthetic CT generation is highly desired. In this work, a novel framework is proposed wherein attenuation correction factors (ACF) are estimated from time-of-flight (TOF) PET emission data using deep learning. Methods: In this approach, referred to as called DL-EM), the different TOF sinogram bins pertinent to the same slice are fed into a multi-input channel deep convolutional network to estimate a single ACF sinogram associated with the same slice. The clinical evaluation of the proposed DL-EM approach consisted of 68 clinical brain TOF PET/CT studies, where CT-based attenuation correction (CTAC) served as reference. A two-tissue class consisting of background-air and soft-tissue segmentation of the TOF PET non-AC images (SEG) as a proxy of the technique used in the clinic was also included in the comparative evaluation. Qualitative and quantitative PET analysis was performed through SUV bias maps quantification in 63 different brain regions. Results: The DL-EM approach resulted in 6.1 +/- 9.7% relative mean absolute error (RMAE) in bony structures compared to SEG AC method with RMAE of 16.1 +/- 8.2% (p-value <0.001). Considering the entire head region, DL-EM led to a root mean square error (RMSE) of 0.3 +/- 0.01 outperforming the SEG method with RMSE of 0.8 +/- 0.02 SUV (p-value <0.001). The region-wise analysis of brain PET studies revealed less than 7% absolute SUV bias for the DL-EM approach, whereas the SEG method resulted in more than 14% absolute SUV bias (p-value <0.05). Conclusions: Qualitative assessment and quantitative PET analysis demonstrated the superior performance of the DL-EM approach over the segmentation-based technique with clinically acceptable SUV bias. The results obtained using the DL-EM approach are comparable to state-of-the-art MRI-guided AC methods. Yet, this approach enables the extraction of interesting features about patient-specific attenuation which could be employed not only as a stand-alone AC approach but also as complementary/prior information in other AC algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101718	10.1016/j.media.2020.101718													
