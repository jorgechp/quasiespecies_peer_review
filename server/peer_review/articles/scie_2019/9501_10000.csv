PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Engineering fast multilevel support vector machines	MACHINE LEARNING										Classification; Support vector machine; Parameter fitting; Imbalanced learning; Hierarchical method; Multilevel method; PETSc	FEATURE-SELECTION; MODEL SELECTION; CLASSIFICATION; ALGORITHMS; PARAMETERS; CLASSIFIERS; LIBRARY; SEARCH	The computational complexity of solving nonlinear support vector machine (SVM) is prohibitive on large-scale data. In particular, this issue becomes very sensitive when the data represents additional difficulties such as highly imbalanced class sizes. Typically, nonlinear kernels produce significantly higher classification quality to linear kernels but introduce extra kernel and model parameters which requires computationally expensive fitting. This increases the quality but also reduces the performance dramatically. We introduce a generalized fast multilevel framework for regular and weighted SVM and discuss several versions of its algorithmic components that lead to a good trade-off between quality and time. Our framework is implemented using PETSc which allows an easy integration with scientific computing tasks. The experimental results demonstrate significant speed up compared to the state-of-the-art nonlinear SVM libraries.																	0885-6125	1573-0565				NOV	2019	108	11					1879	1917		10.1007/s10994-019-05800-7													
J								Asymptotically optimal algorithms for budgeted multiple play bandits	MACHINE LEARNING										Budgeted bandits; KL-UCB; Knapsack bandits; Multiple-play bandits; Thompson sampling	ALLOCATION	We study a generalization of the multi-armed bandit problem with multiple plays where there is a cost associated with pulling each arm and the agent has a budget at each time that dictates how much she can expect to spend. We derive an asymptotic regret lower bound for any uniformly efficient algorithm in our setting. We then study a variant of Thompson sampling for Bernoulli rewards and a variant of KL-UCB for both single-parameter exponential families and bounded, finitely supported rewards. We show these algorithms are asymptotically optimal, both in rate and leading problem-dependent constants, including in the thick margin setting where multiple arms fall on the decision boundary.																	0885-6125	1573-0565				NOV	2019	108	11					1919	1949		10.1007/s10994-019-05799-x													
J								Boosting as a kernel-based method	MACHINE LEARNING										Boosting; Weak learners; Kernel-based methods; Reproducing kernel Hilbert spaces; Robust estimation	MODEL SELECTION; REGRESSION; CLASSIFICATION; REGULARIZATION	Boosting combines weak (biased) learners to obtain effective learning algorithms for classification and prediction. In this paper, we showa connection between boosting and kernel-based methods, highlighting both theoretical and practical applications. In the l(2) context, we show that boosting with a weak learner defined by a kernel K is equivalent to estimation with a special boosting kernel. The number of boosting iterations can then be modeled as a continuous hyperparameter, and fit (along with other parameters) using standard techniques. We then generalize the boosting kernel to a broad new class of boosting approaches for general weak learners, including those based on the l(1), hinge and Vapnik losses. We develop fast hyperparameter tuning for this class, which has a wide range of applications including robust regression and classification. We illustrate several applications using synthetic and real data.																	0885-6125	1573-0565				NOV	2019	108	11					1951	1974		10.1007/s10994-019-05797-z													
J								Risk bound of transfer learning using parametric feature mapping and its application to sparse coding	MACHINE LEARNING										Transfer learning; Sparse coding; Risk bound	MODEL; LASSO	In this study, we consider a transfer-learning problem using the parameter transfer approach, in which a suitable parameter of feature mapping is learned through one task and applied to another objective task. We introduce the notion of local stability and parameter transfer learn-ability of parametric feature mapping, and derive an excess risk bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with a large volume of unlabeled data often show excellent empirical performance, their theoretical analysis has not yet been studied. In this paper, we also provide a theoretical excess risk bound for self-taught learning. In addition, we show that the results of numerical experiments agree with our theoretical analysis.																	0885-6125	1573-0565				NOV	2019	108	11					1975	2008		10.1007/s10994-019-05805-2													
J								A distributed feature selection scheme with partial information sharing	MACHINE LEARNING										Feature selection; Classification; Model selection; Distributed optimization; Parallel processing	PARALLEL FEATURE-SELECTION; FEATURE SUBSET-SELECTION; SWARM OPTIMIZATION; CLASSIFICATION; ALGORITHM	This paper introduces a novel feature selection and classification method, based on vertical data partitioning and a distributed searching architecture. The features are divided into subsets, each of which is associated to a dedicated processor that performs a local search. When all local selection processes are completed, each processor shares the features of its locally selected model with all other processors, and the local searches are repeated until convergence. Thanks to the vertical partitioning and the distributed selection scheme, the presented method is capable of addressing relatively large scale examples. The procedure is efficient since the local processors perform the selection tasks in parallel and on much smaller search spaces. Another important feature of the proposed method is its tendency to produce simple model structures, which is generally advantageous for the interpretability and robustness of the classifier. The proposed approach is evaluated and compared to other well-known feature selection and classification approaches proposed in the literature on several benchmark datasets. The obtained results demonstrate the effectiveness of the proposed approach, both in terms of classification accuracy and computational time.																	0885-6125	1573-0565				NOV	2019	108	11					2009	2034		10.1007/s10994-019-05809-y													
J								A Case-Based Reasoning Approach for the Cybersecurity Incident Recording and Resolution	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Cybersecurity; information security; case-based reasoning	MANAGEMENT	Intelligent computing techniques have a paramount importance to the treatment of cybersecurity incidents. In such Artificial Intelligence (AI) context, while most of the algorithms explored in the cybersecurity domain aim to present solutions to intrusion detection problems, these algorithms seldom approach the correction procedures that are explored in the resolution of cybersecurity incident problems that already took place. In practice, knowledge regarding cybersecurity resolution data and procedures is being under-used in the development of intelligent cybersecurity systems, sometimes even lost and not used at all. In this context, this work proposes the Case-based Cybersecurity Incident Resolution System (CCIRS), a system that implements an approach to integrate case-based reasoning (CBR) techniques and the IODEF standard in order to retain concrete problem-solving experiences of cybersecurity incident resolution to be reused in the resolution of new incidents. Different types of experimental results so far obtained with the CCIRS show that information security knowledge can be retained with our approach in a reusable memory improving the resolution of new cybersecurity problems.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1607	1627		10.1142/S021819401940014X													
J								Using Bayesian Network to Estimate the Value of Decisions within the Context of Value-Based Software Engineering: A Multiple Case Study	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Value-based decision-making; software product and project management; Bayesian network; value estimation; value-based software engineering		Context: Companies must make a paradigm shift in which both short- and long-term value aspects are employed to guide their decision-making. Such need is pressing in innovative industries, such as ICT, and is the core of Value-based Software Engineering (VBSE). Objective: This paper details three case studies where value estimation models using Bayesian Network (BN) were built and validated. These estimation models were based upon value-based decisions made by key stakeholders in the contexts of feature selection, test cases execution prioritization, and user interfaces design selection. Methods: All three case studies were carried out according to a Framework called VALUE - improVing decision-mAking reLating to software-intensive prodUcts and sErvices development. This framework includes a mixed-methods approach, comprising several steps to build and validate company-specific value estimation models. Such a building process uses as input data key stakeholders' decisions (gathered using the Value tool), plus additional input from key stakeholders. Results: Three value estimation BN models were built and validated, and the feedback received from the participating stakeholders was very positive. Conclusions: We detail the building and validation of three value estimation BN models, using a combination of data from past decision-making meetings and also input from key stakeholders.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1629	1671		10.1142/S0218194019400151													
J								Evaluation Techniques for Chatbot Usability: A Systematic Mapping Study	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Usability; chatbots; systematic mapping study; usability techniques	CONVERSATIONAL AGENT	Background: The use of chatbots has increased considerably in recent years. These are used in different areas and by a wide variety of users. Due to this fact, it is essential to incorporate usability in their development. Aim: Our objective is to identify the state-of-the-art in chatbot usability and applied human-computer interaction techniques, to analyze how to evaluate chatbot usability. Method: We have conducted a systematic mapping study, by searching the main scientific databases. The search retrieved 170 references and 21 articles were retained as primary studies. Results: The works were categorized according to four criteria: usability techniques, usability characteristics, research methods and type of chatbots. Conclusions: Chatbot usability is still a very incipient field of research where the published studies are mainly surveys, usability tests, and rather informal experimental studies. Hence, it becomes necessary to perform more formal experiments to measure user experience, and exploit these results to provide usability-aware design guidelines.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1673	1702		10.1142/S0218194019400163													
J								Exposing IoT Objects in the Internet Using the Resource Management Architecture	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										IoT; embedded systems; sensors		This paper proposes an architecture for sharing IoT Objects' resources in the Internet of Things providing a model for its owners to expose devices, which can be consumed by clients inspired by the Sensor-as-a-Service model. The main idea relies on the fact that users, such as developers and researchers, do not always have access to the necessary hardware and resources. Exposing devices in IoT should impact these persons activities. Then, we present the Resource Management Architecture, where several IoT Objects endowed with sensors and actuators can be added to environments that are represented virtually in the architecture. The IoT Objects become available to be consumed by users through the use of applications. The architecture is composed of three layers: one representing devices, the cloud solution, and applications, and how they interact with each other. We also present a study case for testing the whole approach in a smart city scenario.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1703	1725		10.1142/S0218194019400175													
J								Multistep Flow Prediction on Car-Sharing Systems: A Multi-Graph Convolutional Neural Network with Attention Mechanism	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Car-sharing systems; multistepflow prediction; graph convolution network		Multistep flow prediction is an essential task for the car-sharing systems. An accurate flow prediction model can help system operators to pre-allocate the cars to meet the demand of users. However, this task is challenging due to the complex spatial and temporal relations among stations. Existing works only considered temporal relations (e.g. using LSTM) or spatial relations (e.g. using CNN) independently. In this paper, we propose an attention to multi-graph convolutional sequence-to-sequence model (AMGC-Seq2Seq), which is a novel deep learning model for multistep flow prediction. The proposed model uses the encoder-decoder architecture, wherein the encoder part, spatial and temporal relations are encoded simultaneously. Then the encoded information is passed to the decoder to generate multistep outputs. In this work, specific multiple graphs are constructed to reflect spatial relations from different aspects, and we model them by using the proposed multi-graph convolution. Attention mechanism is also used to capture the important relations from previous information. Experiments on a large-scale real-world car-sharing dataset demonstrate the effectiveness of our approach over state-of-the-art methods.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1727	1740		10.1142/S0218194019400187													
J								Formalization and Verification of TESAC Using CSP	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										TESAC; cloud computing; CSP; access control; modeling; verification		Cloud computing is an emerging computing paradigm in IT industries. The wide adoption of cloud computing is raising concerns about management of data in the cloud. Access control and data security are two critical issues of cloud computing. Time-efficient secure access control (TESAC) model is a new data access control scheme which can minimize many significant problems. This scheme has better performance than other existing models in a cloud computing environment. TESAC is attracting more and more attentions from industries. Hence, the reliability of TESAC becomes extremely important. In this paper, we apply Communication Sequential Processes (CSP) to model TESAC, as well as their security properties. We mainly focus on its data access mechanism part and formalize it in detail. Moreover, using the model checker Process Analysis Toolkit (PAT), we have verified that the TESAC model cannot assure the security of data with malicious users. For the purpose of solving this problem, we introduce a new method similar to digital signature. Our study can improve the security and robustness of the TESAC model.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1741	1760		10.1142/S0218194019400199													
J								API Misuse Detection in C Programs: Practice on SSL APIs	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										API misuse; static analysis; SSL bug detection		Libraries offer reusable functionality through Application Programming Interfaces (APIs) with usage constraints such as call conditions or orders. Constraint violations, i.e. API misuses, commonly lead to bugs and security issues. Although researchers have developed various API misuse detectors in the past few decades, recent studies show that API misuse is prevalent in real-world projects, especially for secure socket layer (SSL) certificate validation, which is completely broken in many security-critical applications and libraries. In this paper, we introduce SSLDoc to effectively detect API misuse bugs, specifically for SSL API libraries. The key insight behind SSLDoc is a constraint-directed static analysis technique powered by a domain-specific language (DSL) for specifying API usage constraints. Through studying real-world API misuse bugs, we propose ISpec DSL, which covers majority types of API usage constraints and enables simple but precise specification. Furthermore, we design and implement SSLDoc to automatically parse ISpec into checking targets and employ a static analysis engine to identify potential API misuses and prune false positives with rich semantics. We have instantiated SSLDoc for OpenSSL APIs and applied it to large-scale open-source programs. SSLDoc found 45 previously unknown security-sensitive bugs in OpenSSL implementation and applications in Ubuntu. Up to now, 35 have been confirmed by the corresponding development communities and 27 have been fixed in master branch.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1761	1779		10.1142/S0218194019400205													
J								Point-of-Interest Recommendation Based on User Contextual Behavior Semantics	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Location-based social network; heterogeneous information network; context information; point-of-interest recommendation; behavior semantics		By suggesting new visiting places, point-of-interest (POI) recommendation not only assists users to find their preferred places, but also helps businesses to attract potential customers. Recent studies have proposed many approaches to the POI recommendation. However, the data sparsity and complexity of user check-in behavior still pose big challenges to accurate personalized POI recommendation. To tackle these problems, in this paper, we propose a POI recommendation model named HeteGeoRankRec based on user contextual behavior semantics. First, we employ the meta-path of heterogeneous information network (HIN) to represent the complex semantic relationship among users and POIs. Second, we introduce different context constraints (such as time and weather) into the meta-path, to reveal the fine-grained user behavioral features. Afterwards, we propose a weighted matrix factorization model which considers the influence of geographical distance through the user-POI semantic correlativity matrices generated by multiple meta-paths. Finally, we present a fusion method based on learning to rank, which unifies the recommendation results of different meta-paths as the final user preference. The experiments on the real data collected from Foursquare demonstrate that HeteGeoRankRec has the better performance than the state-of-the-art baselines.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1781	1799		10.1142/S0218194019400217													
J								Improve Language Modeling for Code Completion Through Learning General Token Repetition of Source Code with Optimized Memory	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Language model; code completion; graph neural network; code repetitiveness		In last few years, applying language model to source code is the state-of-the-art method for solving the problem of code completion. However, compared with natural language, code has more obvious repetition characteristics. For example, a variable can be used many times in the following code. Variables in source code have a high chance to be repetitive. Cloned code and templates, also have the property of token repetition. Capturing the token repetition of source code is important. In different projects, variables or types are usually named differently. This means that a model trained in a finite data set will encounter a lot of unseen variables or types in another data set. How to model the semantics of the unseen data and how to predict the unseen data based on the patterns of token repetition are two challenges in code completion. Hence, in this paper, token repetition is modelled as a graph, we propose a novel REP model which is based on deep neural graph network to learn the code toke repetition. The REP model is to identify the edge connections of a graph to recognize the token repetition. For predicting the token repetition of token n, the information of all the previous tokens needs to be considered. We use memory neural network (MNN) to model the semantics of each distinct token to make the framework of REP model more targeted. The experiments indicate that the REP model performs better than LSTM model. Compared with Attention-Pointer network, we also discover that the attention mechanism does not work in all situations. The proposed REP model could achieve similar or slightly better prediction accuracy compared to Attention-Pointer network and consume less training time. We also find other attention mechanism which could further improve the prediction accuracy.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1801	1818		10.1142/S0218194019400229													
J								A Robust Visual Tracker Based on DCF Algorithm	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Visual tracking; correlation filter; background modeling; re-detection and re-search; model update	OBJECT TRACKING; FILTER	Since Correlation Filter appeared in the field of video object tracking, it is very popular due to its excellent performance. The Correlation Filter-based tracking algorithms are very competitive in terms of accuracy and speed as well as robustness. However, there are still some fields for improvement in the Correlation Filter-based tracking algorithms. First, during the training of the classifier, the background information that can be utilized is very limited. Moreover, the introduction of the cosine window further reduces the background information. These reasons reduce the discriminating power of the classifier. This paper introduces more global background information on the basis of the DCF tracker to improve the discriminating ability of the classifier. Then, in some complex scenes, tracking loss is easy to occur. At this point, the tracker will be treated the background information as the object. To solve this problem, this paper introduces a novel re-detection component. Finally, the current Correlation Filter-based tracking algorithms use the linear interpolation model update method, which cannot adapt to the object changes in time. This paper proposes an adaptive model update strategy to improve the robustness of the tracker. The experimental results on multiple datasets can show that the tracking algorithm proposed in this paper is an excellent algorithm.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1819	1834		10.1142/S0218194019400230													
J								Piecewise Aggregation for HMM Fitting: A Pre-Fitting Model for Seamless Integration with Time-Series Data	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Hidden Markov Models; Baum-Welch; time series	HIDDEN MARKOV-MODELS; EM ALGORITHM; MAXIMUM-LIKELIHOOD; MAXIMIZATION; IDENTIFICATION	We propose a simple, fast, deterministic pre-fitting approach which derives the Baum-Welch algorithm initial values directly from the input data. Such prefitting has the purpose of improving the fitting time for a given Hidden Markov Model (HMM) while maintaining the original Baum-Welch algorithm as the fitting one. The fitting time is improved by avoiding the Baum-Welch algorithm sensitiveness through the generation of parameters closer to the global maximum likelihood. Furthermore, by keeping the original Baum-Welch algorithm as the fitting one, we guarantee that all related methods will continue to work properly. On the other hand, the prefitting generates the HMM parameters directly derived from time-series data, without any data transformation, using an O(n) operation.																	0218-1940	1793-6403				NOV-DEC	2019	29	11-12			SI		1835	1850		10.1142/S0218194019400242													
J								Twenty-five years of information extraction	NATURAL LANGUAGE ENGINEERING										Information extraction; Message understanding		Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.																	1351-3249	1469-8110				NOV	2019	25	6					677	692		10.1017/S1351324919000512													
J								An overview of word and sense similarity	NATURAL LANGUAGE ENGINEERING										semantic similarity; word similarity; sense similarity; distributional semantics; knowledge-based similarity	SEMANTIC SIMILARITY; CORPUS STATISTICS; REPRESENTATION; EMBEDDINGS; KNOWLEDGE; MODELS	Over the last two decades, determining the similarity between words as well as between their meanings, that is, word senses, has been proven to be of vital importance in the field of Natural Language Processing. This paper provides the reader with an introduction to the tasks of computing word and sense similarity. These consist in computing the degree of semantic likeness between words and senses, respectively. First, we distinguish between two major approaches: the knowledge-based approaches and the distributional approaches. Second, we detail the representations and measures employed for computing similarity. We then illustrate the evaluation settings available in the literature and, finally, discuss suggestions for future research.																	1351-3249	1469-8110				NOV	2019	25	6					693	714		10.1017/S1351324919000305													
J								Discovering multiword expressions	NATURAL LANGUAGE ENGINEERING										Multiword expressions; Association measures; Compositionality; Idiomaticity	CONSTRUCTIONS; MODELS	In this paper, we provide an overview of research on multiword expressions (MWEs), from a natural language processing perspective. We examine methods developed for modelling MWEs that capture some of their linguistic properties, discussing their use forMWEdiscovery and for idiomaticity detection. We concentrate on their collocational and contextual preferences, along with their fixedness in terms of canonical forms and their lack of word-for-word translatatibility. We also discuss a sample of the MWE resources that have been used in intrinsic evaluation setups for these methods.																	1351-3249	1469-8110				NOV	2019	25	6					715	733		10.1017/S1351324919000494													
J								Automatic summarisation: 25 years On	NATURAL LANGUAGE ENGINEERING										Automatic summarisation; Survey; Evaluation		Automatic text summarisation is a topic that has been receiving attention from the research community from the early days of computational linguistics, but it really took off around 25 years ago. This article presents the main developments from the last 25 years. It starts by defining what a summary is and how its definition changed over time as a result of the interest in processing new types of documents. The article continues with a brief history of the field and highlights the main challenges posed by the evaluation of summaries. The article finishes with some thoughts about the future of the field.																	1351-3249	1469-8110				NOV	2019	25	6					735	751		10.1017/S1351324919000524													
J								A survey of 25 years of evaluation	NATURAL LANGUAGE ENGINEERING										evaluation; survey; systems research		Evaluation was not a thing when the first author was a graduate student in the late 1970s. There was an Artificial Intelligence (AI) boom then, but that boom was quickly followed by a bust and a long AI Winter. Charles Wayne restarted funding in the mid-1980s by emphasizing evaluation. No other sort of program could have been funded at the time, at least in America. His program was so successful that these days, shared tasks and leaderboards have become common place in speech and language (and Vision and Machine Learning). It is hard to remember that evaluation was a tough sell 25 years ago. That said, we may be a bit too satisfied with current state of the art. This paper will survey considerations from other fields such as reliability and validity from psychology and generalization from systems. There has been a trend for publications to report better and better numbers, but what do these numbers mean? Sometimes the numbers are too good to be true, and sometimes the truth is better than the numbers. It is one thing for an evaluation to fail to find a difference between man and machine, and quite another thing to pass the Turing Test. As Feynman said, "the first principle is that you must not fool yourself - and you are the easiest person to fool."																	1351-3249	1469-8110				NOV	2019	25	6					753	767		10.1017/S1351324919000275													
J								Five Tips for a Successful API	NATURAL LANGUAGE ENGINEERING										Text analytics; APIs	TEXT ANALYTICS APIS	It's now remarkably easy to release to the world a cloud-based application programming interface (API) that provides some software function as a service. As a consequence, the cloud API space has become very densely populated, so that even if a particular API offers a service whose potential value is considerable, there are many other factors that play a role in determining whether or not that API will be commercially successful. If you're thinking about entering the API marketplace with your latest and greatest idea, this post offers some entirely subjective advice on how you might increase the chances of your offering not being lost in all the noise.																	1351-3249	1469-8110				NOV	2019	25	6					769	772		10.1017/S1351324919000536													
J								Orientation Control of an Object on a Rotating Base by Using a Two-Stage Electric Drive	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											RIGID-BODY	In this paper, we study the process of controlling the rotation of an object relative to a rotating base using a two-stage electric drive that represents two gearless direct-current motors connected in series. The stator of one of the electric motor is rigidly fixed to the base while an object is fixed to the rotor of the other motor. Between the rotor and the stator of the second motor, there is a torsion spring (torsion) with a relatively low level of stiffness. The aim of the control is tracking by an object a specified orientation relative to the base. The drive control algorithm is built taking into account the incompleteness of the information on the dynamic parameters of the mechanical system, friction acting in it, laws of motion of the base, and changes in the tracked orientation.																	1064-2307	1555-6530				NOV	2019	58	6					829	843		10.1134/S1064230719060029													
J								Motion Control for Platforms Bearing Elastic Links with Unknown Phase States	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												We consider the problem on the controlled motion of a platform such that a solid body bearing a dissipative oscillator is attached to it by means of a spring. The platform moves along a horizontal line under the action of the control force, undergoing the action of a bounded uncontrolled perturbation, e.g., the dry friction force, as well. It is assumed that the phase state of the oscillator is not accessible for measurement. We propose a control law leading the whole system to the prescribed rest state within a finite time period.																	1064-2307	1555-6530				NOV	2019	58	6					844	851		10.1134/S1064230719060030													
J								Multistructural Method of the Triangulation Estimation of the Motion Parameters of a Radiating Target under A Priori Indefiniteness Assumptions	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											VARIATIONAL-SELECTIVE METHOD; PASSIVE LOCATION; TRACKING; POSITION; UAV	Based on the multistructuredness and clustering principles, as well as the filtration theory, we develop a new estimation method for the motion parameters of a radiating target under the assumption of a fundamental indefiniteness of the operating conditions for the triangulation measurement system. The method admits anomalous measurement errors in the measurement channels (the azimuth and the angle of elevation) of particular system locators such that neither the number of uncertain channels neither the time moments of the appearance of these errors are known (only the greatest possible number of uncertain channels is known). The method is implemented in the stochastic variant with or without the participation of an operator. The implementation does not require the traditional extension of the space of states. We provide the results of the comparative analysis demonstrating the efficiency of the method.																	1064-2307	1555-6530				NOV	2019	58	6					852	868		10.1134/S106423071904004X													
J								Two-Stage Algorithm for Estimation of Nonlinear Functions of State Vector in Linear Gaussian Continuous Dynamical Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											ADAPTIVE ESTIMATION; MOMENTS	This paper focuses on the optimal minimum mean square error estimation of a nonlinear function of state (NFS) in linear Gaussian continuous-time stochastic systems. The NFS represents a multivariate function of state variables which carries useful information of a target system for control. The main idea of the proposed optimal estimation algorithm includes two stages: the optimal Kalman estimate of a state vector computed at the first stage is nonlinearly transformed at the second stage based on the NFS and the minimum mean square error (MMSE) criterion. Some challenging theoretical aspects of analytic calculation of the optimal MMSE estimate are solved by usage of the multivariate Gaussian integrals for the special NFS such as the Euclidean norm, maximum and absolute value. The polynomial functions are studied in detail. In this case the polynomial MMSE estimator has a simple closed form and it is easy to implement in practice. We derive effective matrix formulas for the true mean square error of the optimal and suboptimal quadratic estimators. The obtained results we demonstrate on theoretical and practical examples with different types of NFS. Comparison analysis of the optimal and suboptimal nonlinear estimators is presented. The subsequent application of the proposed estimators demonstrates their effectiveness.																	1064-2307	1555-6530				NOV	2019	58	6					869	882		10.1134/S1064230719060169													
J								Decomposition Algorithm for the Linear Three-Index Transportation Problem	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											INTEGRAL CONSTRAINTS; MINIMAX; MODELS	The method of sequential modification of an objective function, which was earlier used for the classical transportation problem, is extended to the case of three indices. In the iterative process, the problem with three constraints and one binding variable is solved. Then, three independent problems with one constraint in which the coefficients for the binding variable are changed are considered. Using the suggested algorithm, a sequence of pseudosolutions with a monotonic growth of the objective function that converges to the optimum is constructed. The degeneracies are analyzed.																	1064-2307	1555-6530				NOV	2019	58	6					883	888		10.1134/S1064230719060157													
J								Express Analysis and Aggregated Representation of the Set of Reachable Flows for a Multicommodity Network System	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												Within the formalism of the mathematical model of transferring a multicommodity flow, the reachability set of multiuser network systems is studied. A method is proposed for constructing an internal supporting frame based on the vectors of the maximum flows that can be transferred between all pairs of nodes in exclusive and limited exclusive modes of flow control. Using the obtained limit values, we construct a polyhedron of a simpler structure (frame) belonging to the set of reachable multi-flows and a cone of possible directions for the generation of boundary points. Methods for obtaining various multicriteria evaluations of the functional capabilities of a transmission network are considered. The developed approach can be used for the a priori analysis of the set of reachable multiflows, including quickly obtaining estimates and acceptable options for the fair distribution of the limited capacity of the transmission network.																	1064-2307	1555-6530				NOV	2019	58	6					889	897		10.1134/S1064230719060133													
J								Diagnosis of Linear Systems Based on Sliding Mode Observers	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											ROBUST FAULT RECONSTRUCTION	In this paper, we consider the problem of the functional diagnosis of technical systems described by linear differential equations of state with constant coefficients in the presence of disturbances. Sliding mode observers are used to solve problems of detecting, isolating, and identifying faults. The proposed approach is based on the idea of constructing a reduced (having a smaller dimension) model of the original system with the selective sensitivity to faults and disturbances, based on which a sliding mode observer is built. Depending on the specific properties of the constructed model, three options (in each of which a diagnostic problem is solved): accurate fault identification, fault detection, and approximate fault identification are considered. Apart from reducing the complexity of diagnostic tools, the proposed approach allows reducing the limitations imposed on the original system for constructing sliding mode observers.																	1064-2307	1555-6530				NOV	2019	58	6					898	914		10.1134/S1064230719040166													
J								Selecting a Single Result from an Aggregate of Contradictory Alternatives with Use of Multiset Theory	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											DOMAINS	We consider basic approaches to decision-making using computer systems. Ambiguous results in decision-making are obtained through the use of different methods for source-data processing. In order to obtain a single result from an aggregate of such decisions, it is suggested to apply the principle of selecting the decision that is most proposed by John von Neumann to increase the reliability of computers. Based on multiset theory, we develop a mathematical model of selecting a single result from an aggregate of alternatives obtained using different methods of decision-making. In this model, a decision is selected based on computing the argument of maximizing the multiplicity functions of elements for the arithmetic sum of multisets of the obtained decisions. The peculiarities of applying this approach are considered and methods of adjusting the source data for obtaining a single result are proposed. By the example of selecting a metal for electroplating products we present the results (of the decisions taken) obtained by a group of experts using the described approach. Recommendations for improving the efficiency of the proposed approach are presented.																	1064-2307	1555-6530				NOV	2019	58	6					915	921		10.1134/S1064230719060121													
J								Generalized Equivalence Set Method for Solving Multiobjective Optimization Problems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												Various aspects of solving multiobjective discrete optimization problems are considered. Advantages of the equivalence set method are shown compared to other methods often used to solve multiobjective problems such as the method of successive concessions and the method of seeking the set of Pareto-optimal solutions. Theorems reflecting the main properties of the equivalence set method and showing the relationship and interrelation between the set of Pareto-optimal solutions and the equivalence set are formulated and proved.																	1064-2307	1555-6530				NOV	2019	58	6					922	931		10.1134/S1064230719060091													
J								Artificial Intelligence Agents in the Knowledge Databases of Onboard Real-Time Advisory Expert Systems for the Typical Situations of the Functioning of an Anthropocentric Object	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												When developing the intelligence support systems designed for the crew of anthropocentric objects that recommend a method of solving the arising tactical-level problem for the crew (real-time targeting + construction of a method for attaining a real-time assigned target of functioning), it is extremely important to adequately understand the subject domain of the database of such systems. One way to accomplish this is to use dynamic fragments of problematic subsituations in a knowledge database with intelligence agents incorporated into them. The general structure of intelligence agents used in the knowledge databases of onboard teal-time advisory expert systems for the typical situations of the functioning of anthropocentric objects is given from the viewpoint of the "Stage" conceptual model of an anthropocentric object. Certain intelligence agents and their use in the knowledge database of a certain onboard real-time advisory expert system for the typical situation of the functioning of anthropocentric objects are considered as an example.																	1064-2307	1555-6530				NOV	2019	58	6					932	944		10.1134/S1064230719040051													
J								Detection of Cylindrical and Spherical Surfaces and Determining Their Parameters in Segmentation of Range Images	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											LIDAR POINT CLOUDS	An analytical solution of the problem of estimating the coordinates of the axis of a cylindrical surface and its radius by processing range images is obtained. The proposed methods and algorithms can detect fragments of such surfaces in range images with the prescribed confidence probability. The method is also generalized for finding spherical surfaces in range images. The efficiency of the proposed methods and algorithms is verified by simulation using real-life range images of urban industrial scenes.																	1064-2307	1555-6530				NOV	2019	58	6					945	959		10.1134/S106423071906011X													
J								Technology for the Visual Inspection of Aircraft Surfaces Using Programmable Unmanned Aerial Vehicles	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The technology for the visual inspection of aircraft surfaces using programmable unmanned aerial vehicles (drones) is presented. When developing the technology, special attention is paid to the problem of the drone's indoor navigation where the signal from satellite positioning systems is weak or there is no signal, as well as to the development of algorithmic programs and software for detecting both the drone and damage to the aircraft surface based on video analysis. The results of testing the technology in enclosed spaces under conditions close to the expected operating conditions are given.																	1064-2307	1555-6530				NOV	2019	58	6					960	968		10.1134/S1064230719060042													
J								Analytical Algorithm for Constructing the Orbital Orientation of a Spacecraft with an Incomplete Measurement of the State Vector Components	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											OUTPUT CONTROL; MOTION	The problem of joint synthesis of a modal controller and observer is solved analytically for a sixth-order system that describes the rotational motion of a spacecraft with the available measurements of five components of the state vector. The modeling results that describe the application of the synthesized control and observation laws for the construction of the orbital orientation of a spacecraft are presented in the paper.																	1064-2307	1555-6530				NOV	2019	58	6					969	979		10.1134/S1064230719040178													
J								Synthesis of the Optimal Control of the Spacecraft Orientation Using Combined Criteria of Quality	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											OPTIMAL TURN PROBLEM	The dynamic problem of a spacecraft's (SC) rotation from an arbitrary initial angular position to a given final angular position is considered and solved. The case is investigated when the control is limited, and the minimized functional combines, in a given, proportion, the time of the maneuver and the integral of the energy of rotation. The construction of the optimal control is based on the quaternionic differential equation relating the vector of the angular momentum of the SC with the quaternion of the orientation of the body-related coordinate system. The control law is formulated in the form of an explicit dependence of the control variables on the phase coordinates. The analysis of the special control regime of the SC is carried out. Based on the conditions of transversality, as the necessary conditions for optimality, the optimal value of the kinetic energy of rotation when moving in a special control regime is determined. The created control algorithms allow turning of an SC with a kinetic rotation energy, which does exceed a predetermined level. For a dynamically symmetric SC, the problem of spatial reorientation is solved completely. The results of the mathematical modeling of the motion of an SC under the optimal control are presented, demonstrating the practical feasibility of the developed algorithm for controlling the spatial orientation of the SC.																	1064-2307	1555-6530				NOV	2019	58	6					980	1003		10.1134/S1064230719040105													
J								Implementation of the Orbital Orientation Mode of an Artificial Satellite of the Earth without Accumulation of the Angular Momentum of the Gyro System	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											ATTITUDE-CONTROL	We consider the motion of an artificial satellite of the Earth of the Progress and Foton-M4 type in different variants of the orbital orientation mode in low Earth orbit. A gyro system (a set of reaction wheels or control moment gyros) is used as the executive part of the satellite control system. The gravitational and restoring aerodynamic torques acting on the satellite are taken into account. Based on the principles of the proportional-differential regulator, we construct the control laws for the gyrostatic momentum, which allow, without its accumulation, maintaining a long and sufficiently accurate orientation of the satellite in the vicinity of gravitationally stable and unstable rest positions that exist in the simplified problem (the satellite moves in a fixed circular orbit under the action of the gravitational torque only; the gyrostatic momentum is zero).																	1064-2307	1555-6530				NOV	2019	58	6					1004	1017		10.1134/S1064230719060066													
J								Bringing an Insectomorphic Robot to a Normal Position from an Abnormal Upside Down Position	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												It is shown that if a robot's body has an upper shell in the shape of a truncated cylinder, then a six-legged insectomorphic robot can autonomously bring itself to a normal position from an abnormal upside down position using a cyclic leg motion. A method for rocking the robot that helps it to turn itself over to the normal position is proposed. An analytical analysis and computer simulation of the complete robot dynamics are performed, which confirm the effectiveness of the proposed method of recovering the normal position of the robot. The results of numerical experiments are presented.																	1064-2307	1555-6530				NOV	2019	58	6					1018	1030		10.1134/S1064230719060054													
J								Improved stochastic fractal search algorithm with chaos for optimal determination of location, size, and quantity of distributed generators in distribution systems	NEURAL COMPUTING & APPLICATIONS										Distribution systems; Optimal placement of distributed generators; Power loss reduction; Chaotic stochastic fractal search algorithm	OPTIMAL PLACEMENT; OPTIMIZATION ALGORITHM; OPTIMAL ALLOCATION; DG ALLOCATION; MULTIPLE DGS; INTEGRATION	In the power system operation, the reduction of the power loss in distribution systems has significance in the reduction of operating cost. In this paper, a novel chaotic stochastic fractal search (CSFS) method is implemented for determining the optimal siting, sizing, and number of distributed generation (DG) units in distribution systems. The objective of the optimal DG placement problem is to minimize the power loss in distribution systems subject to the constraints such as power balance, bus voltage limits, DG capacity limits, current limits, and DG penetration limit. The proposed CSFS method improves the performance of the original SFS by integrating chaotic maps into it. On the other hand, ten chaotic maps are utilized to replace the random scheme of the original SFS to enhance its performance in terms of accuracy of solution and convergence speed, corresponding to ten chaotic variants of the SFS where variant being chosen is the best chaotic variant regarding search performance. For solving the problem, the CSFS is implemented to simultaneously find the optimal siting and sizing of DG units and the optimal number of DG units will be obtained via comparing optimal results from different numbers of DG in the problem. The proposed method is tested on the IEEE 33-bus, 69-bus, and 118-bus radial distribution systems. The obtained results from the CSFS are verified by comparing to those from the original SFS and other methods in the literature. The result comparisons indicate that the proposed CSFS method can obtain higher quality solutions than the original SFS version and many other methods in the literature for the considered cases of the test systems. Moreover, the incorporation of chaos theory allows performing the search process at higher speeds. Therefore, the proposed CSFS method can be a very promising method for solving the problem of optimal placement of DG units in distribution systems.																	0941-0643	1433-3058				NOV	2019	31	11					7707	7732		10.1007/s00521-018-3603-1													
J								Interactive History-Based Vessel Movement Prediction	IEEE INTELLIGENT SYSTEMS										Trajectory; Data models; Prediction algorithms; Predictive models; Computational modeling; Data compression	IDENTIFICATION SYSTEM AIS	Analyzing vessel movements is indispensable for multiple tasks such as collision avoidance or route and logistics planning. We propose a novel approach for vessel movement prediction based on recorded movement data. The predicted movements including uncertainties are calculated and visualized interactively, facilitating visual inspection of relevant vessel movements within the above-mentioned scenarios.																	1541-1672	1941-1294				NOV-DEC	2019	34	6					3	13		10.1109/MIS.2019.2954509													
J								Factual and Counterfactual Explanations for Black Box Decision Making	IEEE INTELLIGENT SYSTEMS										Genetic algorithms; Intelligent systems; Decision making; Decision trees; Machine learning algorithms; Prediction algorithms; Data models; Explainable AI; Interpretable Machine Learning; Open the Black Box; Explanation Rules; Counterfactuals		The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.																	1541-1672	1941-1294				NOV-DEC	2019	34	6					14	22		10.1109/MIS.2019.2957223													
J								Expected Value From a Ranking of Alternatives for Personalized Quantifier	IEEE INTELLIGENT SYSTEMS										Open wireless architecture; Decision making; Intelligent systems; Position measurement; Tools; Uncertainty; Interpolation; Knowledge personalization; Quantifier; Expected value; Decision making	RIM QUANTIFIER; AGGREGATION; INFORMATION; OPERATORS	A novel model is presented in this article to derive the expected value from a ranking of alternatives, with which to tackle some uncertain problems, such as different types of att0itudes, in the creation of personalized quantifier caused by personal preference. There is no doubt that we consider here a reverse process of traditional ranking methods in decision making. More specifically, a representative sample of multi-attribute alternatives is prepared, and an involved person is asked to do nothing but provide, following personal preference or judgment on the whole, a ranking of alternatives of this sample. The relationship between the alternatives' importance weights and their ranking positions is investigated. A TOPSIS-based model can then be established to derive his/her personal expected value from this ranking. The idea of ordered weighted averaging (OWA) aggregations is fully taken into account in the modeling. We use this technique to create a personalized quantifier, thus making the creation much more efficient and safer compared with what we did before. Experimental results show that different rankings of alternatives may lead to different expected values that eventually determine different types of attitudes. Thus, the developed technique could be used as a tool for uncertainty modeling in more complex situations in which various personality traits have to be taken into account, especially for decision making under uncertainty.																	1541-1672	1941-1294				NOV-DEC	2019	34	6					24	33		10.1109/MIS.2019.2949266													
J								Political Homophily in Independence Movements: Analyzing and Classifying Social Media Users by National Identity	IEEE INTELLIGENT SYSTEMS										Twitter; Data collection; Intelligent systems; Media; Urban areas; Geology; social media; national identity; socio-demographics; classification	FEATHER TWEET; TWITTER; BIRDS	Social media and data mining are increasingly being used to analyze political and societal issues. Here, we undertake the classification of social media users as supporting or opposing ongoing independence movements in their territories. Independence movements occur in territories whose citizens have conflicting national identities; users with opposing national identities will then support or oppose the sense of being part of an independent nation that differs from the officially recognized country. We describe a methodology that relies on users' self-reported location to build large-scale datasets for three territories-Catalonia, the Basque Country, and Scotland. An analysis of these datasets shows that homophily plays an important role in determining who people connect with, as users predominantly choose to follow and interact with others from the same national identity. We show that a classifier relying on users' follow networks can achieve accurate, language-independent classification performances ranging from 85% to 97% for the three territories.																	1541-1672	1941-1294				NOV-DEC	2019	34	6					34	41		10.1109/MIS.2019.2958393													
J								Figure Summarization: A Multiobjective Optimization-Based Approach	IEEE INTELLIGENT SYSTEMS										Optimization; Linear programming; Task analysis; Self-organizing feature maps; Biomedical measurement; Sociology; Statistics		In the biomedical domain, figures in the scientific articles attribute significantly in understanding the core concepts. However, these figures are always difficult to interpret by the humans as well as machines and, thus, associated texts in the article are required to summarize the figures. This article proposes an unsupervised automatic summarization system for individual figures present in a scientific biomedical article, where different quality measures capturing relevance of the sentences to the figure are simultaneously optimized using the search capability of a multiobjective optimization technique to obtain a good set of sentences in the summary. A newly designed self-organizing map based genetic operator helping in new solution generation is also introduced in the multiobjective optimization framework. For evaluation of the proposed technique, 94 and 81 figures over two datasets from the biomedical literature are used. Our proposed system, namely MOOFigSum, obtains 5% and 11% improvements in terms of F1-measure metric over the unsupervised technique for both datasets, respectively, while in comparison to supervised techniques, MOOFigSum obtains 9% and 2% improvements over these datasets, respectively.																	1541-1672	1941-1294				NOV-DEC	2019	34	6					43	52		10.1109/MIS.2019.2954400													
J								Type Like a Man! Inferring Gender from Keystroke Dynamics in Live-Chats	IEEE INTELLIGENT SYSTEMS										Presses; Feature extraction; Psychology; Pressing; Affective computing; Sentiment analysis; Planning																			1541-1672	1941-1294				NOV-DEC	2019	34	6					53	59		10.1109/MIS.2019.2948514													
J								Comparing shape descriptor methods for different color space and lighting conditions	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Design for manufacturing; human-computer interaction; man-machine systems; manufacturing	OBJECT DETECTION; RECOGNITION; SEGMENTATION; IMAGES	Detecting and recognizing objects is one of the most important uses of vision systems in nature and is consequently highly evolved. This paper aims to accurately detect an object using its shape and color information from a complex background. In particular, we evaluated our algorithm to detect 19 different integrated circuits (IC) from 10 different printed circuit boards (PCB) of different colors. We have compared three different shape descriptors for four different color space models. We have evaluated shape detection algorithms in different lighting conditions (indoor, outdoor, and controlled light source) to find suitable illumination for image acquisition. We undertook statistical hypothesis testing to find the effect of color space models and shape descriptors on the accuracy, false positive and false negative rates. While measuring accuracy, we have noted that L*a*b* color space is significantly worse, and the best result is obtained in YCbCr color space using bounding box shape descriptors for 2500 Lux using LED.																	0890-0604	1469-1760				NOV	2019	33	4			SI		389	398	PII S0890060419000398	10.1017/S0890060419000398													
J								New access services in HbbTV based on a deep learning approach for media content analysis	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Computer vision; deep learning; face detection; media accessibility		Universal access on equal terms to audiovisual content is a key point for the full inclusion of people with disabilities in activities of daily life. As a real challenge for the current Information Society, it has been detected but not achieved in an efficient way, due to the fact that current access solutions are mainly based in the traditional television standard and other not automated high-cost solutions. The arrival of new technologies within the hybrid television environment together with the application of different artificial intelligence techniques over the content will assure the deployment of innovative solutions for enhancing the user experience for all. In this paper, a set of different tools for image enhancement based on the combination between deep learning and computer vision algorithms will be presented. These tools will provide automatic descriptive information of the media content based on face detection for magnification and character identification. The fusion of this information will be finally used to provide a customizable description of the visual information with the aim of improving the accessibility level of the content, allowing an efficient and reduced cost solution for all.																	0890-0604	1469-1760				NOV	2019	33	4			SI		399	415	PII S0890060419000350	10.1017/S0890060419000350													
J								Making interaction with virtual reality accessible: rendering and guiding methods for subtitles	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										360 degrees Videos; accessibility; human-computer interaction; subtitles; virtual reality		Accessibility in immersive media is a relevant research topic, still in its infancy. This article explores the appropriateness of two rendering modes (fixed-positioned and always-visible) and two guiding methods (arrows and auto-positioning) for subtitles in 360 degrees video. All considered conditions have been implemented and integrated in an end-to-end platform (from production to consumption) for their validation and evaluation. A pilot study with end users has been conducted with the goals of determining the preferred options by users, the options that result in a higher presence, and of gathering extra valuable feedback from the end users. The obtained results reflect that, for the considered 360 degrees content types, always-visible subtitles were more preferred by end users and received better results in the presence questionnaire than the fixed-positioned subtitles. Regarding guiding methods, participants preferred arrows over auto-positioning because arrows were considered more intuitive and easier to follow and reported better results in the presence questionnaire.																	0890-0604	1469-1760				NOV	2019	33	4			SI		416	428	PII S0890060419000362	10.1017/S0890060419000362													
J								Design and development of sign language questionnaires based on video and web interfaces	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Accessibility; HTML5; human-computer interaction; interface; sign language; video	USABILITY EVALUATION	Conventional tests with written information used for the evaluation of sign language (SL) comprehension introduce distortions due to the translation process. This fact affects the results and conclusions drawn and, for that reason, it is necessary to design and implement the same language interpreter-independent evaluation tools. Novel web technologies facilitate the design of web interfaces that support online, multiple-choice questionnaires, while exploiting the storage of tracking data as a source of information about user interaction. This paper proposes an online, multiple-choice sign language questionnaire based on an intuitive methodology. It helps users to complete tests and automatically generates accurate, statistical results using the information and data obtained in the process. The proposed system presents SL videos and enables user interaction, fulfilling the requirements that SL interpretation is not able to cover. The questionnaire feeds a remote database with the user answers and powers the automatic creation of data for analytics. Several metrics, including time elapsed, are used to assess the usability of the SL questionnaire, defining the goals of the predictive models. These predictions are based on machine learning models, with the demographic data of the user as features for estimating the usability of the system. This questionnaire reduces costs and time in terms of interpreter dedication, as well as widening the amount of data collected while employing user native language. The validity of this tool was demonstrated in two different use cases.																	0890-0604	1469-1760				NOV	2019	33	4			SI		429	441	PII S0890060419000374	10.1017/S0890060419000374													
J								Data analysis from cognitive games interaction in Smart TV applications for patients with Parkinson's, Alzheimer's, and other types of dementia	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Alzheimer's; cognitive games; data analysis; HCI; Parkinson's; Smart TV	LEISURE ACTIVITIES; RISK; DISEASE; PEOPLE; USABILITY; DESIGN; ONSET; AGE; PD	Parkinson's disease and Alzheimer's disease are progressive nervous system disorders that affect physical and cognitive capacities of individuals, including memory loss, motion impairment, or problem-solving dysfunctions. Leisure activities are associated with reducing the risk of dementia and are preventive policies for delaying the cognitive impairment in later stages of those neurodegenerative diseases. Electronic games related to cognitive abilities are an easy and inexpensive alternative for stimulating brain activity in this kind of patients. The previous research demonstrated the acceptance of these activities in the environment of Connected TV when playing at home and in daily care centers. Interaction in Connected TV applications has its own particularities that influence the design of the interface, including the viewing distance, the type of interaction through a remote control or other techniques, the size of the screen, or the collectiveness of consumption. Iterative testing with patients of these groups revealed how the physical characteristics and cognitive impairment of these concrete end-users affect the human-computer interaction, offering guidelines and recommendations in good practices for the Smart TV interface design. On the other hand, data analytics extracted from the interaction and evolution of the game offer important information enabling the creation of estimation prediction models about the cognitive state of the patient.																	0890-0604	1469-1760				NOV	2019	33	4			SI		442	457	PII S0890060419000386	10.1017/S0890060419000386													
J								Fuzzy approach for production planning by using a three-dimensional printing-based ubiquitous manufacturing system	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Cycle time; fuzzy; slack; three-dimensional printing; ubiquitous manufacturing	SUPPLY CHAIN; PERFORMANCE; DESIGN; CLOUD; MODEL; RISK	A ubiquitous manufacturing (UM) system is used in manufacturing for obtaining the Internet of things solutions and provides location-based manufacturing services. Human-induced uncertainty and early termination are two complications that hamper the effectiveness of an UM system based on three-dimensional (3D) printing. To resolve these complications, several solutions were considered in this study. First, fuzzy-valued parameters were defined to determine uncertainty. Subsequently, slack was derived to determine whether to restart an early terminated 3D printing process in the same 3D printing facility. Consequently, two optimization models - a fuzzy mixed-integer linear programming model and a fuzzy mixed-integer quadratic programming model - were developed in this study. Based on the two optimization models, a fuzzy 3D printing-based UM system that considers uncertainty and early termination was developed. The effectiveness of the proposed methodology was tested by conducting a regional experiment. The experimental results revealed that the proposed methodology could shorten the average cycle time by 9% and could enable 3D printing facilities to make real-time, online reprinting decisions.																	0890-0604	1469-1760				NOV	2019	33	4			SI		458	468	PII S0890060419000222	10.1017/S0890060419000222													
J								Intelligent product-gene acquisition method based on K-means clustering and mutual information-based feature selection algorithm	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Conceptual design; feature selection; K-means; mutual information; product genes	CONCEPTUAL DESIGN; FUNCTIONAL DESIGN; KNOWLEDGE; REPRESENTATION; MODEL	Conceptual design is a key stage of product design and has received increasing attention in recent years. However, this stage is characterized by limited information, large uncertainty, and multidisciplinary aspects. Thus, increased workload and time cost are associated with conceptual design information acquisition; sometimes, it is difficult to develop novel solutions and the feasibility of the solutions obtained according to these limited and uncertain information is difficult to guarantee. Genetics-based design (GBD) is an effective approach to develop novel solutions and improve the reuse of knowledge, which is consistent with the goal of the conceptual design process. Product-gene acquisition is the premise and basis of GBD. At present, there are few reported studies in this area; most of the existing works are constrained by the structural aspects of the acquisition process, and there are limited studies on specific implementation techniques. To explore the specific implementation technologies of product-gene acquisition, an intelligent acquisition method based on K-means clustering and mutual information-based feature selection algorithm is proposed in this paper. The product genes defined in this paper are key product information that determines the nature of the product and influences the conceptual design process. Thus, solutions obtained according to them are more feasible than that based on limited and uncertain information. An illustrative example is presented. The results show that the proposed method can achieve intelligent acquisition of product genes to a certain extent. Further, the proposed method will allow designers to quickly search for the corresponding product genes when performing similar functional design tasks.																	0890-0604	1469-1760				NOV	2019	33	4			SI		469	483	PII S0890060419000258	10.1017/S0890060419000258													
J								Ensemble of surrogates and cross-validation for rapid and accurate predictions using small data sets	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Ensemble of surrogates; kriging; response surface modeling; small data sets; surrogate models	BIDIRECTIONAL IMPULSE TURBINE; EFFICIENT GLOBAL OPTIMIZATION; RESPONSE-SURFACE; METAMODELS; AUSTENITE	In engineering design, surrogate models are often used instead of costly computer simulations. Typically, a single surrogate model is selected based on the previous experience. We observe, based on an analysis of the published literature, that fitting an ensemble of surrogates (EoS) based on cross-validation errors is more accurate but requires more computational time. In this paper, we propose a method to build an EoS that is both accurate and less computationally expensive. In the proposed method, the EoS is a weighted average surrogate of response surface models, kriging, and radial basis functions based on overall cross-validation error. We demonstrate that created EoS is accurate than individual surrogates even when fewer data points are used, so computationally efficient with relatively insensitive predictions. We demonstrate the use of an EoS using hot rod rolling as an example. Finally, we include a rule-based template which can be used for other problems with similar requirements, for example, the computational time, required accuracy, and the size of the data.																	0890-0604	1469-1760				NOV	2019	33	4			SI		484	501	PII S089006041900026X	10.1017/S089006041900026X													
J								Enhanced function-means modeling supporting design space exploration	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Concept design; design space exploration; DSM; enhanced function-means modeling; function modeling; radical innovation	PRODUCT PLATFORM DESIGN; DECOMPOSITION; TECHNOLOGIES	One problem in incremental product development is that geometric models are limited in their ability to explore radical alternative design variants. In this publication, a function modeling approach is suggested to increase the amount and variety of explored alternatives, since function models (FM) provide greater model flexibility. An enhanced function-means (EF-M) model capable of representing the constraints of the design space as well as alternative designs is created through a reverse engineering process. This model is then used as a basis for the development of a new product variant. This work describes the EF-M model's capabilities for representing the design space and integrating novel solutions into the existing product structure and explains how these capabilities support the exploration of alternative design variants. First-order analyses are executed, and the EF-M model is used to capture and represent already existing design information for further analyses. Based on these findings, a design space exploration approach is developed. It positions the FM as a connection between legacy and novel designs and, through this, allows for the exploration of more diverse product concepts. This approach is based on three steps - decomposition, design, and embodiment - and builds on the capabilities of EF-M to model alternative solutions for different requirements. While the embodiment step of creating the novel product's geometry is still a topic for future research, the design space exploration concept can be used to enable wider, more methodological, and potentially automated design space exploration.																	0890-0604	1469-1760				NOV	2019	33	4			SI		502	516	PII S0890060419000271	10.1017/S0890060419000271													
J								Norm emergence in multiagent systems: a viewpoint paper	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Norms; Norm emergence; Norm convergence; Norm life cycle	ARGUMENTATION; ORGANIZATION; EVOLUTION; COMMONS; RULE	Norms are utilised in agent societies to encourage acceptable behaviour by the participating agents. They can be established or revised from the top-down (authority) or from the bottom-up (populace). The study of norm creation from the bottom-up-or norm emergence/convergence-shows evidence of increasing activity. In consequence, we seek to analyse and categorize the approaches proposed in the literature for facilitating norm emergence. This paper makes three contributions to the study of norm emergence. Firstly, we present the different perspectives of norms and their impact on the norm emergence process, with the aim of comparing their similarities and differences in implementing the norm life cycle. Secondly, we identify the characteristics that support norm emergence that are observed in the emergence literature. Finally, we identify and propose future topics for study for the community, through a discussion of the challenges and opportunities in norm emergence.																	1387-2532	1573-7454				NOV	2019	33	6					706	749		10.1007/s10458-019-09422-0													
J								A Causal Time-Series Model Based on Multilayer Perceptron Regression for Forecasting Taiwan Stock Index	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Granger causality test; time-series; MLPR; forecasting technology	SUPPORT VECTOR MACHINES; ARTIFICIAL NEURAL-NETWORKS; GENETIC ALGORITHMS; PREDICTING STOCK; MARKET; COINTEGRATION; DEMAND; PRICES; SYSTEM; OIL	Stock forecasting technology is always a popular research topic because accurate forecasts allow profitable investments and social change. We postulate, based on past research, three major drawbacks for using time series in forecasting stock prices as follows: (1) a simple time-series model provides insufficient explanations for inner and external interactions of the stock market; (2) the variables of a time series behave in strict stationarity, but economic time-series are usually in a nonlinear or nonstationary state and (3) the forecasting factors of multivariable time-series are selected based on researcher's knowledge, and such a method is a "subjective" way to construct a forecasting model. Therefore, this paper proposes a causal time-series model to select forecasting factors and builds a machine learning forecast model. The "Granger causality test" is utilized first in the proposed model to select the critical factors from technical indicators and market indexes; next, a "multilayer perceptron regression (MLPR)" is employed to construct a forecasting model. This paper collected financial data over a 13-year period (from 2003 to 2015) of the Taiwan stock index (TAIEX) as experimental datasets. Furthermore, the root mean square error (RMSE) was used as a performance indicator, and we use five forecasting models as comparison models. The results reveal that the proposed model outperforms the comparison models in forecasting accuracy and performs well for three key indicators. LAG1, S&P500 and DJIA, are critical factors in all 11 of our time sliding windows (T1-T11). We offer these results to investors to aid in their decision-making processes.																	0219-6220	1793-6845				NOV	2019	18	6					1967	1987		10.1142/S0219622019500421													
J								Cosine similarity measures of bipolar neutrosophic set for diagnosis of bipolar disorder diseases	ARTIFICIAL INTELLIGENCE IN MEDICINE										Bipolar; Cosine similarity measure; Multi-attribute decision making; Bipolar disorder diseases	CORRELATION-COEFFICIENT; ENTROPY	Similarity plays a significant implicit or explicit role in various fields. In some real applications in decision making, similarity may bring counterintuitive outcomes from the decision maker's standpoint. Therefore, in this research, we propose some novel similarity measures for bipolar and interval-valued bipolar neutrosophic set such as the cosine similarity measures and weighted cosine similarity measures. The propositions of these similarity measures are examined, and two multi-attribute decision making techniques are presented based on proposed measures. For verifying the feasibility of proposed measures, two numerical examples are presented in comparison with the related methods for demonstrating the practicality of the proposed method. Finally, we applied the proposed measures of similarity for diagnosing bipolar disorder diseases.																	0933-3657	1873-2860				NOV	2019	101								101735	10.1016/j.artmed.2019.101735													
J								Classifying cancer pathology reports with hierarchical self-attention networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										Cancer pathology reports; Clinical reports; Deep learning; Natural language processing; Text classification	CLASSIFICATION; EXTRACTION	We introduce a deep learning architecture, hierarchical self-attention networks (HiSANs), designed for classifying pathology reports and show how its unique architecture leads to a new state-of-the-art in accuracy, faster training, and clear interpretability. We evaluate performance on a corpus of 374,899 pathology reports obtained from the National Cancer Institute's (NCI) Surveillance, Epidemiology, and End Results (SEER) program. Each pathology report is associated with five clinical classification tasks - site, laterality, behavior, histology, and grade. We compare the performance of the HiSAN against other machine learning and deep learning approaches commonly used on medical text data - Naive Bayes, logistic regression, convolutional neural networks, and hierarchical attention networks (the previous state-of-the-art). We show that HiSANs are superior to other machine learning and deep learning text classifiers in both accuracy and macro F-score across all five classification tasks. Compared to the previous state-of-the-art, hierarchical attention networks, HiSANs not only are an order of magnitude faster to train, but also achieve about 1% better relative accuracy and 5% better relative macro F-score.																	0933-3657	1873-2860				NOV	2019	101								101726	10.1016/j.artmed.2019.101726													
J								A novel Chinese herbal medicine clustering algorithm via artificial bee colony optimization	ARTIFICIAL INTELLIGENCE IN MEDICINE										Artificial bee colony; Cluster analysis; Parameter optimization; Traditional Chinese medicine; Applied intelligence	GRAPH	Traditional Chinese medicine (TCM) has become popular and been viewed as an effective clinical treatment across the world. Accordingly, there is an ever-increasing interest in performing data analysis over TCM data. Aiming to cope with the problem of excessively depending on empirical values when selecting cluster centers by traditional clustering algorithms, an improved artificial bee colony algorithm is proposed by which to automatically select cluster centers and apply it to aggregate Chinese herbal medicines. The proposed method integrates the following new techniques: (1) improving the artificial bee colony algorithm by applying a new searching strategy of neighbour nectar, (2) employing the improved artificial bee colony algorithm to optimize the parameters of the cutoff distance d(c) the local density rho(i) and the minimum distance delta(i) between the element i and any other element with higher density in the cluster algorithm by fast search and finding of density peaks (called DP algorithm) to find the optimal cluster centers, in order to clustering herbal medicines in an accurate fashion with the guarantee of runtime performance. Extensive experiments were conducted on the UCI benchmark datasets and the TCM datasets and the results verify the effectiveness of the proposed method by comparing it with classical clustering algorithms including K-means, K-mediods and DBSCAN in multiple evaluation metrics, that is, Silhouette Coefficient, Entropy, Purity, Precision, Recall and Fl-Measure. The results show that the IABC-DP algorithm outperforms other approaches with good clustering quality and accuracy on the UCI and the TCM datasets as well. In addition, it can be found that the improved artificial bee colony algorithm can effectively reduce the number of iterations when compared to the traditional bee colony algorithm. In particular, the IABC-DP algorithm is applied to cluster multi-dimensional Chinese herbal medicines and the result shows that it outperforms other clustering algorithms in clustering Chinese herbal medicines, which can contribute to a larger effort targeted at advancing the study of discovering composition rules of traditional Chinese prescriptions.																	0933-3657	1873-2860				NOV	2019	101								101760	10.1016/j.artmed.2019.101760													
J								The social phenotype: Extracting a patient-centered perspective of diabetes from health-related blogs	ARTIFICIAL INTELLIGENCE IN MEDICINE										Social phenotype; Diabetic patients; Doctor-patient communications; Social analytics; LDA; Topic clustering; Word embeddings; Quality of life assessment	MEDIA	Motivations: It has recently been argued [1] that the effectiveness of a cure depends on the doctor-patient shared understanding of an illness and its treatment. Although a better communication between doctor and patient can be pursued through dedicated training programs, or by collecting patients' experiences and symptoms by means of questionnaires, the impact of these actions is limited by time and resources. In this paper we suggest that a patient-centered view of a disease - as well as potential misalignment between patient and doctor focuses - can be inferred at a larger scale through automated textual analysis of health-related forums. People are generating an enormous amount of social data to describe their health care experiences, and continuously search information about diseases, symptoms, diagnoses, doctors, treatment options and medicines. By automatically collecting, analyzing and exploiting this information, it is possible to obtain a more detailed and nuanced vision of patients' experience, that we call the "social phenotype" of diseases. Materials and methods: As a use-case for our analysis, we consider diabetes, a widespread disease in most industrialized countries. We create a high quality data sample of diabetic patients' messages in Italy, extracted from popular medical forums during more than 10 years. Next, we use a state-of-the-art topic extraction technique based on generative statistical models improved with word embeddings, to identify the main complications, the frequently reported symptoms and the common concerns of these patients. Finally, in order to detect differences in focus, we compare the results of our analysis with available quality of life (QoL) assessments obtained with standard methodologies, such as questionnaires and survey studies. Results: We show that patients with diabetes, when accessing on-line forums, express a perception of their disease in a way that might be noticeably different from what is inferred from published QoL assessments on diabetes. In our study, we found that issues reported to have a daily impact on these patients are diet, glycemic control, drugs and clinical tests. These problems are not commonly considered in QoL assessments, since they are not perceived by doctors as representing severe limitations. Although limited to the case of Italian diabetic patients, we suggest that the methodology described in this paper, which is language and disease agnostic, could be applied to other diseases and countries, since misalignment between doctor and patients, and the importance of collecting unbiased patient perceptions, has been emphasized in many studies ([2,3] inter aria). Extracting the social phenotype of a disease might help acquiring patient-centered information on health care experiences on a much wider scale.																	0933-3657	1873-2860				NOV	2019	101								101727	10.1016/j.artmed.2019.101727													
J								A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical dataset	ARTIFICIAL INTELLIGENCE IN MEDICINE										Stroke prediction; Clinical decision; Class imbalance; Hybrid machine learning; AutoHPO	CLASSIFICATION	Background and Objective: Cerebral stroke has become a significant global public health issue in recent years. The ideal solution to this concern is to prevent in advance by controlling related metabolic factors. However, it is difficult for medical staff to decide whether special precautions are needed for a potential patient only based on the monitoring of physiological indicators unless they are obviously abnormal. This paper will develop a hybrid machine learning approach to predict cerebral stroke for clinical diagnosis based on the physiological data with incompleteness and class imbalance. Methods: Two steps are involved in the whole process. Firstly, random forest regression is adopted to impute missing values before classification. Secondly, an automated hyperparameter optimization(AutoHPO) based on deep neural network(DNN) is applied to stroke prediction on an imbalanced dataset. Results: The medical dataset contains 43,400 records of potential patients which includes 783 occurrences of stroke. The false negative rate from our prediction approach is only 19.1%, which has reduced by an average of 51.5% in comparison to other traditional approaches. The false positive rate, accuracy and sensitivity predicted by the proposed approach are respectively 33.1, 71.6, and 67.4%. Conclusion: The approach proposed in this paper has effectively reduced the false negative rate with a relatively high overall accuracy, which means a successful decrease in the misdiagnosis rate for stroke prediction. The results are more reliable and valid as the reference in stroke prognosis, and also can be acquired conveniently at a low cost.																	0933-3657	1873-2860				NOV	2019	101								101723	10.1016/j.artmed.2019.101723													
J								Compositional model based on factorial evolution for realizing multi-task learning in bacterial virulent protein prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Multifactorial evolutionary algorithm; Multitask learning; Multi-kernel learning; Virulent protein	LOCATION; DATABASE; FRAMEWORK; SELECTION; ENSEMBLE	The ability of multitask learning promulgated its sovereignty in the machine learning field with the diversified application including but not limited to bioinformatics and pattern recognition. Bioinformatics provides a wide range of applications for Multitask Learning (MTL) methods. Identification of Bacterial virulent protein is one such application that helps in understanding the virulence mechanism for the design of drug and vaccine. However, the limiting factor in a reliable prediction model is the scarcity of the experimentally verified training data. To deal with, casting the problem in a Multitask Learning scenario, could be beneficial. Reusability of auxiliary data from related multiple domains in the prediction of target domain with limited labeled data is the primary objective of multitask learning model. Due to the amalgamation of multiple related data, it is possible that the probability distribution between the features tends to vary. Therefore, to deal with change amongst the feature distribution, this paper proposes a composite model for multitask learning framework which is based on two principles: discovering the shared parameters for identifying the relationships between tasks and common underlying representation of features amongst the related tasks. Through multi-kernel and factorial evolution, the proposed framework able to discover the shared kernel parameters and latent feature representation that is common amongst the tasks. To examine the benefits of the proposed model, an extensive experiment is performed on the freely available dataset at VirulentPred web server. Based on the results, we found that multitask learning model performs better than the conventional single task model. Additionally, our findings state that if the distribution between the tasks is high, then training the multiple models yield slightly better prediction. However, if the data distribution difference is low, multitask learning significantly outperforms the individual learning.																	0933-3657	1873-2860				NOV	2019	101								101757	10.1016/j.artmed.2019.101757													
J								A predictive analytics framework for identifying patients at risk of developing multiple medical complications caused by chronic diseases	ARTIFICIAL INTELLIGENCE IN MEDICINE										Predictive analytics; Chronic disease; Artiftcial neural networks; Multi-Task learning; Regression	ELECTRONIC HEALTH RECORDS; DECISION-SUPPORT; DIABETIC-RETINOPATHY; LOGISTIC-REGRESSION; DATA QUALITY; MODEL; CLASSIFICATION; CARE; TREE; CARDIOMYOPATHY	Chronic diseases often cause several medical complications. This paper aims to predict multiple complications among patients with a chronic disease. The literature uses single-task learning algorithms to predict complications independently and assumes no correlation among complications of chronic diseases. We propose two methods (independent prediction of complications with single-task learning and concurrent prediction of complications with multi-task learning) and show that medical complications of chronic diseases can be correlated. We use a case study and compare the performance of these two methods by predicting complications of hypertrophic cardiomyopathy on 106 predictors in 1078 electronic medical records from April 2009-April 2017, inclusive. The methods are implemented using logistic regression, artificial neural networks, decision trees, and support vector machines. The results show multi-task learning with logistic regression improves the performance of predictions in terms of both discrimination and calibration.																	0933-3657	1873-2860				NOV	2019	101								101750	10.1016/j.artmed.2019.101750													
J								Automated classification of histopathology images using transfer learning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Medical image classification; Histopathology; Deep learning; Transfer learning; CNN	DEEP; CANCER; DIAGNOSIS	Early and accurate diagnosis of diseases can often save lives. Diagnosis of diseases from tissue samples is done manually by pathologists. Diagnostics process is usually time consuming and expensive. Hence, automated analysis of tissue samples from histopathology images has critical importance for early diagnosis and treatment. The computer aided systems can improve the quality of diagnoses and give pathologists a second opinion for critical cases. In this study, a deep learning based transfer learning approach has been proposed to classify histopathology images automatically. Two well-known and current pre-trained convolutional neural network (CNN) models, ResNet-50 and DenseNet-161, have been trained and tested using color and grayscale images. The DenseNet-161 tested on grayscale images and obtained the best classification accuracy of 97.89%. Additionally, ResNet-50 pre-trained model was tested on the color images of the Kimia Path24 dataset and achieved the highest classification accuracy of 98.87%. According to the obtained results, it may be said that the proposed pre-trained models can be used for fast and accurate classification of histopathology images and assist pathologists in their daily clinical tasks.																	0933-3657	1873-2860				NOV	2019	101								101743	10.1016/j.artmed.2019.101743													
J								Motor imagery EEG recognition with KNN-based smooth auto-encoder	ARTIFICIAL INTELLIGENCE IN MEDICINE										KNN-based smooth auto-encoder; BCI; Motor imagery; Feature extraction; EEG recognition	WAVELET TRANSFORM; CLASSIFICATION; MACHINE; PATTERN; SYSTEM	As new human-computer interaction technology, brain-computer interface has been widely used in various fields of life. The study of EEG signals cannot only improve people's awareness of the brain, but also establish new ways for the brain to communicate with the outside world. This paper takes the motion imaging EEG signal as the research object and proposes an innovative semi-supervised model called KNN-based smooth auto-encoder (k-SAE). K-SAE looks for the nearest neighbor values of the samples to construct a new input and learns the robust features representation by reconstructing this new input instead of the original input, which is different from the traditional automatic encoder (AE), The Gaussian filter is selected as the convolution kernel function in k-SAE to smooth the noise in the feature. Besides, the data information and spatial position of the feature map are recorded by max-pooling and unpooling, that help to prevent loss of important information. The method is applied to two data sets for feature extraction and classification experiments of motor imaging EEG signals. The experimental results show that k-SAE achieves good recognition accuracy and outperforms other state-of-the-art recognition algorithms.																	0933-3657	1873-2860				NOV	2019	101								101747	10.1016/j.artmed.2019.101747													
J								Methods for algorithmic diagnosis of metabolic syndrome	ARTIFICIAL INTELLIGENCE IN MEDICINE										Linear regression; Artificial neural network; Decision tree; Random forest; Metabolic syndrome	ARTIFICIAL NEURAL-NETWORK; ADIPOSE-TISSUE; RISK-FACTORS; INTELLIGENCE; STATE	Metabolic Syndrome (MetS) is associated with the risk of developing chronic disease (atherosclerotic cardiovascular disease, type 2 diabetes, cancers and chronic kidney disease) and has an important role in early prevention. Previous research showed that an artificial neural network (ANN) is a suitable tool for algorithmic MetS diagnostics, that includes solely non-invasive, low-cost and easily-obtainabled (NI&LC&EO) diagnostic methods. This paper considers using four well-known machine learning methods (linear regression, artificial neural network, decision tree and random forest) for MetS predictions and provides their comparison, in order to induce and facilitate development of appropriate medical software by using these methods. Training, validation and testing are conducted on the large dataset that includes 3000 persons. Input vectors are very simple and contain the following parameters: gender, age, body mass index, waist-to-height ratio, systolic and diastolic blood pressures, while the output is MetS diagnosis in true/false form, made in accordance with International Diabetes Federation (IDF). Comparison leads to the conclusion that random forest achieves the highest specificity (SPC = 0.9436), sensitivity (SNS = 0.9154), positive (PPV = 0.9379) and negative (NPV = 0.9150) predictive values. Algorithmic diagnosis of MetS could be beneficial in everyday clinical practice since it can easily identify high risk patients.																	0933-3657	1873-2860				NOV	2019	101								101708	10.1016/j.artmed.2019.101708													
J								Study on miR-384-5p activates TGF-beta signaling pathway to promote neuronal damage in abutment nucleus of rats based on deep learning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Anesthesia; Dysfunction module; Multifactorial; Basal lateral nucleus neurons	BASOLATERAL AMYGDALA; INDUCED AMNESIA; EXPRESSION; HIPPOCAMPAL; RECEPTORS	Background: Any ailment in our organs can be visualized by using different modality signals and images. Hospitals are encountering a massive influx of large multimodality patient data to be analysed accurately and with context understanding. The deep learning techniques, like convolution neural networks (CNN), long short-term memory (LSTM), autoencoders, deep generative models and deep belief networks have already been applied to efficiently analyse possible large collections of data. Application of these methods to medical signals and images can aid the clinicians in clinical decision making. Purpose: The aim of this study was to explore its potential application mechanism to the abalone basal ganglia neurons in rats based on deep learning. Patients and methods: Firstly, in the GEO database, we obtained data on rat anesthesia, performing differential analysis, co-expression analysis, and enrichment analysis, and then we received the relevant module genes. Besides, the potential regulation of multi-factors on the module was calculated by hypergeometric test, and a series of ncRNA and TF were identified. Finally, we screened the target genes of anesthetized rats to gain insight into the potential role of anesthesia in rat basal lateral nucleus neurons. Results: A total of 535 differentially expressed genes in rats were obtained, involving Mafb and Ryr2. These genes are clustered into 17 anesthesia-related expression disorder modules. At the same time, the biological processes favored by the module are regulation of neuron apoptotic process and transforming growth factor beta2 production. Pivot analysis found that 39 ncRNAs and 4 TFs drive anesthesia-related disorders. Finally, the mechanism of action was analyzed and predicted. The module was regulated by Acvr1. We believe that miR-384-5p in anesthetized rats can activate the TGF-beta signaling pathway. Further, it promotes anesthesia and causes exposure to the basal ganglia neuron damage of the amygdala. Conclusion: In this study, the imbalance module was used to explore the multi-factor-mediated anesthesia application mechanism, which provided new methods and ideas for subsequent research. The results suggest that miR-384-5p can promote anesthesia damage to the abalone basal ganglia neurons in rats through a variety of biological processes and signaling pathways. This result lays a solid theoretical foundation for biologists to explore the application mechanism of anesthesiology further.																	0933-3657	1873-2860				NOV	2019	101								101740	10.1016/j.artmed.2019.101740													
J								Neural network methodology for real-time modelling of bio-heat transfer during thermo-therapeutic applications	ARTIFICIAL INTELLIGENCE IN MEDICINE										Cellular neural networks; Pennes bio-heat transfer equation; Real-time computation; Thermo-therapeutic treatment	INTENSITY FOCUSED ULTRASOUND; BIOHEAT TRANSFER EQUATION; FINITE-ELEMENT ALGORITHM; NUMERICAL-SIMULATION; INFORMATION PROPAGATION; LIVER-TISSUE; ABLATION; DEFORMATION; PARADIGM; FLOW	Real-time simulation of bio-heat transfer can improve surgical feedback in thermo-therapeutic treatment, leading to technical innovations to surgical process and improvements to patient outcomes; however, it is challenging to achieve real-time computational performance by conventional methods. This paper presents a cellular neural network (CNN) methodology for fast and real-time modelling of bio-heat transfer with medical applications in thermo-therapeutic treatment. It formulates nonlinear dynamics of the bio-heat transfer process and spatially discretised bio-heat transfer equation as the nonlinear neural dynamics and local neural connectivity of CNN, respectively. The proposed CNN methodology considers three-dimensional (3-D) volumetric bio-heat transfer behaviour in tissue and applies the concept of control volumes for discretisation of the Pennes bio-heat transfer equation on 3-D irregular grids, leading to novel neural network models embedded with bio-heat transfer mechanism for computation of tissue temperature and associated thermal dose. Simulations and comparative analyses demonstrate that the proposed CNN models can achieve good agreement with the commercial finite element analysis package, ABAQUS/CAE, in numerical accuracy and reduce computation time by 304 and 772.86 times compared to those of with and without ABAQUS parallel execution, far exceeding the computational performance of the commercial finite element codes. The medical application is demonstrated using a high-intensity focused ultrasound (HIFU)-based thermal ablation of hepatic cancer for prediction of tissue temperature and estimation of thermal dose.																	0933-3657	1873-2860				NOV	2019	101								101728	10.1016/j.artmed.2019.101728													
J								Robust retinal blood vessel segmentation using convolutional neural network and support vector machine	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Convolutional neural network; Feature learning; Median filter; Retinal blood vessel segmentation and classification; Support vector machine	IMAGES	In recent decades, automatic retinal blood vessel segmentation and classification (RBVSC) helps to determine many diseases such as glaucoma, hypertension, macular-degeneration, diabetes-mellitus, etc. The early recognition of these disorders is essential for preventing patients from blindness. In this work, a new supervised system was developed to enhance the performance of RBVSC. At first, the input retinal images were collected from two datasets such as: Digital Retinal Image for Vessel Extraction (DRIVE) and STARE (STructured Analysis of the Retina). Then, the retinal vessels were segmented utilizing mean orientation based super-pixel segmentation. Besides, Convolutional Neural Network (CNN) was applied to extract the feature vectors from segmented regions. Finally, a binary classifier [Support Vector Machine (SVM)] performs classification on the extracted features for classifying the "vessel" and "non-vessel" regions. The combination of CNN and SVM automatically learns the feature values from raw images and classifies the patterns easily. From the experimental study, the proposed system improved RBVSC up to 2-4% compared to other existing systems and classification methodologies: Deep Neural Network (DNN), Random Forest (RF) and Naive Bayes (NB) by means of specificity, accuracy, sensitivity and kappa index.																	1868-5137	1868-5145															10.1007/s12652-019-01559-w		OCT 2019											
J								Integrated computational intelligent paradigm for nonlinear electric circuit models using neural networks, genetic algorithms and sequential quadratic programming	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Nonlinear systems; Nonlinear electric circuits; Genetic algorithms; Sequential quadratic programming	INTERIOR-POINT ALGORITHM; NUMERICAL-SOLUTION; DIFFERENTIAL-EQUATIONS; TOPOLOGY OPTIMIZATION; DESIGN; DYNAMICS; FLOW; ANALYZE; SYSTEMS; FLUID	In this paper, a novel application of biologically inspired computing paradigm is presented for solving initial value problem (IVP) of electric circuits based on nonlinear RL model by exploiting the competency of accurate modeling with feed forward artificial neural network (FF-ANN), global search efficacy of genetic algorithms (GA) and rapid local search with sequential quadratic programming (SQP). The fitness function for IVP of associated nonlinear RL circuit is developed by exploiting the approximation theory in mean squared error sense using an approximate FF-ANN model. Training of the networks is conducted by integrated computational heuristic based on GA-aided with SQP, i.e., GA-SQP. The designed methodology is evaluated to variants of nonlinear RL systems based on both AC and DC excitations for number of scenarios with different voltages, resistances and inductance parameters. The comparative studies of the proposed results with Adam's numerical solutions in terms of various performance measures verify the accuracy of the scheme. Results of statistics based on Monte-Carlo simulations validate the accuracy, convergence, stability and robustness of the designed scheme for solving problem in nonlinear circuit theory.																	0941-0643	1433-3058				JUL	2020	32	14					10337	10357		10.1007/s00521-019-04573-3		OCT 2019											
J								Evolutionary echo state network for long-term time series prediction: on the edge of chaos	APPLIED INTELLIGENCE										Time series prediction; Echo state network; Edge-of-chaos criticality; Evolutionary algorithm	NEURAL-NETWORKS; FUZZY-LOGIC; OPTIMIZATION; PERFORMANCE; ALGORITHM; SELECTION; SYSTEMS; POWER	Quantitative analysis of neural networks is a critical issue to improve their performance. In this paper, we investigate a long-term time series prediction based on the echo state network operating at the edge of chaos. We also assess the eigenfunction of echo state networks and its criticality by the Hermite polynomials. A Hermite polynomial-based activation function design with fast convergence is proposed and the relation between long-term time dependence and edge-of-chaos criticality is given. A new particle swarm optimization-gravitational search algorithm is put forward to improve the parameters estimation that helps attain on the edge of chaos. The method was verified using a chaotic Lorenz system and a real health index data set. The experimental results indicate that evolution makes the reservoir great potential to run on the edge of chaos with rich expression.																	0924-669X	1573-7497				MAR	2020	50	3					893	904		10.1007/s10489-019-01546-w		OCT 2019											
J								An Investigation of Vehicle Behavior Prediction Using a Vector Power Representation to Encode Spatial Positions of Multiple Objects and Neural Networks	FRONTIERS IN NEUROROBOTICS										vehicle prediction; long short-term memories; artificial neural networks; vector symbolic architectures; online learning; spiking neural networks	MODEL	Predicting future behavior and positions of other traffic participants from observations is a key problem that needs to be solved by human drivers and automated vehicles alike to safely navigate their environment and to reach their desired goal. In this paper, we expand on previous work on an automotive environment model based on vector symbolic architectures (VSAs). We investigate a vector-representation to encapsulate spatial information of multiple objects based on a convolutive power encoding. Assuming that future positions of vehicles are influenced not only by their own past positions and dynamics (e.g., velocity and acceleration) but also by the behavior of the other traffic participants in the vehicle's surroundings, our motivation is 3-fold: we hypothesize that our structured vector-representation will be able to capture these relations and mutual influence between multiple traffic participants. Furthermore, the dimension of the encoding vectors remains fixed while being independent of the number of other vehicles encoded in addition to the target vehicle. Finally, a VSA-based encoding allows us to combine symbol-like processing with the advantages of neural network learning. In this work, we use our vector representation as input for a long short-term memory (LSTM) network for sequence to sequence prediction of vehicle positions. In an extensive evaluation, we compare this approach to other LSTM-based benchmark systems using alternative data encoding schemes, simple feed-forward neural networks as well as a simple linear prediction model for reference. We analyze advantages and drawbacks of the presented methods and identify specific driving situations where our approach performs best. We use characteristics specifying such situations as a foundation for an online-learning mixture-of-experts prototype, which chooses at run time between several available predictors depending on the current driving situation to achieve the best possible forecast.																	1662-5218					OCT 16	2019	13								84	10.3389/fnbot.2019.00084													
J								IC-SMART: IoTCloud enabled Seamless Monitoring for Alzheimer Diagnosis and Rehabilitation SysTem	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neurological disorder; Alzheimer's disease (AD) diagnosis; Ontology; Bayesian network (BN); IoTCloud based healthcare	HEALTH-CARE; INCOMPLETE DATA; ALGORITHMS; SELECTION; SERVICES; INTERNET; DISEASE; THINGS	Alzheimer disease (AD), a progressive neurodegenerative disease is related with the gradual loss of structure or disturbance of neuronal functions and deterioration in cognitive functions. Timely diagnosis of this disease countenances prompt treatment and headways to patient's quality of life. This paper proposes IC-SMART, a novel IoTCloud based Seamless Monitoring for Alzheimer Diagnosis and Rehabilitation SysTem (SMART) that leverages semantic modeling, specifically, ontology modeling for structuring and representing pertinent knowledge explicit to AD. Patient's data collected from the "things" such as sensory devices and the inputs provided by the general practitioners and specialists in the AD realm has been integrated in the knowledge base for construction of a Bayesian Network decision model to ascertain the possibility of patient being diagnosed with AD. Furthermore, IC-SMART offers users with the intelligent capabilities of accomplishing multiple control functions such as patient diagnosis, messaging and communication, real time and historical alarm generation, and navigation based assistance to rehabilitation centers. To verify the feasibility of the proposed IC-SMART, an android based mobile cloud software service has been designed and deployed on Amazon EC2 cloud. The classification accuracy of Alzheimer diagnosis using ontology based Bayesian network model has been validated by obtaining the classification results from well-known classifiers such as Naive Bayes, J48 decision tree and decision stump. Further, sensitivity analysis has been carried out to verify the robustness of IC-SMART. The evaluation results attained for the prototype implementation prove to be very promising.																	1868-5137	1868-5145				AUG	2020	11	8					3387	3403		10.1007/s12652-019-01534-5		OCT 2019											
J								Model-Based Robot Imitation with Future Image Similarity	INTERNATIONAL JOURNAL OF COMPUTER VISION										Robot action policy learning; Behavioral cloning; Model-based RL	TIME	We present a visual imitation learning framework that enables learning of robot action policies solely based on expert samples without any robot trials. Robot exploration and on-policy trials in a real-world environment could often be expensive/dangerous. We present a new approach to address this problem by learning a future scene prediction model solely from a collection of expert trajectories consisting of unlabeled example videos and actions, and by enabling action selection using future image similarity. In this approach, the robot learns to visually imagine the consequences of taking an action, and obtains the policy by evaluating how similar the predicted future image is to an expert sample. We develop an action-conditioned convolutional autoencoder, and present how we take advantage of future images for zero-online-trial imitation learning. We conduct experiments in simulated and real-life environments using a ground mobility robot with and without obstacles in reaching target objects. We explicitly compare our models to multiple baseline methods requiring only offline samples. The results confirm that our proposed methods perform superior to previous methods, including 1.5 x and 2.5 x higher success rate in two different tasks than behavioral cloning.																	0920-5691	1573-1405				MAY	2020	128	5					1360	1374		10.1007/s11263-019-01238-5		OCT 2019											
J								Integrating Classical Control into Reinforcement Learning Policy	NEURAL PROCESSING LETTERS										Reinforcement learning; Deep learning; Neural network; Control theory	GAME; GO	Deep reinforcement learning has made impressive advances in sequential decision making problems recently. Constructive reinforcement learning (RL) algorithms have been proposed to focus on the policy optimization process, while further research on the effect of different policy network has not been fully explored. MLPs, LSTMs and linear layer are complementary in their controlling capabilities, as MLPs are appropriate for global control, LSTMs are able to exploit history information and linear layer is good at stabilizing system dynamics. In this paper, we propose a "Proportional-Integral" (PI) neural network architecture that could be easily combined with popular optimization algorithms. This PI-patterned policy network exploits the advantages of integral control and linear control that are widely applied in classic control systems, based on which an ensemble-learning-based model is trained to further improve the sample efficiency and training performance on most RL tasks. Experimental results on public RL simulation platforms demonstrate the proposed architecture could achieve better performance than generally used MLP and other existing applied models.																	1370-4621	1573-773X															10.1007/s11063-019-10127-4		OCT 2019											
J								Hybrid Brain-Computer-Interfacing for Human-Compliant Robots: Inferring Continuous Subjective Ratings With Deep Regression	FRONTIERS IN NEUROROBOTICS										robot behavior; autonomous robots; BCI; regression; CNN; deep learning; random forests; support vector machines	NEURAL-NETWORKS; FRONTAL THETA; EEG; ADAPTATION; ERRORS	Appropriate robot behavior during human-robot interaction is a key part in the development of human-compliant assistive robotic systems. This study poses the question of how to continuously evaluate the quality of robotic behavior in a hybrid brain-computer interfacing (BCI) task, combining brain and non-brain signals, and how to use the collected information to adapt the robot's behavior accordingly. To this aim, we developed a rating system compatible with EEG recordings, requiring the users to execute only small movements with their thumb on a wireless controller to rate the robot's behavior on a continuous scale. The ratings were recorded together with dry EEG, respiration, ECG, and robotic joint angles in ROS. Pilot experiments were conducted with three users that had different levels of previous experience with robots. The results demonstrate the feasibility to obtain continuous rating data that give insight into the subjective user perception during direct human-robot interaction. The rating data suggests differences in subjective perception for users with no, moderate, or substantial previous robot experience. Furthermore, a variety of regression techniques, including deep CNNs, allowed us to predict the subjective ratings. Performance was better when using the position of the robotic hand than when using EEG, ECG, or respiration. A consistent advantage of features expected to be related to a motor bias could not be found. Across-user predictions showed that the models most likely learned a combination of general and individual features across-users. A transfer of pre-trained regressor to a new user was especially accurate in users with more experience. For future research, studies with more participants will be needed to evaluate the methodology for its use in practice. Data and code to reproduce this study are available at https://github.com/TNTLFreiburg/NiceBot.																	1662-5218					OCT 10	2019	13								76	10.3389/fnbot.2019.00076													
J								Robust Event-Based Object Tracking Combining Correlation Filter and CNN Representation	FRONTIERS IN NEUROROBOTICS										event-based vision; object tracking; dynamic vision sensor; convolutional neural network; correlation filter	VISION	Object tracking based on the event-based camera or dynamic vision sensor (DVS) remains a challenging task due to the noise events, rapid change of event-stream shape, chaos of complex background textures, and occlusion. To address the challenges, this paper presents a robust event-stream object tracking method based on correlation filter mechanism and convolutional neural network (CNN) representation. In the proposed method, rate coding is used to encode the event-stream object. Feature representations from hierarchical convolutional layers of a pre-trained CNN are used to represent the appearance of the rate encoded event-stream object. Results prove that the proposed method not only achieves good tracking performance in many complicated scenes with noise events, complex background textures, occlusion, and intersected trajectories, but also is robust to variable scale, variable pose, and non-rigid deformations. In addition, the correlation filter-based method has the advantage of high speed. The proposed approach will promote the potential applications of these event-based vision sensors in autonomous driving, robots and many other high-speed scenes.																	1662-5218					OCT 10	2019	13								82	10.3389/fnbot.2019.00082													
J								Edge detection based on type-1 fuzzy logic and guided smoothening	EVOLVING SYSTEMS										Edge detection; Image enhancement; Fuzzy logic; Guided filtering		Edge detection is an important phenomenon in computer vision. Edge detection is helpful in contour detection and thus helpful in obtaining the important information. Edge detection process heavily depends on chosen technique. Soft computing techniques are considered as powerful edge detection methods due to their adaptability. This paper presents a fuzzy logic based edge detection method where the quality of edges is controlled using sharpening guided filter and noise due to the sharpening is controlled using Gaussian filter. The accuracy of the method is judged using a variety of statistical measures. It has been found that by proper selecting the smoothening parameters a significant improvement in the detected edges can be obtained.																	1868-6478	1868-6486															10.1007/s12530-019-09304-6		OCT 2019											
J								Logistics forum based prediction on stock index using intelligent data analysis and processing of online web posts	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Logistics Forum; Online posts; e-Commerce; Stock Market; Predictability; Hypertext analysis	INFORMATION-CONTENT; INTERNET; TALK; SENTIMENT	This paper focuses on the e-commerce contents of logistics forum. We construct four logistics forums metrics on the degree of busyness and emotional states of the logistics staffs based through hypertext analysis techniques. By choosing a set of individual stocks from e-commerce sector and establishing e-commerce stock index, we examine the empirical evidence of the predictability and degree of influence of e-commerce logistics forum on e-commerce stock index. We find that emotion index and size of web posts may help predict ratio of earnings whereas emotion index, web post index, and disagreement index may help predict exchange volumes. Meanwhile, logistics forum metrics have short term or mid-term influence on e-commerce stock index, ratio of earnings, volatility, and trading volume whereas the influence of emotion index could last more than 40 weeks. Our results help people to understand the interrelationship between the forum contents of logistics social network and stock market activities and provide important decision advice for investors.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3575	3584		10.1007/s12652-019-01520-x		OCT 2019											
J								Expected stock return and mixed frequency variance risk premium data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ADL-MIDAS model; Variance risk premium; Forecast combination; Expected stock return	FORECAST COMBINATION; OUTPUT GROWTH; MARKET VOLATILITY; MODEL	Motivated by the research of the variance risk premium (VRP) and MIDAS model, we employ the VRP with different maturities and the ADL-MIDAS regression model to forecast the expected stock return in Standard & Poor 500 market. The VRP is defined as the difference between the realized variance and the implied variance of the options. By using Standard & Poor 500 index options, we provide the empirical tests of the forecasting performance provided by the VRP with different maturities to the expected stock returns in the Standard & Poor 500 stock index market. Based on the empirical results, we know the VRP with 1-month and 2-month maturities can provide the best out-of-sample forecast.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3585	3596		10.1007/s12652-019-01528-3		OCT 2019											
J								Module partitioning for multilayer brain functional network using weighted clustering ensemble	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Module partitioning; Multilayer brain functional network; Weighted clustering ensemble; FuzzyC-means clustering	CONVOLUTIONAL NEURAL-NETWORK; EFFECTIVE CONNECTIVITY; COMMUNITY STRUCTURE; ORGANIZATION; ALGORITHM	Module can not only affect the integration of network functions, but also contribute to understand the characteristics of local connections in the network. However, the connection between nodes in the network changes with the passage of time, and the module structure changes accordingly. To overcome this drawback, we propose a method of applying weighted clustering ensemble to partition multilayer brain functional networks into modules. Firstly,k-means clustering is adopted to carry out base clustering for a certain layer of functional network for several times, and each clustering is corresponding to a subordinate matrix and a similarity matrix. Then clustering validity index is used to assess each partitioning and the assessed values are taken as the weights of similarity matrix. Finally, the weighted similarity matrix is partitioned by means of fuzzyC-means clustering, and the results are evaluated by modularity function to obtain the optimal partitioned modules. Experimental results show that the effect of module partitioning resulting from weighted clustering ensemble is better than that of other comparable methods. The proposed framework could be promising to analyze the differences between corresponding modules of patients with Alzheimer's disease and normal people, so as to better understanding some dynamical pathological changes in brain connectome of patients.																	1868-5137	1868-5145															10.1007/s12652-019-01535-4		OCT 2019											
J								Mobile sink-based energy efficient cluster head selection strategy for wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network (WSN); Mobile sink; Energy efficient protocols (EEP); Cluster head (CH); Firefly optimization	ROUTING PROTOCOL; LIFETIME; ALGORITHMS	Energy efficient routing protocol is the requirement of today's wireless sensor networks. Various protocols have been developed in order to create an energy efficient wireless sensor network, but still some loopholes exist in this domain and energy hole is one of them. Energy hole refers to the early energy diminution of those nodes that are near to the sink. This study introduced a mobile sink based energy aware clustering mechanism to enhance the lifetime of the network by overcoming the issue of energy holes. In proposed work, the network is initially divided into the number of rectangular regions and each region is comprised of one cluster head (CH). The nature-inspired firefly optimization algorithm is used to select cluster heads where residual energy, average node to node distance and distance from the node to sink are the decisive parameters of the process. The sink moves in the observing field after estimating the centroid location of the CHs. The performance of the proposed work is compared with the LEACH, LEACH-GA, A-LEACH, MIEEPB, and MSIEEP by using Matlab simulation platform. The result section represents the proficiency of the proposed MSECA protocol over traditional techniques in term of network lifetime, packet delivery ratio and packet delay.																	1868-5137	1868-5145															10.1007/s12652-019-01509-6		OCT 2019											
J								Ramp-based twin support vector clustering	NEURAL COMPUTING & APPLICATIONS										Clustering; Plane-based clustering; Ramp function; Twin support vector clustering; Nonconvex programming	MACHINE; SELECTION	Traditional plane-based clustering methods measure the within-cluster or between-cluster scatter by linear, quadratic or some other unbounded functions, which are sensitive to the samples far from the cluster center. This paper introduces the ramp functions into plane-based clustering and proposes a ramp-based twin support vector clustering (RampTWSVC). RampTWSVC is very robust to the samples far from the cluster center, because its within-cluster and between-cluster scatters are measured by the bounded ramp functions. Thus, it is easier to find the intrinsic clusters than other plane-based clustering methods. The nonconvex programming problem in RampTWSVC is solved efficiently through an alternating iteration algorithm, and its local solution can be obtained in a finite number of iterations theoretically. In addition, its nonlinear manifold clustering formation is also proposed via a kernel trick. Experimental results on several benchmark datasets show the better performance of our RampTWSVC compared with other plane-based clustering methods.																	0941-0643	1433-3058				JUL	2020	32	14					9885	9896		10.1007/s00521-019-04511-3		OCT 2019											
J								Unsupervised Learning Approach for Abnormal Event Detection in Surveillance Video by Hybrid Autoencoder	NEURAL PROCESSING LETTERS										Autoencoder; LSTM; Abnormality detection		Abnormal detection plays an important role in video surveillance. LSTM encoder-decoder is used to learn representation of video sequences and applied for detecting abnormal event in complex environment. The learned representation of LSTM encoder-decoder is learned from encoder, and it is crucial for decoder. However, LSTM encoder-decoder generally fails to account for the global context of the learned representation with a fixed dimension representation. In this paper, we explore a hybrid autoencoder architecture, which not only extracts better spatio-temporal context, but also improves the extrapolate capability of the corresponding decoder by the shortcut connection. The experiment shows that the hybrid model performs better than the state-of-the-art anomaly detection methods in both qualitative and quantitative ways on benchmark datasets.																	1370-4621	1573-773X															10.1007/s11063-019-10113-w		OCT 2019											
J								Computational Design of FastFES Treatment to Improve Propulsive Force Symmetry During Post-stroke Gait: A Feasibility Study	FRONTIERS IN NEUROROBOTICS										fast treadmill training; functional electrical stimulation; neuromusculoskeletal modeling; computational modeling; direct collocation optimal control; paretic propulsion; stroke; muscle synergies	FUNCTIONAL ELECTRICAL-STIMULATION; WALKING; OPTIMIZATION; PROGRESSION; SUPPORT; STROKE; REHABILITATION; MUSCLES; SPEED; COST	Stroke is a leading cause of long-term disability worldwide and often impairs walking ability. To improve recovery of walking function post-stroke, researchers have investigated the use of treatments such as fast functional electrical stimulation (FastFES). During FastFES treatments, individuals post-stroke walk on a treadmill at their fastest comfortable speed while electrical stimulation is delivered to two muscles of the paretic ankle, ideally to improve paretic leg propulsion and toe clearance. However, muscle selection and stimulation timing are currently standardized based on clinical intuition and a one-size-fits-all approach, which may explain in part why some patients respond to FastFES training while others do not. This study explores how personalized neuromusculoskeletal models could potentially be used to enable individual-specific selection of target muscles and stimulation timing to address unique functional limitations of individual patients post-stroke. Treadmill gait data, including EMG, surface marker positions, and ground reactions, were collected from an individual post-stroke who was a non-responder to FastFES treatment. The patient's gait data were used to personalize key aspects of a full-body neuromusculoskeletal walking model, including lower-body joint functional axes, lower-body muscle force generating properties, deformable foot-ground contact properties, and paretic and non-paretic leg neural control properties. The personalized model was utilized within a direct collocation optimal control framework to reproduce the patient's unstimulated treadmill gait data (verification problem) and to generate three stimulated walking predictions that sought to minimize inter-limb propulsive force asymmetry (prediction problems). The three predictions used: (1) Standard muscle selection (gastrocnemius and tibialis anterior) with standard stimulation timing, (2) Standard muscle selection with optimized stimulation timing, and (3) Optimized muscle selection (soleus and semimembranosus) with optimized stimulation timing. Relative to unstimulated walking, the optimal control problems predicted a 41% reduction in propulsive force asymmetry for scenario (1), a 45% reduction for scenario (2), and a 64% reduction for scenario (3), suggesting that non-standard muscle selection may be superior for this patient. Despite these predicted improvements, kinematic symmetry was not noticeably improved for any of the walking predictions. These results suggest that personalized neuromusculoskeletal models may be able to predict personalized FastFES training prescriptions that could improve propulsive force symmetry, though inclusion of kinematic requirements would be necessary to improve kinematic symmetry as well.																	1662-5218					OCT 1	2019	13								80	10.3389/fnbot.2019.00080													
J								Fuzzy Adaptive Leader-Following Consensus Control for Nonlinear Multi-Agent Systems with Unknown Control Directions	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Nonlinear multi-agent systems; Consensus control; Adaptive control; Fuzzy neural network; Unknown control directions	TRACKING CONTROL; OBSERVER; DYNAMICS	Two new fuzzy adaptive leader-following consensus control algorithms are proposed for nonlinear multi-agent systems with unknown identical and nonidentical control directions. In this paper, agents are modeled as first-order nonlinear systems with uncertain dynamics and unknown control directions. Two types of multi-agent systems are considered: i.e. all the agents have the same control direction and different agents have different control directions. The objective of this paper is to design distributed controllers for follower agents, such that all the follower agents can keep consensus with the leader even if only a fraction of followers can receive the information from the leader. The fuzzy neural network (FNN) is constructed to approximate the unknown dynamics by its powerful learning performance and exemption from prior knowledge of the systems. Finally, the numerical examples are provided to illustrate the effectiveness of the designed controller.																	1562-2479	2199-3211				OCT	2019	21	7					2066	2076		10.1007/s40815-019-00710-1													
J								Person Reidentification via Structural Deep Metric Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Computer vision; deep metric learning; deep neural network; machine learning; person re-identification (re-ID)	SIMILARITY	Despite the promising progress made in recent years, person reidentification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. This paper proposes to tackle this task by jointly learning feature representation and distance metric in an end-to-end manner. Existing deep metric learning-based re-ID methods usually encounter the following two weaknesses: 1) most works based on pairwise or triplet constraints often suffer from slow convergence and poor local optima, partially because they use very limited samples for each update and 2) hard negative sample mining has been widely applied in existing works. However, hard positive samples, which also contribute to the training of network, have not received enough attention. To alleviate these problems, we develop a novel structural metric learning objective for person re-ID, in which each positive pair is allowed to be compared against all negative pairs in a minibatch and each positive pair is adaptively assigned a hardness-aware weight to modulate its contribution. The introduced positive pair weighting strategy enables the algorithm to focus more on the hard positive samples. Furthermore, we propose to enhance the proposed loss function by adding a global loss term to reduce the variances of positive/negative pair distances, which is able to improve the generalization capability of the network model. By this approach, person images can be nonlinearly mapped into a low-dimensional embedding space where similar samples are kept closer and dissimilar samples are pushed farther apart. We implement the proposed algorithm using the inception architecture and evaluate it on three large-scale re-ID data sets. Experiment results demonstrate that our approach is able to outperform most state of the arts while using much lower dimensional deep features.																	2162-237X	2162-2388				OCT	2019	30	10					2987	2998		10.1109/TNNLS.2018.2861991													
J								A baseband processing ASIC for body area networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Body area networks; Baseband; ASIC; IEEE 802; 15; 6-2012		The development in the field of advanced biomedical sensors has resulted in a large volume of data being collected and transmitted wirelessly. The IEEE 802.15.6-2012 standard has specified a communication protocol for body area networks, however existing general-purpose communication protocols such as Bluetooth and Zigbee are more widely used due to multiple reasons. One of the critical issues is the lack of baseband processing hardware modules that implement the aforementioned standard. In this paper, the authors propose a baseband transceiver implementation in ASIC, which meets the 802.15.6-2012 standard requirements. Compared to other published designs, the proposed implementation exhibits better performance and low hardware cost, while also offering a complete standard implementation.																	1868-5137	1868-5145				OCT	2019	10	10			SI		3975	3982		10.1007/s12652-018-0870-8													
J								Finger Angle Estimation From Array EMG System Using Linear Regression Model With Independent Component Analysis	FRONTIERS IN NEUROROBOTICS										array EMG system; multichannel surface EMG; musculoskeletal models; surface ElectroMyoGraphy (EMG); adaptive mixture ICA (AMIGA); finger motion; convolutional pose machines	SIGNALS	Surface ElectroMyoGraphy (EMG) signals from the forearm used in prosthetic hand and finger control systems require precise anatomy data of finger muscles that are small and located deep within the forearm. The main problem of this method is that the signal quality depends on the placement of EMG sensor, which can significantly affects the accuracy and precision to estimate joint angles or forces. Moreover, in case of amputees, the location of finger muscles is unknown and needed to be identified manually for EMG recording. As a result, most modern prosthetic hands utilize limited number of muscles with pattern recognition to control finger according to pre-defined grip which is unable to mimic natural finger motion. To address such issue, we used array EMG sensors to obtain EMG signals from all possible positions on the forearm and applied regression method to produce natural finger motion. The signals were analyzed using independent component analysis (ICA) to find the best-fitted independent component (IC) that matches the anatomical data taken after the experiment. Next, from the IC and EMG signals, finger angles were estimated using linear regression model (LRM). Each finger was assigned EMG and IC component for flexion and extension muscles, to assess the possibility of controlling each finger angle separately. We compared the joint angles of each finger between calculated from IC and EMG by correlation coefficients (CC) for all fingers. The average CC values were higher than 0.7, demonstrating the strength of the linear relationship. The different between IC and EMG methods suggests that the IC method can reduce noise and increase the signal to noise ratio. The performance of ICA method showed higher CC value at around 0.2 +/- 0.10. In order to confirm the performance of ICA method, we also tested mathematical musculoskeletal model (MSM). The result from this study showed that not only array EMG sensors with ICA significantly improve the quality of signal detected from forearm but also reduce problems of conventional EMG sensors and consequently improve the performance of regression method to imitate natural finger motion.																	1662-5218					SEP 26	2019	13								75	10.3389/fnbot.2019.00075													
J								A Pathological Condition Affects Motor Modules in a Bipedal Locomotion Model	FRONTIERS IN NEUROROBOTICS										motor module; CPG; locomotion; neuromusculoskeletal model; pathological locomotion	CORTICAL CONTROL; STROKE PATIENTS; RECOVERY; WALKING; REHABILITATION; COORDINATION; ACTIVATION; MECHANISMS; SUBACUTE; PATTERNS	Bipedal locomotion is a basic motor activity that requires simultaneous control of multiple muscles. Physiological experiments suggest that the nervous system controls bipedal locomotion efficiently by using motor modules of synergistic muscle activations. If these modules were merged, abnormal locomotion patterns would be realized as observed in patients with neural impairments such as chronic stroke. However, sub-acute patients have been reported not to show such merged motor modules. Therefore, in this study, we examined what conditions in the nervous system merges motor modules. we built a two-dimensional bipedal locomotion model that included a musculoskeletal model with 7 segments and 18 muscles, a neural system with a hierarchical central pattern generator (CPG), and various feedback inputs from reflex organs. The CPG generated synergistic muscle activations comprising 5 motor modules to produce locomotion phases. Our model succeeded to acquire stable locomotion by using the motor modules and reflexes. Next, we examined how a pathological condition altered motor modules. Specifically, we weakened neural inputs to muscles on one leg to simulate a stroke condition. Immediately after the simulated stroke, the model did not walk. Then, internal parameters were modified to recover stable locomotion. We refitted either (a) reflex parameters or (b) CPG parameters to compensate the locomotion by adapting (a) reflexes or (b) the controller. Stable locomotion was recovered in both conditions. However the motor modules were merged only in (b). These results suggest that light or sub-acute stroke patients, who can compensate stable locomotion by just adapting reflexes, would not show merge of motor modules, whereas severe or chronic patients, who needed to adapt the controller for compensation, would show the merge, as consistent with experimental findings.																	1662-5218					SEP 20	2019	13								79	10.3389/fnbot.2019.00079													
J								A System-of-Systems Bio-Inspired Design Process: Conceptual Design and Physical Prototype of a Reconfigurable Robot Capable of Multi-Modal Locomotion	FRONTIERS IN NEUROROBOTICS										bio-inspired design; system-of-systems; multi-model locomotion; reconfigurable robots; mobile robotics	MECHANISM	Modern engineering problems require solutions with multiple functionalities in order to meet their practical needs to handle a variety of applications in different scenarios. Conventional design paradigms for single design purpose may not be able to satisfy this requirement efficiently. This paper proposes a novel system-of-systems bio-inspired design method framed in a solution-driven bio-inspired design paradigm. The whole design process consists of eight steps, that is, (1) biological solutions identification, (2) biological solutions definition/champion biological solutions, (3) principle extraction from each champion biological solution, (4) merging of extracted principles, (5) solution reframing, (6) problem search, (7) problem definition, and (8) principles application & implementation. The steps are elaborated and a case study of reconfigurable robots is presented following these eight steps. The design originates from the multimodal locomotion capabilities of two species (i.e., spiders and primates) and is analyzed based on the Pugh analysis. The resulting robotic platform could be potentially used for urban patrolling purposes.																	1662-5218					SEP 20	2019	13								78	10.3389/fnbot.2019.00078													
J								Generating Pointing Motions for a Humanoid Robot by Combining Motor Primitives	FRONTIERS IN NEUROROBOTICS										neurorobotics; motion generation; spiking neural networks (SNN); pointing a target; motor primitive; humanoid robot (HR); closed-loop	INTERNAL-MODELS; NETWORKS; DYNAMICS; CORTEX	The human motor system is robust, adaptive and very flexible. The underlying principles of human motion provide inspiration for robotics. Pointing at different targets is a common robotics task, where insights about human motion can be applied. Traditionally in robotics, when a motion is generated it has to be validated so that the robot configurations involved are appropriate. The human brain, in contrast, uses the motor cortex to generate new motions reusing and combining existing knowledge before executing the motion. We propose a method to generate and control pointing motions for a robot using a biological inspired architecture implemented with spiking neural networks. We outline a simplified model of the human motor cortex that generates motions using motor primitives. The network learns a base motor primitive for pointing at a target in the center, and four correction primitives to point at targets up, down, left and right from the base primitive, respectively. The primitives are combined to reach different targets. We evaluate the performance of the network with a humanoid robot pointing at different targets marked on a plane. The network was able to combine one, two or three motor primitives at the same time to control the robot in real-time to reach a specific target. We work on extending this work from pointing to a given target to performing a grasping or tool manipulation task. This has many applications for engineering and industry involving real robots.																	1662-5218					SEP 18	2019	13								77	10.3389/fnbot.2019.00077													
J								A new method of online extreme learning machine based on hybrid kernel function	NEURAL COMPUTING & APPLICATIONS										Online sequential extreme learning machine; Hybrid kernel function; Membership function; Sample selection; Classification	ALGORITHM	Computational complexity and sample selection are two main factors that limited the performance of online sequential extreme learning machines (OS-ELMs). This paper proposes a new model that introduces the concept of hybrid kernel and sample selection method based on an online learning model using a membership function. In other words, an online sequential extreme learning machine based on a hybrid kernel function (HKOS-ELM) is presented. The algorithm only calculates the kernel function to determine the final output function, mostly solving the computational complexity of the algorithm. The hybrid kernel function proposed in this paper has the advantages of strong learning ability and good generalization performance of single kernel function. Based on the classification essence of the OS-ELM classification, the membership function is introduced into the sample selection to remove the noise point and the outlier point. The experimental results showed that the HKOS-ELM algorithm adding the membership degree with mixed kernel functions preserves the advantages of kernel functions and online learning and improves the classification performance of the system.																	0941-0643	1433-3058				SEP	2019	31	9			SI		4629	4638		10.1007/s00521-018-3629-4													
J								Intrusion detection using mouse dynamics	IET BIOMETRICS										mouse controllers (computers); performance evaluation; biometrics (access control); pattern classification; data analysis; feature extraction; pattern recognition; security of data; general purpose data sets; unrestricted mouse usage data; Balabit data set; data science competition; adequate publicly available data; raw data; mouse actions; mouse move; mouse data; negative mouse dynamics data; impostor detection performance		Compared to other behavioural biometrics, mouse dynamics is a less explored area. General purpose data sets containing unrestricted mouse usage data are usually not available. The Balabit data set was released in 2016 for a data science competition which, despite the few users covered, can be considered the first adequate publicly available data set. This study presents a performance evaluation study on this data set for impostor detection. The existence of very short benchmark test sessions makes this data set challenging. Raw data were segmented into mouse actions, such as mouse move, point and click, and drag and drop, and then several features were extracted. Mouse data is not sensitive; therefore, it is possible to collect negative mouse dynamics data and to use two-class classifiers. Impostor detection performance was measured using decisions based on several actions. The best performance of 0.92 area under curve was obtained on the benchmark test sessions of the data set. Drag and drop mouse actions proved to be the best actions for impostor detection despite the fact that this type of action represented only 10% of the data set.																	2047-4938	2047-4946				SEP	2019	8	5					285	294		10.1049/iet-bmt.2018.5126													
J								ECG analysis for human recognition using non-fiducial methods	IET BIOMETRICS										biometrics (access control); discrete cosine transforms; principal component analysis; electrocardiography; Hadamard transforms; discrete Fourier transforms; ECG analysis; human recognition; nonfiducial methods; fraudulent attacks; ECG biometric; transformation techniques; discrete Fourier; DFT; dimensionality reduction techniques; principal component analysis; linear discriminant analysis; discrete cosine transform; Walsh-Hadamard transform	IDENTITY VERIFICATION; ELECTROCARDIOGRAM; IDENTIFICATION; BIOMETRICS	The electrocardiogram (ECG) has emerged as a new biometric for human recognition due to its robustness against fraudulent attacks. This article presents a novel method of ECG biometric for human recognition using autocorrelation (AC) followed by one of the three transformation techniques, i.e. discrete cosine transform (DCT), discrete Fourier transform (DFT), and Walsh-Hadamard transform (WHT). The effectiveness of these transformations is evaluated on the dimensionality reduction techniques i.e. principal component analysis and linear discriminant analysis (LDA). Thus, the systems prepared by different combinations of transformations and dimensionality reduction techniques are evaluated on publically available databases of Physionet. The authentication and identification accuracies achieved by these systems are found the best on DFT and LDA combination. The authentication performance is reported to 99.98% (99.83%), whereas the average rank classification accuracy is reported to 100% (97%) on QT database (MIT-BIH arrhythmia database) of Physionet.																	2047-4938	2047-4946				SEP	2019	8	5					295	305		10.1049/iet-bmt.2018.5093													
J								Finger vein verification using a Siamese CNN	IET BIOMETRICS										vein recognition; feature extraction; learning (artificial intelligence); convolutional neural nets; finger vein verification; Siamese CNN; image quality deterioration; finger rotation; finger vein images; heavy image augmentation strategy; pretrained-weights based convolutional neural network; lightweight CNN; pretrained-weights based CNN; handcrafted features; finger offsets; discriminative feature extraction; insufficient training data; Siamese structure; metric learning; modified contrastive loss function; embedded devices; depthwise separable convolution; knowledge distillation method; equal error rates; SDUMLA-HMT datasets; FV-USM datasets; MMCBNU_6000 datasets	FEATURE-EXTRACTION; RECOGNITION; DISTANCE; SCALE	Finger vein verification has received more attention recently due to its unique advantages. However, most existing algorithms rely on handcrafted features, making them less robust to finger rotation and offsets. To alleviate these problems, the authors propose a novel method to extract more discriminative features from finger vein images. First, facing the issue of insufficient training data, they adopt a heavy image augmentation strategy and develop a pretrained-weights based convolutional neural network (CNN). Second, focusing on the characteristics of finger vein verification, they construct a Siamese structure combining with a modified contrastive loss function for training the above CNN, which effectively improves the network's performance. Finally, considering the feasibility of deploying the above CNN on embedded devices, they construct a lightweight CNN with depthwise separable convolution and adopt a knowledge distillation method to learn the knowledge from the pretrained-weights based CNN, which makes it small but effective. The experimental results show that the size of the lightweight CNN shrinks to 1/6th of the pretrained-weights based CNN, while its equal error rates achieved in the MMCBNU_6000, FV-USM and SDUMLA-HMT datasets are 0.08, 0.11 and 0.75% respectively, which nearly stays the same with the pretrained-weights based CNN and surpasses state-of-the-art methods.																	2047-4938	2047-4946				SEP	2019	8	5					306	315		10.1049/iet-bmt.2018.5245													
J								Chaos game theory and its application for offline signature identification	IET BIOMETRICS										chaos; game theory; fractals; biometrics (access control); digital signatures; behavioural feature; human biometric features; fractal theory; signature instance; chaos game signature identification; Chaos game theory; offline signature identification; time complexity	VERIFICATION	Chaos game mechanism is a procedure for creating fractal phenomena from another fractal using a polygon and random walk. Here, the authors propose a novel identification approach and efficient application of Chaos Game theory towards signature identification. In fact, a signature is produced with hand scripting with a special rhythm that belongs to a specific person. Although a signature is a behavioural feature among the human biometric features, this behavioural feature expresses a chaotic property and can be analysed with chaotic systems and fractal theory. With authors' technique, by using a Chaos Game theory, a new fractal is created for each signature instance and, during the creation of the fractal, new features are extracted. These features express the fractal properties of the signature and are unique. In addition, by using fractal theory, this technique benefits from the advantages of fractal phenomena such as stability against rotation, losing some parts of the signature and scale that is desirable for biometrics applications. Authors' approach for offline signature analysis can segregate and identify many signature instances with a desirable time complexity. The authors name the technique that the authors present here the chaos game signature identification (CGSI).																	2047-4938	2047-4946				SEP	2019	8	5					316	324		10.1049/iet-bmt.2018.5188													
J								Improvement for distortion resistance of Delaunay triangulation net-based fingerprint templates	IET BIOMETRICS										image matching; mesh generation; fingerprint identification; improved Delaunay triangulation structure; distortion resistance; Delaunay triangulation net-based fingerprint template; fingerprint protection; Delaunay triangulation net-based template; elastic distortion; Delaunay tentacle; fingerprint verification competition database	IMAGE	Fingerprint template is considered as an effective method to solve the problem of fingerprint protection, and an indispensable key technology in the identification field. Among them, the Delaunay triangulation net-based template is a new type of solution. However, such a triangulation net is greatly affected by the elastic distortion of the fingerprint. In this study, a novel improvement, Delaunay tentacle, is proposed to this issue. Based on the orderly degradation of the Delaunay triangulation net, the Delaunay tentacle, kernel, and centre can be generated in turn. By calculating the deformation of the tentacles, this innovative, improved Delaunay triangulation structure can sense and resist distortion. The proposed algorithm is tested on the Fingerprint Verification Competition databases, and the model is compared with other current similar schemes. Experimental results demonstrate that it has promising performance.																	2047-4938	2047-4946				SEP	2019	8	5					325	331		10.1049/iet-bmt.2018.5263													
J								Deep part-related feature learning for human-level biometrics	IET BIOMETRICS										feature extraction; biometrics (access control); learning (artificial intelligence); image recognition; image representation; deep part-related feature learning; human-level biometrics systems; visual appearance; re-identification problems; local visual regions; separated visual regions; learning discriminative part-related features; novel network architecture; part relation network; appearance features; discriminative ability; local representation; two-branch deep network; PRN	REIDENTIFICATION	Many human-level biometrics systems rely on visual appearance to solve re-identification problems. Most of them focus on recognizing local visual regions individually, without exploiting the relations between these separated visual regions during learning. In this study, the authors target on learning discriminative part-related features and present novel network architecture, part relation network (PRN). It processes a set of part objects simultaneously in an end-to-end way to model relations of their appearance features, which strengthens the discriminative ability of local representation. Specially, the authors design PRN as a two-branch deep network, each branch of which performs uniform partition with different numbers of stripes to obtain multi-scale and cross-over part features for identification. Experimental results demonstrate that this system achieves state-of-the-art performance on three challenging data sets including Market-1501, CUHK03, and DukeMTMC-reID and demonstrate the effectiveness of modelling part relations for biometrics.																	2047-4938	2047-4946				SEP	2019	8	5					332	339		10.1049/iet-bmt.2018.5227													
J								Relevant features for gender classification in NIR periocular images	IET BIOMETRICS										pattern classification; image texture; iris recognition; support vector machines; feature extraction; face recognition; image classification; periocular region; gender information; gender accuracy; relevant features; classification results; periocular iris images; iris-occluded images; gender classification rates; UNAB-Gender; faster classification; NIR periocular images; gender classifications methods; (NIR) images; iris information; recent work; periocular iris region; periocular NIR images	TEXTURE CLASSIFICATION; MUTUAL INFORMATION; IRIS IMAGES; RECOGNITION; BIOMETRICS; FUSION; SCALE	Most gender classifications methods from near-infrared (NIR) images have used iris information. Recent work has explored the use of the whole periocular iris region which has surprisingly achieved better results. This suggests the most relevant information for gender classification is not located in the iris as expected. In this work, the authors analyse and demonstrate the location of the most relevant features that describe gender in periocular NIR images and evaluate their influence in classification. Experiments show that the periocular region contains more gender information than the iris region. They extracted several features (intensity, texture, and shape) and classified them according to their relevance using the XgBoost algorithm. Support vector machine and nine ensemble classifiers were used for testing gender accuracy when using the most relevant features. The best classification results were obtained when 4000 features located on the periocular region were used (89.22%). Additional experiments with the full periocular iris images versus the iris-occluded images were performed. The gender classification rates obtained were 84.35 and 85.75%, respectively. From results, they suggest focusing only on the surrounding area of the iris. This allows us to realise a faster classification of gender from NIR periocular images.																	2047-4938	2047-4946				SEP	2019	8	5					340	350		10.1049/iet-bmt.2018.5233													
J								Effective appearance model update strategy in object tracking	IET COMPUTER VISION											VISUAL TRACKING	Robust and accurate tracking for fast moving object in correlation filter framework is a challenging research problem. The appearance model update strategy impact on performance is usually very significant and hence is worth studying. Unfortunately, very few works focus on this component. This study proposes an adaptive appearance model update method, which utilises both average peak-to-correlation energy (APCE) threshold and gradient APCE threshold to measure tracking reliability. When tracking is unreliable, the update rate is modified and the initial template information is used to assist in correcting the model parameters. Different from conventional methods, both APCE threshold and gradient APCE threshold are enhanced by weighting the coefficients. More importantly, the gradient APCE threshold can capture the rapid decline of APCE which implies that the object is in fast motion. Sufficient evaluations on OTB datasets demonstrate that the proposed method can tackle challenging videos well, especially the authors obtain a gain of 4.6% in precision score on the selected 32 videos with fast motion attribute, and a mean gain of 3.0 and 2.2% in precision and success plots on OTB50, compared with the baseline tracker SAMF.																	1751-9632	1751-9640				SEP	2019	13	6					531	541		10.1049/iet-cvi.2018.5091													
J								Multi-cue combination network for action-based video classification	IET COMPUTER VISION												Action-based video classification (or video-based action recognition) is an active research area in computer vision. However, all currently utilised action-based video classification approaches take spatial and temporal components into consideration while acoustic features (e.g. sound and speech) are neglected. In this study, the authors propose a novel approach to combine multiple cues (i.e. both visual and acoustic information) for action-based video classification. Additionally, they introduce dense connections into their three-stream network to address the gradient vanishing problem. Experimental results in the Kinetics Human Action Video data set and the Kinetics-Sounds data set shows that their approach can effectively improve the accuracy in action-based video classification.																	1751-9632	1751-9640				SEP	2019	13	6					542	548		10.1049/iet-cvi.2018.5492													
J								Vision-based crater and rock detection using a cascade decision forest	IET COMPUTER VISION											AIDED INERTIAL NAVIGATION; SHAPE	Both crater and rock detection are components of the autonomous landing and hazard avoidance technology (ALHAT) sensor suite, as craters and rocks represent the majority of landing hazards. Furthermore, places with scientific values are very probable next to craters and rocks. Unsupervised approaches, which potentially use the pattern recognition techniques of ring threshold finding, perform quickly; however, they suffer from handling small craters. The supervised pattern recognition method is more powerful but is time-consuming. To address these issues, here, a simultaneous multi-size crater and rock detection algorithm is studied. The authors propose a new supervised machine-learning framework using a cascade decision forest. Sliding windows are utilised in order to search basic features, and a multi-grained cascade structure is introduced to enhance the framework's ability to learn the representations of the features. The training time of the proposed algorithm on a PC is comparable to that of deep neural networks, and the efficiency is enhanced for a large-scale database. The outputs of the simulation verify the effectiveness and validity of the introduced technique.																	1751-9632	1751-9640				SEP	2019	13	6					549	555		10.1049/iet-cvi.2018.5600													
J								Crowd counting using a self-attention multi-scale cascaded network	IET COMPUTER VISION												Recent developments of crowd analysis and behaviour prediction have attracted much attention. Crowd counting, as the essential and challenging task in crowd analysis, is riddled with many issues, such as large scale variations, serious occlusion, and so on. In this study, a self-attention-based multi-scale cascaded network called SAMC-Net to estimate density map for crowd counting, especially for high congested scene, is proposed. The proposed SAMC-Net consists of two components: a classification sub-network for density estimation and an end-to-end multi-scale convolution neural network for crowd counting. In order to reduce the negative effect of multi-scale issue on crowd counting task, the main network is designed as a multi-scale structure similar to U-Net. In order to enhance the crowd feature representation, this study proposes a self-attention-based crowd feature extraction way and uses it in the proposed SAMC-Net. Extensive experiments demonstrate the feasibility, effectiveness and robustness of the proposed SAMC-Net.																	1751-9632	1751-9640				SEP	2019	13	6					556	561		10.1049/iet-cvi.2019.0085													
J								Object detection using convolutional networks with adaptively adjusting receptive field of convolutional filter	IET COMPUTER VISION												The receptive field size of a convolutional filter in a deep convolutional network is a crucial issue for object detection task, as the output must response to a suitable size of area in the image to capture proper information. Receptive field size of convolutional filter is fixed due to the inherently fixed geometric structure in its building module. However, objects of interest vary significantly in size within the images for object detection. Different locations of images correspond to objects with different scales, and high level convolutional layers encode semantic features over spatial positions, thus adaptive determination of receptive field size of convolutional filter is desirable for object detection. The authors propose a new module to adaptively determine the receptive field size of convolutional filter, named adaptive convolution. It is based on the idea of dilating the convolutional filter with multiple dilation values and choosing the maximum activation as output, without adding any other parameters. The plain counterparts in existing convolutional neural networks can be easily replaced by adaptive convolution, giving rise to adaptive convolutional networks. Adequate experiments have proven the effectiveness of authors' method.																	1751-9632	1751-9640				SEP	2019	13	6					562	568		10.1049/iet-cvi.2018.5601													
J								Video summarisation by deep visual and categorical diversity	IET COMPUTER VISION											EGOCENTRIC VIDEOS	The authors propose a video-summarisation method based on visual and categorical diversities using pre-trained deep visual and categorical models. Their method extracts visual and categorical features from a pre-trained deep convolutional network (DCN) and a pre-trained word-embedding matrix. Using visual and categorical information they obtain a video diversity estimation, which is used as an importance score to select segments from the input video that best describes it. Their method also allows performing queries during the search process, in this way personalising the resulting video summaries according to the particular intended purposes. The performance of the method is evaluated using different pre-trained DCN models in order to select the architecture with the best throughput. They then compare it with other state-of-the-art proposals in video summarisation using a data-driven approach with the public dataset SumMe, which contains annotated videos with perfragment importance. The results show that their method outperforms other proposals in most of the examples. As an additional advantage, their method requires a simple and direct implementation that does not require a training stage.																	1751-9632	1751-9640				SEP	2019	13	6					569	577		10.1049/iet-cvi.2018.5436													
J								SOD-CED: salient object detection for noisy images using convolution encoder-decoder	IET COMPUTER VISION											VISUAL-ATTENTION; MODEL; EXTRACTION; FEATURES	During the last decade, there has been profound progress in the field of visual saliency. However, there still exist various major challenges that hinder the detection performance for scenes with complex composition, presence of additive noise, objects of diverse scale and rotations etc. Generally, images with additive noise have low spatial resolution and blurred edges, which affects the learning capability of the network and causes inaccurate detection. In order to address these issues, in this study, the authors propose a fully convolutional neural network which jointly denoise the input maps by learning edges and contrast details, followed by learning of residing salient details via colour spatial maps in an end-to-end fashion. Their framework employs convolutional layers that use gradient and contrast details of images to denoise the areas with high edge density. After denoising, the denoised images are subjected to salient object detection (SOD) using convolutional layers. The effectiveness of the proposed network is evaluated on benchmark datasets. The experimental results demonstrate the significant performance improvement of the proposed method over state-of-the-art detection techniques.																	1751-9632	1751-9640				SEP	2019	13	6					578	587		10.1049/iet-cvi.2018.5814													
J								Image unmosaicing without location information using stacked GAN	IET COMPUTER VISION												Image mosaicing is an image processing technique that is most commonly used to conceal identities of sensitive objects. The authors' research features recovering the mosaiced parts in an image, especially focusing on facial parts. While recent image completion methods based on deep learning have shown promising results on recovering damaged parts in an image, they have not addressed the problem of image unmosaicing. Moreover, all those methods necessitate the location information of damaged parts to tackle the recovery problem. They formulate unmosaicing as an image-to-image translation problem, and propose a two-stage method using generative adversarial network (GAN): stage-I GAN generates a coarse prediction followed by stage-II GAN which produces a final unmosaiced image with finer information. A combination of low-level 1, loss and high-level structural similarity loss is used to attain visually plausible and semantically consistent output. They have evaluated their method on the CelebA dataset and achieved better results than state-of-the-art image completion methods without explicitly exploiting the location information of mosaiced parts.																	1751-9632	1751-9640				SEP	2019	13	6					588	594		10.1049/iet-cvi.2018.5623													
J								Robust and adaptive ROI extraction for hyperspectral dorsal hand vein images	IET COMPUTER VISION											RECOGNITION; FEATURES; PATTERN	Hyperspectral dorsal hand vein image analysis for biometrics is a relatively new technology with great potential. Compared to traditional dorsal hand biometrics that use only one spectral band to capture and analyse the veins, hyperspectral imaging allows additional information to be included. Given the difficulties of processing hyperspectral dorsal hand images, including uneven illuminations, a noisy background, translation, and deformation, this study proposes a robust and adaptive region of interest (ROI) extraction algorithm. First, a novel knuckle pinky invariant point is located. Next, based on this invariant point a key line on the dorsal hand representing one side of the squared ROI is found. The remaining three sides of the ROI can then be formed. To evaluate the proposed method, both identification and verification experiments were conducted on a large hyperspectral dorsal hand vein database. The experimental results showed that the proposed method outperformed other dorsal hand ROI extraction algorithms for each hyperspectral band. Having performed well in both experiments, the groundwork has been laid to further analyse the extracted ROI in terms of feature extraction.																	1751-9632	1751-9640				SEP	2019	13	6					595	604		10.1049/iet-cvi.2018.5732													
J								Multi-sample inference network	IET COMPUTER VISION												This study explores whether neural networks can classify multiple samples simultaneously in a forward process. Therefore, a multi-input multi-prediction network architecture has been proposed. The authors call this method a multi-sample inference network (MSIN). In addition to maximising the use of network shared parameters, the network can also use multiple samples for training. MSIN allows multiple samples to be randomly combined to act as data augmentation, and the random combination of corresponding labels can regularise the network as a loss regularisation, which makes MSIN have better generalisation performance. In contrast, category expansion is a problem that is difficult to solve because neural networks can only predict a fixed number of categories. The network proposed in this study can solve the category expansion problem by expanding the initial layers and the final layers. It is trained by using samples of multiple domains at the same time to ensure that the network has no significant decline in the predictive performance of the existing categories. The MSIN method can also be applied to the generative adversarial network to enable it to simultaneously generate samples of multiple sample domains.																	1751-9632	1751-9640				SEP	2019	13	6					605	613		10.1049/iet-cvi.2018.5126													
J								Accelerating incremental attribute reduction algorithm by compacting a decision table	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Rough set; Incremental attribute reduction; Discerptiblity matrix; Compacted decision table	ROUGH-SET; UPDATING APPROXIMATIONS; KNOWLEDGE GRANULARITY; GRANULATION; SELECTION; ACQUISITION; ENTROPY	The evolution of object sets over time is ubiquitous in dynamic data. To acquire reducts for this type of data, researchers have proposed many incremental attribute reduction algorithms based on discernibility matrices. Although all reducts of an updated decision table can be obtained using these algorithms, their high computation time is a critical issue. To address this issue, we first construct three new types of discernibility matrices by compacting a decision table to eliminate redundant entries in the discernibility matrices of the original decision table. We then demonstrate that the set of reducts obtained from the compacted decision table are the same as those acquired from the original decision table. Extensive experiments have demonstrated that an incremental attribute reduction algorithm based on a compacted decision table can significantly accelerate attribute reduction for dynamic data with changing object sets while the acquired reducts are identical to those obtained using existing algorithms.																	1868-8071	1868-808X				SEP	2019	10	9					2355	2373		10.1007/s13042-018-0874-x													
J								Mining Top-k Useful Negative Sequential Patterns via Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Interestingness metric; top-k negative sequential patterns (NSPs); top-k positive sequential patterns (PSPs); useful patterns; weighted support	ALGORITHM	As an important tool for behavior informatics, negative sequential patterns (NSPs) (such as missing a medical treatment) are sometimes much more informative than positive sequential patterns (PSPs) (e.g., attending a medical treatment) in many applications. However, NSP mining is at an early stage and faces many challenging problems, including I) how to mine an expected number of NSPs; 2) how to select useful NSPs; and 3) how to reduce high time consumption. To solve the first problem, we propose an algorithm Topk-NSP to mine the k most frequent negative patterns. In Topk-NSP, we first mine the top-k PSPs using the existing methods, and then we use an idea which is similar to top-k PSPs mining to mine the top-k NSPs from these PSPs. To solve the remaining two problems, we propose three optimization strategies for Topk-NSP. The first optimization strategy is that, in order to consider the influence of PSPs when selecting useful top-k NSPs, we introduce two weights, w p and wN, to express the user preference degree for NSPs and PSPs, respectively, and select useful NSPs by a weighted support wsup. The second optimization strategy is to merge wsup and an interestingness metric to select more useful NSPs. The third optimization strategy is to introduce a pruning strategy to reduce the high computational costs of Topk-NSP. Finally, we propose an optimization algorithm Topk-NSP+. To the hest of our knowledge, Topk-NSP+ is the first algorithm that can mine the top-k useful NSPs. The experimental results on four synthetic and two real-life data sets show that the Topk-NSP+ is very efficient in mining the top-k NSPs in the sense of computational cost and scalability.																	2162-237X	2162-2388				SEP	2019	30	9					2764	2778		10.1109/TNNLS.2018.2886199													
J								Fusions and preference relations based on probabilistic interval-valued hesitant fuzzy information in group decision making	SOFT COMPUTING										Probability; Interval numbers; Information aggregation; Preference relation; Group decision making	LINGUISTIC DISTRIBUTION ASSESSMENTS; AGGREGATION	This paper proposes probabilistic interval-valued hesitant fuzzy set (P-IVHFS) by integrating the probability distribution into IVHFS. The concept can make some vital but seemingly repetitive information [such as the same memberships of interval forms given by different decision makers (DMs)], which is an important basis for decision-making rather than being discarded. Moreover, some operations and aggregation operators of probabilistic interval-valued hesitant fuzzy elements (P-IVHFEs) and probabilistic interval-valued hesitant fuzzy preference relation (P-IVHFPR) are put forward, based on which, a decision-making model is proposed. It can keep almost all information given by the DMs so that the results calculated with the model will be much more reliable than those calculated with approaches based on IVHFSs. Finally, a practical case concerning the evaluation of intelligent transportation systems is given to verify the advantages of the proposed decision-making approach based on P-IVHFPRs.																	1432-7643	1433-7479				SEP	2019	23	17					8291	8306		10.1007/s00500-018-3465-6													
J								Parallel refinement of slanted 3D reconstruction using dense stereo induced from symmetry	JOURNAL OF REAL-TIME IMAGE PROCESSING										Dense stereo estimation; 3D reconstruction; SymStereo; High-resolution images; Parallel processing; Multiple-GPU processing	REAL-TIME; ALGORITHM; OPENCL	Traditional dense stereo estimation algorithms measure photo-similarity to calculate the disparity between image pairs. SymStereo is a new framework of matching cost functions that measure symmetry to evaluate the possibility of two pixels being a match. This article proposes a fully functional real-time parallel 3D reconstruction pipeline that uses dense stereo-based photo-symmetry. The logN variant of SymStereo achieves superior results for images with slanted surfaces, when compared with other algorithms (Antunes and Barreto in Int J Comput Vis 1-22, 2014). This is of particular interest for areas of computer vision such as the processing of datasets for urban scene reconstruction and also for tracking in robotics or intelligent autonomous vehicles. The output results obtained are analyzed by tuning distinct matching cost, aggregation and refinement parameters, targeting the most suitable combinations for slant dominated images. Also, the parallel approach for the aforementioned pipeline consists of a hybrid dual GPU system capable of calculating from 2 up to 132 volumes per second for high- and low-resolution images, respectively.																	1861-8200	1861-8219				AUG	2019	16	4			SI		1037	1055		10.1007/s11554-016-0592-0													
J								Adaptive Fuzzy Control for Nontriangular Structural Stochastic Switched Nonlinear Systems With Full State Constraints	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptive fuzzy control; nontriangular structures; state constraints; stochastic switched nonlinear systems	BARRIER LYAPUNOV FUNCTIONS; DYNAMIC SURFACE CONTROL; NEURAL-NETWORK CONTROL; TRACKING CONTROL; STABILIZATION; DESIGN; STABILITY	The problem of adaptive fuzzy control is investigated for a class of nontriangular structural stochastic switched nonlinear systems with full state constraints in this paper. A remarkable feature of the nontriangular structural nonlinear system is the so called algebraic loop problem in the existing backstepping-based analysis and design. Properties of fuzzy basis functions are utilized to circumvent this algebraic loop problem. Based on the Barrier Lyapunov function, an adaptive fuzzy stochastic switched control scheme is designed. It is proven that all the signals in the closed-loop system are semiglobally uniformly ultimately bounded with full state constraints. The effectiveness of the proposed control scheme is verified via simulation studies.																	1063-6706	1941-0034				AUG	2019	27	8					1587	1601		10.1109/TFUZZ.2018.2883374													
J								Fuzzy-Based Multierror Constraint Control for Switched Nonlinear Systems and Its Applications	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptive control; continuous stirred tank reactor (CSTR); different coordinate transformations; multiple prescribed performance; switched systems	H-INFINITY CONTROL; BARRIER LYAPUNOV FUNCTIONS; OUTPUT-FEEDBACK CONTROL; PRESCRIBED PERFORMANCE; LINEAR-SYSTEMS; ADAPTIVE-CONTROL; TRACKING CONTROL; STABILIZATION; STABILITY; DESIGN	In this paper, a framework of adaptive control for a switched nonlinear system with multiple prescribed performance bounds is established using an improved dwell time technique. Since the prescribed performance bounds for subsystems are different from each other, the different coordinate transformations have to be tackled when the system is transformed, which have not been encountered in some switched systems. We deal with the different coordinate transformations by finding a specific relationship between any two different coordinate transformations. To obtain a much less conservative result, in contrast to the common adaptive law, different adaptive laws are established for both active and inactive time-interval of each subsystem. The proposed controllers and switching signals guarantee that all signals appearing in the closed-loop system are bounded. Furthermore, both transient-state and steady-state performances of the switched system are obtained. Finally, the effectiveness of the developed method is verified by the application to a continuous stirred tank reactor system.																	1063-6706	1941-0034				AUG	2019	27	8					1519	1531		10.1109/TFUZZ.2018.2882173													
J								p-Laplacian Regularization for Scene Recognition	IEEE TRANSACTIONS ON CYBERNETICS										Laplacian regularization (LapR); manifold learning; p-Laplacian; scene recognition; semi-supervised learning (SSL)		The explosive growth of multimedia data on the Internet makes it essential to develop innovative machine learning algorithms for practical applications especially where only a small number of labeled samples are available. Manifold regularized semi-supervised learning (MRSSL) thus received intensive attention recently because it successfully exploits the local structure of data distribution including both labeled and unlabeled samples to leverage the generalization ability of a learning model. Although there are many representative works in MRSSL, including Laplacian regularization (LapR) and Hessian regularization, how to explore and exploit the local geometry of data manifold is still a challenging problem. In this paper, we introduce a fully efficient approximation algorithm of graph p-Laplacian, which significantly saving the computing cost. And then we propose p-LapR (pLapR) to preserve the local geometry. Specifically, p-Laplacian is a natural generalization of the standard graph Laplacian and provides convincing theoretical evidence to better preserve the local structure. We apply pLapR to support vector machines and kernel least squares and conduct the implementations for scene recognition. Extensive experiments on the Scene 67 dataset, Scene 15 dataset, and UC-Merced dataset validate the effectiveness of pLapR in comparison to the conventional manifold regularization methods.																	2168-2267	2168-2275				AUG	2019	49	8					2927	2940		10.1109/TCYB.2018.2833843													
J								Pruning Elman neural network and its application in bolt defects classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Bolt defect classification; Elman neural network; Pruning algorithm; Contribution of neuron		The Elman model is developed based on BP neural network. Because Elman neural network has the advantages of low training complexity, high stability, quick convergence, and simple construction, it can be well applied in the area of classification. The application of Elman has a bottleneck problem, that is, network structure is difficult to determine. This paper proposed an improved Elman neural network model using pruning algorithm for the network structure optimization. Firstly, we choose the bolt defect parameters which are relatively easy to detect as the classification sample, select the initial large-scale network structure, analysis the contribution of hidden nodes in the network, and reduce the dynamic threshold of hidden nodes in combination with the network to obtain the optimal network structure. Simulation results show that pruning Elman neural network (P-Elman) can adaptively obtain the optimal network structure, and achieve a high classification accuracy, which is much higher than other methods.																	1868-8071	1868-808X				JUL	2019	10	7					1847	1862		10.1007/s13042-018-0871-0													
J								Monarch butterfly optimization	NEURAL COMPUTING & APPLICATIONS										Evolutionary computation; Monarch butterfly optimization; Migration; Butterfly adjusting operator; Benchmark problems	KRILL HERD ALGORITHM; BIOGEOGRAPHY BASED OPTIMIZATION; DIFFERENTIAL EVOLUTION; FIREFLY ALGORITHM; HARMONY SEARCH; COLONY; DESIGN	In nature, the eastern North American monarch population is known for its southward migration during the late summer/autumn from the northern USA and southern Canada to Mexico, covering thousands of miles. By simplifying and idealizing the migration of monarch butterflies, a new kind of nature-inspired metaheuristic algorithm, called monarch butterfly optimization (MBO), a first of its kind, is proposed in this paper. In MBO, all the monarch butterfly individuals are located in two distinct lands, viz. southern Canada and the northern USA (Land 1) and Mexico (Land 2). Accordingly, the positions of the monarch butterflies are updated in two ways. Firstly, the offsprings are generated (position updating) by migration operator, which can be adjusted by the migration ratio. It is followed by tuning the positions for other butterflies by means of butterfly adjusting operator. In order to keep the population unchanged and minimize fitness evaluations, the sum of the newly generated butterflies in these two ways remains equal to the original population. In order to demonstrate the superior performance of the MBO algorithm, a comparative study with five other metaheuristic algorithms through thirty-eight benchmark problems is carried out. The results clearly exhibit the capability of the MBO method toward finding the enhanced function values on most of the benchmark problems with respect to the other five algorithms. Note that the source codes of the proposed MBO algorithm are publicly available at GitHub (https://github.com/ggw0122/Monarch-Butterfly-Optimization, C++/MATLAB) and MATLAB Central (http://www.mathworks.com/matlabcentral/fileexchange/50828-monarch-butterfly-optimization, MATLAB).																	0941-0643	1433-3058				JUL	2019	31	7					1995	2014		10.1007/s00521-015-1923-y													
J								Effect of Depth and Width on Local Minima in Deep Learning	NEURAL COMPUTATION												In this paper, we analyze the effects of depth and width on the quality of local minima, without strong overparameterization and simplification assumptions in the literature. Without any simplification assumption, for deep nonlinear neural networks with the squared loss, we theoretically show that the quality of local minima tends to improve toward the global minimum value as depth and width increase. Furthermore, with a locally induced structure on deep nonlinear neural networks, the values of local minima of neural networks are theoretically proven to be no worse than the globally optimal values of corresponding classical machine learning models. We empirically support our theoretical observation with a synthetic data set, as well as MNIST, CIFAR-10, and SVHN data sets. When compared to previous studies with strong overparameterization assumptions, the results in this letter do not require overparameterization and instead show the gradual effects of overparameterization as consequences of general results.																	0899-7667	1530-888X				JUL	2019	31	7					1462	1498		10.1162/neco_a_01195													
J								Reconstructing gene regulatory networks with a memetic-neural hybrid based on fuzzy cognitive maps	NATURAL COMPUTING										Gene regulatory networks; Fuzzy cognitive maps; Memetic algorithms; Neural networks	EXPRESSION PROFILES; TIME-SERIES; INFERENCE; PREDICTION; EVOLUTION; MODELS	Reconstructing gene regulatory networks (GRNs) plays an important role in identifying the complicated regulatory relationships, uncovering regulatory patterns in cells, and gaining a systematic view for biological processes. In order to reconstruct large-scale GRNs accurately, in this paper, we first use fuzzy cognitive maps (FCMs), which are a kind of cognition fuzzy influence graphs based on fuzzy logic and neural networks, to model GRNs. Then, a novel hybrid method is proposed to reconstruct GRNs from time series expression profiles using memetic algorithm (MA) combined with neural network (NN), which is labeled as MANN(FCM)-GRN. In MANN(FCM)-GRN, the MA is used to determine regulatory connections in GRNs and the NN is used to determine the interaction strength of the regulatory connections. In the experiments, the performance of MANN(FCM)-GRN is validated on both synthetic data and the benchmark dataset DREAM3 and DREAM4. The experimental results demonstrate the efficacy of MANN(FCM)-GRN and show that MANN(FCM)-GRN can reconstruct GRNs with high accuracy without expert knowledge. The comparison with existing algorithms also shows that MANN(FCM)-GRN outperforms ant colony optimization, non-linear Hebbian learning, and real-coded genetic algorithms.																	1567-7818	1572-9796				JUN	2019	18	2	1		SI		301	312		10.1007/s11047-016-9547-4													
J								GIFT: Gesture-Based Interaction by Fingers Tracking, an Interaction Technique for Virtual Environment	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Gesture-based Interactions; Virtual Environments; Human Computer Interfaces; Image Processing	ACTIVITY RECOGNITION; FEATURES; REALITY	Three Dimensional (3D) interaction is the plausible human interaction inside a Virtual Environment (VE). The rise of the Virtual Reality (VR) applications in various domains demands for a feasible 3D interface. Ensuring immersivity in a virtual space, this paper presents an interaction technique where manipulation is performed by the perceptive gestures of the two dominant fingers; thumb and index. The two fingertip-thimbles made of paper are used to trace states and positions of the fingers by an ordinary camera. Based on the positions of the fingers, the basic interaction tasks; selection, scaling, rotation, translation and navigation are performed by intuitive gestures of the fingers. Without keeping a gestural database, the features-free detection of the fingers guarantees speedier interactions. Moreover, the system is user-independent and depends neither on the size nor on the color of the users' hand. With a case-study project; Interactions by the Gestures of Fingers (IGF) the technique is implemented for evaluation. The IGF application traces gestures of the fingers using the libraries of OpenCV at the back-end. At the front-end, the objects of the VE are rendered accordingly using the Open Graphics Library; OpenGL. The system is assessed in a moderate lighting condition by a group of 15 users. Furthermore, usability of the technique is investigated in games. Outcomes of the evaluations revealed that the approach is suitable for VR applications both in terms of cost and accuracy.																	1989-1660					JUN	2019	5	5					115	125		10.9781/ijimai.2019.01.002													
J								Campus Data Analysis Based on Positive and Negative Sequential Patterns	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Positive and negative sequential patterns; campus data analysis; data mining		Campus data analysis is becoming increasingly important in mining students' behavior. The consumption data of college students is an important part of the campus data, which can reflect the students' behavior to a great degree. A few methods have been used to analyze students' consumption data, such as classification, association rules, clustering, decision trees, time series, etc. However, they do not use the method of sequential patterns mining, which results in some important information missing. Moreover, they only consider the occurring (positive) events but do not consider the nonoccurring (negative) events, which may lead to some important information missing. So this paper uses a positive and negative sequential patterns mining algorithm, called NegI-NSP, to analyze the consumption data of students. Moreover, we associate students' consumption data with their academic grades by adding the students' academic grades into sequences to analyze the relationship between the students' academic grades and their consumptions. The experimental results show that the students' academic performance has significant correlation with the habits of having breakfast regularly.																	0218-0014	1793-6381				MAY	2019	33	5							1959016	10.1142/S021800141959016X													
J								Analysis and Pinning Control for Output Synchronization and H infinity Output Synchronization of Multiweighted Complex Networks	IEEE TRANSACTIONS ON CYBERNETICS										Complex networks; H infinity output synchronization; multiweights; output synchronization; pinning control	DYNAMICAL NETWORKS; ADAPTIVE-CONTROL; EXPONENTIAL SYNCHRONIZATION; NEURAL-NETWORKS; MULTI-WEIGHTS; SYSTEMS	The output synchronization and H infinity output synchronization problems for multiweighted complex network are discussed in this paper. First, we analyze the output synchronization of multiweighted complex network by exploiting Lyapunov functional and Barbalat's lemma. In addition, some nodes- and edges-based pinning control strategies are developed to ensure the output synchronization of multiweighted complex network. Similarly, the H infinity output synchronization problem of multiweighted complex network is also discussed. Finally, two numerical examples are presented to verify the correctness of the obtained results.																	2168-2267	2168-2275				APR	2019	49	4					1314	1326		10.1109/TCYB.2018.2799969													
J								Adaptive Neural State-Feedback Tracking Control of Stochastic Nonlinear Switched Systems: An Average Dwell-Time Method	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Adaptive tracking control; average dwell time (ADT); neural networks (NNs); nonstrict-feedback structure; stochastic nonlinear systems; switched nonlinear systems	H-INFINITY CONTROL; PRESCRIBED PERFORMANCE; GLOBAL STABILIZATION; STABILITY; DESIGN; FORM	In this paper, the problem of adaptive neural state-feedback tracking control is considered for a class of stochastic nonstrict-feedback nonlinear switched systems with completely unknown nonlinearities. In the design procedure, the universal approximation capability of radial basis function neural networks is used for identifying the unknown compounded nonlinear functions, and a variable separation technique is employed to overcome the design difficulty caused by the nonstrict-feedback structure. The most outstanding novelty of this paper is that individual Lyapunov function of each subsystem is constructed by flexibly adopting the upper and lower bounds of the control gain functions of each subsystem. Furthermore, by combining the average dwell-time scheme and the adaptive backstepping design, a valid adaptive neural state-feedback controller design algorithm is presented such that all the signals of the switched closed-loop system are in probability semiglobally uniformly ultimately bounded, and the tracking error eventually converges to a small neighborhood of the origin in probability. Finally, the availability of the developed control scheme is verified by two simulation examples.																	2162-237X	2162-2388				APR	2019	30	4					1076	1087		10.1109/TNNLS.2018.2860944													
J								Multi-Objective Based Spatio-Temporal Feature Representation Learning Robust to Expression Intensity Variations for Facial Expression Recognition	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Facial expression recognition (FER); expression intensity variation; spatio-temporal feature representation; deep learning; long short-term memory (LSTM)	LOCAL BINARY PATTERNS; FACE; MODEL	Facial expression recognition (FER) is increasingly gaining importance in various emerging affective computing applications. In practice, achieving accurate FER is challenging due to the large amount of inter-personal variations such as expression intensity variations. In this paper, we propose a new spatio-temporal feature representation learning for FER that is robust to expression intensity variations. The proposed method utilizes representative expression-states (e.g., onset, apex and offset of expressions) which can be specified in facial sequences regardless of the expression intensity. The characteristics of facial expressions are encoded in two parts in this paper. As the first part, spatial image characteristics of the representative expression-state frames are learned via a convolutional neural network. Five objective terms are proposed to improve the expression class separability of the spatial feature representation. In the second part, temporal characteristics of the spatial feature representation in the first part are learned with a long short-term memory of the facial expression. Comprehensive experiments have been conducted on a deliberate expression dataset (MMI) and a spontaneous micro-expression dataset (CASME II). Experimental results showed that the proposed method achieved higher recognition rates in both datasets compared to the state-of-the-art methods.																	1949-3045					APR-JUN	2019	10	2					223	236		10.1109/TAFFC.2017.2695999													
J								iCLAP: shape recognition by combining proprioception and touch sensing	AUTONOMOUS ROBOTS										Object recognition; Robot systems; Data fusion; Tactile sensing	TACTILE-OBJECT RECOGNITION; HAPTIC EXPLORATION; ROBOT HANDS; PERCEPTION; CONTACT; REGISTRATION	For humans, both the proprioception and touch sensing are highly utilized when performing haptic perception. However, most approaches in robotics use only either proprioceptive data or touch data in haptic object recognition. In this paper, we present a novel method named Iterative Closest Labeled Point (iCLAP) to link the kinesthetic cues and tactile patterns fundamentally and also introduce its extensions to recognize object shapes. In the training phase, the iCLAP first clusters the features of tactile readings into a codebook and assigns these features with distinct label numbers. A 4D point cloud of the object is then formed by taking the label numbers of the tactile features as an additional dimension to the 3D sensor positions; hence, the two sensing modalities are merged to achieve a synthesized perception of the touched object. Furthermore, we developed and validated hybrid fusion strategies, product based and weighted sum based, to combine decisions obtained from iCLAP and single sensing modalities. Extensive experimentation demonstrates a dramatic improvement of object recognition using the proposed methods and it shows great potential to enhance robot perception ability.																	0929-5593	1573-7527				APR	2019	43	4					993	1004		10.1007/s10514-018-9777-7													
B								Cognitive Code: Post-Anthropocentric Intelligence and the Infrastructural Brain	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN											VUL ET-AL.; DEFAULT MODE NETWORK; PUZZLINGLY HIGH CORRELATIONS; LARGE-SCALE BRAIN; FUNCTIONAL CONNECTIVITY; SEDUCTIVE ALLURE; RESTING-STATE; CULTURAL NEUROSCIENCE; SOCIAL NEUROSCIENCE; POTENTIAL CHANGES																				978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019																							
B								Traditional and Machine-Learning Methods for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning for efficacy analysis		This chapter reviews the general principles of traditional efficacy analyses of clinical trials in a nonmathematical fashion. First, t-tests and analyses of variance, both paired and unpaired, are explained as methods for testing the significance of difference between a new and control treatment. Instead of treatment modalities as causal outcome factors, many more causal factors of health and sickness can be tested in clinical trials, like psychological, social, and physical factors. With non-Gaussian frequency distributions, rank testing is adequate, and various methods are reviewed. Regression analyses for adjustment of baseline covariates, and the discretization of continuous predictors for better data precision are explained. With discrete and discretized predictors three dimensional bar charts and chi-square tests are appropriate. We live in an era of machine learning, and, also in this edition, traditional methods for efficacy analysis will be tested against machine learning methodologies. A summary of methodologies is given in this chapter.																			978-3-030-19918-0; 978-3-030-19917-3				2019							1	35		10.1007/978-3-030-19918-0_1	10.1007/978-3-030-19918-0												
B								Optimal-Scaling for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Optimal-scaling methods		In a 250 patient self-controlled study of drug efficacy scores, measured as differences from baseline, the effect of highly expressed gene polymorphisms on drug efficacy scores was tested, both traditionally and with the help of machine learning. Traditional efficacy analysis consisted of discretization of continuous predictors, unpaired t-tests, simple linear regressions, multiple linear regressions. Bonferroni's adjustment. Machine learning efficacy analysis consisted of optimal-scaling methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							37	53		10.1007/978-3-030-19918-0_2	10.1007/978-3-030-19918-0												
B								Ratio-Statistic for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Ratio-statistic methods		In a 50 patient parallel-group trial, the effects of treatment modalities on baseline minus treatment cholesterol level was tested, both traditionally and with the help of machine learning. Traditional efficacy analysis was consisted of confidence intervals, one-way analyses of variance, Kruskal-Wallis tests. Machine learning efficacy analysis consisted of ratio-statistic methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							55	61		10.1007/978-3-030-19918-0_3	10.1007/978-3-030-19918-0												
B								Complex-Samples for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Complex-samples methods		In a 1000 person random sample, the effects of time and territorial divisions on health scores were tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of confidence intervals, simple linear regressions. Machine learning efficacy analysis consisted of complex-samples methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							63	73		10.1007/978-3-030-19918-0_4	10.1007/978-3-030-19918-0												
B								Bayesian-Network for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Bayesian-networks methods		In a 872 men double-blind placebo-controlled randomized parallel-group trial, the effect of 2 year pravastatin on the decrease of LDL cholesterol was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of unpaired t-test, simple linear regressions, multiple linear regressions. Machine learning efficacy analysis consisted of Bayesian-networks methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							75	85		10.1007/978-3-030-19918-0_5	10.1007/978-3-030-19918-0												
B								Evolutionary-Operations for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Evolutionary-operation methods		In 16 operation settings, the effects of humidity, filter capacity, and air volume change on numbers of infections were tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis was composed of Poisson statistics, z-tests. Machine learning efficacy analysis was composed of evolutionary-operation methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							87	94		10.1007/978-3-030-19918-0_6	10.1007/978-3-030-19918-0												
B								Automatic-Newton-Modeling for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Automatic-Newton-modeling		In two pharmaco-kinetic-and-dynamic (PKD) studies the effects of drug dosages on clinical effectiveness, and of time on plasma concentrations were tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of linearized model of hyperbola function, regression model of exponential function. Machine learning efficacy analysis consisted of automatic-Newton modeling. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							95	105		10.1007/978-3-030-19918-0_7	10.1007/978-3-030-19918-0												
B								High-Risk-Bins for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; High-risk-bin methods		In 1445 families the effect of risk factors on overweight was tested, both traditionally and with the help of machine learning. Traditional efficacy analysis was composed of discretization of continuous predictors, three dimensional bars of effects versus outcome, crosstabs with chi-square statistics. Machine learning efficacy analysis was composed of high-risk-bin methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							107	118		10.1007/978-3-030-19918-0_8	10.1007/978-3-030-19918-0												
B								Balanced-Iterative-Reducing-Hierarchy for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Balanced-Iterative-Reducing-Hierarchy methods		In a random sample of 50 mentally depressed patients the effect of age on depression score was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis was composed of simple linear regressions, discretization of continuous predictors, multiple binary logistic regressions. Machine learning efficacy analysis was composed of balanced-iterative-reducing-hierarchy methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							119	135		10.1007/978-3-030-19918-0_9	10.1007/978-3-030-19918-0												
B								Cluster-Analysis for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Cluster-analysis methods		In a parallel-group study of 50 depressed patients with different ages, the effect of ages on the levels of depression was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of discretization of continuous predictors, simple linear regressions. Machine learning efficacy analysis consisted of cluster-analysis methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							137	146		10.1007/978-3-030-19918-0_10	10.1007/978-3-030-19918-0												
B								Multidimensional-Scaling for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Multidimensional-scaling methods		In 42 patients the effects of the random administrations of 15 different pain-killers on preference scoring was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of paired t-tests, confidence intervals, equivalence testing. Machine learning efficacy analysis consisted of multidimensional-scaling methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							147	171		10.1007/978-3-030-19918-0_11	10.1007/978-3-030-19918-0												
B								Binary Decision-Trees for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Binary decision-tree methods		In a 1004 patient random sample, the effects of age, cholesterol levels, smoking levels, education levels, and weight levels on infarct rating was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of discretization of continuous predictors, crosstabs with chi-square statistics. Machine learning efficacy analysis consisted of binary decision-tree methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							172	184		10.1007/978-3-030-19918-0_12	10.1007/978-3-030-19918-0												
B								Continuous Decision-Trees for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Continuous decision-tree methods		In a 953 patient random sample the effect of weight reduction, gender, sport, medical treatments, and diet on ldl cholesterol reduction was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis was composed of one-way analyses of variance (anova), multiple linear regressions. Machine learning efficacy analysis was composed of continuous decision-tree methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							185	194		10.1007/978-3-030-19918-0_13	10.1007/978-3-030-19918-0												
B								Automatic-Data-Mining for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Automatic-data-mining methods		In a parallel-group trial of 90 septic patients, the effects of three treatments on laboratory outcomes, and on risks of low blood pressure and those of death were tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of one-way analyses of variance, 3 x 2 crosstabs with 3 x 2 chi-square statistics, 3 dimensional bars of treatment modalities versus outcomes. Machine learning efficacy analysis consisted of automatic-data-mining methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							195	210		10.1007/978-3-030-19918-0_14	10.1007/978-3-030-19918-0												
B								Support-Vector-Machines for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Support-vector-machines methods		In a random 200 septic patient sample, the effect of laboratory values on the risk of death was tested, both traditionally, and with help of machine learning. Traditional efficacy analysis was composed of discretization of continuous predictors, crosstabs with chi-square statistics, multiple binary logistic regressions. Machine learning efficacy analysis was composed of support-vector-machine methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							211	221		10.1007/978-3-030-19918-0_15	10.1007/978-3-030-19918-0												
B								Neural-Networks for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Neural-network methods		In a 250 patient self-controlled study of drug efficacy scores, measured as differences from baseline, the effect of highly expressed gene polymorphisms on drug efficacy scores was tested, both traditionally and with the help of machine learning. Traditional efficacy analysis consisted of discretization of continuous predictors, 3 x 2 crosstabs with 3 x 2 chi-square statistics. Machine learning efficacy analysis was composed of neural-network methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							223	236		10.1007/978-3-030-19918-0_16	10.1007/978-3-030-19918-0												
B								Ensembled-Accuracies for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Ensembled-accuracy methods		In a 200 septic-patient random sample, the effect of laboratory values on the risk of death was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of discretization of continuous predictors, crosstabs and chi-square statistics, multiple binary logistic regressions. Machine learning efficacy analysis consisted of ensembled-accuracy methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							237	251		10.1007/978-3-030-19918-0_17	10.1007/978-3-030-19918-0												
B								Ensembled-Correlations for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Ensembled-correlation methods		In a 250 patient self-controlled study, the effects of highly expressed gene polymorphisms on drug efficacy was tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of simple linear regressions, multiple linear regressions, Bonferroni's adjustments. Machine learning efficacy analysis consisted of ensembled-correlation methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							253	267		10.1007/978-3-030-19918-0_18	10.1007/978-3-030-19918-0												
B								Gamma-Distributions for Efficacy Analysis	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Clinical trials; Traditional efficacy analysis; Machine learning efficacy analysis; Gamma-distribution methods		In a 110 patient random sample, the effects of age, psychological, and social scores on health scores were tested, both traditionally, and with the help of machine learning. Traditional efficacy analysis consisted of simple linear regressions, multiple linear regressions, Bonferroni's adjustments. Machine learning efficacy analysis consisted of gamma-distribution methods. The machine learning methods provided better sensitivity of testing, and were more informative.																			978-3-030-19918-0; 978-3-030-19917-3				2019							269	278		10.1007/978-3-030-19918-0_19	10.1007/978-3-030-19918-0												
B								Validating Big Data, a Big Issue	EFFICACY ANALYSIS IN CLINICAL TRIALS AN UPDATE: EFFICACY ANALYSIS IN AN ERA OF MACHINE LEARNING										Big data validation; Clinical trial validation; Scientific rules for clinical trials; Diagnostic test validation		Big data consist of multiple fractions of small data. If you wish your big data to be valid, then you will, first, have to make sure, that the fractions are validated: by the use of the scientific rules for clinical trials, and, in addition, by the use of traditional diagnostic test validations. Once this is all done well and good, only then you will be at the starting point of a serious big data analysis. Unfortunately, this is a pretty laborious scenario, and, although, currently, many data bases of big data do exist, most of them are, documentedly, of a poor quality and un-validated. Big data analyses tend to suffer from too many null-values, lack of experienced analysis teams, lacking validation tools, limited validation checklists. Big data tools are in expensive commercial software, and have not been judged by Academia. The best approach to big data analyses may be the use of large checklists, multiple analysis teams, and the use of multiple independent computers with simple programs rather than supercomputers with complex programs.																			978-3-030-19918-0; 978-3-030-19917-3				2019							279	298		10.1007/978-3-030-19918-0_20	10.1007/978-3-030-19918-0												
J								Balancing stochastic U-lines using particle swarm optimization	JOURNAL OF INTELLIGENT MANUFACTURING										Assembly line balancing; U-lines; Stochastic; Particle swarm optimization	ALGORITHM; TIMES	U-lines are important parts of the Just-In-Time production system in order to improve productivity and quality. In real life applications of assembly lines, the tasks may have varying execution times defined as a probability distribution. In this study, a novel particle swarm optimization algorithm is proposed to solve the U-line balancing problem with stochastic task times. A computational study is conducted to compare the performance of the proposed approach to the existing methods in the literature. The results of the computational study show that the proposed approach performs quite effectively. It also yields good solutions for all test problems within a short computational time.																	0956-5515	1572-8145				JAN	2019	30	1					97	111		10.1007/s10845-016-1234-x													
J								Image segmentation by minimum cross entropy using evolutionary methods	SOFT COMPUTING										Image processing; Segmentation; Evolutionary algorithms; Cross entropy; Electromagnetism optimization	ELECTROMAGNETISM-LIKE MECHANISM; PARTICLE SWARM OPTIMIZATION; ALGORITHM; SELECTION; SEARCH; COLONY	The segmentation of digital images is one of the most important steps in an image processing or computer vision system. It helps to classify the pixels in different regions according to their intensity level. Several segmentation techniques have been proposed, and some of them use complex operators. The techniques based on thresholding are the easiest to implement; the problem is to select correctly the best threshold that divides the pixels. An interesting method to choose the best thresholds is the minimum cross entropy (MCET), which provides excellent results for bi-level thresholding. Nevertheless, the extension of the segmentation problem into multiple thresholds increases significantly the computational effort required to find optimal threshold values. Each new threshold adds complexity to the formulation of the problem. Classic methods for image thresholding perform extensive searches, while new approaches take advantage of heuristics to reduce the search. Evolutionary algorithms use heuristics to optimize criteria over a finite number of iterations. The correct selection of an evolutionary algorithm to minimize the MCET directly impacts the performance of the method. Current approaches take a large number of iterations to converge and a high rate of MCET function evaluations. The electromagnetism-like optimization (EMO) algorithm is an evolutionary technique which emulates the attraction-repulsion mechanism among charges for evolving the individuals of a population. Such technique requires only a small number of evaluations to find the optimum. This paper proposes the use of EMO to search for optimal threshold values by minimizing the cross entropy function while reducing the amount of iterations and function evaluations. The approach is tested on a set of benchmark images to demonstrate that is able to improve the convergence and velocity; additionally, it is compared with similar state-of-the-art optimization approaches.																	1432-7643	1433-7479				JAN	2019	23	2					431	450		10.1007/s00500-017-2794-1													
B								Neuroscience(s)	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							14	27															
B								Informational Labour	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							28	45															
B								False Positives	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							46	64															
B								False Negatives	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							69	90															
B								Cloud Geographies	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							91	108															
B								Coding Cognition	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							109	122															
B								DEBRIEFING Infrastructural Reveries and Psychic Lives	COGNITIVE CODE: POST-ANTHROPOCENTRIC INTELLIGENCE AND THE INFRASTRUCTURAL BRAIN																															978-0-7735-5917-2; 978-0-7735-5969-1; 978-0-7735-5916-5				2019							123	137															
J								A hybrid ensemble and AHP approach for resilient supplier selection	JOURNAL OF INTELLIGENT MANUFACTURING											IMPERIALIST COMPETITIVE ALGORITHM; DECISION-SUPPORT-SYSTEM; INTEGRATED APPROACH; ORDER ALLOCATION; MODEL; PORTFOLIO; CHAINS; DEA	Suppliers play a crucial role in achieving the supply chain goals. In the context of risk management, suppliers are the most common source of external risks in modern supply chains. The recognition that continuity of supply chain flow under disruptive event is a critical issue has brought the attention of companies to the selection of resilient suppliers. In contrast to the extensive number of researches on traditional and green criteria of supplier selection, the criteria associated with resilient supplier selection are not well explored yet. This paper first seeks to explore the resilience criteria for supplier selection based on the notion of resilience capacities which can be divided into three categories: absorptive capacity, adaptive capacity, and restorative capacity. Absorptive capacity refers to the capability of system to withstand against disruptive event in prior or called as preparedness of supplier, while adaptive and restoration capacities imply the capability of supplier to adopt itself and restore from disruption or recoverability of supplier. We identified eight effective elements for resilience capacities which contribute to the resilience of suppliers. Advanced data mining approaches like predictive analytics models are used to predict the resilience value of each supplier. We applied ensemble methods by combining binomial logistics regression, classification and regression trees, and neural network to obtain better predictive performance than individual algorithm from the historical data to predict individual supplier's resiliency. This resilience value, obtained from ensemble methods, is coupled with additional four variables to assess the suppliers' overall performance and rank them using different supplier selection models. Finally, a case study has been performed on international plastic raw material suppliers for a U.S. based manufacturer.																	0956-5515	1572-8145				JAN	2019	30	1					207	228		10.1007/s10845-016-1241-y													
J								Fuzzy Optimal Energy Management for Fuel Cell and Supercapacitor Systems Using Neural Network Based Driving Pattern Recognition	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Driving pattern recognition (DPR); fuel cell (FC)/supercapacitor (SC) hybrid electric vehicle (HEV); fuzzy energy management; genetic algorithm (GA); neural network (NN) classifier	HYBRID ELECTRIC VEHICLE; POWER MANAGEMENT; MODELING APPROACH; STRATEGY; OPTIMIZATION; CONTROLLER; ALGORITHM; DESIGN	A novel adaptive energy management strategy is proposed for real-time power split between fuel cells (FCs) and supercapacitors (SCs) in a hybrid electric vehicle in view of the fact that driving patterns greatly affect fuel economy. The driving pattern recognition (DPR) is achieved based on the features extracted from the historical velocity window with a multilayer perceptron neural network. After the DPR has been obtained, an adaptive fuzzy energy management controller is utilized for power split according to the required power for vehicle running. In order to prolong the FC lifetime while decreasing the hydrogen consumption, a genetic algorithm is applied to optimize critical factors such as adaptive gains and fuzzy membership function parameters for several standard driving cycles. In the proposed method, the future driving cycles are not required and the current driving pattern can be successfully recognized, demonstrating that less current fluctuations and fuel consumption can be achieved under various driving conditions. Compared with conventional energy management systems, the proposed framework can ensure the state of charge of SCs within the desired limit.																	1063-6706	1941-0034				JAN	2019	27	1					45	57		10.1109/TFUZZ.2018.2856086													
J								Differential game theory for versatile physical human-robot interaction	NATURE MACHINE INTELLIGENCE											STROKE; COORDINATION; ADAPTATION; MUSCLES; ARM	Robots need to estimate and adapt to human behaviour, especially when human dynamics change over time. Now adaptive game theory controllers can help robots adapt to human behaviour in a reaching task. The last decades have seen a surge of robots working in contact with humans. However, until now these contact robots have made little use of the opportunities offered by physical interaction and lack a systematic methodology to produce versatile behaviours. Here, we develop an interactive robot controller able to understand the control strategy of the human user and react optimally to their movements. We demonstrate that combining an observer with a differential game theory controller can induce a stable interaction between the two partners, precisely identify each other's control law, and allow them to successfully perform the task with minimum effort. Simulations and experiments with human subjects demonstrate these properties and illustrate how this controller can induce different representative interaction strategies.																		2522-5839				JAN	2019	1	1					36	43		10.1038/s42256-018-0010-3													
J								Learnability can be undecidable	NATURE MACHINE INTELLIGENCE											UNIFORM-CONVERGENCE; INDEPENDENCE	The mathematical foundations of machine learning play a key role in the development of the field. They improve our understanding and provide tools for designing new learning paradigms. The advantages of mathematics, however, sometimes come with a cost. Godel and Cohen showed, in a nutshell, that not everything is provable. Here we show that machine learning shares this fate. We describe simple scenarios where learnability cannot be proved nor refuted using the standard axioms of mathematics. Our proof is based on the fact the continuum hypothesis cannot be proved nor refuted. We show that, in some cases, a solution to the 'estimating the maximum' problem is equivalent to the continuum hypothesis. The main idea is to prove an equivalence between learnability and compression. Not all mathematical questions can be resolved, according to Godel's famous incompleteness theorems. It turns out that machine learning can be vulnerable to undecidability too, as is illustrated with an example problem where learnability cannot be proved nor refuted.																		2522-5839				JAN	2019	1	1					44	48		10.1038/s42256-018-0002-3													
J								Long short-term memory networks in memristor crossbar arrays	NATURE MACHINE INTELLIGENCE											IDENTIFICATION	Deep neural networks are increasingly popular in data-intensive applications, but are power-hungry. New types of computer chips that are suited to the task of deep learning, such as memristor arrays where data handling and computing take place within the same unit, are required. A well-used deep learning model called long short-term memory, which can handle temporal sequential data analysis, is now implemented in a memristor crossbar array, promising an energy-efficient and low-footprint deep learning platform. Recent breakthroughs in recurrent deep neural networks with long short-term memory (LSTM) units have led to major advances in artificial intelligence. However, state-of-the-art LSTM models with significantly increased complexity and a large number of parameters have a bottleneck in computing power resulting from both limited memory capacity and limited data communication bandwidth. Here we demonstrate experimentally that the synaptic weights shared in different time steps in an LSTM can be implemented with a memristor crossbar array, which has a small circuit footprint, can store a large number of parameters and offers in-memory computing capability that contributes to circumventing the 'von Neumann bottleneck'. We illustrate the capability of our crossbar system as a core component in solving real-world problems in regression and classification, which shows that memristor LSTM is a promising low-power and low-latency hardware platform for edge inference.																		2522-5839				JAN	2019	1	1					49	57		10.1038/s42256-018-0001-4													
J								Causal deconvolution by algorithmic generative models	NATURE MACHINE INTELLIGENCE											COMPLEXITY	Most machine learning approaches extract statistical features from data, rather than the underlying causal mechanisms. A different approach analyses information in a general way by extracting recursive patterns from data using generative models under the paradigm of computability and algorithmic information theory. Complex behaviour emerges from interactions between objects produced by different generating mechanisms. Yet to decode their causal origin(s) from observations remains one of the most fundamental challenges in science. Here we introduce a universal, unsupervised and parameter-free model-oriented approach, based on the seminal concept and the first principles of algorithmic probability, to decompose an observation into its most likely algorithmic generative models. Our approach uses a perturbation-based causal calculus to infer model representations. We demonstrate its ability to deconvolve interacting mechanisms regardless of whether the resultant objects are bit strings, space-time evolution diagrams, images or networks. Although this is mostly a conceptual contribution and an algorithmic framework, we also provide numerical evidence evaluating the ability of our methods to extract models from data produced by discrete dynamical systems such as cellular automata and complex networks. We think that these separating techniques can contribute to tackling the challenge of causation, thus complementing statistically oriented approaches.																		2522-5839				JAN	2019	1	1					58	66		10.1038/s42256-018-0005-0													
S								Pancreas Segmentation in CT and MRI via Task-Specific Network Design and Recurrent Neural Contextual Learning	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											Automatic pancreas segmentation in radiology images, e.g., computed tomography (CT), and magnetic resonance imaging (MRI), is frequently required by computer-aided screening, diagnosis, and quantitative assessment. Yet, pancreas is a challenging abdominal organ to segment due to the high inter-patient anatomical variability in both shape and volume metrics. Recently, convolutional neural networks (CNN) have demonstrated promising performance on accurate segmentation of pancreas. However, the CNN-based method often suffers from segmentation discontinuity for reasons such as noisy image quality and blurry pancreatic boundary. In this chapter, we first discuss the CNN configurations and training objectives that lead to the state-of-the-art performance on pancreas segmentation. We then present a recurrent neural network (RNN) to address the problem of segmentation spatial inconsistency across adjacent image slices. The RNN takes outputs of the CNN and refines the segmentation by improving the shape smoothness.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							3	21		10.1007/978-3-030-13969-8_1	10.1007/978-3-030-13969-8												
S								Deep Learning for Muscle Pathology Image Analysis	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											Inflammatory myopathy (IM) is a kind of heterogeneous disease that relates to disorders of muscle functionalities. The identification of IM subtypes is critical to guide effective patient treatment since each subtype requires distinct therapy. Image analysis of hematoxylin and eosin (H&E)-stained whole-slide specimens of muscle biopsies are considered as a gold standard for effective IM diagnosis. Accurate segmentation of perimysium plays an important role in early diagnosis of many muscle inflammation diseases. However, it remains as a challenging task due to the complex appearance of the perimysium morphology and its ambiguity to the background area. The muscle perimysium also exhibits strong structure spanned in the entire tissue, which makes it difficult for current local patch-based methods to capture this long-range context information. In this book chapter, we propose a novel spatial clockwork recurrent neural network (spatial CW-RNN) to address those issues. Besides perimysium segmentation, we also introduce a fully automatic whole-slide image analysis framework for IM subtype classification using deep convolutional neural networks (DCNNs).																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							23	41		10.1007/978-3-030-13969-8_2	10.1007/978-3-030-13969-8												
S								2D-Based Coarse-to-Fine Approaches for Small Target Segmentation in Abdominal CT Scans	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										LIVER	Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of small organs (e.g., pancreas) or neoplasms (e.g., pancreatic cyst) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupy a large fraction of the input volume. In this chapter, we propose two coarse-to-fine mechanisms which use prediction from the first (coarse) stage to shrink the input region for the second (fine) stage. More specifically, the two stages in the first method are trained individually in a stepwise manner, so that the entire input region and the region cropped according to the bounding box are treated separately. While the second method inserts a saliency transformation module between the two stages so that the segmentation probability map from the previous iteration can be repeatedly converted as spatial weights to the current iteration. In training, it allows joint optimization over the deep networks. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments are performed on several CT datasets, including NIH pancreas, JHMI multi-organ, and JHMI pancreatic cyst dataset. Our proposed approach gives strong results in terms of DSC.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							43	67		10.1007/978-3-030-13969-8_3	10.1007/978-3-030-13969-8												
S								Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											Although deep neural networks have been a dominant method for many 2D vision tasks, it is still challenging to apply them to 3D tasks, such as medical image segmentation, due to the limited amount of annotated 3D data and limited computational resources. In this chapter, by rethinking the strategy to apply 3D Convolutional Neural Networks to segment medical images, we propose a novel 3D-based coarse-to-fine framework to efficiently tackle these challenges. The proposed 3D-based framework outperforms their 2D counterparts by a large margin since it can leverage the rich spatial information along all three axes. We further analyze the threat of adversarial attacks on the proposed framework and show how to defend against the attack. We conduct experiments on three datasets, the NIH pancreas dataset, the JHMI pancreas dataset and the JHMI pathological cyst dataset, where the first two and the last one contain healthy and pathological pancreases, respectively, and achieve the current state of the art in terms of Dice-Sorensen Coefficient (DSC) on all of them. Especially, on the NIH pancreas dataset, we outperform the previous best by an average of over 2%, and the worst case is improved by 7% to reach almost 70%, which indicates the reliability of our framework in clinical applications.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							69	91		10.1007/978-3-030-13969-8_4	10.1007/978-3-030-13969-8												
S								Unsupervised Domain Adaptation of ConvNets for Medical Image Segmentation via Adversarial Learning	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											Deep convolutional networks (ConvNets) have achieved the state-of-the-art performance and become the de facto standard for solving a wide variety of medical image analysis tasks. However, the learned models tend to present degraded performance when being applied to a new target domain, which is different from the source domain where the model is trained on. This chapter presents unsupervised domain adaptation methods using adversarial learning, to generalize the ConvNets for medical image segmentation tasks. Specifically, we present solutions from two different perspectives, i.e., feature-level adaptation and pixel-level adaptation. The first is to utilize feature alignment in latent space, and has been applied to cross-modality (MRI/CT) cardiac image segmentation. The second is to use image-to-image transformation in appearance space, and has been applied to cross-cohortX-ray images for lung segmentation. Experimental results have validated the effectiveness of these unsupervised domain adaptation methods with promising performance on the challenging task.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							93	115		10.1007/978-3-030-13969-8_5	10.1007/978-3-030-13969-8												
S								Glaucoma Detection Based on Deep Learning Network in Fundus Image	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										OPTIC DISC; CUP SEGMENTATION; VALIDATION	Glaucoma is a chronic eye disease that leads to irreversible vision loss. In this chapter, we introduce two state-of-the-art glaucoma detection methods based on deep learning technique. The first is the multi-label segmentation network, named M-Net, which solves the optic disc and optic cup segmentation jointly. M-Net contains a multi-scale U-shape convolutional network with the side-output layer to learn discriminative representations and produces segmentation probability map. Then the vertical cup to disc ratio (CDR) is calculated based on segmented optic disc and cup to assess the glaucoma risk. The second network is the disc-aware ensemble network, named DENet, which integrates the deep hierarchical context of the global fundus image and the local optic disc region. Four deep streams on different levels and modules are, respectively, considered as global image stream, segmentation-guided network, local disc region stream, and disc polar transformation stream. The DENet produces the glaucoma detection result from the image directly without segmentation. Finally, we compare two deep learning methods with other related methods on several glaucoma detection datasets.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							119	137		10.1007/978-3-030-13969-8_6	10.1007/978-3-030-13969-8												
S								Thoracic Disease Identification and Localization with Limited Supervision	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images. We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							139	161		10.1007/978-3-030-13969-8_7	10.1007/978-3-030-13969-8												
S								Deep Reinforcement Learning for Detecting Breast Lesions from DCE-MRI	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										CANCER STATISTICS; TUMOR SIZE; SURVIVAL; SYSTEM	We present a detection model that is capable of accelerating the inference time of lesion detection from breast dynamically contrast-enhanced magnetic resonance images (DCE-MRI) at state-of-the-art accuracy. In contrast to previous methods based on computationally expensive exhaustive search strategies, our method reduces the inference time with a search approach that gradually focuses on lesions by progressively transforming a bounding volume until the lesion is detected. Such detection model is trained with reinforcement learning and is modeled by a deep Q-network (DQN) that iteratively outputs the next transformation to the current bounding volume. We evaluate our proposed approach in a breast MRI data set containing the T1-weighted and the first DCE-MRI subtraction volume from 117 patients and a total of 142 lesions. Results show that our proposed reinforcement learning based detection model reaches a true positive rate (TPR) of 0.8 at around three false positive detections and a speedup of at least 1.78 times compared to baselines methods.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							163	178		10.1007/978-3-030-13969-8_8	10.1007/978-3-030-13969-8												
S								Automatic Vertebra Labeling in Large-Scale Medical Images Using Deep Image-to-Image Network with Message Passing and Sparsity Regularization	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										REGISTRATION; CT	Efficient and accurate vertebra labeling in medical images is important for longitudinal assessment, pathological diagnosis, and clinical treatment of the spinal diseases. In practice, the abnormal conditions in the images increase the difficulties to accurately identify the vertebrae locations. Such conditions include uncommon spinal curvature, bright imaging artifacts caused by metal implants, and limited field of the imaging view, etc. In this chapter, we propose an automatic vertebra localization and labeling method with high accuracy and efficiency for medical images. First, we introduce a deep image-to-image network (DI2IN) which generates the probability maps for vertebral centroids. The DI2IN adopts multiple prevailing techniques, including feature concatenation and deep supervision, to boost its performance. Second, a message-passing scheme is used to evolve the probability maps from DI2IN within multiple iterations, according to the spatial relationship of vertebrae. Finally, the locations of vertebra are refined and constrained with a learned sparse representation. We evaluate the proposed method on two categories of public databases, 3D CT volumes, and 2D X-ray scans, under various pathologies. The experimental results show that our method outperforms other state-of-the-art methods in terms of localization accuracy. In order to further boost the performance, we add 1000 extra 3D CT volumes with expert annotation when training the DI2IN for CT images. The results justify that large databases can improve the generalization capability and the performance of the deep neural networks. To the best of our knowledge, it is the first time that more than 1000 3D CT volumes are utilized for the anatomical landmark detection and the overall identification rate reaches 90% in spine labeling.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							179	197		10.1007/978-3-030-13969-8_9	10.1007/978-3-030-13969-8												
S								Anisotropic Hybrid Network for Cross-Dimension Transferable Feature Learning in 3D Medical Images	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											While deep convolutional neural networks (CNN) have been successfully applied for 2D image analysis, it is still challenging to apply them to 3D anisotropic volumes, especially when the within-slice resolution is much higher than the between-slice resolution and when the amount of 3D volumes is relatively small. On one hand, direct learning of CNN with 3D convolution kernels suffers from the lack of data and likely ends up with poor generalization; insufficient GPU memory limits the model size or representational power. On the other hand, applying 2D CNN with generalizable features to 2D slices ignores between-slice information. Coupling 2D network with LSTM to further handle the between-slice information is not optimal due to the difficulty in LSTM learning. To overcome the above challenges, 3D anisotropic hybrid network (AH-Net) transfers convolutional features learned from 2D images to 3D anisotropic volumes. Such a transfer inherits the desired strong generalization capability for within-slice information while naturally exploiting between-slice information for more effective modeling. We show the effectiveness of the 3D AH-Net on two example medical image analysis applications, namely, lesion detection from a digital breast tomosynthesis volume, and liver, and liver tumor segmentation from a computed tomography volume.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							199	216		10.1007/978-3-030-13969-8_10	10.1007/978-3-030-13969-8												
S								Deep Hashing and Its Application for Histopathology Image Analysis	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										RETRIEVAL; SYSTEM	Content-based image retrieval (CBIR) has attracted considerable attention for histopathology image analysis because it can provide more clinical evidence to support the diagnosis. Hashing is an important tool in CBIR due to the significant gain in both computation and storage. Because of the tremendous success of deep learning, deep hashing simultaneously learning powerful feature representations and binary codes has achieved promising performance on microscopic images. This chapter presents several popular deep hashing techniques and their applications on histopathology images. It starts introducing the automated histopathology image analysis and explaining the reasons why deep hashing is a significant and urgent need for data analysis in histopathology images. Then, it specifically discusses three popular deep hashing techniques and mainly introduces pairwise-based deep hashing. Finally, it presents their applications on histopathology image analysis.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							219	237		10.1007/978-3-030-13969-8_11	10.1007/978-3-030-13969-8												
S								Tumor Growth Prediction Using Convolutional Networks	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										GLIOMA GROWTH; REACTION-DIFFUSION; MR-IMAGES; MODEL	Prognostic tumor growth modeling via volumetric medical imaging observations is a challenging yet important problem in precision and predictive medicine. It can potentially imply and lead to better outcomes of tumor treatment management and surgical planning. Traditionally, this problem is tackled through mathematical modeling. Recent advances of convolutional neural networks (ConvNets) have demonstrated higher accuracy and efficiency than conventional mathematical models can be achieved in predicting tumor growth. This indicates that deep learning based data-driven techniques may have great potentials on addressing such problem. In this chapter, we first introduce a statistical group learning approach to predict the pattern of tumor growth that incorporates both the population trend and personalized data, where deep ConvNet is used to model the voxel-wise spatiotemporal tumor progression. We then present a two-stream ConvNets which directly model and learn the two fundamental processes of tumor growth, i.e., cell invasion and mass effect, and predict the subsequent involvement regions of a tumor. Experiments on a longitudinal pancreatic tumor data set show that both approaches substantially outperform a state-of-the-art mathematical model-based approach in both accuracy and efficiency.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							239	260		10.1007/978-3-030-13969-8_12	10.1007/978-3-030-13969-8												
S								Deep Spatial-Temporal Convolutional Neural Networks for Medical Image Restoration	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										PROJECTED CANCER-RISKS; UNITED-STATES; CT PERFUSION; RADIATION; EXPOSURE; QUALITY; NOISE	Computed tomography perfusion (CTP) facilitates low-cost diagnosis and treatment of acute stroke. Cine scanning allows users to visualize brain anatomy and blood flow in virtually live time. However, effective visualization exposes patients to radiocontrast pharmaceuticals and extended scan times. Higher radiation dosage exposes patients to potential risks including hair loss, cataract formation, and cancer. To alleviate these risks, radiation dosage can be reduced along with tube current and/or X-ray radiation exposure time. However, resulting images may lack sufficient information or be affected by noise and/or artifacts. In this chapter, we propose a deep spatial-temporal convolutional neural network to preserve CTP image quality at reduced tube current, low spatial resolution, and shorter exposure time. This network structure extracts multi-directional features from low-dose and low-resolution patches at different cross sections of the spatial-temporal data and reconstructs high-quality CT volumes. We assess the performance of the network concerning image restoration at different tube currents and multiple resolution scales. The results indicate the ability of our network in restoring high-quality scans from data captured at as low as 21% of the standard radiation dose. The proposed network achieves an average improvement of 7% in perfusion maps compared to the state-of-the-art method.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							261	275		10.1007/978-3-030-13969-8_13	10.1007/978-3-030-13969-8												
S								Generative Low-Dose CT Image Denoising	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										SINOGRAM NOISE-REDUCTION; COMPUTED-TOMOGRAPHY; RECONSTRUCTION; PROJECTION	The continuous development and extensive use of CT in medical practice have raised a public concern over the associated radiation dose to patients. Reducing the radiation dose may lead to increased noise and artifacts, which can adversely affect radiologists' judgment and confidence. Hence, advanced image reconstruction from low-dose CT data is needed to improve the diagnostic performance, which is a challenging problem due to its ill-posed nature. Over the past years, various low-dose CT methods have produced impressive results. However, most of the algorithms developed for this application, including the recently popularized deep learning techniques, aim for minimizing the mean squared error (MSE) between a denoised CT image and the ground truth under generic penalties. Although the peak signal-to-noise ratio (PSNR) is improved, MSE- or weighted-MSE-based methods can compromise the visibility of important structural details after aggressive denoising. This paper introduces a new CT image denoising method based on the generative adversarial network (GAN) with Wasserstein distance and perceptual similarity. The Wasserstein distance is a key concept of the optimal transport theory, and promises to improve the performance of GAN. The perceptual loss suppresses noise by comparing the perceptual features of a denoised output against those of the ground truth in an established feature space, while the GAN focuses more on migrating the data noise distribution from strong to weak statistically. Therefore, our proposed method transfers our knowledge of visual perception to the image denoising task and is capable of not only reducing the image noise level but also trying to keep the critical information at the same time. Promising results have been obtained in our experiments with clinical CT images.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							277	297		10.1007/978-3-030-13969-8_14	10.1007/978-3-030-13969-8												
S								Image Quality Assessment for Population Cardiac Magnetic Resonance Imaging	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										SPARSE REPRESENTATION; ACQUISITION; STATISTICS; NETWORKS	Cardiac magnetic resonance (CMR) images play a growing role in diagnostic imaging of cardiovascular diseases. MRI is arguably the most comprehensive imaging modality for noninvasive and nonionizing imaging of the heart and great vessels and, hence, most suited for population imaging cohorts. Ensuring full coverage of the left ventricle (LV) is a basic criterion of CMR image quality. Complete LV coverage, from base to apex, precedes accurate cardiac volume and functional assessment. Incomplete coverage of the LV is identified through visual inspection, which is time-consuming and usually done retrospectively in large imaging cohorts. In this chapter, we propose a novel automatic method to check the coverage of LV from CMR images by using Fisher discriminative and dataset invariance (FDDI) three-dimensional (3D) convolutional neural networks (CNN) independently of image-acquisition parameters, such as imaging device, magnetic field strength, variations in protocol execution, etc. The proposed model is trained on multiple cohorts of different provenance to learn the appearance and identify missing basal and apical slices. To address this, a two-stage framework is proposed. First, the FDDI 3D CNN extracts high-level features in the common representation from different CMR datasets using adversarial approach; then these image features are used to detect missing basal and apical slices. Compared with the traditional 3D CNN strategy, the proposed FDDI 3D CNN can minimize the within-class scatter and maximize the between-class scatter, which can be adapted to other CMR image data for LV coverage assessment.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							299	321		10.1007/978-3-030-13969-8_15	10.1007/978-3-030-13969-8												
S								Agent-Based Methods for Medical Image Registration	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										FUSION	Medical imaging registration is a critical step in a wide spectrum of medical applications from diagnosis to therapy and has been an extensively studied research field. Prior to the popularity of deep learning, image registration was commonly performed by optimizing an image matching metric as a cost function in search for the optimal registration. However, the optimization task is known to be challenging due to (1) the non-convex nature of the matching metric over the registration parameter space and (2) the lack of effective approaches for robust optimization. With the latest advance in deep learning and artificial intelligence, the field of medical image registration had a major paradigm shift, whereby learning-based image registration methods are developed to employ deep neural networks to analyze images in order to estimate plausible registrations. Among the latest advances in learning-based registration methods, agent-based methods have been shown to be effective in both 3-D/3-D and 2-D/3-D registrations with significant robustness advantage over conventional optimization-based methods. In this chapter, we give an overview of agent-based methods for medical image registration and its two applications on rigid-body 3-D/3-D and 2-D/3-D registrations.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							323	345		10.1007/978-3-030-13969-8_16	10.1007/978-3-030-13969-8												
S								Deep Learning for Functional Brain Connectivity: Are We There Yet?	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										ATTENTION-DEFICIT/HYPERACTIVITY DISORDER; DEFICIT HYPERACTIVITY DISORDER; FMRI; ADOLESCENTS; ADHD; INHIBITION; DIAGNOSIS; CHILDREN; STATES	The detection of behavioral disorders rooted in neurological structure and function is an important research goal for neuroimaging communities. Recently, deep learning has been used successfully in diagnosis and segmentation applications using anatomical magnetic resonance imaging (MRI). One of the reasons for its popularity is that with repeated nonlinear transformations, the algorithm is capable of learning complex patterns in the data. Another advantage is that the feature selection step commonly used with machine learning algorithms in neuroimaging applications is eliminated which could lead to less bias in the result. However, there has been little progress in the application of these black-box approaches to functional MRI (fMRI). In this study, we explore the use of deep learning methods in comparison with conventional machine learning classifiers as well as their ensembles to analyze fMRI scans. We compare the benefits of deep learning against an ensemble of classical machine learning classifiers with a suitable feature selection strategy. Specifically, we focus on a clinically important problem of Attention Deficit Hyperactivity Disorder (ADHD). Functional connectivity information is extracted from fMRI scans of ADHD and control patients (ADHD-200), and analysis is performed by applying a decision fusion of various classifiers-the support vector machine, support vector regression, elastic net, and random forest. We selectively include features by a nonparametric ranking method for feature selection. After initial classification is performed, the decisions are summed in various permutations for an ensemble classifier, and the final results are compared with the deep learning-based results. We achieved a maximum accuracy of 93.93% on the KKI dataset (a subset of the ADHD-200) and also identified significantly different connections in the brain between ADHD and control subjects. In the blind testing with different subsets of the target data (Peking-1), we achieved a maximum accuracy of 72.9%. In contrast, the deep learning-based approaches yielded a maximum accuracy of 70.5% on the Peking-1 dataset and 67.74% on the complete ADHD-200 dataset, significantly inferior to the classifier ensemble approach. With more data being made publicly available, deep learning in fMRI may show a strong potential but as of now deep learning does not provide a magical solution for fMRI-based diagnosis.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							347	365		10.1007/978-3-030-13969-8_17	10.1007/978-3-030-13969-8												
S								ChestX-ray: Hospital-Scale Chest X-ray Database and Benchmarks onWeakly Supervised Classification and Localization of Common Thorax Diseases	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' picture archiving and communication systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high-precision computer-aided diagnosis (CAD) systems. In this chapter, we present a chest X-ray database, namely, "ChestX-ray", which comprises 121,120 frontal-view X-ray images of 30,805 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially located via a unified weakly supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network-based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully automated high-precision CAD systems.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							369	392		10.1007/978-3-030-13969-8_18	10.1007/978-3-030-13969-8												
S								Automatic Classification and Reporting of Multiple Common Thorax Diseases Using Chest Radiographs	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition											Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports that accompany X-ray images in hospital picture and archiving communication systems can be utilized as a priori knowledge for tackling these two key problems. We propose a novel text-image embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together, with X-ray images being the only input. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							393	412		10.1007/978-3-030-13969-8_19	10.1007/978-3-030-13969-8												
S								Deep Lesion Graph in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-Scale Lesion Database	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										CONVOLUTIONAL NEURAL-NETWORKS; PULMONARY NODULES; RETRIEVAL; WORKFLOW; CANCER	Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals' picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. Alarge-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates, and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. An algorithm for intra-patient lesion matching is proposed and validated with experiments.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							413	435		10.1007/978-3-030-13969-8_20	10.1007/978-3-030-13969-8												
S								Simultaneous Super-Resolution and Cross-Modality Synthesis in Magnetic Resonance Imaging	DEEP LEARNING AND CONVOLUTIONAL NEURAL NETWORKS FOR MEDICAL IMAGING AND CLINICAL INFORMATICS	Advances in Computer Vision and Pattern Recognition										SPARSE REPRESENTATION; RECONSTRUCTION; INTERPOLATION; NORMALIZATION	Multi-modality magnetic resonance imaging (MRI) has enabled significant progress to both clinical diagnosis and medical research. Applications range from differential diagnosis to novel insights into disease mechanisms and phenotypes. However, there exist many practical scenarios where acquiring high-quality multi-modality MRI is restricted, for instance, owing to limited scanning time. This imposes constraints on multi-modality MRI processing tools, e.g., segmentation and registration. Such limitations are not only recurrent in prospective data acquisition but also when dealing with existing databases with either missing or low-quality imaging data. In this work, we explore the problem of synthesizing high-resolution images corresponding to one MRI modality from a low-resolution image of another MRI modality of the same subject. This is achieved by introducing the cross-modality dictionary learning scheme and a patch-based globally redundant model based on sparse representations. We use high-frequency multi-modality image features to train dictionary pairs, which are robust, compact, and correlated in this multimodal feature space. A feature clustering step is integrated into the reconstruction framework speeding up the search involved in the reconstruction process. Images are partitioned into a set of overlapping patches to maintain the consistency between neighboring pixels and increase speed further. Extensive experimental validations on two multi-modality databases of real brain MR images show that the proposed method outperforms state-of-the-art algorithms in two challenging tasks: image super-resolution and simultaneous SR and cross-modality synthesis. Our method was assessed on both healthy subjects and patients suffering from schizophrenia with excellent results.																	2191-6586		978-3-030-13969-8; 978-3-030-13968-1				2019							437	457		10.1007/978-3-030-13969-8_21	10.1007/978-3-030-13969-8												
J								PROBABILISTIC PAGE REPLACEMENT POLICY IN BUFFER CACHE MANAGEMENT FOR FLASH-BASED CLOUD DATABASES	COMPUTING AND INFORMATICS										Cloud databases; data mining; flash solid state drives; adaptive optimization; genetic algorithm	ALGORITHM; LRU; INTEGRATION; WSR	In the fast evolution of storage systems, the newly emerged flash memory-based Solid State Drives (SSDs) are becoming an important part of the computer storage hierarchy. Amongst the several advantages of flash-based SSDs, high read performance, and low power consumption are of primary importance. Amongst its few disadvantages, its asymmetric I/O latencies for read, write and erase operations are the most crucial for overall performance. In this paper, we proposed two novel probabilistic adaptive algorithms that compute the future probability of reference based on recency, frequency, and periodicity of past page references. The page replacement is performed by considering the probability of reference of cached pages as well as asymmetric read-write-erase properties of flash devices. The experimental results show that our proposed method is successful in minimizing the performance overheads of flash-based systems as well as in maintaining the good hit ratio. The results also justify the utility of a genetic algorithm in maximizing the overall performance gains.																	1335-9150						2019	38	6					1237	1271		10.31577/cai_2019_6_1237													
J								CELLULAR AUTOMATA BASED IMAGE AUTHENTICATION SCHEME USING EXTENDED VISUAL CRYPTOGRAPHY	COMPUTING AND INFORMATICS										Image authentication; cellular automata; extended visual cryptography; watermarking; normalized Hamming similarity; PSNR; SSIM	SEMI-FRAGILE WATERMARKING; SHARING SCHEME; STEGANOGRAPHY; ENCRYPTION; ALGORITHM	Most of the Visual Cryptography based image authentication schemes hide the share and authentication data into cover images by using an additional data hiding process. This process increases the computational cost of the schemes. Pixel expansion, meaningless shares and use of codebook are other challenges in these schemes. To overcome these issues, an authentication scheme is proposed in which no embedding into the cover images is performed and meaningful authentication shares are created using the watermark and cover images. This makes the scheme completely imperceptible. The watermark can be retrieved just by superimposing these authentication shares, thus reducing the computational complexity at receiver's side. Cellular Automata is used to construct the master share that provides self-construction ability to the shares. The meaningful authentication shares help in enhancing the security of the scheme while size invariance saves transmission and storage cost. The scheme possesses the ability of tamper detection and its localization. Experimental results demonstrate the improved security and quality of the generated shares of the proposed scheme as compared to existing schemes.																	1335-9150						2019	38	6					1272	1300		10.31577/cai_2019_6_1272													
J								IMAGE SUPER-RESOLUTION BASED ON SPARSE CODING WITH MULTI-CLASS DICTIONARIES	COMPUTING AND INFORMATICS										Image patch classification; multi-class dictionaries; phase congruency; sparse coding; super-resolution		Sparse coding-based single image super-resolution has attracted much interest. In this paper, a super-resolution reconstruction algorithm based on sparse coding with multi-class dictionaries is put forward. We propose a novel method for image patch classification, using the phase congruency information. A sub-dictionary is learned from patches in each category. For a given image patch, the sub-dictionary that belongs to the same category is selected adaptively. Since the given patch has similar pattern with the selected sub-dictionary, it can be better represented. Finally, iterative back-projection is used to enforce global reconstruction constraint. Experiments demonstrate that our approach can produce comparable or even better super-resolution reconstruction results with some existing algorithms, in both subjective visual quality and numerical measures.																	1335-9150						2019	38	6					1301	1319		10.31577/cai_2019_6_1301													
J								PARALLEL PEER GROUP FILTER FOR IMPULSE DENOISING IN DIGITAL IMAGES ON GPU	COMPUTING AND INFORMATICS										Impulse denoising; color images; CUDA; parallel architecture; fuzzy metrics	NOISE REMOVAL	A new two-steps impulsive noise parallel Peer Group filter for color images using Compute Unified Device Architecture (CUDA) on a graphic card is proposed. It consists of two steps: impulsive noise detection, which uses a Fuzzy Metric as a distance criterion and a filtering step. For the needed ordering algorithm we are using the Marginal Median Filter with forgetful selection sort. Comparisons with other color filters for Graphics Processing Unit (GPU) architectures are presented, demonstrating that our proposal presents better performance in color preservation and noise suppression.																	1335-9150						2019	38	6					1320	1340		10.31577/cai_2019_6_1320													
J								AUTOMATIC QUERY REFINING BASED ON EYE-TRACKING FEEDBACK	COMPUTING AND INFORMATICS										Web search; query refinement; eye-tracking; groupization; implicit feedback	RELEVANCE FEEDBACK	This paper presents a new method named AQueReBET, which automatically refines a query set by an information seeker searching on the web. A revelation of the intention of an information seeker who is running a search can bring a significant improvement to the search process and to browsing as well. It is practically impossible to acquire such intention by the explicit indication (feedback) due to the fact that web browsing takes place in real time. Therefore the intention must be determined in some other way. We hypothesize that it can be approximated by means of the implicit feedback preferably in the form of data from an eye tracker and mouse. We propose a method which automatically refines a seeker's search query, and thus we can offer documents with higher relevance, decrease the number of query reformulations and increase the seeker's satisfaction. The query refinement is based on an analysis of gaze data from an eye tracker and on groupization. In the proposed method, we calculate word-level importance based on term frequency, term uniqueness (tf-idf) and total fixation duration within the subdocument (word's snippet in search results).																	1335-9150						2019	38	6					1341	1374		10.31577/cai_2019_6_1341													
J								FRAMEWORK FOR KNOWLEDGE DISCOVERY IN EDUCATIONAL VIDEO REPOSITORIES	COMPUTING AND INFORMATICS										Semantic annotation; knowledge discovery; video repositories	SPEECH; IMAGE	The ease of creating digital content coupled with technological advancements allows institutions and organizations to further embrace distance learning. Teaching materials also receive attention, because it is difficult for the student to obtain adequate didactic material, being necessary a high effort and knowledge about the material and the repository. This work presents a framework that enables the automatic metadata generation for materials available in educational video repositories. Each module of the framework works autonomously and can be used in isolation, complemented by another technique or replaced by a more appropriate approach to the field of use, such as repositories with other types of media or other content.																	1335-9150						2019	38	6					1375	1402		10.31577/cai_2019_6_1375													
J								FUZZY KNOWLEDGE INFERENCE: QUICKLY ESTIMATE EVIDENCE VIA FORMULA EMBEDDING	COMPUTING AND INFORMATICS										Fuzzy logic; fast inference; knowledge base completion; formula mining		Inference on Knowledge Bases (KBs) is an important way to construct more complete KBs and answer KB questions. Inference can be viewed as a process from evidence to conclusion following specific formulas. Traditional methods usually search on the KB to collect evidence, which cannot apply to large-scale KBs, because the running time of searching increases radically as the scale of KBs increases. What is worse, evidence cannot be found if one fact in it is missing, which may result in the failure of inference. To this end, we propose a fuzzy method of estimating evidence, which replaces searching by estimating the existence of evidence by constructing formula embeddings, and then we merge these estimations into a probabilistic model to infer conclusions. This method can apply to large-scale KBs, because estimating evidence is very fast and is irrelevant to the KB scale. Estimating evidence can also be viewed as fuzzy matching, so this method can handle the situation where facts are missing. We evaluate this method on the knowledge base completion task, and it achieves a better performance than state-of-the-art methods and has a shorter running time.																	1335-9150						2019	38	6					1403	1417		10.31577/cai_2019_6_1403													
J								GENERALIZED SELECTION METHOD	COMPUTING AND INFORMATICS										Selection method; evolutionary algorithm (EA); genetic algorithm (GA); continuous scalability of selection pressure; continuous scalability of randomness	GENETIC ALGORITHMS	In this paper we introduce new selection method, 3-selection method. This method tries to generalize the most used selection methods in Genetic Algorithms (GA). Our new method involves both proportional and rank-based methods (order-based) and, moreover, it allows scaling of selection pressure with higher precision. This method is based on defining the shape of probability density distribution which is adjustable by parameters of our method. In addition, our method has one more attribute which adds randomness of selection.																	1335-9150						2019	38	6					1418	1443		10.31577/cai_2019_6_1418													
J								CHAOTIC ELECTION ALGORITHM	COMPUTING AND INFORMATICS										Optimization; meta-heuristic; election algorithm; Chaotic Election Algorithm (CEA)	IMPERIALIST COMPETITIVE ALGORITHM; DIFFERENTIAL EVOLUTION; SWARM OPTIMIZATION; KRILL HERD; FIREFLY ALGORITHM; METAHEURISTICS	A novel Chaotic Election Algorithm (CEA) is presented for numerical function optimization. CEA is a powerful enhancement of election algorithm. The election algorithm is a socio-politically inspired strategy that mimics the behavior of candidates and voters in presidential election process. In election algorithm, individuals are organized as electoral parties. Advertising campaign forms the basis of the algorithm in which individuals interact or compete with one other using three operators: positive advertisement, negative advertisement and coalition. Advertising campaign hopefully causes the individuals converge to the global optimum point in solution space. However, election algorithm suffers from a fundamental challenge: it gets stuck at local optima due to the inability of advertising campaign in searching solution space. CEA enhances the election algorithm through modifying party formation step, introducing chaotic positive advertisement and migration operator. By chaotic positive advertisement, CEA exploits the entire solution space, what increases the probability of obtaining global optimum point. By migration, CEA increases the diversity of the population and prevents early convergence of the individuals. The proposed CEA algorithm is tested on 28 well-known standard boundary-constrained test functions, and the results are verified by a comparative study with several well-known meta-heuristics. The results demonstrate that CEA is able to provide significant improvement over canonical election algorithm and other comparable algorithms.																	1335-9150						2019	38	6					1444	1478		10.31577/cai_2019_6_1444													
B								Clinical motivation and the needs for RIA in healthcare	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										DIABETIC MACULAR EDEMA; RETINAL VESSEL CALIBER; MICROVASCULAR CALIBER; VASCULAR CALIBER; RETINOPATHY; RISK; ABNORMALITIES; HYPERTENSION; METAANALYSIS; PREVALENCE																				978-0-08-102817-9; 978-0-08-102816-2				2019							5	17		10.1016/B978-0-08-102816-2.00002-2	10.1016/C2018-0-00865-8												
B								The physics, instruments and modalities of retinal imaging	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										OPTICAL COHERENCE TOMOGRAPHY; SPECTRAL REFLECTANCE; ULTRAHIGH-RESOLUTION; SWEPT SOURCE; HUMAN-EYE; OXIMETRY; FIELD; OCT; MICROVASCULATURE; PRINCIPLES																				978-0-08-102817-9; 978-0-08-102816-2				2019							19	57		10.1016/B978-0-08-102816-2.00003-4	10.1016/C2018-0-00865-8												
B								Retinal image preprocessing, enhancement, and registration	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										OPTICAL COHERENCE TOMOGRAPHY; SPECKLE NOISE-REDUCTION; VESSEL SEGMENTATION; BLOOD-VESSELS; MULTIMODAL REGISTRATION; ILLUMINATION CORRECTION; CONTRAST NORMALIZATION; NONLINEAR DIFFUSION; AUTOMATIC DETECTION; FUNDUS IMAGES																				978-0-08-102817-9; 978-0-08-102816-2				2019							59	77		10.1016/B978-0-08-102816-2.00004-6	10.1016/C2018-0-00865-8												
B								Automatic landmark detection in fundus photography	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										COLOR RETINAL IMAGES; OPTIC DISC; SEGMENTATION; VESSELS; NERVE; LOCALIZATION; FOVEA																				978-0-08-102817-9; 978-0-08-102816-2				2019							79	93		10.1016/B978-0-08-102816-2.00005-8	10.1016/C2018-0-00865-8												
B								Retinal vascular analysis: Segmentation, tracing, and beyond	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										BLOOD-VESSEL SEGMENTATION; OPTICAL COHERENCE TOMOGRAPHY; CURVILINEAR STRUCTURES; DIABETIC-RETINOPATHY; FUNDUS IMAGES; MODEL; CLASSIFICATION; ALGORITHMS; EXTRACTION; IDENTIFICATION																				978-0-08-102817-9; 978-0-08-102816-2				2019							95	120		10.1016/B978-0-08-102816-2.00006-X	10.1016/C2018-0-00865-8												
B								OCT layer segmentation	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										OPTICAL COHERENCE TOMOGRAPHY; AUTOMATIC SEGMENTATION; RETINAL LAYER; IMAGES; BOUNDARIES																				978-0-08-102817-9; 978-0-08-102816-2				2019							121	133		10.1016/B978-0-08-102816-2.00007-1	10.1016/C2018-0-00865-8												
B								Image quality assessment	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										CARDIOVASCULAR RISK-FACTORS; DIABETIC-RETINOPATHY; FUNDUS PHOTOGRAPHS; COST-EFFECTIVENESS; RETINAL IMAGES; POPULATION; INFANTS; SYSTEM; MODEL																				978-0-08-102817-9; 978-0-08-102816-2				2019							135	155		10.1016/B978-0-08-102816-2.00008-3	10.1016/C2018-0-00865-8												
B								Validation	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										IMAGE-ANALYSIS; SEGMENTATION; AGREEMENT; MODEL																				978-0-08-102817-9; 978-0-08-102816-2				2019							157	170		10.1016/B978-0-08-102816-2.00009-5	10.1016/C2018-0-00865-8												
B								Statistical analysis and design in ophthalmology: Toward optimizing your data	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										RANDOMIZED CONTROLLED-TRIALS; OCULAR PERFUSION-PRESSURE; MISSING DATA; EPIDEMIOLOGY; STATEMENT																				978-0-08-102817-9; 978-0-08-102816-2				2019							171	197		10.1016/B978-0-08-102816-2.00010-1	10.1016/C2018-0-00865-8												
B								Structure-preserving guided retinal image filtering for optic disc analysis	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										DIABETIC-RETINOPATHY; MACULAR DEGENERATION; AUTOMATIC DETECTION; FEATURE-EXTRACTION; VISUAL IMPAIRMENT; CUP SEGMENTATION; FUNDUS IMAGES; GLAUCOMA; ENHANCEMENT; PREVALENCE																				978-0-08-102817-9; 978-0-08-102816-2				2019							199	221		10.1016/B978-0-08-102816-2.00011-3	10.1016/C2018-0-00865-8												
B								Diabetic retinopathy and maculopathy lesions	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										MICROANEURYSM DETECTION; AUTOMATED DETECTION; RETINAL IMAGES; SEGMENTATION; PREVENTION; BLINDNESS; QUANTIFICATION; CLASSIFICATION; MODEL																				978-0-08-102817-9; 978-0-08-102816-2				2019							223	243		10.1016/B978-0-08-102816-2.00012-5	10.1016/C2018-0-00865-8												
B								Drusen and macular degeneration	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										OPTICAL COHERENCE TOMOGRAPHY; AGE-RELATED MACULOPATHY; RETINAL-PIGMENT EPITHELIUM; SPECTRAL-DOMAIN; AUTOMATED DETECTION; GEOGRAPHIC ATROPHY; CHOROIDAL NEOVASCULARIZATION; MORPHOMETRIC-ANALYSIS; FUNDUS PHOTOGRAPHS; BRUCHS MEMBRANE																				978-0-08-102817-9; 978-0-08-102816-2				2019							245	272		10.1016/B978-0-08-102816-2.00013-7	10.1016/C2018-0-00865-8												
B								OCT fluid detection and quantification	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										OPTICAL-COHERENCE-TOMOGRAPHY; PIGMENT EPITHELIAL DETACHMENT; FULLY AUTOMATED DETECTION; MACULAR DEGENERATION; VISUAL-ACUITY; RETINAL LAYER; SUBRETINAL FLUID; INTRARETINAL FLUID; SEGMENTATION; IMAGES																				978-0-08-102817-9; 978-0-08-102816-2				2019							273	298		10.1016/B978-0-08-102816-2.00015-0	10.1016/C2018-0-00865-8												
B								Retinal biomarkers and cardiovascular disease: A clinical perspective	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										CORONARY-HEART-DISEASE; WHITE-MATTER LESIONS; MICROVASCULAR ABNORMALITIES; ATHEROSCLEROSIS RISK; DIABETIC-RETINOPATHY; VESSEL DIAMETERS; CEREBRAL ATROPHY; ANTIHYPERTENSIVE TREATMENT; INTRACEREBRAL HEMORRHAGE; VASCULAR CALIBER																				978-0-08-102817-9; 978-0-08-102816-2				2019							299	318		10.1016/B978-0-08-102816-2.00016-2	10.1016/C2018-0-00865-8												
B								Vascular biomarkers for diabetes and diabetic retinopathy screening	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										RETINAL VESSEL TORTUOSITY; SUB-RIEMANNIAN GEODESICS; IMAGE-ANALYSIS; COMPLICATIONS; SEGMENTATION; COMPLETION; FRACTALS; WAVELET; SYSTEM; FIELDS																				978-0-08-102817-9; 978-0-08-102816-2				2019							319	352		10.1016/B978-0-08-102816-2.00017-4	10.1016/C2018-0-00865-8												
B								Image analysis tools for assessment of atrophic macular diseases	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										OPTICAL COHERENCE TOMOGRAPHY; SUBRETINAL DRUSENOID DEPOSITS; RETINAL-PIGMENT EPITHELIUM; GEOGRAPHIC-ATROPHY; FUNDUS AUTOFLUORESCENCE; VISUAL-ACUITY; PHOTORECEPTOR INNER; SEVERITY SCALE; FELLOW-EYES; DEGENERATION																				978-0-08-102817-9; 978-0-08-102816-2				2019							353	378		10.1016/B978-0-08-102816-2.00018-6	10.1016/C2018-0-00865-8												
B								Artificial intelligence and deep learning in retinal image analysis	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										COHERENCE TOMOGRAPHY IMAGES; DIABETIC-RETINOPATHY; MACULAR DEGENERATION; VESSEL SEGMENTATION; OPTIC DISC; BLOOD-VESSELS; OCT IMAGES; CLASSIFICATION; LAYER; IDENTIFICATION																				978-0-08-102817-9; 978-0-08-102816-2				2019							379	404		10.1016/B978-0-08-102816-2.00019-8	10.1016/C2018-0-00865-8												
B								AI and retinal image analysis at Baidu	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series																														978-0-08-102817-9; 978-0-08-102816-2				2019							405	427		10.1016/B978-0-08-102816-2.00020-4	10.1016/C2018-0-00865-8												
B								The challenges of assembling, maintaining and making available large data sets of clinical data for research	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										IMAGES																				978-0-08-102817-9; 978-0-08-102816-2				2019							429	444		10.1016/B978-0-08-102816-2.00021-6	10.1016/C2018-0-00865-8												
B								Technical and clinical challenges of AI in retinal image analysis	COMPUTATIONAL RETINAL IMAGE ANALYSIS: TOOLS, APPLICATIONS AND PERSPECTIVES	Elsevier and MICCAI Society Book Series										DIABETIC-RETINOPATHY; AUTOMATED DETECTION; MEDICAL IMAGE; ARTIFICIAL-INTELLIGENCE; NEURAL-NETWORKS; RISK-FACTORS; OPTIC DISC; MICROANEURYSMS; QUANTIFICATION; SEGMENTATION																				978-0-08-102817-9; 978-0-08-102816-2				2019							445	466		10.1016/B978-0-08-102816-2.00022-8	10.1016/C2018-0-00865-8												
B								What Is Consciousness?	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							1	10			10.7551/mitpress/11705.001.0001												
B								Who Is Conscious?	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							11	24			10.7551/mitpress/11705.001.0001												
B								Animal Consciousness	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							25	31			10.7551/mitpress/11705.001.0001												
B								Consciousness and the Rest	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							33	38			10.7551/mitpress/11705.001.0001												
B								Consciousness and the Brain	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							39	51			10.7551/mitpress/11705.001.0001												
B								Tracking the Footprints of Consciousness	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							53	69			10.7551/mitpress/11705.001.0001												
B								Why We Need a Theory of Consciousness	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							71	77			10.7551/mitpress/11705.001.0001												
B								Of Wholes	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							79	91			10.7551/mitpress/11705.001.0001												
B								Tools to Measure Consciousness	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							93	104			10.7551/mitpress/11705.001.0001												
B								The Uber-Mind and Pure Consciousness	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							105	117			10.7551/mitpress/11705.001.0001												
B								Does Consciousness Have a Function?	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							119	128			10.7551/mitpress/11705.001.0001												
B								Consciousness and Computationalism	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							129	140			10.7551/mitpress/11705.001.0001												
B								Why Computers Can't Experience	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							141	154			10.7551/mitpress/11705.001.0001												
B								Is Consciousness Everywhere?	FEELING OF LIFE ITSELF: WHY CONSCIOUSNESS IS WIDESPREAD BUT CAN'T BE COMPUTED																															978-0-262-04281-9				2019							155	167			10.7551/mitpress/11705.001.0001												
J								Edge, Ridge, and Blob Detection with Symmetric Molecules	SIAM JOURNAL ON IMAGING SCIENCES										edge detection; ridge detection; blob detection; alpha-molecules; symmetry; phase congruency	VESSEL; IMAGES; NORMALIZATION; INVARIANT; DESIGN; CELLS	We present a novel approach to the detection and characterization of edges, ridges, and blobs in two-dimensional images which exploits the symmetry properties of directionally sensitive analyzing functions in multiscale systems that are constructed in the framework of alpha-molecules. The proposed feature detectors are inspired by the notion of phase congruency, stable in the presence of noise and, by definition, invariant to changes in contrast. We also show how the behavior of coefficients corresponding to differently scaled and oriented analyzing functions can be used to obtain a comprehensive characterization of the geometry of features in terms of local tangent directions, widths, and heights. The accuracy and robustness of the proposed measures are validated and compared to various state-of-the-art algorithms in extensive numerical experiments in which we consider sets of clean and distorted synthetic images that are associated with reliable ground truths. To further demonstrate the applicability, we show how the proposed ridge measure can be used to detect and characterize blood vessels in digital retinal images and how the proposed blob measure can be applied to automatically count the number of cell colonies in a Petri dish.																	1936-4954						2019	12	4					1585	1626		10.1137/19M1240861													
J								Issues with Common Assumptions about the Camera Pipeline and Their Impact in HDR Imagnig from Multiple Exposures	SIAM JOURNAL ON IMAGING SCIENCES										high dynamic range; multiexposure sequence; projective transformation; color stabilization	IMPLEMENTATION; TIME	Multiple-exposure approaches for high dynamic range (HDR) image generation share a set of building assumptions: that color channels are independent and that the camera response function (CRF) remains constant while changing the exposure. The first contribution of this paper is to highlight how these assumptions, which were correct for film photography, do not hold in general for digital cameras. As a consequence, results of multiexposure HDR methods are less accurate, and when tone-mapped they often present problems like hue shifts and color artifacts. The second contribution is to propose a method to stabilize the CRF while coupling all color channels, which can be applied to both static and dynamic scenes, and yield artifact-free results that are more accurate than those obtained with state-of-the-art methods according to several image metrics.																	1936-4954						2019	12	4					1627	1642		10.1137/19M1250248													
J								Invariant phi-Minimal Sets and Total Variation Denoising on Graphs	SIAM JOURNAL ON IMAGING SCIENCES										total variation; denoising; taut string; invariant phi-minimal sets; Rudin-Osher-Fatemi model; total variation flow	TOTAL VARIATION MINIMIZATION; SUBMODULAR FUNCTIONS; IMAGE DECOMPOSITION; ALGORITHM; DISCRETE; REGULARIZATION; OPTIMIZATION; RESTORATION; EQUIVALENCE	Total variation flow, total variation regularization, and the taut string algorithm are known to be equivalent filters for one-dimensional discrete signals. In addition, the filtered signal simultaneously minimizes a large number of convex functionals in a certain neighborhood of the data. In this article we study the question of to what extent this situation remains true in a more general setting, namely for data given on the vertices of an oriented graph and the total variation being J(f) = Sigma(i,j) vertical bar f (v(i)) - f (v(i))vertical bar. Relying on recent results on invariant phi-minimal sets we prove that the minimizer to the corresponding Rudin-Osher-Fatemi (ROF) model on the graph has the same universal minimality property as in the one-dimensional setting. Interestingly, this property is lost if J is replaced by the discrete isotropic total variation. Next, we relate the ROF minimizer to the solution of the gradient flow for J. It turns out that, in contrast to the one-dimensional setting, these two problems are not equivalent in general, but conditions for equivalence are available.																	1936-4954						2019	12	4					1643	1668		10.1137/19M124126X													
J								Well-Posedness for Photoacoustic Tomography with Fabry-Perot Sensors	SIAM JOURNAL ON IMAGING SCIENCES										thermoacoustic; imaging; inverse problems; Fabry-Perot sensor; ultrasound transducers	SURFACE RADIATION CONDITIONS; IMAGE-RECONSTRUCTION; BOUNDARY-CONDITIONS; TRANSDUCER; HYDROPHONE; SCATTERING; WAVES; MHZ	In the mathematical analysis of photoacoustic imaging, it is usually assumed that the acoustic pressure (Dirichlet data) is measured on a detection surface. However, actual ultrasound detectors gather data of a different type. In this paper, we propose a more realistic mathematical model of ultrasound measurements acquired by the Fabry-Perot sensor. This modeling incorporates directional response of such sensors. We study the solvability of the resulting photoacoustic tomography problem, concluding that the problem is well-posed under certain assumptions. Numerical reconstructions are implemented using the Landweber iterations, after discretization of the governing equations using the finite element method.																	1936-4954						2019	12	4					1669	1685		10.1137/19M1248297													
J								A New Variational Model for Joint Image Reconstruction and Motion Estimation in Spatiotemporal Imaging	SIAM JOURNAL ON IMAGING SCIENCES										spatiotemporal imaging; image reconstruction; motion estimation; joint variational model; shape theory; large diffeomorphic deformations	POSITRON-EMISSION-TOMOGRAPHY; RESPIRATORY MOTION; COMPUTED-TOMOGRAPHY; CARDIAC MOTION; CT; REGISTRATION; DIFFEOMORPHISMS; FLOWS	We propose a new variational model for joint image reconstruction and motion estimation applicable to spatiotemporal imaging, which is investigated along a general framework that we present with shape theory. This model consists of two parts, one that conducts modified image reconstruction in a static setting and the other that estimates the motion by sequentially indirect image registration. For the latter, we generalize the large deformation diffeomorphic metric mapping framework into the sequentially indirect registration setting. The proposed model is compared theoretically against alternative approaches (optical flow based model and diffeomorphic motion models), and we demonstrate that the proposed model has desirable properties in terms of the optimal solution. The theoretical derivations and efficient algorithms are also presented for a time-discretized scenario of the proposed model, which show that the optimal solution of the time-discretized version is consistent with that of the time-continuous one, and most of the computational components are the easily implemented linearized deformations. The complexity of the algorithm is analyzed as well. This work is concluded by some numerical examples in two-dimensional space + time tomography with very sparse and/or highly noisy data.																	1936-4954						2019	12	4					1686	1719		10.1137/18M1234047													
J								Entropy-Based Selection of Cluster Representatives for Document Image Compression	SIAM JOURNAL ON IMAGING SCIENCES										bilevel image; compression technique; hierarchic clustering algorithm; digitalized document; minimum entropy	ALGORITHM; CODES	In this work, we introduce an efficient method for lossy compression of digitalized documents. The method uses a dictionary which consists of class representatives defined using a minimum entropy criterion. The algorithm initially identifies the different symbols contained in a document image, and then the symbols are grouped in classes by means of a hierarchic clustering algorithm. For each class, a representative is selected using the principle of minimum entropy and suitable similarity distances. The technique creates a file in which every object belonging to a class is replaced by its class representative. Finally, the resulting file is compressed. The performance of the proposed algorithm is assessed using digitized files from a standard database for document compression along with different resolutions. Comparisons against other state-of-the-art algorithms are performed in this manuscript. The results establish quantitatively that the present methodology is a more efficient technique.																	1936-4954						2019	12	4					1720	1738		10.1137/19M1243312													
J								Directional Compactly Supported Tensor Product Complex Tight Framelets with Applications to Image Denoising and Inpainting	SIAM JOURNAL ON IMAGING SCIENCES										complex tight framelets; directionality; tensor product; complex-valued framelet filter banks; image denoising and inpainting; frequency separation property; compactly supported framelets	REPRESENTATIONS; TRANSFORM; WAVELETS	Compactly supported tight framelets are of great interest and importance in both theory and application. In this paper we discuss how to construct directional compactly supported tensor product complex tight framelets having varied directionality and good performance for applications in image processing. Our construction algorithms employ optimization techniques and put extensive emphasis on frequency response and spatial localization of their underlying one-dimensional tight framelet filter banks. Several concrete examples of directional compactly supported tensor product complex tight framelet filter banks are provided in this paper. Our numerical experiments show that such constructed directional compactly supported tensor product complex tight framelets have good performance for applications such as image denoising and inpainting compared with several other state-of-the-art transform-based methods.																	1936-4954						2019	12	4					1739	1771		10.1137/19M1249734													
J								Continuum Limits of Nonlocal p-Laplacian Variational Problems on Graphs	SIAM JOURNAL ON IMAGING SCIENCES										variational problems; nonlocal p-Laplacian; discrete solutions; error bounds; graph limits	REGULARIZATION; IMAGE; CONVERGENCE; CONSISTENCY	In this paper, we study a nonlocal variational problem which consists of minimizing in L-2 the sum of a quadratic data fidelity and a regularization term corresponding to the L-p-norm of the nonlocal gradient. In particular, we study convergence of the numerical solution to a discrete version of this nonlocal variational problem to the unique solution of the continuum one. To do so, we derive an error bound and highlight the role of the initial data and the kernel governing the nonlocal interactions. When applied to variational problems on graphs, this error bound allows us to show the consistency of the discretized variational problem as the number of vertices goes to infinity. More precisely, for networks in convergent graph sequences (simple and weighted deterministic dense graphs as well as random inhomogeneous graphs), we prove convergence and provide rates of convergence of solutions for the discrete models to the solution of the continuum problem as the number of vertices grows.																	1936-4954						2019	12	4					1772	1807		10.1137/18M1223927													
J								Analysis of the Block Coordinate Descent Method for Linear Ill-Posed Problems	SIAM JOURNAL ON IMAGING SCIENCES										ill-posed problems; convergence analysis; regularization theory; coordinate descent; multi-spectral CT	CONVERGENCE ANALYSIS; KACZMARZ METHODS; CT; ITERATION; SYSTEMS	Block coordinate descent (BCD) methods approach optimization problems by performing gradient steps along alternating subgroups of coordinates. This is in contrast to full gradient descent, where a gradient step updates all coordinates simultaneously. BCD has been demonstrated to accelerate the gradient method in many practical large-scale applications. Despite its success no convergence analysis for inverse problems is known so far. In this paper, we investigate the BCD method for solving linear inverse problems. As the main theoretical result, we show that for operators having a particular tensor product form, the BCD method combined with an appropriate stopping criterion yields a convergent regularization method. To illustrate the theory, we perform numerical experiments comparing the BCD and the full gradient descent method for a system of integral equations. We also present numerical tests for a nonlinear inverse problem not covered by our theory, namely one-step inversion in multispectral X-ray tomography.																	1936-4954						2019	12	4					1808	1832		10.1137/19M1243956													
J								Highly Accurate Matching of Weakly Localized Features	SIAM JOURNAL ON IMAGING SCIENCES										image registration; geometric context; match expansion; low-texture matching; transformation estimation; covariance propagation; image matching; local matching; affine transformation; perspective transformation; guided matching; geometric feedback; match verification	IMAGE; EXPANSION; SCALE	Matching corresponding local patches between images is a fundamental building block in many computer vision algorithms. It is extensively used in applications like three-dimensional structure estimation, model fitting, superresolution, image retrieval, and image recognition. As opposed to global image registration approaches, local matching reduces the high-dimensional challenge of recovering geometric relations between images to a series of relatively simple and independent tasks, defined in a limited local context. This approach is geometrically very flexible, simple to model, and has clear computational advantages. But it also has two fundamental practical shortcomings: (1) sparsity: the need to rely on repeatable features for matching limits the potential coverage of the scene to highly textured locations; (2) reliability: the limited spatial context in which those methods work often does not contain enough information for achieving reliable matches. These shortcomings also tend to trade off. While highly textured features, such as corners and blobs, often produce reliable matches, they are also relatively rare and thus very sparse in the image. And conversely, more common textures, such as edges or ridges, are largely discarded due to their low reliability for matching. We observe that while classic methods avoided using poorly localized features (e.g., edges) as matching candidates, these features contain highly valuable information for matching. We show how, given the appropriate geometric context, reliable matches can be produced from these features, contributing to a better coverage of the scene, while also producing highly accurate geometric transformation estimation. We present a statistically attractive framework for encoding the uncertainty that stems from using these features into a coupled geometric estimation and match extraction process. We examine the practical application of the proposed framework to the problems of guided matching and affine region expansion and show significant improvement over preceding methods. We compare our approach to state-of-the-art point matching methods and show its attractiveness in a variety of geometrically challenging scenarios.																	1936-4954						2019	12	4					1833	1863		10.1137/18M1231626													
J								Multivariate Myriad Filters Based on Parameter Estimation of Student-t Distributions	SIAM JOURNAL ON IMAGING SCIENCES										multivarite Student-t distribution; parameter estimation; generalized myriad filter; nonlocal robust denoising; patch based method	MAXIMUM-LIKELIHOOD; EM; IMAGES; ALGORITHM; LOCATION; MIXTURE	The contribution of this study is twofold. First, we propose an efficient algorithm for the computation of the (weighted) maximum likelihood estimators for the parameters of the multivariate Student-t distribution, which we call the generalized multivariate myriad filter. Second, we use the generalized multivariate myriad filter in a nonlocal framework for the denoising of images corrupted by different kinds of noise. The resulting method is very flexible and can handle heavy-tailed noise such as Cauchy noise, as well as the other extreme, namely Gaussian noise. Furthermore, we detail how the limiting case v -> 0 of the projected normal distribution in two dimensions can be used for the robust denoising of periodic data, in particular for images with circular data corrupted by wrapped Cauchy noise.																	1936-4954						2019	12	4					1864	1904		10.1137/19M1242203													
J								Proximal Activation of Smooth Functions in Splitting Algorithms for Convex Image Recovery	SIAM JOURNAL ON IMAGING SCIENCES										convex optimization; image recovery; inconsistent convex feasibility problem; proximal splitting algorithm; proximity operator	FORWARD-BACKWARD ALGORITHM; MONOTONE INCLUSIONS; VARIATIONAL FORMULATION; THRESHOLDING ALGORITHM; SIGNAL RECOVERY; RESTORATION; CONVERGENCE; DECOMPOSITION; PROJECTIONS; CONSISTENCY	Structured convex optimization problems typically involve a mix of smooth and nonsmooth functions. The common practice is to activate the smooth functions via their gradient and the nonsmooth ones via their proximity operator. We show that although intuitively natural, this approach is not necessarily the most efficient numerically and that, in particular, activating all the functions proximally may be advantageous. To make this viewpoint viable computationally, we derive a number of new examples of proximity operators of smooth convex functions arising in applications. A novel variational model to relax inconsistent convex feasibility problems is also investigated within the proposed framework. Several numerical applications to image recovery are presented to compare the behavior of fully proximal versus mixed proximal/gradient implementations of several splitting algorithms.																	1936-4954						2019	12	4					1905	1935		10.1137/18M1224763													
J								Extraction of Digital Wavefront Sets Using Applied Harmonic Analysis and Deep Neural Networks	SIAM JOURNAL ON IMAGING SCIENCES										wavefront set; deep learning; convolutional neural networks; shearlets	TOMOGRAPHY; RESOLUTION; TRANSFORM	Microlocal analysis provides deep insight into singularity structures and is often crucial for solving inverse problems, predominately, in imaging sciences. Of particular importance is the analysis of wavefront sets and the correct extraction of those. In this paper, we introduce the first algorithmic approach to extract the wavefront set of images, which combines data-based and model-based methods. Rased on a celebrated property of the shearlet transform to unravel information on the wavefront set, we extract the wavefront set of an image by first applying a discrete shearlet transform and then feeding local patches of this transform to a deep convolutional neural network trained on labeled data. The resulting algorithm outperforms all competing algorithms in edge-orientation and ramp-orientation detection.																	1936-4954						2019	12	4					1936	1966		10.1137/19M1237594													
J								Global Optimality in Separable Dictionary Learning with Applications to the Analysis of Diffusion MRI	SIAM JOURNAL ON IMAGING SCIENCES										separable dictionary learning; tensor factorization; global optimality; diffusion magnetic resonance imaging; HARDI	OVERCOMPLETE DICTIONARIES; SPARSE; ALGORITHM	Sparse dictionary learning is a popular method for representing signals as linear combinations of a few elements from a dictionary that is learned from the data. In the classical setting, signals are represented as vectors, and the dictionary learning problem is posed as a matrix factorization problem where the data matrix is approximately factorized into a dictionary matrix and a sparse matrix of coefficients. However, in many applications in computer vision and medical imaging, signals are better represented as matrices or tensors (e.g., images or videos). In such cases, instead of learning a large-scale dictionary tensor, it may be beneficial to exploit the multidimensional structure of the data to learn a more compact representation. One such approach is separable dictionary learning, where one learns separate dictionaries for different dimensions of the data (e.g., spatial and temporal dimensions of a video). However, while there has been significant recent work on separable dictionary learning, typical formulations involve solving a nonconvex optimization problem; thus, guaranteeing global optimality remains a challenge. In this work, we propose a framework that builds upon recent developments in matrix factorization to provide theoretical and numerical guarantees of global optimality for separable dictionary learning. Specifically, we prove that local minima are guaranteed to be global when some dictionary atoms and the corresponding coefficients are zero. We also propose an algorithm to find such a globally optimal solution, which alternates between following local descent steps and checking a certificate for global optimality. We illustrate our approach on diffusion magnetic resonance imaging (dMRI) data, a medical imaging modality that measures water diffusion along multiple angular directions in every voxel of a magnetic resonance imaging volume. State-of-the-art methods in dMRI either learn dictionaries only for the angular domain of the signals or in some cases learn spatial and angular dictionaries independently. In this work, we apply the proposed separable dictionary learning framework to learn spatial and angular dMRI dictionaries jointly and provide preliminary validation on denoising phantom and real dMRI brain data.																	1936-4954						2019	12	4					1967	2008		10.1137/18M121976X													
J								Fast Nonoverlapping Block Jacobi Method for the Dual Rudin-Osher-Fatemi Model	SIAM JOURNAL ON IMAGING SCIENCES										domain decomposition method; block Jacobi method; Rudin-Osher-Fatemi model; convergence rate; FISTA; parallel computation	DOMAIN DECOMPOSITION METHODS; TOTAL VARIATION MINIMIZATION; SUBSPACE CORRECTION METHODS; CONVERGENCE; ALGORITHM; FIDELITY	We consider nonoverlapping domain decomposition methods for the Rudin-Osher-Fatemi (ROF) model, which is one of the standard models in mathematical image processing. The image domain is partitioned into rectangular subdomains, and local problems in subdomains are solved in parallel. Local problems can adopt existing state-of-the-art solvers for the ROF model. We show that the nonoverlapping relaxed block Jacobi method for a dual formulation of the ROF model has the O(1/n) convergence rate of the energy functional, where n is the number of iterations. Moreover, by exploiting the forward-backward splitting structure of the method, we propose an accelerated version with a convergence rate of O(1/n(2)). The proposed method converges faster than existing domain decomposition methods both theoretically and practically, while the main computational cost of each iteration remains the same. We also provide the dependence of the convergence rates of the block Jacobi methods on the image size and the number of subdomains. Numerical results for comparison with existing methods are presented.																	1936-4954						2019	12	4					2009	2034		10.1137/18M122604X													
J								Large-Scale Bayesian Spatial-Temporal Regression with Application to Cardiac MR-Perfusion Imaging	SIAM JOURNAL ON IMAGING SCIENCES										Bayesian regression; perfusion imaging; large-scale regression; spatial-temporal regression	MYOCARDIAL BLOOD-FLOW; MICROSPHERE VALIDATION; QUANTITATIVE-ANALYSIS; QUANTIFICATION; MODEL; RECONSTRUCTION; APPROXIMATIONS; DISTRIBUTIONS; FEASIBILITY; PERFORMANCE	We develop a hierarchical Bayesian approach for the inference of large-scale spatial-temporal regression as often encountered in the analysis of imaging data. For each spatial location a linear temporal Gaussian regression model is considered. Large-scale refers to the large number of spatially distributed regression parameters to be inferred. The spatial distribution of the sought regression parameters, which typically represent physical quantities, is assumed to be smooth and bounded from below. Truncated, intrinsic Gaussian Markov random field priors are employed to express this prior knowledge. The dimensionality of the spatially distributed parameters is high, which challenges the calculation of results using standard Markov chain Monte Carlo procedures. A calculation scheme is developed that utilizes an approximate analytical expression for the marginal posterior of the amount of smoothness and the strength of the noise in the data, in conjunction with a conditional high-dimensional truncated Gaussian distribution for the spatial distribution of the regression parameters. We prove propriety of the posterior and explore the existence of its moments. The proposed approach is applicable to high-dimensional imaging problems arising in the use of Gaussian Markov random field priors for inferring spatially distributed parameters from spatial or spatial-temporal data. We demonstrate its application for the quantification of myocardial perfusion imaging from first-pass contrast-enhanced cardiovascular magnetic resonance data. The Bayesian approach allows for a quantification of perfusion, including a complete characterization of uncertainties which is desirable in diagnostics. It therefore provides not just a perfusion estimate but also a measure for how reliable this estimate is.																	1936-4954						2019	12	4					2035	2062		10.1137/19M1246274													
J								Lattice Identification and Separation: Theory and Algorithm	SIAM JOURNAL ON IMAGING SCIENCES										superposed lattice; lattice space; lattice separation	PATTERNS; SPACE; PERCEPTION; TEXTURES; ELEMENTS; MODEL	Motivated by the problems of lattice mixture identification and grain irregularity detection, we present a framework for lattice pattern representation and comparison and propose an efficient algorithm for lattice separation. We define new scale and shape descriptors, which considerably reduce the size of equivalence classes of lattice bases. These finite number of equivalence relations are fully characterized by the modular group theory. We construct the lattice space L based on the equivalent descriptors and define a metric d(L) to accurately quantify the visual similarities and differences between lattices. We introduce the lattice identification and separation algorithm (LISA), which identifies individual lattice patterns from superposed lattices. LISA finds lattice candidates from the high responses in the image spectrum, then extracts different layers of lattice patterns one by one. By analyzing the frequency components, we explore the intricate dependency of LISA's performances on particle radius, lattice density, and relative translations. Various numerical experiments are presented to show LISA's robustness against a large number of lattice layers, moire patterns, and missing particles.																	1936-4954						2019	12	4					2063	2096		10.1137/18M1234400													
J								Reconstruction of Domains with Algebraic Boundaries from Generalized Polarization Tensors	SIAM JOURNAL ON IMAGING SCIENCES										generalized polarization tensors; algebraic domains; inverse problems; shape classification	PART I	This paper aims at showing the stability of the recovery of a smooth planar domain with a real algebraic boundary from a finite number of its generalized polarization tensors. It is a follow-up of the work [H. Ammari, M. Putinar, A. Steenkamp, and F. Triki, Math. Ann., 375 (2019), pp. 1337-1354], where it is proved that the minimal polynomial with real coefficients vanishing on the boundary can be identified as the generator of a one-dimensional kernel of a matrix whose entries are obtained from a finite number of generalized polarization tensors. The recovery procedure is implemented without any assumption on the regularity of the domain to be reconstructed, and its performance and limitations are illustrated.																	1936-4954						2019	12	4					2097	2118		10.1137/19M125995X													
J								A Generalization of Wirtinger Flow for Exact Interferometric Inversion	SIAM JOURNAL ON IMAGING SCIENCES										interferometric inversion; Wirtinger flow; phase retrieval; wave-based imaging; low rank matrix recovery; PhaseLift; interferometric imaging	STABLE SIGNAL RECOVERY; LOCALIZATION; ALGORITHM	Interferometric inversion involves recovery of a signal from cross-correlations of its linear transformations. A close relative of interferometric inversion is the generalized phase retrieval problem, for which significant advancements were made in recent years despite the ill-posed and nonconvex nature of the problem. One such prominent phase retrieval method is Wirtinger flow (WF) [E. J. Candes, X. Li, and M. Soltanolkotabi, IEEE Trans. Inform. Theory, 61 (2015), pp. 1985-2007], a computationally efficient nonconvex optimization framework that provides high probability guarantees for exact recovery under certain measurement models, specifically coded diffraction patterns, and Gaussian sampling vectors. In this paper, we develop a generalization of WF for interferometric inversion, which we refer to as generalized Wirtinger flow (GWF). Our approach treats the theory of low rank matrix recovery and the nonconvex optimization approach of WF in a unified framework. Such a treatment facilitates the identification of a new sufficient condition on the lifted forward model for exact recovery via GWF and results in a deterministic framework based on geometric arguments for convergence. Thereby, GWF extends the model specific probabilistic guarantees in [12] to arbitrary measurement maps characterized over the equivalent lifted domain in the context of interferometric inversion, covering both random and deterministic measurement models. We then establish our sufficient condition for the cross-correlations of linear measurements collected by complex Gaussian sampling vectors. In the particular case of interferometric inversion with the Gaussian model, we show that the exact recovery theory of standard WF implies our sufficient condition when we have cross-correlations, and the regularity condition of WF is redundant. Finally, we demonstrate the effectiveness of GWF numerically in a deterministic, interferometric multistatic radar imaging scenario.																	1936-4954						2019	12	4					2119	2164		10.1137/19M1238599													
J								Load balancing with Job Migration Algorithm for improving performance on grid computing: Experimental Results	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Job Migration; Resource Utilization; Queue Length; CPU utilization; Memory utilization; Threshold; grid computing; load balancing		Grid is the collection of geographically distributed computing resources. For efficient management of these resources, the manager must maximize its utilization, which can be achieved by efficient load balancing with Job Migration techniques. Job Migration from overloaded resources to underloaded is an attempt to load balancing across all processors, thus reduce average response time. The decision of migration is based on the information exchange between resources. In this paper, the authors propose a novel Job Migration Algorithm for Dynamic Load Balancing (JMADLB), in which parameters such as CPU load and queue length have been considered and have been used for the selection of overloaded resources (or underloaded ones) in Grid. Here, the overloaded resources do not accept any new job; but, the new jobs are migrated to underloaded resources, even though this mechanism migrate extra jobs to obtain load balancing. The performances of the proposed algorithms were tested in Alea 2 simulator by using different parameters like response time, resources utilization and waiting time in the global queue. In addition, they were compared with other scheduling algorithms such as First Come First Served (FCFS) and Earliest Deadline First (EDF).																	2255-2863						2019	8	4					5	18		10.14201/ADCAIJ201984518													
J								Deep Learning in Biometrics: A Survey	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Deep learning; biometrics; fingerprint; ocular recognition; convolutional neural networks; electrocardiogram		Deep learning has been established in the last few years as the gold standard for data processing, achieving peak performance in image, text and audio understanding. At the same time, digital security is of utmost importance in this day and age, where everyone could get into our personal devices like cellphones or laptops, where we store our most valuable information. One of the possible ways to prevent this is via advanced and personalized security: biometrics. In this survey, it is considered how the scientific advances in the field of deep learning are applied to biometrics in order to enhance the protection of our data. Firstly, a study will be conducted on tackling ocular identification of twins using deep learning. Then, an improved method for avoiding fingerprint spoofing will be presented, thus solving this method's biggest issue. Finally, a brand new method for biometric identification is proposed based on the usage of the user's electrocardiogram. On every one of these methods, the results manage to top the standard alternative performances.																	2255-2863						2019	8	4					19	32		10.14201/ADCAIJ2019841932													
J								Multi-agent system for anomaly detection in Industry 4.0 using Machine Learning techniques	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Industry 4.0; Multiagent system; Machine Learning; Failure diagnosis	FAULT-DIAGNOSIS; MAINTENANCE; TRANSFORM	Industry 4.0 is the new industrial stage that's committed to greater automation, connectivity and globalization. The interrelation between different areas has penetrated the industrial world thanks to the Internet of things and the world of Big Data. This amount of information is available in plants and is growing increasingly, also aided by the network computing services offered by cloud computing or edge computing. That is why it's necessary to carry out complex fusion methods and data analysis using Machine Learning techniques to address specific industrial requirements and needs. The central challenge of industry 4.0 from the perspective of data science is to predict the history within monitored processes, providing as much information as possible, avoiding them and stave off severe economic losses. This article will show a review of the application of Artificial Intelligence (AI) techniques such as Machine Learning (ML) immersed in multi-agent systems (MAS) in Industry 4.0. For this, a bibliographic search has been carried out in databases recognized as Science Direct, Google Scholar, Scopus or Springer, filtering the investigations from 2018 to actuality. The article concludes pointing out possible future lines and the importance of transition towards the implementation of new technologies for the competitiveness of factories.																	2255-2863						2019	8	4					33	40		10.14201/ADCAIJ2019843340													
J								Detection of Hard Exudates in Retinopathy Images	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Hard Exudates; Retinopath; Fundus Image; Cotton Wool Spot; Retina		The tissue layer located at the back of the eye is known as retina which converts the incoming light into nerve signals and those signals are sent to the brain for understanding. The damage onto the retina is termed as retinopathy and that may lead to vision weakening or vision loss. The hard exudates are small white or yellowish white deposits with their edges being clear and sharp. In the proposed methods we take color image of retina then extract the green channel of that image then apply top hat transformation and bottom hat transformation on that image. The DIARETDB1 and High-Resolution Fundus (HRF) databases are used for performance evaluation of the proposed method. The proposed technique achieves accuracy 97%, sensitivity 95%, and specificity 96% and it takes average 5.6135 second for detection of hard exudates in an image.																	2255-2863						2019	8	4					41	48		10.14201/ADCAIJ2019844148													
J								NorMAS-ML: Supporting the Modeling of Normative Multi-agent Systems	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Normative Multiagent System; Norms; Modeling Language; NorMAS-ML	ORGANIZATIONS; VIEW	Context: A normative multi-agent system (NMAS) is composed of agents that their behavior is regulated by norms. The modeling of those elements (agents and norms) together at design time can be a good way for a complete understanding of their structure and behavior. Multi-agent system modeling language (MAS-ML) supports the representation of NMAS entities, but the support for concepts related to norms is somewhat limited. MAS-ML is founded in taming agents and objects (TAO) framework and has a support tool called the MAS-ML tool. Goal: The present work aims to present a UML-based modeling language called normative multi-agent system (NorMAS-ML) able to model the MAS main entities along with the static normative elements. Method: We extend the TAO adding normative concepts and spread out those concepts in two syntaxes of MAS-ML. Either abstract, adding or updating metaclasses and stereotypes or concrete, defining new graphic elements for representing the elements defined in the abstract syntax. Besides, we evolve the MAS-ML tool, considering the extension of MAS-ML by the model-driven approach. Results: NorMAS-ML, the new version of MAS-ML, allows a complete view of NMAS entities and has a support tool called NorMAS-ML tool. Beyond the definition of NorMAS-ML and its tool, we generate a new static diagram called "norm diagram" supported by the NorMAS-ML tool. In order to illustrate the syntax of NorMAS-ML, the entities of a conference management system and its norms are modeled jointly. Conclusion: NorMAS-ML can help software designers (i) to understand the properties and behavior of NMAS entities and (ii) to provide a software modeling following the stakeholders' need and less complex for the development phase.																	2255-2863						2019	8	4					49	81		10.14201/ADCAIJ2019844981													
J								Towards a model-theoretic framework for describing the semantic aspects of cognitive processes	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Heuristics; cognitive architectures; model-theoretic semantics; episodic memory		Semantics is one of the most challenging aspects of cognitive architectures. Mathematical logic, or linguistics, highlights that semantics is essential to human cognition. The Cognitive Theory of True Conditions (CTTC) is a proposal to implement cognitive abilities and to describe the semantics of symbolic cognitive architectures based on model-theoretic semantics. This article focuses on the concepts supporting the mathematical formulation of the CTTC, its relationship to other proposals, and how it can be used as a framework for designing cognitive abilities in agents.																	2255-2863						2019	8	4					83	96		10.14201/ADCAIJ2019848396													
J								Stock Market Prediction Using Machine Learning(ML)Algorithms	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Stock Market Prediction; Machine Learning(ML); Algorithms; Linear Regression; Exponential Smoothing; Time Series Forecasting		Stocks are possibly the most popular financial instrument invented for building wealth and are the centerpiece of any investment portfolio. The advances in trading technology has opened up the markets so that nowadays nearly anybody can own stocks. From last few decades, there seen explosive increase in the average person's interest for stock market. In a financially explosive market, as the stock market, it is important to have a very accurate prediction of a future trend. Because of the financial crisis and recording profits, it is compulsory to have a secure prediction of the values of the stocks. Predicting a non-linear signal requires progressive algorithms of machine learning with help of Artificial Intelligence (AI). In our research, we are going to use Machine Learning Algorithm specially focus on Linear Regression (LR), Three month Moving Average(3MMA), Exponential Smoothing (ES) and Time Series Forecasting using MS Excel as best statistical tool for graph and tabular representation of prediction results. We obtained data from Yahoo Finance for Amazon (AMZN) stock, AAPL stock and GOOGLE stock after implementation LR we successfully predicted stock market trend for next month and also measured accuracy according to measurements.																	2255-2863						2019	8	4					97	116		10.14201/ADCAIJ20198497116													
B								Nature-inspired algorithms for the optimal tuning of fuzzy controllers	NATURE-INSPIRED OPTIMIZATION ALGORITHMS FOR FUZZY CONTROLLED SERVO SYSTEMS										Charged systems search algorithms; Gravitational search algorithms; Gray wolf optimizer algorithms; Particle swarm optimization; Takagi-Sugeno proportional-integral fuzzy controllers	PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZER; GRAVITATIONAL SEARCH ALGORITHM; CONTROL-SYSTEMS; VARIANTS; LOGIC	This chapter describes the mechanisms occurring in four representative nature-inspired optimization algorithms: particle swarm optimization, gravitational search algorithms, charged systems search algorithms, and gray wolf optimizer algorithms. These algorithms are inserted in two steps of the design approach dedicated to the optimal tuning of simple Takagi-Sugeno proportional-integral fuzzy controllers involved in the position control of servo systems. Four optimization problems are solved and some results concerning the algorithms' behavior are outlined.																			978-0-12-816606-2; 978-0-12-816358-0				2019							55	80		10.1016/B978-0-12-816358-0.00002-3													
B								Adaptive nature-inspired algorithms for the optimal tuning of fuzzy controllers	NATURE-INSPIRED OPTIMIZATION ALGORITHMS FOR FUZZY CONTROLLED SERVO SYSTEMS										Adaptive charged systems search algorithms; Adaptive gravitational search algorithms; Fuzzy logic-based adaptation; Popov sum; Takagi-Sugeno proportional-integral fuzzy controllers	GRAVITATIONAL SEARCH ALGORITHM; PARAMETER ADAPTATION; LOGIC; SYSTEMS; PSO; GSA	This chapter describes the adaptation mechanisms in two representative nature-inspired optimization algorithms, namely gravitational search algorithms (GSAs) and charged systems search algorithms. The adaptation in both algorithms is carried out in terms of the 5E learning cycle, and fuzzy logic is inserted in GSAs to adapt two parameters. These algorithms are inserted in the design approach dedicated to the optimal tuning of simple Takagi-Sugeno proportional-integral fuzzy controllers for the position control of servo systems. Four optimization problems are solved and representative results concerning the algorithms' behavior are outlined.																			978-0-12-816606-2; 978-0-12-816358-0				2019							81	101		10.1016/B978-0-12-816358-0.00003-5													
B								Hybrid nature-inspired algorithms for the optimal tuning of fuzzy controllers	NATURE-INSPIRED OPTIMIZATION ALGORITHMS FOR FUZZY CONTROLLED SERVO SYSTEMS										Exploration; Exploitation; Hybrid particle swarm optimization-gravitational search algorithms; Hybrid gray wolf optimization-particle swarm optimization; Takagi-Sugeno proportional-integral fuzzy controllers	DESIGN; PSOGSA; GSA	This chapter first describes the hybridization of particle swarm optimization (PSO) and gravitational search algorithms (GSAs) and later introduces a gray wolf optimization (GWO) using PSO's search mechanism. The operating algorithm of a hybrid PSOGSA and GWOPSO are presented. Combining two nature-inspired algorithms is necessary in order to reduce one's search process drawbacks by using the other's fortes. In the case of the PSOGSA, the PSO's exploitation capabilities and GSA's exploration abilities are combined to avoid getting trapped in local minima situations, while in GWOPSO's case, the exploitation advantages of PSO are employed in order to speed the GWO's convergence. The hybrid algorithms are inserted in the design approach dedicated to the optimal tuning of simple Takagi-Sugeno proportional-integral fuzzy controllers for the position control of servo systems. Four optimization problems are solved and some results concerning the hybrid PSOGSA's behavior are presented.																			978-0-12-816606-2; 978-0-12-816358-0				2019							103	114		10.1016/B978-0-12-816358-0.00004-7													
J								An Experimental Performance Comparison of Widely Used Face Detection Tools	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										face detection; face localization; computer vision	TRACKING	Face detection is the task of detecting faces on photos, videos as well as the streaming data such as a webcam. Face detection, which is a specific type of general-purpose object detection, is a key prerequisite for many other artificial intelligence tasks such as face verification, face tagging and retrieval, and face tracking. In addition to that, convolutional neural nowadays, face detection is commonly used in daily routines such as social media, and network camera software of smartphones. As a result of this necessity, several face detection tools have been proposed. In this study, an experimental performance comparison of well-known face detection tools in terms of (1) accuracy, and (2) elapsed time of detection, which has become even more critical criteria especially when the face detection mechanism is utilized for a real-time system, is proposed. As a result of this experimental study, it is aimed that shed light on the much-concerned query "which face detection tool provides the best performance?". In addition to that, this study succeeds in showing that convolutional neural networks achieve great accuracy for face detection.																	2255-2863						2019	8	3					5	12		10.14201/ADCAIJ201983512													
J								LSTM Based Lip Reading Approach for Devanagiri Script	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Lip-reading; LSTM; Machine learning; Deep Learning; Feature extraction	FEATURES	Speech Communication in a noisy environment is a difficult and challenging task. Many professionals work in noisy environments like aviation, constructions, or manufacturing, and find it difficult to communicate orally. Such noisy environments need an automated lip-reading system that could be helpful in communicating some instructions and commands. This paper proposes a novel lip-reading solution, which extracts the geometrical shape of lip movement from the video and predicts the words/sentences spoken. An Indian specific language data set is developed which consists of lip movement information captured from 50 persons. This includes students in the age group of 18 to 20 years and faculty in the age group of 25 to 40 years. All have spoken a paragraph of 58 words within 10 sentences in Hindi (Devanagari, spoken in India) language which was recorded under various conditions. The implementation consists of facial parts detection, along with Long short term memory's. The proposed solution is able to predict the words spoken with 77% and 35% accuracy for data set of 3 and 10 words respectively. The sentences are predicted with 20% accuracy, which is encouraging.																	2255-2863						2019	8	3					13	26		10.14201/ADCAIJ2019831326													
J								Estimation of Number of Flight Using Particle Swarm Optimization and Artificial Neural Network	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										artificial neural network; airport; particle swarm optimization; estimation	AIR PASSENGER	The number of flight (NF) is one of the key factors for the administration of the airport to evaluate the apron capacity and airline companies to fix the size of the flight. This paper aims to estimate the monthly NF by performing particle swarm optimization (PSO) and artificial neural network (ANN). Performed PSO-ANN algorithm aims to minimize the proposed evaluation criterion in the training stage. PSO-ANN based on the proposed evaluation criterion offers satisfying fitness values with respect to correlation coefficient and mean absolute percentage error in the training and testing stage.																	2255-2863						2019	8	3					27	33		10.14201/ADCAIJ2019832733													
J								Ranking Factors Affecting Organizational Readiness to Implement Enterprise Resource Planning Systems Using Fuzzy-Dimensional Network Analysis	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Enterprise Resource Planning (ERP); SWOT Analysis; Network Analysis; Fuzzy Dematel	RELEVANT ACTIVITIES; ERP	Whenever an organization decides to implement an ERP, it must assess its readiness to implement these complex systems. Therefore, the present study aims at considering the pre-implementation phase of ERP and the factors affecting the readiness of the organization for successful implementation of these systems. In this research, the SWOT matrix was used to classify the identified factors. Moreover, fuzzy-dimensional network analysis was used to evaluate decision options due to the weakness of SWOT technique. Since the factors involved in SWOT analysis are not only disjointed, but sometimes there are relationships among some of its factors. Therefore, internal and external factors of the organization are evaluated and prioritized in the research. In addition, finding the strategic position of the organization and identifying the appropriate strategies and prioritizing them to improve the organization's readiness for ERP implementation were other tasks in this research. After analyzing the data, 25 organizational factors are prioritized. The numerical results indicated that the organization is in the offensive position therefore, four strategies are designed and prioritized according to its position.																	2255-2863						2019	8	3					35	50		10.14201/ADCAIJ2019833550													
J								The Importance Of Development of Control Processes and Methods for Urban Bus Services	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										urban mobility; public transport; externalities; operation; control		This article aims to discuss the importance of the development of methodologies and control processes by the public power for the proper management of the urban bus system. It reflects on the idea of management control from the perspective of an operational, strategic and innovative perspective. It proposes a new look at the solution of negative externalities that arise in the urban environment of cities such as collisions, roadkill, works that will obstruct the roadbed and various events disrupting local traffic and directly impacting the mobility of public transport on tires. The theme is current, as it addresses the most problematic urban problem in the contemporary world, which is mobility. Thus, the article tells the experience of Sao Paulo and discusses a new control methodology for urban bus service.																	2255-2863						2019	8	3					51	65		10.14201/ADCAIJ2019835165													
J								ck-NN: A Clustered k-Nearest Neighbours Approach for Large-Scale Classification	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										k-Nearest Neighbor Classification; k-Means Clustering; Machine Learning; Artificial Intelligence; Distributed Computing	EFFICIENT	k-Nearest Neighbor (k-NN) is a non-parametric algorithm widely used for the estimation and classification of data points especially when the dataset is distributed in several classes. It is considered to be a lazy machine learning algorithm as most of the computations are done during the testing phase instead of performing this task during the training of data. Hence it is practically inefficient, infeasible and inapplicable while processing huge datasets i.e. Big Data. On the other hand, clustering techniques (unsupervised learning) greatly affect results if you do normalization or standardization techniques, difficult to determine "k" Value. In this paper, some novel techniques are proposed to be used as pre-state mechanism of state-of-the-art k-NN Classification Algorithm. Our proposed mechanism uses unsupervised clustering algorithm on large dataset before applying k-NN algorithm on different clusters that might running on single machine, multiple machines or different nodes of a cluster in distributed environment. Initially dataset, possibly having multi dimensions, is pass through clustering technique (K-Means) at master node or controller to find the number of clusters equal to the number of nodes in distributed systems or number of cores in system, and then each cluster will be assigned to exactly one node or one core and then applies k-NN locally, each core or node in clusters sends their best result and the selector choose best and nearest possible class from all options. We will be using one of the gold standard distributed framework. We believe that our proposed mechanism could be applied on big data. We also believe that the architecture can also be implemented on multi GPUs or FPGA to take flavor of k-NN on large or huge datasets where traditional k-NN is very slow.																	2255-2863						2019	8	3					67	77		10.14201/ADCAIJ2019836777													
J								An Information Recognition System for Complex Images	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										software package; artificial intelligence; pattern recognition; ultrasonic irradiation; closed contour		An approach to objective assessment of ultrasound examination is presented. To this end, modern information technologies and a set of mathematical methods in the form of a package are proposed. In this paper, diagnosis is viewed as a three-step process, and closed sub-objects are investigated using complex images, which pertains to the earliest diagnostic stage. For this purpose, three new features related to the disclosure of a growth are included in the paper. A system that performs the detection of the growth and finds the coordinates, area, gravity center and color palette of the obtained image is developed. By means of the created software package, the image is cleared from noise, filtering operations are performed, boundaries are defined more clearly and recognition by the mathematical morphology method is completed using selected classifiers. The main purpose is to direct doctor's attention to the presence of the pre-indicator of a non-specific symptom and to control the future development of the growth. The accuracy of the system is confirmed by the detection and identification of closed growths in the images taken in an ultrasound examination of internal organs of the human body. The system's operability has been tested directly on the ultrasound images (138 cases investigated), with the result of 98.8% at the diagnostic stage, 92, 03% at the early diagnostic stage; 2 cases have been recorded at the earliest diagnostic stage in 2018 and the frequency of monitoring has been determined.																	2255-2863						2019	8	3					79	93		10.14201/ADCAIJ2019837993													
J								Learning Representations from Spatio-Temporal Distance Maps for 3D Action Recognition with Convolutional Neural Networks	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Spatio-Temporal; Distance Maps; CNN; Skeleton maps	TERM	This paper addresses the action recognition problem using skeleton data. In this work, a novel method is proposed, which employs five Distance Maps (DM), named as Spatio-Temporal Distance Maps (ST-DMs), to capture the spatio-temporal information from skeleton data for 3D action recognition. Among five DMs, four DMs capture the pose dynamics within a frame in the spatial domain and one DM captures the variations between consecutive frames along the action sequence in the temporal domain. All DMs are encoded into texture images, and Convolutional Neural Network is employed to learn informative features from these texture images for action classification task. Also, a statistical based normalization method is introduced in this proposed method to deal with variable heights of subjects. The efficacy of the proposed method is evaluated on two datasets: UTD MHAD and NTU RGB+D, by achieving recognition accuracies 91.63% and 80.36% respectively.																	2255-2863						2019	8	2					5	18		10.14201/ADCAIJ201982518													
J								Segmentation and detection of cattle branding images using CNN and SVM classification	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Convolutional Neural Network; Support Vector Machines; Cattle Branding		This article presents a hybrid method that uses Convolutional Neural Networks (CNN) to segmentation and Support Vector Machines (SVM) to detection the brandings. The experiments were performed using a cattle branding images. Metrics of Overall Accuracy, Recall, Precision, Kappa Coefficient, and Processing Time were used in order to assess the proposed tool. The results obtained here were satisfactory, reaching a Overall Accuracy of 93% in the first experiment with 39 brandings and 1,950 sample images, and 95% of accuracy in the second experiment, with the same 39 brandings, but with 2,730 sample images. The processing time attained in the experiments was 32s and 42s, respectively.																	2255-2863						2019	8	2					19	31		10.14201/ADCAIJ2019821931													
J								An Intelligent Multi-Resolutional and Rotational Invariant Texture Descriptor for Image Retrieval Systems	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Content Based Image Retrieval (CBIR); Local Binary Pattern (LBP); Discrete Wavelet Transform (DWT); Support vector machine; Extreme learning machine	PATTERNS; MACHINE; COLOR	To find out the identical or comparable images from the large rotated databases with higher retrieval accuracy is the challenging task in Content based Image Retrieval systems (CBIR). Considering this problem, an intelligent and efficient technique is proposed for texture based images. In this method, firstly a new joint feature vector is created which inherits the properties of Local binary pattern (LBP) which has steadiness regarding changes in illumination and rotation and discrete wavelet transform (DWT) which is multi-resolutional and multi-oriented along with higher directionality. Secondly, after the creation of hybrid feature vector, to increase the accuracy of the system, classifiers are employed on the combination of LBP and DWT. The performance of two machine learning classifiers is proposed here which are Support Vector Machine (SVM) and Extreme learning machine (ELM). Both proposed methods P1 (LBP+DWT+SVM) and P2 (LBP+DWT+ELM) are tested on rotated Brodatz dataset consisting of 1456 texture images and MIT VisTex dataset of 640 images. In both experiments the results of both the proposed methods are much better than simple combination of DWT +LBP and much other state of art methods in terms of precision and accuracy when different number of images is retrieved. But the results obtained by ELM algorithm shows some more improvement than SVM. Such as when top 25 images are retrieved then in case of Brodatz database the precision is up to 94% and for MIT VisTex database its value is up to 96% with ELM classifier which is very much superior to other existing texture retrieval methods.																	2255-2863						2019	8	2					33	49		10.14201/ADCAIJ2019823349													
J								Education System re-engineering with AI (artificial intelligence) for Quality Improvements with proposed model	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Reengineering; Artificial intelligence (AI); Multi-face Recognition; Facial Expression (FE); Decision making; Quality assurance; Brain simulation; Quality Education; AI; Expert (ES)		Re-engineering (RE) of existing educational institutions (EI) with adoption of latest technology trends (LTT) in form of artificial intelligence (AI) can be great effective in term of quality systems. Increase in student's strength in class and terrorist attacks on EI urged us to introduce such approach that can assure education quality. Class monitoring with heavy strength always remain major issue for teacher during lecture delivery. In this paper, we implemented reengineering using artificial intelligence based two theories of 1) Multi-face recognition (MFR) system 2) Facial expression recognition (FER) system. Both of these theories supported by intelligent techniques as principal component analysis (PCA), discrete wavelet transform (DWT) and k-nearest neighbor (KNN). After implementation of these intelligent techniques student's attentiveness will increase. Our developed system can detect expressions like happiness, repulsion, fear, anger, and confusion. Student's attentiveness score will be displayed on screen. Teacher can interpret on the basis of attentiveness %age. System decision making can be helpful for class continuity or short break. This system is also an application of an expert system (ES) and knowledge base system (KBS) for educational quality assurance. A similar monitoring system was imposed in china with Hikvision Digital Technology. Predations results proved monitoring can be best way for education quality.																	2255-2863						2019	8	2					51	60		10.14201/ADCAIJ2019825160													
J								IoT based intelligent irrigation support system for smart farming applications	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										IoT; smart farming; DCT; neural network; automatic irrigation	INTERNET; ARCHITECTURES; PRECISION	India is an agricultural country with an ample amount of arable land that produces wide variety of crops. Growing population and urbanization puts up challenges: more and quality yield in limited area, effective utilization of water resources, inculcating technology with traditional mechanisms, to be faced. A crop irrigation management system with sensor data fetch, transfer and operate functionalities is proposed to meet the expectations. The system comprises of: sensing, data processing and actuator sections, with a network of ambient temperature and humidity at a height and, soil moisture sensor placed at the root zone of the subject. The sensor generated data is compressed and then sent to an FTP server for processing. At the server, a 2-layer Neural Network with 4-Inputs, plant growth, temperature, humidity and soil moisture is used for decision making that controls water supply, fertilizer spray, etc. and a plant is used as the test object. Results show that there is tolerable error in the reconstructed data and 62.5% and 67.5% compression is achieved for ambient temperature, humidity and soil moisture respectively. The decisions are only 2% erroneous when done using Neural Networks using this data. Thus, due to its good data handling, decision making capabilities for precise water usage, being portable and user-friendly, this system proves beneficial in home gardens, greenhouses.																	2255-2863						2019	8	2					73	85		10.14201/ADCAIJ2019827385													
J								Perception policies for intelligent virtual agents	ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE JOURNAL										Virtual Intelligent Agent; Perception; Sigon	ACTIVE PERCEPTION	Agents deployed to dynamic environments, such as virtual and augmented reality, need specific mechanisms to capture relevant features from the environment. These mechanisms enable agents to avoid process some useless information and act quickly. The primary goal of this work is to investigate the perception policies of an agent situated in a virtual environment. Perception policies allow giving more priority to sensors perceiving the changes occurring in the environment. Based on the proposed model, each sensor follows a strategy that can change its priority in the overall system. We developed two policies to change the sensors prioritization. The performance evaluation of the proposed model consists of comparing both approaches in a highly dynamic environment.																	2255-2863						2019	8	2					87	96		10.14201/ADCAIJ2019828796													
J								AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Affective computing in the wild; facial expressions; continuous dimensional space; valence; arousal	FACE; RECOGNITION; EMOTION; MODEL	Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1,250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.																	1949-3045					JAN-MAR	2019	10	1			SI		18	31		10.1109/TAFFC.2017.2740923													
J								Truncated Cauchy Non-Negative Matrix Factorization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Non-negative matrix factorization; truncated cauchy loss; robust statistics; half-quadratic programming	LINEAR-ESTIMATION; FACE RECOGNITION; MINIMIZATION; PARAMETERS; OBJECTS; PARTS	Non-negative matrix factorization (NMF) minimizes the euclidean distance between the data matrix and its low rank approximation, and it fails when applied to corrupted data because the loss function is sensitive to outliers. In this paper, we propose a Truncated CauchyNMF loss that handle outliers by truncating large errors, and develop a Truncated CauchyNMF to robustly learn the subspace on noisy datasets contaminated by outliers. We theoretically analyze the robustness of Truncated CauchyNMF comparing with the competing models and theoretically prove that Truncated CauchyNMF has a generalization bound which converges at a rate of order O(root ln n/n), where n is the sample size. We evaluate Truncated CauchyNMF by image clustering on both simulated and real datasets. The experimental results on the datasets containing gross corruptions validate the effectiveness and robustness of Truncated CauchyNMF for learning robust subspaces.																	0162-8828	1939-3539				JAN	2019	41	1					246	259		10.1109/TPAMI.2017.2777841													
B								Preliminary Information on Fuzzy Logic	FUZZY LOGIC-BASED MATERIAL SELECTION AND SYNTHESIS																															978-9-81-327656-7				2019							1	58			10.1142/11164												
B								State-of-the-Art of Material Selection and Synthesis	FUZZY LOGIC-BASED MATERIAL SELECTION AND SYNTHESIS																															978-9-81-327656-7				2019							59	82			10.1142/11164												
B								Fuzzy Material Selection Methodology	FUZZY LOGIC-BASED MATERIAL SELECTION AND SYNTHESIS																															978-9-81-327656-7				2019							83	148			10.1142/11164												
B								Intelligent System for Synthesis of Materials with Characteristics Required	FUZZY LOGIC-BASED MATERIAL SELECTION AND SYNTHESIS																															978-9-81-327656-7				2019							149	189			10.1142/11164												
B								Case Study	FUZZY LOGIC-BASED MATERIAL SELECTION AND SYNTHESIS																															978-9-81-327656-7				2019							191	206			10.1142/11164												
B								Basic Concepts	ADAPTIVE SLIDING MODE NEURAL NETWORK CONTROL FOR NONLINEAR SYSTEMS	Emerging Methodologies and Applications in Modelling, Identification and Control																														978-0-12-815372-7				2019							1	16		10.1016/B978-0-12-815372-7.00001-X													
J								Improving strategic decision making by the detection of weak signals in heterogeneous documents by text mining techniques	AI COMMUNICATIONS										Weak signal of the future; strategic decision making; text mining; business intelligence architecture; unstructured information	BUSINESS INTELLIGENCE	At present, one of the greatest threats to companies is not being able to cope with the constant changes that occur in the market because they do not predict them well in advance. Therefore, the development of new processes that facilitate the detection of significant phenomena and future changes is a key component for correct decision making that sets a correct course in the company. For this reason, a business intelligence architecture system is hereby proposed to allow the detection of discrete changes or weak signals in the present, indicative of more significant phenomena and transcendental changes in the future. In contrast to work currently available focusing on structured information sources, or at most with a single type of data source, the detection of these signals is here quantitatively based on heterogeneous and unstructured documents of various kinds (scientific journals, newspaper articles and social networks), to which text mining and natural language processing techniques (a multi-word expression analysis) are applied. The system has been tested to study the future of the artificial intelligence sector, obtaining promising results to help business experts in the recognition of new driving factors of their markets and the development of new opportunities.																	0921-7126	1875-8452					2019	32	5-6					347	360		10.3233/AIC-190625													
J								A feature selection approach combining neural networks with genetic algorithms	AI COMMUNICATIONS										Feature selection; classification accuracy; Genetic Algorithms; neural networks; population size; computational time	UNSUPERVISED FEATURE-SELECTION; PARTICLE SWARM OPTIMIZATION; CLASSIFICATION; INFORMATION; POPULATION	Value Feature selection is an effective method to solve the curse of dimensionality, which widely employs Evolutionary Computation (EC), such as Genetic Algorithms (GA), by regarding feature subsets as individuals. However, it is impossible for EC based feature selection approaches to possess big population sizes because of very long and infeasible computational time. We have proposed a method screening individuals by estimating their classification performances rapidly instead of deriving theirs with a certain classifier dilatorily. Consequently, aiming at improving classification accuracies, we propose an approach named as FS-NN-GA (Feature Selection approach based on Neural Networks and Genetic Algorithms) in this work. The proposed approach employs the neural networks trained with some randomly generated individuals, and their actual classification accuracies to estimate individuals' classification accuracies and screens them in each round of GA. The individuals with low estimated accuracies are directly eliminated. Only a small number of individuals with high estimated accuracies are reserved, evaluated by deriving their accuracies with a certain classifier, and participate GA operations to be explored emphatically. As a result, big population sizes become feasible, and a huge number of individuals can be considered by GA in acceptable and feasible time, which improves performances of GA and derives high accuracies. We perform the experiments with 10 data sets in comparison with 11 available approaches. The experimental results show that FS-NN-GA outperforms other approaches on most data sets.																	0921-7126	1875-8452					2019	32	5-6					361	372		10.3233/AIC-190626													
J								The CADE-27 Automated theorem proving System Competition - CASC-27	AI COMMUNICATIONS										Automated theorem proving; competition		The CADE ATP System Competition (CASC) is the annual evaluation of fully automatic, classical logic Automated Theorem Proving (ATP) systems. CASC-27 was the twenty-fourth competition in the CASC series. Twenty-five ATP systems and system variants competed in the various competition divisions. This paper presents an outline of the competition design, and a commentated summary of the results.																	0921-7126	1875-8452					2019	32	5-6					373	389		10.3233/AIC-190627													
J								Intelligent avatars and emotion in medical-based virtual learning environments	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Intelligent avatar; virtual environment; virtual simulation; serious game; emotion	RECOGNITION; COMMUNICATION; REALITY; EXPRESSIONS; DISORDERS; EDUCATION; TEXT	The paper presents an overview of the current state of research in intelligent avatars, with a particular focus on the detection and expression of emotions in virtual environments. Our interest lies particularly in the potential use of such tools for the development of medical (mental and physical health) training virtual environments, and virtual simulations and serious games in particular.																	1872-4981	1875-8843					2019	13	4					407	416		10.3233/IDT-190112													
J								Multi-agent system based sequential energy management strategy for Micro-Grid using optimal weighted regularized extreme learning machine and decision tree	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Renewable energies; Micro-Grid; multi-agent system; extreme learning machine; optimal weighted regularized extreme; learning machine; particle swarm optimization; decision tree	FUZZY-LOGIC	Renewable energies are fundamentally changing the traditional power grid. Their integration in micro grid constitutes the best way to produce clean energy in a large scale. However, classical control methods based centralized approaches are not efficient to manage and control the different operations in micro grid. In this paper, an intelligent energy management system is presented for micro grid power control based on the distributed paradigm of Multi-Agent System. Its main objective is to find the optimal control of a MG with grid-connected mode in order to control the amount of power delivered or taken from the Distribution Network so as to minimize the cost and maximize the benefit. We present also a photovoltaic and wind power prediction method using an Optimal Weighted Regularized Extreme Learning Machine algorithm in which the Particle Swarm Optimization method is used to optimize the regularization parameter. The algorithm is tested on real weather data and has shown a good generalization performance and better results than the basic Extreme Learning Machine algorithm while keeping its extremely fast training speed ability. To establish an efficient energy management strategy, a Decision Tree is used to ensure the availability of power on demand by taking a reasonable decision about charging batteries/selling electricity and discharging batteries/buying electricity in order to reduce the balance between cost and benefit.																	1872-4981	1875-8843					2019	13	4					479	494		10.3233/IDT-190003													
J								A deep learning based CNN approach on MRI for Alzheimer's disease detection	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Backpropagation; Convolutional Neural Networks; deep learning; OASIS; supervised learning	MILD COGNITIVE IMPAIRMENT; CLASSIFICATION; OASIS	Alzheimer's disease is a brain disorder which causes the malfunction of neurons. This disease can cause loss of brain function and dementia which can further damage memory, thought process and human behaviour. Regardless of being a worst disease, it has no cure. Only handful of classification strategies have been proposed in the literature that too with a small set of training images. Existing methods for the detection of Alzheimer's disease from MRI images make use of only certain selective subsets of data based on age, gender etc., and often rely on clinical data to aid in their classification. This paper proposes a Convolutional Neural Network (CNN) model for recognition and detection of Alzheimer's disease from MRI images, trained on the Open Access Series of Imaging Studies (OASIS) dataset. CNNs are the most popular deep learning architectures used for image related problems in recent times. In addition to that, CNNs are also robust for classification, which eliminates the need to ignore certain subsets of data, and solely focus on the image data. The proposed model achieves an accuracy of 80% and can be expected to achieve even higher accuracy with a substantial increase in the amount of data provided for training. We have incorporated Keras library in the python environment for building proposed CNN.																	1872-4981	1875-8843					2019	13	4					495	505		10.3233/IDT-190005													
J								A unified model of video-based human action categorization using Chaotic Quantum Swarm Intelligence on Intuitionistic fuzzy 3D Convolution Neural Network	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Human action categorization; deep learning; intuitionistic fuzzy; Chaotic Quantum Swarm intelligence; 3D Convolutional; Neural Network	RECOGNITION	In the contemporary surveillance schemes of Computer Vision, videos concerning human action categorization have become a predominant zone, involving Pattern Recognition tasks. Factually, most of the human actions comprise complex temporal information, and it is quite difficult to discover the diverse activities of humans precisely, in an unpredictable variety of environmental circumstances. A Deep Learning paradigm can tackle this issue, by providing additional capabilities to vision-based human action recognition. However, there are more complex challenges in extracting the spatio-temporal features, for instance, the presence of noise in videos and the highly vague feature points. This paper proposes a hybrid intelligent Intuitionistic Fuzzy 3D Convolution Neural Network that uses Chaotic Quantum Swarm Intelligence (CQSI-IFCNN), to optimize video-based human action categorization. Vagueness and ambiguity of input video frames are inherited by Intuitionistic Fuzzy networks in terms of membership, hesitation and non-membership components. By applying Chaotic Quantum Swarm Intelligence (CQSI), the learning parameters and error rates that occur in standard convolutional neural network are considerably reduced. The chaotic searching scheme is applied to overcome premature local optima in Quantum Swarm Intelligence. Therefore, this model produces optimized outcomes in Intuitionistic fuzzy 3D Convolutional Neural Networks, thus improving the categorization of human actions in videos. The Performance of CQSI-IFCNN is assessed by using the KTH and UCF Sports Action datasets. From the simulation outcomes, it is observed that CQSI-IFCNN has attained a higher rate of action categorization accuracy than standard CNN and PSO-CNN.																	1872-4981	1875-8843					2019	13	4					507	521		10.3233/IDT-190006													
J								An intuitionistic based novel approach to highest response ratio next CPU scheduler	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										CPU schedulers; intuitionistic fuzzy set theory; highest response ratio next (HRRN) scheduling; intuitionistic fuzzy based HRRN scheduler		The extension of CPU schedulers with fuzzy logic has been ascertained better because of its capability of handling imprecise information. Though, other generalized forms of fuzzy logic can be used which can further extend the performance of the scheduler. This paper introduces a novel approach for Highest Response Ratio Next (HRRN) CPU scheduler which uses the concept of intuitionistic fuzzy set theory. The proposed scheduler has the capability to dynamically deal with the impreciseness of the response ratio and to make the system adaptive based on the continuous feedback. An intuitionistic fuzzy inference system has been implemented. In this paper, to validate the performance of the scheduler a simulation environment has been implemented and the performance is compared with the other three baseline schedulers (Conventional HRRN scheduler, Fuzzy based HRRN scheduler and Vague based HRRN scheduler). Simulation results prove the effectiveness and efficiency of intuitionistic fuzzy based HRRN scheduler.																	1872-4981	1875-8843					2019	13	4					523	536		10.3233/IDT-190007													
J								Prognostics of multiple failure modes in rotating machinery using a pattern-based classifier and cumulative incidence functions	JOURNAL OF INTELLIGENT MANUFACTURING										Failure prognostics; Multiple failure modes; Logical analysis of data; Machine learning; CBM; Survival analysis; Rotating machinery	USEFUL LIFE ESTIMATION; LOGICAL ANALYSIS; COMPETING RISKS; RELIABILITY-ANALYSIS; SYSTEMS SUBJECT; FAULT-DIAGNOSIS; PREDICTION; STATE; DEGRADATION; ALGORITHM	This paper presents a novel methodology for multiple failure modes prognostics in rotating machinery. The methodology merges a machine learning and pattern recognition approach, called logical analysis of data (LAD), with non-parametric cumulative incidence functions (CIFs). It considers the condition monitoring data collected from a system that experiences several competing failure modes over its life span. LAD is used as a non-statistical classification technique to detect the actual state of the system, based on the condition monitoring data. The CIF provides an estimate for the marginal probability of each failure mode in the presence of the other competing failure modes. Accordingly, the assumption of independence between the failure modes, which is essential in many prognostic methods, is irrelevant in this paper. The proposed methodology is validated using vibration data collected from bearing test rigs. The obtained results are compared to those of two common machine learning prediction techniques: the artificial neural network and support vector regression. The comparison shows that the proposed methodology has a stable performance and can predict the remaining useful life of an individual system accurately, in the presence of multiple failure modes.																	0956-5515	1572-8145				JAN	2019	30	1					255	274		10.1007/s10845-016-1244-8													
J								Dynamic Knowledge Update Using Three-Way Decisions in Dominance-Based Rough Sets Approach While the Object Set Varies	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Dominance-based rough sets approach (DRSA); Three-way decisions (3wds); Approximations; Dynamic maintenance; Vector inner product	INCREMENTAL APPROACH; APPROXIMATIONS; SYSTEMS; MAINTENANCE; DISCOVERY	Dominance-based rough set approach is the extension of classical Pawlak rough set theories and methodologies, in which the information with preference-ordered relation on the domain of attribute value is fully considered. In the dominance-based information system, upper and lower approximations will be changed while the object set varies over time and the approximations need to be updated correspondingly for their variations result in changes of knowledge and rules. Considering that three-way decisions is a special class of general and effective human ways of problem solving and information processing, a new incremental maintenance mechanism using three-way decisions is proposed in this paper, namely, the universe is divided into three pair-wise disjoint subsets firstly, then appropriate strategies are developed and acted on each subsets. Furthermore, the corresponding methods for updating the approximations of upward unions and downward unions of decision classes are analyzed systematically under the variations of object set in the dominance-based information system from the perspective of three-way decisions. Moreover, considering vector representation and calculation is intuitive and concise, two incremental update algorithms of approximations are suggested and implemented in the MATLAB platform. Finally, some tests on data sets from UCI (UC Irvine Machine Learning Repository) are undertaken to verify the effectiveness of the proposed methods. Compared with the non-incremental updating methods, the proposed incremental updating method with three-way decisions generally exhibits a better performance. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					914	928		10.2991/ijcis.d.190807.001													
J								Distributed Synthetic Minority Oversampling Technique	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										SMOTE; apache spark; prediction; machine learning; imbalanced classification	IMBALANCED DATA; CLASSIFICATION; SMOTE	Real world problems for prediction usually try to predict rare occurrences. Application of standard classification algorithm is biased toward against these rare events, due to this data imbalance. Typical approaches to solve this data imbalance involve oversampling these "rare events" or under sampling the majority occurring events. Synthetic Minority Oversampling Technique is one technique that addresses this class imbalance effectively. However, the existing implementations of SMOTE fail when data grows and can't be stored on a single machine. In this paper present our solution to address the "big data challenge." We provide a distributed version of SMOTE by using scalable k-means++ and M-Trees. With this implementation of SMOTE, we were able to oversample the "rare events" and achieve results which are better than the existing python version of SMOTE. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					929	936		10.2991/ijcis.d.190719.001													
J								Testing Brain-Computer Interfaces with Airplane Pilots under New Motor Imagery Tasks	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Brain computer interface; Motor imagery; Aircraft pilots	CLASSIFICATION; PERFORMANCE; SELECTION; FEATURES	The purpose of a brain-computer interface (BCI) is the recording of brain signals to translate them into commands. This work proposes new naturalistic and intuitive motor imagery (MI) BCI task for aircraft Pilots for a BCI System, and explore if they take advantage of their previous motor experience in MI-BCI experiments. Results show that Pilots get better performance than a control group and that it is possible to get good classification accuracy results with new naturalistic MI tasks. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					937	946		10.2991/ijcis.d.190806.001													
J								Effective Channel Order Determination Algorithm for Convolutive Blind Channel Identification	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Effective channel order determination; Numerical analysis arguments; Blind channel identification; Higher-order cumulant tensor	TENSOR DECOMPOSITIONS; SOURCE SEPARATION; SIGNALS; MATRIX; NUMBER; FACTORIZATION; CRITERIA	Effective channel order determination is an important problem in convolutive blind channel identification. The classical techniques are based on information theoretic criteria, which show a great potentiality to estimate the effective channel order. However, these methods are just effective for the overdetermined case, i.e., the number of sensors is larger than the number of source signals. When the number of sensors is less than or equal to the number of source signals, i.e., in the underdetermined or determined case, it is difficult to detect the effective channel order. In this paper, an improved algorithm is proposed to estimate the effective channel order by integrating numerical analysis arguments and higher-order cumulant tensor. In the proposed algorithm, we exploit the information contained in the higher-order data statistics and rearrange the tensor as a matrix using the unfolding operation, then utilize the eigenvalues of the matrix and combine numerical analysis arguments to detect the effective channel order. Finally, a series of experiment results demonstrate the effectiveness and superiority of the proposed algorithm. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					947	954		10.2991/ijcis.d.190819.001													
J								A Novel Distance Measure for Pythagorean Fuzzy Sets and its Applications to the Technique for Order Preference by Similarity to Ideal Solutions	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Pythagorean fuzzy set; Multicriteria decision-making (MCDM); Distance measure; Technique for order preference by similarity to ideal solutions (TOPSIS)	POWER AGGREGATION OPERATORS; CRITERIA DECISION-ANALYSIS; DISSIMILARITY FUNCTIONS; TODIM APPROACH; TOPSIS	Ever since the introduction of Pythagorean fuzzy (PF) sets, many scholars have focused on solving multicriteria decision-making (MCDM) problems with PF information. The technique for order preference by similarity to ideal solutions (TOPSIS) is a well-known and effective method for MCDM problems. The objective of this study is to extend the TOPSIS to tackle MCDM problems under the PF environment. In this study, we develop a novel distance measure that considers the length, the angle, and the greater space, which reflect the properties of PF sets. Then, we apply the proposed distance measure in PF-TOPSIS to calculate the distances from the PF positive ideal solution and the PF negative ideal solution. Finally, we take the evaluation of emerging technology commercialization as an MCDM problem to illustrate the proposed approaches, and we then compare these approaches to demonstrate the scalar type PF-TOPSIS is the most feasible and effective approach in practice. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					955	969		10.2991/ijcis.d.190820.001													
J								Fuzzy-Based Language Grounding of Geographical References: From Writers to Readers	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Natural language generation; Linguistic descriptions of data; Data-to-text; Geo-referenced data; Language grounding; Fuzzy sets	LINGUISTIC DESCRIPTIONS; AUTOMATIC-GENERATION; CARDINALITY; SYSTEM	We describe an applied methodology to build fuzzy models of geographical expressions, which are meant to be used for natural language generation purposes. Our approach encompasses a language grounding task within the development of an actual datato-text system for the generation of textual descriptions of live weather data. For this, we gathered data from meteorologists through a survey and built consistent fuzzy models that aggregate the interpersonal variations found among the experts. A subset of the models was utilized in an illustrative use case, where we generated linguistic descriptions of weather maps for specific geographical expressions. These were used in a task-based evaluation to determine how well potential readers are able to identify the geographical expressions grounded on the models. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					970	983		10.2991/ijcis.d.190826.002													
J								Heterogeneous Interrelationships among Attributes in Multi-Attribute Decision-Making: An Empirical Analysis	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Heterogeneous interrelationships; Generalized extended Bonferroni mean; Simple additive weighting; Preference modeling; Multi-attribute decision function	OWA OPERATOR; AGGREGATION OPERATORS; CONSTRUCTION; WEIGHTS; SELECTION; CRITERIA; MODEL	Tremendous effort has been exerted over the past few decades to construct multi-attribute decision functions with the capacity to model heterogeneous interrelationships among attributes. In this paper, we report an empirical study aiming to test whether or not considering interrelationships among attributes can benefit the representation of real preferences in multi-attribute ranking tasks. The generalized extended Bonferroni mean (GEBM) has recently been advocated as a promising and efficient tool for modeling heterogeneous interrelationships among attributes. We compare the GEBM with one of its most widely adopted competitors, simple additive weighting (SAW), in terms of their fitting quality when applied to preference elicitation. The attribute performances are manifested uniformly with the use of three widely-adopted utility measurements. Subsequently afterwards, the maximum split approach to establish the constraint objective function in regression for both the GEBM and the SAW to test whether or not all constraints resulting from the subject's ranking can be fulfilled. On this bases, the number of fully or partly fitted subjects, consistency for subjects according to the better fitting model, and reliability of attribute weights learned by either the GEBM or the SAW are empirically examined in a bid to demonstrate the quantitative construction of fitting quality measurement. With the established fitting quality measurement, the necessity of taking heterogeneous interrelationships among attributes into account when constructing multi-attribute decision functions to represent real preferences can be analyzed. The main conclusion from the empirical study suggests that the relative performance of the two aggregation paradigms examined here depends on which fitting quality measurements are adopted. Researchers enthusiastic to discover the heterogeneous interrelationships among attributes when constructing multi-attribute decision functions might find the present results relevant when modeling actual preferences, and consequently this work should serve as a useful reference for enterprises and service providers seeking to strategically drive customer purchasing decisions. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					984	997		10.2991/ijcis.d.190827.001													
J								Constructing Novel Operational Laws and Information Measures for Proportional Hesitant Fuzzy Linguistic Term Sets with Extension to PHFL-VIKOR for Group Decision Making	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Proportional hesitant fuzzy linguistic term set (PHFLTS); Comprehensive weighting model; Proportional hesitant fuzzy linguistic cosine similarity measure; Proportional hesitant fuzzy linguistic entropy measure; Multi-attribute group decision making (MAGDM); Extended PHFL-VKIOR method	AGGREGATION; SELECTION; DISTRIBUTIONS; UNCERTAINTY; OPERATORS; PROJECTS; MODELS	To obtain reliable results in a qualitative multi-attribute group decision-making (MAGDM) problem, how to retain the evaluation information as much as possible and how to determine the reasonable weights of experts and attributes are two important issues. Proportional hesitant fuzzy linguistic term set (PHFLTS) is beneficial for retaining evaluation information as it would consider the linguistic terms and corresponding proportional information simultaneously. However, PHFLTS is a relatively new concept. Some novel manipulations, such as comparison, arithmetic operations, aggregation operators, and cosine similarity and distance measures are defined in this study with the purpose of improving the completeness and applicability of PHFLTS. Furthermore, cosine similarity measure-based weight determination model and entropy measure-based weight determination model under proportional hesitant fuzzy linguistic (PHFL) environment are constructed to derive the objective weights of experts and those of attributes as well. Subsequently, an integrated weighting model is proposed to determine the comprehensive weights of experts and attributes. Based on the defined operational laws for PHFLTS and comprehensive weighting model, two MAGDM methods, PHFL aggregation operator-based method and extended PHFL-VIKOR method, are developed to deal with MAGDM problems with PHFL information. To demonstrate the applicability, efficiency, and advantages of the proposed MAGDM methods, an illustrative example and a comparison example are provided. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					998	1018		10.2991/ijcis.d.190902.001													
J								Expdf: Exploits Detection System Based on Machine-Learning	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Malware; Exploit; Pdf; Machine learning		Due to the seriousness of the network security situation, as a low-cost, high-efficiency email attack method, it is increasingly favored by attackers. Most of these attack vectors were embedded in email attachments and exploit vulnerabilities in Adobe and Office software. Among these attack samples, PDF-based exploit samples are the main ones. In this paper, we proposed Expdf, different from existing research on detecting pdf malware, a robust recognition system for exploitable code-based machine learning. We demonstrate the effectiveness of Expdf on the dataset collected from Virus Total filtered by the labels of multiple antivirus software. With the experimental evaluation compared to Hidost, Expdf demonstrates its superiority in detecting exploits, reaching the accuracy rate of 95.54% and the recall rate of 97.54%. Additionally, as the supplementary experiment, Expdf could identify specific exploit vulnerability types. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1019	1028		10.2991/ijcis.d.190905.001													
J								Revisiting the Role of Hesitant Multiplicative Preference Relations in Group Decision Making With Novel Consistency Improving and Consensus Reaching Processes	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Group decision making; Hesitant multiplicative preference relation; Consistency improving process; Consensus reaching process; Mathematical programming model	AGGREGATION OPERATORS; ADDITIVE CONSISTENCY; PRIORITY WEIGHTS; FUZZY; FRAMEWORK; MODEL; PRIORITIZATION; INFORMATION; TAXONOMY; GDM	In recent years, hesitant multiplicative preference relation (HMPR) has been a powerful means to represent the evaluation information of decision makers during the pairwise comparison concerning alternatives. As the important parts of group decision making (GDM) issues with HMPRs, the consistency improving and consensus reaching processes have been researched by many scholars; however, the existing approaches present several limitations, including defining the consistency index depend on the other HMPR instead of the original HMPR, improving the consistency and consensus levels of HMPRs independently, and that the high computational complexity of the existing iterative algorithms. To overcome these drawbacks, this paper proposes a mathematical programming model to improve the consistency and consensus levels of HMPRs, simultaneously. First, a consistency index according to multiplicative consistency is put forward to compute the consistency degree of normalized HMPR (NHMPR) after the normalization procedure. Second, a programming model by minimizing the difference between the original NHMPR and the revised NHMPR is developed to obtain the acceptably consistent NHMPR; then, a consistency-based method is constructed to solve the decision making problems with an HMPR. Third, considering the GDM issues, a consistency- and consensus-based programming model is established to obtain the acceptably consistent and consensus NHMPRs, in which the original evaluation information can remain as much as possible. Fourth, the normalized hesitant multiplicative weighted geometric operator is introduced to fuse the revised NHMPRs and an algorithm for GDM is proposed with novel consistency improving and consensus reaching processes. Finally, two numerical examples are applied to show the practicality and advantages of the proposed approaches. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1029	1046		10.2991/ijcis.d.190823.001													
J								A Novel Hybrid Autoregressive Integrated Moving Average and Artificial Neural Network Model for Cassava Export Forecasting	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Time series forecasting; Hybrid model; Artificial neural network; ARIMA; Cassava export	SUPPORT VECTOR MACHINES; TIME-SERIES; ARIMA	This paper proposes a novel hybrid forecasting model combining autoregressive integrated moving average (ARIMA) and artificial neural network (ANN) with incorporating moving average and the annual seasonal index for Thailand's cassava export (i.e., native starch, modified starch, and sago). The comprehensive experiments are conducted to investigate the appropriate parameters of the proposed model as well as other forecasting models compared. In particular, the proposed model is experimentally compared to the ARIMA, the ANN and the other hybrid models according to three popular prediction accuracy measures, namely mean square error (MSE), mean absolute error (MAE) and mean absolute percentage error (MAPE). The empirical results show that the proposed model gives the lowest error in all three measures for the native starch and the modified starch which are major cassava exported products (98% of the total export volume). However, the Khashei and Bijari's model is the best model for the sago (2% of the total export volume). Therefore, the proposed model can be used as an alternative forecasting method for stakeholders making a decision in cassava international trading to obtain better accuracy in predicting future export of native starch and modified starch which are the majority of the total export. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1047	1061		10.2991/ijcis.d.190909.001													
J								Soft Sensor Modeling Method by Maximizing Output-Related Variable Characteristics Based on a Stacked Autoencoder and Maximal Information Coefficients	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Soft sensor; Deep learning; Stacked autoencoder (SAE); Maximal information coefficient (MIC); Modeling method	PREDICTION; RECOGNITION	The key factors required to establish a precise soft sensor model for industrial processes include selection of variables affecting vital indicators from a large number of online measurement variables and elimination of the effects of unrelated disturbance variables. How to compress redundant information and retain the unique characteristic information contained by the selected variables is worthy of in-depth research. A novel soft sensor modeling method based on weighted maximal information coefficients (MICs) and a stacked autoencoder (SAE), hereinafter referred to as MICW-SAE, is proposed in this work. In our model, the MICs between each input and output variable are calculated and compared with the threshold before training each network in SAE. Then, input variables with low MICs are selected, and the average MIC index is calculated using other input variables. If the index is higher than the second threshold, the MIC of this specific variable is set to 0. Finally, the weights of all input variables are determined in accordance with the scale and placed into the loss function for training. The Boston house-price and naphtha dry point temperature datasets are used to prove the prediction ability of our model. Results demonstrate that MICW-SAE can enhance the output-related features of the input variables. Moreover, redundant information that can also be represented by other input variables are identified and excluded. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1062	1074		10.2991/ijcis.d.190826.001													
J								Using Fuzzy Sets in Surgical Treatment Selection and Homogenizing Stratification of Patients with Significant Chronic Ischemic Mitral Regurgitation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Conditional degrees of membership; Fuzzy samples; Product t-norm; Bayesian classifier; Independent continuous features	CARDIOMYOPATHY; IMPACT; VALVE	We present three (main one and two auxiliary) fuzzy algorithms to stratify observations in homogenous classes. These algorithms modify, upgrade and fuzzify crisp algorithms from our earlier works on a medical case study to select the most appropriate surgical treatment for patients with ischemic heart disease complicated with significant chronic ischemic mitral regurgitation. Those patients can be treated with either surgical revascularization and mitral valve repair (group A) or with isolated surgical revascularization (group B) depending on their health status. The main algorithm results in a fuzzy partition of patients in two fuzzy sets (groups A and B) through identification of their degrees of membership. The resulting groups are highly non-homogenous, which impedes subsequent proper comparisons. So, the two auxiliary algorithms further stratify each group into two homogenous subgroups with comparatively preserved medical condition (A(1) and B-1) and with comparatively deteriorated medical condition (A(2) and B-2). Those two algorithms perform fuzzy partition of patients from A and B respectively into A(1), A(2), B-1 and B-2 by identifying their conditional degrees of membership to those subgroups. We then utilize the product t-norm to calculate the degree of membership of patients to their respective subgroup as an intersection of two fuzzy sets. We demonstrate how to form fuzzy samples for medical parameters for any subgroup. We also compare the performance of the fuzzy algorithms with their preceding crisp version, as well as with eight Bayesian classifiers. We then assess the quality of classification by modified confusion matrices, summarized further into four criteria. The fuzzy algorithms show total superiority over the other methods, and excellent differentiation of typical patients and outliers. On top, only the fuzzy algorithms provide a measure of how typical a patient is to its subgroup. The fuzzy algorithms clearly outline the role of the Heart Team, which is missing in the Bayesian classifiers. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1075	1090		10.2991/ijcis.d.190923.002													
J								Green Supplier Selection Based on Dombi Prioritized Bonferroni Mean Operator with Single-Valued Triangular Neutrosophic Sets	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Single-valued triangular Neutrosophic sets; Dombi operations; Prioritized operator; Bonferroni mean operator; Dombi prioritized normalized Bonferroni mean operator	DECISION-MAKING METHOD; AGGREGATION OPERATORS; FUZZY	The choice of green suppliers involves a large amount of inaccurate, incomplete, and inconsistent information, and the single-valued triangular Neutrosophic number that is an extension of the single-valued Neutrosophic number can effectively handle such problems. Considering the advantages of the single-valued triangular Neutrosophic number, this paper proposes a new aggregate operator to solve the problem of multi-criteria decision making. The new aggregate operator takes into account the priority relationship and the interrelationship between the criteria. To make the new aggregate operator more flexible, this paper introduces the Dombi operations. This paper combines the Dombi operations with the prioritized average operator and the Bonferroni mean operator to propose the single-valued triangular Neutrosophic Dombi prioritized normalized Bonferroni mean (SVTNDPNBM) operator. Finally, the SVTNDPNBM operator is applied to the problem of the green supplier selection, which proves its feasibility and stability. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1091	1101		10.2991/ijcis.d.190923.001													
J								A Decision Making Approach with Linguistic Weight and Unavoidable Incomparable Ranking	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Linguistic lattice implication algebra; Linguistic-real valuation function; Incomparable ranking; Linguistic decision making	TYPE-2 FUZZY-SETS; AGGREGATION OPERATORS; MODEL; 2-TUPLES	In order to deal with the decision making problem including some linguistic values uncertainty information, we propose an approach for decision making with linguistic weighted and unavoidable incomparable ranking based on Linguistic-valued lattice implication algebra (LV-LIA). The properties of binary operations. and. are discussed in LV-LIA, and used to handle importance degree of the attributes expressed by linguistic values. We define a linguistic-real valuation function which is a positive valuation function and a linguistic-real metric distance implied by the linguistic-real valuation function is introduced, to process incomparable linguistic values in the results which need further procedure to make a certain decision. Illustrating examples show the effectiveness of the proposed approach which can rank the incomparable elements elastic. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1102	1112		10.2991/ijcis.d.190923.003													
J								Reduce Cost Smart Power Management System by Utilize Single Board Computer Artificial Neural Networks for Smart Systems	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Management; Power saving; Smart power system; Raspberry PI 3; CNN		The plan and usage of a smart power management system for household and buildings that control numerous electrical appliances in real time have been reported in this work. The system is based on using artificial intelligence with low-cost single board computer in order to design a smart power management system that can analyzed some aspects that can serve power management aspects such as, electricity consumption to reduce power consumption to lower limits as possible, temperature to control, and human activity to control lighting and power on/off some devises like TV. A Raspberry PI 3 version B has been utilized as a computer unit, in a fast and accurate way to control, for example, switching lighting/TV when human in or left the area. The system utilized some devises in that purpose that includes, a Raspberry Pi camera to streamed real-time video for detection the existence of human and his activity, an ultrasonic sensor to compute distance of human in area, temperature sensor to detect room temperature in home or buildings in order to control air conditioning systems and odor/gas sensors to control ventilation systems, power sensor to compute electricity consumption. The proposed system is programmed by a used Python programming language that manages all aspect at the same time. The recognition part is based on utilized conversational neural network (CNN) that optimized by used saliency object detection so as to improve the CNN in acknowledgment exactness and acknowledgment speed. The outcomes endorsed that the proposed system can manage the power in smooth and accurate that can serve both electrical consumption and lifestyle where all operation run in fast and automated way, furthermore, the recognition algorithm success in detect objects and isolate it from background with 100% accuracy and in fast time reach to 0.7 seconds. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1113	1120		10.2991/ijcis.d.191001.001													
J								Rigorous Analysis of Multi-Factorial Evolutionary Algorithm as Multi-Population Evolution Model	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multi-factorial evolutionary algorithm; Multi-population evolution model; Multi-task optimization; Knowledge transfer; Across-population	MULTITASKING	Multi-task optimization algorithm is an emergent paradigm which solves multiple self-contained tasks simultaneously. It is thought that multi-factorial evolutionary algorithm (MFEA) can be seen as a novel multi-population algorithm, wherein each population is represented independently and evolved for the selected task only. However, the theoretical and experimental evidence to this conclusion is not very convincing and especially, the coincidence relation between MFEA and multi-population evolution model is ambiguous and inaccurate. This paper aims to make an in-depth analysis of this relationship, and to provide more theoretical and experimental evidence to support the idea. In this paper, we clarify several key issues unsettled to date, and design a novel across-population crossover approach to avoid population drift. Then MFEA and its variation are reviewed carefully in view of multi-population evolution model, and the coincidence relation between them are concluded. MFEA is completely recoded along with this idea and tested on 25 multi-task optimization problems. Experimental results illustrate its rationality and superiority. Furthermore, we analyze the contribution of each population to algorithm performance, which can help us design more efficient multi-population algorithm for multi-task optimization. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1121	1133		10.2991/ijcis.d.191004.001													
J								A Ship Target Location and Mask Generation Algorithms Base on Mask RCNN	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Mask RCNN; Mask; Region proposal network; Upsample; ROI align		Ship detection is a canonical problem in computer vision. Motivated by the observation that the major bottleneck of ship detection lies on the different scales of ship instances in images, we focus on improving the detection rate, especially for the small-sized ships which are relatively far from the camera. We use the Smooth function combined with L1 and L2 norm to optimize the region proposal network (RPN) loss function and reduce the deviation between the prediction frame and the actual target to ensure the accurate location of the ship target. With the Two-Way sampling combined with the shared weight to generate the mask, we solve the problems of inaccurate segmentation, target loss and small interference when Mask Region Convolution Neural Network (RCNN) is used to segment an instance. We create the experimental data sets from the deep learning annotation tool-Labelme. Experiments show that the improved Mask-RCNN model has a confidence rate of 82.17%. Serving as the basic network, the test accuracy rate of ResNetXt-101 is 3.3% higher than that of the original ResNet-101, which can better realize the function of ship target location and mask generation. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1134	1143		10.2991/ijcis.d.191008.001													
J								Efficient Time-Series Forecasting Using Neural Network and Opposition-Based Coral Reefs Optimization	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Meta-heuristics; Coral reefs optimization; Opposition-based learning; Neural networks; Time series forecasting; Nature-inspired algorithms; Distributed systems	GENETIC ALGORITHMS; PREDICTION; RECURRENT; MODEL	In this paper, a novel algorithm called opposition-based coral reefs optimization (OCRO) is introduced. The algorithm is built as an improvement for coral reefs optimization (CRO) using opposition-based learning (OBL). For efficient modeling as the main part of this work, a novel time series forecasting model called OCRO-multi-layer neural network (MLNN) is proposed to explore hidden relationships in the non-linear time series data. The model thus combines OCRO with MLNN for data processing, which enables reducing the model complexity by faster convergence than the traditional back-propagation algorithm. For validation of the proposed model, three real-world datasets are used, including Internet traffic collected from a private internet service provider (ISP) with distributed centers in 11 European cities, WorldCup 98 contains request numbers to the server in football world cup season in 1998, and Google cluster log dataset gathered from its data center. Through the carried out experiments, we demonstrated that with both univariate and multivariate data, the proposed prediction model gains good performance in accuracy, run time and model stability aspects as compared with other modern learning techniques like recurrent neural network (RNN) and long short-term memory (LSTM). In addition, with used real datasets, we intend to concentrate on applying OCRO-MLNN to distributed systems in order to enable the proactive resource allocation capability for e-infrastructures (e.g. clouds services, Internet of Things systems, or blockchain networks). (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1144	1161		10.2991/ijcis.d.190930.003													
J								Fuzzy Systems-as-a-Service in Cloud Computing	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Fuzzy system software; Cloud computing; Cloud platforms; Services deployment; Fuzzy model building		Fuzzy systems have become widely accepted and applied in a host of domains such as control, electronics or mechanics. The software for construction of these systems has traditionally been exploited from tools, platforms and languages run on-premise computing infrastructure. On the other hand, rise and ubiquity of the cloud computing model has brought a revolutionary way for computing services deployment. The boost of cloud services is leading towards increasingly specific service offering just as data mining and machine learning service. Unfortunately, so far, no definition for fuzzy system as service is available. This paper identifies this opportunity and focus on developing a proposal for fuzzy system-as-a-service definition. To achieve this, the proposal pursues three objectives: the complete description of cloud services for fuzzy systems using semantic technology, the composition of services and the exploitation of the model in cloud platforms for integration with other services. As an illustrative case, a real-world problem is addressed with the proposed specification. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1162	1172		10.2991/ijcis.d.190912.001													
J								Fuzzy-Based Energy Management System With Decision Tree Algorithm for Power Security System	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Energy security; Power security service; Incursion detection system; Artificial neural network		Energy security (ES) has great impact on power grids. Therefore it is important to have power security service (PSS). The PSS should be designed to handle interference and interruption attack in the grid. The interference and interruption attack in the grid is handled by incursion-detection system (IDS). The IDS is the most obtainable strategy to sense and classify different security issues and abnormal issues that occur in power grids. Therefore the IDS have to be up to date with the latest issues that happened in the entire grid. The proposed concept illustrate the collection of data with reference to intrude-attacks in power-grid and to test and evaluate different machine-learning strategy to illustrate the different attacks and issues that happen in power system grid. The research work is illustrated with a modified J48 decision tree algorithm. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1173	1178		10.2991/ijcis.d.191016.001													
J								Aggregating Interrelated Attributes in Multi-Attribute Decision-Making With ELICIT Information Based on Bonferroni Mean and Its Variants	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										ELICIT information; Aggregation operator; Interrelationship; Bonferroni mean	LINGUISTIC TERM SETS; FUZZY NUMBERS; REPRESENTATION MODEL; OPERATORS; 2-TUPLE	In recent times, to improve the interpretability and accuracy of computing with words processes, a rich linguistic representation model has been developed and referred to as Extended Comparative Linguistic Expressions with Symbolic Translation (ELICIT). This model extends the definition of the comparative linguistic expressions into a continuous domain due to the use of the symbolic translation concept related to the 2-tuple linguistic model. The aggregation of ELICIT information via a suitable rule that reflects the underlying interrelation among the aggregated information in output is the key tool to design decision-making algorithm for solving multi-attribute decision-making problems under linguistic information. In this study, we introduce three aggregation operators for aggregating ELICIT information in aim of capturing three different types of interrelationship patterns among inputs, which we refer to as ELICIT Bonferroni mean, ELICIT extended Bonferroni mean and ELICIT partitioned Bonferroni mean. Further, the key aggregation properties of these proposed operators are investigated with the proposal of weighted forms. Based on the proposed aggregation operators, an approach for solving multi-attribute decision-making problems, in which attributes are interrelated is developed. Finally, a didactic example is presented to illustrate the working of the proposal and demonstrate its feasibility. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1179	1196		10.2991/ijcis.d.190930.002													
J								Adaptive Fuzzy Mediation for Multimodal Control of Mobile Robots in Navigation-Based Tasks	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Fuzzy mediation; Adaptive control; Mobile robot; Neural networks; Teleoperation	NEURAL-NETWORKS; DEEP; VISION	The paper proposes and analyses performance of a fuzzy-based mediator with showcase examples in robot navigation. The mediator receives outputs from two controllers and uses estimated collision probability for adapting the signal proportions in the final output. The approach was implemented and tested in simulation and on real robots with different footprints. The task complexity during testing varied from single obstacle avoidance to a realistic navigation in real environments. The obtained results showed that this approach is simple but effective. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1197	1211		10.2991/ijcis.d.190930.001													
J								WiBiA: Wireless Sensor Networks Based on Biomimicry Algorithms	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Biomimicry; ACO algorithm; Wireless sensor networks; Pheromone	ANT COLONY OPTIMIZATION	The goal of biomimicry is to resolve problems by studying and mimicking the characteristics of organisms or design elements in nature. Although wireless sensor networks are used in various fields, they have limited network lifespan. Research has thus focused on observing and modeling the behavioral principles of various organisms to use in biomimicry algorithms for efficient routing techniques in large-scale networks. In this study, we examine the pheromones used in ant communication, and accordingly design techniques for energy-efficient traffic distribution implemented on a network. We designed a biomimicry technology called the wireless sensor networks based on biomimicry algorithms or WiBiA. By analyzing and applying similarities between communication and biological systems, the performance of our system improved in terms of network lifespan and optimized path selection. In simulations, the proposed routing algorithm yielded short information-collection time and lowenergy consumption, and thus maximized the energy efficiency of the network. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1212	1220		10.2991/ijcis.d.191029.001													
J								Composable Instructions and Prospection Guided Visuomotor Control for Robotic Manipulation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Composable instructions; Motion generation; Prospection; Imitation learning; Visuomotor control; Robotic manipulation		Deep neural network-based end-to-end visuomotor control for robotic manipulation is becoming a hot issue of robotics field recently. One-hot vector is often used for multi-task situation in this framework. However, it is inflexible using one-hot vector to describe multiple tasks and transmit intentions of humans. This paper proposes a framework by combining composable instructions with visuomotor control for multi-task problems. The framework mainly consists of two modules: variational autoencoder (VAE) networks and long short-term memory (LSTM) networks. Perception information of the environment is encoded by VAE into a small latent space. The embedded perception information and composable instructions are combined by the LSTM module to guide robotic motion based on different intentions. Prospection is also used to learn the purposes of instructions, which means not only predicting the next action but also predicting a sequence of future actions at the same time. To evaluate this framework, a series of experiments are conducted in pick-and-place application scenarios. For new tasks, the framework could obtain a success rate of 91.2%, which means it has a good generalization ability. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1221	1231		10.2991/ijcis.d.191017.001													
J								Face Inpainting with Deep Generative Models	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Face inpainting; Structural loss; Semantic inpainting; Deep generative models; GANs		Semantic face inpainting from corrupted images is a challenging problem in computer vision and has many practical applications. Different from well-studied nature image inpainting, the face inpainting task often needs to fill pixels semantically into a missing region based on the available visual data. In this paper, we propose a new face inpainting algorithm based on deep generative models, which increases the structural loss constraint in the image generation model to ensure that the generated image has a structure as similar as possible to the face image to be repaired. At the same time, different weights are calculated in the corrupted image to enforce edge consistency at the repair boundary. Experiments on different face data sets and qualitative and quantitative analyses demonstrate that our algorithm is capable of generating visually pleasing face completions. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1232	1244		10.2991/ijcis.d.191016.003													
J								A Contradiction Separation Dynamic Deduction Algorithm Based on Optimized Proof Search	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Automated theorem proving; Binary resolution; Multi-clause inference rule; S-CS rule; Heuristic strategy	PROVING SYSTEM COMPETITION; RESOLUTION	Most of the advanced first-order logic automated theorem proving (ATP) systems adopt binary resolution methods as the core inference mechanism, where only two clauses are involved and a complementary pair of literals are eliminated during each deduction step. Recently, a novel multi-clause inference rule is introduced along with its soundness and completeness, which is called as standard contradiction separation rule (in short, S-CS rule) and allows multiple (two or more) clauses to be involved in each deduction step. This paper introduces and evaluates the application of S-CS rule in first-order logic ATP. Firstly, it analyzes several deduction methods of S-CS rule. It is then focused on how this multi-clause deduction theory can be achieved through forming a specific and effective algorithm, and finally how it can be applied in the top ATP systems in order to improve their performances. Concretely, two novel multi-clause S-CS dynamic deduction algorithms are proposed based on optimized proof search, including related heuristic strategy, then the application method applied in the state of the art ATP system Eprover (the version of Eprover 2.3) is introduced. Eprover with the proposed multi-clause deduction algorithms are evaluated through the FOF division of the CASC-J9 (in 2018) ATP system competition. Experimental results show that Eprover with the proposed multi-clause deduction algorithms outperform the plain Eprover itself to a certain extent. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1245	1254		10.2991/ijcis.d.191022.002													
J								A Bargaining Solution With Level Structure	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Game theory; Level structure; Bargaining solution; Bankruptcy problem		Presently, a conventional coalition structure can no more cover all the types of cooperative structures in practice, external cooperation between the coalitions also affects the payoff allocation between the participants. We propose a solution to solve the bargaining problem with level structure by defining for each coalition and each level. The solution concentrates on the bargaining of the coalitions with each other at each level. Furthermore, we discuss the applications of this solution to bankruptcy games with level structure. The proposed solution generalizes the bargaining solution with conventional coalition structure, which can be utilized in more cooperative types with level structures. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1255	1260		10.2991/ijcis.d.191016.002													
J								Hybrid Dragonfly Optimization-Based Artificial Neural Network for the Recognition of Epilepsy	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Electroencephalography Kalman filter; Variable mode decomposition; Modified principal component analysis; Artificial neural network; Hybrid dragonfly algorithm	SEIZURE DETECTION; WAVELET TRANSFORM	Epilepsy can well be stated as a disorder of the central nervous systems (CNS) that brought about recurring seizures owing to chronic abnormal blasts of electrical discharge on the brain. Knowing if an individual is having a seizure and diagnosing the seizure type or epilepsy syndrome could be hard. Many methods were developed to recognize this disease. But the existing techniques for detection of epilepsy are not satisfied with accuracy, and cannot identify the diseases effectively. To trounce these drawbacks, this paper proposes an approach for the recognition of Epilepsy as of the electroencephalography (EEG) signals. This is implemented as follows. Primarily, the Kalman filter (KF) is utilized for pre-processing to eradicate the impulse noise present in the EEG signals. This filtered signal is then decomposed utilizing variable modes decomposition (VMD). Feature extraction (FE) is performed by computing 7 features. The dimensionality of this signal is then lessened using Modified-Principal Components Analysis (M-PCA). Finally, classification is conducted utilizing the artificial neural networks (ANN) that is optimized using the hybrid dragonfly algorithm (HDA). Disparate performance metrics such as sensitivity, accuracy, and false discovery rates (FDR) are ascertained and as well weighted against with the existent works. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1261	1269		10.2991/ijcis.d.191022.001													
J								A Hybrid Global Optimization Algorithm Based on Particle Swarm Optimization and Gaussian Process	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Swarm optimization; Gaussian process; Global optimization; Surrogate approach	APPROXIMATION; DESIGN; COMPUTATION; MODEL; PSO	The optimization problems and algorithms are the basics subfield in artificial intelligence, which is booming in the almost any industrial field. However, the computational cost is always the issue which hinders its applicability. This paper proposes a novel hybrid optimization algorithm for solving expensive optimizing problems, which is based on particle swarm optimization (PSO) combined with Gaussian process (GP). In this algorithm, the GP is used as an inexpensive fitness function surrogate and a powerful tool to predict the global optimum solution for accelerating the local search of PSO. In order to improve the predictive capacity of GP, the training datasets are dynamically updated through sorting and replacing the worst fitness function solution with the better solution during the iterative process. A numerical study is carried out using twelve different benchmark functions with 10, 20 and 30 dimensions, respectively. Regarding solving of the ill-conditioned computationally expensive optimization problems, results show that the proposed algorithm is much more efficient and suitable than the standard PSO alone. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1270	1281		10.2991/ijcis.d.191101.004													
J								Artificial Neural Network to Model Managerial Timing Decision: Non-linear Evidence of Deviation from Target Leverage	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Capital structure; Market timing; Multiple regression model; Artificial neural network	OPTIMAL CAPITAL STRUCTURE; FIRM PERFORMANCE; EQUITY; ADJUSTMENT; DEBT; DETERMINANTS; SPEED; VALUATION; IMPACT; HETEROGENEITY	The current study highlights the utilization of a non-linear model to analyze an important decision-making process in the study of corporate finance where managers are deciding on the capital structure of a firm. This study compares the results from based on the unbalanced panel data multiple regression for firm fixed effects relative to the artificial neural networks, i.e., ANN, with known determinants of capital structure as control variables for a sample of UK firms respectively. Results of the study show that firms are timing away from target levels which challenges the current findings in the literature. The ANN model achieves a better fit based on the root of mean-squared error (RMSE) values which provides a more accurate forecast. Thus, the nature of balancing between cost of being off-target versus benefits gained from timing the equity market is non-linear and which is captured by ANN. Implications from the study allow market players to understand the process of achieving optimal capital structure to maximize firm value and thus benefit all stakeholders. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1282	1294		10.2991/ijcis.d.191101.002													
J								Economic Impact of Artificial Intelligence: New Look for the Macroeconomic Assessment in Asia-Pacific Region	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Artificial intelligence; Deep learning; Analysis	EMPLOYMENT	Objective: To determine the impact of artificial intelligence (AI) on the selected economies in the Asia-Pacific region. Methods: This secondary research collected data from macroeconomic and AI-specific data sets. The sources of data from which insights were gained included digital technology sectors and corporations and their functions. The focus was on the need to assess the capability of AI on business operations The macroeconomic data was collected from data resources of international organizations' including the World Economic Forum, the Organization for Economic Co-operation and Development (OECD), the World Intellectual Property Organization (WIPO), and the International Telecommunication Union (ITU). In addition, this study has considered 19 economic indicators to analyze the economic outcome of AI in selected economies of Asia-Pacific. Results: From the results, the period between 2014 and 2016 witnessed China leading with over 25,000 citable documents on the AI topic. Regarding institutions that were observed to publish over 500 times on the AI topic, the countries in the ascending order include China (600), Hong Kong (1,100), and Singapore (2,000). As such, this study established that Asia-Pacific economies such as Hong Kong and Singapore though have smaller populations, but the majority of their higher education institutions have made a significant contribution to AI research; with the small economies also having a relatively higher number of computer scientists among the top 1,000 individuals. Additionally, through empirically analyses, during 1998-2016 with annual observations, it is found that various economic outcomes of AI were presented in 8 economies of targeted region. Limitations: At first, the future outlook of AI is just discussed in conceptual meaning while empirical context still needs to be examined in upcoming studies. At second, covering the overall South Asian region provides better findings with more generalization which is missing in current research. At third, other dimensions of AI and economy like implication of AI impact index and its relationship with macroeconomic variables is also missing in current research which could be reconsidered in coming studies. Conclusion: It is evident that AI exhibits the potential to be the main driver of Asia-Pacific's economic growth. Relative to the net and gross effect of AI on labor markets and the gross domestic product (GDP) of the top Asia-Pacific economies demonstrate that by 2030, AI might yield a 16-percent increase in output, translating into an estimated amount of $13 trillion. Overall, it is concluded that Asia-Pacific, when compared to developed regions such as North America, is lagging but the availability of a large pool of user data implies that the region can move ahead-given better resource and talent allocation. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1295	1310		10.2991/ijcis.d.191025.001													
J								Regional Input-Output Multiple Choice Goal Programming Model and Method for Industry Structure Optimization on Energy Conservation and GHG Emission Reduction in China	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Industrial restructuring; Energy conservation; Greenhouse gas emission reduction; Big data analysis; Input-output model	CARBON-DIOXIDE EMISSIONS; CO2 EMISSIONS; IMPACT	To assess the potential of China's industrial restructuring on energy conservation and greenhouse gas (GHG) emission reduction in 2020, this study proposes an input-output multi-choice goal programming model and method. In this model, the goals include the maximization of gross domestic product (GDP), minimization of energy consumption and GHG emission. They are subjected to the input-output balance, economy development, energy supply, and industry diversity. And four scenarios with different decision preferences are taken into accounted in the solutions of the industrial structure optimization model. The results demonstrate that industrial restructuring has potential in energy saving and emission reducing. First, after optimization, energy consumption intensity and GHG emission intensity can drop by 13.88246% and 5.33767% over 2012, and GDP can grow up at annual growth rate 6.6% from 2013-2020. Second, promoting the development of the low energy-intensive and low GHG emission intensive sectors is an effective method for energy conservation and emission reduction. Three, compared to energy intensity reduction, GHG emission intensity reduction is less effective for four scenarios. Four, there are several difficulties to achieve the amounts and intensity control targets of energy conservation and GHG emission reduction simultaneously. It is suggested that China had better strive to promote progress of technologies of energy conservation and GHG emission reduction while adjusting the industrial structure. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1311	1322		10.2991/ijcis.d.191104.002													
J								II-Learn-A Novel Metric for Measuring the Intelligence Increase and Evolution of Artificial Learning Systems	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Machine learning; Machine intelligence; Intelligent system; Evolving system; Cooperative multiagent system; Machine intelligence measure	KOLMOGOROV-SMIRNOV TEST; ANALYSIS FRAMEWORK; DECISION-SUPPORT; MANN-WHITNEY; T-TEST; DIAGNOSIS; AGENTS; OPTIMIZATION; ENHANCEMENT; COLONY	A novel accurate and robust metric called II-Learn for measuring the increase of intelligence of a system after a learning process is proposed. We define evolving learning systems, as systems that are able to make at least one measurable evolutionary step by learning. To prove the effectiveness of the metric we performed a case study, using a learning system. The universality of II-Learn is based on the fact that it does not depend on the architecture of the studied system. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1323	1338		10.2991/ijcis.d.191101.001													
J								On Consistency and Priority Weights for Uncertain 2-Tuple Linguistic Preference Relations	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Group decision-making; Uncertain 2-tuple linguistic preference relations; Additive consistency; Uncertain 2-tuple linguistic priority weights	GROUP DECISION-MAKING; AGGREGATION OPERATORS; EVALUATION MODEL; VIKOR METHOD; COMPATIBILITY; INFORMATION	Consistency and priority weights of preference relations are two important phases of decision-making process since the decision-making solutions are determined by them. Therefore, it is meaningful to investigate consistency and priority weights for preference relations. In this paper, consistency and uncertain 2-tuple linguistic priority weights of uncertain 2-tuple linguistic preference relations (U2TLPRs) are investigated. First, based on the additive consistency, an additive consistency index is developed to measure the additive consistency level of U2TLPRs. Second, a goal programming model is proposed to adjust the unacceptable additive consistent U2TLPR until it satisfies acceptable additive consistency. Furthermore, an optimization model is developed to derive the uncertain 2-tuple linguistic priority weights from an U2TLPR. Meanwhile, in group decision-making (GDM) problems, similarities and confidence degrees of decision makers (DMs) are defined to determine DMs' weights. Subsequently the properties of collective U2TLPR are discussed. Finally, the proposed methods are implemented in two examples including a GDM problem to verify the validity of the proposed methods. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1339	1352		10.2991/ijcis.d.191104.001													
J								A Biform Game Approach to Preventing Block Withholding Attack of Blockchain Based on Semi-CIS Value	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Blockchain; Mining pool; Block withholding attack; Biform game; Big data	INCENTIVES	In proof-of-work (PoW)-based blockchain network, the blockchain miners publish blocks by contributing computing power to solve crypto-puzzles. Due to the weak computing power of single miner, miners tend to join a mining pool and share the profits from the mining pool according to the contribution proportions of the miners. However, some miners may initiate block withholding attack which may result in wasting computing power, even threatening the efficiency of the blockchain network. To address this problem, in this paper, we use the biform game model to optimize the miners' strategy choices. We firstly formulate the mining process as a non-cooperative-cooperative biform game model. We use the model to exhibit miners' strategy choices (non-cooperation stage) and the cooperation mining process (cooperation stage). Then we set the conditions to maintain the voluntary honest behavior of miners. After that, we employ the semi-CIS (semi-the center of imputation set value) value to compute the solutions of the cooperative games in the cooperation stage, and optimize miners' strategy choices to prevent the block withholding attack. Hence we can ensure the blockchain network is secure. Finally, the validity and applicability of the proposed model and method are verified by a numerical example. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1353	1360		10.2991/ijcis.d.191030.001													
J								EDAS Method for Multiple Attribute Group Decision Making with Probabilistic Uncertain Linguistic Information and Its Application to Green Supplier Selection	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multiple attribute group decision making (MAGDM); Probabilistic uncertain linguistic term sets (PULTSs); Information entropy; EDAS method; Green supplier selection	PYTHAGOREAN FUZZY-SETS; HESITANT FUZZY; AGGREGATION OPERATORS; TERM SETS; REPRESENTATION MODEL; PREFERENCE RELATIONS; SIMILARITY MEASURES; CONSENSUS; APPRAISAL	In order to adapt to the development of the new times, enterprises should not only care for the economic benefits, but also properly cope with environmental and social problems to achieve the integration of environmental, economic and social performance of sustainable development, so as to maximize the efficiency of resource use and minimize the negative effects of environmental pollution. Hence, in order to select a proper green supplier, integration of the information entropy and Evaluation based on Distance from Average Solution (EDAS) under probabilistic uncertain linguistic sets (PULTSs) offered a novel integrated model, in which information entropy is used for deriving priority weights of each attribute and EDAS with PULTSs is employed to obtain the final ranking of green supplier. Furthermore, in order to show the applicability of the proposed method, it is validated by a case study for green supplier selection along with some comparative analysis. Thus, the advantage of this proposed method is that it is simple to understand and easy to compute. The proposed method can also contribute to the selection of suitable alternative successfully in other selection issues. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1361	1370		10.2991/ijcis.d.191028.001													
J								An Ensemble Approach for Extended Belief Rule-Based Systems with Parameter Optimization	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Extended belief rule base; AdaBoost; Differential evolution algorithm	COMBINING MULTIPLE CLASSIFIERS	The reasoning ability of the belief rule-based system is easy to be weakened by the quality of training instances, the inconsistency of rules and the values of parameters. This paper proposes an ensemble approach for extended belief rule-based systems to address this issue. The approach is based on the AdaBoost algorithm and the differential evolution (DE) algorithm. In the AdaBoost algorithm, the weights of samples are updated to allow the new subsequent subsystem to pay more attention to those samples misclassified by pervious system. And the DE algorithm is used as the parameter optimization engine to ensure the reasoning ability of the learned extended belief rule-based sub-systems. Since the learned sub-systems are complementary, the reasoning ability of the belief rule-based system can be boosted by combing these sub-systems. Some case studies about many classification test datasets are provided in this paper in the last. The feasibility and efficiency of the proposed approach has been proven by the experimental results. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1371	1381		10.2991/ijcis.d.191112.001													
J								Exploring Fuzzy Rating Regularities for Managing Natural Noise in Collaborative Recommendation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Recommender systems; Natural noise; Regularities; Fuzzy logic	SYSTEMS; FRAMEWORK; SUPPORT	Recommender systems have played a relevant role in e-commerce for supporting online users to obtain suggestions about products that best fit their preferences and needs in overloaded search spaces. In such a context, several authors have proposed methods focused on removing the users' inconsistencies when they rate items, so-called natural noise, improving in this way the recommendation performance. The current paper explores the use of rating regularities for managing the natural noise in collaborative filtering recommendation, having as key feature the use of fuzzy techniques for coping with the uncertainty associated to such scenarios. Specifically, such regularities are used for representing common rating patterns and thus detect noisy ratings when they tend to contradict such patterns. An experimental study is developed for showing the performance of the proposal, as well as analyzing its behavior in contrast to previous natural noise management procedures. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1382	1392		10.2991/ijcis.d.191115.001													
J								Interval-Valued Probabilistic Dual Hesitant Fuzzy Sets for Multi-Criteria Group Decision-Making	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Interval-valued probabilistic dual hesitant fuzzy sets; Multi-criteria group decision-making; Ordered weighted averaging operator; Ordered distance and similarity measures; Risk evaluation	POWER AGGREGATION OPERATORS; LINGUISTIC TERM SETS; INFORMATION	As a powerful extension to hesitant fuzzy sets (HFSs), dual hesitant fuzzy sets (DHFSs) have been closely watched by many scholars. The DHFSs can reflect the disagreement and hesitancy of decision-makers (DMs) flexibly and conveniently. However, all the evaluation values under the same membership degree are endowed with similar importance. And DHFSs are not able to express DMs' preference degrees on different variables. To overcome this drawback, in this paper, we propose the concept of interval-valued probabilistic dual hesitant fuzzy sets (IVPDHFSs) by providing each element with an interval-valued probability value, which can describe DMs' preferences, hesitancy and disapproval simultaneously. Then we define the basic operation laws, score function and deviation function for interval-valued probabilistic dual hesitant fuzzy elements (IVPDHFEs). Besides, the ordered distance and similarity measures are proposed to calculate the deviation of any two IVPDHFSs and to derive the weight vector for DMs objectively, respectively. To aggregate decision-making information, we present interval-valued probabilistic dual hesitant fuzzy ordered weighted averaging (IVPDHFOWA) operator. Moreover, the water-filling theory is first introduced into IVPDHFSs environment and utilized to obtain unified criteria weights mathematically. Furthermore, a three-phased multi-criteria group decision-making (MCGDM) framework is constructed to address IVPDHFSs information. Finally, a case study concerning Arctic risk evaluation is provided to verify the effectiveness and superiority of the proposed three-phased framework. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1393	1411		10.2991/ijcis.d.191119.001													
J								A-SMOTE: A New Preprocessing Approach for Highly Imbalanced Datasets by Improving SMOTE	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Imbalanced datasets; SMOTE; Machine learning; Oversampling; Undersampling	SAMPLING METHOD; CLASSIFICATION; CLASSIFIERS; IDENTIFICATION; SYSTEM; NOISY	Imbalance learning is a challenging task for most standard machine learning algorithms. The Synthetic Minority Oversampling Technique (SMOTE) is a well-known preprocessing approach for handling imbalanced datasets, where the minority class is oversampled by producing synthetic examples in feature vector rather than data space. However, many recent works have shown that the imbalanced ratio in itself is not a problem and deterioration of the model performance is caused by other reasons linked to the minority class sample distribution. The blind oversampling by SMOTE leads to two major problems: noise and borderline examples. Noisy examples are those from one class located in the safe zone of the other. Borderline examples are those located in the neighborhood of the class boundary. These samples are associated with deteriorating performance of the models developed. Therefore, it is critical to concentrate on the minority class data structure and regulate the positioning of the newly introduced minority class samples for better performance of classifiers. Hence, this paper proposes the advanced SMOTE, denoted as A-SMOTE, to adjust the newly introduced minority class examples based on distance to the original minority class samples. To achieve this objective, we first employ the SMOTE algorithm to introduce new samples to the minority and eliminate those examples that are closer to the majority than the minority. We apply the proposed method to 44 datasets at various imbalance ratios. Ten widely used data sampling methods selected from the literature are employed for performance comparison. The C4.5 and Naive Bayes classifiers are utilized for experimental validation. The results confirm the advantage of the proposed method over the other methods in almost all the datasets and illustrate its suitability for data preprocessing in classification tasks. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1412	1422		10.2991/ijcis.d.191114.002													
J								Exploitation of Social Network Data for Forecasting Garment Sales	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Social Media Data; Forecasting; Naive Bayes; Sentiment analysis; Fuzzy forecasting model	MODEL; MEMBERSHIP; REVIEWS; MEDIA	Growing use of social media such as Twitter, Instagram, Facebook, etc., by consumers leads to the vast repository of consumer generated data. Collecting and exploiting these data has been a great challenge for clothing industry. This paper aims to study the impact of Twitter on garment sales. In this direction, we have collected tweets and sales data for one of the popular apparel brands for 6 months from April 2018 - September 2018. Lexicon Approach was used to classify Tweets by sentence using Naive Bayes model applying enhanced version of Lexicon dictionary. Sentiments were extracted from consumer tweets, which was used to map the uncertainty in forecasting model. The results from this study indicate that there is a correlation between the apparel sales and consumer tweets for an apparel brand. "Social Media Based Forecasting (SMBF)" is designed which is a fuzzy time series forecasting model to forecast sales using historical sales data and social media data. SMBF was evaluated and its performance was compared with Exponential Forecasting (EF) model. SMBF model outperforms the EF model. The result from this study demonstrated that social media data helps to improve the forecasting of garment sales and this model could be easily integrated to any time series forecasting model. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1423	1435		10.2991/ijcis.d.191109.001													
J								Optimized Intelligent Design for Smart Systems Hybrid Beamforming and Power Adaptation Algorithms for Sensor Networks Decision-Making Approach	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Optimized intelligent design for smart systems; Wireless network beamforming; Power adaption; Interference; Strategic decision-making; Game theory; Reinforcement learning	LINEAR PRECODING STRATEGIES; MIMO CHANNELS; GAME-THEORY; ALLOCATION	During last two decades, power adaptation and beamforming solutions have been proposed for multiple input multiple output (MIMO) Ad Hoc networks. Game theory based methods such as cooperative and non-cooperative joint beamforming and power control for the MIMO ad hoc systems consider the interference and overhead reduction, but have failed to achieve the trade-off between communication overhead and power minimization. Cooperative method using game theory achieves the power minimization, but introduced the overhead. The non-cooperative solution using game theory reduced the overhead, but it takes more power and iterations for convergence. In this paper, a novel game theory based algorithms proposed to achieve the trade-off between power control and communication overhead for multiple antennas enabled wireless ad-hoc networks operating in multiple-users interference environment. The optimized joint iterative power adaption and beamforming method designed to minimize the mutual interference at every wireless node with constant received signal to interference noise ratio (SINR) at every receiver node. First cooperative potential game theory based algorithm designed for the power and interference minimization in which users cluster and binary weight books along used to reduce the overhead. Then the non-cooperative based approach using the reinforcement learning (RL) method is proposed to reduce the number of iterations and power consumption in networks, the proposed RL procedure is fully distributed as every transmit node require only an observation of its instantaneous beamformer label which can be obtained from its receive node. The simulation results of both methods prove the efficient power adaption and beamforming for small and large networks with minimum overhead and interference compared to state-of-art methods. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1436	1445		10.2991/ijcis.d.191121.001													
J								Application of Collaborative Filtering Algorithm in Mathematical Expressions of User Personalized Information Recommendation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Collaborative filtering algorithm; User personalized information; Mathematical expression; User similarity measure	SYSTEMS	In order to solve the shortcomings of the existing mathematical expressions of the user's personalized recommendation search and to increase the accuracy of the user's personalized information recommendation, in this study, the collaborative filtering algorithm, the related theory of fuzzy sets, and the evaluation methods were introduced at first. Then, the establishment of collaborative filtering model and working principle, user similarity measure, generation of mathematical expression recommendation list, design of mathematical expression recommendation model and implementation of function were introduced. Finally, the application of the collaborative filtering algorithm in the user's personalized information recommendation mathematical expression was verified by collecting data. The results showed that the recommendation mathematical expression model based on collaborative filtering algorithm had higher accuracy in collecting user personalized information, and the accuracy of the system was higher than other recommendation algorithms, which can better meet the individual needs of users. This research can provide a fast and convenient way for users to search for personalized information, and has a good guiding significance for the retrieval and recommendation of informational scientific and technical literature. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1446	1453		10.2991/ijcis.d.191129.001													
J								Econometric Analysis of Disequilibrium Relations Between Internet Finance and Real Economy in China	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Internet finance; Real economy; Disequilibrium relations; Financial interrelations ratio (FIR)	RISK	In this paper, we evaluate the development level of internet finance by focusing on three major virtual economies, internet "Baby" fund, internet financial credit and Shanghai Composite Index for the first time. We examine the disequilibrium relations between the development of both internet finance and real economy by using Vector Autoregression (VAR) model, and measure the level of the deviation between them by using Financial Interrelations Ratio (FIR). We obtain the following results: 1) There is no Granger causality between internet finance and real economy, and the characteristics of non-balanced development is shown; 2) Their synergy is in the development stage of "mismatch"; 3) During the later stage of sample observation, internet financial credit is the key element of the mutual restraint between internet finance and real economy, and the deviation between stock market and real economy is within the controllable range. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1454	1464		10.2991/ijcis.d.191128.001													
J								A Method to Multi-Attribute Group Decision-Making Problem with Complex q-Rung Orthopair Linguistic Information Based on Heronian Mean Operators	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Complex q-rung orthopair fuzzy sets; Linguistic sets; Complex q-rung orthopair linguistic sets; Heronian mean operators; Geometric Heronian mean operators	PYTHAGOREAN FUZZY-SETS; AGGREGATION OPERATORS	The notions of complex q-rung orthopair fuzzy sets (Cq-ROFSs) and linguistic sets (LSs) are two different concepts to deal with uncertain information in multi-attribute group decision-making (MAGDM) problems. The Heronain mean (HM) and geometric Heronain mean (GHM) operators are an effective tool used to aggregate some q-rung orthopair linguistic fuzzy numbers (q-ROLFNs) into a single element. The purpose of this manuscript is to propose a new concept called complex q-rung orthopair linguistic sets (Cq-ROLSs) to cope with complex uncertain information in real decision-making problems. Then the fundamental laws and their examples of the Cq-ROLSs are also given. Furthermore, the notions of complex q-rung orthopair linguistic Heronian mean (Cq-ROLHM) operator, complex q-rung orthopair linguistic weighted Heronian mean (Cq-ROLWHM) operator, complex q-rung orthopair linguistic geometric Heronian mean (Cq-ROLGHM) operator, complex q-rung orthopair linguistic weighted geometric Heronian mean (Cq-ROLWGHM) operator are proposed and their basic properties are also discussed. Moreover, we develop a novel approach to MAGDM using proposed operators and a numerical example is used to describe the flexibility and explicitly of the initiated operators. In last, the comparison between proposed method and existing work is also discussed in detail. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1465	1496		10.2991/ijcis.d.191030.002													
J								Fuzzy System Based on Two-Step Cascade Genetic Optimization Strategy for Tobacco Tar Prediction	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Adaptive neuro fuzzy system (ANFISs); Genetic algorithm (GA); Fuzzy logic (FUZZY); Tar; GA-ANFIS; GA-FUZZY; GA-GA-FUZZY	NEURO-FUZZY; INFERENCE SYSTEM; ANFIS-GA; ALGORITHM	There are many challenges in accurately measuring cigarette tar constituents. These include the need for standardized smoke generation methods related to unstable mixtures. In this research were developed algorithms using fusion of artificial intelligence methods to predict tar concentration. Outputs of development are three fuzzy structures optimized with genetic algorithms resulting in genetic algorithm (GA)-FUZZY, GA-adaptive neuro fuzzy inference system (ANFIS), GA-GA-FUZZY algorithms. Proposed algorithms are used for the tar prediction in the cigarette production process. The results of prediction are compared with gas chromatograph (high-performance liquid chromatography (HPLC)) readings. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1497	1511		10.2991/ijcis.d.191122.001													
J								An Extreme Learning Machine and Gene Expression Programming-Based Hybrid Model for Daily Precipitation Prediction	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Extreme Learning Machine; Gene Expression Programming; Quantitative precipitation prediction; Rainfall prediction; Soft computing	SINGULAR SPECTRUM ANALYSIS; RAINFALL; ENSEMBLE; COMPUTATION; REGRESSION; FORECAST; WAVELET	Accurate daily precipitation prediction is crucially important. However, it is difficult to predict the precipitation accurately due to inherently complex meteorological factors and dynamic behavior of weather. Recently, considerable attention has been devoted in soft computing-based prediction approaches. This work presents a scheme to reduce the risk of Extreme Learning Machine (ELM) modeling error using Gene Expression Programming (GEP) to improve the prediction performance, and develops an ELM-GEP hybrid model for regional daily quantitative precipitation prediction. In this study, firstly, we use ELM for modeling the data sample of daily rainfall to construct a main model. Secondly, we use GEP for modeling the error of the main model as a compensation of the main model to reduce the prediction error. We conducted eight experiments of two different types of daily precipitation prediction problems using five metrics to evaluate our proposed model performance. Experimental results show that our model is comparable or even superior to five state-of-the-art models with high reliability in terms of all metrics on all datasets. It indicates that the proposed method is a promising alternative prediction tool for higher accuracy and credibility of regional daily precipitation prediction. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1512	1525		10.2991/ijcis.d.191126.001													
J								Energy Analysis on Localization Free Routing Protocols in UWSNs	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Localization free Routing protocols; Underwater wireless sensing network; Energy Analysis		The challenges faced in underwater communication systems are limited bandwidth, Energy consumption rate, larger propagation delay time, End-End Delay (E-ED), 3D topology, media access control, routing, resource utilization and power constraints. These issues and challenges are solved by means of deploying the energy efficient protocol. The protocol can either be localization-based protocol or localization free protocol. In this paper, review of localization free protocols were discussed and reviewed with reference to environmental factors, data transmission rate, transmission efficiency, energy consumption rate, E-ED and propagation delay. The review analysis gives the pros and cons to give rise to the new directions of research for future improvements in Underwater Wireless Sensor Networks (UWSNs). This manuscript propose a survey on localization free protocol according to the problem addressed or the major parameter considered during routing in UWSNs. Unlike the existing survey, the present survey focuses on present state and art of routing protocols, in terms of routing strategy issues addressed. The solutions of energy efficient protocol is arrived by highlighting the pros of each protocols. The description of routing strategy for each protocol is presented to understand its operation in an understandable form. The cons of each protocol is taken into consideration for further investigation and to arrive the best protocol. The presented routing strategy, pros and cons provide open challenges and research directions for future investigation. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1526	1536		10.2991/ijcis.d.191203.001													
J								A Siamese Neural Network Application for Sales Forecasting of New Fashion Products Using Heterogeneous Data	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Sales forecasting; Siamese neural networks; Fashion products; Fashion retail		In the fashion market, the lack of historical sales data for new products imposes the use of methods based on Stock Keeping Unit (SKU) attributes. Recent works suggest the use of functional data analysis to assign the most accurate sales profiles to each item. An application of siamese neural networks is proposed to perform long-term sales forecasting for new products. A comparative study using benchmark models is conducted on data from a European fashion retailer. This shows that the proposed application can produce valuable item level sales forecasts. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1537	1546		10.2991/ijcis.d.191122.002													
J								GRA Method for Probabilistic Linguistic Multiple Attribute Group Decision Making with Incomplete Weight Information and Its Application to Waste Incineration Plants Location Problem	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multiple attribute group decision making (MAGDM); Probabilistic linguistic term sets (PLTSs); GRA method; Incomplete weight information; Waste incineration plants	GREY RELATIONAL ANALYSIS; MUIRHEAD MEAN OPERATORS; TERM SETS; AGGREGATION OPERATORS; FINANCIAL PERFORMANCE; QUALIFLEX APPROACH; 2-TUPLE; ENTERPRISES; SELECTION; COMPANY	In this essay, we investigate the probabilistic linguistic multiple attribute group decision making (PL-MAGDM) with incomplete weight information. In this method, the linguistic representation developed recently is converted into probabilistic linguistic information. For deriving the weight information of the attribute, an optimization model is built on the basis of the fundamental idea of grey relational analysis (GRA), by which the attribute weights can be decided. Then, the optimal alternative is chosen through calculating largest relative relational degree from the probabilistic linguistic positive ideal solution (PLPIS) which considers both the largest grey relational coefficient (GRC) from the PLPIS and the smallest GRC form probabilistic linguistic negative ideal solution (PLNIS). In the end, a case study concerning waste incineration plants location problem is given to demonstrate the merits of the developed methods. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1547	1556		10.2991/ijcis.d.191203.002													
J								2-Dimension Linguistic Bonferroni Mean Aggregation Operators and Their Application to Multiple Attribute Group Decision Making	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										MAGDM; Bonferroni mean operator; 2-dimension linguistic weight Bonferroni mean aggregation operator; 2-dimension linguistic variable; 2-dimension linguistic lattice implication algebra	INFORMATION; MODEL	The aim of this paper is to provide a multiple attribute group decision making (MAGDM) method based on the 2-dimension linguistic weight Bonferroni mean aggregation (2DLWBMA) operator. Firstly, the new operations of 2-dimension linguistic variables are defined. Then, the 2-dimension linguistic Bonferroni mean aggregation operator is proposed to describe the correlations of input arguments. Subsequently, the 2DLWBMA operator is investigated to consider the importance of attributes. Furthermore, a novel MAGDM method is introduced and two illustrative examples are given. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1557	1574		10.2991/ijcis.d.191125.001													
J								Building an Artificial Neural Network with Backpropagation Algorithm to Determine Teacher Engagement Based on the Indonesian Teacher Engagement Index and Presenting the Data in a Web-Based GIS	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Artificial neural networks; Backpropagation Stochastic learning; Steepest gradient descent; Indonesian Teacher Engagement Index; Executive information system		Teacher engagement is a newly-emerged concept in the field of Indonesian teacher education. To support this concept, we designed an artificial neural network (ANN) using backpropagation, stochastic learning, and steepest gradient descent algorithms to determine teacher engagement based on the Indonesian Teacher Engagement Index (ITEI). The resulting ANN may be used in a data-gathering website for teachers to use for self-evaluation and self-intervention. The optimal architecture for the ANN has 44 input nodes, 26 first hidden layer nodes, 20 second hidden layer nodes, and 7 output nodes, with a learning rate of 0.05 and trained over 5000 iterations. The sample data used for training was gathered by ITEI researchers and the Executive Board of Indonesian Teachers Association (Pengurus Besar Persatuan Guru Republik Indonesia, PB-PGRI) and includes data of teachers from all around Indonesia. The maximum accuracy of this ANN was 97.98%. The sample data were then used to create an executive information system presented in the form of a map created using ArcGIS Pro software. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1575	1584		10.2991/ijcis.d.191101.003													
J								Strategic Management of Organizational Knowledge and Employee's Awareness About Artificial Intelligence With Mediating Effect of Learning Climate	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Strategic knowledge management; Artificial intelligence; Learning climate; Confirmatory Factor Analysis; Structural Equation Modeling	INTEGRATION; CREATION	This study has aimed to examine the empirically relationship between strategic management of organizational knowledge and awareness about artificial intelligence (AI) through mediating effect of learning climate in service sector of Saudi Arabia. For better understanding, a structural questionnaire was developed, and both questionnaire and interview techniques were applied to collect the data from targeted respondents. Statistical methods like confirmatory factor analysis, structural equation modeling and mediation technique were applied to check the direct and indirect effects with overall valid responses of 390. Findings under factor analysis specify that all the indicators of strategic management of knowledge, learning climate and awareness about AI have demonstrated good factor loadings. Additionally, results under structural equation model for strategic management of knowledge and awareness about AI indicates that knowledge acquisition, knowledge dissemination and knowledge responsiveness have their significant and positive relationship with AI awareness. While mediating effect of learning climate (LC), shows that relationship between knowledge acquisition and AI, between knowledge dissemination and AI is significantly mediated by LC under full sample. While mediating effect of LC between knowledge responsiveness and AI is not significant. In addition, practical implications of this study reveals the fact that management in all three layers of the business firms should consider these findings and positive role by learning climate to promote awareness about AI. However, this study has observed several limitations. At first overall impact of strategic management of knowledge is yet to be explored as this study has examined its impact through knowledge acquisition, dissemination, and responsiveness factors under sperate models. At second, study has a regional limitations and specific organizational implications only for the service sector is observed. At third, earlier studies have raised serious concern over data collection through questionnaire as it covers only one dimensions of employee's attitude. Other instruments like open ended non-structural interview should be reconsidered in more significant manner. Future studies could be reimplemented on similar topic while addressing these limitations. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1585	1591		10.2991/ijcis.d.191025.002													
J								An Advanced Deep Residual Dense Network (DRDN) Approach for Image Super-Resolution	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Deep residual dense network (DRDN); Single image super-resolution; Fusion reconstruction; Residual dense connection; Multi-hop connection		In recent years, more and more attention has been paid to single image super-resolution reconstruction (SISR) by using deep learning networks. These networks have achieved good reconstruction results, but how to make better use of the feature information in the image, how to improve the network convergence speed, and so on still need further study. According to the above problems, a novel deep residual dense network (DRDN) is proposed in this paper. In detail, DRDN uses the residual-dense structure for local feature fusion, and finally carries out global residual fusion reconstruction. Residual-dense connection can make full use of the features of low-resolution images from shallow to deep layers, and provide more low-resolution image information for super-resolution reconstruction. Multi-hop connection can make errors spread to each layer of the network more quickly, which can alleviate the problem of difficult training caused by deepening network to a certain extent. The experiments show that DRDN not only ensure good training stability and successfully converge but also has less computing cost and higher reconstruction efficiency. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1592	1601		10.2991/ijcis.d.191209.001													
J								Subgroup Discovery on Multiple Instance Data	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Supervised descriptive patterns; Subgroup discovery; Multi-instance data; Metaheuristics	DESCRIPTIVE RULE DISCOVERY; FUZZY RULES; ALGORITHM; PATTERN	To date, the subgroup discovery (SD) task has been considered in problems where a target variable is unequivocally described by a set of features, also known as instance. Nowadays, however, with the increasing interest in data storage, new data structures are being provided such as the multiple instance data in which a target variable value is ambiguously defined by a set of instances. Most of the proposals related to multiple instance data are based on predictive tasks and no supervised descriptive analysis can be provided when data is organized in this way. At this point, the aim of this work is to extend the SD task to cope with this type of data. SD is a really interesting task that aims at discovering interesting relationships between different features with respect to a specific target variable that is of interest for the user or the problem under study. In this regard, this paper presents three different approaches for mining interesting subgroups in multiple instance problems. The proposed models represent three different ways of tackling the problem and they are based on three well-known algorithms in the SD field: SD-Map (exhaustive search approach), CGBA-SD (Comprehensible Grammar-Based Algorithm for Subgroup Discovery) and NMEEF-SD (multi-objective evolutionary fuzzy system). The proposals have been tested on a wide set of datasets, including 10 real-world and 20 synthetic datasets, aiming at describing how the three methodologies behave on different scenarios. Any comparison is unfair since they are completely different methodologies. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1602	1612		10.2991/ijcis.d.191213.001													
J								Character-Level Quantum Mechanical Approach for a Neural Language Model	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Character-level; Quantum theory; Network-in-network; Language model	SEMANTIC ANALYSIS	This article proposes a character-level neural language model (NLM) that is based on quantum theory. The input of the model is the character-level coding represented by the quantum semantic space model. Our model integrates a convolutional neural network (CNN) that is based on network-in-network (NIN). We assessed the effectiveness of our model through extensive experiments based on the English-language Penn Treebank dataset. The experiments results confirm that the quantum semantic inputs work well for the language models. For example, the PPL of our model is 10%-30% less than the states of the arts, while it keeps the relatively smaller number of parameters (i.e., 6 m). (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1613	1621		10.2991/ijcis.d.191114.001													
J								Music Emotion Recognition by Using Chroma Spectrogram and Deep Visual Features	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Music emotion recognition; Deep learning; Deep features; Chroma spectrogram; AlexNet; VGG-16		Music has a great role and importance in human life since it has the ability to trigger or convey feelings. As recognizing music emotions is the subject of many studies conducted in many disciplines like science, psychology, musicology and art, it has attracted the attention of researchers as an up-to-date research topic in recent years. Many researchers extract acoustic features from music and investigate relations between emotional tags corresponding to these features. In recent studies, on the other hand, music types are classified emotionally by using deep learning through music spectrograms that involved both time and frequency domain information. In the present study, a new method is presented for music emotion recognition by employing pre-trained deep learning model with chroma spectrograms extracted from music recordings. The AlexNet architecture is used as the pre-trained network model. The conv5, Fc6, Fc7 and Fc8 layers of the AlexNet model are chosen as the feature extracting layer, and deep visual features are extracted from these layers. The extracted deep features are used to train and test the Support Vector Machines (SVM) and the Softmax classifiers. Besides, deep visual features are extracted from conv5_3, Fc6, Fc7 and Fc8 layers of the VGG-16 deep network model and the same experimental applications are made in order to find out the effective power of pre-trained deep networks in music emotion recognition. Several experiments are conducted on two datasets, and better results are obtained with the proposed method. The best result is obtained from the VGG-16 in the Fc7 layer as 89.2% on our dataset. According to the obtained results, it is observed that the presented method performs better. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1622	1634		10.2991/ijcis.d.191216.001													
J								A Statistical Approach to Provide Explainable Convolutional Neural Network Parameter Optimization	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Optimization; Convolutional neural network; Hyperparameter; Design of experiment; Taguchi; Deep learning	TAGUCHI METHOD	Algorithms based on convolutional neural networks (CNNs) have been great attention in image processing due to their ability to find patterns and recognize objects in a wide range of scientific and industrial applications. Finding the best network and optimizing its hyperparameters for a specific application are central challenges for CNNs. Most state-of-the-art CNNs are manually designed, while techniques for automatically finding the best architecture and hyperparameters are computationally intensive, and hence, there is a need to severely limit their search space. This paper proposes a fast statistical method for CNN parameter optimization, which can be applied in many CNN applications and provides more explainable results. The authors specifically applied Taguchi based experimental designs for network optimization in a basic network, a simplified Inception network and a simplified Resnet network, and conducted a comparison analysis to assess their respective performance and then to select the hyperparameters and networks that facilitate faster training and provide better accuracy. The results show that up to a 6% increase in classification accuracy can be achieved after parameter optimization. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1635	1648		10.2991/ijcis.d.191219.001													
J								Evaluation on the Efficiency of Water-Energy-Food Nexus Based on Data Envelopment Analysis (DEA) and Malmquist in Different Regions of China	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										DEA Model; Malmquist index; W-E-F nexus; Efficiency	LIFE-CYCLE ASSESSMENT; SUSTAINABLE DEVELOPMENT; DECISION-MAKING; CONSUMPTION	Water-energy-food (W-E-F) nexus is an essential element for human survival, as well as the basis for the sustainable development of regional economy and ecological environment. Water resource is an important input element of energy production and food production, which makes the W-E-F nexus become closer and more complicated. Taking water resource as the main input variable and grain as the output variable, and taking water resource and grain consumption as the main input variable and energy output as the output variable, the W-F and W/F-E coupling efficiency evaluation index system was constructed. Using data envelopment analysis (DEA) and Malmquist index model, the W-E-F coupling efficiency of 31 regions in China from 2007 to 2016 was evaluated from static and dynamic perspectives. In view of the low comprehensive efficiency in most areas, this paper gives corresponding strategies and suggestions. (C) 2019 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2019	12	2					1649	1659		10.2991/ijcis.d.191209.002													
J								A MORE FAITHFUL FORMAL DEFINITION OF THE DESIRED PROPERTY FOR DISTRIBUTED SNAPSHOT ALGORITHMS TO MODEL CHECK THE PROPERTY	COMPUTING AND INFORMATICS										Distributed snapshot algorithm; reachability; state machine; property specification; model checking		The first distributed snapshot algorithm was invented by Chandy and Lamport: Chandy-Lamport distributed snapshot algorithm (CLDSA). Distributed snapshot algorithms are crucial components to make distributed systems fault tolerant. Such algorithms are extremely important because many modern key software systems are in the form of distributed systems and should be fault tolerant. There are at least two desired properties such algorithms should satisfy: 1) the distributed snapshot reachability property (called the DSR property) and 2) the ability to run concurrently with, but not alter, an underlying distributed system (UDS). This paper identifies subtle errors in a paper on formalization of the DSR property and shows how to correct them. We give a more faithful formal definition of the DSR property; the definition involves two state machines - one state machine M-UDS that formalizes a UDS and the other M-CLDSA that formalizes the UDS on which CLDSA is superimposed (UDS-CLDSA) - and can be used to more precise model checking of the DSR property for CLDSA. We also prove a theorem on equivalence of our new definition and an existing one that only involves M-CLDSA to guarantee the validity of the existing model checking approach. Moreover, we prove the second property, namely that CLDSA does not alter the behaviors of UDS.																	1335-9150						2019	38	5					1009	1038		10.31577/cai_2019_5_1009													
J								GATHERING INFORMATION ON THE WEB BY CONSISTENT ENTITY AUGMENTATION	COMPUTING AND INFORMATICS										Web table; data integration; entity augmentation; consistency	TABLES	Users usually want to gather information about what they are interested in, which could be achieved by entity augmentation using a vast amount of web tables. Existing techniques assume that web tables are entity-attribute binary tables. As for tables having multiple columns to be augmented, they will be split into several entity-attribute binary relations, which would cause semantic fragmentation. Furthermore, the result table consolidated by binary relations will suffer from entity inconsistency and low precision. The objective of our research is to return a consistent result table for entity augmentation when given a set of entities and attribute names. In this paper we propose a web information gathering framework based on consistent entity augmentation. To ensure high consistency and precision of the result table we propose that answer tables for building result table should have consistent matching relationships with each other. Instead of splitting tables into pieces we regard web tables as nodes and consistent matching relationships as edges to make a consistent clique and expand it until its coverage for augmentation query reaches certain threshold gamma. It is proved in this paper that a consistent result table could be built by considering tables in consistent clique to be answer tables. We tested our method on four real-life datasets, compared it with different answer table selection methods and state-of-the-art entity augmentation technique based on table fragmentation as well. The results of a comprehensive set of experiments indicate that our entity augmentation framework is more effective than the existing method in getting consistent entity augmentation results with high accuracy and reliability.																	1335-9150						2019	38	5					1039	1066		10.31577/cai_2019_5_1039													
J								NEW APPROACH TO EDGE DETECTION ON DIFFERENT LEVEL OF WAVELET DECOMPOSITION	COMPUTING AND INFORMATICS										Edge detection; wavelet decomposition; compression; F measure; figure of merit; performance ratio		This paper proposes a new approach to edge detection on the images over which the wavelet decomposition was done to the third level and consisting of different levels of detail (small, medium and high level of detail). Images from the BSD (Berkeley Segmentation Dataset) database with the corresponding ground truth were used. Daubechies wavelet was used from second to tenth order. Gradient and Laplacian operators were used for edge detection. The proposed approach is applied in systems where information is processed in real time, where fast image processing is required and in systems where high compression ratio is used. That is, it can find practical application in many systems, especially in television systems where the level of details in the image changes. The new approach consists in the fact that when wavelet transform is applied, an edge detection is performed over the level 1 image to create a filter. The filter will record only those pixels that can be potential edges. The image is passed through a median filter that filters only the recorded pixels and 8 neighbors of pixel. After that, the edge detection with one of the operators is applied onto the filtered image. F measure, FoM (Figure of Merit) and PR (Performance Ratio) were used as an objective measure. Based on the obtained results, the application of the proposed approach achieves significant improvements and these improvements are very good depending on the number of details in the image and the compression ratio. These results and improvements can be used to improve the quality of edge detection in many systems where compressed images are processed, that is, where work with images with a high compression ratio is required.																	1335-9150						2019	38	5					1067	1090		10.31577/cai_2019_5_1067													
J								OVERLAPPING COMMUNITY DETECTION EXTENDED FROM DISJOINT COMMUNITY STRUCTURE	COMPUTING AND INFORMATICS										Disjoint community detection; overlapping community detection; potential member; overlapping node	COMPLEX NETWORKS; ALGORITHM	Community detection is a hot issue in the study of complex networks. Many community detection algorithms have been put forward in different fields. But most of the existing community detection algorithms are used to find disjoint community structure. In order to make full use of the disjoint community detection algorithms to adapt to the new demand of overlapping community detection, this paper proposes an overlapping community detection algorithm extended from disjoint community structure by selecting overlapping nodes (ONS-OCD). In the algorithm, disjoint community structure with high qualities is firstly taken as input, then, potential members of each community are identified. Overlapping nodes are determined according to the node contribution to the community. Finally, adding overlapping nodes to all communities they belong to and get the final overlapping community structure. ONS-OCD algorithm reduces the computation of judging overlapping nodes by narrowing the scope of the potential member nodes of each community. Experimental results both on synthetic and real networks show that the community detection quality of ONS-OCD algorithm is better than several other representative overlapping community detection algorithms.																	1335-9150						2019	38	5					1091	1110		10.31577/cai_2019_5_1091													
J								INFORMATION TECHNOLOGY OF GENERALIZED MODEL CREATION OF COMPLEX TECHNICAL OBJECTS	COMPUTING AND INFORMATICS										Knowledge base; geometric model; managing parametric model		The paper introduces a knowledge representation framework for design and geometrical modelling of complex technical objects such as ships, aircrafts, cars, etc. The design process cannot be fully automated yet because of a lot of technical and economical factors that influence the decisions during that process. In order to make the process more efficient, a knowledge modelling framework is suggested. The basic principles of conceptual knowledge modelling and data exchange framework are presented. A practical use case of aircraft ramp modelling is provided.																	1335-9150						2019	38	5					1111	1130		10.31577/cai_2019_5_1111													
J								MOBILE EDGE COMPUTING BASED IMMERSIVE VIRTUAL REALITY STREAMING SCHEME	COMPUTING AND INFORMATICS										Mobile edge computing; multi-access edge computing; mobile edge computing video service; mobile cloud computing; virtual reality streaming; video streaming	SCENARIOS; CLOUD; IOT	Recently, new services using virtual reality (VR)/augmented reality (AR) have appeared and then exploded in entertainment fields like video games and multimedia contents. In order to efficiently provide these services to users, an infrastructure for mobile cloud computing with powerful computing capabilities is widely utilized. However, existing mobile cloud system utilizes a cloud server located at a relatively long distance, so that there are problems that a user is not effectively provided with personalized immersive multimedia service. So, this paper proposes the home VR streaming system that can provide fast content access time and high immersiveness by using mobile edge computing (MEC).																	1335-9150						2019	38	5					1131	1148		10.31577/cai_2019_5_1131													
J								FORMAL VERIFICATION OF SECURITY PATTERN COMPOSITION: APPLICATION TO SCADA	COMPUTING AND INFORMATICS										Information security; security patterns; formal verification; model checking; SCADA		Information security was initially required in specific applications, however, nowadays, most companies and even individuals are interested in securing their information assets. The new requirement can be costly, especially with the high demand on security solutions and security experts. Security patterns are reusable security solutions that prove to be efficient and can help developers achieve some security goals without the need for expertise in the security domain. Some security pattern combinations can be beneficial while others are inconsistent. Model checking can be used to verify the production of combining multiple security patterns with an architecture. Supervisory control and data acquisition (SCADA) systems control many of our critical industrial infrastructures. Due to their limitations, and their augmented connectivity, SCADA systems have many unresolved security issues. In this paper, we demonstrate how we can automatically generate a secure SCADA model based on an insecure one and how to verify the generated model.																	1335-9150						2019	38	5					1149	1180		10.31577/cai_2019_5_1149													
J								COALGEBRAIC OPERATIONAL SEMANTICS FOR AN IMPERATIVE LANGUAGE	COMPUTING AND INFORMATICS										Category theory; coalgebra; operational semantics; programming language		Operational semantics is a known and popular semantic method for describing the execution of programs in detail. The traditional definition of this method defines each step of a program as a transition relation. We present a new approach on how to define operational semantics as a coalgebra over a category of configurations. Our approach enables us to deal with a program that is written in a small but real imperative language containing also the common program constructs as input and output statements, and declarations. A coalgebra enables to define operational semantics in a uniform way and it describes the behavior of the programs. The state space of our coalgebra consists of the configurations modeling the actual states; the morphisms in a base category of the coalgebra are the functions defining particular steps during the program's executions. Polynomial endofunctor determines this type of systems. Another advantage of our approach is its easy implementation and graphical representation, which we illustrate on a simple program.																	1335-9150						2019	38	5					1181	1209		10.31577/cai_2019_5_1181													
J								CASE STUDY ON HUMAN-ROBOT INTERACTION OF THE REMOTE-CONTROLLED SERVICE ROBOT FOR ELDERLY AND DISABLED CARE	COMPUTING AND INFORMATICS										Service robots; human-robot interaction; remote control; speech recognition; gesture recognition; ROS	QUALITY-OF-LIFE; USABILITY	The tendency of continuous aging of the population and the increasing number of people with mobility difficulties leads to increased research in the field of Assistive Service Robotics. These robots can help with daily life tasks such as reminding to take medications, serving food and drinks, controlling home appliances and even monitoring health status. When talking about assisting people in their homes, it should be noted that they will, most of the time, have to communicate with the robot themselves and be able to manage it so that they can get the most out of the robot's services. This research is focused on different methods of remote control of a mobile robot equipped with robotic manipulator. The research investigates in detail methods based on control via gestures, voice commands, and web-based graphical user interface. The capabilities of these methods for Human-Robot Interaction (HRI) have been explored in terms of usability. In this paper, we introduce a new version of the robot Robco 19, new leap motion sensor control of the robot and a new multi-channel control system. The paper presents methodology for performing the HRI experiments from human perception and summarizes the results in applications of the investigated remote control methods in real life scenarios.																	1335-9150						2019	38	5					1210	1236		10.31577/cai_2019_5_1210													
J								Introduction to NeutroAlgebraic Structures and AntiAlgebraic Structures (revisited)	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Triplets; (Axiom, NeutroAxiom, AntiAxiom); (Law, NeutroLaw, AntiLaw); (Associativity, NeutroAssociaticity, AntiAssociativity); (Commutativity, NeutroCommutativity, AntiCommutativity); (WellDefined, NeutroDefined, AntiDefined); (Semigroup, NeutroSemigroup, AntiSemigroup); (Group, NeutroGroup, AntiGroup); (Ring, NeutroRing, AntiRing); (Algebraic Structures, NeutroAlgebraic Structures, AntiAlgebraic Structures); (Structure, NeutroStructure, AntiStructure); (Theory, NeutroTheory, AntiTheory); S-denying an Axiom; S-geometries; Multispace with Multistructure		In all classical algebraic structures, the Laws of Compositions on a given set are well-defined. But this is a restrictive case, because there are many more situations in science and in any domain of knowledge when a law of composition defined on a set may be only partially-defined (or partially true) and partially-undefined (or partially false), that we call NeutroDefined, or totally undefined (totally false) that we call AntiDefined. Again, in all classical algebraic structures, the Axioms (Associativity, Commutativity, etc.) defined on a set are totally true, but it is again a restrictive case, because similarly there are numerous situations in science and in any domain of knowledge when an Axiom defined on a set may be only partially-true (and partially-false), that we call NeutroAxiom, or totally false that we call AntiAxiom. Therefore, we open for the first time in 2019 new fields of research called NeutroStructures and AntiStructures respectively.																	2331-6055	2331-608X					2019	31						1	16															
J								A Hybrid Neutrosophic Approach of DEMATEL with AR-DEA in Technology Selection	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic sets; Technology Selection; DEMATEL; Assurance Region; Data Envelopment Analysis	DEVELOPING SUPPLIER SELECTION; DATA ENVELOPMENT ANALYSIS; DECISION-MAKING APPROACH; EFFICIENCY; SYSTEM; GENERATION; FRAMEWORK; BOUNDS; SETS	Technology selection is a leading step for decision makers throughout the technology selection process. The extraction of convenient technology is pretended to be a real challenge that faces decision makers. The technology selection considers the qualitative and quantitative criteria which needs to a special representation due to the conditions of non-compensation and uncertainty on real life. The objectives of this study is to make a hybrid approach using decision making trial and evaluation laboratory (DEMATEL) for detecting the positive and negative regions, and assurance region data envelopment analysis (AR-DEA) for evaluating the efficiency of Decision Making Units (DMUs). The hybrid model is protracted with neutrosophic philosophy in representing the perspectives of specialists and experts to achieve the most optimized outputs. An illustrative case study, about technology revolution and digital transformation in EGYPT, is presented to demonstrate the proposed model.																	2331-6055	2331-608X					2019	31						17	30															
J								BMBJ-neutrosophic subalgebra in BCI/BCK-algebras	NEUTROSOPHIC SETS AND SYSTEMS										BMBJ-neutrosophic set; BMBJ-neutrosophic subalgebra; BMBJ-neutrosophic S-extension	SUPPLIER SELECTION	For the first time Smarandache introduced neutrosophic sets which can be used as a mathematical tool for dealing with indeterminate and inconsistent information. the notion of BMBJ-neutrosophic set and subalge-bra, as a generalization of a neutrosophic set, is introduced, and it's application to BCI/BCK-algebras is investigated. The concept of BMBJ-neutrosophic subalgebras in BCI/BCK-algebras is introduced, and related properties are investigated. New BMBJ-neutrosophic subalgebra is established by using an BMBJ-neutrosophic subalgebra of a BCI/BCK-algebra. Alos, homomorphic (inverse) image of BMBJ-neutrosophic subalgebra and translation of BMBJ-neutrosophic subalgebra is investigated. At the end, we provided conditions for an BMBJ-neutrosophic set to be an BMBJ-neutrosophic subalgebra.																	2331-6055	2331-608X					2019	31						31	43															
J								New Open Sets in N-Neutrosophic Supra Topological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										N-neutrosophic supra topology; N-neutrosophic supra alpha-open set; N-neutrosophic supra semi- open set; N-neutrosophic supra pre-open set; N-neutrosophic supra beta-open set	INTUITIONISTIC FUZZY-SETS; NUMBERS	The neutrosophic set is an imprecise set to deal the concepts of uncertainty, vagueness and irregularity, which consists of three independent functions called truth-membership, indeterminacy-membership and falsity-membership. This set is a generalization of Atanassov's intuitionistic fuzzy sets. The neutrosophic supra topological space is a set together with neutrosophic supra topology. The intension of this paper is to develop the concept of N-neutrosophic supra topological spaces. We further investigate the closure and interior operators in N-neutrosophic supra topological spaces. Moreover, some weak form of N-neutrosophic supra topological open sets are defined and establish their relations with suitable examples.																	2331-6055	2331-608X					2019	31						44	62															
J								A Novel Methodology for Assessment of Hospital Service according to BWM, MABAC, PROMETHEE II	NEUTROSOPHIC SETS AND SYSTEMS										Hospital service; Neutrosophic Sets; Bipolar; BWM; MABAC; PROMETHEE II	GROUP DECISION-MAKING; BEST-WORST; FUZZY; QUALITY; PROJECT; PERFORMANCE; MANAGEMENT; FRAMEWORK; SELECTION; TOPSIS	In this study, a proposed methodology of Best Worst Method (BWM), Multi-Attributive Border Approximation Area Comparison (MABAC), and Preference Ranking Organization Method for Enrichment Evaluations (PROMETHEE II) are suggested to achieve a methodical and systematic procedure to assess the hospital serving under the canopy of neutrosophic theory. The assessing of hospital serving challenges of ambiguity, vagueness, inconsistent information, qualitative information, imprecision, subjectivity and uncertainty are handled with linguistic variables parameterized by bipolar neutrosophic scale. Hence, the hybrid methodology of Bipolar Neutrosophic Linguistic Numbers (BNLNs) of BWM is suggested to calculate the significance weights of assessment criteria, and MABAC as an accurate method is presented to assess hospital serving. In addition to consider the qualitative criteria compensation in hospital service quality in MABAC in order to overcome drawbacks PROMETHEE II of non-compensation to reinforce the serving effectiveness arrangements of the possible alternatives. An experiential case including 9 assessment criteria, 2 public and 3 private hospitals in Sharqiyah EGYPT assessed by 3 evaluators from several scopes of medical industry to prove validity of the suggested methodology. The case study shows that the service effectiveness of private hospitals is superior to public hospitals, since the public infirmaries are scarcely reinforced by governmental institutions.																	2331-6055	2331-608X					2019	31						63	79															
J								Some Results on Single Valued Neutrosophic Hypergroup	NEUTROSOPHIC SETS AND SYSTEMS										Hypergroup; Level sets; Single valued neutrosophic sets; Single valued neutrosophic hypergroup	SUPPLIER SELECTION	We introduced the theory of Single valued neutrosophic hypergroup as the initial theory of single valued neutrosophic hyper algebra and also developed some results on single valued neutrosophic hypergroup.																	2331-6055	2331-608X					2019	31						80	85															
J								Neutrosophic Bipolar Fuzzy Set and its Application in Medicines Preparations	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic bipolar fuzzy sets; multi-attribute group decision making; neutrosophic bipolar fuzzy transformation techniques; interval values and linguistic variables	DEVELOPING SUPPLIER SELECTION; GROUP DECISION-MAKING; SIMILARITY	To tackle the real life problems we come across, in various fields like computer sciences, medical sciences, social sciences and engineering works where we are facing many ambiguities and imprecisions. Here we bring an idea of neutrosophic bipolar fuzzy decision making where hybridized multi-attributes are involved, which is a very helpful tool to tackle the ambiguities and imprecisions. We present the neutrosophic bipolar fuzzy transformation techniques. The different types of attributes are transformed into unified neutrosophic bipolar fuzzy values. It includes the group decision making mode based on hybrid decision making problems with exact values, interval values and linguistic variables. Calculations of weights by decision makers, composition of aggregated weighted neutrosophic bipolar fuzzy decision matrices, determination of entropy weights, finding positive ideal solution(PIS),and negative ideal solution(NIS), calculation of grey relational coefficient,calculation of degree of weighted grey relational coefficient of each alternative, determination of relative relational degree of each alternative from the positive ideal solution (PIS) and negative ideal solution (NIS) and ranking of the alternatives are the concepts which are introduced in the case of neutrosophic bipolar fuzzy hybrid multi-attribute group decision making. Eventually, we apply these concepts and techniques upon hybrid multi-attributes decision making problem of selecting the best medicine to cure some particular diseases and develop an algorithm for neutrosophic bipolar fuzzy hybrid multi-attribute group decision making.																	2331-6055	2331-608X					2019	31						86	100															
J								ELECTRE Approach for Multi-attribute Decision-making in Refined Neutrosophic Environment	NEUTROSOPHIC SETS AND SYSTEMS										ELECTRE; Multi-attribute Decision Making; Refined Neutrosophic Environment	SIMILARITY; SELECTION; ENTROPY; SETS	Uncertainty, imprecise, incomplete, and inconsistent information can be found in many real-life systems and may enter some problems in a much more complex way. Neutrosophic set is the effective and useful tool to describe problems with Uncertainty, imprecise, incomplete, and inconsistent information. In this regard, the present study is trying to present a neutrosophic electrode model through an example to demonstrate the efficiency of the proposed model. In this example, 3 alternatives were evaluated on 5 criteria by 4 experts based on the neutrosophic linguisting variables. After converting the neutrosophic linguisting variables to neutrosophic numbers, it is paid to calculate the integrated matrix and after that, weights of criteria and experts. In the next steps, the concordance and disconcordance matrices are calculated and after that the calculations are done based on the description of section 3. Finally, are ranked the alternatives in this numerical example. The results show that A(3), A(2) and A(1) were ranked first to third respectively.																	2331-6055	2331-608X					2019	31						101	119															
J								A Note on the Concept of alpha - Level Sets of Neutrosophic Set	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic set; alpha - lower level and..; alpha - upper level sets of a neutrosophic set		Neutrosophic set is a unique concept endowed with unconnected degree of indeterminacy excluded in the non-classical sets it generalizes. This paper communicates shortly on the notions of alpha - lower level and alpha - upper level sets of a neutrosophic set and investigates some basic properties.																	2331-6055	2331-608X					2019	31						120	126															
J								T-Neutrosophic Cubic Set on BF-Algebra	NEUTROSOPHIC SETS AND SYSTEMS										BF-algebra; t-neutrosophic cubic set; t-neutrosophic cubic subalgebra; t-neutrosophic cubic closed ideal	SUPPLIER SELECTION; IDEALS; SUBALGEBRAS	In this paper, the concept of t-neutrosophic cubic set is introduced and investigated the t-neutrosophic cubic set through subalgebra, ideal and closed ideal of BF-algebra. Homomorphic properties of t-neutrosophic cubic subalgebra and ideal are also investigated with some related properties.																	2331-6055	2331-608X					2019	31						127	147															
J								Neutrosophic Inventory Backorder Problem Using Triangular Neutrosophic Numbers	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic EOQ; Neutrosophic set; Signed distance method; Triangular neutrosophic numbers	SUPPLIER SELECTION	A company may have backorders if they run out of the stock in their stores, in which case, it can just place a new order to restock its shelves. A customer who is willing to wait for some time until the company has restocked the products would have to place a backorder. A backorder only exists if customers are willing to wait for the order. In this paper, a neutrosophic inventory backorder problem using a triangular neutrosophic numbers is introduced. First, we fuzzify the carrying cost and shortage cost as triangular neutrosophic numbers and the signed distance method is used to defuzzify them. From these, we can obtain the neutrosophic optimal shortage quantity and the neutrosophic total cost. A numerical example is provided to illustrate the proposed model in neutrosophic environment.																	2331-6055	2331-608X					2019	31						148	155															
J								Generalized Neutrosophic Competition Graphs	NEUTROSOPHIC SETS AND SYSTEMS										Competition graph; neutrosophic graph; generalized neutrosophic competition graph; competition number	NUMBER	The generalized neutrosophic graph is a generalization of the neutrosophic graph that represents a system perfectly. In this study, the concept of a neutrosophic digraph, generalized neutrosophic digraph and out-neighbourhood of a vertex of a generalized neutrosophic digraph is studied. The generalized neutrosophic competition graph and matrix representation are analyzed. Also, the minimal graph and competition number corresponding to generalized neutrosophic competition graph are defined with some properties. At last, an application in real life is discussed.																	2331-6055	2331-608X					2019	31						156	171															
J								Operations of Single Valued Neutrosophic Coloring	NEUTROSOPHIC SETS AND SYSTEMS										single-valued neutrosophic graphs; single-valued neutrosophic vertex coloring; strong single-valued neutrosophic graph; complete single-valued neutrosophic graph.		Smarandache introduced the concept of Neutrosophic which deals with membership, non-membership and indeterminacy values. Wang discussed the Single Valued Neutrosophic sets in 2010. Single Valued Neutrosophic graph was introduced by Broumi and in 2019 Single Valued Neutrosophic coloring was introduced. In this paper, some properties of the Single Valued Neutrosophic Coloring of Strong Single Valued Neutrosophic graph, Complete Single Valued Neutrosophic graph and Complement of Single Valued Neutrosophic graphs are discussed.																	2331-6055	2331-608X					2019	31						172	178															
J								Neutrosophic Fuzzy Hierarchical Clustering for Dengue Analysis in Sri Lanka	NEUTROSOPHIC SETS AND SYSTEMS										Dengue; Hierarchical clustering; Fuzzy hierarchical clustering; Neutrosophic Logic		In the structure of nature, we believe that there is an underlying knowledge in all the phenomena we wish to understand. Mainly in the area of epidemiology we often tend to seek the structure of the data obtained, pattern of the disease, nature or cause of its emergence among living organisms. Sometimes, we could see the outbreak of disease is ambiguous and the exact cause of the disease is unknown. A significant number of algorithms and methods are available for clustering disease data. We could see that literature has no traces of including indeterminacy or vagueness in data which has to be much concentrated in epidemiological field. This study analyzes the attack of dengue in 26 districts of Sri Lanka for the period of seven years from 2012 to 2018. Clusters with low risk, medium risk and high risk areas affected by dengue are identified. In this paper, we propose a new algorithm called Neutrosophic-Fuzzy Hierarchical Clustering algorithm (NFHC) that includes indeterminacy. Proposed algorithm is compared with fuzzy hierarchical clustering algorithm and hierarchical clustering algorithm. Finally the results are evaluated with the benchmarking indexes and the performance of the clustering algorithm is studied. NFHC has performed a way better than the other two algorithms.																	2331-6055	2331-608X					2019	31						179	199															
J								On the Isotopy of some Varieties of Fenyves Quasi Neutrosophic Triplet Loop (Fenyves BCI-algebras)	NEUTROSOPHIC SETS AND SYSTEMS										BCI-algebra; quasi neutrosophic loops; Fenyves identities; Bol-Moufang Type		Neutrosophy theory has found application in health sciences in recent years. There is the need to develop neutrosophic algebraic systems which are good and appropriate for studying and understanding the effects of diseases and their possible treatments. In order to achieve this, special types of quasi neutrosophic loops and their isotopy needed to be introduced for this purpose. Fenyves BCI-algebras are BCI-algebras (special types of quasi neutrosophic loops) that satisfy the 60 Bol-Moufang identities. In this paper, the isotopy of BCI-algebras are studied. Neccessary and sufficient conditions for a groupoid isotope of a BCI-algebra to be a BCI-algebra are established. It is shown that p-semisimplicity, quasi-associativity and BCK-algebra are invariant under isotopies which are determined by some regular permutation groups. Furthermore, the isotopy of both the 46 associative and 14 non-associative Fenyves BCI-algebras are also studied. It is shown that for BCI-alegbras, associativity is isotopic invariant. Hence, the following set of Fenyves BCI algebras (F-i-algebras) are invariant under any isotopy: i epsilon{1,2,4,6,7,9,10,11,12,13,14,15,16,17,18,20,22,23,24 ,25,26,27,28,30,31,32,33,34,35,36,37,38,40,41,43,44,45,47,48,49,50,51,53,57,58,60}. It is shown that the following sets of non-associative Fenyves BCI algebras (F-i-algebras) are invariant under isotopies which are determined by some regular permutation groups: i epsilon {3,5,8,19,21,29,39,42,46,52,55,56,59},{56},{8,19,29,39,46,59}. In conclusion, this is the isotopic study of 120 particular types of the 540 varieties of Fenyves quasi neutrosophic triplet loops (FQNTLs) which were recently discovered, wherein the 14 non-associative Fenyves BCI-algebras do not necessarily have the Iseki's conditions (S). Importantly, applying these results, the initial (old, sick or healthy) state of a person can be represented by a type of Fenyves BCI-algebra, while the Fenyves BCI-algebra isotope will represent the final (new, healthy or sick) state of the person as a result of the prescribed medical treatment, which the isotopism represents. The isotopism is a measure of the change from the old state of body condition to the new state.																	2331-6055	2331-608X					2019	31						200	223															
J								Multi-Aspect Decision-Making Process in Equity Investment Using Neutrosophic Soft Matrices	NEUTROSOPHIC SETS AND SYSTEMS										Single valued neutrosophic sets; Neutrosophic soft matrix (NSM); weighted neutrosophic vector; Score and value function; Multi-aspect decision-analysis	SET	Neutrosophic theory alleviates the ambiguity situation more effectively than fuzzy sets. Neutrosophic soft set deals with the combination of truth, indeterminacy and falsity membership. This provides a space for the convention with multi-aspect decision-making (MADM) problems that involve these combinations. The main aim of this paper is to provide a unique ranking for the alternatives to overcome the existing drawbacks in the said environment. Initially, a new score function and the weighted neutrosophic vector are discussed. Secondly, to show the supremacy of the proposed score function a comparison analysis is discussed between the existing score method and the proposed approach. Thirdly, algorithm and flowchart are discussed for the case study. Lastly, a new technique for ranking the alternatives is discussed which enables us to determine the unique highest score. The working model is illustrated with suitable examples to authenticate the tool and to demonstrate the effectiveness of the planned approach.																	2331-6055	2331-608X					2019	31						224	241															
J								Structural Equivalence between Electrical Circuits via Neutrosophic Nano Topology Induced by Digraphs	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic nano topology; Neutrosophic nano neighborhood; Neutrosophic nano continuous; Neutrosophic nano homeomorphism; Neutrosophic nano isomorphism		The purpose of the present work was to study the real life problems using neutrosophic nano topological graph theory. Most real-life situations need some sort of approximation to fit mathematical models. The beauty of using neutrosophic nano topology in approximation is achieved via approximation for qualitative sub graphs without coding or using assumption. By certain nano equivalence relation, we are formalizing the structural equivalence of basic circuit of the LED light from the graphs and their corresponding neutrosophic nano topologies generated by them.																	2331-6055	2331-608X					2019	31						242	249															
J								Neutrosophic Fixed Point Theorems and Cone Metric Spaces	NEUTROSOPHIC SETS AND SYSTEMS										neutrosophic theory; neutrosophic Fixed Point; neutrosophic topology; neutrosophic cone metric space; neutrosophic metric space		The intention of this paper is to give the general definition of cone metric space in the context of the neutrosophic theory. In this relation, we obtain some fundamental results concerting fixed points for weakly compatible mapping.																	2331-6055	2331-608X					2019	31						250	265															
J								Neutrosophic quadruple alpha-ideals	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic quadruple BCK/BCI-number; neutrosophic quadruple BCK/BCI-algebra; neutrosophic quadruple (closed) ideal; neutrosophic quadruple p(q, alpha)-ideal		The notion of neutrosophic quadruple a-ideal is introduced, and related properties are investigated. Relations between a neutrosophic quadruple p-ideal, a neutrosophic quadruple q-ideal, a neutrosophic quadruple alpha-ideal and a neutrosophic quadruple closed ideal are discussed. Conditions for the neutrosophic quadruple (A;B)-set N-q (A;B) to be a neutrosophic quadruple alpha-ideal are provided.																	2331-6055	2331-608X					2019	31						266	281															
J								Neutrosophic LI-ideals in lattice implication algebras	NEUTROSOPHIC SETS AND SYSTEMS										Lattice implication algebra; neutrosophic LI-ideals; neutrosophic lattice ideal; implication homomorphism		The notion of neutrosophic set theory is applied to lattice implication algebras, and the concept of neutrosophic LI-ideals and neutrosophic lattice ideals in a lattice implication algebra are introduced. Several properties are investigated. Relationships between a neutrosophic LI-ideal and a neutrosophic lattice ideal are established, and conditions for a neutrosophic lattice ideal to be a neutrosophic LI-ideal are provided. Characterizations of a neutrosophic LI-ideal are discussed. The properties of implication homomorphism of lattice implication algebras related to neutrosophic LI-ideals are studied.																	2331-6055	2331-608X					2019	31						282	296															
J								Introduction to neutrosophic soft topological spatial region	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic soft set; neutrosophic soft topology; neutrosophic soft connected; neutrosophic soft spatial region; GIS	CLOSED-SETS	Spatial information often deals with regions which are vague or incompletely determined. Understanding vagueness, indeterminacy and imprecision are the most important in GIS. Smarandache's neutrosophic set is a computational method to tackle problems involving incomplete, infinite and reliable data. The definition of soft sets was introduced by Molodtsov as a new mathematical method to tackle uncertainty. Maji presented the Neutrosophic Soft Set theory. This paper provides concepts of a neurtrosophic soft spatial region for its possible application in GIS. The notions of neutrosophic soft alpha-open, neutrosophic soft pre-open, neutrosophic soft semi-open and neutrosophic soft beta-open sets are introduced.																	2331-6055	2331-608X					2019	31						297	304															
J								A low rank robust kernel ridge regression classifier for power quality disturbance pattern recognition	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Short time modified hilbert transform; fourier kernel based s-transform; mathematical morphology; power quality events; kernel ridge regression	EXTREME LEARNING-MACHINE; S-TRANSFORM; WAVELET TRANSFORM; AUTOMATIC CLASSIFICATION; FEATURE-SELECTION; SYSTEM; EVENTS; DECOMPOSITION; OPTIMIZATION; FOURIER	This paper presents the development of a new low rank robust kernel ridge regression (KRR) classifier for power quality (PQ) disturbance pattern recognition. It is well known that the kernel methods are used extensively for regression and classification problems using support vector machines (SVM) by mapping the original nonlinear data into a high-dimensional space thereby increasing the generalization performance, and accuracy of classification. On the other hand, the nonlinear kernel ridge regression approach is known for its simple implementation, fast processing speed and accuracy in comparison to the widely used least square support vector machine (LS-SVM). However, for large scale training patterns, the size of kernel matrix becomes large thereby the execution time becomes prohibitive. Besides, the presence of noise and outliers in the data affects the accuracy of the conventional KRR classifier. This paper, therefore, attempts to develop a dimensionally reduced but robust KRR classifier by choosing a random set of support vectors from the training subset to reduce substantially the training time at the cost of a slight loss in accuracy. Further, the KRR algorithm is modified by using a new objective function to minimize the mean and variance of the error to provide robustness and accuracy. For applying this new classifier to power quality disturbance events three relatively new signal processing techniques like the Short-time modified Hilbert Transform (STMHT), Morphological filters, and Fourier kernel S-transform are used to extract the relevant features from the data samples. The simulation results imply that the proposed methods have a higher recognition rate compared with other established techniques such as ELM and Poly SVM while classifying the PQ disturbances. A PC integrated hardware assembly has been used to verify the PQ events classification in real-time.																	1327-2314	1875-8827					2019	23	4					219	240		10.3233/KES-190414													
J								Fine-grained k-anonymity for privacy preserving in cloud	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Clustering; k-anonymity; privacy preservation; information loss; data privacy		Data sensitive information is a crucial concern of every individual. Hospitals lag their trust in privacy to take up the newest technologies of cloud like Information-as-a-service, storage-as-a-service, to deploy their patient's data for better health management. Intensive study is being undertaken to run-over the shortcomings of data privacy for the published information as well as the publisher, One amongst the methods is privacy by statistics using data mining techniques such as k-anonymity. The fundamental technique of k-anonymity is to anonymize sensitive information of an individual person published that could not be determined from at least (k - 1) instances. The best way to attain k-anonymity is by grouping similar records into a cluster by choosing the best seed value to balance utility and privacy in the published data. This paper proposes a Fine-grained k-anonymity algorithm which uses a systematic procedure of seed selection. The proposed method exhibits a minimum information loss than existing clustering algorithms.																	1327-2314	1875-8827					2019	23	4					241	247		10.3233/KES-190415													
J								Content relative thresholding technique for key frame extraction	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Key frame extraction; automated framework; data replication control; reduced time complexity; video stabilization	VIDEO; RETRIEVAL	The growth in communication methods have motivated a good number of users to migrate the existing communication methods towards video-based communications. Thus, the use of video-based communications have become the basic communication method for various fields and domains as distance education, business, physical security monitoring and also in the field of news and media. The summarization process demands to extract key components from the video data in order to reduce the size of the data without compromising on any information loss. This processing is called key frame extraction process. Realizing the priority of the key frame extraction process, a few parallel research attempts were executed to match with the bottleneck of information loss and size reduction. Nevertheless, the processes were highly criticised for being time complex and sometimes for information loss. The issue with the standard or parallel methods for extraction of key frames is either high or low rate of key frame extractions, which in turn results into high size or high information loss respectively. Thus, this work aims to provide a novel key frame extraction process using the image meta data and further the adaptive thresholding method. The work demonstrates a nearly 50% reduction in time complexity with 100% accuracy of the key frame extraction process and finally a nearly 30% reduction in the key frame replication control.																	1327-2314	1875-8827					2019	23	4					249	258		10.3233/KES-190416													
J								Models for multiple attribute decision making with some interval-valued 2-tuple linguistic Pythagorean fuzzy Bonferroni mean operators	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Multiple attribute decision making (MADM); neutrosophic numbers; interval-valued 2-tuple linguistic Pythagorean fuzzy sets (IV2TLPFSs); IV2TLPFWBM operator; GIV2TLPFWBM operator; DGIV2TLPFWBM operator; green supplier selection; green supply chain management	HAMACHER AGGREGATION OPERATORS; FUNDAMENTAL PROPERTIES; SIMILARITY MEASURES; PROGRAMMING METHOD; TOPSIS METHOD; STOCK-MARKET; EXTENSION; CHINA; SETS	In this paper, we extend the Bonferroni mean (BM) operator, generalized Bonferroni mean (GBM), and dual generalized Bonferroni mean (DGBM) operator with interval-valued 2-tuple linguistic Pythagorean fuzzy numbers (IV2TLPFNs) to propose interval-valued 2-tuple linguistic Pythagorean fuzzy weighted Bonferroni mean (IV2TLPFWBM) operator, interval-valued 2-tuple linguistic Pythagorean fuzzy weighted geometric Bonferroni mean (IV2TLPFWGBM) operator, generalized interval-valued 2-tuple linguistic Pythagorean fuzzy weighted Bonferroni mean (GIV2TLPFWBM) operator, generalized interval-valued 2-tuple linguistic Pythagorean fuzzy weighted geometric Bonferroni mean (GIV2TLPFWGBM) operator, dual generalized interval-valued 2-tuple linguistic Pythagorean fuzzy weighted Bonferroni mean (DGIV2TLPFWBM) operator, dual generalized interval-valued 2-tuple linguistic Pythagorean fuzzy weighted geometric Bonferroni mean (DGIV2TLPFWGBM) operator. Then the MADM methods are proposed with these operators. In the end, we utilize an applicable example for green supplier selection in green supply chain management to prove the proposed methods.																	1327-2314	1875-8827					2019	23	4					259	294		10.3233/KES-190417													
J								Model for evaluating the teaching effect of the college English public speaking course under the flipped classroom hybrid teaching mode with intuitionistic trapezoidal fuzzy numbers	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Multiple attribute decision making; intuitionistic trapezoidal fuzzy numbers; intuitionistic trapezoidal fuzzy Einstein weighted averaging (ITFEWA) operator; intuitionistic trapezoidal fuzzy Einstein ordered weighted averaging (ITFEOWA) operator; intuitionistic trapezoidal fuzzy Einstein hybrid aggregation (ITFEHA) operator; teaching effect; college English public speaking course	AGGREGATION OPERATORS; DECISION-MAKING; MEAN OPERATORS; TODIM METHOD; SETS; EXTENSION; SELECTION	In this paper, we investigate the multiple attribute decision making problems with the intuitionistic trapezoidal fuzzy information. Some operational laws of intuitionistic trapezoidal fuzzy numbers are introduced. Then, we proposed the some Einstein information aggregating operators with intuitionistic trapezoidal fuzzy information including intuitionistic trapezoidal fuzzy Einstein weighted averaging (ITFEWA) operator, intuitionistic trapezoidal fuzzy Einstein ordered weighted averaging (IT-FEOWA) operator and intuitionistic trapezoidal fuzzy Einstein hybrid aggregation (ITFEHA) operator, are proposed. An approach to multiple attribute decision making with intuitionistic trapezoidal fuzzy information is developed based on the ITFEWA operator. Finally, an illustrative example for evaluating the teaching effect of the college English public speaking course under the flipped classroom hybrid teaching mode is given to verify the developed approach.																	1327-2314	1875-8827					2019	23	4					295	301		10.3233/KES-190418													
J								Aesthetic evaluation of poetry translation based on the perspective of Xu Yuanchong's poetry translation theories with intuitionistic fuzzy information	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Intuitionistic fuzzy numbers; operational laws; intuitionistic fuzzy hamacher correlated geometric (IFHCG) operator; aesthetic evaluation	DECISION-MAKING PROBLEMS; AGGREGATION OPERATORS; MEAN OPERATORS; SETS; MODELS	In this paper, we first introduce some operations on the intuitionistic fuzzy sets, such as Hamacher sum, Hamacher product, Hamacher exponentiation, etc., and further develop the intuitionistic fuzzy Hamacher correlated geometric (IFHCG) operator. The prominent characteristic of this operator is that they can not only consider the importance of the elements or their ordered positions, but also reflect the correlation among the elements or their ordered positions. We have applied the IFHCG operator to multiple attribute decision making with intuitionistic fuzzy information. Finally, a practical example for aesthetic evaluation of poetry translation based on the perspective of Xu Yuanchong's poetry translation theories is used to illustrate the developed procedures.																	1327-2314	1875-8827					2019	23	4					303	309		10.3233/KES-190419													
J								An efficient approach for video retrieval by spatio-temporal features	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Video data base; feature extraction; video shot transitions; representative frame extraction; measurement of similarity; retrieval of videos	SHOT BOUNDARY DETECTION; EXTRACTION	The rapid development of video capture and information sharing technology has resulted in an overwhelming number of online video archives. This makes it difficult to retrieve videos from a large database using a traditional text-based system. The only solution to this problem is to retrieve videos based on their content. Ample algorithms have been hypothesized to reclaim videos from an enormous data base. Besides they could not diminish the time consumption and their coherence couldn't satisfy the users. We postulated the new approach which clubs the spatial features along with temporal features by making use of widespread video data and this amplifies the efficiency of video retrieval. In this regard, we move oncolour and motion features to get full data of video. Comparing the features of a demand video to those of a video to be retrieved from a server is easy, rather than comparing the entire content of the video. Therefore, designers of CBVR systems will follow two major steps to balance extraction and similarity of features [24].																	1327-2314	1875-8827					2019	23	4					311	316		10.3233/KES-190420													
J								Software reusability metrics prediction and cost estimation by using machine learning algorithms	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Object-Oriented Metrics; software reusability metrics; machine learning techniques; software cost estimation		In this research, a highly robust and efficient software design optimization model has been proposed for object-oriented programming based software solutions while considering the importance of quality and reliability. Due to a piece of information that software component reusability has allowed cost and time-efficient software design. The software reusability metrics prediction and cost estimation play a vital role in the software industry. Software quality prediction is an important feature that can be achieved a novel machine learning approach. It is a process of gathering and analyzing recurring patterns in software metrics. Machine learning techniques play a crucial role in intelligent decision making and proactive forecasting. This paper focuses on analyzing software reusability and cost estimation metrics by providing the data set. In the present world software, cost estimation and reusability prediction problem has been resolved using various newly developed methods. This paper emphasizes to solve the novel machine learning algorithms as well as improved Output layer self-connection recurrent neural networks (OLSRNN) with kernel fuzzy c-means clustering (KFCM). The investigational results confirmed the competence of the proposed method for solving software reusability and cost estimation.																	1327-2314	1875-8827					2019	23	4					317	328		10.3233/KES-190421													
J								Study on the commercial value evaluation of commercial photography with uncertain linguistic information	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Multiple attribute decision making; uncertain linguistic variables; uncertain linguistic hybrid weighted distance (ULHWD) measure; commercial value evaluation; commercial photography	LEAF-AREA INDEX; GROUP DECISION-MAKING; PRIORITIZED AGGREGATION OPERATORS; BONFERRONI MEAN OPERATORS; WAVE-FORM LIDAR; SUPPLIER SELECTION; TODIM METHOD; FUZZY-SETS; METHODOLOGY; RETRIEVAL	In this paper, we have developed the uncertain linguistic hybrid weighted distance (ULHWD) measure. We have proved both the uncertain linguistic weighted distance (ULWD) measure and the uncertain linguistic ordered weighted distance (ULOWD) measure are the special case of the ULHWD measure. The ULHWD measure first weights the given arguments, and then reorders the weighted arguments in descending order and weights these ordered arguments by the ULHWD weights, and finally aggregates all the weighted arguments into a collective one. Obviously, the ULHWD measure generalizes both the ULWD measure and ULOWD measure, and reflects the importance degrees of both the given argument and the ordered position of the argument. Furthermore, the ULHWD measure can relieve the influence of unfair arguments on the decision results by using the ULHWD weights to assign low weights to those "false" or "biased" ones. Finally, based on the ULHWD measure, we have proposed a practical method for evaluating the commercial value evaluation of commercial photography with uncertain linguistic information.																	1327-2314	1875-8827					2019	23	4					329	335		10.3233/KES-190422													
J								Using argumentation schemes to find motives and intentions of a rational agent	ARGUMENT & COMPUTATION										Multiagent systems; evidential reasoning in law; finding intentions; artificial intelligence	MODEL	Because motives and intentions are internal, and not directly observable by another agent, it has always been a problem to find a pathway of reasoning linking them to externally observable evidence. This paper proposes an argumentation-based method that one can use to support or attack hypotheses about the motives or intentions of an intelligent autonomous agent based on verifiable evidence. The method is based on a dialectical argumentation approach along with a commitment-based theory of mind. It is implemented using the Carneades Argumentation System, which already has 106 programmed schemes available to it and has an argument search tool. The method uses schemes, notably ones for abductive reasoning, argument from action to intention, argument from action to motive, and some new ones.																	1946-2166	1946-2174					2019	10	3					233	275		10.3233/AAC-190480													
J								Strong admissibility revisited: Theory and applications	ARGUMENT & COMPUTATION										Strong admissibility; grounded semantics; argument games	ARGUMENTATION	In the current paper, we re-examine the concept of strong admissibility, as was originally introduced by Baroni and Giacomin. We examine the formal properties of strong admissibility, both in its extension-based and in its labelling-based form, and analyse the computational complexity of the relevant decision problems. Moreover, we show that strong admissibility plays a vital role in discussion-based proof procedures for grounded semantics. In particular it allows one to compare the performance of alternative dialectical proof procedures for grounded semantics, and obtain some remarkable differences between the Standard Grounded Game and the Grounded Discussion Game.																	1946-2166	1946-2174					2019	10	3					277	300		10.3233/AAC-190463													
J								Computational opposition analysis using word embeddings: A method for strategising resonant informal argument	ARGUMENT & COMPUTATION										Informal argumentation; automated persuasion; semiotics; computational opposition analysis	SEMIOTICS; LOGIC	In informal argument, an essential step is to ask what will "resonate" with a particular audience and hence persuade. Marketers, for example, may recommend a certain colour for a new soda can because it "pops" on Instagram; politicians may "fine-tune" diction for different social demographics. This paper engages the need to strategise for such resonance by offering a method for automating opposition analysis (OA), a technique from semiotics used in marketing and literary analysis to plot objects of interest on oppositional axes. Central to our computational approach is a reframing of texts as proxies for thought and opposition as the product of oscillation in thought in response to those proxies, a model to which the contextual similarity information contained in word embeddings is relevant. We illustrate our approach with an analysis of texts on gun control from ProCon.org, implementing a three-step method to: 1) identify relatively prominent signifiers; 2) rank possible opposition pairs on prominence and contextual similarity scores; and 3) derive plot values for proxies on opposition pair axes. The results are discussed in terms of strategies for informal argument that might be derived by those on each side of gun control.																	1946-2166	1946-2174					2019	10	3					301	317		10.3233/AAC-190467													
J								Sampled-Data Adaptive Output Feedback Fuzzy Stabilization for Switched Nonlinear Systems With Asynchronous Switching	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptive fuzzy stabilization; asynchronous switching; nonlinear systems; sampled-data control; switched systems	GLOBAL STABILIZATION; TRACKING CONTROL; PRESCRIBED PERFORMANCE; STABILITY ANALYSIS; DESIGN	This paper considers the problem of sampled-data adaptive output feedback fuzzy stabilization for switched uncertain nonlinear systems associated with asynchronous switching. A state observer is designed to estimate the unmeasured states and fuzzy logic systems are employed to deal with the unknown nonlinear terms. Sampled-data controller and novel switched adaptive laws are constructed based on the recursive design method and an average dwell time constraint is given to ensure that the closed-loop system is stable. The proposed scheme is employed in a mass-spring-damper system to demonstrate its effectiveness.																	1063-6706	1941-0034				JAN	2019	27	1					200	205		10.1109/TFUZZ.2018.2881660													
J								Optimized RRT-A* Path Planning Method for Mobile Robots in Partially Known Environment	INFORMATION TECHNOLOGY AND CONTROL										Rapidly exploring random tree (RRT); mobile robots; path planning; morphological dilation; autonomous ground vehicles	CLUTTERED ENVIRONMENT; MOTION; CONTROLLER	This paper presents an optimized rapidly exploring random tree A* (ORRT-A*) method to improve the performance of RRT-A* method to compute safe and optimal path with low time complexity for mobile robots in partially known complex environments. OFtRT-A* method combines morphological dilation, goal-biased RRT, A* and cubic spline algorithms. Goal-biased RRT is modified by introducing additional step-size to speed up the generation of the tree towards the goal after which A* is applied to obtain the shortest path. Morphological dilation technique is used to provide safety for the robots while cubic spline interpolation is used to smoothen the path for easy navigation. Results indicate that ORRT-A* method demonstrates improved path quality compared to goal-biased RRT and RRT-A* methods. ORRT-A* is, therefore, a promising method in achieving autonomous ground vehicle navigation in partially known environments.																	1392-124X						2019	48	2					179	194		10.5755/j01.itc.48.2.21390													
J								Frequent Subgraph Mining Based Collaboration Pattern Analysis for Wikipedia	INFORMATION TECHNOLOGY AND CONTROL										collaboration; data mining; Wikipedia; article quality; social network analysis		Online knowledge collaborations, where distributed members without hierarchies self-organize themselves to create valuable contents, are prevalent in many open production systems such as Wikipedia, GitHub and social networks. While many existing studies from network science have been brought to analyze the general interactive behavior patterns embedded in these systems, how the collaborations influence the achievement outcomes has not been thoroughly investigated. In this paper, we mine the collaboration patterns from a micro perspective to deeply understand the relationships between the collaboration among participants and the qualities of the Wikipedia articles. In particular, the subgraphs contained in the collaboration networks derived from the Wikipedia revision histories are taken as the fundamental units to analyze the collaboration diversities from the subgraph properties such as size and topology. In contrast to the predefined static motifs adopted by the previous works, the collaboration subgraphs are directly found from Wikipedia dataset by a frequent subgraph mining algorithm GRAMI, which is able to capture the real dynamic collaboration patterns. Moreover, the relationships between the co-authors in the subgraphs are also discriminated to further explore the collaboration patterns. The experiments exhibit the statistical properties of the collaboration subgraphs and the efficiency of them as the metrics for the article quality assessments. We conclude that a small group of editors with relative frequent fixed collaboration patterns contribute more to the excellent article quality than the professional extents of arbitrary individuals in the collaboration group. This discovery confirms the common insight about collaboration that many heads are always better than one and concretely suggests a potential explanation for the increasing prevalence and success of the online knowledge collaborations.																	1392-124X						2019	48	2					195	210		10.5755/j01.itc.48.2.20028													
J								An Improved Biometric Multi-Server Authentication Scheme for Chang et al.'s Protocol	INFORMATION TECHNOLOGY AND CONTROL										Multiserver authentication; cryptanalysis; biometrics; remote authentication; attacks	KEY AGREEMENT; PASSWORD AUTHENTICATION; MUTUAL AUTHENTICATION; CHAOTIC MAP; SMART CARD; EFFICIENT; USER; CRYPTANALYSIS	The remote authentication has been advancing with the growth of online services being offered on remote basis. This calls for an optimal authentication framework other than single-server authentication. In this connection, the multi-server authentication architecture has been introduced in the literature that enables the users to avail variety of services of various service providers, using a single pair of identity and password. Lately, we have witnessed a few multi-server authentication protocols in the literature that had several limitations. One of those multi-server authentication protocols has been put forward by Chang et al. recently. Our analysis shows that the Chang et al.'s scheme is susceptible to impersonation threat, stolen smart card threat. In this study, we have reviewed the protocol thoroughly, and proposed an improved model, that is resistant to all known and identified threats. The formal security analysis along with discussion of informal analysis for contributed model is also presented in this study, besides performance and its evaluation analysis.																	1392-124X						2019	48	2					211	224		10.5755/j01.itc.48.2.17417													
J								Classification of Motor Imagery Using Combination of Feature Extraction and Reduction Methods for Brain Computer Interface	INFORMATION TECHNOLOGY AND CONTROL										Brain computer interface; feature extraction; feature reduction; combination of methods	EEG; PCA	The motor imagery (MI) based brain-computer interface systems (BCIs) can help with new communication ways. A typical electroencephalography (EEG)-based BCI system consists of several components including signal acquisition, signal pre-processing, feature extraction and feature classification. This paper focuses on the feature extraction step and proposes to use a combination of several feature extraction and feature reduction methods. The research presented in the paper explores the methods of band power, time domain parameters, fast Fourier transform and channel variance for feature extraction. These methods are investigated by combining them in pairs. The application of two feature extraction methods increases the number of selected features that can be redundant or irrelevant. The utilization of too many features can lead to wrong classification results. Therefore, the methods of feature reduction have to be applied. The following feature reduction methods are investigated: principal component analysis, sequential forward selection, sequential backward selection, locality preserving projections and local Fisher discriminant analysis. The combination of the methods of fast Fourier transform, channel variance and principal component analysis performed the best among the combinations of methods. The obtained classification accuracy of the above-mentioned combination of the methods is much higher than that of the individual feature extraction method. The novelty of the approach is based on consolidated sequence of methods for feature extraction and feature reduction.																	1392-124X						2019	48	2					225	234		10.5755/j01.itc.48.2.23091													
J								What Static Analysis Can Utmost Offer for Android Malware Detection	INFORMATION TECHNOLOGY AND CONTROL										Android malware; Android malware detection; static analysis; machine learning; Android		Malicious applications are widespread for Android despite the taken serious actions by the operating system. Static and dynamic analysis techniques are utilized to detect malware by identifying the signatures of malicious applications by inspecting both the resources and behaviors of malware, respectively. In this study, what static analysis can utmost offer to detect malware in Android ecosystem is discussed and experimented on commonly used datasets in the literature by proposing a novel Android malware detection approach based on static analysis techniques. With the proposed study, the effectiveness of novel static analysis features' in terms of detecting malware in Android ecosystem are proved. These features were underestimated by the related work in the literature. The experimental result shows that the proposed Android malware detection approach is very effective in terms of detecting Android malware. Each feature used by the proposed approach is evaluated by using different types of machine learning techniques in order to highlight its impact on detecting malware and inform the digital investigators. The accuracy of the proposed static analysis approach is calculated as high as 0.987 for 10,865 applications.																	1392-124X						2019	48	2					235	249		10.5755/j01.itc.48.2.21457													
J								A Comparison of the Control Schemes of Human Response to a Dynamic Virtual 3D Face	INFORMATION TECHNOLOGY AND CONTROL										Virtual 3D Face; Human Excitement; Predictive Input-Output Model; Minimum variance and Generalized Minimum Variance Control with Constraints	EMOTION RECOGNITION; EEG; MUSIC	This paper introduces the application of predictor-based control principles for the control of human response to a virtual 3D face. A dynamic woman 3D face is observed in virtual reality. We use changing distance-between-eyes in a 3D face as a stimulus - control signal. Human responses to the stimulus are observed using EEG-based excitement signals - output signal. The technique of dynamic systems identification which ensures stability and possible higher gain of the model for building a predictive input-output model of control plant is applied. Three predictor-based control schemes with a minimum variance or a generalized minimum variance control quality and constrained control signal magnitude and change rate are developed. High prediction accuracies and control quality are demonstrated by modelling results.																	1392-124X						2019	48	2					250	267		10.5755/j01.itc.48.2.21667													
J								Linear Data Model Based Adaptive ILC for Freeway Ramp Metering Without Identical Conditions on Initial States and Reference Trajectory	INFORMATION TECHNOLOGY AND CONTROL										Adaptive ILC; Linear data-model; Traffic control; Ramp metering; Random initial conditions; Iteration-varying target trajectory	ITERATIVE LEARNING CONTROL; DISCRETE-TIME-SYSTEMS; NONLINEAR-SYSTEMS; FLOW; CONTROLLER; DESIGN	Although freeway traffic system is conducted with a repeatable pattern day-to-day, the initial volume/or speed and the desired density of the traffic flow may vary with days due to the external disturbances. In this paper, a new linear data-model based adaptive iterative learning control (LDM-AILC) is proposed to address ramp metering in a macroscopic level freeway environment. A linear data-model is developed for the nonlinear macroscopic traffic flow model by introducing an equivalent dynamical linearization approach in the time domain. Then the LDM-AILC is designed with a feedback control law and a parameter updating law. The proposed scheme is data-driven intrinsically, where only the input and output data are required for the controller design and analysis. The convergence is shown by rigorous analysis without any identical conditions exposed on both the initial state and the reference trajectory. Extensive simulation results are provided to verify the effectiveness of the proposed LDM-AILC.																	1392-124X						2019	48	2					268	277		10.5755/j01.itc.48.2.20406													
J								Model Checking Based Approach for Compliance Checking	INFORMATION TECHNOLOGY AND CONTROL										Model discovery; process mining; model checking; compliance checking		Process mining is the set of techniques to retrieve a process model starting from available logging data. The discovered process model has to be analyzed to verify whether it respects the defined properties, i.e., the so-called compliance checking. Our aim is to use a model checking based approach to verify compliance. First, we propose an integrated-tool approach using existing tools as ProM (a framework supporting process mining techniques) and CADP (a formal verification environment). More precisely, the execution traces from a software system are extracted. Then, using the "Mine Transition System" plugin in ProM, we obtain a labelled transition system, that can be easily used to verify formal properties through CADP. However, this choice presents the "state explosion" problem, i.e., models discovered through the classical process mining techniques tend to be large and complex. In order to solve this problem, another custom-made approach is shown, which accomplishes a pre-processing on the traces to obtain abstract traces, where abstraction is based on the set of temporal logic formulae specifying the system properties. Then, from the set of abstracted traces, we discover a system described in Lotos, a process algebra specification language; in this way we do not build an operational model for the system, but we produce only a language description from which a model checking environment will automatically obtain the reduced corresponding transition system. Real systems have been used as case studies to evaluate the proposed methodologies.																	1392-124X						2019	48	2					278	298		10.5755/j01.itc.48.2.21724													
J								Information Flow Analysis of Combined Simulink/Stateflow Models	INFORMATION TECHNOLOGY AND CONTROL										Embedded Systems; Information Flow Analysis; UPPAAL; MATLAB; Simulink; Stateflow; Safety		Simulink and its state machine design toolbox Stateflow are widely-used industrial tools for the development of complex embedded systems. Due to the strongly differing execution semantics of Simulink and Stateflow, the analysis of combined models posesa difficult challenge, especially when considering their timing behavior. In this paper, we present a novel approach to relate the semantics of both the dynamic Simulink components and the Stateflow controller and use it to perform an information flow analysis on combined models. The key idea of our approach is that we analyze the information flow in a given model by computing an over-approximation of the control flow through the Simulink components, and deduce whether all control flow conditions combined permit information to flow on a given path or not. The main contributions of our control flow analysis approach are: (1) we identify timed path conditions which capture the conditions for time-dependent information flow on paths of interest for (discrete) Simulink components, and translate them into a UPPAAL timed automata representation, (2) we translate the Stateflow components to UPPAAL timed automata, and (3) we perform model checking on the translated set of automata in order to analyze the existence of paths in the combined model. With our approach, we safely rule out the existence of information flow on specific paths through a model, which enables us to reason about non-interference between model parts and the compliance with security policies. Furthermore, our approach presents a starting point to generate feasible, efficient test cases and to perform compositional verification. We demonstrate the applicability of our approach using two versions of a complex case study from the automotive domain consisting of multiple safety-critical components communicating over a shared bus system. For this example, an approach based on timed path conditions alone is sound but highly imprecise compared to our combined approach.																	1392-124X						2019	48	2					299	315		10.5755/j01.itc.48.2.21759													
J								Automatic Test Set Generation for Event-Driven Systems in the Absence of Specifications Combining Testing with Model Inference	INFORMATION TECHNOLOGY AND CONTROL										Event-Based testing; Android GUI testing; Model Learning; Model-Based exploration; Automata	ANDROID APPS	The growing dependency of human activities on software technologies is leading to the need for designing more and more accurate testing techniques to ensure the quality and reliability of software components. A recent literature review of software testing methodologies reveals that several new approaches, which differ in the way test inputs are generated to efficiently explore systems behaviour, have been proposed. This paper is concerned with the challenge of automatically generating test input sets for Event-Driven Systems (EDS) for which neither source code nor specifications are available, therefore we propose an innovative fully automatic testing with model learning technique. It basically involves active learning to automatically infer a behavioural model of the System Under Test (SUT) using tests as queries, generates further tests based on the learned model to systematically explore unseen parts of the subject system, and makes use of passive learning to refine the current model hypothesis as soon as an inconsistency is found with the observed behaviour. Our passive learning algorithm uses the basic steps of Evidence-Driven State Merging (EDSM) and introduces an effective heuristic for choosing the pair of states to merge to obtain the target machine. Finally, the effectiveness of the proposed testing technique is demonstrated within the context of event-based functional testing of Android Graphical User Interface (GUI) applications and compared with that of existing baseline approaches.																	1392-124X						2019	48	2					316	334		10.5755/j01.itc.48.2.21725													
J								Experimental Analysis of Hybrid Genetic Algorithm for the Grey Pattern Quadratic Assignment Problem	INFORMATION TECHNOLOGY AND CONTROL										computational intelligence; heuristics; hybrid genetic algorithms; combinatorial optimization; grey pattern quadratic assignment problem; component-based analysis	DISPERSION; SEARCH	In this paper, we present the results of the extensive computational experiments with the hybrid genetic algorithm (HGA) for solving the grey pattern quadratic assignment problem (GP-QAP). The experiments are on the basis of the component-based methodology where the important algorithmic ingredients (features) of HGA are chosen and carefully examined. The following components were investigated: initial population, selection of parents, crossover procedures, number of offspring per generation, local improvement, replacement of population, population restart. The obtained results of the conducted experiments demonstrate how the methodical redesign (reconfiguration) of particular components improves the overall performance of the hybrid genetic algorithm.																	1392-124X						2019	48	2					335	356		10.5755/j01.itc.48.2.23114													
J								Recovery of a Compressed Sensing CT Image Using a Smooth Re-weighted Function- Regularized Least-Squares Algorithm	INFORMATION TECHNOLOGY AND CONTROL										Compressed sensing; CT Image; Re-weighted function; Regularized Least-Squares; Sparse representation	FRAMEWORK; RECONSTRUCTION; NAVIGATION	It is challenging to recover the required compressed CT (Computed Tomography, CT) image, which is got by transferred through the internet or is stored in a signal library after being compressed. We present a recovery method for compressed sensing CT images. At present, minimizing 0-norm, 1-norm and p-norm is used to recover compressed sensing signals. However, sometimes 0-norm is an NP problem, 1-norm has no solution in theory and p-norm is not a convex function. We introduce a recovery method of compressed sensing signal based on regularized smooth convex optimization. In order to avoid solving the non-convex optimization problems and no solution condition, a convex function is designed as the objective function of optimization to fit 0-norm of signal and a fast iterative shrinkage-thresholding algorithm is proposed to find solution with the convergence speed is quadratic convergence. Experimental results show that our method has a sound recovery effect and is well suitable for processing big data of compressed CT images.																	1392-124X						2019	48	2					357	365		10.5755/j01.itc.48.2.21864													
J								Proficient 3-class classification model for confident overlap value based fuzzified aquatic information extracted tsunami prediction	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Aquatic behavior; sea turtle; earthworm; fuzzification; rules; opinion; confidence score; alert	BEHAVIOR	Tsunami occurrence has created havoc for humans. This unfortunate event however does provide some pre-alert signals which if carefully analyzed can help in pre-sensing this havoc. The signals can be in form of certain indicators in environment and in nature which can be sensed dynamically. Various studies have reported cases of animal behavioral signals prior to tsunami. These changes are a result of environmental changes before any such event occurrence. This paper proposes a confident fuzzified system derived over marine behavior to predict tsunamis (3CC(cov)FTP). The system uses an overlap function based linguistic rule derived algorithm creating three class of classifications. Using this system, aquatic behavior is studied, analyzed and classified to produce alerts in real time. Initially, the behavioral attributes are identified for both turtle and earthworm dataset which do produce anomalous signals. The attributed data is further analyzed to retrieve fuzzy rules using which alert, pre-alert or no alert overlapped opinions are extracted. The dataset for analysis includes the following derived parameters: adaptive electromagnetic field, Underwater count of particular specie Deviation in angle of motion (calculated for both sea turtles and earthworms defining their navigation activity per month and per day). The proposed system generates criteria three class categorization on the basis of the derived inputs. From the result obtained, confidence level for each extracted rule is formulated to derive optimized rule set that can serve mathematical formulation to in time alert generation. For prediction of any similar activity in the coming years, 2004 has been utilized as the baseline opinion year with resulting constraint as default rule. The tertiary classification formulated using the proposed algorithm classifies the behavior into three alert categories: Alert, Pre-Alert and No-Alert. Based on the quantified confident opinions, alerts primarily based on aquatic animal behavior can be generated for future years.																	1872-4981	1875-8843					2019	13	3					295	303		10.3233/IDT-180003													
J								Traffic signs classification by deep learning for advanced driving assistance systems	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Deep learning; convolutional neural network; adam optimization; self-driving car; traffic sign recognition		In this paper, we have proposed and developed a comprehensive Convolutional Neural Network (CNN) classifier "WAF-LeNet" to be used in traffic signs recognition and identification as an empowerment of autonomous driving technologies. The implemented architecture is a deep fifteen-layer network that has been selected after extensive trials to be fast enough to suit the designated application. The CNN got trained using Adam's optimization algorithm as a variant of the Stochastic Gradient Descent (SGD) technique. The learning process is carried out using the well-known "German Traffic Sign Dataset - GTSRB". The data has been partitioned into training, validation and testing data sets. Additionally, more random traffic signs images are collected from the web and further used to test the robustness of the proposed CNN classifier. The paper goes through the development process in details and shows the image processing pipeline harnessed in the development. The proposed approach proved successful in identifying correctly 96.5% of the testing data set and 100% of the robustness dataset with the much smaller and faster network than other counterparts.																	1872-4981	1875-8843					2019	13	3					305	314		10.3233/IDT-180064													
J								Video summarization using sparse representation of local descriptors	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Keyframe extraction; shot boundary detection; sparse representation; video summarization; local descriptors	MODEL; FEATURES; SVD	In this paper, a new method is proposed for video summarization and keyframe extraction using combined color, texture and motion information of the video as well as the sparse representation of local descriptors. To reduce the computational overhead of the algorithm, a non-uniform frame sampling strategy is employed using a shot detection algorithm. Subsequently, Binary Robust Invariant Scalable Keypoints (BRISK) and Histogram of Oriented Gradient (HOG) around the keypoints in the sampled frames are extracted as the local descriptors. By sparse representation and spatially partitioning of local features, the frame discriminating curve is constructed. We extract initial keyframes by detecting local maxima of frame discriminating curve and removing weak maxima. In order to remove redundant keyframes, we use similarity measure and motion model between the initial keyframes to extract final keyframes. Experimental results and the comparison of the results of the proposed algorithm with those of other methods show that the proposed algorithm enhances recall and F-measure indices.																	1872-4981	1875-8843					2019	13	3					315	327		10.3233/IDT-180112													
J								A semi-supervised learning model for intrusion detection	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Network security; intrusion detection systems; feature selection; semi-supervised learning		The Internet, e-commerce and telecommunication networks have become a driving force for modern economic growth and development throughout the world. They have also made the underlying network infrastructure the backbone of contemporary life, which enables us to connect to global flows of information, people and goods. Unfortunately, hostile attacks on various network infrastructures by malicious predators have grown significantly over recent years. In this paper, we propose a semi-supervised learning approach, STBoost, which is based on a self-training process and the standard boosting algorithm, for network intrusion detection. The approach has its unique features and can be used with a small set of labeled training data to build up initial models of normal and anomalous network activity behaviors, and then it employs additional unlabeled audit data to further refine the behavior models. We have conducted a number of experiments with the approach on the KDD Cup 99 data set and also compared it with another fuzziness based semi-supervised algorithm and several widely used supervised learning approaches. The experimental results have shown that the proposed semi-supervised approach represents a viable and competitive technique for detecting potential network intrusions.																	1872-4981	1875-8843					2019	13	3					343	353		10.3233/IDT-180127													
J								Figure/ground modeling combined with the context matching for visual object tracking	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Context matching; tracking; gaussian mixture model; kalman filter; object/background modeling	RECOGNITION; SELECTION	The aim of this paper is to track objects in a sequence of surveillance video frames by providing accurate object contours. Video analysis is performed in four key phases: (i) modeling the object/background by means of the Gaussian Mixture Model and the Kalman Filter, (ii) extracting a rough contour for the object through separating the moving objects from the still background, (iii) finding the corresponding points and the object location using the shape context matching algorithm, and eventually, (iv) extracting the exact shape contour. The suggested methodology is accurate enough to track objects in terms of scale change, translation, rotation, and partial occlusion and it can be used in real-time applications. Results on different video sequences establish the capability of this technique for a variety of applications such as traffic control cameras, speedometers, CCTV security cameras, photography and etc.																	1872-4981	1875-8843					2019	13	3					355	366		10.3233/IDT-180133													
J								A semi-supervised self-trained two-level algorithm for forecasting students' graduation time	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Data mining; machine learning; educational data; prediction model; semi-supervised learning; self-labeled algorithms; two-level classification algorithm; student academic performance		During the last decades, educational data mining has become a significant tool for the prediction of students' progress and performance. In this work, we present a new semi-supervised self-trained two-level classification algorithm for predicting students' graduation time. The proposed algorithm has three major features: Firstly, it identifies with high accuracy the students at-risk of not completing their studies; secondly, it classifies the students based on their expected graduation time; thirdly, it meaningfully relates the explicit classification information of labeled data with the information hidden in the unlabeled data. Our preliminary numerical experiments indicate that the proposed algorithm exhibits reliable predictions based on the students' performance during the first two years of their studies.																	1872-4981	1875-8843					2019	13	3					367	378		10.3233/IDT-180136													
J								A robust face emotion recognition approach through optimized SIFT features and adaptive deep belief neural network	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Face emotion recognition; feature extraction; SIFT; deep belief neural networks; whale optimization	ALGORITHM; CLASSIFICATION; IDENTIFICATION; REPRESENTATION	Facial expression is a significant indications for non verbal communication between individuals. The assignment of face emotion recognition is predominantly intricate for two reasons. Initial one is the non-existence of large database of training images and second issue is about classifying the emotions, which can be complex based on if the input image is static or not. Hence, this paper intends to propose a model, where two contributions are made both in the feature extraction and classification process. Initially, in feature extraction process, SIFT features are extracted as it is more associated with facial emotion features. However, numerous key point during SIFT extraction tends to provide redundant information. Hence, the key points are optimized using Whale Optimization Algorithm (WOA). Subsequently, the features extracted from the selected keypoints are multiplied with a weight factor, which has to be optimized by WOA. The resultant features are given to Deep Belief Network (DBN) for the classification of face emotion, in which the number of hidden neurons is also optimized by WOA along with feature weight. It is because of complexity that occurs with the utilization of both forward and reverse training in standard DBN model. Hence, after feature extraction and classification, the face emotion image is recognized accurately, while comparing the proposed Adaptive DBN (ADBN) using WOA over traditional classifiers and other optimization algorithms.																	1872-4981	1875-8843					2019	13	3					379	390		10.3233/IDT-190022													
J								Design and FPGA Implementation of Real-Time Edge Detectors Based on Interval Type-2 Fuzzy Systems	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Interval type-2 fuzzy systems; edge detection; real time image processing	HARDWARE IMPLEMENTATION; LOGIC CONTROLLERS	The present work presents a hardware architecture design for real- time edge detection based on Interval Type-2 fuzzy systems. The proposed architecture is designed in order to be implemented on FPGAs, taking advantage of the parallelism, pipelining and fixed-point notation strategies. The proposed approach of hardware implementation is put forward because one of the main obstacles of Interval Type-2 fuzzy systems in applications is that they require a high processing rate and this kind of systems demands a high computational cost. The proposed approach is evaluated in three aspects, the first aspect is the processing rate, the proposed architecture must be able to realize at least 24 frames per second of edge detection, that corresponds to a real-time edge detection, the second aspect is their noise robustness, and the third aspect is the error with respect to the software implementation, and this is relevant because the proposed architecture handles the data at a binary level and there exist an inherent error in the hardware implementation.																	1542-3980	1542-3999					2019	33	4-5			SI		295	320															
J								Performance Measurement Model for Software Development Teams Using Interval-valued Intuitionistic Fuzzy Analytic Hierarchy Process	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Performance measurement; intuitionistic fuzzy sets; analytic hierarchy process; software development		As a vital component of digitalization software development has become a very important issue for management. Software development is the process of, identifying, planning, coding, documenting, testing, and fixing operations accomplished for creating and maintaining different applications. Since the process is complex and various individuals work as a team in a project, performance management is a very important issue. In this study, a performance measurement model for software testing teams is proposed. The model employs interval-valued intuitionistic preference relations to weigh the performance indicators and to determine the performance score of the team members.																	1542-3980	1542-3999					2019	33	4-5			SI		321	339															
J								An Approach for Decision Making with Linguistic Intuitionistic Fuzzy Interval Value	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Lattice implication algebra; linguistic truth-valued intuitionisitic fuzzy lattice; linguistic intuitionistic fuzzy interval value; LGOWA operator	REASONING APPROACH; MODEL; ALGORITHM; ALGEBRA; SCALE	In the reality, people use linguistic value rather than numerical information to express their evaluations or preferences in decision making problems. To deal with both positive and negative information,we propose a linguistic decision making approach based on linguistic intuitionistic fuzzy interval pair. The fuzzy logical algebra is an important tool to deal with fuzzy information. As a prelinear residuated structure, triangle algebras are used to develop a logic that formally characterizes tautologies (true formulas) in interval valued residuated lattices. In this paper, based on 2n-element linguistic truth-valued lattice implication algebra LI2n , = (LI2n, boolean OR, boolean AND, ->, ((h(n), t), (h(n), f)), ((h(1), t), (h(1), f ))), we discuss the 6-element linguistic truth-valued intuitionistic fuzzy lattice triangular algebra structure,transforming it into the linguistic truth-valued intuitionistic fuzzy interval value by using two unary operations mu and nu of triangular algebra, defining the transformation function [mu circle dot nu] and comparing the interval values by using scoring functions. Then combine the interval values with lingusitic generalized ordered weighted averaging operator(referred to as LGOWA) and apply it to decision-making. The illustration example shows that the proposed approach more effective for decision making under a fuzzy environment with both positive and negative linguistic truth values.																	1542-3980	1542-3999					2019	33	4-5			SI		341	362															
J								A Novel Hybrid Approach Based on Rough Set for Classification: An Empirical Comparative Study	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Hybrid approach; Uncertainty; Rough set; Machine learning; Classification; Extracting decision rules	UNCERTAINTY; FUZZY	Uncertainty learning is an essential research direction of rough set theory. Uncertainty is defined as a situation with inadequate information and includes three types: inexactness, unreliability and ignorance; it is not merely the absence of knowledge. However, uncertainty can still exist in cases where there is a big chunk of information. In this regard, Rough Set Theory (RST) is a powerful mathematical model that deals with uncertain information. The RST offers a practical method for extracting decision rules from datasets. To deal with the uncertainty within the datasets, we initially propose a new hybrid approach that combines RST with Machine Learning (ML) algorithms. Furthermore, a novel algorithm based on RST (RSTPNN) is proposed by the idea of combining probability based Naive Bayes (NB) and nearest neighbor based K-Nearest Neighbors (KNN) to efficiently improve the classification performance. For experimental validation, we use 12 different datasets from the Open ML and UCI repositories. The empirical results evidently show that the proposed method may improve classification performance.																	1542-3980	1542-3999					2019	33	4-5			SI		363	380															
J								Prioritization of Investment Alternatives for a Hospital by Using Neutrosophic CODAS Method	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										CODAS; neutrosophic sets; investment selection; investment in hospital	SITE SELECTION; AHP; TOPSIS	Hospital investment analysis is significant analysis for decision making. There are numerous criteria are effective about investment selection in hospital. Neutrosophic sets are characterized by three paramaeters which are truth-membership (T) indeterminacy-membership (I), and falsity-membership (F). COmbinative Distance-based ASsessment (CODAS) method which is a new multi criteria decision making (MCDM) technique. In this study, we analyze investment analysis for hospitals with interval-valued neutrosophic CODAS method. The same problem is solved with interval-neutrosophic TOPSIS. The results of interval-valued neutrosophic CODAS and interval-valued neutrosophic TOPSIS are compared and the suggestions for future work about investment analysis in conclusion are presented.																	1542-3980	1542-3999					2019	33	4-5			SI		381	396															
J								Improvements of Categorical Propositions on Consistency and Computability	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Categorical proposition; particular quantifier; partial quantifier; existential quantifier; syllogism	NATURAL-LANGUAGE QUANTIFIERS	The author asserts that Aristotelian categorical propositions (ACPs), formalized as the structure "QX apply to/ not to Y" where Q is either universal or particular, and X, Y are terms, remain the two limitations: 1) the particular quantifier is ambiguous for its restricted reading by Euler as "non-empty but not universal", and the unrestricted reading by Gergonne as "non-empty and possibly universal"; 2) Y formally lacks a quantifier to be modified, rendering vaguely, at least insufficiently expressing its quantity. So, expanded categorical propositions (ECPs) with dyadic (in two places in left-hand of both X and Y) and generalized quantifiers are proposed for overcoming the two limitations. ECPs are proved to be calculated logically and operated mathematically; moreover, these two kinds of computations are corresponding, i.e., ECPs and their mathematical models are in homomorphic algebraic structures, which enable ECPs' logical calculi to be consistent and running in precisely mathematical significance.																	1542-3980	1542-3999					2019	33	4-5			SI		397	413															
J								A Modified Interval-valued Pythagorean Fuzzy CODAS Method and Evaluation of AS/RS Technologies	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										AS/RS technology; Pythagorean fuzzy sets; CODAS; multicriteria decision making	DECISION	AS/RS technology selection is a critical issue for production companies. This process usually involves intangible criteria to consider them under vague and imprecise environment. As a new extension of ordinary fuzzy sets, Pythagorean fuzzy sets (PFSs) are characterized by a membership degree and a non-membership degree satisfying the condition that their squared sum is equal to or less than 1. COmbinative Distance-based Assessment (CODAS) method is relatively a new multi criteria decision making (MCDM) technique in the literature. We modify the CODAS method by making a new distance measurement definition. The aim of this paper is to apply the modified Pythagorean fuzzy extension of CODAS method for the selection of the best AS/RS technology.																	1542-3980	1542-3999					2019	33	4-5			SI		415	429															
J								Edge Detection Approach Based on Type-2 Fuzzy Images	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Interval type-2 fuzzy sets; type-1 fuzzy sets; general type-2 fuzzy sets; fuzzy edge detection; fuzzy images	INTERVAL TYPE-2; CLUSTERING-ALGORITHM; LOGIC SYSTEMS; SETS; CONTROLLERS	This paper presents a new fuzzy edge detection method applied on fuzzy images. The aim of this approach is that each pixel value in a digital image can be extended to be a fuzzy number; therefore, the images can be fuzzified using interval type-2 (IT2 FS), general type-2 (GT2 FS) and type-1 fuzzy sets (T1 FS). In an image processing system when an image is captured by any acquisition hardware, there are diverse factors that could introduce noise to the image (distance, environment and lighting) and consequently add uncertainty, varying the brightness or color information; so, in this approach the idea is to have a better handling of the imprecision that could exist in each numerical pixel. Due to the fact that we are not sure if each pixel value is precise, we can add a level of uncertainty in the image processing and handle each crisp pixel as a fuzzy pixel. We are presenting results using different types of membership functions (MFs), Triangular and Trapezoidal for the T1 and IT2 FS. For the GT2 FS we use two types of MFs: Triangular on the primary MY and Gaussian in the secondary MFs and Trapezoidal on the primary MY and Gaussian in the secondary MFs. The parameters for the IT2 and GT2 MFs are obtained using a Genetic Algorithm. According with the results, we provide statistical evidence to confirm that the results achieved by the GT2 FS have a significant advantage with respect to the T1 and IT2 FS.																	1542-3980	1542-3999					2019	33	4-5			SI		431	458															
J								Selection Among GSM Operators Using Pythagorean Fuzzy WASPAS Method	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Intuitionistic fuzzy sets; WASPAS method; weighted sum; weighted product; IFS2 sets; pythagorean fuzzy sets		GSM operators serve a huge amount of customers and this causes difficulties in meeting the customer requirements. Selection of the GSM operator having the best quality of service is a multicriteria decision making problem under vague and imprecise evaluations rather than exact numerical evaluations. Pythagorean fuzzy sets (PFS) let experts assign membership and nonmembersip degrees in a wider area than it is in ordinary intuitionistic fuzzy sets. In this paper, we employ Pythagorean fuzzy sets for the extension of WASPAS method. WASPAS is a multicriteria method integrating simple additive weighting and weighted product methods. Single valued PFS are used in the proposed WASPAS method. We illustrate the application of the proposed Pythagorean fuzzy WASPAS through a selection problem among communication firms.																	1542-3980	1542-3999					2019	33	4-5			SI		459	469															
J								Estimation of Fuzzy Reliability: A Case Study for Flash Vessel in Ammonia Storage Tank	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Fault teee analysis; fuzzy probability; risk assessment; fuzzy reliability; flash vessel in amonia tank; tanaka fuzzy arithmetic operator	FAULT-TREE ANALYSIS; ANALYSIS FFTA; RISK ANALYSIS; LEAKAGE; OIL	Fault tree analysis is one of the most effective techniques for estimating the frequency of occurrence of hazardous events in probabilistic risk assessment study. The uncertainty present in fault tree evaluation can also be modeled using fuzzy set-theoretic operations. In this paper, we have attempted to estimate reliability of hazardous event via Top Event Probability (FTEP) in fuzzy fault tree analysis. The case study relates to flash vessel in an ammonia tank in a large Fertilizer complex in Mumbai, India.																	1542-3980	1542-3999					2019	33	4-5			SI		471	479															
J								Extension of CODAS with Spherical Fuzzy Sets	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Spherical fuzzy sets; multicriteria decision making; CODAS; negative ideal solution; spherical distance	SELECTION; DISTANCE; ENERGY	The extensions of ordinary fuzzy sets such as intuitionistic fuzzy sets (IFS), Pythagorean fuzzy sets (PFS), and neutrosophic sets (NS), whose membership functions are based on three dimensions, aim to describe expert's judgments more informatively and explicitly. Generalized three-dimensional spherical fuzzy sets are introduced with their arithmetic, aggregation, and defuzzification operations in the literature. Our aim is to extend classical CODAS (COmbine Distance-based Assessment) method to spherical fuzzy CODAS (SF-CODAS) method and to show its application with an illustrative example. The paper also defines spherical fuzzy distances based on the membership, nonmembership and hesitancy parameters. To calculate the desirability of an alternative, SF-CODAS method uses the Euclidean distance as the primary and the Spherical distance as the secondary measure. These distances are calculated based on the negative ideal solution (NIS) and the alternative that has the greatest distance to NIS is the best alternative. The paper also carries out comparative and sensitivity analyses between IF-TOPSIS, IF-CODAS and SF- CODAS.																	1542-3980	1542-3999					2019	33	4-5			SI		481	505															
J								What to consider about events: A survey on the ontology of occurrents	APPLIED ONTOLOGY										Ontology; occurrents; perdurants; events; processes		This work presents a review of the ideas that are currently in use on the ontology-based conceptual modeling of occurrents (sometimes referred to as "events", "perdurants", or "processes"). It collects such ideas from a set of 11 ontologies, which includes some of the most important and widely used upper ontologies (i.e., BFO, UFO, DOLCE, YAMATO, SUMO, GFO). We analyze the ontologies with respect to the definition of occurrent they present and their understanding about participation, mereology, and causation. The commitments regarding these four facets of occurrents are gathered in three categories (pervasive aspects, complementary aspects, and conflicting aspects). Additionally, we identify the main occurrent classification criteria used to branch the taxonomy of the ontologies. These findings are summarized in two tables at the end of the paper, which may be used by modeling practitioners as reference. The review shows that the considered ontologies agree in a significant set of common aspects as well as present some relevant divergences. However, there is a considerable set of non-conflicting, complementary aspects scattered among the diverse ontologies. It suggests an opportunity for efforts aiming to harmonize those views in a single approach that may enrich the analysis and representation of occurrents.																	1570-5838	1875-8533					2019	14	4					343	378		10.3233/AO-190217													
J								Embodied contextualization: Towards a multistratal ontological treatment	APPLIED ONTOLOGY										Embodiment; simulation; linguistic semantics; ontological analysis; formal ontology	LANGUAGE; SPACE; KNOWLEDGE; MOTION; COMPREHENSION	A fundamental issue concerning the treatment of meaning in context is how to deal with the extremely flexible relationship that appears to hold between descriptions, which are taken as the exchangeable bearers of meaning, and the actual contexts which those descriptions are taken to pick out. Such contexts appear always only to be suggested, or constrained, by descriptions of contexts and so relating levels of description, such as linguistic utterances, to actual contexts of use, such as a situated, fully embodied environment in which language users find themselves, remains an unsolved challenge. The present article sets out a framework and illustrative implementation of an approach to contextualization that combines ontological engineering principles and situated embodied simulations. For concreteness, we illustrate the approach within an already established architecture for situated robotic agents in order to allow implementation and experimentation in a manner that is not generally accessible when considering linguistic analysis alone. The paper then proposes a hybrid, multi-level architecture that contextualizes linguistic utterances by means of embodied simulations, which then serve further as contextualization constraints on semantic interpretation.																	1570-5838	1875-8533					2019	14	4					379	413		10.3233/AO-190218													
J								Context for language understanding by intelligent agents	APPLIED ONTOLOGY										Context; natural language understanding; intelligent agents; computational semantics	NLP	This paper describes the layers of context leveraged by language-endowed intelligent agents (LEIAs) during incremental natural language understanding (NLU). Context is defined as a combination of (a) the perceptual stimuli available to the agent at the given point in time, and (b) the knowledge elements and reasoning activated at the given stage of the agent's interpretation of those stimuli. This approach to NLU addresses the treatment of a large number of difficult linguistic phenomena that are essential for high-quality NLU but are not being tackled by the knowledge-lean approaches that are typical of modern-day natural language processing. Although LEIAs are being developed as components of prototype application systems, this paper is not about implementations or evaluations - its contribution is conceptual, with everything described applicable to any artificial intelligent agent environment.																	1570-5838	1875-8533					2019	14	4					415	449		10.3233/AO-190216													
J								Implicit entity linking in tweets: An ad-hoc retrieval approach	APPLIED ONTOLOGY										Implicit entity linking; Semantic retrieval; DBpedia; Knowledge graph		Within the context of Twitter analytics, the notion of implicit entity linking has recently been introduced to refer to the identification of a named entity, which is central to the topic of the tweet, but whose surface form is not present in the tweet itself. Compared to traditional forms of entity linking where the linking process revolves around an identified surface form of a potential entity, implicit entity linking relies on contextual clues to determine whether an implicit entity is present within a given tweet and if so, which entity is being referenced. The objective of this paper, while introducing and publicly sharing a comprehensive gold standard dataset for implicit entity linking, is to perform the task of implicit entity linking The dataset consists of 7,870 tweets, which are classified as either containing implicit entities, explicit entities, both, or neither. The implicit entities are then linked to three levels of entities on Wikipedia, namely coarse-grained level, e.g., Person, Fine-grained level, e.g., Comedian, and the actual entity, e.g., Seinfeld. The proposed model in this work formulates the problem of implicit entity linking as an ad-hoc document retrieval process where the input query is the tweet, which needs to be implicitly linked and the document space is the set of textual descriptions of entities in the knowledge base. The novel contributions of our work include: 1) designing and collecting a gold standard dataset for the task of implicit entity linking; 2) defining the implicit entity linking process as an ad-hoc document retrieval task; and 3) proposing a neural embedding-based feature function that is interpolated with prior term dependency and entity-based feature functions to enhance implicit entity linking We systematically compare our work with existing work in this area and show that our method is able to provide improvements on a number of retrieval measures.																	1570-5838	1875-8533					2019	14	4					451	477		10.3233/AO-190215													
J								WMFP-Outlier: An Efficient Maximal Frequent-Pattern-Based Outlier Detection Approach for Weighted Data Streams	INFORMATION TECHNOLOGY AND CONTROL										outlier detection; weighted maximal frequent-pattern mining; weighted data stream; deviation indices; data mining		Since outliers are the major factors that affect accuracy in data science, many outlier detection approaches have been proposed for effectively identifying the implicit outliers from static datasets, thereby improving the reliability of the data. In recent years, data streams have been the main form of data, and the data elements in a data stream are not always of equal importance. However, the existing outlier detection approaches do not consider the weight conditions; hence, these methods are not suitable for processing weighted data streams. In addition, the traditional pattern-based outlier detection approaches incur a high time cost in the outlier detection phase. Aiming at overcoming these problems, this paper proposes a two-phase pattern-based outlier detection approach, namely, WMFP-Outlier, for effectively detecting the implicit outliers from a weighted data stream, in which the maximal frequent patterns are used instead of the frequent patterns to accelerate the process of outlier detection. In the process of maximal frequent-pattern mining, the anti-monotonicity property and MFP-array structure are used to accelerate the mining operation. In the process of outlier detection, three deviation indices are designed for measuring the degree of abnormality of each transaction, and the transactions with the highest degrees of abnormality are judged as outliers. Last, several experimental studies are conducted on a synthetic dataset to evaluate the performance of the proposed WMFP-Outlier approach. The results demonstrate that the accuracy of the WMFP-Outlier approach is higher compared to the existing pattern-based outlier detection approaches, and the time cost of the outlier detection phase of WMFP-Outlier is lower than those of the other four compared pattern-based outlier detection approaches.																	1392-124X						2019	48	4					505	521		10.5755/j01.itc.48.4.22176													
J								Mining Hot-Personae Approach Based on Local Social Microblog Graph	INFORMATION TECHNOLOGY AND CONTROL										Microblog; focus mining; graph clustering; online social network	LINK PREDICTION; FOLLOWEE RECOMMENDATION; NETWORKS	With the increasing popularity of online social media platforms, netizens always chat with their friends and share information, such as what they like in their daily lives, on these platforms. Netizens publish tons of information on social platforms every day. These platforms converge many people and information. The processes by which the publishers find the sharers who are interested in their publications and the sharers find some interesting things and information in what the publishers published have resulted in the challenge of retrieving information from social network fields. To address these issues, we propose a novel algorithm, named Hot Persona Mining, to analyze the users' focus personae from microblog posts in the online social networks. During mining, we first utilize local-based graph clustering to establish the nearest neighbor nodes of target users. Then, we mine users' focused personae entities from their neighbors' published microblog posts in different periods. Then, we construct the users' active score vector and their interest matrix to mine the hot personae in every local social graph. The experimental results show that our algorithm effectively mines current focus of the target user, and exhibits good performance as shown by its precision, recall and F-measures.																	1392-124X						2019	48	4					522	537		10.5755/j01.itc.48.4.21950													
J								Tracking Video Target via Particle Filtering on Manifold	INFORMATION TECHNOLOGY AND CONTROL										Target tracking; particle filtering; covariance matrix; Lie group	VISUAL TRACKING	Most of existing particle filtering-based video target tracking algorithms are in Euclidean space, when object posture and scale size changes, and to track high dimensional system, it is difficult to guarantee the tracking effect. This paper describes the covariance descriptor to represent the object image region, the geometric deformation of the object image region can be realized by an affine transformation, and the affine transformation matrix is one element of the Lie group. Then particle filter algorithm based on lie group of manifold is proposed, the video tracking system state lies directly on a low dimensional manifold, state samples are drawn moving on the manifold geodesics, thus state space of intrinsic geometrical characteristic can be in full use, which provides a new idea for improving the tracking efficiency and robustness. Simulation results show that object in the case of geometric deformation including scale size changes, rotating, etc. The proposed manifold particle filtering algorithm can still realize target tracking well and improve the real-time performance.																	1392-124X						2019	48	4					538	544		10.5755/j01.itc.48.4.23939													
J								The Placement of Virtual Machines Under Optimal Conditions in Cloud Datacenter	INFORMATION TECHNOLOGY AND CONTROL										Cloud Computing; Virtualization; Sine Cosine Algorithm; Source Management; Power Consumption; Multi-Verse Optimizer	ALGORITHM	Nowadays cloud computing is progressing very fast and has resulted in advances in other technologies too. Cloud computing provides quite a convenient platform for millions of users to use computing resources through the internet. Cloud computing provides the possibility of only concentrating on business goals instead of expanding hardware resources for users. Using virtualization technology in computing resources results in the efficient use of resources. A challenging work in virtualization technology is the placement of virtual machines under optimal conditions on physical machines in cloud data centers. Optimal placement of virtual machines on physical machines in cloud data centers can lead to the management of resources and prevention of the resources waste. In this paper, a new method is proffered based on the combination of hybrid discrete multi-object sine cosine algorithm and multi-verse optimizer for optimal placement. The first goal of the proposed approach is to decrease the power consumption which is consumed in cloud data centers by reducing active physical machines. The second goal is to cut in resource wastage and managing resources using the optimal placement of virtual machines on physical machines in cloud data centers. With this approach, the increasing rate of virtual migration to physical machines is prevented. Finally, the results gained from our proposed algorithm are compared to some algorithms like the first fit (FF), virtual machine placement ant colony system (VMPACS), modified best fit decreasing (MBFD).																	1392-124X						2019	48	4					545	556		10.5755/j01.itc.48.4.23062													
J								Model Predictive Control of UCG: An Experiment and Simulation Study	INFORMATION TECHNOLOGY AND CONTROL										UCG; Coal; Gasification; Prediction; Model Predictive Control; MARS; Matlab-Simulink	UNDERGROUND COAL-GASIFICATION; OPTIMIZATION; COMBUSTION; TRACKING	Underground coal gasification (UCG) is a potential technology that enables to mine coal without traditional mining equipment. The coal is gasified deep in underground and produced syngas is processed on the surface. The most important technical problem in UCG is unstable quality of syngas and control. This paper proposes advanced control based on an adaptive predictive controller. The maintaining of desired calorific value depends on flow rates of gasification agents injected to the underground geo-reactor and controlled exhaust. The paper proposes a physical model of UCG technology and applies a method of multivariate adaptive regression splines (MARS) to model the gasification process. This method satisfactorily approximates nonlinearity in the process variables. The paper proposes adaptive model predictive control (MPC) using online model estimation and applied it on the MARS model of UCG that imitates the real process. The results have shown that optimization of manipulation variables can replace manual control in UCG. Getting better quality of syngas depends on setpoints, optimized manipulation variables, and constraints used in MPC. In simulations, the adaptive MPC has shown better performance in comparison with manual and PI control.																	1392-124X						2019	48	4					557	578		10.5755/j01.itc.48.4.23303													
J								A Novel CCA Secure Verifiable Authenticated Encryption Scheme Using BSDH and q-SDH Assumptions	INFORMATION TECHNOLOGY AND CONTROL										verifiable authenticated encryption; non-delegatable; bilinear square Diffie-Hellman problem; q-strong Diffie-Hellman problem; public key system	SIGNATURE SCHEME	When it comes to secure transactions online, the requirements of confidentiality and authenticity are usually concerned the most. The former prevents unauthorized reading, while the latter ensures authorized access. Hybrid cryptographic mechanisms such as authenticated encryption (AE) schemes, simultaneously combine the functions of public key encryption and digital signature. Some AE schemes also provide a cost-free arbitration mechanism to deal with the signer's later repudiation. Such schemes have been found to have numerous practical applications like on-line credit card transactions, confidential contract signing and the protection of digital evidence, etc. However, a designated verifier should also have the ability to convince any third party that he/ she is indeed the intended recipient. In this paper, the author presents a novel verifiable authenticated encryption (VAE) scheme with the functionality of recipient proof. Furthermore, the paper shows that the proposed VAE scheme is non-delegatable and provably secure under the random oracle proof models. A non-delegatable hybrid cryptographic scheme provides a higher security level even if the shared common key is compromised. Specifically, the author of the paper will demonstrate that the designed construction is proved secure against adaptive chosen-ciphertext attacks (CCA2) assuming the hardness of Bilinear Square Diffie-Hellman Problem (BSDHP) and secure against adaptive chosen-message attacks (CMA) assuming the hardness of q-Strong Diffie-Hellman Problems (q-SDHP).																	1392-124X						2019	48	4					579	589		10.5755/j01.itc.48.4.23454													
J								An Adaptive Hybrid Ant Colony Optimization Algorithm for the Classification Problem	INFORMATION TECHNOLOGY AND CONTROL										ant colony optimization; artificial bee colony optimization; classification; classification rule; rule evaluation function	RULES	Classification is an important data analysis and data mining technique. Taking into account the comprehensibility of the classifier generated, an adaptive hybrid ant colony optimization algorithm called A_HACO is proposed which can effectively solve classification problem and get the comprehensible classification rules at the same time. The algorithm incorporates the artificial bee colony optimization strategy into the ant colony algorithm. The ant colony global optimization process is used to adaptively select the appropriate rule evaluation function for the data set given. Based on the classification rules obtained, the artificial bee colony optimization strategy is used to tackle the continuous attributes for further optimization of classification rules. This approach is evaluated experimentally using different standard real datasets, and compared with some proposed related classification algorithms. It shows that A_HACO can adaptively select the appropriate rule evaluation function and has better accuracy compared with related works.																	1392-124X						2019	48	4					590	601		10.5755/j01.itc.484.22330													
J								Nonsingular Fast Terminal Sliding Mode Based Model-Free Control: Application to Glycemia Regulation Systems	INFORMATION TECHNOLOGY AND CONTROL										MFC; NFTSM-MFC; time-delay estimation; glycemia regulation	BLOOD-GLUCOSE CONTROL; PREDICTIVE CONTROL ALGORITHM; DESIGN	Anew Model-Free Control (MFC) is derived to enhance the control performance of the well-known Nonlinear Integral Backstepping based MFC (NIB-MFC). A Nonsingular Fast Terminal Sliding Mode (NFTSM) component is added to NIB-MFC, which makes possible to compensate the estimation error of the time-delay estimation module of NIB-MFC. The obtained in this way new control structure is called NFTSM-MFC. The system stability with NFTSM-MFC is proved and the application of NFTSM-MFC for glycemia regulation is considered. The performances of NFTSM-MFC are compared with those of the NIB-MFC and the intelligent proportional control for a glucose-insulin model of type 1 diabetes patients under a long term simulation.																	1392-124X						2019	48	4					602	617		10.5755/j01.itc.48.4.24541													
J								Detection of Network Intrusion Threat Based on the Probabilistic Neural Network Model	INFORMATION TECHNOLOGY AND CONTROL										Probabilistic Neural Network; network security; Principal Component Analysis; detection algorithm		With the popularity of the Internet, people's lives are becoming more and more convenient. However, the network security problems are becoming increasingly serious. This paper, aiming to better protect users' network security from the internal and external malicious attacks, briefly introduces the probabilistic neural network and principal component analysis method, and combines them for detection of network intrusion data. Simulation analysis of Probabilistic Neural Network (PNN) and Principal Component Analysis-Probabilistic Neural Network (PCA-PNN) are carried out in MATLAB software. The results suggest that the Principal Component Analysis (PCA) algorithm greatly reduce the dimension of the original data and the amount of calculation. Compared with PNN, PCA-PNN has higher accuracy and precision rate, lower false alarm rate, and faster detecting speed. Moreover, PCA-PNN has better detecting performance when there are few training samples. In summary, PCA-PNN can be used for the detection of network intrusion threat.																	1392-124X						2019	48	4					618	625		10.5755/j01.itc.48.4.24036													
J								Incorporating Semantic Word Representations into Query Expansion for Microblog Information Retrieval	INFORMATION TECHNOLOGY AND CONTROL										Microblog retrieval; query expansion; word embeddings; word vectors; information retrieval	MODEL	Microblog information retrieval has attracted much attention of researchers to capture the desired information in daily communications on social networks. Since the contents of microblogs are always non-standardized and flexible, including many popular Internet expressions, the retrieval accuracy of microblogs has much room for improvement. To enhance microblog information retrieval, we propose a novel query expansion method to enrich user queries with semantic word representations. In our method, we use a neural network model to map each word in the corpus to a low-dimensional vector representation. The mapped word vectors satisfy the algebraic vector addition operation, and the new vector obtained by the addition operation can express some common attributes of the two words. In this sense, we represent keywords in user queries as vectors, sum all the keyword vectors, and use the obtained query vectors to select the expansion words. In addition, we also combine the traditional pseudo-relevance feedback query expansion method with the proposed query expansion method. Experimental results show that the proposed method is effective and reduces noises in the expanded query, which improves the accuracy of microblog retrieval.																	1392-124X						2019	48	4					626	636		10.5755/j01.itc.48.4.22487													
J								Research on Intention Recognition Method Based on Radial Basis Function Neural Network	INFORMATION TECHNOLOGY AND CONTROL										Machine Learning; Radial Basis Function Neural Network; Human-Robot Collaboration; Intention Recognition		A robot should be endowed with certain collaboration experience to recognize human's behavioral intention. This paper provides a method based on machine learning to recognize the collaborator's intention. A radial basis function neural network model was built for offline practice of a robot to recognize intention. Some collaboration skills can be obtained by the robot by building a map between the collaborator's intention and the system state, deducing human's intention based on the dynamic characteristics of collaborator and robot and taking the collaborator's intention as the feedforward information for controlling the robot so as to estimate the human's intention online based on collaborator's force and robot's motion characteristics during collaboration. The proposed method can overcome the difficulties in building the human-robot collaboration model by traditional method, especially the complicated human motion model, and difficulties in estimation of impedance parameters of human body. An experiment was conducted on a motion platform with single degree of freedom. The results prove that the collaborator's force is reduced while synchronization of human-robot collaboration is improved, so that the compliance of collaborated motion is also improved.																	1392-124X						2019	48	4					637	647		10.5755/j01.itc.48.4.23031													
J								Fuzzy Logic Based Adaptive Proportional Integral Sliding Mode Control for Active Suspension Vehicle Systems: Kalman Filtering Approach	INFORMATION TECHNOLOGY AND CONTROL										Fuzzy-logic-based; Sliding mode control; Kalman filtering; Suspension vehicle systems; hardware-in-the-loop (HiL)	NONLINEAR-SYSTEMS; CONTROL DESIGN	This paper deals with the problem of synthesizing a fuzzy-logic-based adaptive proportional-integral sliding mode control (FAPISMC) for active suspension systems based on Kalman filtering approach. To improve the performance of the controller and eliminate the effect of the chattering, the switching input is designed based on the fuzzy-logic-based approach with a minimum number of rules. In order to facilitate the stability analysis, the estimation of the state variables is used in designing the sliding surface platform. Furthermore, the gain of the controller is updated by an adaptive law to avoid any pre-knowledge of the disturbance amplitude. Subsequently, the proposed approach is more implementable in real-world processes. Finally, in order to illustrate the effectiveness and merits of the proposed approach, a suspension system is considered and simulated by the real-time hardware-in-the-loop (HiL). In this example, a quarter-car model of suspension systems is considered. Then, the obtained real-time results are compared with the linear quadratic regulator approach.																	1392-124X						2019	48	4					648	659		10.5755/j01.itc.48.4.20590													
J								Robust Backstepping Sliding Mode Control with L-2-Gain Performance for Reference Input Wheel Slip Tracking of Vehicle	INFORMATION TECHNOLOGY AND CONTROL										Wheel slip control; Backstepping method; Sliding mode control; L-2-gain stable; Robustness	ANTILOCK BRAKING SYSTEM; DESIGN; STABILITY	The wheel slip control is the basis of active safety control systems and intelligent driver assistance systems. This paper presents a new robust backstepping sliding mode controller for reference input wheel slip tracking based on the single-corner model combined with the actuator dynamics. The proposed controller is realized by combining backstepping method, which has the merits of simplified and flexible design procedure, with sliding mode control, which has robustness against system uncertainty and external disturbance. Moreover, the closed-loop wheel dynamic system is L-2-gain stable by Lyapunov-based method, and the simulation results show that the proposed controller has better performance.																	1392-124X						2019	48	4					660	672		10.5755/j01.itc.48.4.24297													
J								Parallel Implementation of Improved K-Means Based on a Cloud Platform	INFORMATION TECHNOLOGY AND CONTROL										K-Means; MapReduce; Sample Density; Max-Min Distance		In order to solve the problem of traditional K-Means clustering algorithm in dealing with large-scale data set, a Hadoop K-Means (referred to HKM) clustering algorithm is proposed. Firstly, according to the sample density, the algorithm eliminates the effects of noise points in the data set. Secondly, it optimizes the selection ofthe initial center point using the thought of the max-min distance. Finally, it uses a MapReduce programming model to realize the parallelization. Experimental results show that the proposed algorithm not only has high accuracy and stability in clustering results, but can also solve the problems of scalability encountered by traditional clustering algorithms in dealing with large scale data.																	1392-124X						2019	48	4					673	681		10.5755/j01.itc.48.4.23881													
J								AANMF: Attribute Aware Attentional Neural Matrix Factorization	INFORMATION TECHNOLOGY AND CONTROL										Recommender system; Matrix factorization; Neural network; Attention mechanism; Collaborative filtering	RECOMMENDATION	Matrix Factorization (MF) is one of the most intuitive and effective methods in the Recommendation System domain. It projects sparse (user, item) interactions into dense feature products which endues strong generality to the MF model. To leverage this interaction, recent works use auxiliary information of users and items. Despite effectiveness, irrationality still exists among these methods, since almost all of them simply add the feature of auxiliary information in dense latent space to the feature of the user or item. In this work, we propose a novel model named AANMF, short for Attribute-aware Attentional Neural Matrix Factorization. AANMF cornbines two main parts, namely, neural-network-based factorization architecture for modeling inner product and attention-mechanism-based attribute processing cell for attribute handling. Extensive experiments on two real-world data sets demonstrate the robust and stronger performance of our model. Notably, we show that our model can deal with the attributes of user or item more reasonably. Our implementation of AANMF is publicly available at https://github.com/Holy-Shine/AANMF.																	1392-124X						2019	48	4					682	693		10.5755/j01.itc.48.4.23149													
J								New approaches in metaheuristics to solve the fixed charge transportation problem in a fuzzy environment	NEURAL COMPUTING & APPLICATIONS										Fixed charge transportation problem; Metaheuristic; Prufer number; Spanning tree; Fuzzy; Priority based	ARTIFICIAL IMMUNE ALGORITHM; GENETIC ALGORITHM; CHAIN NETWORK; MINIMIZE	Fixed charge transportation problem (FCTP) is a primary and important problem which attracts researchers in the last decade. Recently, solution approaches typically metaheuristics are in focus. Therefore, metaheuristics have been developed to solve such a nondeterministic polynomial-time hard (NP-hard) problem. Since the real world is a complicated system and we could not formulate it as an exact problem, therefore it is necessary to describe an approximate and a fuzzy model. In this paper, both fixed costs and variable costs are considered as the fuzzy numbers. Three well-known algorithms that included a single point-based and two population-based metaheuristics are developed. Besides, a new population-based algorithm that has not been used in the previous works is developed: whale optimization algorithm (WOA). Contrary to previous works, this paper proposes new approaches in solution algorithms using both spanning tree-based Prufer number and priority-based representation. Also, Taguchi method is used to guarantee the proper performance of algorithms and calibration of parameters. In addition, several problems with different sizes are generated to assess the capability of the algorithms and commercial software according to the real-world case.																	0941-0643	1433-3058				JAN	2019	31			1	SI		477	497		10.1007/s00521-017-3027-3													
J								A Physics Based Novel Approach for Travelling Tournament Problem: Optics Inspired Optimization	INFORMATION TECHNOLOGY AND CONTROL										Computational intelligence optimization; optics inspired optimization; travelling tournament problem; artificial intelligence	ALGORITHM	Computational intelligence search and optimization algorithms have been efficiently adopted and used for many types of complex problems. Optics Inspired Optimization (OIO) is one of the most recent physics inspired computational intelligence methods which treats the search space of the problem to be optimized as a wavy mirror in which each peak is assumed to reflect as a convex mirror and each valley to reflect as a concave one. Each candidate solution is treated as an artificial light point that its glittered ray is reflected back by the search space of the problem and the artificial image is formed based on mirror equations adopted from Optics, as a new candidate solution. In this study, OIO for the first time has been designed as a solution search strategy for travelling tournament problem which is one of the current sport problems and aids to minimize transportation and total movement of teams. Furthermore, this problem has been firstly solved by League Championship Algorithm and obtained results from both synthetic and real datasets have been compared. By this study, new application areas for OIO and LCA have been introduced. Obtained results show the superiority of OIO which is a novel algorithm and seems to efficiently solve many complex problems.																	1392-124X						2019	48	3					373	388		10.5755/j01.itc.48.3.20627													
J								A New Genetic Algorithm with Agent-Based Crossover for the Generalized Assignment Problem	INFORMATION TECHNOLOGY AND CONTROL										Generalized Assignment Problem; Genetic Algorithm; Agent-Based Crossover	DIFFERENTIAL EVOLUTION ALGORITHMS; TABU SEARCH	Generalized assignment problem (GAP) considers finding minimum cost assignment of n tasks to m agents provided each task should be assigned to one agent only. In this study, a new Genetic Algorithm (GA) with some new methods has been proposed to solve GAPs. The agent-based crossover is based on the concept of dominant gene in genotype science and increases the fertility rate of the feasible solutions. The solutions are classified as infeasible, feasible and mature with reference to their conditions. The new local searches provide not only feasibility in high diversity but high profitability for the solutions. A solution is not given up through maturation-based replacement until it reaches its best. The computational results show that the agent-based crossover has much higher fertility rate than classical crossover. Finally, the proposed GA creates either optimal or near-optimal solutions.																	1392-124X						2019	48	3					389	400		10.5755/j01.itc.48.3.21893													
J								Stabilization of Arbitrary Switched Nonlinear Fractional Order Dynamical Systems: Application to Francis Hydro-Turbine Governing System	INFORMATION TECHNOLOGY AND CONTROL										Fractional Order System; Switched Dynamical System; Lyapunov Theory; Stabilization; Linear Matrix Inequality; Hydro-Turbine Governing System	H-INFINITY CONTROL; LINEAR-SYSTEMS; STABILITY	This paper is a theoretical and practical study on the stabilization of fractional order Lipschitz nonlinear systems under arbitrary switching. The investigated system is a generalization of both switched and fractional order dynamical systems. Firstly, a switched frequency distributed model is introduced as an equivalent for the system. Subsequently, a sufficient condition is obtained for the stabilizability of the system based on the Lyapunov approach. Finally, the results are extended to synthesis mode-dependent state feedback controller for the system. All the results are expressed in terms of coupled linear matrix inequalities, which are solvable by optimization tools and directly reducible to the conditions of the integer order nonlinear switching systems as well as the conventional non-switched nonlinear fractional order systems. The proposed method has various practical implications. As an example, it is utilized to control Francis hydro-turbine governing system. This system is represented as a switching structure and supposed to supply a load suffering abrupt changes driven by an arbitrary switching mechanism. The simulation results support the usefulness of the method.																	1392-124X						2019	48	3					401	414		10.5755/j01.itc.48.3.20470													
J								HARPP: HARnessing the Power of Power Sets for Mining Frequent Itemsets	INFORMATION TECHNOLOGY AND CONTROL										Association Rules; Frequent Itemset Mining; Apriori; FP-Growth; Recommendation Systems	N-LIST; ALGORITHM; PATTERNS; CBAR	Modern algorithms for mining frequent itemsets face the noteworthy deterioration of performance when minimum support tends to decrease, especially for sparse datasets. Long-tailed itemsets, frequent itemsets found at lower minimum support, are significant for present-day applications such as recommender systems. In this study, a novel power set based method named as HARnessing the Power of Power sets (HARPP) for mining frequent itemsets is developed. HARPP is based on the concept of power set from set theory and incorporates efficient data structures for mining. Without storing it entirely in memory, HARPP scans the dataset only once and mines frequent itemsets on the fly. In contrast to state-of-the-art, the efficiency of HARPP increases with a decrease in minimum support that makes it a viable technique for mining long-tailed itemsets. A performance study shows that HARPP is efficient and scalable. It is faster up to two orders of magnitude than FP-Growth algorithm at lower minimum support, particularly when datasets are sparse. HARPP memory consumption is less than that of state-of-the-art by an order of magnitude, on most datasets.																	1392-124X						2019	48	3					415	431		10.5755/j01.itc.48.3.21137													
J								Study of the Performance of Various Classifiers in Labeling Non- Functional Requirements	INFORMATION TECHNOLOGY AND CONTROL										Requirements Engineering; Feedback processing; Natural Language Processing; Machine Learning		Software systems are to be developed based on expectations of the customers. These expectations are expressed using natural languages. To design software meeting the needs of the customer and the stakeholders, the intentions, feedback and reviews are to be understood accurately and without ambiguity. These textual inputs often contain inaccuracies, contradictions and are seldom given in a well-structured form. The issues mentioned in the previous thought frequently result in the program not satisfying the expectation of the stakeholders. In particular, for non-functional requirements, clients rarely emphasize these specifications as much as they might be justified. Identifying, classifying and reconciling the requirements is one of the main duty of the System Analyst, which without using a proper tool, can be very demanding and time-consuming. Tools which support text processing are expected to improve the accuracy of identification and classification of requirements even in an unstructured set of inputs. System Analysts can also use them in document archeology tasks where many documents, regulations, standards, etc. have to be processed. Methods elaborated in natural language processing and machine learning offer a solid basis. However, their usability and the possibility to improve the performance utilizing the specific knowledge from the domain of the software engineering are to be examined thoroughly. In this paper, we present the results of our work adapting natural language processing and machine learning methods for handling and transforming textual inputs of software development. The major contribution of our work is providing a comparison of the performance and applicability of the stateof-the-art techniques used in natural language processing and machine learning in software engineering. Based on the results of our experiments, tools which can support System Analysts working on textual inputs can be designed.																	1392-124X						2019	48	3					432	445		10.5755/j01.itc.48.3.21973													
J								Speaker Discrimination Using Long-Term Spectrum of Speech	INFORMATION TECHNOLOGY AND CONTROL										Speech signal; Long-term spectrum; Efficient features; Speaker discrimination; Evaluation	DE-IDENTIFICATION	In this article, a specific long-term speech spectrum was investigated with respect to its use for speaker recognition. The long-term effect was satisfied by averaging short-term autocorrelation coefficients over the whole utterance. The long-term spectrum was calculated by means of second-order linear prediction using the average autocorrelation coefficients. First, speaker discriminability of 32 individual parameters was evaluated by combining spectral energy and spectral slope in eight different frequency bands covering the range 0-4 kHz (seven narrow nonoverlapping subbands and one band spanning over the full range). Then, four subbands with the most discriminative capability were selected for speaker recognition. These subbands involve the frequencies of 0-1.2 kHz in total. In the main experiments, text-independent speaker recognition based on relative Euclidean distance was performed in each single subband as well as in multiple 2 to 4 subbands applying two types of speech data, complete continuous speech and voiced part of the same speech. The voiced speech seems to be generally more effective for speaker recognition using the long-term speech spectrum. The best recognition rates, i.e. 91.7% on complete speech and 100% on voiced speech, were achieved in optimal paired subbands. The long-term speech spectrum can complement the traditional voice features.																	1392-124X						2019	48	3					446	453		10.5755/j01.itc.48.3.21248													
J								Simple Speech Transform Coding Scheme Using Forward Adaptive Quantization for Discrete Input Signal	INFORMATION TECHNOLOGY AND CONTROL										Speech coding; Transform coding; Forward adaptive quantization; Quasi-logarithmic quantizer	DESIGN; SYSTEM; MODEL	The speech coding scheme based on the simple transform coding and forward adaptive quantization for discrete input signal processing is proposed in this paper. The quasi-logarithmic quantizer is applied for discretization of continuous input signal, i.e. for preparing discrete input. The application of forward adaptation based on the input signal variance provides more efficient bandwidth usage, whereas utilization of transform coding provides sub-sequences with more predictable signal characteristics that ensure higher quality of signal reconstruction at the receiving end. In order to provide additional compression, transform coding precedes adaptive quantization. As an objective measure of system performance, signal-to-quantization-noise ratio is used. System performance is discussed for two typical cases. In the first case, it was considered that the information about continuous signal variance is available, whereas the second case considers system performance estimation when only the information about discretized signal variance is present, which means that there is a loss of input signal information. The main goal of such performance estimation comparison of the proposed speech signal coding model is to explore what is the objectivity of performance if the information about a continuous source is absent, which is a common phenomenon in digital systems. The advantages of the proposed coding scheme are demonstrated by comparing the performance of the reconstructed signal with other similar exiting speech signal coding systems.																	1392-124X						2019	48	3					454	463		10.5755/j01.itc.48.3.21685													
J								The Analysis of Energy Performance in Use Parallel Merge Sort Algorithms	INFORMATION TECHNOLOGY AND CONTROL										power-aware testing; functional power component; parallel algorithm; data sorting; data mining; analysis of computer algorithms		The issue of productivity and energy is an important objective of the optimization of parallel applications. The size of the problem for a large number of data on multiprocessor platforms forces the use of parallel algorithms. Efficient management of large memories using modern processors in Big data processing requires innovative techniques and efficient algorithms. For years have found the results of tests conducted on methods for use in various computing environments and improvements. This article shows the energy consumption analysis by parallel sorting algorithms. Sort algorithms are used in information systems and databases, to select and organize the information. The subject of this article is research into energy consumption and computational complexity for parallel sorting methods by merging compared to classic methods. The tests carried out confirm the reduction of energy consumption by using parallel sorting algorithms. The presented parallel fast sort and parallel modified merge sort for large task dimensions have less power consumption than classic methods and can be used successfully in NoSQL databases.																	1392-124X						2019	48	3					487	498		10.5755/j01.itc.48.3.23696													
J								Beyond reasonable doubt: A proposal for undecidedness blocking in abstract argumentation	INTELLIGENZA ARTIFICIALE										Abstract argumentation semantics; ambiguity blocking; standard of proofs; undecidedness		In Dung's abstract semantics, the label undecided is always propagated from the attacker to the attacked argument, unless the latter is also attacked by an accepted argument. In this work we propose undecidedness blocking abstract argumentation semantics where the undecided label is confined to the strong connected component where it was generated and it is not propagated to the other parts of the argumentation graph. We show how undecidedness blocking is a fundamental reasoning pattern absent in abstract argumentation but present in similar fashion in the ambiguity blocking semantics of Defeasible logic, in the beyond reasonable doubt legal principle or when someone gives someone else the benefit of the doubt. The resulting semantics, called SCC-void semantics, are defined using an SCC-recursive schema. The semantics are conflict-free and non-admissible, but they incorporate a more relaxed defence-based notion of admissibility. They allow reinstatement and they credulously accept what the corresponding Dung's complete semantics accepts at least credulously.																	1724-8035	2211-0097					2019	13	2			SI		123	135		10.3233/IA-190030													
J								Power index-based semantics for ranking arguments in abstract argumentation frameworks	INTELLIGENZA ARTIFICIALE										Argumentation; semantics; ranking; cooperative game theory; Shapley Value; Banzhaf Index		Ranking-based semantics for Abstract Argumentation Frameworks represent a well-established concept used for sorting a group of arguments from the most to the least acceptable. This paper describes a ranking-based semantics that makes use of power indexes such as the Shapley Value and the Banzhaf Power Index. This power index-based semantics is parametric to a chosen Dung semantics and inherits the properties of the index that is used for evaluating the arguments. We highlight the characteristics of the rankings obtained through different evaluation functions and we verify which of the properties from the literature are satisfied. Finally, we study the relation between the (skeptical/credulous) acceptability of an argument and its position in the ranking, thus designing new properties.																	1724-8035	2211-0097					2019	13	2			SI		137	154		10.3233/IA-190031													
J								Annotation with adpositional argumentation Guidelines for building a Gold Standard Corpus of argumentative discourse	INTELLIGENZA ARTIFICIALE										Computational argumentation; natural language processing; gold standard corpora; argumentation theory; rhetoric		This paper explains Adpositional Argumentation (AdArg), a new method for annotating arguments expressed in natural language. In describing this method, it provides the guidelines for designing a Gold Standard Corpus (GSC) of argumentative discourse in terms of so-called argumentative adpositional trees (arg-adtrees). The theoretical starting points of AdArg draw on the combination of the linguistic representation framework of Constructive Adpositional Grammars (CxAdGrams) with the argument categorisation framework of the Periodic Table of Arguments (PTA). After an explanation of these two frameworks, it is shown how AdArg can be used for annotating arguments expressed in natural language. This is done by providing the arg-adtrees of four concrete examples of arguments, which substantiate the four basic argument forms distinguished in the PTA. The present exposition of the fundamental tenets of AdArg enables the building of a GSC of argumentative discourse, that means an annotated corpus of texts and discussions of undisputable high-quality according to argumentation theory experts. Such a GSC should be conveniently annotated in terms of arg-adtrees, which is a time-consuming process, as it needs highly skilled annotators and human supervision. However, its role is crucial for developing instruments for computer-assisted argumentation analysis and eventual application based on machine learning natural language processing algorithms.																	1724-8035	2211-0097					2019	13	2			SI		155	172		10.3233/IA-190028													
J								An abstract argumentation approach for the prediction of analysts' recommendations following earnings conference calls	INTELLIGENZA ARTIFICIALE										Argumentation; natural language processing; sentiment analysis; machine learning	ACCEPTABILITY; BIPOLAR	Financial analysts constitute an important element of financial decision-making in stock exchanges throughout the world. By leveraging on argumentative reasoning, we develop a method to predict financial analysts' recommendations in earnings conference calls (ECCs), an important type of financial communication. We elaborate an analysis to select those reliable arguments in the Questions & Answers (Q&A) part of ECCs that analysts evaluate to estimate their recommendation. The observation date of stock recommendation update may variate during the next quarter: it can be either the day after the ECC or it can take weeks. Our objective is to anticipate analysts' recommendations by predicting their judgment with the help of abstract argumentation. In this paper, we devise our approach to the analysis of ECCs, by designing a general processing framework which combines natural language processing along with abstract argumentation evaluation techniques to produce a final scoring function, representing the analysts' prediction about the company's trend. Then, we evaluate the performance of our approach by specifying a strategy to predict analysts recommendations starting from the evaluation of the argumentation graph properly instantiated from an ECC transcript. We also provide the experimental setting in which we perform the predictions of recommendations as a machine learning classification task. The method is shown to outperform approaches based only on sentiment analysis.																	1724-8035	2211-0097					2019	13	2			SI		173	188		10.3233/IA-190026													
J								Efficiently computing extensions' probabilities over probabilistic Bipolar Abstract Argumentation Frameworks	INTELLIGENZA ARTIFICIALE										Probabilistic bipolar argumentation; computational complexity	ACCEPTABILITY; SEMANTICS; SUPPORT	Probabilistic Bipolar Abstract Argumentation Frameworks (prBAFs), combining the possibility of specifying supports between arguments with a probabilistic modeling of the uncertainty, have been recently considered [34, 35] and the complexity of the problem of computing extensions' probabilities has been characterized [22]. In this paper we deal with the problem of computing extensions' probabilities over prBAFs where the probabilistic events that arguments, supports and defeats occur in the real scenario are assumed to be independent probabilistic events (prBAFS of type IND). Specifically an algorithm for efficiently computing extensions' probabilities under the stable and admissible semantics has been devised and its efficiency has been experimentally validated w.r.t. the exhaustive approach, i.e. the approach consisting in the generation of all the possible scenarios, showing that the proposed algorithm outperforms the exhaustive approach.																	1724-8035	2211-0097					2019	13	2			SI		189	200		10.3233/IA-190029													
J								Recent advances in multirobot exploration of communication-restricted environments	INTELLIGENZA ARTIFICIALE										Multirobot systems; multirobot exploration; communication maps; multirobot reconnection; planning and scheduling	CONNECTIVITY; INTRACTABILITY; GRAPHS	In the multirobot exploration problem, a team of robots is deployed in an unknown environment with the aim of efficiently mapping free space and obstacles. The study of this problem is often simplified by assuming that communication between robots is possible between any two locations. In practice, this could be a far too optimistic assumption due to the limited range of the robots' local communication modules. This paper presents, under a unified framework, a series of recent contributions in the field of multirobot exploration under communication limitations addressing three of its key research challenges. First, the development of an exploration strategy able to enforce timely reports at a supervising control station without excessively hindering the exploration process. Second, the construction of "communication maps" of the environment: maps that -when queried by the robots to determine the possibility of a future communication with a teammate- return the estimate of the radio signal strength between two given locations, along with a numerical value representing the confidence in the prediction. Third, the problem of computing backup reconnection plans to be used in presence of mispredicted communication links. The paper also discusses some promising research directions derived from the lessons learned.																	1724-8035	2211-0097					2019	13	2			SI		203	230		10.3233/IA-180013													
J								Cubic Intuitionistic Fuzzy Sets and its Fundamental Properties	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Cubic intuitionistic fuzzy set; internal (external) cubic intuitionistic fuzzy set; P-(R)- union; P-(R)- intersection	DECISION-MAKING METHOD; AGGREGATION OPERATORS; NORM	Traditionally, the information related to an element is collected either by an intuitionistic fuzzy set (IFS) or interval-valued IFS (IVIFS) which may lose some useful information and hence may affect on the decision results. To overcome this loss, in this paper, we present a new concept of the cubic intuitionistic fuzzy set (CIFS) by extending the concept of the IFSs. As a generalization of the IFSs and IVIFSs, CIFS is a strong and valuable tool to represent the imprecise information by embedding both the features of IFSs and IVIFSs instantaneously. Further, based on its inherent property, we further define the internal (external) CIFS, P(R)-union and P(R)-intersection of CIFSs. The several desirable properties of these operations are defined along with their proof.																	1542-3980	1542-3999					2019	33	6					507	537															
J								Forecasting Currency Pairs with RBF Neural Network Using Activation Function Based on Generalized Normal Distribution Experimental Results	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										RBF; neural network; GED; finance; time series; activation function	TIME-SERIES; CONDITIONAL HETEROSCEDASTICITY; STOCK RETURNS	In this paper, we implement an effective way for forecasting financial time series with nonlinear relationships. We use the artificial neural network of feedforward type for making the decision-process in a company more efficient, more flexible and more accurate. The main objective of this study is to design new method for improving the performance of RBF artificial neural networks. Based on pre-experimental statistical analysis of 1225 financial time series and inspired by GARCH model, RBF neural networks with new shapes of activation functions based on generalized Normal distribution function (GED) are suggested and discussed. Within this study various types of GED activation function in RBF networks are investigated to find the best ones. Firstly, the presence of homoscedasticity and the occurrence of normality of the time series data is investigated. To test our hypothesis about the application of GED distribution in the RBF neural network, we implemented a neural network application (RBFNN) in JAVA. Using the software, we investigate the RMSE error based on the value of p parameter in GED. The optimized size of the p parameter is determined for classic and soft RBF network related to minimal prediction error. We then test our model on 25 financial datasets to explore the contribution of our suggested and implemented method. We also evaluate the forecasting performance of suggested neural network in comparison to established models based on RMSE. Our results show that the proposed approach achieves higher forecasting accuracy on the validation set than available techniques. The suggested modification form of the shape of activation function of the RBF neural network using GED distribution improves the approximation and prediction accuracy of the RBF network models used for financial time series. From performed experiments we find that the optimal size of the parameter p will likely be in the interval (1.4, 2.4) for a standard RBF and less than 2 for the soft RBF.																	1542-3980	1542-3999					2019	33	6					539	563															
J								Harmonic Mean Aggregation Operators in Spherical Fuzzy Environment and Their Group Decision Making Applications	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Aggregation operators; harmonic mean; spherical fuzzy sets (SFS5); algebraic strict archimedean T-norm and T- conorm; Einstein strict archimedean T-norm and T- conorm; multi-attribute group decision making (MAGD)		Harmonic Mean as a conservative mean is the best choice to avoid outlier data. The scope of this article is to present the novel and advanced aggregation operators for the spherical fuzzy set which is based on Harmonic Mean operator by using Algebraic and Einstein Strict Archimedean T-Norm and T- Conorm. To achieve this scope, we developed and proved Spherical Fuzzy Number Algebraic Weighted Harmonic Mean (SFNAWHM), Spherical Fuzzy Number Einstein Weighted Harmon Mean (SFNEWHM), Spherical Fuzzy Number Algebraic Ordered Weighted Harmonic Mean (SFNAOWHM), Spherical Fuzzy Number Einstein Ordered Weighted Harmonic Mean (SFNEOWHM), Spherical Fuzzy Number Algebraic Hybrid Ordered Weighted Harmonic Mean (SFNAHOWHM) and Spherical Fuzzy Number Einstein Hybrid Ordered Weighted Harmonic Mean (SFNEHOWHM). Then based on this developed aggregation operators, the new MAGDM method has been established for decision making problems and this method's effectiveness and reliability examined by illustrative example.																	1542-3980	1542-3999					2019	33	6					565	592															
J								Representation Theory for Complete L-lattices	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										L-set; L-poset; L-lattice; complete L-lattice; L-equality; closure system	VALUED EQUIVALENCE-RELATIONS; FUZZY FUNCTIONS; VAGUE LATTICES; PART I; FOUNDATIONS; SETS; CONSTRUCTIONS; IDEALS	In the framework of L-valued (fuzzy) sets, where L is a complete lattice, we introduce complete L-lattices, based on L-structures investigated by the authors. An L-poset is a set equipped with an L-valued equality E and an L-valued transitive relation R, which is antisymmetric with respect to E. A complete L-lattice is an L-poset in which every subset has a so called pseudo-supremum and a pseudo-infimum. Several properties concerning special elements of these L-structures are investigated. Among our main results, we prove that an L-poset is a complete L-lattice if and only if particular quotient substructures with respect to the L-valued equality are classical complete lattices. As another important result obtained by using closure systems, we present a Representation theorem dealing with a general construction of L-posets and L-complete lattices.																	1542-3980	1542-3999					2019	33	6					593	617															
J								MULTI USER DETECTION USING FUZZY LOGIC EMPOWERED ADAPTIVE BACK PROPAGATION NEURAL NETWORK	NEURAL NETWORK WORLD										MC-CDMA; FLeABPNN; MIMO; BER; ANN; STBC; MMSE; GA; LMS	MC-CDMA; CHANNEL ESTIMATION; PERFORMANCE; ACCESS; OFDM	In Wireless communication, Multiple Input and Multiple Output (MIMO) systems have always been quite popular. Multicarrier systems are established along with different techniques of space-time coding to accomplish the demands of these systems. One of the most popular techniques is Multi-Carrier Code Division Multiple Access (MC-CDMA) with Alamouti's Space-Time Block Codes (STBC). This article, proposed the Fuzzy Logic empowered Adaptive Back Propagation Neural Network (FLeABPNN) based Multi User Detection (MUD) system, which is used to determine the receiver weights of MC-CDMA with the scheme of two variations. The proposed FLeABPNN approach takes advantage of a neuro-fuzzy hybrid system which conglomerates the competences of both fuzzy logic and neural networks for multi-user detection. It is observed that due to the fuzzy logic-based learning rate, proposed FLeABPNN based receiver without relationship & with relationship achieved the 3.04 x 10(-06) and 2.05 x 10(-06) Bit Error Rate (BER) respectively. The proposed FLeABPNN based receiver gives fast convergence rate & low BER as compared to other suboptimal published techniques like GA & LMS. It also observed that the Computational Complexity of the proposed FLeABPNN based MC-CDMA receiver is less then LMS based receiver up to 18 users, but higher than GA based receiver.																	1210-0552						2019	29	6					381	401		10.14311/NNW.2019.29.024													
J								TOWARDS AN OPTIMAL SET OF INITIAL WEIGHTS FOR A DEEP NEURAL NETWORK ARCHITECTURE	NEURAL NETWORK WORLD										Deep Neural Networks; centroids; arabic language processing; big data; morphosyntactic tagging; machine translation; regression	TRAINING SPEED	Modern neural network architectures are powerful models. They have been proven efficient in many fields, such as imaging and acoustic. However, these neural networks involve a long-running and time-consuming process. To accelerate the training process, we propose a two-stage approach based on data analysis and focus on the gravity center concept. The neural network is first trained on reduced data represented by a set of centroids of the original data points, and then the learned weights are used to initialize a second training phase of the neural network over the full-blown data. The design of deep neural networks is extremely difficult, and the primary objective is to achieve high performance. In this study, we apply the Taguchi method to select good values for the factors required to build the proposed architecture.																	1210-0552						2019	29	6					403	426		10.14311/NNW.2019.29.025													
J								DYNAMICS OF SINUSOIDAL ALPHA WAVES ASYMMETRY IN BRAIN ELECTRICAL FIELD	NEURAL NETWORK WORLD										EEG; electrical brain signals; basic alpha rhythm; alpha pattern recognition; alpha amplitude; alpha steepness; alpha waves asymmetry; reinforced thinking; uncertain factors		There is no methodical approach suitable for definition of the periodical or non-periodical, stationary or nonstationary curves of brain signals with a help of amplitude, frequency, phase etc. values. It is difficult to determinate the wave shape, i.e. the problem is how to solve the respective pattern recognition. Therefore, we tried to propose a simple method for praxis by help of measurement two main wave time components, interpreting a sinusoidal alpha wave as a triangle, where there is an anterior and a posterior part of wave ascending and descending abscissas in a hope that the sufficient measure are presented by the "legs" only or distances between upper and bottom peak of the wave. All the values of total ascendants are divided by all values of total descendants. For the method validity estimation it was made for this computation separately in two different psychical states - the relaxation and the calculation activity, both with eyes closed. Results are presented as quotient (quotus alpha) which means alpha waves symmetry. If the quotient is equal to 1, or is near to 1, is the alpha wave full or almost symmetrical. When the quotient is lower than 1 the ascendant is shorter than descendent, then alpha wave is asymmetric and has inclination to the left side. In contrary if the quotient is higher than 1 the ascendant is longer than descendent, alpha wave is again asymmetrical, but inclination is oriented to the right side. During mentation is usually quotient lower one and the ascendant is still more lover, alpha waves are sheer, the inclination to the left is more expressive.																	1210-0552						2019	29	6					427	445		10.14311/NNW.2019.29.026													
J								VOLTAGE STABILITY ASSESSMENT OF COMPLEX POWER SYSTEM BASED ON GA-SVM	NEURAL NETWORK WORLD										genetic algorithm; power flow; power system simulation; SVM; voltage stability assessment; meta-learning		The dynamic stability assessment and prediction of a complex power system is a precondition to take the action of protecting control. This paper presents the four support vector machines (SVMs) with an improved genetic algorithm (GA) to compute their parameters automatically, that one SVM is used to simulate the tangent vector and the others for identifying the instable area. Besides, the GA was initialized by Meta-Learning method to enhance the performance and its optimal solution was selected by last test. Furthermore, a large network simplification was taken for reducing the amount of calculation and responding in real time. Study with the IEEE 118-bus test system indicated that the system status of a complex power system subjected a fault could be predicted based on this technique of the GA-SVM for simulating the tangent vector accurately. Besides, three binary SVM classifiers were trained to locate the instable area, and ranking the levels by the analysis of critical bus is help to management. Based on the test on the networks, the suggested approach can predict accurately with 98.87 % success rate and identify the fault area with 94.91 % success rate.																	1210-0552						2019	29	6					447	463		10.14311/NNW.2019.29.027													
J								An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory	JOURNAL OF MACHINE LEARNING RESEARCH											SIGNAL RECOVERY; NUMBER	In this paper, the problem of one-bit compressed sensing (OBCS) is formulated as a problem in probably approximately correct (PAC) learning. It is shown that the Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in R-n generated by k-sparse vectors is bounded below by k(left perpendicular lg(n/k) right perpendicular + 1) and above by left perpendicular 2k lg(en) right perpendicular. By coupling this estimate with well-established results in PAC learning theory, we show that a consistent algorithm can recover a k-sparse vector with O(k lg n) measurements, given only the signs of the measurement vector. This result holds for all probability measures on R-n. The theory is also applicable to the case of noisy labels, where the signs of the measurements are flipped with some unknown probability.																	1532-4435						2019	20								11														
J								iNNvestigate Neural Networks!	JOURNAL OF MACHINE LEARNING RESEARCH										Artificial neural networks; deep learning; analyzing classifiers; explaining classifiers; computer vision		In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short-coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-thebox implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.																	1532-4435						2019	20								93														
J								Best Arm Identification for Contaminated Bandits	JOURNAL OF MACHINE LEARNING RESEARCH										multi-armed bandits; best arm identification; robust statistics; contamination model; partial identifiability	MULTIARMED BANDIT	This paper studies active learning in the context of robust statistics. Specifically, we propose a variant of the Best Arm Identification problem for contaminated bandits, where each arm pull has probability epsilon of generating a sample from an arbitrary contamination distribution instead of the true underlying distribution. The goal is to identify the best (or approximately best) true distribution with high probability, with a secondary goal of providing guarantees on the quality of this distribution. The primary challenge of the contaminated bandit setting is that the true distributions are only partially identifiable, even with infinite samples. To address this, we develop tight, non-asymptotic sample complexity bounds for high-probability estimation of the first two robust moments (median and median absolute deviation) from contaminated samples. These concentration inequalities are the main technical contributions of the paper and may be of independent interest. Using these results, we adapt several classical Best Arm Identification algorithms to the contaminated bandit setting and derive sample complexity upper bounds for our problem. Finally, we provide matching information-theoretic lower bounds on the sample complexity (up to a small logarithmic factor).																	1532-4435						2019	20								91														
J								Non-Convex Matrix Completion and Related Problems via Strong Duality	JOURNAL OF MACHINE LEARNING RESEARCH										strong duality; non-convex optimization; matrix factorization; matrix completion; robust principal component analysis; sample complexity	RANK; OPTIMIZATION; INCOHERENCE	This work studies the strong duality of non-convex matrix factorization problems: we show that under certain dual conditions, these problems and the dual have the same optimum. This has been well understood for convex optimization, but little was known for non-convex problems. We propose a novel analytical framework and prove that under certain dual conditions, the optimal solution of the matrix factorization program is the same as that of its bi-dual and thus the global optimality of the non-convex program can be achieved by solving its bi-dual which is convex. These dual conditions are satisfied by a wide class of matrix factorization problems, although matrix factorization is hard to solve in full generality. This analytical framework may be of independent interest to non-convex optimization more broadly. We apply our framework to two prototypical matrix factorization problems: matrix completion and robust Principal Component Analysis. These are examples of efficiently recovering a hidden matrix given limited reliable observations. Our framework shows that exact recoverability and strong duality hold with nearly-optimal sample complexity for the two problems.																	1532-4435						2019	20								102														
J								Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations	JOURNAL OF MACHINE LEARNING RESEARCH										invariant representations; deep learning; stability; kernel methods		The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they a ff ect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of di ff eomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as recti fi ed linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.																	1532-4435						2019	20								25														
J								Pyro: Deep Universal Probabilistic Programming	JOURNAL OF MACHINE LEARNING RESEARCH										Probabilistic programming; graphical models; approximate Bayesian inference; generative models; deep learning		Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.																	1532-4435						2019	20								28														
J								Forward-Backward Selection with Early Dropping	JOURNAL OF MACHINE LEARNING RESEARCH										Feature Selection; Forward Selection; Markov Blanket Discovery; Bayesian Networks; Maximal Ancestral Graphs	MODEL SELECTION; LIKELIHOOD RATIO; PERFORMANCE; DISCOVERY; INDUCTION; CRITERIA; EVENTS	Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this paper, we propose a heuristic that significantly improves its running time, while preserving predictive performance. The idea is to temporarily discard the variables that are conditionally independent with the outcome given the selected variable set. Depending on how those variables are reconsidered and reintroduced, this heuristic gives rise to a family of algorithms with increasingly stronger theoretical guarantees. In distributions that can be faithfully represented by Bayesian networks or maximal ancestral graphs, members of this algorithmic family are able to correctly identify the Markov blanket in the sample limit. In experiments we show that the proposed heuristic increases computational efficiency by about 1-2 orders of magnitude, while selecting fewer or the same number of variables and retaining predictive performance. Furthermore, we show that the proposed algorithm and feature selection with LASSO perform similarly when restricted to select the same number of variables, making the proposed algorithm an attractive alternative for problems where no (efficient) algorithm for LASSO exists.																	1532-4435						2019	20								8														
J								Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations	JOURNAL OF MACHINE LEARNING RESEARCH										Simulated Annealing; Stochastic Optimization; Markov Process; Convergence Rate; Aircraft Trajectory Optimization	EFFICIENT GLOBAL OPTIMIZATION	In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.																	1532-4435						2019	20								4														
J								AffectiveTweets: a Weka Package for Analyzing Affect in Tweets	JOURNAL OF MACHINE LEARNING RESEARCH										Twitter; Sentiment Analysis; Emotion Analysis; Affective Computing; Lexicon Induction; Distant Supervison		AffectiveTweets is a set of programs for analyzing emotion and sentiment of social media messages such as tweets. It is implemented as a package for the Weka machine learning workbench and provides methods for calculating state-of-the-art affect analysis features from tweets that can be fed into machine learning algorithms implemented in Weka. It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabeled tweets. The package was used by several teams in the shared tasks: Emolnt 2017 and Affect in Tweets SemEval 2018 Task 1.																	1532-4435						2019	20								92														
J								Simultaneous Private Learning of Multiple Concepts	JOURNAL OF MACHINE LEARNING RESEARCH										Differential privacy; PAC learning; Agnostic learning; Direct-sum	BOUNDS; VAPNIK	We investigate the direct-sum problem in the context of differentially private PAC learning: What is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual example consists of a domain element x labeled by k unknown concepts (c(1), ..., c(k)). The goal of a multi-learner is to output k hypotheses (h(1), ..., h(k)) that generalize the input examples. Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy of learning each hypothesis independently yields sample complexity that grows polynomially with k. For some concept classes, we give multi-learners that require fewer samples than the basic strategy. Unfortunately, however, we also give lower bounds showing that even for very simple concept classes, the sample cost of private multi-learning must grow polynomially in k.																	1532-4435						2019	20								94														
J								Accelerated Alternating Projections for Robust Principal Component Analysis	JOURNAL OF MACHINE LEARNING RESEARCH										Robust PCA; Alternating Projections; Matrix Manifold; Tangent Space; Subspace Projection	RANK MATRIX COMPLETION; MINIMIZATION ALGORITHM	We study robust PCA for the fully observed setting, which is about separating a low rank matrix L and a sparse matrix S from their sum D = L + S. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which signi fi cantly improves the computational e ffi ciency of the existing alternating projections proposed in (Netrapalli et al., 2014) when updating the low rank factor. The acceleration is achieved by fi rst projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-theart algorithms for robust PCA.																	1532-4435						2019	20								20														
J								Automated Scalable Bayesian Inference via Hilbert Coresets	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian inference; scalable; automated; coreset; Hilbert; Frank-Wolfe		The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern data sets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the data set itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms-one based on importance sampling, and one based on the Frank-Wolfe algorithm-along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic data sets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.																	1532-4435						2019	20								15														
J								Delay and Cooperation in Nonstochastic Bandits	JOURNAL OF MACHINE LEARNING RESEARCH										Multi-armed bandits; distributed learning; cooperative multi-agent systems; regret minimization; LOCAL communication		We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce Exp3-Coop, a cooperative version of the Exp3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order root(d + 1 + K/N alpha(<= d)) (T ln K), where alpha(<= d) is the independence number of the d-th power of the communication graph G. We then show that for any connected graph, for d = root K the regret bound is K-1/4 root T, strictly better than the minimax regret root KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret root T ln K when G is dense. When G has sparse components, we show that a variant of Exp3-Coop, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.																	1532-4435						2019	20								17														
J								Iterated Learning in Dynamic Social Networks	JOURNAL OF MACHINE LEARNING RESEARCH											EVOLUTION; RECALL	A classic finding by (Kalish et al., 2007) shows that no language can be learned iteratively by rational agents in a self-sustained manner. In other words, if A teaches a foreign language to B, who then teaches what she learned to C, and so on, the language will quickly get lost and agents will wind up teaching their own common native language. If so, how can linguistic novelty ever be sustained? We address this apparent paradox by considering the case of iterated learning in a social network: we show that by varying the lengths of the learning sessions over time or by keeping the networks dynamic, it is possible for iterated learning to endure forever with arbitrarily small loss.																	1532-4435						2019	20								29														
J								Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression	JOURNAL OF MACHINE LEARNING RESEARCH										tensors; non-convex optimization; high-dimensional regression; low-rank		In this paper, we consider the problem of learning high-dimensional tensor regression problems with low-rank structure. One of the core challenges associated with learning high-dimensional models is computation since the underlying optimization problems are often non-convex. While convex relaxations could lead to polynomial-time algorithms they are often slow in practice. On the other hand, limited theoretical guarantees exist for non-convex methods. In this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set Theta in terms of its localized Gaussian width (due to Gaussian design). We juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches. The two main differences between the convex and non-convex approach are: (i) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and (ii) from a statistical error bound perspective, the non-convex approach has a superior rate for a number of examples. We provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches. We supplement our theoretical results with simulations which show that, under several common settings of generalized low rank tensor regression, the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen.																	1532-4435						2019	20								5														
J								Learnability of Solutions to Conjunctive Queries	JOURNAL OF MACHINE LEARNING RESEARCH										concept learning; computational learning theory; dichotomy theorems; reductions; universal algebra	CONSTRAINT SATISFACTION; VARIETIES; COMPLEXITY	The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classi fi cation program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In addition, a further negative learnability result is exhibited, which indicates a dichotomy within the problems to which the fi rst negative result applies. In order to obtain our negative results, we make use of universal- algebraic concepts.																	1532-4435						2019	20								67														
J								Approximation Hardness for A Class of Sparse Optimization Problems	JOURNAL OF MACHINE LEARNING RESEARCH										nonconvex optimization; computational complexity; variable selection; NP-hardness; sparsity	VARIABLE SELECTION; PENALIZED LIKELIHOOD; SIGNALS	In this paper, we consider three typical optimization problems with a convex loss function and a nonconvex sparse penalty or constraint. For the sparse penalized problem, we prove that fi nding an O (n(c1)d(c2))-optimal solution to an n x d problem is strongly NP-hard for any c(1), c(2) epsilon [0; 1) such that c(1) + c(2) < 1. For two constrained versions of the sparse optimization problem, we show that it is intractable to approximately compute a solution path associated with increasing values of some tuning parameter. The hardness results apply to a broad class of loss functions and sparse penalties. They suggest that one cannot even approximately solve these three problems in polynomial time, unless P = NP.																	1532-4435						2019	20								38														
J								Hamiltonian Monte Carlo with Energy Conserving Subsampling	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian inference; Big Data; Markov chain Monte Carlo; Estimated likelihood; Stochastic gradient Hamiltonian Monte Carlo; Stochastic Gradient Langevin Dynamics	BAYESIAN-INFERENCE; LANGEVIN; MCMC	Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large data sets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that use gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients.																	1532-4435						2019	20								100														
J								Variance-based Regularization with Convex Objectives	JOURNAL OF MACHINE LEARNING RESEARCH										variance regularization; robust optimization; empirical likelihood	INEQUALITIES; OPTIMIZATION	We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally e ffi cient trading between approximation and estimation error. Our approach builds o ff of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of fi nite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certi fi cates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classi fi cation problems.																	1532-4435						2019	20								68														
J								Streaming Principal Component Analysis From Incomplete Data	JOURNAL OF MACHINE LEARNING RESEARCH										Principal component analysis; Subspace identification; Matrix completion; Streaming algorithms; Nonconvex optimization; Global convergence	SUBSPACE; MATRIX; ALGORITHM; IDENTIFICATION; APPROXIMATION; OPTIMIZATION; CONVERGENCE; RECOVERY	Linear subspace models are pervasive in computational sciences and particularly used for large datasets which are often incomplete due to privacy issues or sampling constraints. Therefore, a critical problem is developing an efficient algorithm for detecting low-dimensional linear structure from incomplete data efficiently, in terms of both computational complexity and storage. In this paper we propose a streaming subspace estimation algorithm called Subspace Navigation via Interpolation from Partial Entries (SNIPE) that efficiently processes blocks of incomplete data to estimate the underlying subspace model. In every iteration, SNIPE finds the subspace that best fits the new data block but remains close to the previous estimate. We show that SNIPE is a streaming solver for the underlying nonconvex matrix completion problem, that it converges globally to a stationary point of this program regardless of initialization, and that the convergence is locally linear with high probability. We also find that SNIPE shows state-of-the-art performance in our numerical simulations.																	1532-4435						2019	20								86														
J								Scalable Approximations for Generalized Linear Problems	JOURNAL OF MACHINE LEARNING RESEARCH										Generalized Linear Problems; Stochastic optimization; Subsampling; Dimension reduction in optimization	REGRESSION; BOUNDS; LASSO	In stochastic optimization, the population risk is generally approximated by the empirical risk which is in turn minimized by an iterative algorithm. However, in the large-scale setting, empirical risk minimization may be computationally restrictive. In this paper, we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models. We focus on large-scale problems where the iterative minimization of the empirical risk is computationally intractable, i.e., the number of observations n is much larger than the dimension of the parameter p (n >> p >> 1). We show that under random sub-Gaussian design, the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares (OLS) estimator. Using this relation, we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a quadratic convergence rate, and that are computationally cheaper than any batch optimization algorithm by at least a factor of O (p). We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm on well-known classification and regression problems, through extensive numerical studies on large-scale datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.																	1532-4435						2019	20								7														
J								Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions	JOURNAL OF MACHINE LEARNING RESEARCH										Graphical Lasso; Graphical Model; Sparse Graphs; Optimization	INVERSE COVARIANCE ESTIMATION; SELECTION; SPARSITY; MODEL	Graphical Lasso (GL) is a popular method for learning the structure of an undirected graphical model, which is based on an l(1) regularization technique. The objective of this paper is to compare the computationally-heavy GL technique with a numerically-cheap heuristic method that is based on simply thresholding the sample covariance matrix. To this end, two notions of sign-consistent and inverse-consistent matrices are developed, and then it is shown that the thresholding and GL methods are equivalent if: (i) the thresholded sample covariance matrix is both sign-consistent and inverse-consistent, and (ii) the gap between the largest thresholded and the smallest un-thresholded entries of the sample covariance matrix is not too small. By building upon this result, it is proved that the GL method-as a conic optimization problem-has an explicit closed-form solution if the thresholded sample covariance matrix has an acyclic structure. This result is then generalized to arbitrary sparse support graphs, where a formula is found to obtain an approximate solution of GL. Furthermore, it is shown that the approximation error of the derived explicit formula decreases exponentially fast with respect to the length of the minimum-length cycle of the sparsity graph. The developed results are demonstrated on synthetic data, functional MRI data, traffic flows for transportation networks, and massive randomly generated data sets. We show that the proposed method can obtain an accurate approximation of the GL for instances with the sizes as large as 80, 000 x 80, 000 (more than 3.2 billion variables) in less than 30 minutes on a standard laptop computer running MATLAB, while other state-of-the-art methods do not converge within 4 hours.																	1532-4435						2019	20								10														
J								Joint PLDA for Simultaneous Modeling of Two Factors	JOURNAL OF MACHINE LEARNING RESEARCH										Probabilistic linear discriminant analysis; speaker recognition; factor analysis; language variability; robustness to acoustic conditions		Probabilistic linear discriminant analysis (PLDA) is a method used for biometric problems like speaker or face recognition that models the variability of the samples using two latent variables, one that depends on the class of the sample and another one that is assumed independent across samples and models the within-class variability. In this work, we propose a generalization of PLDA that enables joint modeling of two sample-dependent factors: the class of interest and a nuisance condition. The approach does not change the basic form of PLDA but rather modifies the training procedure to consider the dependency across samples of the latent variable that models within-class variability. While the identity of the nuisance condition is needed during training, it is not needed during testing since we propose a scoring procedure that marginalizes over the corresponding latent variable. We show results on a multilingual speaker-verification task, where the language spoken is considered a nuisance condition. The proposed joint PLDA approach leads to significant performance gains in this task for two different data sets, in particular when the training data contains mostly or only monolingual speakers.																	1532-4435						2019	20								24														
J								Approximations of the Restless Bandit Problem	JOURNAL OF MACHINE LEARNING RESEARCH											REGRET BOUNDS	The multi-armed restless bandit problem is studied in the case where the pay-off distributions are stationary phi-mixing. This version of the problem provides a more realistic model for most real-world applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The objective of this paper is to characterize a sub-class of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that under some conditions on the phi-mixing coefficients, a modified version of UCB can prove effective. The main challenge is that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same characteristics as those of the original bandit arms. In particular, the phi-mixing property does not necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on the pay-off distributions. Some of the proof techniques developed in this paper can be more generally used in the context of online sampling under dependence. Proposed algorithms are accompanied with corresponding regret analysis.																	1532-4435						2019	20								14														
J								Learning Attribute Patterns in High-Dimensional Structured Latent Attribute Models	JOURNAL OF MACHINE LEARNING RESEARCH											DINA MODEL; COGNITIVE ASSESSMENT; CONVERGENCE-RATES; IDENTIFIABILITY; CLASSIFICATION; LIKELIHOOD; DECOMPOSITIONS; UNIQUENESS; SELECTION; FAMILY	Structured latent attribute models (SLAMs) are a special family of discrete latent variable models widely used in social and biological sciences. This paper considers the problem of learning significant attribute patterns from a SLAM with potentially high-dimensional configurations of the latent attributes. We address the theoretical identifiability issue, propose a penalized likelihood method for the selection of the attribute patterns, and further establish the selection consistency in such an overfitted SLAM with a diverging number of latent patterns. The good performance of the proposed methodology is illustrated by simulation studies and two real datasets in educational assessments.																	1532-4435						2019	20								115														
J								Change Surfaces for Expressive Multidimensional Changepoints and Counterfactual Prediction	JOURNAL OF MACHINE LEARNING RESEARCH										Change surface; changepoint; counterfactual; Gaussian process; scalable inference; kernel method	CAUSAL INFERENCE; STATISTICS	Identifying changes in model parameters is fundamental in machine learning and statistics. However, standard changepoint models are limited in expressiveness, often addressing unidimensional problems and assuming instantaneous changes. We introduce change surfaces as a multidimensional and highly expressive generalization of changepoints. We provide a model-agnostic formalization of change surfaces, illustrating how they can provide variable, heterogeneous, and non-monotonic rates of change across multiple dimensions. Additionally, we show how change surfaces can be used for counterfactual prediction. As a concrete instantiation of the change surface framework, we develop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual prediction with Bayesian posterior mean and credible sets, as well as massive scalability by introducing novel methods for additive nonseparable kernels. Using two large spatio-temporal datasets we employ GPCS to discover and characterize complex changes that can provide scientific and policy relevant insights. Specifically, we analyze twentieth century measles incidence across the United States and discover previously unknown heterogeneous changes after the introduction of the measles vaccine. Additionally, we apply the model to requests for lead testing kits in New York City, discovering distinct spatial and demographic patterns.																	1532-4435						2019	20								99														
J								Dynamic Pricing in High-dimensions	JOURNAL OF MACHINE LEARNING RESEARCH										Revenue Management; Dynamic Pricing; High-dimensional Regression; Maximum Likelihood; Regret; Sparsity	GENERALIZED 2ND-PRICE AUCTION; MINIMAX RATES; CHOICE	We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. Customers independently make purchasing decisions according to a general choice model that includes products features and customers' characteristics, encoded as d-dimensional numerical vectors, as well as the price offered. The parameters of the choice model are a priori unknown to the firm, but can be learned as the (binary-valued) sales data accrues over time. The firm's objective is to maximize its revenue. We benchmark the performance using the classic regret minimization framework where the regret is de fined as the expected revenue loss against a clairvoyant policy that knows the parameters of the choice model in advance, and always offers the revenue-maximizing price. This setting is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We assume a structured choice model, parameters of which depend on s(0) out of the d product features. Assuming that the market noise distribution is known, we propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of the high-dimensional model and obtains a logarithmic regret in T. More specifically, the regret of our algorithm is of O(s(0) log d . log T). Furthermore, we show that no policy can obtain regret better than O(s(0) (log d + log T)). In addition, we propose a generalization of our policy to a setting that the market noise distribution is unknown but belongs to a parametrized family of distributions. This policy obtains regret of O(root(log d)T). We further show that no policy can obtain regret better than Omega(root T) in such environments.																	1532-4435						2019	20								9														
J								An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search	JOURNAL OF MACHINE LEARNING RESEARCH										Change point regression; High dimensional models; l(1); l(0) regularization; Rate of convergence; Two phase regression	MAXIMUM-LIKELIHOOD ESTIMATOR; MULTIPLE CHANGE-POINTS; LINEAR-REGRESSION; LASSO; SELECTION; ASYMPTOTICS; SHRINKAGE	We propose a two step algorithm based on l(1)/l(0) regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only 2.Lasso(n, p), where Lasso(n, p) represents the computational burden of one Lasso optimization in a model of size (n, p). In comparison, existing grid search based approaches to this problem require a computational cost of at least n.Lasso(n, p) optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of 'no change', i.e., where no finite change point exists in the model. We allow the true change point parameter tau(0) to possibly move to the boundaries of its parametric space, and the jump size parallel to beta(0) - gamma(0)parallel to(2) to possibly diverge as n increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.																	1532-4435						2019	20								111														
J								Spectrum Estimation from a Few Entries	JOURNAL OF MACHINE LEARNING RESEARCH										spectrum estimation; matrix completion; counting subgraphs	MATRIX COMPLETION; TRACE; INVERSE; CYCLES	Singular values of a data in a matrix form provide insights on the structure of the data, the e ff ective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative fi ltering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. In this paper, we address the problem of directly recovering the spectrum, which is the set of singular values, and also in sample-e ffi cient approaches for recovering a spectral sum function, which is an aggregate sum of a fi xed function applied to each of the singular values. Our approach is to fi rst estimate the Schatten k -norms of a matrix for a small set of values of k, and then apply Chebyshev approximation when estimating a spectral sum function or apply moment matching in Wasserstein distance when estimating the singular values directly. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures called network motifs in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we signi fi cantly improve upon a competing approach of using matrix completion methods, below the matrix completion threshold, above which matrix completion algorithms recover the underlying low-rank matrix exactly.																	1532-4435						2019	20								21														
J								Parsimonious Online Learning with Kernels via Sparse Projections in Function Space	JOURNAL OF MACHINE LEARNING RESEARCH										kernel methods; online learning; stochastic optimization; supervised learning; orthogonal matching pursuit; nonparametric regression	SUPPORT VECTOR MACHINES; REGULARIZATION; NETWORKS	Despite their attractiveness, popular perception is that techniques for nonparametric function approximation do not scale to streaming data due to an intractable growth in the amount of storage they require. To solve this problem in a memory-affordable way, we propose an online technique based on functional stochastic gradient descent in tandem with supervised sparsification based on greedy function subspace projections. The method, called parsimonious online learning with kernels (POLK), provides a controllable tradeoff between its solution accuracy and the amount of memory it requires. We derive conditions under which the generated function sequence converges almost surely to the optimal function, and we establish that the memory requirement remains finite. We evaluate POLK for kernel multi-class logistic regression and kernel hinge-loss classification on three canonical data sets: a synthetic Gaussian mixture model, the MNIST hand-written digits, and the Brodatz texture database. On all three tasks, we observe a favorable trade-off of objective function evaluation, classification performance, and complexity of the nonparametric regressor extracted by the proposed method.																	1532-4435						2019	20								3														
J								TensorLy: Tensor Learning in Python	JOURNAL OF MACHINE LEARNING RESEARCH											DECOMPOSITIONS	Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly																	1532-4435						2019	20								26														
J								Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data	JOURNAL OF MACHINE LEARNING RESEARCH										Dictionary Learning; Multi-Resolution Analysis; Adaptive Approximation; Manifold Learning; Compression	UNIVERSAL ALGORITHMS; STRUCTURE DEFINITION; SAMPLE COMPLEXITY; HARMONIC-ANALYSIS; LEARNING-THEORY; SETS; DIFFUSIONS; REDUCTION; EIGENMAPS; POINTS	We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution rho in R-D, that is nearly supported on a d-dimensional set M - for example supported on a d-dimensional manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of M at varying resolutions. We introduce GMRA approximations that adapt to the unknown regularity of M, by introducing a thresholding algorithm on the geometric wavelet coefficients. We show that these data-driven, empirical geometric approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples n, on a large class of measures rho, that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These GMRA approximations are associated to a dictionary, together with a fast transform mapping data to d-dimensional coefficients, and an inverse of such a map, all of which are data-driven. The algorithms for both the dictionary construction and the transforms have complexity CDn log n with the constant C exponential in d. Our work therefore establishes Adaptive GMRA as a fast dictionary learning algorithm, with approximation guarantees, for intrinsically low-dimensional data. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of Adaptive GMRA.																	1532-4435						2019	20								98														
J								Graph Reduction with Spectral and Cut Guarantees	JOURNAL OF MACHINE LEARNING RESEARCH										graph reduction and coarsening; spectral methods; unsupervised learning	DIMENSIONALITY REDUCTION; SPARSIFICATION; INEQUALITIES; ALGORITHMS	Can one reduce the size of a graph without significantly altering its basic properties? The graph reduction problem is hereby approached from the perspective of restricted spectral approximation, a modification of the spectral similarity measure used for graph sparsification. This choice is motivated by the observation that restricted approximation carries strong spectral and cut guarantees, and that it implies approximation results for unsupervised learning problems relying on spectral embeddings. The article then focuses on coarsening|-the most common type of graph reduction. Sufficient conditions are derived for a small graph to approximate a larger one in the sense of restricted approximation. These findings give rise to algorithms that, compared to both standard and advanced graph reduction methods, find coarse graphs of improved quality, often by a large margin, without sacrificing speed.																	1532-4435						2019	20								116														
J								Solving the OSCAR and SLOPE Models Using a Semismooth Newton-Based Augmented Lagrangian Method	JOURNAL OF MACHINE LEARNING RESEARCH										Linear Regression; OSCAR; Sparsity; Augmented Lagrangian Method; Semismooth Newton method	GENE-EXPRESSION; REGRESSION SHRINKAGE; VARIABLE SELECTION; REGULARIZATION; LASSO	The octagonal shrinkage and clustering algorithm for regression (OSCAR), equipped with the l(1)-norm and a pair-wise l(infinity)-norm regularizer, is a useful tool for feature selection and grouping in high-dimensional data analysis. The computational challenge posed by OSCAR, for high dimensional and/or large sample size data, has not yet been well resolved due to the non-smoothness and non-separability of the regularizer involved. In this paper, we successfully resolve this numerical challenge by proposing a sparse semismooth Newton-based augmented Lagrangian method to solve the more general SLOPE (the sorted L-one penalized estimation) model. By appropriately exploiting the inherent sparse and low-rank property of the generalized Jacobian of the semismooth Newton system in the augmented Lagrangian subproblem, we show how the computational complexity can be substantially reduced. Our algorithm offers a notable computational advantage in the high-dimensional statistical regression settings. Numerical experiments are conducted on real data sets, and the results demonstrate that our algorithm is far superior, in both speed and robustness, to the existing state-of-the-art algorithms based on first-order iterative schemes, including the widely used accelerated proximal gradient (APG) method and the alternating direction method of multipliers (ADMM).																	1532-4435						2019	20								106														
J								Multiplicative local linear hazard estimation and best one-sided cross-validation	JOURNAL OF MACHINE LEARNING RESEARCH										Aalen's multiplicative model; multiplicative bias correction; bandwidth; indirect cross-validation	BANDWIDTH SELECTION; BIAS CORRECTION; DENSITY	This paper develops detailed mathematical statistical theory of a new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. The new class of cross-validation combines principles of local information and recent advances in indirect cross-validation. A few applications of cross-validating multiplicative kernel hazard estimation do exist in the literature. However, detailed mathematical statistical theory and small sample performance are introduced via this paper and further upgraded to our new class of best one-sided cross-validation. Best one-sided cross-validation turns out to have excellent performance in its practical illustrations, in its small sample performance and in its mathematical statistical theoretical performance.																	1532-4435						2019	20								18														
J								A Particle-Based Variational Approach to Bayesian Non-negative Matrix Factorization	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian; Non-negative Matrix Factorization; Stein discrepancy; Non-identifiability; Transfer Learning	ALGORITHMS; INITIALIZATION	Bayesian Non-negative Matrix Factorization (BNMF) is a promising approach for understanding uncertainty and structure in matrix data. However, a large volume of applied work optimizes traditional non-Bayesian NMF objectives that fail to provide a principled understanding of the non-identifiability inherent in NMF-an issue ideally addressed by a Bayesian approach. Despite their suitability, current BNMF approaches have failed to gain popularity in an applied setting; they sacrifice flexibility in modeling for tractable computation, tend to get stuck in local modes, and can require many thousands of samples for meaningful uncertainty estimates. We address these issues through a particle-based variational approach to BNMF that only requires the joint likelihood to be differentiable for computational tractability, uses a novel transfer-based initialization technique to identify multiple modes in the posterior, and thus allows domain experts to inspect a small set of factorizations that faithfully represent the posterior. On several real datasets, we obtain better particle approximations to the BNMF posterior in less time than baselines and demonstrate the significant role that multimodality plays in NMF-related tasks.																	1532-4435						2019	20								90														
J								Train and Test Tightness of LP Relaxations in Structured Prediction	JOURNAL OF MACHINE LEARNING RESEARCH											APPROXIMATION ALGORITHMS; FACETS	Structured prediction is used in areas including computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation for the striking observation that approximations based on linear programming (LP) relaxations are often tight (exact) on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that this training tightness generalizes to test data.																	1532-4435						2019	20								13														
J								Model Selection via the VC Dimension	JOURNAL OF MACHINE LEARNING RESEARCH										Vapnik-Chervonenkis dimension; model selection; Bayesian information criterion; sparsity methods; empirical risk minimization; multi-type data	TRAITS	We derive an objective function that can be optimized to give an estimator for the Vapnik-Chervonenkis dimension for use in model selection in regression problems. We verify our estimator is consistent. Then, we verify it performs well compared to seven other model selection techniques. We do this for a variety of types of data sets.																	1532-4435						2019	20								88														
J								High-Dimensional Poisson Structural Equation Model Learning via l(1)-Regularized Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian Networks; Directed Acyclic Graph; Identifiability; Structure Learning; l(1)-Regularization; Multivariate Count Distribution	BAYESIAN NETWORKS	In this paper, we develop a new approach to learning high-dimensional Poisson structural equation models from only observational data without strong assumptions such as faithfulness and a sparse moralized graph. A key component of our method is to decouple the ordering estimation or parent search where the problems can be efficiently addressed using l(1)-regularized regression and the moments relation. We show that sample size n = Omega(d(2) log(9) p) is sufficient for our polynomial time Moments Ratio Scoring (MRS) algorithm to recover the true directed graph, where p is the number of nodes and d is the maximum indegree. We verify through simulations that our algorithm is statistically consistent in the high-dimensional p > n setting, and performs well compared to state-of-the-art ODS, GES, and MMHC algorithms. We also demonstrate through multivariate real count data that our MRS algorithm is well-suited to estimating DAG models for multivariate count data in comparison to other methods used for discrete data.																	1532-4435						2019	20								95														
