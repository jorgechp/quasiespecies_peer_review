PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Genetic algorithm-based fuzzy clustering applied to multivariate time series	EVOLUTIONARY INTELLIGENCE										Genetic algorithm; Fuzzy-C-Means; Multivariate time series; Fault detection; Tennessee Eastman process	C-MEANS; PREDICTION	Despite the fact that the fuzzy clustering of time series based on genetic algorithm (GA) is mostly used in applications involving univariate time series, this paper presents an approach based on GA and Fuzzy C-Means (FCM) for clustering multivariate time series. Each chromosome is an individual or solution which encodes the clusters' centroids (patterns) and a bi-criterion constrained clustering is proposed to maximize both the similarity of objects in the same cluster (based on the SPCA metric) and the distance between the centers of the clusters. The proposed method is applied in two case studies involving a real industrial case which comprises pattern recognition for detecting operation failures in a gas turbine and a well-known benchmark industrial system (Tennessee Eastman process) used to evaluate techniques for detecting and diagnosing failures. The proposed approach was able to obtain better classification results compared to FCM based on classical optimization methods.																	1864-5909	1864-5917															10.1007/s12065-020-00422-8		MAY 2020											
J								Factor relation analysis for sustainable recycling partner evaluation using probabilistic linguistic DEMATEL	FUZZY OPTIMIZATION AND DECISION MAKING										DEMATEL; PLTS; Sustainable recycling partner evaluation; GWOWA	TERM SETS; FUZZY; AGGREGATION	Evaluating and selecting suitable sustainable recycling partners is a key work in sustainable supply chain management. In order to deal with the probabilistic linguistic influence relations between criteria and obtain the key factors that influence the evaluation results of sustainable recycling partners, we propose a new decision-making trial and evaluation laboratory (DEMATEL) method. First, we propose a new generalized weighted ordered weighted averaging (GWOWA) operator and discuss its properties. Second, we use probabilistic linguistic term sets (PLTSs) to aggregate the experts' hesitant fuzzy linguistic decision-making information and develop a novel method of transforming PLTSs into triangular fuzzy numbers (TFNs) based on the proposed GWOWA operator and the characteristics of PLTSs. Furthermore, we propose a method of making criteria relation analysis based on DEMATEL with TFNs. With the method, we not only access the importance weights of criteria but also obtain the influence relation among the criteria and cluster the criteria into two groups: cause group and effect group. Finally, we apply our method to a real case of sustainable recycling partner selection.																	1568-4539	1573-2908															10.1007/s10700-020-09326-9		MAY 2020											
J								Optimal day-ahead scheduling of autonomous operation for the hybrid micro-grid including PV, WT, diesel generator, and pump as turbine system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hybrid micro-grid; Optimal autonomous operation; Pump as turbine; Photovoltaic; Wind turbine; Diesel generator	IMPERIALIST COMPETITIVE ALGORITHM; OPTIMIZATION ALGORITHM; POWER-GENERATION; STORAGE; DESIGN; PERFORMANCE; MANAGEMENT; WIND/PV; COST	A diesel generator can be used in joint with sustainable energy sources such as wind turbine, photovoltaic which develop an ideal option for autonomous power generation in islanded mode. However, the stochastic inherent of irradiation and wind speed as well as the variable load demand prevent these generation systems from being totally reliable without suitable energy storage system. However, considering other research works show a pump as turbine system is used more less as energy storage in the agricultural industry and isolated rural micro-grid. Also, practical constraints of pump as turbine system may be challenging not considered in the later papers. Hence, this paper intends to present an optimal operation and energy management method for a hybrid micro-grid, including photovoltaic, wind turbine, pump as turbine system, and diesel generator, with study on day-ahead scheduling. The optimal energy management minimize the fuel cost of diesel generator, daily operating cost as well as the balance between generation and load for both warm and cold days. In this paper, imperialist competitive algorithm, which is not commonly used in the applications above, is used because of its high convergence rate and accuracy. To verify proposed method, results are compared to artificial bee colony and particle swarm optimization to determine the optimal diesel generator generation, pumped storage operating mode, pumping and turbine real power. The simulation results show the effectiveness and improvement of the TSA compared to other works.																	1868-5137	1868-5145															10.1007/s12652-020-02114-8		MAY 2020											
J								Cooperative flow regulation protocol for real-time and non-real-time applications over satellite network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Satellite; Real-time; Non-real-time; Fairness; Congestion; Error	FAIRNESS	In current times, the intensity of Internet usage has been on a relentless rise, in pace with the release of numerous real-time applications like video transmission and non-real-time applications. Due to high error channels, the end-users will observe poor quality video over satellite network. The applications are competing themselves to inject their traffic into the network and hence end users get unfair shares of the bandwidth for the video transmissions. Also, the high transmission rate of video applications affects traditional Internet applications like e-mail. TCP-friendly rate control (TFRC), TFRC-satellite and multiple TFRC streams (MulTFRC), which are intended to provide congestion control, error control and increase bandwidth (BW) utilization respectively, may fail over the satellite networks with high error rate and high congestion. Besides, these connection-oriented protocols increase the applications overhead. In this paper, we present a cooperative flow regulation protocol (CFR), which provides fair access to both real-time and non-real-time applications, over networks with high congestion and high error. Verification of the proffered approach, by simulations, show that the CFR protocol reduces the loss of video frames by 30-40%, compared to DCCP and MulTFRC, and 50-60% compared to user datagram protocol (UDP) traffic, without compromise of the video quality.																	1868-5137	1868-5145															10.1007/s12652-020-02115-7		MAY 2020											
J								Preclinical diagnosis of asthma with GMR sensor and RADWT algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Asthma; RADWT; Subband energy; Mucus; Lung; Airway constriction	MUCUS HYPERSECRETION	Asthma, a chronic lung disease which causes severe problem in breathing. The asthma cause due to lung airway constriction by triggers, such as allergic reactions to dust or pollen. The asthma never diagnose at early stage since symptoms such as coughing confuse with cold. The Asthma only diagnose when shortness of breath occur and it causes an examination by a physician. In this paper, we propose a non-invasive way to diagnose asthma at early stage via bio magnetic signal acquired from a person lung. The mucus in airway of lung clogs airway during constriction. The Giant magnetoresistance (GMR) sensor place on lung region and bio-magnetic emission from mucus in lung acquire. The biomagnetic signal of mucus in lung constricted airway change due to mucus normal and dynamic condition. The constricted lung airway cure by Ventolin entolin inhaler, which makes constricted muscle to relax and provide airflow. The mucus accumulation and mucus flow provide varying bio-magnetic emission. The bio-magnetic emission change correlate with asthma through RADWT subband energy level. The RADWT subband energy level and airflow meter test model with linear regression for lung airway opening and asthma diagnosis.																	1868-5137	1868-5145															10.1007/s12652-020-01966-4		MAY 2020											
J								Enhanced F-OFDM candidate for 5G applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										4G; 5G; CP-OFDM; FBMC; Filter design; Filtered-OFDM (f-OFDM); Time-frequency localization; UFMC; UF-OFDM	SYSTEM; CHALLENGES; NETWORKS; DESIGN; WILL; PAPR	The demand for high data rate, the generation of Internet of Things (IoT), and various Machine Type Communications (MTC) emerged for a new transmission phenomenon. In other words, it is substantial to communicate without synchronization, or synchronization overhead, with mixed signal types. such specifications cannot be covered by the Fourth Generation (4G) systems, which is based on Cyclic Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM). However, to achieve the specifications of the next generation system, numerous waveform replacements for the CP-OFDM were suggested, Filter Bank Multi-Carrier (FBMC), Generalized Frequency Division Multiplexing (GFDM), Universal Filtered Multi-Carrier (UFMC), and Filtered OFDM (F-OFDM). The filter design occupies essential part in these replacements, thus, in this paper, novel filters are introduced where simulation results show that the proposed filters outperform previous designs in terms of spectral efficiency improved dramatically by releasing the synchronization overhead.																	1868-5137	1868-5145															10.1007/s12652-020-02046-3		MAY 2020											
J								Survival neural networks for time-to-event prediction in longitudinal study	KNOWLEDGE AND INFORMATION SYSTEMS										Time-to-event prediction; Longitudinal study; Survival neural network; Classification probability; Time-dependent covariate	COX REGRESSION-MODELS; NONPARAMETRIC-ESTIMATION; VARIABLE SELECTION; COEFFICIENTS	Time-to-event prediction has been an important practical task for longitudinal studies in many fields such as manufacturing, medicine, and healthcare. While most of the conventional survival analysis approaches suffer from the presence of censored failures and statistically circumscribed assumptions, few attempts have been made to develop survival learning machines that explore the underlying relationship between repeated measures of covariates and failure-free survival probability. This requires a purely dynamic-data-driven prediction approach, free of survival models or statistical assumptions. To this end, we propose two real-time survival networks: a time-dependent survival neural network (TSNN) with a feed-forward architecture and a recurrent survival neural network (RSNN) incorporating long short-term memory units. The TSNN additively estimates a latent failure risk arising from the repeated measures and performs multiple binary classifications to generate prognostics of survival probability, while the RSNN with time-dependent input covariates implicitly estimates the relation between these covariates and the survival probability. We propose a novel survival learning criterion to train the neural networks by minimizing the censoring Kullback-Leibler divergence, which guarantees monotonicity of the resulting probability. Besides the failure-event AUC, C-index, and censoring Brier score, we redefine a survival time estimate to evaluate the performance of the competing models. Experiments on four datasets demonstrate the great promise of our approach in real applications.																	0219-1377	0219-3116				SEP	2020	62	9					3727	3751		10.1007/s10115-020-01472-1		MAY 2020											
J								An enhanced monarch butterfly optimization with self-adaptive crossover operator for unconstrained and constrained optimization problems	NATURAL COMPUTING										Monarch butterfly optimization; Migration operator; Butterfly adjusting operator; Crossover operator; Self-adaptive; Constrained optimization	CUCKOO SEARCH ALGORITHM; ARTIFICIAL BEE COLONY; KRILL HERD; EVOLUTIONARY ALGORITHMS; DIFFERENTIAL EVOLUTION; FIREFLY ALGORITHM; KNAPSACK-PROBLEMS; STRATEGIES; CRYPTANALYSIS; ENSEMBLE	Inspired by the phenomenon of migration of monarch butterflies, Wang et al. developed a novel promising swarm intelligence algorithm, called monarch butterfly optimization (MBO), for addressing unconstrained low-dimensional optimization problems. In this paper, we firstly extend the application area of the basic MBO to solve the constrained optimization problems. At the same time, the crossover operator originally used in evolutionary algorithms (EAs) is incorporated into the butterfly adjusting operator in order to strengthen the exploitation of the basic MBO algorithm. Furthermore, the crossover rate is self-adaptively adjusted according to the fitness of the corresponding individual instead of the fixed crossover rate used in EAs. For migration operator, only individuals having better fitness are accepted and passed to the next generation instead of accepting all the individuals in the basic MBO algorithm. After incorporated all the modifications into the basic MBO algorithm, an improved MBO algorithm with self-adaptive crossover namely SACMBO, is proposed for unstrained and constrained optimization problems. Finally, the proposed SACMBO algorithm is further used to solve 22 unstrained optimization problems (with dimension of 100, 300, 500, 1000, and 1500) and 28 constrained real-parameter optimization functions from CEC 2017 competition (with dimension of 50 and 100), respectively. The experimental results indicate that the proposed SACMBO algorithm outperforms the basic MBO and other five state-of-the-art metaheuristic algorithms.																	1567-7818	1572-9796															10.1007/s11047-020-09794-3		MAY 2020											
J								An image database of handwritten Bangla words with automatic benchmarking facilities for character segmentation algorithms	NEURAL COMPUTING & APPLICATIONS										Character segmentation; Handwritten word; Bangla script; Image database; Word recognition	RECOGNITION	Recognition of unconstrained handwritten word images is an interesting research problem which gets more challenging when lexicon-free words are considered. Prerequisite for developing a lexicon-free handwritten word recognition technique is the segmentation of a word image into its constituent character set. Therefore, a competent character segmentation technique is required to design a comprehensive word recognition module. However, the literature study reveals that there is no standard word image database with ground truth information. As a result, most character segmentation algorithms found in the literature rely on self-made databases with manual evaluation. To fill the research need, in the present scope of the work, a comprehensive database consisting of handwritten Bangla word images is prepared primarily for evaluating any character segmentation algorithms. Additionally, the present work also provides two types of ground truth images related to segmented character shapes of the word images. Besides, an evaluation tool is developed for assessing the performance of any character segmentation algorithm on the developed benchmark database. The benchmark result, as found here, is 0.9212 (F-score) which outperforms some state-of-the-art methods.																	0941-0643	1433-3058															10.1007/s00521-020-04981-w		MAY 2020											
J								A stacked ensemble learning model for intrusion detection in wireless network	NEURAL COMPUTING & APPLICATIONS										Network intrusion detection; Gradient boosting; Classification algorithms; Machine learning; Ensemble learning; Random forest tree	MACHINE	Intrusion detection pretended to be a major technique for revealing the attacks and guarantee the security on the network. As the data increases tremendously every year on the Internet, a single algorithm is not sufficient for the network security. Because, deploying a single learning approach may suffer from statistical, computational and representational issues. To eliminate these issues, this paper combines multiple machine learning algorithms called stacked ensemble learning, to detect the attacks in a better manner than conventional learning, where a single algorithm is used to identify the attacks. The stacked ensemble system has been taken the benchmark data set, NSL-KDD, to compare its performance with other popular machine learning algorithms such as ANN, CART, random forest, SVM and other machine learning methods proposed by researchers. The experimental results show that stacked ensemble learning is a proper technique for classifying attacks than other existing methods. And also, the proposed system shows better accuracy compare to other intrusion detection models.																	0941-0643	1433-3058															10.1007/s00521-020-04986-5		MAY 2020											
J								A deeply coupled ConvNet for human activity recognition using dynamic and RGB images	NEURAL COMPUTING & APPLICATIONS										Bi-LSTM; Deep learning; Dynamic motion image; Human activity recognition	REPRESENTATIONS	This work is motivated by the tremendous achievement of deep learning models for computer vision tasks, particularly for human activity recognition. It is gaining more attention due to the numerous applications in real life, for example smart surveillance system, human-computer interaction, sports action analysis, elderly healthcare, etc. Recent days, the acquisition and interface of multimodal data are straightforward due to the invention of low-cost depth devices. Several approaches have been developed based on RGB-D (depth) evidence at the cost of additional equipment's setup and high complexity. Contrarily, the methods that utilize RGB frames provide inferior performance due to the absence of depth evidence and these approaches require to less hardware, simple and easy to generalize using only color cameras. In this work, a deeply coupled ConvNet for human activity recognition proposed that utilizes the RGB frames at the top layer with bi-directional long short-term memory (Bi-LSTM). At the bottom layer, the CNN model is trained with a single dynamic motion image. For the RGB frames, the CNN-Bi-LSTM model is trained end-to-end learning to refine the feature of the pre-trained CNN, while dynamic images stream is fine-tuned with the top layers of the pre-trained model to extract temporal information in videos. The features obtained from both the data streams are fused at decision level after the softmax layer with different late fusion techniques and achieved high accuracy with max fusion. The performance accuracy of the model is assessed using four standard single as well as multiple person activities RGB-D (depth) datasets. The highest classification accuracies achieved on human action datasets are compared with similar state of the art and found significantly higher margin such as 2% on SBU Interaction, 4% on MIVIA Action, 1% on MSR Action Pair, and 4% on MSR Daily Activity.																	0941-0643	1433-3058															10.1007/s00521-020-05018-y		MAY 2020											
J								Prediction of TBM penetration rate based on Monte Carlo-BP neural network	NEURAL COMPUTING & APPLICATIONS										TBM; Penetration rate; Machine learning; BP neural network; Monte Carlo method	PERFORMANCE PREDICTION; MACHINE; ROCK; DRIVERS; MODEL	Based on the BP neural network model of machine learning method, the corresponding random input parameters are generated by Monte Carlo method, and the prediction of TBM driving speed is studied. In this study, the machine learning method is applied to the prediction of TBM penetration rate, and the established empirical model has higher accuracy and practicability. After selecting the predictive control type of BP neural network, according to the control requirements of TBM, system composition and the characteristics of different geological tunneling, the appropriate data are selected to train the neural network, and the predictive control model of neural network for TBM with high convergence and real-time performance is established. Monte Carlo method has strong optimization and control functions in the realistic planning of many complex problems. In the process of TBM velocity prediction, the random input of parameters is realized by Monte Carlo method, which makes the prediction more accurate. BP neural network is used to predict the penetration rate of TBM. Its accuracy mainly depends on the accuracy of input parameters. The actual measured and predicted values of TBM driving speed are basically near the straight line x = y as the horizontal and vertical coordinates, and the correlation coefficient R = 0.9789. Therefore, the BP neural network combined with genetic algorithm has a high reference value for the prediction of TBM driving speed. When the TBM type is the same and the system equipment is the same, four factors, namely uniaxial compressive strength, Brazilian tensile strength, peak slope index, and distance between planes of weakness, are taken as input parameters of BP network by calculating the weight of influencing factors. In the specific operation, the genetic algorithm is used to iterate continuously to find the optimal solution of the initial weight parameters of BP neural network. In this study, this prediction method is applied to practical prediction. The feasibility of this method is verified by comparing with the final actual measurement result, which is of great practical significance to the evaluation of engineering, design scheme and cost control.																	0941-0643	1433-3058															10.1007/s00521-020-04993-6		MAY 2020											
J								Investigation of multiple heterogeneous relationships using a q-rung orthopair fuzzy multi-criteria decision algorithm	NEURAL COMPUTING & APPLICATIONS										Q-rung orthopair fuzzy set; Multiple heterogeneous relationships; Interaction operators; Maclaurin symmetric mean operators	INTERACTION AGGREGATION OPERATORS; GEOMETRIC INTERACTION OPERATORS; MEAN OPERATORS; OPERATIONS	Q-rung orthopair fuzzy (q-ROF) set is one of the powerful tools for handling the uncertain multi-criteria decision-making (MCDM) problems, various MCDM methods under q-ROF environment have been developed in recent years. However, most existing studies merely concerned about the relationship between the criteria but they have not investigated the interactions between membership function and non-membership function. To explore the multiple heterogeneous relationships among membership functions and criteria, we propose a novel decision algorithm based on q-ROF set to deal with these using interactive operators and Maclaurin symmetric mean (MSM) operators. Specifically, the new interaction laws in the membership pairs of q-ROF sets are explained, and their properties are analyzed as the initial stage. Then, taking into account the influence of two or more factors on decision analysis, a q-ROF interaction Maclaurin symmetry mean (q-ROFIMSM) operator is formed based on the proposed interaction law to identify these factors' interrelationship. Thirdly, based on the proposed operator with q-ROF information, a MCDM algorithm is developed and illustrated by numerical examples. An analysis of the feasibility, sensitivity, and superiority of the proposed framework is provided to validate our proposed method.																	0941-0643	1433-3058															10.1007/s00521-020-05003-5		MAY 2020											
J								Fukunaga-Koontz Convolutional Network with Applications on Character Classification	NEURAL PROCESSING LETTERS										Subspace method; Shallow networks; Fukunaga-Koontz transform	NEURAL-NETWORK; RECOGNITION; SUBSPACE; VISION	Several convolutional neural network architectures have been proposed for handwritten character recognition. However, most of the conventional architectures demand large scale training data and long training time to obtain satisfactory results. These requirements prevent the use of these methods in a broader range of applications. As an alternative to cope with these problems, we present a new convolutional network for handwritten character recognition based on the Fukunaga-Koontz transform (FKT). Our approach lies in the assumption that Fukunaga-Koontz convolutional kernels can be efficiently learned from subspaces and directly employed to produce high discriminant features in a shallow network architecture. When representing image classes by subspaces, the within-class separability is reduced, since the subspaces form clusters in a low-dimensional space. To increase the between-class separability, we compute a discriminative space from the training subspaces using FKT. By learning convolutional kernels from subspaces, it is possible to extract representative and discriminative features from an image with only a few parameters. Another contribution of the proposed network is the use of pooling layers, which further improves its performance. The proposed method, called Fukunaga-Koontz Network (FKNet), is suitable for solving practical problems, especially when training and processing times are constraints. Four publicly available handwritten character datasets are employed to evaluate the advantages of FKNet. In addition, we demonstrate the flexibility of the proposed method by experiments on LFW dataset.																	1370-4621	1573-773X				AUG	2020	52	1			SI		443	465		10.1007/s11063-020-10244-5		MAY 2020											
J								Unsupervised Domain Adaptation via Discriminative Classes-Center Feature Learning in Adversarial Network	NEURAL PROCESSING LETTERS										Unsupervised domain adaptation; Layer normalization; Adversarial learning; Center loss	KERNEL	Adversarial learning has achieved remarkable advance in learning transferable representations across different domains. Generally, previous works are mainly devoted to reducing domain shift between labeled source data and unlabeled target data by extracting domain-invariant features. However, these adversarial methods rarely consider task-specific decision boundaries among classes, causing classification performance degradation in cross domain tasks. In this paper, we propose a novel approach for the task of unsupervised domain adaptation via discriminative classes-center feature learning in adversarial network (C2FAN), which concentrates on learning domain-invariant representation and paying close attention to classification decision boundary simultaneously to improve the ability of transferable knowledge across different domains. C2FAN consists of a feature extractor, a classifier and a discriminator. Firstly, for reducing domain gaps between source and target domains in the feature extractor, we propose to utilize a conditional adversarial learning module to extract domain-invariant feature and improve discriminability of the classifier simultaneously. Further, we present a high-efficiency layer normalization module to reduce domain shift existing in the classifier. Secondly, we design a discriminative classes-center feature learning module in the classifier to diminish the distribution distance of the same-class samples so that the decision boundary can distinguish different classes easily, which can reduce the misclassification on target samples. What's more, C2FAN is an effective yet considerable simple approach which can be embedded into current domain adaptation approaches conveniently. Extensive experiments demonstrate that our proposed model achieves satisfactory results on some standard domain adaptation benchmarks.																	1370-4621	1573-773X				AUG	2020	52	1			SI		467	483		10.1007/s11063-020-10266-z		MAY 2020											
J								Fixed-Time Lag Synchronization Analysis for Delayed Memristor-Based Neural Networks	NEURAL PROCESSING LETTERS										Memristor; Neural networks; Time delays; Fixed-time lag synchronization	CHAOTIC SYSTEMS; STABILIZATION; STABILITY	In this paper, the fixed-time lag synchronization for a general class of memristor-based neural networks (MNNs) with time delays is considered. Under the extended Filippov-framework theory, some sufficient criteria for fixed-time lag synchronization of delayed MNNs are derived based on the Lyapunov function. Besides, two types of controllers are given to ensure the fixed-time lag synchronization of the corresponding system, while the settling time of synchronization are also estimated. Finally, a numerical example is given to demonstrate the effectiveness of the developed method and the theoretical results.																	1370-4621	1573-773X				AUG	2020	52	1			SI		485	509		10.1007/s11063-020-10249-0		MAY 2020											
J								Two-stage approach for the inference of the source of high-dimensional and complex chemical data in forensic science	JOURNAL OF CHEMOMETRICS										forensic science; FTIR; kernel method; paint evidence	LEVEL	Forensic chemists are often criticised for the lack of quantitative support for the conclusions of their examinations. While scholars advocate for the use of a Bayes factor to quantify the weight of forensic evidence, it is often impossible to assign the necessary probability measures to perform likelihood-based inference on chemical data. To address this issue, we leverage the properties of kernel functions to offer a method that allows for statistically supporting the inference of the identity of source of sets of trace and control objects by way of a single test. Our method is generic in that it can be easily tailored to any type of data encountered in forensic chemistry, and our method does not depend on the dimension or the type of the considered data. The application of our method to paint evidence analysed by FTIR shows that this type of evidence carries substantial probative value. Finally, our approach can easily be extended to other types of chemical evidence such as glass, fibres, and dust.																	0886-9383	1099-128X														e3247	10.1002/cem.3247		MAY 2020											
J								A probability first memetic algorithm for the dynamic multiple-fault diagnosis problem with non-ideal tests	MEMETIC COMPUTING										Evolutionary framework; Factorial hidden markov models (FHMM); Memetic algorithm (MA); Multiple faults diagnosis; Non-ideal tests; Local search strategy	BOND GRAPH; OPTIMIZATION; MODEL	The factorial hidden Markov modeling framework, has been proposed in the literature as an effective approach for intermittent fault diagnosis applications. Moreover, the dynamic multiple fault diagnosis problem (DMFD) with non-ideal tests is computationally challenging. Several methodologies have been introduced for the solution of the DMFD problem, namely: the Lagrangian relaxation algorithm, the deterministic simulated annealing and the block coordinate ascent. However, to the best of our knowledge, no evolutionary algorithm including analysis of DMFD characteristics has ever been attempted. This research paper, presents a probability first memetic algorithm (PFMA) for solving the DMFD combinatorial optimization problem, by considering input probability matrices. PFMA improves the performance of the genetic algorithm, with less computational cost. This is achieved by exploiting input probability information, using a local search strategy and a crossover operator. Computational results show that the proposed algorithm finds the optimal solutions on small-scale stochastic synthesis models. Additionally, high-quality solutions are obtained on medium and large-scale cases. The algorithm has been tested successfully on a real-world application. The experimental results highlight the efficiency of the genetic algorithm, following a reasonable local search strategy, towards the solution of an NP-hard combinatorial optimization problem.																	1865-9284	1865-9292				JUN	2020	12	2					101	113		10.1007/s12293-020-00304-7		MAY 2020											
J								A pointer network based deep learning algorithm for unconstrained binary quadratic programming problem	NEUROCOMPUTING										UBQP; Pointer network; Supervised learning; Deep reinforcement learning	COMBINATORIAL OPTIMIZATION	Combinatorial optimization problems have been widely used in various fields. And many types of combinatorial optimization problems can be generalized into the model of unconstrained binary quadratic programming (UBQP). Therefore, designing an effective and efficient algorithm for UBQP problems will also contribute to solving other combinatorial optimization problems. Pointer network is an end-to-end sequential decision structure and combines with deep learning technology. With the utilization of the structural characteristics of combinatorial optimization problems and the ability to extract the rule behind the data by deep learning, pointer network has been successfully applied to solve several classical combinatorial optimization problems. In this paper, a pointer network based algorithm is designed to solve UBQP problems. The network model is trained by supervised learning (SL) and deep reinforcement learning (DRL) respectively. Trained pointer network models are evaluated by self-generated benchmark dataset and ORLIB dataset respectively. Experimental results show that pointer network model trained by SL has strong learning ability to specific distributed dataset. Pointer network model trained by DRL can learn more general distribution data characteristics. In other words, it can quickly solve problems with great generalization ability. As a result, the framework proposed in this paper for UBQP has great potential to solve large scale combinatorial optimization problems. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						1	11		10.1016/j.neucom.2019.06.111													
J								Study of the influence of lexicon and language restrictions on computer assisted transcription of historical manuscripts	NEUROCOMPUTING										Handwritten text recognition; Deep learning; Interactive transcription	HANDWRITING RECOGNITION; INFORMATION	State-of-the-art Handwritten Text Recognition (HTR) systems allow transcribers to speed-up the transcription of handwritten text images. These systems provide transcribers an initial draft transcription that can be corrected with less effort than transcribing the handwritten text images from scratch. Currently, even the draft transcriptions offered by the most advanced HTR systems contain errors. Therefore, the supervision of this draft by a human transcriber is still necessary to obtain the correct transcription of the handwritten text images. This supervision can be eased by using interactive and assistive transcription systems, where the transcriber and the automatic system cooperate in the amending process. In this paper, the draft transcription is provided by an HTR system based on Convolutional and Recurrent Neural Networks with Bidirectional Long-Short Term Memory units, and the assistive system is fed by lattices generated by using Weighted Finite State Transducers. The influence of the lexicon and language restrictions on the performance of our computer assisted transcription system is evaluated on three historical manuscripts. The transcriptions offered by the proposed HTR system present very low error rates for the studied historical manuscripts. However, our assistive transcription system without lexicon or language restrictions is able to provide an additional reduction on the human effort required to correct the transcriptions in more than 50% over the transcriptions offered by the HTR system. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						12	27		10.1016/j.neucom.2020.01.081													
J								RF-OSFBLS: An RFID reader-fault-adaptive localization system based on online sequential fuzzy broad learning system	NEUROCOMPUTING										Radio frequency identification (RFID); Indoor localization; Online sequential fuzzy broad learning system (OSFBLS); Reader-fault-adaptive (RF)	OPTIMIZATION; SCHEME	Indoor localization technology has recently attracted an increasing attention in research, among which radio frequency identification (RFID) technology has become a preferred solution due to its advantages in non-line of sight, non-contact and rapid identification. However, in the practical RFID indoor localization application scenarios, the RFID readers need to keep working for an extensive time. When some readers malfunction and fail to be repaired in time, the existing algorithms usually cannot maintain the accuracy of the original localization system. In this paper, we propose an RFID reader-fault-adaptive localization algorithm based on online sequential fuzzy broad learning system, called RF-OSFBLS algorithm. The RF-OSFBLS algorithm improves the fuzzy broad learning system (FBLS) with the ability of online sequential learning (OSFBLS) by using the updating algorithm, so that it can process data streams that continue to arrive in the environment. Meanwhile, RF-OSFBLS algorithm proposes RFID reader-fault-adaptive strategy by introducing a transformation matrix, which can process subsequent data streams when reader fault occurs. We have carried out experiments to study the influence factors and validate the performance, both the simulation and realistic experiment results show that our proposed RF-OSFBLS algorithm can achieve better positioning effect and maintain a relatively high accuracy in the dynamically changing and reader-fault environment. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						28	39		10.1016/j.neucom.2020.01.080													
J								Multi-agent actor centralized-critic with communication	NEUROCOMPUTING										Multi-agent systems; Neural networks; Emergent communication; Reinforcement learning; Distributed deep learning	COMPREHENSIVE SURVEY; REINFORCEMENT; ALGORITHMS	Multiple real-world problems are naturally modeled as cooperative multi-agent systems, ranging from satellite formation to traffic monitoring. These systems require algorithms that can learn successful policies with independent agents that rely solely on local partial-observations of the environment. However, multi-agent environments are more complex, due to their partial-observability and non-stationarity from an agent's perspective, as well as the structural credit assignment problem and the curse of dimensionality, and achieving coordination in such systems remains a complex challenge. To this end, we propose a multi-agent actor-critic algorithm called Asynchronous Advantage Actor Centralized-Critic with Communication (A3C3). A3C3 uses a centralized critic to estimate a value function, decentralized actors to approximate each agent's policy function, and decentralized communication networks for each agent to share relevant information with its team. The critic can incorporate additional information, like the environment's global state, when available, and optimizes the actor networks. The actor networks of an agent's teammates optimize that agent's communication network, such that each agent learns to output information that is relevant to the policies of others. A3C3 supports a dynamic amount of agents, noisy communication mediums, and can be horizontally scaled to shorten its learning phase. We evaluate A3C3 in two partially-observable multi-agent suites where agents benefit from communicating local information to each other. A3C3 outperforms state-of-the-art multi-agent algorithms, independent approaches, and centralized controllers with access to all agents' observations. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						40	56		10.1016/j.neucom.2020.01.079													
J								Group consensus of multi-agent systems with cooperative-competitive interaction and communication delay in switching topologies networks based on the delta operator method	NEUROCOMPUTING										Group consensus; Multi-agent systems; Delta operator method; Cooperation-competition network; Communication delay; Switching topologies	COUPLE-GROUP CONSENSUS; LEADER-FOLLOWING CONSENSUS; H-INFINITY; AGENTS; SUBJECT	In this paper, group consensus of multi-agent systems with cooperative-competitive interaction and communication delay is investigated in switching topologies networks. The delta operator method and linear matrix inequality technique are used to establish the criteria on group consensus of this system. Combined with the stability theory, some sufficient conditions are given to guard the success of the group consensus of the multi-agent systems with switching topologies. Using this method, the problem of group consensus can be transformed into determining the globally asymptotically stable of a system with communication delay. The results can be applied to two situations on faults and communication delay without meeting the in-degree balance condition or spanning tree of part topologies. Several numeric simulations are given to illustrate the effectiveness of the theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						57	68		10.1016/j.neucom.2020.01.076													
J								Optimal periodic DoS attack with energy harvester in cyber-physical systems	NEUROCOMPUTING										Periodic DoS attacks; Cyber-physical systems; Energy harvesting; Remote state estimation	STATE ESTIMATION; ALLOCATION; SUBJECT	This paper investigates the security of cyber-physical systems (CPSs) in the presence of periodic denial-of-service (DoS) attacks with energy harvester. The objective is designing periodic DoS attacks to maximize the attack performance on estimation error. For the jammer equipped with an energy harvester, two scenarios are studied: the energy harvester collects constant or time-varying energy during every period. For the first scenario, we present the optimal periodic attack strategy by dealing with the Markov decision process. For the second scenario, we offer an algorithm to obtain the optimal attack strategy, which is an improved strategy search method adopted to solve the optimization problems. The numerical results are presented to illustrate the effectiveness of proposed methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						69	77		10.1016/j.neucom.2020.01.075													
J								Design and analysis of three nonlinearly activated ZNN models for solving time-varying linear matrix inequalities in finite time	NEUROCOMPUTING										Zeroing neural network (ZNN); Time-varying linear matrix inequalities; Finite-time convergence; Vectorization technique; Sign-bi-power activation function	RECURRENT NEURAL-NETWORK; GLOBAL ASYMPTOTIC STABILITY; OBSTACLE-AVOIDANCE; ONLINE SOLUTION; DISCRETE; DYNAMICS; EQUATION; ROBUST	To obtain the superiority property of solving time-varying linear matrix inequalities (LMIs), three novel finite-time convergence zeroing neural network (FTCZNN) models are designed and analyzed in this paper. First, to make the Matlab toolbox calculation processing more conveniently, the matrix vectorization technique is used to transform matrix-valued FTCZNN models into vector-valued FTCZNN models. Then, considering the importance of nonlinear activation functions on the conventional zeroing neural network (ZNN), the sign-bi-power activation function (AF), the improved sign-bi-power AF and the tunable signbi-power AF are explored to establish the FTCZNN models. Theoretical analysis shows that the FTCZNN models not only can accelerate the convergence speed, but also can achieve finite-time convergence. Computer numerical results ulteriorly confirm the effectiveness and advantages of the FTCZNN models for finding the solution set of time-varying LMIs. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						78	87		10.1016/j.neucom.2020.01.070													
J								Siamese capsule networks with global and local features for text classification	NEUROCOMPUTING										Text classification; Capsule networks; Siamese networks; Neural networks; Global and local features		Text classification is a popular research topic in the field of natural language processing and provides wide applications. The existing text classification methods based on deep neural networks can completely extract the local features of text. The text classification models constructed based on these methods yield good experimental results. However, these methods generally ignore the global semantic information of different categories of text and global spatial distance between categories. To some extent, this adversely affects the accuracy of classification. In this study, to address this problem, Siamese capsule networks with global and local features were proposed. A Siamese network was used to glean information about the global semantic differences between categories, which could more accurately represent the semantic distance between different categories. A global memory mechanism was established to store global semantic features, which were then incorporated into the text classification model. Capsule vectors were used to obtain the spatial position relationships of local features, thereby improving the representation capabilities of the features. The experimental results showed that the proposed model achieved better results and performed significantly better on six different public datasets, as compared with ten baseline algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						88	98		10.1016/j.neucom.2020.01.064													
J								DT-LET: Deep transfer learning by exploring where to transfer	NEUROCOMPUTING										Transfer learning; Where to transfer; Deep learning		Previous transfer learning methods based on deep network assume the knowledge should be transferred between the same hidden layers of the source domain and the target domains. This assumption doesn't always hold true, especially when the data from the two domains are heterogeneous with different resolutions. In such case, the most suitable numbers of layers for the source domain data and the target domain data would differ. As a result, the high level knowledge from the source domain would be transferred to the wrong layer of target domain. Based on this observation, "where to transfer" proposed in this paper might be a novel research area. We propose a new mathematic model named DT-LET to solve this heterogeneous transfer learning problem. In order to select the best matching of layers to transfer knowledge, we define specific loss function to estimate the corresponding relationship between high-level features of data in the source domain and the target domain. To verify this proposed cross-layer model, experiments for two cross-domain recognition/classification tasks are conducted, and the achieved superior results demonstrate the necessity of layer correspondence searching. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						99	107		10.1016/j.neucom.2020.01.042													
J								Deep graph regularized non-negative matrix factorization for multi-view clustering	NEUROCOMPUTING										Multi-view clustering; Deep non-negative matrix factorization; Graph regularization	ALGORITHMS; NMF	Multi-view clustering is an unsupervised method which aims to enhance the clustering performance by combining the knowledge from multiple view data. Non-negative matrix factorization (NMF) is one of the most favourable multi-view clustering methods due to its strong representation ability of non-negative data. However, NMF only factorizes the data matrix into two non-negative factor matrices, which may limit its ability to learn higher level and more complex hierarchical information. To overcome this shortcoming, in this paper, we propose a multi-view clustering method based on deep graph regularized non-negative matrix factorization (MvDGNMF). MvDGNMF is able to extract more abstract representation by constructing a multilayer NMF model with graph Laplacian regularization and drive the last layer representation from each view to a common consensus representation. Meanwhile, an efficient algorithm using alternating multiplicative update rules is developed. Furthermore, in order to demonstrate the effectiveness of this proposed method, we employ several open datasets including image and text datasets to evaluate the clustering performance of MvDGNMF and the state-of-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						108	116		10.1016/j.neucom.2019.12.054													
J								Zero-shot learning for action recognition using synthesized features	NEUROCOMPUTING										Generalized zero shot learning; Inverse autoregressive flow; Bi-directional generative adversarial network; Transductive ZSL setting; Inductive ZSL setting		The major disadvantage of supervised methods for action recognition is the need for a large amount of annotated data, where the data is matched to its label accurately. To address this issue, Zero-Shot Learning (ZSL) is introduced. Zero short learning primarily uses data that is synthesized to compensate for lack of training examples. In this paper, two different approaches are proposed for the synthesis of artificial examples for novel classes; namely, inverse autoregressive flow (IAF) based generative model and bi-directional adversarial GAN(Bi-dir GAN). A consequence of the proposed approach is a transductive setting using a semi-supervised variational autoencoder, where the unlabelled data from unseen classes are used to train the model. This enables the generation of novel class examples from textual descriptions. The proposed models perform well in the following settings, namely, i) Standard setting(ZSL), where the test data comes only from unseen classes, and ii) Generalized setting(GZSL), where the test data comes from both seen and unseen classes. In the case of the generalized setting, examples with pseudo labels are generated for unseen classes. Experiments are performed on three baseline datasets, UCF101, HMDB51, and Olympic. In comparison with state-of-the-art approaches, both the proposed models, IAF based generative model and Bi-dir GAN model outperform in UCF101, and Olympic datasets in all the settings and achieve comparative results in HMDB51. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						117	130		10.1016/j.neucom.2020.01.078													
J								Global dissipativity of delayed discrete-time inertial neural networks	NEUROCOMPUTING										Global dissipativity; Discrete-time inertial neural networks; Generalized matrix measure; Robust dissipativity	EXPONENTIAL DISSIPATIVITY; ROBUST DISSIPATIVITY; STABILITY; SYNCHRONIZATION	This paper addresses global dissipativity for a class of delayed discrete-time inertial neural networks (DINN). With a newly developed discrete-time Halanay inequality, a novel criterion of the global dissipativity for the DINN is established. Moreover, the global robust dissipativity of DINN with bounded parameter uncertainties is also investigated. Meanwhile, some specific estimates of global attractive sets and positive invariant sets are derived. Finally, two simulation examples validate the efficacy of the proposed results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						131	138		10.1016/j.neucom.2020.01.073													
J								Neural networks-based fixed-time control for a robot with uncertainties and input deadzone	NEUROCOMPUTING										Fixed-time control; Neural network control; Input deadzone; Uncertainty; Robot	MULTIAGENT SYSTEMS; NONLINEAR-SYSTEMS; CONSENSUS; STABILIZATION; STABILITY; TRACKING; DESIGN	In this paper, neural networks-based fixed-time control is presented for a robot with uncertainties and actuator input deadzone. Model-based fixed-time control for the robot has been proposed based on an assumption parameters of the robot are completely known. To deal with the uncertainties of the robot and improve the stability of the robot, the adaptive neural network approximation method is proposed. Compared with existing works, the proposed control scheme can not only eliminate the influence of input deadzone, but also make all the state variables converge to a small region of zero in fixed-time, which can accelerate the convergence speed and improve the transient performance of the tracking control for the robot. In order to verify the feasibility of the proposed control algorithm, extensive simulations are carried on two joint robot manipulator. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						139	147		10.1016/j.neucom.2020.01.072													
J								Matrix factorization with heterogeneous multiclass preference context	NEUROCOMPUTING										Multiclass preference context (MPC); Dual MPC; Pipelined MPC; Matrix factorization; Collaborative filtering	RECOMMENDER SYSTEMS; COMPLETION; NORM	The spreading use of the Internet and big data technology has spawned the need for recommendation systems. However, to alleviate public anxiety about privacy, this paper advocates making recommendation with internal context, which refers to the implicit context hidden beneath the rating matrix only. Inspired by a recent work that embeds neighborhood information among users (as represented by multi-class preference context, MPC) into a factorization-based method, we extend it to both user-oriented and item-oriented, and put forward both user-oriented MPC and item-oriented MPC into a generic factorization framework called matrix factorization with heterogeneous MPC (MF-HMPC). In particular, we derive two specific recommendation methods from MF-HMPC with different structures, including MF with dual MPC (MF-DMPC) and MF with pipelined MPC (MF-PMPC), which are corresponding to concurrent structure and sequential structure, respectively. Extensive empirical studies on four public datasets show that our two forms of MF-HMPC outperform the referenced state-of-the-art MF methods as well as two representative deep learning methods. Moreover, we also discover some interesting facts about how these two kinds of internal contextual information complement each other. The main advantage of our methods is that they manage to strike a good balance between user-oriented neighborhood information and item-oriented neighborhood information. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						148	157		10.1016/j.neucom.2020.01.060													
J								Scalable spectral ensemble clustering via building representative co-association matrix	NEUROCOMPUTING										Scalable ensemble clustering; Co-association matrix; Spectral methods; Data graph partition	ALTERNATING DIRECTION METHOD; LOW-RANK REPRESENTATION; LARGE-SCALE; SUBSPACE; RECOGNITION; MULTIPLIERS	Ensemble clustering via building co-association matrix and combining multiple basic partitions from the same dataset into the consensus one has been widely used in spectral clustering and subspace clustering. However, with the ever-increasing cost of calculating the co-association matrix, the conventional ensemble clustering algorithm is no longer fit for dealing with the large-scale datasets due to its less scalability and time-consuming. In this paper, we propose a scalable spectral ensemble clustering method via building a representative co-association matrix to improve the ensemble clustering problem. Our method mainly includes constructing a sparse matrix to select the representative points and building the co-association matrix, and a robust and denoising representation for the co-association matrix can be learned through a low-rank constraint in a unified optimization framework. The experiments verify the high efficiency and scalability but less time cost of our method compared with state-of-art clustering methods in the six real-world datasets, especially in the large-scale datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						158	167		10.1016/j.neucom.2020.01.055													
J								Composite adaptive NN learning and control for discrete-time nonlinear uncertain systems in normal form	NEUROCOMPUTING										Adaptive dynamics learning control; Deterministic learning theory; Neural networks; Discrete time nonlinear uncertain systems	PERFORMANCE	This paper addresses the problem of composite adaptive learning and tracking control for discrete-time nonlinear uncertain systems in general normal form. To deal with the system's unstructured nonlinear uncertain dynamics, novel adaptive neural network (NN) learning control strategies are proposed by extending methodologies from the continuous-time deterministic learning theory. Both state-feedback and output-feedback cases are considered. The proposed control schemes are compelling in the sense that (i) they are capable of rendering not only stable tracking control, but also locally-accurate learning/identification of unknown system dynamics during stable closed-loop control; (ii) the learned knowledge can be effectively represented and stored as constant NN models, whose weights are guaranteed to partially converge to ideal/optimal values. Based on this, experience-based controllers are also developed to pursue improved tracking control performance without online adaptation. In particular for the output-feedback control case, a new observer-less adaptive NN learning control scheme is proposed without resorting to high-gain observers, followed by an observer-less output-feedback experience-based controller. Numerical simulations have been conducted to demonstrate the effectiveness of the proposed approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						168	184		10.1016/j.neucom.2020.01.052													
J								Robust optimal control for a class of nonlinear systems with unknown disturbances based on disturbance observer and policy iteration	NEUROCOMPUTING										Adaptive critic designs; Adaptive dynamic programming; Approximate dynamic programming; Optimal control; On-policy; Disturbance	ZERO-SUM GAMES; CONTROL SCHEME; APPROXIMATION; TRACKING; STABILIZATION; ATTENUATION; ALGORITHM	A robust optimal control method for a class of nonlinear systems with unknown disturbances is addressed in this paper. In this framework, adaptive dynamic programming (ADP) is presented to obtain the optimal control. On-policy learning allows the performance index function and the optimal control to be obtained iteratively. It is shown that the iterative performance index function is non-increasing. A nonlinear disturbance observer is designed to estimate external disturbances. The compensation control is used to compensate for the influence of the disturbances. It is proven that the disturbance observer error is exponentially stable, under some conditions. The properties of the nonlinear system with unknown disturbance steered by the robust optimal control input are also proven. Simulation results demonstrate the performance of the proposed robust optimal control scheme for the nonlinear system with unknown disturbance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						185	195		10.1016/j.neucom.2020.01.082													
J								Robust real-time hand detection and localization for space human-robot interaction based on deep learning	NEUROCOMPUTING										Astronaut assistant robot; Deep learning; Hand detection and localization; SSD		Hand gestures are quite suitable for space human-robot interaction (SHRI) because of their natural and convenient features. While the detection and localization of hands are the premise and foundation for SHRI based on hand gestures. But hand gestures are very complicated and hand sizes are very small in some images. These problems make the robust real-time hand detection and localization very difficult. In this paper, a feature-map-fused single shot multibox detector (FF-SSD) which is a deep learning network is designed to deal with the problems of hand detection and localization in SHRI. First, the background of the method is introduced in this paper, including an astronaut assistant robot platform, the difficulties of hand detection and localization, and introduction of the state-of-the-art deep learning networks for object detection and localization. Then, the FF-SSD is proposed for detecting and localizing hands especially pony-size hands. This network takes into consideration both accuracy and speed with balanced performance. And in the experiment part, the FF-SSD is trained and tested on hand databases which include a homemade database and two public databases. At last, the superiority of the proposed method is demonstrated compared with the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						198	206		10.1016/j.neucom.2019.02.066													
J								Counting crowds using a scale-distribution-aware network and adaptive human-shaped kernel	NEUROCOMPUTING										Crowd counting; Intelligent bus system; Multi-column convolutional neural network; Weighted euclidean loss; Human-machine system	PEOPLE	Intelligent bus system plays a key role in the modern smart city. The number of passengers in the buses or at the stations is necessary for making an optimal scheduling policy of public buses. We develop a crowd counting algorithm to provide the counting information for a bus dispatch system in a human-machine system. In consideration of the challenges (e.g., pedestrian occlusions, non-uniform crowd distributions, and scale variations) existed in hand-crafted features based crowd counting, a scale-distributionaware multi-column convolutional neural network (SDA-MCNN) is presented to count crowds by summing up the output (denoted as the density map) of the SDA-MCNN. The SDA-MCNN is robust to scale variations by processing a crowd image with multiple convolutional neural network (CNN) columns and minimizing the per-scale loss. A weighted Euclidean loss is proposed to handle non-uniform crowd distributions. The loss can increase activations in dense regions and restrain activations in backgrounds. A new approach to estimate perspective maps of dense crowds is put forward to offer necessary information for generating density maps with human-shaped kernels. Evaluations on benchmarks are performed with other state-of-the-art counting approaches using deep neural networks. Comparative results verify the accuracy of our counting approach in challenging crowds. Evaluations on the real world BUS data reveal the accuracy of the proposed approach in counting passengers in spite of the complex environment. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						207	216		10.1016/j.neucom.2019.02.071													
J								Improved itracker combined with bidirectional long short-term memory for 3D gaze estimation using appearance cues	NEUROCOMPUTING										Gaze estimation; CNN; RNN; LSTM	TRACKING	Gaze is an important non-verbal cue for speculating human's attention, which has been widely employed in many human-computer interaction-based applications. In this paper, we propose an improved Itracker to predict the subject's gaze for a single image frame, as well as employ a many-to-one bidirectional Long Short-Term Memory (bi-LSTM) to fit the temporal information between frames to estimate gaze for video sequence. For single image frame gaze estimation, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset and has robust estimation accuracy for different image resolutions under the premise of greatly reducing network complexity. For video sequence gaze estimation, by employing the bi-LSTM to fit the temporal information between frames, experimental results on EyeDiap dataset further demonstrate 3% accuracy improvement. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 21	2020	390						217	225		10.1016/j.neucom.2019.04.099													
J								Classifying ASD children with LSTM based on raw videos	NEUROCOMPUTING										Autism spectrum disorder; Tracking-learning-detection (TLD); Accumulative histogram; Deep learning; Long Short-Term Memory (LSTM)	AUTISM SPECTRUM DISORDER; EYE-TRACKING; WILLIAMS-SYNDROME; VISUAL TRACKING; BRAIN	Autism spectrum disorder (ASD) is a serious neurodevelopmental disorder that impairs a child's ability to communicate and interact with others. Usually, recognizing a child with ASD needs the diagnosis by professional doctors. However, it is not only expensive and time-consuming, but also the results are influenced by subjective factors, such as the experience of a doctor. Recently, some methods which identify ASD based on biomarkers have been developed, but there are rarely works specific to raw video data. This paper is the first attempt to help diagnose the children with ASD in raw video data using a deep learning technique. Firstly, in order to investigate different gaze patterns between ASD children and typically developing (TD) children, we track the eye movement in each video by the tracking-learning-detection method. Secondly, we divide these tracking trajectories into two components: (i) the length; and (ii) the angle. Afterwards, we calculate an accumulative histogram for each component. Finally, we adopt three-layer Long Short-Term Memory (LSTM) network for classification. Experimental results on our extended dataset (Ext-Dataset) containing 272 videos captured from 136 ASD children and 136 TD children show the LSTM network outperforms the traditional machine learning methods, e.g., Support Vector Machine, with the improvement of accuracy by 6.2% (from 86.4% to 92.6%). Especially, for ASD, we obtain the sensitivity (the true positive rate, TPR) of 91.9% and the specificity (the true negative rate, TNR) of 93.4%, which demonstrates the effectiveness of our method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						226	238		10.1016/j.neucom.2019.05.106													
J								Text-based indoor place recognition with deep neural network	NEUROCOMPUTING										Indoor place recognition; CNN; LSTM; Natural language processing	DESIGN	Indoor place recognition is a challenging problem because of the hard representation to complicated intra-class variations and inter-class similarities.This paper presents a new indoor place recognition scheme using deep neural network. Traditional representations of indoor place almost utilize image feature to retain the spatial structure without considering the object's semantic characteristics. However, we argue that the attributes, state and relationships of objects are much more helpful in indoor place recognition. In particular, we improve the recognition framework by utilizing Place Descriptors (PDs) in text from to connect different types of place information with their categories. Meanwhile, we analyse the ability of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) for classification in natural language, for which we use them to process the indoor place descriptions. In addition, we improve the robustness of the designed deep neural network by combining a number of effective strategies, i.e. L2-regularization, data normalization, and proper calibration of key parameters. Compared with existing state of the art, the proposed approach achieves well performance of 70.73%, 70.08% and 70.16% of accuracy, precision and recall on Visual Genome database respectively. Meanwhile, the accuracy becomes 98.6% after adding voting mechanics. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						239	247		10.1016/j.neucom.2019.02.065													
J								Visual Recognition of traffic police gestures with convolutional pose machine and handcrafted features	NEUROCOMPUTING										Gesture recognition; Human-machine interface; Traffic police gestures; Convolutional neural network; Deep learning	NETWORKS	Autonomous vehicles have become a hot spot of the automotive industry, many cities have claimed that autonomous vehicles should be capable of recognizing gestures used by traffic police. Traditional traffic police gesture recognition methods rely on depth-sensor or wearable-devices, which limits their availability in the domain of the intelligent vehicle. Vision-based methods have fewer requirements for distance, but the modeling process is challenging due to the complexity of the visual scenes. Inspired by the recent success in vision-based pose estimation networks such as Convolutional Pose Machine (CPM), in this paper, we propose a novel vision-based human-machine interface to recognize eight kinds of Chinese traffic police gestures and apply it in the real-time recognition tasks. This method integrates a modified CPM network and two kinds of handcrafted features: Relative Bone Length and Angle with Gravity as spatial domain features, and adopt a Long short-term memory (LSTM) network to extract temporal domain features. To train and validate our method, we create a gestures dataset with two hours of traffic police gesture videos, which has 3354 gesture instances. The experiment results show that the proposed method is capable of recognizing traffic police gestures, and is fast enough for online gesture prediction. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						248	259		10.1016/j.neucom.2019.07.103													
J								A robot learning framework based on adaptive admittance control and generalizable motion modeling with neural network controller	NEUROCOMPUTING										Robot learning; Adaptive admittance control; Motion generalization; Neural network	TRACKING CONTROL; IMITATION; TASK	Robot learning from demonstration (LfD) enables robots to be fast programmed. This paper presents a novel LfD framework involving a teaching phase, a learning phase and a reproduction phase, and proposes methods in each of these phases to guarantee the overall system performance. An adaptive admittance controller is developed to take into account the unknown human dynamics so that the human tutor can smoothly move the robot around in the teaching phase. The task model in this controller is formulated by the Gaussian mixture regression to extract the human-related motion characteristics. In the learning and reproduction phases, the dynamic movement primitive is employed to model a robotic motion that is generalizable. A neural network-based controller is designed for the robot to track the trajectories generated from the motion model, and a radial basis function neural network is used to compensate for the effect caused by the dynamic environments. Experiments have been performed using a Baxter robot and the results have confirmed the validity of the proposed robot learning framework. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						260	267		10.1016/j.neucom.2019.04.100													
J								Estimation of human impedance and motion intention for constrained human-robot interaction	NEUROCOMPUTING										Human motion intention estimation; Impedance learning; Adaptive neural network control; Full-state constraints; Barrier Lyapunov functions	NEURAL-NETWORK CONTROL; NONLINEAR-SYSTEMS; TRACKING CONTROL; ARM IMPEDANCE; DESIGN	In this paper, a complete framework for safe and efficient physical human-robot interaction (pHRI) is developed for robot by considering both issues of adaptation to the human partner and ensuring the motion constraints during the interaction. We consider the robot's learning of not only human motion intention, but also the human impedance. We employ radial basis function neural networks (RBFNNs) to estimate human motion intention in real time, and least square method is utilized in robot learning of human impedance. When robot has learned the impedance information about human, it can adjust its desired impedance parameters by a simple tuning law for operative compliance. An adaptive impedance control integrated with RBFNNs and full-state constraints is also proposed in our work. We employ RBFNNs to compensate for uncertainties in the dynamics model of robot and barrier Lyapunov functions are chosen to ensure that full-state constraints are not violated in pHRI. Results in simulations and experiments show the better performance of our proposed framework compared with traditional methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						268	279		10.1016/j.neucom.2019.07.104													
J								A learning-based multiscale modelling approach to real-time serial manipulator kinematics simulation	NEUROCOMPUTING										Human Machine Interaction; Serial Manipulator Kinematics; Multiscale Modelling; Kernel Ridge Regression; LSTM Neural Network	NEURAL-NETWORK; PARALLEL MANIPULATOR; ALGORITHM; SURROGATE; OPTIMIZATION; DESIGN	Kinematics simulation is central to the control of serial manipulators in human machine interaction, which highly depends upon computational efficiency and accuracy for the sake of enabling real-time analysis and interaction. In this paper, a novel learning-based multiscale modelling approach is proposed to effectively address the efficiency and accuracy trade-offs through a combination of models with different levels of fidelity. Specifically, low-fidelity models are formulated using Kernel Ridge Regression (KRR) to achieve much lower computational cost as its kinematic approximation. Additionally, high-fidelity models are created based on the Long Short Term Memory (LSTM) neural network, which can calibrate fidelity by training the significant samples and eliminate position singularity of a serial manupulator. The proposed approach is evaluated both by using numerical tests and by applying it to a collaborative industrial robot arm. The experimental results obtained demonstrate that it can achieve better performance in terms of both accuracy and efficiency compared with the other popular methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						280	293		10.1016/j.neucom.2019.04.101													
J								DEVDAN: Deep evolving denoising autoencoder	NEUROCOMPUTING										Denoising autoencoder; Data streams; Incremental learning	NETWORKS	The Denoising Autoencoder (DAE) enhances the flexibility of data stream method in exploiting unlabeled samples. Nonetheless, the feasibility of DAE for data stream analytic deserves in-depth study because it characterizes a fixed network capacity which cannot adapt to rapidly changing environments. Deep evolving denoising autoencoder (DEVDAN), is proposed in this paper. It features an open structure in the generative phase and the discriminative phase where the hidden units can be automatically added and discarded on the fly. The generative phase refines the predictive performance of discriminative model exploiting unlabeled data. Furthermore, DEVDAN is free of the problem-specific threshold and works fully in the single-pass learning fashion. We show that DEVDAN can find competitive network architecture compared with state-of-the-art methods on the classification task using ten prominent datasets simulated under the prequential test-then-train protocol. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						297	314		10.1016/j.neucom.2019.07.106													
J								Learning elastic memory online for fast time series forecasting	NEUROCOMPUTING										Time series forecasting; Temporality determination; Past dependency in time series forecasting; Temporal neural network; Online learning; Drift detection; Elastic memory; Adaptive temporal network	NEURAL-NETWORK; INFERENCE SYSTEM; IDENTIFICATION; CLASSIFICATION; PREDICTION; ALGORITHM	It is well known that any kind of time series algorithm requires past information to model the inherent temporal relationship between past and future. This temporal dependency (i.e. number of past samples required for a good prediction) is generally addressed by feeding a number of past instances to the model in an empirical manner. Conventional approaches mostly rely on offline model, making them impractical to be adopted in the online or streaming context. Hence, a novel method of online temporality analysis is proposed in this paper. The estimated temporality is then employed to form an Adaptive Temporal Neural Network (ATNN) with an elastic memory capable of automatically selecting number of past samples to be used. Temporality change or drift can be a common occurrence in data streams. Hence a drift detection mechanism is also proposed. Once such drift is detected, a drift handling mechanism kicks in which utilizes the rate of drift, making our solution truly autonomous. The entire mechanism is termed as LEMON: Learning Elastic Memory Online. LEMON although not a time series model in itself, can work with any predictive models to improve their performance. Synthetic datasets are used here as proof of correct temporality estimation and drift detection whereas real world datasets are employed to demonstrate how LEMON improves the predictive performance and speed of an existing model with the knowledge of temporality and drift. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						315	326		10.1016/j.neucom.2019.07.105													
J								Hybrid neural networks for big data classification	NEUROCOMPUTING										Morphological neurons; Dendrite processing; Neural networks; Multilayer perceptron; Big data	MACHINE	Two new hybrid neural architectures combining morphological neurons and perceptrons are introduced in this paper. The first architecture, called Morphological - Linear Neural Network (MLNN) consists of a hidden layer of morphological neurons and an output layer of classical perceptrons has the capability of extracting features. The second architecture, called Linear-Morphological Neural Network (LMNN) is composed of one or several perceptron layers as a feature extractor, it is then followed by an output layer of morphological neurons for non-linear classification. Both architectures are trained by stochastic gradient descent. One of the main contributions of this paper is to show that the morphological layer offers a greater capacity to extract features than the perceptron layer. This claim is supported both theoretically and experimentally. We prove that the morphological layer possesses a greater capacity per computation unit to segment the 2D input space than the perceptron layer. In other words, adding more hyper-boxes produces more response regions than adding hyperplanes. From an empirical point of view, we test the two new models on 25 standard datasets at low dimensionality and one big data dataset. The result is that MLNN requires a lesser number of learning parameters than the other tested architectures while achieving better accuracies. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						327	340		10.1016/j.neucom.2019.08.095													
J								Online active learning for human activity recognition from sensory data streams	NEUROCOMPUTING										Activity recognition; Data streams; Active learning; Online learning	UNKNOWN NUMBER; ALGORITHM	Human activity recognition (HAR) is highly relevant to many real-world domains like safety, security, and in particular healthcare. The current machine learning technology of HAR is highly human-dependent which makes it costly and unreliable in non-stationary environment. Existing HAR algorithms assume that training data is collected and annotated by human a prior to the training phase. Furthermore, the data is assumed to exhibit the true characteristics of the underlying distribution. In this paper, we propose a new autonomous approach that consists of novel algorithms. In particular, we adopt active learning (AL) strategy to selectively query the user/resident about the label of particular activities in order to improve the model accuracy. This strategy helps overcome the challenge of labelling sequential data with time dependency which is highly time-consuming and difficult. Because of the changes that may affect the way activities are performed, we regard sensor data as a stream and human activity learning as an online continuous process. In such process the leaner can adapt to changes, incorporate novel activities and discard obsolete ones. To this extent, we propose a novel semi-supervised classifier (OSC) that works together with a novel Bayesian stream-based active learning (BSAL). Because of the changes in the sensor layouts across different houses' settings, we use Conditional Restricted Boltzmann Machine (CRBM) to handle the features engineering issue by learning the features regardless of the environment settings. CRBM is then applied to extract low-level features from unlabelled raw high-dimensional activity input. The resulting approach will then tackle the challenges of activity recognition using a three-module architecture composed of a feature extractor (CRBM), an online semi-supervised classifier (OSC) equipped with BSAL. CRBM-BSAL-OSC allows completely autonomous learning that adjusts to the environment setting, explores the changes and adapt to them. The paper provides the theoretical details of the proposed approach as well as an extensive empirical study to evaluate the performance of the approach. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						341	358		10.1016/j.neucom.2019.08.092													
J								Deep online hierarchical dynamic unsupervised learning for pattern mining from utility usage data	NEUROCOMPUTING										Non-intrusive load monitoring; Bayesian modelling; Online learning; Human activity recognition	ALGORITHM	While most non-intrusive load monitoring (NILM) work has focused on supervised algorithms, unsupervised approaches can be more interesting and practical. Specifically, they do not require labelled training data to be acquired from the individual appliances and can be deployed to operate on the measured aggregate data directly. We propose a fully unsupervised novel NILM framework based on Dynamic Bayesian hierarchical mixture model and Deep Belief network (DBN). The deep network learns, in unsupervised fashion, low-level generic appliance-specific features from the raw signals of the house utilities usage, then the hierarchical Bayesian model learns high-level features representing the consumption patterns of the residents captured by the correlations among the low-level features. The temporal ordering of the high-level features is captured by the Dynamic Bayesian Model. Using this architecture, we overcome the computational complexity that would occur if temporal modelling was directly applied to the raw data or even to the constructed features. The computational efficiency is crucial as our application involves massive data from different utilities usage. Moreover, we develop a novel online inference algorithm to cope with this big data. Finally, we propose different evaluation methods to analyse the results which show that our algorithm finds useful patterns. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						359	373		10.1016/j.neucom.2019.08.093													
J								Competitive regularised regression	NEUROCOMPUTING										Regression; Regularisation; Online learning; Competitive analysis	LMS ALGORITHM; LOSS BOUNDS; ONLINE; PREDICTION; EFFICIENCY; SHRINKAGE; SELECTION; GRADIENT; PATHS	Regularised regression uses sparsity and variance to reduce the complexity and over-fitting of a regression model. The present paper introduces two novel regularised linear regression algorithms: Competitive Iterative Ridge Regression (CIRR) and Online Shrinkage via Limit of Gibbs Sampler (OSLOG) for fast and reliable prediction on "Big Data" without making distributional assumption on the data. We use the technique of competitive analysis to design them and show their strong theoretical guarantee. Furthermore, we compare their performance against some neoteric regularised regression methods such as Online Ridge Regression (ORR) and the Aggregating Algorithm for Regression (AAR). The comparison of the algorithms is done theoretically, focusing on the guarantee on the performance on cumulative loss, and empirically to show the advantages of CIRR and OSLOG. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 21	2020	390						374	383		10.1016/j.neucom.2019.08.094													
J								Financial quantitative investment using convolutional neural network and deep learning technology	NEUROCOMPUTING										Financial investment; Quantitative investment; Convolutional neural network; Deep belief network		In order to make financial investment more stable and more profitable, convolutional neural network (CNN) and deep learning technology are used to quantify financial investment, so as to obtain more robust investment and returns. With the continuous development of in-depth learning technology, people are applying it more and more widely. Deep learning is put forward on the basis of neural network. It contains more hidden layers, shows more powerful learning ability, and can abstract data at a higher level, so as to obtain more accurate data. CNN is a multi-layer network structure which simulates the operation mechanism of biological vision system. Its special structure can obtain more useful feature descriptions from original data and is very effective in extracting data. Therefore, in this study, the two technologies are combined to quantify financial investment. The results show that the convolution neural network and deep learning algorithm can obtain relatively accurate investment strategies, thus ensuring investment returns and reducing investment risks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 21	2020	390						384	390		10.1016/j.neucom.2019.09.092													
J								One-class support vector classifiers: A survey	KNOWLEDGE-BASED SYSTEMS										One-class classification (OCC); One-class support vector classifiers (OCSVCs); Parameter estimation; Feature selection; Sample reduction; Distributed environment	ONE-CLASS SVM; LINE SIGNATURE VERIFICATION; NOVELTY DETECTION; INTRUSION DETECTION; COMPONENT ANALYSIS; FEATURE-SELECTION; MACHINE; ENSEMBLE; ROBUST; CLASSIFICATION	Over the past two decades, one-class classification (OCC) becomes very popular due to its diversified applicability in data mining and pattern recognition problems. Concerning to OCC, one-class support vector classifiers (OCSVCs) have been extensively studied and improved for the technology-driven applications; still, there is no comprehensive literature available to guide researchers for future exploration. This survey paper presents an up to date, structured and well-organized review on one-class support vector classifiers. This survey comprises available algorithms, parameter estimation techniques, feature selection strategies, sample reduction methodologies, workability in distributed environment and application domains related to OCSVCs. In this way, this paper offers a detailed overview to researchers looking for the state-of-the-art in this area. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105754	10.1016/j.knosys.2020.105754													
J								Find you if you drive: Inferring home locations for vehicles with surveillance camera data	KNOWLEDGE-BASED SYSTEMS										Urban computing; Surveillance camera data; Home location inference; Kernel density estimation; Vehicle trajectory	USERS; INFERENCE	Inferring home locations for users from spatiotemporal data has become increasingly important for real-world applications ranging from security, recommendation, advertisement targeting, to transportation scheduling. Existing home location inference studies are based either on geo-tagged social media data or continuous GPS data. Yet this inference problem in highly sparse vehicle trajectories in urban surveillance systems remains largely unexplored. In this paper, we propose an accurate home location inference framework for vehicles in urban traffic surveillance systems by considering both spatial and temporal characteristics. To the best of our knowledge, we are the first to predict exact home community for vehicles at such a fine granularity using the sparse and noisy surveillance camera data. First, we collect and preprocess multiple contextual datasets to obtain a context-rich road network with residential communities and surveillance cameras. Second, we detect the potential home location areas for each vehicle by clustering Origin-Destination (O-D) pairs extracted in vehicle's camera-based trajectories. Then we further propose an in/out time pattern to distinguish the home area candidate from the O-D clusters by leveraging time-aware constraints. Furthermore, to find the exact home community, we propose a Kernel Density Estimation (KDE) based inference method with a local camera selection strategy to effectively identify the home community from the residential communities near/in the home area candidate. Our comprehensive experiments on a large-scale real-world dataset demonstrate the effectiveness of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105766	10.1016/j.knosys.2020.105766													
J								CAVIAR: Context-driven Active and Incremental Activity Recognition	KNOWLEDGE-BASED SYSTEMS										Activity recognition; Mobile computing; Semi-supervised learning; Context-awareness; Hybrid reasoning	SYSTEM; PHONE; MODEL	Activity recognition on mobile device sensor data has been an active research area in mobile and pervasive computing for several years. While the majority of the proposed techniques are based on supervised learning, semi-supervised approaches are being considered to reduce the size of the training set required to initialize the model. These approaches usually apply self-training or active learning to incrementally refine the model, but their effectiveness seems to be limited to a restricted set of physical activities. We claim that the context which surrounds the user (e.g., time, location, proximity to transportation routes) combined with common knowledge about the relationship between context and human activities could be effective in significantly increasing the set of recognized activities including those that are difficult to discriminate only considering inertial sensors, and the highly context-dependent ones. In this paper, we propose CAVIAR, a novel hybrid semi-supervised and knowledge-based system for real-time activity recognition. Our method applies semantic reasoning on context-data to refine the predictions of an incremental classifier. The recognition model is continuously updated using active learning. Results on a real dataset obtained from 26 subjects show the effectiveness of our approach in increasing the recognition rate, extending the number of recognizable activities and, most importantly, reducing the number of queries triggered by active learning. In order to evaluate the impact of context reasoning, we also compare CAVIAR with a purely statistical version, considering features computed on context-data as part of the machine learning process. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105816	10.1016/j.knosys.2020.105816													
J								An online Bayesian approach to change-point detection for categorical data	KNOWLEDGE-BASED SYSTEMS										Bayes factor; Change-point detection; Dirichlet-multinomial mixtures; Online strategy	TIME-SERIES DATA; MULTINOMIAL DATA	Change-point detection for categorical data has wide applications in many fields. Existing methods either are distribution-free, not utilizing categorical information sufficiently, or have limited performance when there exists "rare events" (events that occur with low frequency). In this paper, we propose a Bayesian change-point detection model for categorical data based on Dirichlet-multinomial mixtures. Because of the introduction of prior information, our method performs well for the existence of "rare events". An online parameter estimation procedure and an online detection strategy are then designed to adapt to data streams. Monte Carlo simulations discuss the power of the proposed method and show advantages compared with existing algorithms. Applications in biomedical research, document analysis, health news case study and location monitoring indicate practical values of our method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105792	10.1016/j.knosys.2020.105792													
J								On modeling and predicting popularity dynamics via integrating generative model and rich features	KNOWLEDGE-BASED SYSTEMS										Social media; Popularity prediction; Popularity dynamics; Rich features; Reinforced Poisson process		Understanding the mechanisms governing how an online message acquires more popularity than another, modeling how it gains popularity dynamically, and determining the method for predicting its dynamics popularity are of tremendous interest to related decision support systems. However, one major limitation of existing generative dynamics models is that the learning parameters are difficult to interpret and it is unclear whether it can be generalized for other messages, as they are trained for different messages independently and the feature data-based connections between messages are ignored. To alleviate the defects, we first perform experiments on real-world data from Sina Weibo to identify the general correlation between the dynamics model and rich features of online messages. Consequently, we present a novel feature-regularized dynamics model based on reinforced Poisson process (FRRPP), which regulates the parameter learning of popularity dynamics by integrating a feature regression term to capture the revealed correlation across online posts. Specifically, in addition to the objective of the maximum likelihood function, we assume that the competitiveness parameter of the different posts can be predicted by rich features, to enhance the explicability and generality of the point-process model and learn the dynamics process of different posts together. The proposed model is then evaluated on two real Sina Weibo datasets, and conclusive experimental results indicate that the proposed model achieves a remarkable improvement over baseline methods in terms of MAPE and Accuracy with various settings, which further verifies our findings about how to improve the generality of popularity dynamics modeling and prediction. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105786	10.1016/j.knosys.2020.105786													
J								A kernel based learning method for non-stationary two-player repeated games	KNOWLEDGE-BASED SYSTEMS										Game theory; Repeated games; Sequence prediction; String kernel		Repeated games is a branch of game theory, where a game can be played several times by the players involved. In this setting, players may not always play the optimal strategy or they may be willing to engage in collaboration or other types of behavior which might lead to a higher long-term profit. Since the same game is repeated for several rounds, and considering a scenario with complete information, it is possible for a player to analyze its opponent's behavior in order to find patterns. These patterns can then be used to predict the opponent's actions. Such a setting, where players have mutual information about past moves and do not always play in equilibrium, leads naturally to non-stationary environments, where the players can frequently modify their strategies in order to get ahead in the game. In this work, we propose a novel algorithm based on a string kernel density estimation, which is capable of predicting the opponent's actions in repeated games and can be used to optimize the player's profit over time. The prediction is not limited to the next round action. It can also be used to predict a finite sequence of future rounds, which can be combined with a lookahead search scheme with limited depth. In the experiments section, it is shown that the proposed algorithm is able to learn and adapt rapidly, providing good results even if the opponent also adopts an adaptive strategy. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105820	10.1016/j.knosys.2020.105820													
J								Credibility score based multi-criteria recommender system	KNOWLEDGE-BASED SYSTEMS										Credibility; Collaborative filtering; Genetic algorithm; Multi-criteria ratings; Recommender system	TRUST	Recommender system has been emerged as a personalization tool to solve the issue of information overload in an e-commerce environment. Traditional collaborative filtering (CF) based recommender systems (RSs) suggest items to users based on their overall ratings which are used to find out similar users. Multi-criteria ratings are used to capture user preferences efficiently in multi-criteria recommender systems (MCRS), and incorporation of various criteria ratings can lead to higher performance in MCRS. Usually, user relies on the credibility of an item provided through his/her social circle or similar users, which is called a personal view on items from their close ones. However, it is not generally sufficient to depend exclusively on the personal view of the user. Therefore, public view that includes whole community can play a key role in the credibility of an item. In this paper, we propose a MCRS based on the credibility score of an item, which is an aggregated value of credibility scores on various criteria of an item. These credibility scores are computed based on personal and public views. However, different users have different priorities to various criteria of an item. Therefore, we use genetic algorithm (GA) to learn appropriate weights in the aggregation task of credibility score. The experiment results on Yahoo! Movies and modified MovieLens dataset demonstrate the effectiveness of proposed credibility score based MCRS in terms of coverage, recall, precision, and f-measure. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105756	10.1016/j.knosys.2020.105756													
J								Detecting community in attributed networks by dynamically exploring node attributes and topological structure	KNOWLEDGE-BASED SYSTEMS										Community detection; Attributed network; Nonnegative matrix factorization; Graph optimization	ALGORITHMS	Graph clustering assigns nodes into tightly connected groups known as communities, and community detection algorithms traditionally focus on non-attributed networks that only provide a partial representation of the underlying systems. Thus, community detection in attributed networks becomes a hot topic since it provides an insight into the structure-function relation of the underlying systems. However, identifying community in attributed networks is highly non-trivial since it simultaneously takes into consideration both topological structure and node attributes. Current algorithms detect communities by either incorporating attributes into structure of networks or directly fusing features of nodes and structure of networks, which cannot effectively handle the heterogeneity of structure and attributes, hampering fully exploitation of them. To overcome these problems, we propose a novel algorithm by joint nonnegative matrix factorization and graph optimization (called NMFjGO) for community detection in attributed networks. To overcome the heterogeneity of structure and attributes, an attribute similarity matrix for nodes is constructed in terms of attribute profiles. To obtain the latent feature of attributed networks for community, nonnegative matrix factorization (NMF) is adopted to jointly factorize the established attribute similarity matrix and adjacent matrix of networks. The latent feature of node attributes are dynamically fused with the topological structure of attributed networks. The advantage of NMFjGO is that it jumps out of local minima by dynamically exploring the attribute and links of networks. The experimental results demonstrate that the proposed algorithm significantly outperforms state-of-the-art algorithms in terms of accuracy on a number of attributed networks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105760	10.1016/j.knosys.2020.105760													
J								Toward semantic data imputation for a dengue dataset	KNOWLEDGE-BASED SYSTEMS										Missing data; Ontology; Data imputation; Data cleansing; Semantic	MISSING DATA IMPUTATION; GENETIC ALGORITHM; TIME-COMPLEXITY; CLASSIFICATION; VALUES; RULES	Missing data are a major problem that affects data analysis techniques for forecasting. Traditional methods suffer from poor performance in predicting missing values using simple techniques, e.g., mean and mode. In this paper, we present and discuss a novel method of imputing missing values semantically with the use of an ontology model. We make three new contributions to the field: first, an improvement in the efficiency of predicting missing data utilizing Particle Swarm Optimization (PSO), which is applied to the numerical data cleansing problem, with the performance of PSO being enhanced using K-means to help determine the fitness value. Second, the incorporation of an ontology with PSO for the purpose of narrowing the search space, to make PSO provide greater accuracy in predicting numerical missing values while quickly converging on the answer. Third, the facilitation of a framework to substitute nominal data that are lost from the dataset using the relationships of concepts and a reasoning mechanism concerning the knowledge-based model. The experimental results indicated that the proposed method could estimate missing data more efficiently and with less chance of error than conventional methods, as measured by the root mean square error. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105803	10.1016/j.knosys.2020.105803													
J								Multi-focus image fusion based on dynamic threshold neural P systems and surfacelet transform	KNOWLEDGE-BASED SYSTEMS										Multi-focus image fusion; Dynamic threshold neural P systems; Local topology; Surfacelet domain	SHEARLET TRANSFORM; FRAMEWORK; CURVELET; SCHEME	Dynamic threshold neural P systems (DTNP systems) are recently proposed distributed and parallel computing models, inspired from the intersecting cortical model. DTNP systems differ from spiking neural P systems (SNP systems) due to the introduction of dynamic threshold mechanism of neurons. DTNP systems have been theoretically proven to be Turing universal computing devices. This paper discusses how to apply DTNP systems to deal with the fusion of multi-focus images, and proposes a novel image fusion method based on DTNP systems in surfacelet domain. Based on four DTNP systems with local topology, a multi-focus image fusion framework in surfacelet domain is developed, where DTNP systems are applied to control the fusion of low- and high-frequency coefficients in surfacelet domain. The proposed fusion method is evaluated on an open dataset of 20 multi-focus images in terms of five fusion quality metrics, and compared with 10 state-of-the-art fusion methods. Quantitative and qualitative experimental results demonstrate the advantages of the proposed fusion method in terms of visual quality, fusion performance and computational efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105794	10.1016/j.knosys.2020.105794													
J								ACO Resampling: Enhancing the performance of oversampling methods for class imbalance classification	KNOWLEDGE-BASED SYSTEMS										Machine learning; Imbalanced learning; Oversampling; Ant colony optimization resampling	PREDICTION; SELECTION; CANCER; TUMOR; SMOTE	Many sampling-based preprocessing methods have been proposed to solve the problem of unbalanced dataset classification. The fundamental principle of these methods is rebalancing an unbalanced dataset by a concrete strategy. Herein, we introduce a novel hybrid proposal named ant colony optimization resampling (ACOR) to overcome class imbalance classification. ACOR primarily includes two steps: first, it rebalances an imbalanced dataset by a specific oversampling algorithm; next, it finds an (sub)optimal subset from the balanced dataset by ant colony optimization. Unlike other oversampling techniques, ACOR does not focus on the mechanics of generating new samples. The main advantage of ACOR is that existing oversampling algorithms can be fully utilized and an ideal training set can be obtained by ant colony optimization. Therefore, ACOR can enhance the performance of existing oversampling algorithms. Experimental results on 18 real imbalanced datasets prove that ACOR yields significantly better results compared with four popular oversampling methods in terms of various assessment metrics, such as AUC, G-mean, and BACC. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105818	10.1016/j.knosys.2020.105818													
J								Measures of uncertainty based on Gaussian kernel for a fully fuzzy information system	KNOWLEDGE-BASED SYSTEMS										Fully fuzzy information system; Uncertainty measure; Fuzzy information structure; Gaussian kernel	GRANULARITY MEASURES; ENTROPY MEASURES; KNOWLEDGE GRANULATION; ROUGH SETS; SELECTION; MODEL; APPROXIMATION; ALGORITHM; INTERVAL	The uncertainty of information plays an important role in practical applications, so how to capture the uncertainty of information systems becomes more and more popular. Uncertainty measures can supply new viewpoints for processing information systems, and they can help us in disclosing the substantive characteristics of information. Fuzzy information systems are important research objects in artificial intelligence. As a special kind of fuzzy information system, fully fuzzy information system (FFIS) is worth studying. This article is devoted to search indicators for measuring uncertainty in a FFIS according to fuzzy information structures in view of Gaussian kernel, and the fuzzy information structures can be viewed as granular structures under granular computing. Firstly, by employing Gaussian kernel for calculating similarities among objects in a FFIS, the fuzzy Tcos-similarity relation is obtained. Then, based on this relation, fuzzy information structures in a FFIS are introduced. Next, according to the information structures, granulation measure of a given FFIS is advanced. Moreover, entropy measure is also considered for a given FFIS. Finally, two numerical experiments are conducted to interpret the realistic significance and potential applications for measuring uncertainty in a FFIS. Theoretical research, numerical experiments and validity analysis make clear that the proposed measures are efficacious and applicable for a FFIS. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105791	10.1016/j.knosys.2020.105791													
J								LR-SMOTE - An improved unbalanced data set oversampling based on K-means and SVM	KNOWLEDGE-BASED SYSTEMS										Unbalanced data sets; SMOTE; Loose particles signal; LR-SMOTE algorithm	CLASSIFICATION; PREDICTION	Machine learning classification algorithms are currently widely used. One of the main problems faced by classification algorithms is the problem of unbalanced data sets. Classification algorithms are not sensitive to unbalanced data sets, therefore, it is difficult to classify unbalanced data sets. There is also a problem of unbalanced data categories in the field of loose particle detection of sealed electronic components. The signals generated by internal components are always more than the signals generated by loose particles, which easily leads to misjudgment in classification. To classify unbalanced data sets more accurately, in this paper, based on the traditional oversampling SMOTE algorithm, the LR-SMOTE algorithm is proposed to make the newly generated samples close to the sample center, avoid generating outlier samples or changing the distribution of data sets. Experiments were carried out on four sets of UCI public data sets and six sets of self-built data sets. Unmodified data sets balanced by LR-SMOTE and SMOTE algorithms used random forest algorithm and support vector machine algorithm respectively. The experimental results show that the LR-SMOTE has better performance than the SMOTE algorithm in terms of G-means value, F-measure value and AUC. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105845	10.1016/j.knosys.2020.105845													
J								Layer-constrained variational autoencoding kernel density estimation model for anomaly detection	KNOWLEDGE-BASED SYSTEMS										Anomaly detection; Variational autoencoder; Kernel density estimation; Layer constraint; Deep learning	ONE-CLASS SVM; ALGORITHMS	Unsupervised techniques typically rely on the probability density distribution of the data to detect anomalies, where objects with low probability density are considered to be abnormal. However, modeling the density distribution of high dimensional data is known to be hard, making the problem of detecting anomalies from high-dimensional data challenging. The state-of-the-art methods solve this problem by first applying dimension reduction techniques to the data and then detecting anomalies in the low dimensional space. Unfortunately, the low dimensional space does not necessarily preserve the density distribution of the original high dimensional data. This jeopardizes the effectiveness of anomaly detection. In this work, we propose a novel high dimensional anomaly detection method called LAKE. The key idea of LAKE is to unify the representation learning capacity of layer-constrained variational autoencoder with the density estimation power of kernel density estimation (KDE). Then a probability density distribution of the high dimensional data can be learned, which is able to effectively separate the anomalies out. LAKE successfully consolidates the merits of the two worlds, namely layer-constrained variational autoencoder and KDE by using a probability density-aware strategy in the training process of the autoencoder. Extensive experiments on six public benchmark datasets demonstrate that our method significantly outperforms the state-of-the-art methods in detecting anomalies and achieves up to 37% improvement in F-1 score. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105753	10.1016/j.knosys.2020.105753													
J								A filter-based feature construction and feature selection approach for classification using Genetic Programming	KNOWLEDGE-BASED SYSTEMS										Genetic Programming; Feature construction; Feature selection; Classification	PARTICLE SWARM OPTIMIZATION; MULTIPLE FEATURE CONSTRUCTION; EVOLUTIONARY; INFORMATION; DESIGN	Feature construction and feature selection are two common pre-processing methods for classification. Genetic Programming (GP) can be used to solve feature construction and feature selection tasks due to its flexible representation. In this paper, a filter-based multiple feature construction approach using GP named FCM that stores top individuals is proposed, and a filter-based feature selection approach using GP named FS that uses correlation-based evaluation method is employed. A hybrid feature construction and feature selection approach named FCMFS that first constructs multiple features using FCM then selects effective features using FS is proposed. Experiments on nine datasets show that features selected by FS or constructed by FCM are all effective to improve the classification performance comparing with original features, and our proposed FCMFS can maintain the classification performance with smaller number of features comparing with FCM, and can obtain better classification performance with smaller number of features than FS on the majority of the nine datasets. Compared with another feature construction and feature selection approach named FSFCM that first selects features using FS then constructs features using FCM, FCMFS achieves better performance in terms of classification and the smaller number of features. The comparisons with three state-of-art techniques show that our proposed FCMFS approach can achieve better experimental results in most cases. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105806	10.1016/j.knosys.2020.105806													
J								A novel twin minimax probability machine for classification and regression	KNOWLEDGE-BASED SYSTEMS										Minimax probability machine; Classification; Regression; Non-parallel hyperplane; Second-order cone programming	OPTIMIZATION	As an excellent machine learning tool, the minimax probability machine (MPM) has been widely used in many fields. However, MPM does not include a regularization term for the construction of the separating hyperplane, and it needs to solve a large-scale second-order cone programming problem (SOCP) in the solution process, which greatly limits it development and application. In this paper, to improve the performance of MPM, we propose a novel binary classification method called twin minimax probability machine classification (TMPMC). The TMPMC constructs two non-parallel hyperplanes for final classification by solving two smaller SOCPs to improve the performance of the MPM. For each hyperplane, TMPMC attempts to minimize the worst case (maximum) probability that a class of samples is misclassified while being as far away as possible from the other class. Additionally, we extend TMPMC to the regression problem and propose a new regularized twin minimax probability machine regression (TMPMR). Intuitively, the idea of TMPMR is to convert the regression problem into a classification problem to solve. Both TMPMC and TMPMR avoid the assumption of distribution of conditional density. Finally, we extend the linear models of TMPMC and TMPMR to nonlinear case. Experimental results on several datasets show that TMPMC and TMPMR are competitive in terms of generalization performance compared to other algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105703	10.1016/j.knosys.2020.105703													
J								Deep generative models for reject inference in credit scoring	KNOWLEDGE-BASED SYSTEMS										Reject inference; Deep generative models; Credit scoring; Semi-supervised learning	SAMPLE SELECTION BIAS; AUGMENTATION; PERFORMANCE	Credit scoring models based on accepted applications may be biased and their consequences can have a statistical and economic impact. Reject inference is the process of attempting to infer the creditworthiness status of the rejected applications. Inspired by the promising results of semi-supervised deep generative models, this research develops two novel Bayesian models for reject inference in credit scoring combining Gaussian mixtures and auxiliary variables in a semi-supervised framework with generative models. To the best of our knowledge this is the first study coupling these concepts together. The goal is to improve the classification accuracy in credit scoring models by adding reject applications. Further, our proposed models infer the unknown creditworthiness of the rejected applications by exact enumeration of the two possible outcomes of the loan (default or non-default). The efficient stochastic gradient optimization technique used in deep generative models makes our models suitable for large data sets. Finally, the experiments in this research show that our proposed models perform better than classical and alternative machine learning models for reject inference in credit scoring, and that model performance increases with the amount of data used for model training. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105758	10.1016/j.knosys.2020.105758													
J								Multi-Objective Volleyball Premier League algorithm	KNOWLEDGE-BASED SYSTEMS										Multi-objective evolutionary algorithm; Global optimization; Pareto solution; Engineering design optimization problems	PARTICLE SWARM OPTIMIZATION; LOCATION-ALLOCATION PROBLEM; GENETIC ALGORITHM; EVOLUTIONARY ALGORITHMS; OBJECTIVE OPTIMIZATION; OPTIMAL-DESIGN; SEARCH; DECOMPOSITION; PERFORMANCE	This paper proposes a novel optimization algorithm called the Multi-Objective Volleyball Premier League (MOVPL) algorithm for solving global optimization problems with multiple objective functions. The algorithm is inspired by the teams competing in a volleyball premier league. The strong point of this study lies in extending the multi-objective version of the Volleyball Premier League algorithm (VPL), which is recently used in such scientific researches, with incorporating the well-known approaches including archive set and leader selection strategy to obtain optimal solutions for a given problem with multiple contradicted objectives. To analyze the performance of the algorithm, ten multi-objective benchmark problems with complex objectives are solved and compared with two well-known multi-objective algorithms, namely Multi-Objective Particle Swarm Optimization (MOPSO) and Multi-Objective Evolutionary Algorithm Based on Decomposition (MOEA/D). Computational experiments highlight that the MOVPL outperforms the two state-of-the-art algorithms on multi-objective benchmark problems. In addition, the MOVPL algorithm has provided promising results on well-known engineering design optimization problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105781	10.1016/j.knosys.2020.105781													
J								Unsupervised dimensionality reduction for very large datasets: Are we going to the right direction?	KNOWLEDGE-BASED SYSTEMS										Unsupervised dimensionality reduction; Descriptive data mining; Very large datasets; Fractal theory	FEATURE-SELECTION	Given a set of millions or even billions of complex objects for descriptive data mining, how to effectively reduce the data dimensionality? It must be performed in an unsupervised way. Unsupervised dimensionality reduction is essential for analytical tasks like clustering and outlier detection because it helps to overcome the drawbacks of the "curse of high dimensionality". The state-of-the-art approach is to preserve the data variance by means of well-known techniques, such as PCA, KPCA, SVD, and other techniques based on those that have been mentioned, such as PUFS. But, is it always the best strategy to follow? This paper presents an exploratory study performed to compare two distinct approaches: (a) the standard variance preservation, and; (b) one alternative, Fractal-based solution that is rarely used, for which we propose one fast and scalable Spark-based algorithm using a novel feature partitioning approach that allows it to tackle data of high dimensionality. Both strategies were evaluated by inserting into 11 real-world datasets, with up to 123.5 million elements and 518 attributes, at most 500 additional attributes formed by correlations of many kinds, such as linear, quadratic, logarithmic and exponential, and verifying their abilities to remove this redundancy. The results indicate that, at least for large datasets of dimensionality with up to similar to 1,000 attributes, our proposed Fractal-based algorithm is the best option. It accurately and efficiently removed the redundant attributes in nearly all cases, as opposed to the standard variance-preservation strategy that presented considerably worse results, even when applying the KPCA approach that is made for non-linear correlations. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105777	10.1016/j.knosys.2020.105777													
J								Combining multi-label classifiers based on projections of the output space using Evolutionary algorithms	KNOWLEDGE-BASED SYSTEMS										Multi-label classification; Ensemble; Evolutionary algorithm	ENSEMBLES; SELECTION; SETS	The multi-label classification task has gained a lot of attention in the last decade thanks to its good application to many real-world problems where each object could be attached to several labels simultaneously. Several approaches based on ensembles for multi-label classification have been proposed in the literature; however, the vast majority are based on randomly selecting the different aspects that make the ensemble diverse and they do not consider the characteristics of the data to build it. In this paper we propose an evolutionary method called Evolutionary AlGorithm for multi-Label Ensemble opTimization, EAGLET, for the selection of simple, accurate and diverse multi-label classifiers to build an ensemble considering the characteristics of the data, such as the relationship among labels and the imbalance degree of the labels. In order to model the relationships among labels, each classifier of the ensemble is focused on a small subset of the label space, resulting in models with a relative low computational complexity and lower imbalance in the output space. The resulting ensemble is generated incrementally given the population of multi-label classifiers, so the member that best fits to the ensemble generated so far, considering both predictive performance and diversity, is selected. The experimental study comparing EAGLET with state-of-the-art methods in multi-label classification over a wide set of sixteen datasets and five evaluation measures, demonstrated that EAGLET significantly outperformed standard MLC methods and obtained better and more consistent results than state-of-the-art multi-label ensembles. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105770	10.1016/j.knosys.2020.105770													
J								IntelliBot: A Dialogue-based chatbot for the insurance industry	KNOWLEDGE-BASED SYSTEMS										Chatbot; Dialogue-based System; Deep neural networks; Recurrent Neural Networks; Deep Learning; Seq2Seq Model	RESPONSE SELECTION; KAPPA	Chatbots have become the go-to platform for users to receive answers to their queries. They are now being used by many businesses too to provide their customers with a virtual assistant to answer their queries. But when it comes to engaging with a user in a dialogue, existing chatbots have several shortcomings, with issues such as failing to provide a meaningful response to the user, offering semantically incorrect information etc. This paper studies the working styles of existing chatbots in generating a response and then identifies their shortcomings from the viewpoint of engaging in a dialogue with a user. It then proposes a domain-specific chatbot named IntelliBot, which is a response-generating dialogue-based chatbot that uses multiple strategies to generate a response. IntelliBot was trained on two datasets, namely the Cornell movie dialogue and a custom-built insurance dataset so it has domain-specific knowledge. The performance of IntelliBot was then validated and compared with three other chatbots from the literature, namely RootyAI, ChatterBot and DeepQA. The results demonstrate IntelliBot's superiority in engaging with the user and providing a complete answer in the insurance domain. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105810	10.1016/j.knosys.2020.105810													
J								A two-individual based path-relinking algorithm for the satellite broadcast scheduling problem	KNOWLEDGE-BASED SYSTEMS										Solution-based tabu search; Path-relinking; Evolutionary algorithm; Metaheuristics; Satellite broadcast scheduling	TABU SEARCH; NEURAL-NETWORK; DECOMPOSITION; SOLVE	Population-based metaheuristic algorithms normally manage a large number of 'individuals' to achieve diversity in the search process, which in turn lead to good quality solutions. Although the quality of the solutions produced by these algorithms is observed to be good for a variety of combinatorial optimization problems, managing a large number of individuals within the algorithm is often complex and time-consuming. In this paper, we develop a two-individual based path relinking (TPR) algorithmic framework harnessing the power of the solution-based tabu search and that of a distance-controlled relinking operator to solve the satellite broadcast scheduling problem (SBSP). The results of extensive computational experiments carried out demonstrate that our TPR algorithm outperforms state-of-the-art heuristic algorithms for SBSP with respect to various performance metrics, in a statistically significant way. The aim of this study is to compare the two-individual based search algorithm with the traditional population-based metaheuristic methods in the literature for the SBSP (i.e., ant colony optimization and differential evolution variant algorithms) to provide a highly competitive algorithm for solving this important practical problem with numerous applications, and to promote research on two-individual based search method, which received very little attention in the literature. Our results underscore that the two-individual based search algorithmic framework is a viable alternative for solving complex optimization problems and reconfirm the validity of analogous observations made by other researchers in the context of graph coloring and flexible job shop scheduling. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105774	10.1016/j.knosys.2020.105774													
J								Pair-wise Preference Relation based Probabilistic Matrix Factorization for Collaborative Filtering in Recommender System	KNOWLEDGE-BASED SYSTEMS										Recommender System; Probabilistic Matrix Factorization; Collaborative Filtering; Side Information		Matrix Factorization (MF) is one of the most popular techniques used in Collaborative Filtering (CF) based Recommender System (RS). Most of the MF methods tend to remove sparsity or predict missing ratings. However, the prime objective of any RS is to generate recommendation of items, where items having higher preferences should be placed at higher positions in the recommendation list. Further, the integration of user and item features (i.e. Side Information) in any RS model, increases the quality of recommendation. Keeping these in mind, we propose a Probabilistic MF (PMF) model that takes Preference Relation as input (instead of ratings) for generating efficient ranking of items. The user and item side information are integrated into the model using matrix co-factorization technique. Also, user and item neighborhood information (known as second-order interaction among similar users or items) are integrated into the model. The use of PMF enables the proposed method to capture user-item interaction of higher-order. Hence, the proposed RS model can capture all the desired user-item interaction information. Experimental results obtained using two 'MovieLens' datasets indicate the superiority in recommendation quality of the proposed model over the recent state-of-the-art MF methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105798	10.1016/j.knosys.2020.105798													
J								Deep model with neighborhood-awareness for text tagging	KNOWLEDGE-BASED SYSTEMS										Neighborhood-aware; Negative sampling; Deep neural networks; Text tagging		In recent years, many efforts based on deep learning have been made to address the issue of text tagging. However, these work generally neglect to consider the neighborhood effect which may help improve the accuracy of predictions. For this, we present a neighborhood-aware deep model for text tagging (NATT). A neural component which combines bi-directional recurrent neural network and self-attention mechanism, is firstly selected as the text encoder to encode the target document into one feature vector. Then, k-nearest-neighbor documents of the target document are identified and encoded into feature vectors one by one with the same text encoder. Simultaneously, an independent attention module is introduced to aggregate these neighboring documents into a special feature vector, which will represent features of the neighborhood. Finally, the two feature vectors are fused to match the embedding vectors of tags. To optimize the NATT model, we build the objective function with pairwise hinge loss and specially develop a neighborhood-aware negative sampling strategy to form training data. Experimental results on four datasets demonstrate that NATT outperforms some state-of-the-art neural models. Additionally, NATT is economical on achieving the best results with less training epochs and a smaller number of nearest neighbors. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105750	10.1016/j.knosys.2020.105750													
J								Multi-scale generative adversarial inpainting network based on cross-layer attention transfer mechanism	KNOWLEDGE-BASED SYSTEMS										Image inpainting; Deep learning; Generative adversarial network; Attention mechanism; Multi-scale	OBJECT REMOVAL; IMAGE; RECOGNITION	Deep learning-based methods have recently shown promising results in image inpainting. These methods generate patches with visually plausible image structures and textures, which are semantically coherent with the context of surrounding regions. However, existing methods tend to generate artifacts which are inconsistent with surrounding regions, especially when dealing with complex images. Aiming at the limitations current in deep learning-based methods, this paper proposes a multi-scale generative adversarial network model based on cross-layer attention transfer mechanism. Cross-Layer Attention Transfer Module (CL-ATM) is presented to guide the filling of the corresponding low-level semantic feature map by using the high-level semantic feature map, so as to ensure visual and semantic consistency of inpainting. Meanwhile, a multi-scale generator and the multi-scale discriminators are added into the network structure. Different scales of discriminators have different receptive fields, which enable the generator to produce images with better global consistency and more details. Qualitative and quantitative experiments show that our method has superior performance against state-of-art inpainting models. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105778	10.1016/j.knosys.2020.105778													
J								Software defect prediction based on correlation weighted class association rule mining	KNOWLEDGE-BASED SYSTEMS										Software defect prediction; Associative classification; Class imbalance; Attribute weighting; Apriori; Association rule	FAULT PREDICTION; CLASSIFICATION; ALGORITHM	Software defect prediction based on supervised learning plays a crucial role in guiding software testing for resource allocation. In particular, it is worth noticing that using associative classification with high accuracy and comprehensibility can predict defects. But owing to the imbalance data distribution inherent, it is easy to generate a large number of non-defective class association rules, but the defective class association rules are easily ignored. Furthermore, classical associative classification algorithms mainly measure the interestingness of rules by the occurrence frequency, such as support and confidence, without considering the importance of features, resulting in combinations of the insignificant frequent itemset. This promotes the generation of weighted associative classification. However, the feature weighting based on domain knowledge is subjective and unsuitable for a high dimensional dataset. Hence, we present a novel software defect prediction model based on correlation weighted class association rule mining (CWCAR). It leverages a multi-weighted supports-based framework rather than the traditional support-confidence approach to handle class imbalance and utilizes the correlation-based heuristic approach to assign feature weight. Besides, we also optimize the ranking, pruning and prediction stages based on weighted support. Results show that CWCAR is significantly superior to state-of-the-art classifiers in terms of Balance, MCC, and Gmean. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105742	10.1016/j.knosys.2020.105742													
J								Penalized multiple distribution selection method for imbalanced data classification	KNOWLEDGE-BASED SYSTEMS										Imbalance training; Knowledge extraction; Mixture distribution; Distribution selection; Text classification	WORD EMBEDDINGS; SMOTE	In reality, the amount of data from different categories varies significantly, which results in learning bias towards prominent classes, hindering the overall classification performance. In this paper, by proving that traditional classification methods that use single softmax distribution are limited for modeling complex and imbalanced data, we propose a general Multiple Distribution Selection (MDS) method for imbalanced data classification. MDS employs a mixture distribution that is composed of a single softmax distribution and a set of degenerate distributions to model imbalanced data. Furthermore, a dynamic distribution selection method, based on L-1 regularization, is also proposed to automatically determine the weights of distributions. In addition, the corresponding two-stage optimization algorithm is designed to estimate the parameters of models. Extensive experiments conducted on three widely used benchmark datasets (IMDB, ACE2005, 20NewsGroups) show that our proposed mixture method outperforms previous methods. Moreover, under highly imbalanced setting, our method achieves up to a 4.1 absolute F1 gain over high-performing baselines. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105833	10.1016/j.knosys.2020.105833													
J								The concept of unavoidable features in fuzzy relational compositions	KNOWLEDGE-BASED SYSTEMS										Fuzzy relational compositions; Unavoidable features; Excluding features; Bandler-Kohout products; Fuzzy quantifiers; Dragonflies; Amphibians	EXCLUDING FEATURES; QUANTIFIERS	Fuzzy relational calculus, and especially, the compositions of fuzzy relations, had a great impact on many theoretical as well as practical areas of fuzzy modeling and it attracted numerous researchers. Recently, the compositions had been extended in distinct directions including the incorporation of excluding features or the direct use of generalized quantifiers instead of the standard ones. This article provides an investigation of an extension that is mathematically similarly constructed to the concept of excluding features although from the semantic point of view it is opposite. In particular, we introduce the concept of unavoidable features. Formally, the concept is again constructed as conjunctive fusion of an existing fuzzy relational composition with another composition, namely the Bandler-Kohout superproduct, employing a newly defined fuzzy relation of unavoidable features for particular classes. We provide an investigation of mathematical properties of the new concept and we demonstrate the appropriateness of the proposed concept on the classification task. This is provided with help of a real biological classification problems running on a data-sets of dragonflies and amphibians. A significant improvement of the results is emphasizing how strong influence may be provided with simple ideas and concepts fused appropriately together to a more complicated yet still explainable concept. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105785	10.1016/j.knosys.2020.105785													
J								Exploring ubiquitous relations for boosting classification and localization	KNOWLEDGE-BASED SYSTEMS										Deep learning; Relation knowledge exploring; Computer vision; Few-shot classification; Saliency detection	SMOTE	Although the weakly supervised learning can effectively avoid the tedious data annotating process of deep learning approaches, the performance is still in urgent need of enhancement. In this paper, we endeavor to mine a ubiquitous and fundamental knowledge-Relation, to boost several existing classification and localization models without changing the original structure. We first propose a universal relation exploring scheme to mine the relations among entities. This scheme can be specialized into different instantiations including object, superpixel and pixel relations to stimulating different learning models. We adopt the object relations on a few-shot classification model to concentrate on the dominant object, and to boost its discriminative capacity. The superpixel relation is utilized to improve the performance of the saliency object detection models. The sensitivity of the pixel relations to the uncertain regions makes it suitable for distinguishing the disputed area in saliency detection results. Our experiments demonstrate that all the three relation instantiations can significantly boost the performance of the state-of-the-art learning models and optimize the visual result. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105824	10.1016/j.knosys.2020.105824													
J								Churn and Net Promoter Score forecasting for business decision-making through a new stepwise regression methodology	KNOWLEDGE-BASED SYSTEMS										Churn prediction; Net Promoter Score; Stepwise regression; WOE variables	MODEL	Companies typically have to make relevant decisions regarding their clients' fidelity and retention on the basis of analytical models developed to predict both their churn probability and Net Promoter Score (NPS). Although the predictive capability of these models is important, interpretability is a crucial factor to look for as well, because the decisions to be made from their results have to be properly justified. In this paper, a novel methodology to develop analytical models balancing predictive performance and interpretability is proposed, with the aim of enabling a better decision-making. It proceeds by fitting logistic regression models through a modified stepwise variable selection procedure, which automatically selects input variables while keeping their business logic, previously validated by an expert. In synergy with this procedure, a new method for transforming independent variables in order to better deal with ordinal targets and avoiding some logistic regression issues with outliers and missing data is also proposed. The combination of these two proposals with some competitive machine-learning methods earned the leading position in the NPS forecasting task of an international university talent challenge posed by a well-known global bank. The application of the proposed methodology and the results it obtained at this challenge are described as a case-study. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105762	10.1016/j.knosys.2020.105762													
J								Learning discriminative domain-invariant prototypes for generalized zero shot learning	KNOWLEDGE-BASED SYSTEMS										Generalized Zero Shot Learning (GZSL); Domain-invariant learning; Orthogonal constraint; Dictionary learning	CLASSIFICATION	Zero-shot learning (ZSL) aims to recognize objects of target classes by transferring knowledge from source classes through the semantic embeddings bridging. However, ZSL focuses the recognition only on unseen classes, which is unreasonable in realistic scenarios. A more reasonable way is to recognize new samples on combined domains, namely Generalized Zero Shot Learning (GZSL). Due to the fact that the source domain and target domain are disjoint and have unrelated classes potentially, ZSL and GZSL often suffer from the problem of projection domain shift. Besides, some semantic embeddings of prototypes are very similar, which makes the recognition less discriminative. To circumvent these issues, in this paper, we propose a novel method, called Learning Discriminative Domain-Invariant Prototypes (DDIP). In DDIP, both target and source domains are combined and projected into a hyper-spherical space, which is automatically learned by a regularized dictionary learning. In addition, an orthogonal constraint is employed to the latent hyper-spherical space to ensure all the class prototypes, including seen classes and unseen classes, to be orthogonal to each other to make them more discriminative. Extensive experiments on four popular benchmark and a large-scale datasets are conducted on both GZSL and standard ZSL settings, and the results show that our DDIP can outperform the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105796	10.1016/j.knosys.2020.105796													
J								The autonomous navigation and obstacle avoidance for USVs with ANOA deep reinforcement learning method	KNOWLEDGE-BASED SYSTEMS										Autonomous navigation; Obstacle avoidance; Reinforcement learning; Unmanned surface vehicle (USV)	PATH; ALGORITHM	The unmanned surface vehicle (USV) has been widely used to accomplish missions in the sea or dangerous marine areas for ships with sailors, which greatly expands protective capability and detection range. When USVs perform various missions in sophisticated marine environment, autonomous navigation and obstacle avoidance will be necessary and essential. However, there are few effective navigation methods with real-time path planning and obstacle avoidance in dynamic environment. With tailored design of state and action spaces and a dueling deep Q-network, a deep reinforcement learning method ANOA (Autonomous Navigation and Obstacle Avoidance) is proposed for the autonomous navigation and obstacle avoidance of USVs. Experimental results demonstrate that ANOA outperforms deep Q-network (DQN) and Deep Sarsa in the efficiency of exploration and the speed of convergence not only in static environment but also in dynamic environment. Furthermore, the ANOA is integrated with the real control model of a USV moving in surge, sway and yaw and it achieves a higher success rate than Recast navigation method in dynamic environment. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105201	10.1016/j.knosys.2019.105201													
J								NetNCSP: Nonoverlapping closed sequential pattern mining	KNOWLEDGE-BASED SYSTEMS										Sequential pattern mining; Closed pattern mining; Nonoverlapping sequence pattern; Periodic wildcard gaps; Nettree; COVID-19	MAXIMAL FREQUENT PATTERNS; SEQUENCES; ALGORITHM	Sequential pattern mining (SPM) has been applied in many fields. However, traditional SPM neglects the pattern repetition in sequence. To solve this problem, gap constraint SPM was proposed and can avoid finding too many useless patterns. Nonoverlapping SPM, as a branch of gap constraint SPM, means that any two occurrences cannot use the same sequence letter in the same position as the occurrences. Nonoverlapping SPM can make a balance between efficiency and completeness. The frequent patterns discovered by existing methods normally contain redundant patterns. To reduce redundant patterns and improve the mining performance, this paper adopts the closed pattern mining strategy and proposes a complete algorithm, named Nettree for Nonoverlapping Closed Sequential Pattern (NetNCSP) based on the Nettree structure. NetNCSP is equipped with two key steps, support calculation and closeness determination. A backtracking strategy is employed to calculate the nonoverlapping support of a pattern on the corresponding Nettree, which reduces the time complexity. This paper also proposes three kinds of pruning strategies, inheriting, predicting, and determining. These pruning strategies are able to find the redundant patterns effectively since the strategies can predict the frequency and closeness of the patterns before the generation of the candidate patterns. Experimental results show that NetNCSP is not only more efficient but can also discover more closed patterns with good compressibility. Furtherly, in biological experiments NetNCSP mines the closed patterns in SARS-CoV-2 and SARS viruses. The results show that the two viruses are of similar pattern composition with different combinations. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105812	10.1016/j.knosys.2020.105812													
J								A deep transfer maximum classifier discrepancy method for rolling bearing fault diagnosis under few labeled data	KNOWLEDGE-BASED SYSTEMS										Long-short term memory; Batch normalization; Transfer maximum classifier discrepancy; Fault diagnosis; Few labeled data	SUPPORT VECTOR MACHINE; PERFORMANCE; ENTROPY	Rolling bearing fault diagnosis is closely related to the safety of mechanical system. In real-world diagnosis, it is difficult to obtain abundant labeled data due to varying operation conditions, complex working environment and inevitable indirect measurement, which will affect the ability of diagnosing. To tackle this problem, a deep transfer maximum classifier discrepancy method is proposed under few labeled data, which utilizes fully deep learning and transfer learning. Firstly, a batch-normalized long-short term memory (BNLSTM) model which can learn the mapping relationship between two kinds of datasets is designed to generate some auxiliary samples. Then, a transfer maximum classifier discrepancy (TMCD) method, which considers the characteristics of each data type by an adversarial strategy, is applied to align probability distributions of auxiliary samples generated by BNLSTM and unlabeled data from target domain. Sufficient experimental results indicate the effectiveness of the proposed method under few labeled data. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105814	10.1016/j.knosys.2020.105814													
J								NetSRE: Link predictability measuring and regulating	KNOWLEDGE-BASED SYSTEMS										Network data; Link prediction; Link predictability; Structural patterns; Low-rank coding; Structure perturbation	COMPLEX NETWORKS; COMMUNITY STRUCTURE; SCALE-FREE; PREDICTION; PRIVACY; RECONSTRUCTION; IDENTIFICATION; ALGORITHM	Link prediction is an elemental issue for network-structured data mining, which has already found a wide range of applications. The organization of real-world networks usually embodies both regularities and irregularities, and the precision of link prediction algorithms coincides with the portion of a network being categorized as regular. Quantifying and controlling how well an unobserved link can be predicted is a fundamental problem in link prediction. This paper proposes a structural regularity-exploring architecture, called NetSRE, for measuring and regulating link predictability of networks. The proposed NetSRE assumes that there are consistent interaction patterns across the local subgraphs of networks and one of them can be represented by a linear summation of the others, and thus, link predictability can be characterized by the self-representation degree of network structures. Specifically, NetSRE includes (1) a low Frobenius norm pursuit-based self-representation network model for predicting the "true" underlying networks, (2) a "structural regularity" index for measuring the link predictability of networks, i.e., the inherent difficulty of link prediction independent of specific algorithms, and (3) an importance measuring method for structural role exploration of network links and a link-based structure perturbation algorithm for link predictability regulation. Experimental results on real-world networks validate the performance of our method. It is found that real-world networks have various structural regularities and link predictability can be estimated based on structure mining directly. We show that network heterogeneity provides a way to intrinsically segregate network links into qualitatively distinct groups, which have different influences on the link predictability of networks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105800	10.1016/j.knosys.2020.105800													
J								Inverse projection group sparse representation for tumor classification: A low rank variation dictionary approach	KNOWLEDGE-BASED SYSTEMS										Tumor classification; Low rank variation dictionary; Inverse projection group sparse representation; Microarray gene expression data	GENE-EXPRESSION DATA; MOLECULAR CLASSIFICATION; FACE RECOGNITION; CANCER; PREDICTION; CLASSIFIERS; SELECTION	Sparse representation based classification (SRC) achieves good results by addressing recognition problem with sufficient training samples per subject. Tumor classification, however, is a typical small sample problem. In this paper, an inverse projection group sparse representation (IPGSR) model is presented for tumor classification based on constructing a low rank variation dictionary (LRVD), for short, LRVD-IPGSR model. Firstly, an IPGSR model is constructed based on making full use of existing training and test samples, and group sparsity effect of genetic data. Furthermore, from a new viewpoint, a LRVD is constructed for improving the performance of IPGSR-based tumor classification. The LRVD can be independently constructed by detecting and utilizing variations of normals and typical patients, rather than directly using and changed with the genetic data or their corresponding feature data. And the LRVD can be automatic updated and extended to fit the case of new types of diseases. Finally, the LRVD-IPGSR model is fully analyzed from feasibility, stability, optimization and convergence. The performance of the LRVD-IPGSR model-based tumor classification framework is verified on eight microarray gene expression datasets, which contain early diagnosis, tumor type recognition and postoperative metastasis. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105768	10.1016/j.knosys.2020.105768													
J								Node proximity preserved dynamic network embedding via matrix perturbation	KNOWLEDGE-BASED SYSTEMS										Network embedding; Matrix factorization; Node proximity; Dynamic network		In recent years, network embedding has attracted extensive interests, which aims at representing nodes of an original network in a low-dimensional vector space while preserving the inherent topological structures of the network. Despite the remarkable advantages of complex networks, most existing network embedding methods are mainly focused on static networks while ignoring the evolving characteristic, which is proved to be an essential property of real-world networks. In this paper, we propose Node Proximity Preserved Dynamic Network Embedding via Matrix Perturbation (NPDNE) to tackle the dilemma. Specifically, our method implements a low-rank transformation on the normalized Laplacian matrix of the given networks and then derives the embedding vectors through generalized SVD. Subsequently, the node proximities are preserved in the embedding vectors by exploiting the eigen-decomposition reweighing theorem, which reveals the intrinsic relationship among different-order proximities. Moreover, a generalized eigen perturbation is adopted to update the embedding vectors so that the evolution of given networks can be captured over time. Finally, we conduct experiments of multi-label classification, link prediction, and visualization on several real-world datasets. The experimental results demonstrate the superiority of the proposed NPDNE model compared with state-of-the-art baselines. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105822	10.1016/j.knosys.2020.105822													
J								Particle swarm optimization with adaptive learning strategy	KNOWLEDGE-BASED SYSTEMS										Particle swarm optimization; Adaptive learning strategy; Multiswarm; Dynamic particle classification	BEE COLONY; PERFORMANCE; ALGORITHM	Population diversity maintenance is a crucial task for preventing a particle swarm optimization (PSO) algorithm from being trapped in local optima. A learning strategy is an effective means of improving population diversity. However, for the canonical PSO algorithm, the learning strategy focuses mainly on the global best particle, which leads to a loss of diversity. To increase the population diversity and strengthen the global search ability in PSO, this paper proposes a PSO algorithm with an adaptive learning strategy (PSO-ALS). To better promote the performance of the learning strategy, the swarm is adaptively grouped into several subswarms. The particles in each subswarm are further classified into ordinary particles and the locally best particle, and two different learning strategies without an explicit velocity are devised for updating the particles to increase the population diversity. Thus, the global optimum is determined by comparing the fitness values of the updated best particles in each subswarm. The proposed algorithm is compared with state-of-the-art PSO variants. The experimental results illustrate that the performance of PSO-ALS is promising and competitive in terms of enhanced population diversity and global search ability. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105789	10.1016/j.knosys.2020.105789													
J								Ensemble deep contractive auto-encoders for intelligent fault diagnosis of machines under noisy environment	KNOWLEDGE-BASED SYSTEMS										Intelligent fault diagnosis; Ensemble deep contractive auto-encoder; Noisy signal; Deep learning	ROTATING MACHINERY; SPARSE AUTOENCODER; NEURAL-NETWORK	Intelligent fault diagnosis methods based on deep auto-encoder have achieved great success in the past several years. However, these methods cannot effectively handle the data collected under noisy environment. Therefore, this paper proposes a new ensemble deep contractive auto-encoder (EDCAE) to address the problem. First, we design fifteen deep contractive auto-encoders (DCAE) to learn invariant feature representation automatically. Due to the Jacobian penalty term in DCAE and different characteristics, these models can deal with various noisy data effectively. Second, fisher discriminant analysis is applied to select low-dimensional features with the maximum class separability. Softmax classifier is adopted to identify the selected features and produce fifteen classification results. Finally, a new combination strategy is developed to combine these individual results. Benefitting from the combination strategy, it can produce accurate diagnosis results even under strong background noise. Additionally, to prove the effectiveness of EDCAE, theory analysis about error bound is conducted. The proposed method is verified on three case studies including bearing, gear box and self-priming centrifugal pump. Experiments are conducted under seven different signal-to-noise-ratios. Results show that EDCAE is better than other intelligent diagnosis methods, including individual DCAE, deep auto-encoder, sparse deep auto-encoder, deep denoising auto-encoder and several ensemble methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105764	10.1016/j.knosys.2020.105764													
J								Personalized location recommendation by fusing sentimental and spatial context	KNOWLEDGE-BASED SYSTEMS										Data mining; Location based social network; POI recommendation; Recommender system; Sentiment analysis	SOCIAL NETWORKS; USERS	Internet users would like to obtain interesting location information for a travel. With the rapid development of social media, many kinds of location recommender systems are proposed in recent years. Existing methods mostly focus on mining user check-in information that could be leveraged to understand their trajectories. However, the characteristics and attributes of geographical locations also play an important role in recommender systems. In this paper, sentimental attributes of locations are explored and we propose a Point of Interest (POI) mining method and a personalized recommendation model by fusing sentimental spatial context. First, a Sentimental-Spatial POI Mining (SPM) method is utilized to mine the POIs by fusing the sentimental and geographical attributes of locations. Second, we recommend the POIs to users by a Sentimental-Spatial POI Recommendation (SPR) model incorporating the factors of sentiment similarity and geographical distance. Last, the advantages and superior performance of our methods are demonstrated by extensive experiments on a real-world dataset. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				MAY 21	2020	196								105849	10.1016/j.knosys.2020.105849													
J								Knowledge graph entity typing via learning connecting embeddings	KNOWLEDGE-BASED SYSTEMS										Knowledge graph; Entity typing; KG completion; Embedding model	WORD EMBEDDINGS	Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion. In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs. Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference. Accordingly, we build two novel embedding models to realize the mechanisms. Afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs. Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 21	2020	196								105808	10.1016/j.knosys.2020.105808													
J								Generalized feature similarity measure	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Feature evaluation measures; Feature selection; Information gain; chi(2); Unified framework	FEATURE-SELECTION	Quantifying the degree of relation between a feature and target class is one of the key aspects of machine learning. In this regard, information gain (IG) and chi(2) are two of the most widely used measures in feature evaluation. In this paper, we discuss a novel approach to unifying these and other existing feature evaluation measures under a common framework. In particular, we introduce a new generalized family of measures to estimate the similarity between features. We show that the proposed set of measures satisfies all the general criteria for quantifying the relationship between features. We demonstrate that IG and chi(2) are special cases of the generalized measure. We also analyze some of the topological and set-theoretic aspects of the family of functions that satisfy the criteria of our generalized measure. Finally, we produce novel feature evaluation measures using our approach and analyze their performance through numerical experiments. We show that a diverse array of measures can be created under our framework which can be used in applications such fusion based feature selection.																	1012-2443	1573-7470				SEP	2020	88	9					987	1002		10.1007/s10472-020-09700-8		MAY 2020											
J								HDPA: historical document processing and analysis framework	EVOLVING SYSTEMS										CNN; Document analysis; Framework; Historical documents; LSTM; Neural networks; OCR	IMAGE; SEGMENTATION	Nowadays, the accessibility of digitized historical documents is extremely important to facilitate fast and efficient retrieval of historical information and knowledge extraction from such data. To provide such functionality, it is necessary to convert document images into plain text using optical character recognition (OCR). Many OCR related methods and tools have been proposed, however, they are often too complicated for a standard user, some important parts are missing or they are not available in free versions. Therefore, this paper describes a complex and flexible web framework for historical document manipulation and analysis with the main focus on OCR. The framework contains eight modules to facilitate three main tasks: image pre-processing and segmentation, creation of data for OCR model training and the OCR itself. This framework is freely available for non commercial purposes. We have experimentally evaluated this framework on real data and we have shown that this system is efficient and can save human labour in the process of annotated data preparation. Moreover, we have reached state-of-the-art OCR results.																	1868-6478	1868-6486															10.1007/s12530-020-09343-4		MAY 2020											
J								A Computational Model for Measuring Trust in Mobile Social Networks Using Fuzzy Logic	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Trust; fuzzy clustering; mobile social networks; trust calculation model; fuzzy logic	MANAGEMENT; FRAMEWORK; DISTRUST	Large-scale mobile social networks (MSNs) facilitate communications through mobile devices. The users of these networks can use mobile devices to access, share and distribute information. With the increasing number of users on social networks, the large volume of shared information and its propagation has created challenges for users. One of these challenges is whether users can trust one another. Trust can play an important role in users' decision making in social networks, so that, most people share their information based on their trust on others, or make decisions by relying on information provided by other users. However, considering the subjective and perceptive nature of the concept of trust, the mapping of trust in a computational model is one of the important issues in computing systems of social networks. Moreover, in social networks, various communities may exist regarding the relationships between users. These connections and communities can affect trust among users and its complexity. In this paper, using user characteristics on social networks, a fuzzy clustering method is proposed and the trust between users in a cluster is computed using a computational model. Moreover, through the processes of combination, transition and aggregation of trust, the trust value is calculated between users who are not directly connected. Results show the high performance of the proposed trust inference method.																	1476-8186	1751-8520															10.1007/s11633-020-1232-5		MAY 2020											
J								Network security analysis using big data technology and improved neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data technology; Network security; Neural network; Blocked fuzzy C-means clustering	CLASSIFICATION; SYSTEMS	Network security is an important part of the future information world. The degree of social informatization is maturing and network infrastructure is gradually improving, the network has become an indispensable part of human life. With the frequent occurrence of cyber security incidents around the world, cyber security research has become very important. The amount of network data is huge, and it is necessary to use big data (BD) analysis technology and various machine learning algorithms to analyze and predict the network security (NS) situation. In the NS situation awareness technology, the network behavior and the possible impact are mainly identified by analyzing data records. However, existing NS situation awareness models need to be improved, such as large resource overhead, low accuracy of analysis results, low processing efficiency, and inability to apply to real-time and large-scale scenarios. To solve these shortcomings, a new NS BD analysis model is proposed using improved BP neural network. In the model, data is simplified and cleaned according to the characteristics of the data records, thereby solving the problems of heterogeneous data sources and high noise. With the classic BP neural network as the core of the model, the error inverse feedback strategy of the neural network is used to improve the accuracy of the model analysis. The innovation of this article is that when preprocessing the input data, the proposed blocked fuzzy C-means clustering algorithm (BFCM) is used to cluster the features of the data records to strengthen the features of the data, thereby improving the model's precision. First, the structure of the NS awareness model is introduced in detail, and then the relevant steps in the model are described in detail. Finally, a series of experiments have been performed to verify that the proposed model has certain reference value for the perception of NS situations.																	1868-5137	1868-5145															10.1007/s12652-020-02080-1		MAY 2020											
J								A microsimulation-based analysis for driving behaviour modelling on a congested expressway	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sustainability; Microscopic simulation; Driving behaviour; Traffic incident management	MICROSCOPIC SIMULATION; CALIBRATION; CAPACITY	Recently, simulation models have been widely used around the world to evaluate the performance of different traffic facilities and management strategies for efficient and sustainable transportation systems. One of the keys factors for ensuring the reliability of the models in reflecting local conditions is the calibration and validation of microsimulation models. The majority of the existing calibration efforts focus is on the experimental designs of driver behaviour and lane-changing parameters. Towards this end, this paper describes the necessary procedure for the calibration and validation of a microscopic model using the VISSIM software, during peak hours. The procedure is applied on Muscat Expressway in the Sultanate of Oman. The calibration parameters and the measure-of-effectiveness are identified by using multi-parameter sensitivity analysis. The optimum values for these parameters are obtained by minimising errors between simulated data and field data. In our proposed model, we used traffic volume and travel speed for model calibration, as well as average travel time for validation of the calibrated model. The achieved results showed that driving characteristics significantly impacted the merging/diverging traffic flow ratio in the merging area, the link length and the distance between on-ramps and off-ramps, as well as the percentage of heavy vehicles. The results also showed that having both the advanced merging and cooperative lane-change settings active, along with safety distance reduction factor, necessary lane change, minimum headway (front/rear), and emergency stop, had a significant influence on simulation precision, especially at on-ramps and off-ramps. Finally, our proposed model can be utilized as a base for future traffic strategy analysis and intelligent transportation systems evaluation to help decision makers with long-term and sustainable development decisions.																	1868-5137	1868-5145															10.1007/s12652-020-02098-5		MAY 2020											
J								Integrating machine learning and open data into social Chatbot for filtering information rumor	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Chatbot; Cloud computing; Open data		Social networks have become a major platform for people to disseminate information, which can include negative rumors. In recent years, rumors on social networks has caused grave problems and considerable damages. We attempted to create a method to verify information from numerous social media messages. We propose a general architecture that integrates machine learning and open data with a Chatbot and is based cloud computing (MLODCCC), which can assist users in evaluating information authenticity on social platforms. The proposed MLODCCC architecture consists of six integrated modules: cloud computing, machine learning, data preparation, open data, chatbot, and intelligent social application modules. Food safety has garnered worldwide attention. Consequently, we used the proposed MLODCCC architecture to develop a Food Safety Information Platform (FSIP) that provides a friendly hyperlink and chatbot interface on Facebook to identify credible food safety information. The performance and accuracy of three binary classification algorithms, namely the decision tree, logistic regression, and support vector machine algorithms, operating in different cloud computing environments were compared. The binary classification accuracy was 0.769, which indicates that the proposed approach accurately classifies using the developed FSIP.																	1868-5137	1868-5145															10.1007/s12652-020-02119-3		MAY 2020											
J								Performance analysis of video transmission in vertical-UWOC link in mid-sea oil rig IoT systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Underwater wireless optical communication (UWOC); Industrial IoT; Video transmission; Hermite gaussian beam; Signal misalignment	WIRELESS; TURBULENCE; OFDM	In recent years, need of transmitting high quality video signal for industrial Internet of Things (IoT) is increased for monitoring and manufacturing process automation. In mid sea oil rigs, during oil exploration autonomous systems are used widely which requires better video transmission through Underwater Wireless Optical Connection (UWOC). The underwater optical links are affected due to various water types, distinctive water profundities, various seasons, diverse weakening attributes the signal transmitted may endure different issue like absorption, scattering, reduced SNR, expanded BER, link misalignment issues, which will prompt low quality video. In this paper, the UWOC link characteristics such as BER, link mis alignment, video quality, pertaining to video transmission for various modulation techniques are analysed. The simulation results state that, UWOC communication link equipped with better error correction scheme results in better video quality.																	1868-5137	1868-5145															10.1007/s12652-020-02081-0		MAY 2020											
J								Shape and color feature based melanoma diagnosis using dermoscopic images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Skin cancer; Melanoma; Dermoscopic images; Feature extraction; Classification	COMPUTER-AIDED DIAGNOSIS	In this paper, an essential system to identify the melanoma in skin at an early stage is proposed. Skin Cancer (SC) is one of the deadliest disease and its morality rates is very high. A SC classification model is designed based on the novel Color, Shape feature extraction and Classifier to detect the Melanoma which is known as CSC-Mel identification model. In preprocessing, feature and gradient adaptive of contour model is employed to segment the skin lesion. Along with ABCD rule, a novel shape and colour features are extracted as features and K-Nearest Neighbor (KNN) classification is employed for the classification. The CSC-Mel Identification Model is tested on PH2 dermoscopic image dataset with 3-Fold Cross Validation (FCV) for testing and development process. Results shows that the CSC-Mel identification model identifies the skin cancer with an accuracy of 90.5%.																	1868-5137	1868-5145															10.1007/s12652-020-02022-x		MAY 2020											
J								Improved performance accuracy in detecting tumor in liver using deep learning techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Liver lesions; Disease diagnosis; Liver tumor; Classification	CT	One of the foremost decease causes in the world is the Liver Cancer. Using practical radiology the liver lesions can be determined with accurate dramatization, due to the hurt caused by wound or disease, those ranges of tissues are damaged the body are liver lesions Those anomalous tissues which are found in the liver are referred as liver lesions. These damaged regions having different intensities of pixel can be recognized by differentiating it from other regions, in the CT scan. The most prohibitive, difficult and time-consuming task is physical cleavage of this CT scan in proper clinical treatment. On the other hand, automatic segmentation is identical challenging task, due to several factors, including liver stretch over 150 slices in a CT image, having small ferocity conflict between lesions and other nearby similar tissues and indefinite shape of the lesions is to detected An important prerequisite task before any surgical intervention is liver tumors segmentation. This paper reviews a variety of liver tumor detection algorithms and methodologies used for liver tumor analysis. The proposed deep learning approach such as Probabilistic neural network is proposed to detect the liver tumor and diagnose with the experimental results and it is compared with different methodologies.																	1868-5137	1868-5145															10.1007/s12652-020-02107-7		MAY 2020											
J								Study of the key technology on the Geo-hazard spatial information sharing platform in Meizoseismal Region of Wenchuan Earthquake Zone	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Geo-hazard; Sharing service; SOA; GeoWeb	DISASTERS	With great changes in global climate and unusual and extreme weather in recent years, the geological disaster happens more frequently than ever before, so people attach more and more importance to the geo-disaster defending and decreasing for the social economic development especially after the great earthquake in Wenchuan, Sichuan province. As our nation's great strategy informational source, Geo-hazard information is the key technology in the area of government managing decision, which plays an important role in a project. How to use it has become the key problem. The development of new industry and the promotion of life conditions makes the service based on the geo-information in huge demand. Geo-hazard spatial information sharing service platform is to build a basic spatial information sharing environment. In this environment, the information will be shared in the form of Web services for government agencies, businesses, and the public, provided that geological hazard specifications and standards are followed. Actually, with the development of Internet technology and cloud computing, preliminary results have been achieved on the problem of geographic information sharing and corresponding evaluation work has been carried out. Based on the idea of disaster ontology, the geographic shared service framework is designed and expounded, which is adapted to Wenchuan zone by using data catalog rules. The geo-shared service model is derived from Geo-hazard Spatial Information Sharing Service Model. The design of service interface is expounded. The evaluation and application as well as effective system test is finished based on the Wenchuan geo-disaster evaluation and application system.																	1868-5137	1868-5145															10.1007/s12652-020-02117-5		MAY 2020											
J								Mission Capability Estimation of Multicopter UAV for Low-Altitude Remote Sensing	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Mission capability estimation; Multicopter UAV; Energy consumption; Remote sensing; Mission planning	AERIAL VEHICLES UAVS; FLIGHT PARAMETERS; TERRESTRIAL; ENERGY; PHOTOGRAMMETRY	Restricted mission capability becomes one of the most important challenges facing multicopter UAVs with the increasing complexity of the low-altitude remote sensing mission. Mission capability estimation is the major precondition and the foundation of the mission planning, decomposition, and execution. It's constrained by platform performance, payload characteristics, mission requirements, and environmental conditions. Flight energy consumption has become the main capability metric criterion of multicopter UAVs in coverage path planning based remote sensing missions. Modeling on flight energy consumption has been the principal subject of many pieces of research. However, current models have not established a connection between inflight performance and remote sensing mission characteristics. Therefore, a mechanistic model is proposed in this study to estimate a multicopter UAV's mission capability during the mission planning procedure. This model is based on the combination of multicopter flight mechanics and remote sensing mission-oriented coverage path planning theories. The field wind condition is taken into consideration as well. The applicability and performance of the model were evaluated by combining simulation and 60 field mission sorties. The model estimated results are in accordance with the true values. The mean error in the total energy consumption is 5% of the battery used in the experiment. This model will ultimately provide a theoretical basis for mission decision making, help to reduce the low-battery risk and guarantee the safety and efficiency of mission operations prior.																	0921-0296	1573-0409				NOV	2020	100	2					667	688		10.1007/s10846-020-01199-9		MAY 2020											
J								Criminal psychological emotion recognition based on deep learning and EEG signals	NEURAL COMPUTING & APPLICATIONS										Deep learning; Neural network; EEG; EEG signals; Criminal psychology; Emotion recognition	CLASSIFICATION	The difficulty of criminal psychological recognition is that it is difficult to classify emotions, and the accuracy of traditional recognition methods is insufficient. Therefore, it is necessary to improve the accuracy rate in combination with modern computer technology. This study uses deep learning as technical support and combines EEG computer signals to classify criminal psychological emotions. Moreover, a method for classifying EEG signals based on the state of mind of neural networks was constructed in the study. In addition, the EEG is denoised preprocessed by time-domain regression method, and features of the EEG signal parameters of different criminal psychological tasks are extracted and used as the input of the neural network. Finally, in order to verify the effectiveness of the algorithm, a simulation experiment is designed to study the effectiveness of the algorithm. The results show that the method proposed in this paper has certain practical effects.																	0941-0643	1433-3058															10.1007/s00521-020-05024-0		MAY 2020											
J								Spiking neural firefly optimization scheme for the capacitated dynamic vehicle routing problem with time windows	NEURAL COMPUTING & APPLICATIONS										Spiking neural P systems; Firefly optimization; Dynamic vehicle routing; Membrane computing	FLEET MANAGEMENT; P SYSTEMS; EVOLUTIONARY ALGORITHM; PROGRAMMING ALGORITHM; WAITING STRATEGIES; GENETIC ALGORITHM; DELIVERY PROBLEM; PICK-UP; TRANSPORTATION; HEURISTICS	A number of technological improvements have prompted a great concern on 'dynamism' in vehicle routing problems (VRP). In real-world applications, the dynamic information happens simultaneously with the plan being carried out. In order to effectively solve dynamic VRP (DVRP), many optimization strategies have been introduced in the literature. A new variant of vehicle routing problem is proposed which combines DVRP with time windows and capacity constraints, named capacitated DVRP with time windows (CDVRPTW). Apart from the traditional way of handling the problem, this paper proposes a novel strategy that incorporates improved firefly algorithm (IFA) into the framework of spiking neural P (SN P) systems, named spiking neural firefly optimization (SFO). A mathematical model of the problem is formulated, and the solution scheme is designed by associating a number of SN P systems that work in parallel to find optimal solutions in a reasonable time. Additionally, the parameters in the IFA are optimized by adjusting the rule probabilities using SN P systems. Being a NP-hard problem with real-world applications, the benefits of this study are far-reaching. The proposed scheme has been tested on benchmark instances and proved novelty, feasibility, and potentiality of the system.																	0941-0643	1433-3058															10.1007/s00521-020-04983-8		MAY 2020											
J								Software defect prediction model based on LASSO-SVM	NEURAL COMPUTING & APPLICATIONS										Software defect prediction; Feature selection; Support vector machine; Cross-validation		A software defect report is a bug in the software system that developers and users submit to the software defect library during software development and maintenance. Managing a software defect report that is overwhelming is a challenging task. The traditional method is manual identification, which is time-consuming and laborious and delays the repair of important software defects. Based on the above background, the purpose of this paper is to study the software defect prediction (SDP) model based on LASSO-SVM. In this paper, the problem of poor prediction accuracy of most SDP models is proposed. A SDP model combining minimum absolute value compression and selection method and support vector machine algorithm is proposed. Firstly, the feature selection ability of the minimum absolute value compression and selection method is used to reduce the dimension of the original data set, and the data set not related to SDP is removed. Then, the optimal value of SVM is obtained by using the parameter optimization ability of cross-validation algorithm. Finally, the SDP is completed by the nonlinear computing ability of SVM. The accuracy of simulation results is 93.25% and 66.67%, recall rate is 78.04%, and f-metric is 72.72%. The results show that the proposed defect prediction model has higher prediction accuracy than the traditional defect prediction model, and the prediction speed is faster.																	0941-0643	1433-3058															10.1007/s00521-020-04960-1		MAY 2020											
J								Using Locality Preserving Projections to Improve the Performance of Kernel Clustering	NEURAL PROCESSING LETTERS										Nonlinear; Clustering; Kernel function; Locality preserving projections; Local structure	DIMENSIONALITY REDUCTION; SEGMENTATION	Many clustering methods may have poor performance when the data structure is complex (i.e., the data has an aspheric shape or non-linear relationship). Inspired by this view, we proposed a clustering model which combines kernel function and Locality Preserving Projections (LPP) together. Specifically, we map original data into the high-dimensional feature space according to the idea of kernel function. Secondly, it is feasible to explore the local structure of data in clustering tasks. LPP is used to preserve the original local structure information of data to improve the validity of the clustering model. Finally, some outliers are often included in real data, so we embedded sparse regularization items in the model to adjust feature weights and remove outliers. In addition, we design a simple iterative optimization method to solve the final objective function and show the convergence of the optimization method in the experimental part. The experimental analysis of ten public data sets showed that our proposed method has better efficiency and performance in clustering tasks than existing clustering methods.																	1370-4621	1573-773X															10.1007/s11063-020-10252-5		MAY 2020											
J								Optimized channel allocation in emerging mobile cellular networks	SOFT COMPUTING										ANFIS-DCA; Network optimization; Erlang-B model; Service quality	ALGORITHM	The task of optimizing service quality in wireless networks is a continuous research that requires the design of efficient channel allocation schemes. The problem is how limited channel resources can be maximally utilized, to guarantee seamless communication while maintaining excellent service quality. Whereas, fixed channel allocation (FCA) schemes treat new and handoff calls equally without preference to normally prioritized handoff calls; dynamic channel allocation (DCA) schemes accommodate users mobility in randomly changing network conditions. However, classical Erlang-B models are deficient and do not consider users mobility and dynamically changing traffic of the mobile network environment. A modified Erlang-B dynamic channel allocation (MEB-DCA) scheme is therefore introduced in this paper, for improved network performance. The MEB-DCA algorithm introduces a conditional threshold for handoff request assignment to ensure that communication systems do not unnecessarily prioritize handoff calls at the detriment of new calls. Deriving knowledge from imprecise network data is difficult when developing functional relationships between parameters, requiring advanced modeling techniques with cognitive experience. Soft computing techniques have been shown to handle this challenge given its ability to represent precisely, both data and expert knowledge. An adaptive neuro-fuzzy inference system-based dynamic channel allocation (ANFIS-DCA) framework was proposed to automate the learning of communication parameters for optimized channel allocation decisions. Network parameters considered were received signal strength indication impacted by user mobility, number of guard and general channels, carried traffic, and handoff blocking threshold. The performance of the proposed ANFIS-DCA model was found to outsmart the static FCA and back propagation neural network-based DCA (NN-DCA) schemes using mean square error and root mean square error as performance measures. Our approach can be effectively deployed to improve channel allocation, resource utilization, network capacity, and satisfy users experience.																	1432-7643	1433-7479				NOV	2020	24	21					16361	16382		10.1007/s00500-020-04947-z		MAY 2020											
J								Creating a road map for industry 4.0 by using an integrated fuzzy multicriteria decision-making methodology	SOFT COMPUTING										Industry 4; 0; Multicriteria decision making; Strategy selection; Interval-valued intuitionistic fuzzy sets; AHP; TOPSIS	TOPSIS METHOD; MODEL; RISK; SELECTION; TECHNOLOGIES; HEALTH; AHP	Industry 4.0 can be defined as a creative manufacturing concept which is the integration of up-to-date technologies such as wireless systems, cyberphysical systems, Internet of things, cloud computing, big data concept to increase flexibility, and speed in production systems. The concept also aims to transform the manufacturing industry into the next generation. Selection among appropriate strategies for transition to industry 4.0 is crucial and should be considered in a multidimensional perspective since the decision process involves many strategies with respect to multicriteria based on the judgments of multiexperts. In this paper, this critical decision has been considered as a multicriteria decision-making (MCDM) problem under the uncertainty and vagueness environments. To increase the applicability of the uncertain data for the proposed methodology, intuitionistic fuzzy sets have been adopted. In other words, an integrated fuzzy MCDM methodology consists of interval-valued intuitionistic fuzzy analytic hierarchy process and interval-valued intuitionistic fuzzy technique for order performance by similarity to ideal solution has been suggested to prioritize of transition strategies for industry 4.0. According to proposed approach, "Training and continuing professional development" is determined as the most important strategy during the transition process, while "Technology" and "Equipment and Tools" are specified as the most crucial main and sub-criterion, respectively. For the validation process, we also applied two distance-based methods on the same decision matrices as a comparative analysis. Both analyses' results yield that the proposed methodology is applicable and effective for the decision-making process. Besides, the results of the one-at-a-time sensitivity analyses based on the changes of main criteria weights confirm the sensitivity and flexibility of the proposed methodology. Finally, a road map for transition to industry 4.0 has been determined with respect to priorities of strategies based on the constructed context.																	1432-7643	1433-7479															10.1007/s00500-020-05041-0		MAY 2020											
J								TAILOR: time-aware facility location recommendation based on massive trajectories	KNOWLEDGE AND INFORMATION SYSTEMS										Facility location recommendation; Temporal influence; Approximation; Online algorithm; Trajectory data		In traditional facility location recommendations, the objective is to select the best locations which maximize the coverage or convenience of users. However, since users' behavioral habits are often influenced by time, the temporal impacts should not be neglected in recommendation. In this paper, we study the problem of time-aware facility location recommendation problem, taking the time factor into account. To solve this problem, we develop a framework, TAILOR, which incorporates the temporal influence, user-coverage, and user-convenience. Based on TAILOR, we derive a greedy algorithm with (1-1e)-approximation and an online algorithm with (14)-competitive ratio. Extensive experimental evaluation and two case studies demonstrate the efficiency and effectiveness of the proposed approaches.																	0219-1377	0219-3116				SEP	2020	62	9					3783	3810		10.1007/s10115-020-01477-w		MAY 2020											
J								Multi-criteria sorting decision making based on dominance and opposition relations with probabilistic linguistic information	FUZZY OPTIMIZATION AND DECISION MAKING										Multi-criteria sorting decision making; Dominance relation; Opposition relation; Probabilistic linguistic term set	TERM SETS	The probabilistic linguistic term set (PLTS) is a powerful tool for describing linguistic evaluations derived from expert teams and has adequate capability to identify preferences among different evaluations. Due to the practicability of PLTSs, probabilistic linguistic decision making problems have been widely investigated in recent years. However, no study on probabilistic linguistic outranking relations has been conducted. This study aims to explore effective processing for the complex two-dimension structure of PLTSs and formulate probabilistic linguistic dominance and opposition relations for multi-criteria sorting decision making. Linguistic scale functions, which can generate different semantics for linguistic variables under different decision making environments, are introduced to deal with the linguistic terms in PLTSs. In this way, the probabilistic linguistic dominance degree, concordance and discordance indices are defined by systematically comparing the probabilities of PLTSs. Then, two kinds of outranking relations with dominance and opposition for PLTSs are formulated based on the defined outranking indices. Subsequently, an innovative sorting decision making framework is constructed by exploring the outranking relations between alternatives and characteristic actions under multiple criteria and implementing the outranking aggregation and exploitation. Finally, this framework is demonstrated using an illustrative example with result analyses and comparison discussions.																	1568-4539	1573-2908															10.1007/s10700-020-09330-z		MAY 2020											
J								Online path planning of mobile robot using grasshopper algorithm in a dynamic and unknown environment	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Path planning; grasshopper optimisation algorithm; dynamic environment; mobile robot navigation; obstacle avoidance		The navigation of mobile robots using heuristic algorithms is one of the important issues in computer and control sciences. Path planning and obstacle avoidance are current topics of navigational challenges for mobile robots. The major drawbacks of conventional methods are the inability to plan motion in a dynamic and unknown environment, failure in crowded and complex environments, and inability to predict the velocity vector of obstacles and non-optimality of the synthesised path. This paper presents a novel path planning approach using a grasshopper algorithm for navigation of a mobile robot in dynamic and unknown environments. To accomplish this goal, two different approaches are presented. First, a sensory system is used to detect the obstacles and then a new method is developed to predict and avoid static and dynamic obstacles while the velocity of obstacles is unknown. The robot uses the obtained information and finds a collision-free, optimal and safe path. The controller proposed in this paper is tested in crowded and complex environments. Simulation results show that the approach is successful in all test environments. Also, the proposed controller is compared with several heuristic methods. The comparison work stipulates that the introduced controller here is promising in terms of running time, optimality, stability and failure rate.																	0952-813X	1362-3079															10.1080/0952813X.2020.1764631		MAY 2020											
J								A case-based reasoning recommender system for sustainable smart city development	AI & SOCIETY										AI in society; Case-based reasoning; Recommender systems; Sustainable society; Smart city dimensions	CITIES; ARCHITECTURE	With the deployment of information and communication technologies (ICTs) and the needs of data and information sharing within cities, smart city aims to provide value-added services to improve citizens' quality of life. But, currently city planners/developers are faced with inadequate contextual information on the dimensions of smart city required to achieve a sustainable society. Therefore, in achieving sustainable society, there is need for stakeholders to make strategic decisions on how to implement smart city initiatives. Besides, it is required to specify the smart city dimensions to be adopted in making cities smarter for sustainability attainment. But, only a few methods such as big data, internet of things, cloud computing, etc. have been employed to support smart city attainment. Thus, this study integrates case-based reasoning (CBR) as an artificial intelligence technique to develop a recommender system towards promoting smart city planning. CBR provides suggestions on smart city dimensions to be adopted by city planners/decision-makers in making cities smarter and sustainable. Accordingly, survey data were collected from 115 respondents to evaluate the applicability of the implemented CBR recommender system in relation to how the system provides best practice recommendations and retaining of smart city initiatives. Results from descriptive and exploratory factor analyses suggest that the developed system is applicable in supporting smart city adoption. Besides, findings from this study are expected to provide valuable insights for practitioners to develop more practical strategies and for researchers to better understand smart city dimensions.																	0951-5666	1435-5655															10.1007/s00146-020-00984-2		MAY 2020											
J								Building the summarization model of micro-blog topic	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Micro-blog topic summarization; ASN-MT; The lexical features; Confidence threshold; Optimal value	ASSOCIATION RULES; SET; EXTRACTION; FRAMEWORK; MINE	Publishing review comments of micro-blog hot topic is becoming the daily round in modern smart city. With the in-depth discussion about the micro-blog topic, it is a challenge for users to understand quickly the basic content of the topic due to its gradually complex inner relationship. To solve the problem, this paper presents the construction of micro-blog topic summarization. First, the keywords are extracted based on the lexical features of words occurred in comments published by users. And then the association rules between keywords are extracted by Apriori. Second, the associated semantic network for micro-blog topic (ASN-MT), the visualization of micro-blog topic summarization, is constructed according to the extracted keywords and association rules. Third, ASN-MT is optimized by reducing its scale based on the confidence threshold. And then its optimal value is selected according to the integrity of ASN-MT. The experimental results show that the proposed algorithm can construct accurately and quickly a concise and complete micro-blog topic summarization.																	1868-5137	1868-5145															10.1007/s12652-020-02078-9		MAY 2020											
J								Accurate classification of ECG arrhythmia using MOWPT enhanced fast compression deep learning networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electrocardiogram (ECG); Arrhythmia classification; Maximal overlap wavelet packets transform (MOWPT); Fast compression; Convolutional neural networks; Model parameter optimization	CONVOLUTIONAL NEURAL-NETWORK; WAVELET TRANSFORM; SIGNALS; DIAGNOSIS; FILTER; INTEGRATION; RECOGNITION	Accurate classification of electrocardiogram (ECG) signals is of significant importance for automatic diagnosis of heart diseases. In order to enable intelligent classification of arrhythmias with high accuracy, an accurate classification method based intelligent ECG classifier using the fast compression residual convolutional neural networks (FCResNet) is proposed. In the proposed method, the maximal overlap wavelet packet transform (MOWPT), which provides a comprehensive time-scale paving pattern and possesses the time-invariance property, was utilized for decomposing the original ECG signals into sub-signal samples of different scales. Subsequently, the samples of the five arrhythmia types were utilized as input to the FCResNet such that the ECG arrhythmia types were identified and classified. In the proposed FCResNet model, a fast down-sampling module and several residual block structural units were incorporated. The proposed deep learning classifier can substantially alleviate the problems of low computational efficiency, difficult convergence and model degradation. Parameter optimizations of the FCResNet were investigated via single-factor experiments. The datasets from MIT-BIH arrhythmia database were employed to test the performance of the proposed deep learning classifier. An averaged accuracy of 98.79% was achieved when the number of the wide-stride convolution in fast down-sampling module was set as 2, the batch size parameter was set as 20 and wavelet subspaces of low frequency bands in MOWPT were selected as input of the classifier. These analysis results were compared with those generated by some comparison methods to validate the superiorities and enhancements of the proposed method.																	1868-5137	1868-5145															10.1007/s12652-020-02110-y		MAY 2020											
J								Biomedical image compression using fuzzy transform and deterministic binary compressive sensing matrix	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sparsity; Compressive sensing; Binary matrix; Direct fuzzy transform; Inverse fuzzy transform; Orthogonal matching pursuit (OMP)	SIGNAL RECONSTRUCTION; ALGORITHM	Bio medical images are very important in analysing the internal structure of the body in a non-invasive method, to diagnose for any abnormalities and provide proper treatment to the patients. Medical images include X-rays, CT scan and MRI scan etc. It is necessary to compress these images, so that they can be easily stored and communicated from one place to another. The result of compression should be in such a way that, quality of the compressed image should be good and the compression ratio should be high. The biomedical images are mainly prone to impulse noise which may lead to degradation of image quality. So it is important to filter the impulse noise from the biomedical images without affecting the details of the image. In this paper the filtering of impulse noise from the biomedical images is done using fuzzy transform, followed by compressive sensing to compress the bio medical images without losing information content. Compressive sensing uses a sensing matrix to measure random samples from the image signal. This paper proposes the compression of biomedical images using a deterministic binary compressive sensing matrix and for the recovery of the biomedical image, Orthogonal Matching Pursuit (OMP) is used.																	1868-5137	1868-5145															10.1007/s12652-020-02103-x		MAY 2020											
J								Smart city oriented optimization of residential blocks on intensive urban sensing data based on fuzzy evaluation algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart city; Residential block; Intensive land use; Street vitality; Fuzzy evaluation algorithm		The unreasonable planning of land use in residential blocks affects the quality of the city and the comfort and convenience of urban residents. How to improve the vitality of the street from the source is an urgent problem to be solved in urban and rural planning disciplines. This paper selects 18 residential districts in Xi'an, and uses a multivariate linear stepwise regression to establish a relationship model between street vitality and intensive land use indices in three different periods of non-commuting, weekly commuting, and weekend. Using fuzzy evaluation techniques, the degree of land use intensification in different residential blocks was analyzed. Using the relational model and evaluation conclusions, the appropriate range of street vitality in residential blocks was determined, and the constructed model was used to obtain the suitable index range for intensive land use under street vitality. The results show that the street vitality and building density, the proportion of commercial space along the street and the red line width of the street are most affected during the non-commuting period during the week. The street vitality during the commute period is most closely related to the building density, building mix and street network density. Street dynamics are most closely related to building density, building mix, and street red line width during weekends. The conclusions of the study provide theoretical support and data support for the optimization and transformation of land use in residential blocks, and ultimately aim to stimulate street vitality and promote the realization of intensive land use in residential blocks.																	1868-5137	1868-5145															10.1007/s12652-020-02104-w		MAY 2020											
J								A Normative Approach to Artificial Moral Agency	MINDS AND MACHINES										Moral agency; Moral responsibility; Artificial intelligence; Artificial agency; Artificial moral agent; Machine ethics; Moral machine; Machine consciousness; Consciousness; Demarcation problem; Moral status	RESPONSIBILITY; ETHICS; ROBOTS; CONSCIOUSNESS; INTELLIGENT; SYSTEMS	This paper proposes a methodological redirection of the philosophical debate on artificial moral agency (AMA) in view of increasingly pressing practical needs due to technological development. This "normative approach" suggests abandoning theoretical discussions about what conditions may hold for moral agency and to what extent these may be met by artificial entities such as AI systems and robots. Instead, the debate should focus on how and to what extent such entities should be included in human practices normally assuming moral agency and responsibility of participants. The proposal is backed up by an analysis of the AMA debate, which is found to be overly caught in the opposition between so-called standard and functionalist conceptions of moral agency, conceptually confused and practically inert. Additionally, we outline some main themes of research in need of attention in light of the suggested normative approach to AMA.																	0924-6495	1572-8641				JUN	2020	30	2					195	218		10.1007/s11023-020-09525-8		MAY 2020											
J								A novel system for fast and accurate decisions of gold-stock markets in the short-term prediction	NEURAL COMPUTING & APPLICATIONS										Gold exchange markets; Short-term prediction; Filter selection method; Artificial neural network; Genetic algorithm	NEURAL-NETWORK; INDEX; EXCHANGE	The prediction of gold-stock price indices is considered a great challenge for traders, who try to analyze the historical developments and movements of gold-stock indices. Especially in the short-term prediction, that required a fast accurate decision in order to predict the next price after 5, 10, 15, 30 and 60 min. In this paper, a novel system is proposed to predict gold-stock prices in the short term for gold exchange markets. It is based on the wrapper selection method, artificial neural network and genetic algorithm which consist of three steps. The first step is to determine the number of the effective previous gold-stock indices and selecting the most significant technical parameters. Then, the prediction model construction will be trained by World gold council database. Finally, the next gold-stock price for each case of the short-term periods will be predicted. The results show that the proposed system achieves fast decisions with higher prediction accuracy and nuance rate between the actual and predicted values of the next gold-stock price after 5 min in the short term, which consumed 18 ms for the run time of the price prediction process.																	0941-0643	1433-3058															10.1007/s00521-020-05019-x		MAY 2020											
J								J-LDFR: joint low-level and deep neural network feature representations for pedestrian gender classification	NEURAL COMPUTING & APPLICATIONS										Handcrafted features; Deep learning; Joint feature representations; Visual surveillance applications	LOCAL BINARY PATTERNS; RECOGNITION; FUSION; HYBRID; ALGORITHM; OBJECT; AGE	Appearance-based gender classification is one of the key areas in pedestrian analysis, and it has many useful applications such as visual surveillance, predict demographics statistics, population prediction, and human-computer interaction. For pedestrian gender classification, traditional and deep convolutional neural network (CNN) approaches are employed individually. However, they are facing issues, for instance, discriminative feature representations, lower classification accuracy, and small sample size for model learning. To address these issues, this article proposes a framework that considers the combination of both traditional and deep CNN approaches for gender classification. To realize it, HOG- and LOMO-assisted low-level features are extracted to handle rotation, viewpoint and illumination variances in the images. Simultaneously, VGG19- and ResNet101-based standard deep CNN architectures are employed to acquire the deep features which are robust against pose variations. To avoid the ambiguous and unnecessary feature representations, the entropy-controlled features are picked from both low-level and deep representations of features that reduce the dimension of computed features. By merging the selected low-level features with deep features, we obtain a robust joint feature representation. The extensive experiments are conducted on PETA and MIT datasets, and computed results suggest that using the integration of both low-level and deep feature representations can improve the performance as compared to using these feature representations, individually. The proposed framework achieves AU-ROC of 96% and accuracy of 89.3% on the PETA dataset, and AU-ROC of 86% and accuracy of 82% on the MIT dataset. The experimental outcomes show that the proposed J-LDFR framework outperformed the existing gender classification methods.																	0941-0643	1433-3058															10.1007/s00521-020-05015-1		MAY 2020											
J								An improved CapsNet applied to recognition of 3D vertebral images	APPLIED INTELLIGENCE										CapsNet; RNN; CNN; 3D vertebral images; Vertebrae classification		Deep learning is currently widely applied in medical image processing and has achieved good results. However, recognizing vertebrae via image processing remains a challenging problem due to their complex spatial structures. CapsNet is a newly proposed network whose characteristics compensate for some shortcomings of traditional CNNs, and it has been shown to perform well on many tasks, including medical image recognition. In this paper, we applied a modified CapsNet to recognise 3D vertebral images by introducing an RNN module into CapsNet to further enhance its learning ability. This new network is called RNNinCaps, and it achieves the highest recognition performance on 3D vertebral images (the average accuracy of RNNinCaps exceeds the accuracy of the original CapsNet by 46.2% and that of a traditional CNN by 12.6%). RNNinCaps also performs better than several mainstream networks. RNNinCaps can promotes CapsNet's application in the field of 3D medical image recognition.																	0924-669X	1573-7497				OCT	2020	50	10					3276	3290		10.1007/s10489-020-01695-3		MAY 2020											
J								An Enhanced Extreme Learning Machine Based on Liu Regression	NEURAL PROCESSING LETTERS										Extreme learning machine; Neural networks; Liu estimator; Regression	RIDGE-REGRESSION	Extreme learning machine (ELM) is one of the most remarkable machine learning algorithm in consequence of superior properties particularly its speed. ELM algorithm tends to have some drawbacks like instability and poor generalization performance in the presence of perturbation and multicollinearity. This paper introduces a novel algorithm based on Liu regression estimator (L-ELM) to handle these drawbacks. Different selection approaches have been used to determine the appropriate Liu biasing parameter. The new algorithm is tested against the basic ELM, RR-ELM, AUR-ELM and OP-ELM on nine well-known benchmark data sets. Statistical significance tests have been carried out. Experimental results show that L-ELM for at least one Liu biasing parameter generally outperforms basic ELM, RR-ELM, AUR-ELM and OP-ELM in terms of stability and generalization performance with a little lost of speed. Conversely, the training time of L-ELM is generally much slower than RR-ELM, AUR-ELM and OP-ELM. Consequently, the proposed algorithm can be considered a powerful alternative to avoid the loss of performance in regression studies																	1370-4621	1573-773X				AUG	2020	52	1			SI		421	442		10.1007/s11063-020-10263-2		MAY 2020											
J								Conrad's F-condition for partially ordered monoids	SOFT COMPUTING										Lattice-ordered monoid; Riesz monoid; Pre-Riesz monoid; F-condition; Homogeneous element; Basis		In this paper, we study Conrad's F-condition for lattice-ordered monoids, Riesz monoids, and pre-Riesz monoids.																	1432-7643	1433-7479				JUL	2020	24	13					9375	9381		10.1007/s00500-020-04976-8		MAY 2020											
J								Bipolar N-soft set theory with applications	SOFT COMPUTING										Soft sets; N-soft sets; Bipolar N-soft sets; Operations of bipolar N-soft sets; Decision making	FUZZY; MODEL	In this paper, the notion of bipolar N-soft set, which is the bipolar extension of N-soft set, and its fundamental properties are introduced. This new idea is illustrated with real-life examples. Moreover, some useful operations and products on the bipolar N-soft sets are derived. We thoroughly discuss the idempotent, commutative, associative, and distributive laws for these emerging operations and products. Also, we set forth two outstanding algorithms to handle the decision-making problems under bipolar N-soft set environments. We give potential applications and comparison analysis to demonstrate the efficiency and advantages of algorithms.																	1432-7643	1433-7479				NOV	2020	24	22					16727	16743		10.1007/s00500-020-04968-8		MAY 2020											
J								Solving variable-order fractional differential algebraic equations via generalized fuzzy hyperbolic model with application in electric circuit modeling	SOFT COMPUTING										Variable-order fractional differential algebraic equations; Fuzzy systems; Generalized fuzzy hyperbolic model; Atangana-Baleanu derivative; Convergence	NUMERICAL-SOLUTIONS; SYSTEMS; DESIGN; IDENTIFICATION; ALGORITHM	In this paper, a new approach based on a generalized fuzzy hyperbolic model is used for the numerical solution of variable-order fractional differential algebraic equations. The fractional derivative is described in the Atangana-Baleanu sense that is a new derivative with fractional order based on the generalized Mittag-Leffler function. First, by using fuzzy solutions with adjustable parameters, the variable-order fractional differential algebraic equations are reduced to a problem consisting of solving a system of algebraic equations. For adjusting the parameters of fuzzy solutions, an unconstrained optimization problem is then considered. A learning algorithm is also presented for solving the unconstrained optimization problem. Finally, some numerical examples are given to verify the efficiency and accuracy of the proposed approach.																	1432-7643	1433-7479				NOV	2020	24	22					16745	16758		10.1007/s00500-020-04969-7		MAY 2020											
J								The effect of cutting tool material on chatter vibrations and statistical optimization in turning operations	SOFT COMPUTING										Chatter vibration; Stable cutting depths; Statistical optimization; Ceramic inserts	MACHINING PARAMETERS; STABILITY; PITCH; PREDICTION; SELECTION; DESIGN; ALGORITHM; FORCE	Machine tool vibrations consist of a self-stimulating mechanism during chip removal by machining operations. The system, which is a structural mode of the tool/workpiece, is initially stimulated by force. In turning operations, a wavy surface is formed on the workpiece because of both the previous revolution and structural vibrations. The maximum chip thickness may exponentially increase due to the phase shift between two consecutive waves, while the system oscillates at a chatter frequency very close to its structural mode. Variable growth in chip thickness increases vibrations, cutting forces and tool wear and causes a wavy surface. The purpose of the study is to compare different cutting tool inserts in terms of stable cutting depths. First, an experimental study was carried out and the stable cutting depths without chatter vibrations were determined in turning operations using various materials (AISI-1010, AISI-1050, Al-7075). Alumina inserts (Al2O3) were used in the study. Stable cutting depths were compared with the literature study that was performed using titanium carbide (TiC) inserts. A paired t test was used for the comparison. After the experimental study, a statistical/optimization study was performed to optimize stable cutting depths. It was observed that the chatter frequency was generally higher than the natural frequency of the cutting tool. Also, it was observed that the decrease in the number of revolutions, tool overhang lengths and yield strength of workpiece results in higher stable cutting depths. When the number of revolution is 125 rpm, the overhang length is 70 mm, and the yield strength is 124 MPa, the stable cutting depths maximize. In addition, stable cutting depths were higher when using Al2O3 cutting tool inserts and chatter vibrations were prevented. There was a significant difference between the two inserts (p < 0.05). It was found that the Al2O3 cutting tool inserts had better performances than TiC cutting inserts for the hard materials.																	1432-7643	1433-7479				NOV	2020	24	22					17319	17331		10.1007/s00500-020-05022-3		MAY 2020											
J								Identifying cancer-associated modules from microRNA co-expression networks: a multiobjective evolutionary approach	SOFT COMPUTING										MicroRNA; Multiobjective optimization; NSGA-II; Differentially co-expressed module; Semantic similarity; Gene ontology	TARGETING TRANSCRIPTION FACTORS; DIFFERENTIAL COEXPRESSION; ALGORITHM	MicroRNAs (miRNAs) are a class of very small noncoding RNA molecules. Although they are not directly involved in protein translation process, they indirectly regulate production of proteins by targeting different protein-coding genes or messenger RNAs (mRNAs). Several miRNAs are known to have crucial role in progression of different diseases in the human body such as cancer, diabetes, viral infection and cardiovascular diseases. Therefore, it is very important to understand the regulatory relationship among the genes and miRNAs in order to find the potential drug targets for these life-threatening diseases. In this article, a multiobjective miRNA module detection algorithm has been proposed to identify a group of miRNAs associated with several cancer types. This module detection algorithm optimizes two objective functions simultaneously. The first objective function is based on the change in miRNA co-expression pattern across the different phenotypic conditions, and the second objective function is based on the functional similarity within the miRNA pairs. Here, non-dominated sorting genetic algorithm-II (NSGA-II) has been utilized to optimize both the objective functions simultaneously so that differentially co-expressed miRNA modules having greater functional similarity can be detected. The superiority of the proposed technique is demonstrated by comparing its performance in identifying microRNA markers with that of the other existing module detection algorithms. Furthermore, the biological significance of the mRNA targets of the identified miRNA markers has been investigated.																	1432-7643	1433-7479				NOV	2020	24	22					17365	17376		10.1007/s00500-020-05025-0		MAY 2020											
J								Real power loss reduction by Duponchelia fovealis optimization and enriched squirrel search optimization algorithms	SOFT COMPUTING										Optimal reactive power; Transmission loss; Light source; Duponchelia fovealis optimization; Enriched squirrel search optimization	KRILL HERD ALGORITHM; SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; DISPATCH PROBLEM; ENERGY-STORAGE; COMBINED HEAT; FLOW MODEL	In this work, Duponchelia fovealis optimization (DFO) algorithm and enriched squirrel search optimization (ESSO) algorithm are designed to solve optimal reactive power problem. DFO algorithm is based on the natural progression of the Duponchelia fovealis. In the exploration space, Duponchelia fovealis population will act as search agent and the light source is considered as optimal places of Duponchelia fovealis which attained so far. Around the light source, each Duponchelia fovealis will explore and its position has been updated. Gaussian mutation, chaotic local search and Kernel extreme learning machine which are based on extreme learning machine are applied successively in order to perk up the performance of the algorithm. Then, in this work enriched squirrel search optimization (ESSO) algorithm is projected to solve the problem. Proposed algorithm is based on the actions of squirrel foraging behavior. Naturally, squirrels are very less active and consume the stored nuts in the winter time to get ample of energy. Hickory tree (hickory nuts are found), oak tree (acorn nuts are found) and normal tree are the three types of food sources for squirrel. Naturally, the behavior (foraging) will be varied with reference to the seasonal variations. Proposed Duponchelia fovealis optimization (DFO) algorithm and enriched squirrel search optimization (ESSO) algorithm have been tested in standard IEEE 30, bus test system. The results show that the projected DFO and ESSO algorithms reduced the power loss comprehensively. Mainly, projected Duponchelia fovealis optimization (DFO) algorithm and enriched squirrel search optimization (ESSO) algorithm solved the multi-objective formulation of the problem and with reference to power loss, voltage deviation minimization and voltage stability enhancement results have been analyzed.																	1432-7643	1433-7479															10.1007/s00500-020-05036-x		MAY 2020											
J								Novel classes of coverings based multigranulation fuzzy rough sets and corresponding applications to multiple attribute group decision-making	ARTIFICIAL INTELLIGENCE REVIEW										MGRS; Covering based (optimistic; pessimistic and variable precision) MGFRS; Fuzzy (complementary) beta-neighborhood; Multigranulation fuzzy (complementary) measure degree; MAGDM with fuzzy information	NEIGHBORHOOD OPERATORS; APPROXIMATION OPERATORS; AGGREGATION OPERATORS; GRANULATION; INFORMATION; MODELS	The notion of covering based multigranulation fuzzy rough set (CMGFRS) models is a generalization of both granular computing and covering based fuzzy rough sets. Therefore it has become a powerful tool for coping with vague and multigranular information in cognition. In this paper we introduce three kinds of CMGFRS models by means of fuzzy beta-neighborhoods and fuzzy complementary beta-neighborhoods, and we investigate their axiomatic properties. We investigate three respective types of coverings based CMGFRS models, namely, optimistic, pessimistic and variable precision setups. In particular, by using multigranulation fuzzy measure degrees and multigranulation fuzzy complementary measure degrees, we derive three types of coverings based gamma-optimistic (gamma-pessimistic) CMGFRSs and E (F, G)-optimistic and E (F, G)-pessimistic CMGFRSs, respectively. We discuss the interrelationships among these three types of CMGFRS models and covering based Zhan-CMGFRS models. In view of the theoretical analysis for these three types of CMGFRS models, we put forward a novel methodology to multiple attribute group decision-making problem with evaluation of fuzzy information. An effective example is fully developed, hence concluding the applicability of the proposed methodology.																	0269-2821	1573-7462				DEC	2020	53	8					6197	6256		10.1007/s10462-020-09846-1		MAY 2020											
J								A neural approach for detecting inline mathematical expressions from scientific documents	EXPERT SYSTEMS										deep learning; independent mathematical expression; inline mathematical expression; natural language processing; scientific text mining	IDENTIFICATION; EXTRACTION	Scientific documents generally contain multiple mathematical expressions in them. Detecting inline mathematical expressions are one of the most important and challenging tasks in scientific text mining. Recent works that detect inline mathematical expressions in scientific documents have looked at the problem from an image processing perspective. There is little work that has targeted the problem from NLP perspective. Towards this, we define a few features and applied Conditional Random Fields (CRF) to detect inline mathematical expressions in scientific documents. Apart from this feature based approach, we also propose a hybrid algorithm that combines Bidirectional Long Short Term Memory networks (Bi-LSTM) and feature-based approach for this task. Experimental results suggest that this proposed hybrid method outperforms several baselines in the literature and also individual methods in the hybrid approach.																	0266-4720	1468-0394															10.1111/exsy.12576		MAY 2020											
J								Filtration Simplification for Persistent Homology via Edge Contraction	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Topological data analysis; Persistent homology; Edge contraction	GRAPH MINORS	Persistent homology is a popular data analysis technique that is used to capture the changing homology of an indexed sequence of simplicial complexes. These changes are summarized in persistence diagrams. A natural problem is to contract edges in complexes in the initial sequence to obtain a sequence of simplified complexes while controlling the perturbation between the original and simplified persistence diagrams. This paper is an extended version of Dey and Slechta (in: Discrete geometry for computer imagery, Springer, New York, 2019), where we developed two contraction operators for the case where the initial sequence is a filtration. In addition to the content in the original version, this paper presents proofs relevant to the filtration case and develops contraction operators for towers and multiparameter filtrations.																	0924-9907	1573-7683				JUN	2020	62	5			SI		704	717		10.1007/s10851-020-00956-7		MAY 2020											
J								Development of a multi-objective artificial tree (MOAT) algorithm and its application in acoustic metamaterials	MEMETIC COMPUTING										Multi-objective optimization problems; Bio-inspired algorithm; Multi-objective artificial tree algorithm; Acoustic metamaterials	EVOLUTIONARY ALGORITHM; OPTIMIZATION	Although there are many algorithms that can solve the multi-objective optimization problems (MOPs) efficiently, each algorithm has its own disadvantages. The emergence of new algorithms is beneficial to make up the deficiencies of existing algorithms. Inspired by the organic matter transport process and the branch update theory of the banyan, this work proposed a new bio-inspired algorithm, named the multi-objective artificial tree (MOAT) algorithm to solve the MOPs. In MOAT, an improved crossover operator and an improved self-evolution operator are introduced to update solutions, a adaptive grid method is applied to manage the non-dominated solutions, and the strategy of variable number of branches in population is adopted to enhance the accuracy of this algorithm. Many typical test functions and seven well-known multi-objective algorithms, including MOEAD, NSGAII, MOPSO, GDE3, epsilon MOEA, IBEA and MPSO/D, are applied to study the accuracy and efficiency of MOAT. Experimental tests show that the results of MOAT are better than those of the seven algorithms, and the performance of MOAT is demonstrated. In addition, this new algorithm is also applied to solve the MOPs of two-dimensional acoustic metamaterials (AMs). The key parameters of AMs are optimized by MOAT to mitigate impact load and reduce structural mass, and the performance of these AMs is significantly improved.																	1865-9284	1865-9292				JUN	2020	12	2					165	184		10.1007/s12293-020-00302-9		MAY 2020											
J								Quantized Control for Synchronization of Delayed Fractional-Order Memristive Neural Networks	NEURAL PROCESSING LETTERS										Synchronization; Fractional-order systems; Memristive neural networks; Quantized control	FINITE-TIME SYNCHRONIZATION; STABILITY; STABILIZATION	This research addresses the synchronization of delayed fractional-order memristive neural networks (DFMNNs) via quantized control. The motivations are twofold: (1) the transmitted information may be constrained by limited bandwidths; (2) the existing analysis techniques are difficult to establish LMI-based synchronization criteria for DFMNNs within a networked control environment. To overcome these difficulties, the logarithmic quantization is adopted to design two types of energy-saving and cost-effective quantized controllers. Then, under the framework of sector bound approach, the closed-loop drive-response DFMNNs can be represented as an interval system with uncertain feedback gains. By utilizing appropriate fractional-order Lyapunov functional and some inequality techniques, two LMI-based synchronization criteria for DFMNNs are derived to establish the relationship between the feedback gain and the quantization parameter. Finally, two illustrative examples are presented to validate the effectiveness of the proposed control schemes.																	1370-4621	1573-773X				AUG	2020	52	1			SI		403	419		10.1007/s11063-020-10259-y		MAY 2020											
J								A Non-convex Optimization Model for Signal Recovery	NEURAL PROCESSING LETTERS										Compressive sensing; Multichannel EEG; Cosparsity; Low-rank property; Weighted schatten-p norm	EEG SIGNALS	The electroencephalogram (EEG) signal is one of the most frequently used biomedical signals. In order to accurately exploit the cosparsity and low-rank property which is nature in multichannel EEG signals, motivated by the fact that weighted schatten-p norm and lq norm can better approximate the matrix rank and l0norm, in this paper, a non-convex optimization model is proposed to precisely reconstruct the multichannel EEG signal. weighted schatten-p norm and lq norm are used to enforce low-rank property and cosparsity. In addition, an efficient iterative optimization method based on alternating direction method of multipliers is used to solve the resulting non-convex optimization problem. Experimental results have demonstrated that the proposed algorithm can significantly outperform existing state-of-the-art CS methods for compressive sensing of multichannel EEG signals.																	1370-4621	1573-773X															10.1007/s11063-020-10253-4		MAY 2020											
J								Applicability of a Single Depth Sensor in Real-Time 3D Clothes Simulation: Augmented Reality Virtual Dressing Room Using Kinect Sensor	ADVANCES IN HUMAN-COMPUTER INTERACTION												A busy lifestyle led people to buy readymade clothes from retail stores with or without fit-on, expecting a perfect match. The existing online cloth shopping systems are capable of providing only 2D images of the clothes, which does not lead to a perfect match for the individual user. To overcome this problem, the apparel industry conducts many studies to reduce the time gap between cloth selection and final purchase by introducing "virtual dressing rooms." This paper discusses the design and implementation of augmented reality "virtual dressing room" for real-time simulation of 3D clothes. The system is developed using a single Microsoft Kinect V2 sensor as the depth sensor, to obtain user body parameter measurements, including 3D measurements such as the circumferences of chest, waist, hip, thigh, and knee to develop a unique model for each user. The size category of the clothes is chosen based on the measurements of each customer. The Unity3D game engine was incorporated for overlaying 3D clothes virtually on the user in real time. The system is also equipped with gender identification and gesture controllers to select the cloth. The developed application successfully augmented the selected dress model with physics motions according to the physical movements made by the user, which provides a realistic fitting experience. The performance evaluation reveals that a single depth sensor can be applied in the real-time simulation of 3D cloth with less than 10% of the average measurement error.																	1687-5893	1687-5907				MAY 18	2020	2020								1314598	10.1155/2020/1314598													
J								Adaptive divergence for rapid adversarial optimization	PEERJ COMPUTER SCIENCE										Adversarial optimization; Black-box optimization; Computer simulations	NETWORKS	Adversarial Optimization provides a reliable, practical way to match two implicitly defined distributions, one of which is typically represented by a sample of real data, and the other is represented by a parameterized generator. Matching of the distributions is achieved by minimizing a divergence between these distribution, and estimation of the divergence involves a secondary optimization task, which, typically, requires training a model to discriminate between these distributions. The choice of the model has its tradeoff: high-capacity models provide good estimations of the divergence, but, generally, require large sample sizes to be properly trained. In contrast, low-capacity models tend to require fewer samples for training; however, they might provide biased estimations. Computational costs of Adversarial Optimization becomes significant when sampling from the generator is expensive. One of the practical examples of such settings is finetuning parameters of complex computer simulations. In this work, we introduce a novel family of divergences that enables faster optimization convergence measured by the number of samples drawn from the generator. The variation of the underlying discriminator model capacity during optimization leads to a significant speed-up. The proposed divergence family suggests using low-capacity models to compare distant distributions (typically, at early optimization steps), and the capacity gradually grows as the distributions become closer to each other. Thus, it allows for a significant acceleration of the initial stages of optimization. This acceleration was demonstrated on two fine-tuning problems involving Pythia event generator and two of the most popular black-box optimization algorithms: Bayesian Optimization and Variational Optimization. Experiments show that, given the same budget, adaptive divergences yield results up to an order of magnitude closer to the optimum than Jensen-Shannon divergence. While we consider physics-related simulations, adaptive divergences can be applied to any stochastic simulation.																	2376-5992					MAY 18	2020									e274	10.7717/peerj-cs.274													
J								Influence of tweets and diversification on serendipitous research paper recommender systems	PEERJ COMPUTER SCIENCE										Recommender system; Experimental study; User study; Scholarly articles; Serendipity; Digital library		In recent years, a large body of literature has accumulated around the topic of research paper recommender systems. However, since most studies have focused on the variable of accuracy, they have overlooked the serendipity of recommendations, which is an important determinant of user satisfaction. Serendipity is concerned with the relevance and unexpectedness of recommendations, and so serendipitous items are considered those which positively surprise users. The purpose of this article was to examine two key research questions: firstly, whether a user's Tweets can assist in generating more serendipitous recommendations; and secondly, whether the diversification of a list of recommended items further improves serendipity. To investigate these issues, an online experiment was conducted in the domain of computer science with 22 subjects. As an evaluation metric, we use the serendipity score (SRDP), in which the unexpectedness of recommendations is inferred by using a primitive recommendation strategy. The results indicate that a user's Tweets do not improve serendipity, but they can reflect recent research interests and are typically heterogeneous. Contrastingly, diversification was found to lead to a greater number of serendipitous research paper recommendations.																	2376-5992					MAY 18	2020									e273	10.7717/peerj-cs.273													
J								A framework for the analysis and synthesis of Swarm Intelligence algorithms	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Swarm Intelligence; bio-inspired algorithms; natural computing; information processing; decision-making; social insects	ANT COLONY OPTIMIZATION; DIVISION-OF-LABOR; PHEROMONES; EVOLUTION; BEHAVIOR	Over the last decades, the number of Swarm Intelligence algorithms proposed in the literature has increased considerably. However, most algorithms do not follow an adequate scientific rigour neither the principles of Swarm Intelligence, reproducing similar computational procedures of many other approaches, but by means of a different metaphor. In this scenario, the aim of this paper is to propose a framework for the analysis and synthesis of Swarm Intelligence algorithms that can contribute to a structured understanding (analysis) and development (synthesis) of these algorithms. The main objective of the proposed framework is to guide the construction of Swarm Intelligence metaphors in a consistent and well-founded way, as well as to contribute with the analysis and development of new algorithms that had better encompass the features and abilities of insects' societies.																	0952-813X	1362-3079															10.1080/0952813X.2020.1764635		MAY 2020											
J								Fuzzy recurrence plot-based analysis of dynamic and static spiral tests of Parkinson's disease patients	NEURAL COMPUTING & APPLICATIONS										Parkinson's disease; Machine learning systems; Decision support systems; Handwritten dynamics; DST; SST	SPEECH SIGNALS; DIAGNOSIS; CLASSIFICATION	Parkinson's disease (PD) is a chronic and progressive neurological illness affecting millions of people in the world. The cure for PD is not available. Drug therapies can handle some symptoms of disease like reducing tremor. PD is diagnosed with decrease in dopamine concentrations in the brain by using clinical tests. Early detection of the disease is important for the treatment. In this study, dynamic spiral test (DST) and static spiral test (SST) of PD patients were analyzed with pre-trained deep learning algorithms for early detection of PD. Fuzzy recurrence plot (FRP) technique was used to convert time-series signals to grayscale texture images. Several time-series signals were tested to observe the performances. The deep learning algorithms were employed as classifiers and feature extractors. Drawing and signal types' performances for classifying PD were comprehensively investigated. In short, according to the experimental results Y signal produced the best results in DST approach and arithmetic combination of the Y and P signals performed better in SST method.																	0941-0643	1433-3058															10.1007/s00521-020-05014-2		MAY 2020											
J								Comparative analysis of time series model and machine testing systems for crime forecasting	NEURAL COMPUTING & APPLICATIONS										Crime in society; Crime forecasting; Crime rate; Time series model; Machine testing	PREDICTION	Crime forecasting has been one of the most complex challenges in law enforcement today, especially when an analysis tends to evaluate inferable and expanded crime rates, although a few methodologies for subsequent equivalents have been embraced before. In this work, we use a strategy for a time series model and machine testing systems for crime estimation. The paper centers on determining the quantity of crimes. Considering various experimental analyses, this investigation additionally features results obtained from a neural system that could be a significant alternative to machine learning and ordinary stochastic techniques. In this paper, we applied various techniques to forecast the number of possible crimes in the next 5 years. First, we used the existing machine learning techniques to predict the number of crimes. Second, we proposed two approaches, a modified autoregressive integrated moving average model and a modified artificial neural network model. The prime objective of this work is to compare the applicability of a univariate time series model against that of a variate time series model for crime forecasting. More than two million datasets are trained and tested. After rigorous experimental results and analysis are generated, the paper concludes that using a variate time series model yields better forecasting results than the predicted values from existing techniques. These results show that the proposed method outperforms existing methods.																	0941-0643	1433-3058															10.1007/s00521-020-04998-1		MAY 2020											
J								Forecasting tunnel geology, construction time and costs using machine learning methods	NEURAL COMPUTING & APPLICATIONS										Gaussian Process Regression; Support Vector Regression; Decision Tree; Tunneling	SUPPORT VECTOR REGRESSION; HARMONIC FOURIER MOMENTS; GAUSSIAN-PROCESSES; PREDICTION; MODEL; SETTLEMENT; NETWORKS; PROJECTS; SYSTEM	This research intends to use machine learning approaches to predict tunnel geology and its construction time and costs. For this purpose, the Gaussian Process Regression (GPR), Support Vector Regression (SVR), and Decision Tree (DT) have been utilized. An estimation of the geological conditions of the Garan road tunnel and its construction time and cost has been conducted. In addition, after constructing about 200 m from the inlet and outlet sides of the tunnel, using the field-observed data of these sectors in the tools, all the previously forecasted results were updated for unconstructed parts. Fivefold cross-validation has been applied to assess the performance of each model. The obtained models are used to predict construction time and cost in real scenarios, and the accuracy of each model was investigated through different statistical evaluation criteria. Finally, it turns out that all the models provide relatively high performance and reduce the uncertainties of tunnel geology. However, the GPR provides more accurate results compared to the SVR and DT tools. Thus, we recommend the GPR for the prediction of geology and construction time and costs in future levels of a tunnel.																	0941-0643	1433-3058															10.1007/s00521-020-05006-2		MAY 2020											
J								Institutions and other things: critical hermeneutics, postphenomenology and material engagement theory	AI & SOCIETY										Critical hermeneutics; Postphenomenology; Material engagement; Extended mind		Don Ihde and Lambros Malafouris (Philosophy and Technology 32:195-214, 2019) have argued that "we are homo faber not just because we make things but also because we are made by them." The emphasis falls on the idea that the things that we create, use, rely on-that is, those things with which we engage-have a recursive effect on human existence. We make things, but we also make arrangements, many of which are long-standing, material, social, normative, economic, institutional, and/or political, and many of which are supported by various technologies, including AI, more and more. Critical theorists, such as Habermas, have argued that we need a "depth" or critical hermeneutics (one that combines hermeneutical understanding with scientific explanation) to provide a full account of this kind of recursivity. For Habermas, the explanatory aspect of critical hermeneutics has been modeled on neo-Marxist and neo-Freudian theories. We propose a new critical hermeneutical approach that uses the tools of embodied cognitive science, affordance theory, material engagement theory, and the concept of the socially extended mind.																	0951-5666	1435-5655															10.1007/s00146-020-00987-z		MAY 2020											
J								IoT networks 3D deployment using hybrid many-objective optimization algorithms	JOURNAL OF HEURISTICS										IoT collection networks; 3D indoor redeployment; Experimental validation; Many-objective optimization; Preference incorporation; Dimensionality reduction	REDUCTION; DIVERSITY	When resolving many-objective problems, multi-objective optimization algorithms encounter several difficulties degrading their performances. These difficulties may concern the exponential execution time, the effectiveness of the mutation and recombination operators or finding the tradeoff between diversity and convergence. In this paper, the issue of 3D redeploying in indoor the connected objects (or nodes) in the Internet of Things collection networks (formerly known as wireless sensor nodes) is investigated. The aim is to determine the ideal locations of the objects to be added to enhance an initial deployment while satisfying antagonist objectives and constraints. In this regard, a first proposed contribution aim to introduce an hybrid model that includes many-objective optimization algorithms relying on decomposition (MOEA/D, MOEA/DD) and reference points (Two_Arch2, NSGA-III) while using two strategies for introducing the preferences (PI-EMO-PC) and the dimensionality reduction (MVU-PCA). This hybridization aims to combine the algorithms advantages for resolving the many-objective issues. The second contribution concerns prototyping and deploying real connected objects which allows assessing the performance of the proposed hybrid scheme on a real world environment. The obtained experimental and numerical results show the efficiency of the suggested hybridization scheme against the original algorithms.																	1381-1231	1572-9397				OCT	2020	26	5					663	709		10.1007/s10732-020-09445-x		MAY 2020											
J								Multiple time-series convolutional neural network for fault detection and diagnosis and empirical study in semiconductor manufacturing	JOURNAL OF INTELLIGENT MANUFACTURING										Fault detection and diagnosis; Time series classification; Deep learning; Convolutional neural network; Smart manufacturing	PRINCIPAL COMPONENT ANALYSIS; SENSOR DATA; IDENTIFICATION; CLASSIFICATION; EQUIPMENT; MODEL	The development of information technology and process technology have been enhanced the rapid changes in high-tech products and smart manufacturing, specifications become more sophisticated. Large amount of sensors are installed to record equipment condition during the manufacturing process. In particular, the characteristics of sensor data are temporal. Most the existing approaches for time series classification are not applicable to adaptively extract the effective feature from a large number of sensor data, accurately detect the fault, and provide the assignable cause for fault diagnosis. This study aims to propose a multiple time-series convolutional neural network (MTS-CNN) model for fault detection and diagnosis in semiconductor manufacturing. This study incorporates data augmentation with sliding window to generate amounts of subsequences and thus to enhance the diversity and avoid over-fitting. The key features of equipment sensor can be learned automatically through stacked convolution-pooling layers. The importance of each sensor is also identified through the diagnostic layer in the proposed MTS-CNN. An empirical study from a wafer fabrication was conducted to validate the proposed MTS-CNN and compare the performance among the other multivariate time series classification methods. The experimental results demonstrate that the MTS-CNN can accurately detect the fault wafers with high accuracy, recall and precision, and outperforms than other existing multivariate time series classification methods. Through the output value of the diagnostic layer in MTS-CNN, we can identify the relationship between each fault and different sensors and provider valuable information to associate the excursion for fault diagnosis.																	0956-5515	1572-8145															10.1007/s10845-020-01591-0		MAY 2020											
J								Bound smoothing based time series anomaly detection using multiple similarity measures	JOURNAL OF INTELLIGENT MANUFACTURING										Anomaly detection; Time series; Bound smoothing; Multiple similarity measures		Time series data is pervasive in many applications and the anomaly detection about it is important, which will provide the early warning of some unexpected patterns. In this paper, we propose a multiple similarity based anomalous subsequences detection method, which is unsupervised and domain knowledge free. Firstly, to improve the time efficiency, an anomaly candidates selection scheme is introduced based on the locality sensitive hashing (LSH), which considers a subsequence that does not collide with the others as a potential anomaly. However, if the raw time series is noisy and the anomaly is subtle, the performance of LSH will be degraded. In order to address this problem, we present a smoothing method to remove the noise and highlight the anomalous part in a time series, which can help to decrease the collision probability between an anomaly and the other subsequences. Secondly, we employ Pareto analysis to incorporate multiple similarity measures since there are different types of anomalies in real applications. It is unlikely that a single similarity measure can perform consistently well on different types of anomalies. Thirdly a new anomaly score scheme is provided to evaluate each anomaly candidate, which is based on the number of non-dominated vectors. Finally, we conduct extensive experiments on benchmark datasets from diverse domains and compare our method with the state-of-the-art approaches. The results show that our method can reach higher accuracy.																	0956-5515	1572-8145															10.1007/s10845-020-01583-0		MAY 2020											
J								Fast deep neural networks for image processing using posits and ARM scalable vector extension	JOURNAL OF REAL-TIME IMAGE PROCESSING										Deep neural networks (DNNs); Posit arithmetic; Scalable vector extension; Auto-vectorization; Real-time image processing; Autonomous driving		With the advent of image processing and computer vision for automotive under real-time constraints, the need for fast and architecture-optimized arithmetic operations is crucial. Alternative and efficient representations for real numbers are starting to be explored, and among them, the recently introduced positTM number system is highly promising. Furthermore, with the implementation of the architecture-specific mathematical library thoroughly targeting single-instruction multiple-data (SIMD) engines, the acceleration provided to deep neural networks framework is increasing. In this paper, we present the implementation of some core image processing operations exploiting the posit arithmetic and the ARM scalable vector extension SIMD engine. Moreover, we present applications of real-time image processing to the autonomous driving scenario, presenting benchmarks on the tinyDNN deep neural network (DNN) framework.																	1861-8200	1861-8219				JUN	2020	17	3					759	771		10.1007/s11554-020-00984-x		MAY 2020											
J								Picture fuzzy matrix and its application	SOFT COMPUTING										Picture fuzzy matrix; <theta, phi, psi >-cut of special restricted square picture fuzzy matrix; Determinant of square picture fuzzy matrix; Adjoint of square picture fuzzy matrix	DECISION-MAKING APPROACH	In this paper, the notions of picture fuzzy matrix, restricted picture fuzzy matrix and special restricted picture fuzzy matrix are established. Two types of <theta, phi, psi >-cut of special restricted square picture fuzzy matrix are introduced and corresponding properties are studied. Also, determinant and adjoint of square picture fuzzy matrix are established and some related properties are investigated. An application of picture fuzzy matrix in decision-making problem is presented here.																	1432-7643	1433-7479				JUL	2020	24	13					9413	9428		10.1007/s00500-020-05021-4		MAY 2020											
J								Markov frameworks and stock market decision making	SOFT COMPUTING										Rough set; Markov chain; Rough approximation framework	FUZZY IDEALS	In this paper, we present applications of Markov rough approximation framework (MRAF). The concept of MRAF is defined based on rough sets and Markov chains. MRAF is used to obtain the probability distribution function of various reference points in a rough approximation framework. We consider a set to be approximated together with its dynamacity and the effect of dynamacity on rough approximations is stated with the help of Markov chains. An extension to Pawlak's decision algorithm is presented, and it is used for predictions in a stock market environment. In addition, suitability of the algorithm is illustrated in a multi-criteria medical diagnosis problem. Finally, the definition of fuzzy tolerance relation is extended to higher dimensions using reference points and basic results are established.																	1432-7643	1433-7479				NOV	2020	24	21					16413	16424		10.1007/s00500-020-04950-4		MAY 2020											
J								A random-fuzzy portfolio selection DEA model using value-at-risk and conditional value-at-risk	SOFT COMPUTING										Value-at-risk; Conditional value-at-risk; Portfolio selection; Possibility measure; Necessity measure; Credibility measure; Random-fuzzy variable	DATA ENVELOPMENT ANALYSIS; OPTIMIZATION MODEL; RANDOM-VARIABLES; EXPECTED VALUE; EFFICIENCY; ALGORITHMS	The complexity involved in portfolio selection has resulted in the development of a large number of methods to support ambiguous financial decision making. We consider portfolio selection problems where returns from investment securities are random variables with fuzzy information and propose a data envelopment analysis model for portfolio selection with downside risk criteria associated with value-at-risk (V@R) and conditional value-at-risk (CV@R). Both V@R and CV@R criteria are used to define possibility, necessity, and credibility measures, which are formulated as stochastic nonlinear programming programs with random-fuzzy variables. Our constructed stochastic nonlinear programs for analyzing portfolio selection are transformed into deterministic nonlinear programs. Moreover, we show an enumeration algorithm can solve the model without any mathematical programs. Finally, we demonstrate the applicability of the proposed framework and the efficacy of the procedures with a numerical example.																	1432-7643	1433-7479				NOV	2020	24	22					17167	17186		10.1007/s00500-020-05010-7		MAY 2020											
J								On biased random walks, corrupted intervals, and learning under adversarial design	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Random walks; Classification noise; Adversarial learning	ALGORITHMS; BOUNDS	We tackle some fundamental problems in probability theory on corrupted random processes on the integer line. We analyze when a biased random walk is expected to reach its bottommost point and when intervals of integer points can be detected under a natural model of noise. We apply these results to problems in learning thresholds and intervals under a new model for learning under adversarial design.																	1012-2443	1573-7470				AUG	2020	88	8					887	905		10.1007/s10472-020-09696-1		MAY 2020											
J								Evaluating skills in hierarchical reinforcement learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Hierarchical reinforcement learning; Temporal abstraction; Option; Skill; Option evaluation		Despite the benefits mentioned in previous works of automatically acquiring skills for using them in hierarchical reinforcement learning algorithms such as solving the curse of dimensionality, improving exploration, and speeding up value propagation, they have not paid much attention to evaluating the effect of each skill on these factors. In this paper, we show that depending on the given task, a skill may be useful for learning it or not. In addition, the focus of the related work of automatically acquiring skills is on detecting subgoals, i.e., the skill termination condition, but there is not a precise method for extracting the initiation set of skills. In this paper, we propose not only two methods for evaluating skills but also two other methods for pruning the initiation set of them. Experimental results show significant improvements in learning different test domains after evaluating and pruning skills.																	1868-8071	1868-808X				OCT	2020	11	10					2407	2420		10.1007/s13042-020-01141-3		MAY 2020											
J								Robust metric learning based on the rescaled hinge loss	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Metric learning; Rescaled hinge loss; Robust algorithm; Label noise; Outlier; Half quadratic (HQ) optimization	DISTANCE; RECOGNITION; SIMILARITY	Distance/Similarity learning is a fundamental problem in machine learning. For example, kNN classifier or clustering methods are based on a distance/similarity measure. Metric learning algorithms enhance the efficiency of these methods by learning an optimal distance function from data. Most metric learning methods need training information in the form of pair or triplet sets. Nowadays, this training information often is obtained from the Internet via crowdsourcing methods. Therefore, this information may contain label noise or outliers leading to the poor performance of the learned metric. It is even possible that the learned metric functions perform worse than the general metrics such as Euclidean distance. To address this challenge, this paper presents a new robust metric learning method based on the Rescaled Hinge loss. This loss function is a general case of the popular Hinge loss and initially introduced in Xu et al. (Pattern Recogn 63:139-148, 2017) to develop a new robust SVM algorithm. In this paper, we formulate the metric learning problem using the Rescaled Hinge loss function and then develop an efficient algorithm based on HQ (Half-Quadratic) to solve the problem. Experimental results on a variety of both real and synthetic datasets confirm that our new robust algorithm considerably outperforms state-of-the-art metric learning methods in the presence of label noise and outliers.																	1868-8071	1868-808X				NOV	2020	11	11					2515	2528		10.1007/s13042-020-01137-z		MAY 2020											
J								Single and multi-objective optimal power flow using a new differential-based harmony search algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Differential-based harmony search algorithm; Voltage profile improvement; Power loss minimization; Active power generation minimization; Optimal power flow; Dh; best	PARTICLE SWARM OPTIMIZATION	This article proposes a new differential evolutionary-based approach to solve the optimal power flow (OPF) problem in power systems. The proposed approach employs a differential-based harmony search algorithm (DH/best) for optimal settings of OPF control variables. The proposed algorithm benefits from having a more effective initialization method and a better updating procedure in contrast with other algorithms. Here, real power losses minimization, voltage profile improvement, and active power generation minimization are considered as the objectives and formulated in the form of single-objective and multi-objective functions. For proving the performance of the proposed algorithm, comprehensive simulations have been performed by MATLAB software in which IEEE 118-bus and 57-bus systems are considered as the test systems. Besides, thorough comparisons have been performed between the proposed algorithm and other well-known algorithms like PSO, NSGAII, and Harmony search in three different load levels indicating the higher efficiency and robustness of the proposed algorithm in contrast with others.																	1868-5137	1868-5145															10.1007/s12652-020-02089-6		MAY 2020											
J								Research on the system function-structure analysis based on its implicit relation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent science; Factor space; Space fault tree; System function-structure; Implicit relation	RELIABILITY-ANALYSIS; SPACE	To understand the system function-structure under different background relations, two analysis methods are proposed based on space fault tree and factor space theory. They are the system function-structure minimal disjunctive formula and the system function-structure simplest formula. The former establishes the axiom system of system function-structure analysis based on factor logic, and proves that the system function-structure obtained by function-structure analysis method is a minimal disjunction formula. The latter gives a more detailed analysis steps based on the former idea, which is mainly used to determine the implicit relation between system function and component function. The background sets consisting of the 16 and 32 background relations are studied with these two methods. The results show that the complete background relations can obtain a certain system function-structure uniquely; the incomplete background relations can obtain a family of certain system function-structures. If the incomplete background relations are a subset of the complete background relations, then the incomplete background relations must has a linear relation between functions to complement the incomplete background relations. At the same time, the system function equivalent and replacement relation are obtained. It is also proved that the system function-structure obtained from several subsets of the background sets is the simplest and the condition required for the simplest structure of the background set is strict. At present, the above methods are mainly used for system reliability analysis in safety system engineering.																	1868-5137	1868-5145															10.1007/s12652-020-02058-z		MAY 2020											
J								Data-driven child behavior prediction system based on posture database for fall accident prevention in a daily living space	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart home; Injury prevention; Climbing behavior; Data-driven simulation; Configuration space; Rapidly exploring random tree	SERVICE	Ten thousand children are admitted to emergency rooms due to accidents every year in Tokyo. The most frequent accident is a fall accident. Fall accidents may occur when climbing to a high place in a daily living space. Since injury prevention by human supervision does not work well, the World Health Organization recommends an environmental modification approach as an effective preventive countermeasure to this problem. Predicting children's behavior is necessary in order to improve the environment. However, even for advanced human modeling technology, predicting where children can climb in everyday life situations remains difficult. In the present study, the authors developed a new method for predicting places that children can climb in a data-driven manner by integrating cameras, a behavior recognition system (OpenPose), and a climbing motion planning algorithm based on a rapidly exploring random tree. Thirty five children participated in an experiment to collect climbing posture data. A simulation is performed based on the posture database and allows us to visually understand how children climb up in daily living space. This makes it possible to improve to achieve a safe environment for children without the need for specialized knowledge, which is useful for parents, nursery teachers, nurses, etc. The present paper describes fundamental functions of the developed system and presents an evaluation of the feasibility of the prediction function.																	1868-5137	1868-5145															10.1007/s12652-020-02097-6		MAY 2020											
J								Cognition based spam mail text analysis using combined approach of deep neural network classifier and random forest	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Random forest; Deep neural network; Backpropagation algorithm; Support vector machine	NEGATIVE SELECTION ALGORITHM; LEARNING ALGORITHM; BELIEF NETWORKS; OPTIMIZATION	Email Spam is a variety of automated spam where unbidden messages, used for business purpose, sent extensively to multiple mailing lists, individuals or newsgroups. To build a fruitful system for spam detection, we introduced Random Forest integrated with Deep Neural network to find the classification accuracy. The Random Forest algorithm uses a preordained probability of attributes in constructing their decision trees. The Gini measure is examined to rank the important features. The main objective is to grade the features using RF algorithm and to train the data using Deep Neural Network Classifier. Deep Neural Network Classifier model (DNNs) are trained using backpropagation algorithm in batch learning mode, which requires the entire training data to learn at once. The detector process was dynamically fit to the new data patterns till it reaches the spam coverage. Experimental results shows that classification rate of DNN is higher than compared to KNN and Support Vector Machine(SVM) with an accuracy of 88.59% while considering the top ranked five features.																	1868-5137	1868-5145															10.1007/s12652-020-02087-8		MAY 2020											
J								DC-Gnet for detection of glaucoma in retinal fundus imaging	MACHINE VISION AND APPLICATIONS										Glaucoma; Optic disc; Optic cup; Deep neural network; CDR; DDLS; ISNT	OPTIC DISC; CUP SEGMENTATION; IMAGES; LOCALIZATION; CLASSIFICATION; EXTRACTION; DIAGNOSIS	Glaucoma is a retinal disease caused due to increased intraocular pressure in the eyes. It is the second most dominant cause of irreversible blindness after cataract, and if this remains undiagnosed, it may become the first common cause. Ophthalmologists use different comprehensive retinal examinations such as ophthalmoscopy, tonometry, perimetry, gonioscopy and pachymetry to diagnose glaucoma. But all these approaches are manual and time-consuming. Thus, a computer-aided diagnosis system may aid as an assistive measure for the initial screening of glaucoma for diagnosis purposes, thereby reducing the computational complexity. This paper presents a deep learning-based disc cup segmentation glaucoma network (DC-Gnet) for the extraction of structural features namely cup-to-disc ratio, disc damage likelihood scale and inferior superior nasal temporal regions for diagnosis of glaucoma. The proposed approach of segmentation has been tested on RIM-One and Drishti-GS dataset. Further, based on experimental analysis, the DC-Gnet is found to outperform U-net, Gnet and Deep-lab architectures.																	0932-8092	1432-1769				MAY 18	2020	31	5							34	10.1007/s00138-020-01085-2													
J								Soft dominance based multigranulation decision theoretic rough sets and their applications in conflict problems	ARTIFICIAL INTELLIGENCE REVIEW										Rough set; Soft set; Preference relation; Decision theoretic; Soft preference relation	FUZZY-SETS; PREFERENCE-RELATION; INFORMATION; MODEL; PRECISION; APPROXIMATIONS; PARAMETERS	The extension of rough set model is a crucial and vast research direction in rough set theory. Meanwhile decision making can be considered as a mental process in which human beings make a choice among several alternatives. However, with the increasing complexity of real decision making problems, the decision makers frequently face the challenge of characterizing their preferences in an uncertain context. In the present paper, we initiate a multi attribute group decision making problem in the presence of multi attribute and multi decision in decision making with preferences. We further present the concept of soft preference relation and soft dominance relation corresponding to decision attribute in the multi criteria and multi decision information system. Further we put forward the idea of two types of optimistic/pessimistic multigranulation (soft dominance based optimistic/pessimistic multigranulation decision theoretic) approximations and their applications in solving a multi agent conflict analysis decision problem. The proposed method addresses the limitations of the Pawlak model and Sun's conflict analysis model and thus improve these models. Finally, the results on labor management negotiation problems show that the proposed algorithms are more effective and efficient for feasible consensus strategy when compared with other techniques.																	0269-2821	1573-7462				DEC	2020	53	8					6079	6110		10.1007/s10462-020-09843-4		MAY 2020											
J								An effective co-evolutionary algorithm based on artificial bee colony and differential evolution for time series predicting optimization	COMPLEX & INTELLIGENT SYSTEMS										Artificial bee colony; Differential evolution; Extreme learning machine; Traffic flow prediction	EXTREME LEARNING-MACHINE; TRAFFIC FLOW PREDICTION; MODEL	Non-linear model optimization for predicting time series is a challenge problem. In Intelligent Transportation Systems (ITS) application, the indispensable short-term traffic flow prediction with big data makes the problem worst. To improve the prediction accuracy and ensure real-time performance in the big data environment, we propose a novel co-evolutionary artificial bee colony (ABC) improved by differential evolution (DE) optimization algorithm combined with a traffic flow predicting model trained by extreme learning machine (ELM) neural network. The proposed model can inherit the better generalization performance and the less training time consumption of the standard ELM, and can achieve a more balanced search strategy with the optimized weights and biases to overcome the random initialization deficiency of the typical ELM, and successfully obtain higher prediction accuracy compared with state-of-the-art methods. To verify the efficiency of the proposed model, we apply it to Lozi and Tent chaotic time series simulations and measured traffic flow time series experiments. Simulation and experimental results demonstrate that the proposed model has superior performance and competitive computational efficiency.																	2199-4536	2198-6053				JUL	2020	6	2					299	308		10.1007/s40747-020-00149-0		MAY 2020											
J								An improved multi-objective learning automata and its application in VLSI circuit design	MEMETIC COMPUTING										Multi-objective optimization; Improved learning automata; Integrated circuit design; Pareto-front	EVOLUTIONARY ALGORITHM; OPTIMIZATION	In this paper, an improved multi-objective optimization method, based on learning automata (called IMOLA), is proposed and its performance on the design of a variety of functional circuits is investigated. The most important feature of the proposed method is to provide a suitable schedule for effective compromise between exploration and exploitation during the search process. To evaluate the capability of the proposed method on multi-objective problems, digital and analog circuits have been selected. The results show the superiority in comparison with new and common algorithms called non-dominated sorting genetic algorithm III, multi-objective multi verse optimization, adaptive multi-objective black hole algorithm, multi-objective modified inclined planes system optimization, and multi-objective grasshopper optimization algorithm. Evaluation of the results was reported in terms of power-delay-product, power-area-product, success rate, Pareto-front, multi-objective criteria, circuit variables, design constraints, runtime, and performance analysis.																	1865-9284	1865-9292				JUN	2020	12	2					115	128		10.1007/s12293-020-00303-8		MAY 2020											
J								A two-level authentication scheme for clone node detection in smart cities using Internet of things	COMPUTATIONAL INTELLIGENCE										authentication; clone node; clustering algorithm; Internet of things; security	DISTRIBUTED DETECTION; REPLICATION ATTACKS; SENSOR; SECURITY	Clone node attack in IoT sensor devices remains a grave security concern as it paves the way for sinkhole, wormhole, and selective forwarding attacks. In this paper, a two-level authentication scheme named Fingerprint-based Zero-Knowledge Authentication (FZKA) algorithm is proposed to improve the detection rate of clone node among the sensor devices. In the fingerprint generation phase, the base station calculates a distinct fingerprint value for each and every node in the network by gathering neighborhood information, represented in the form of superimposed s-disjunct code matrix. The calculated fingerprint is considered as a secret value and distributed to each cluster nodes for the process of authentication. The FZKA algorithm improves the cloned node detection accuracy with minimal detection time. The simulation results highlight the cloned node detection rate of the proposed scheme by a margin of 92.5% against the existing Exponential Smoothing Algorithm (ETS), Position Verification Method, and Message Verification and Passing algorithms.																	0824-7935	1467-8640				AUG	2020	36	3					1200	1220		10.1111/coin.12330		MAY 2020											
J								Machine learning approach to handle data-driven model for simulation and forecasting of the cone crusher output in the stone crushing plant	COMPUTATIONAL INTELLIGENCE										cone crushers; data-driven modeling; forecasting; production; simulation	OPTIMIZATION	Grinding and crushing of stones and other particles are associated with various significant applications. Different sectors have continuously evolved in this area. In the crushing industry, plants function under strict conditions, many of which involve grinding materials. Therefore, various factors are responsible for how the crushers perform. This research investigated the ability of the adaptive neuro fuzzy inference system (ANFIS) to simulate the effects of throw, eccentric speed, closed side setting, and the size of the particle on crusher output. The developed simulation model was adjusted and authenticated alongside the experimental data of the investigated parameters. The model's performance was computed by the use of several prediction criteria skills. The results of the study indicated that the developed ANFIS model could simulate the Cone crusher output and give a dependable forecast of the cumulative weight fraction. The researchers resolved that the model fostered was a suitable instrument for the onsite cone crusher assessment.																	0824-7935	1467-8640															10.1111/coin.12338		MAY 2020											
J								Enhancing Cartesian genetic programming through preferential selection of larger solutions	EVOLUTIONARY INTELLIGENCE										Evolvability; Robustness; Cartesian genetic programming; Boolean circuits; Regression problem; Even-parity problem	EVOLVABILITY; DEGENERACY; ROBUSTNESS; COMPLEXITY; REDUNDANCY; EVOLUTION	We demonstrate how the efficiency of Cartesian genetic programming methods can be enhanced through the preferential selection of phenotypically larger solutions among equally good solutions. The advantage is demonstrated in two qualitatively different problems: the eight-bit parity problems and the "Paige" regression problem. In both cases, the preferential selection of larger solutions provides an advantage in term of the performance and of speed, i.e. number of evaluations required to evolve optimal or high-quality solutions. Performance can be further enhanced by self-adapting the mutation rate through the one-fifth success rule. Finally, we demonstrate that, for problems like the Paige regression in which neutrality plays a smaller role, performance can be further improved by preferentially selecting larger solutions also among candidates with similar fitness.																	1864-5909	1864-5917															10.1007/s12065-020-00421-9		MAY 2020											
J								VREDI: virtual representation for a digital twin application in a work-center-level asset administration shell	JOURNAL OF INTELLIGENT MANUFACTURING										Asset description; Digital twin; Digital-twin-based technical functionality; Service-oriented architecture; Virtual representation; Work-center-level asset administration shell	REFERENCE MODEL; SERVICE; INTERNET; THINGS; SIMULATION; DESIGN	The asset administration shell (AAS) has a virtual representation as an asset description and technical functionality as a smart manufacturing service. A digital twin (DT) is an advanced virtual factory technology that has simulation as its core technical functionality, which it performs in the type and instance stages of the physical asset. For providing an efficient information object to the DT application, this paper proposes Virtual REpresentation for a DIgital twin application (VREDI): an asset description for the operation procedures of a work-center-level DT application. For the successful application of DT as a smart factory technology, VREDI is designed to meet four core technical requirements-DT definition, AAS property inheritance, improving the existing asset description, and supporting DT-based technical functionalities. Based on the analysis of the technical requirements, the elements of VREDI are derived and the reference relationships between them are designed. It is then possible to provide the required technical functionality using the VREDI header, and a detailed P4R structure and elements of the body are defined. VREDI is applied to the concept to support the main properties of the DT. It is designed to inherit the AAS properties for efficient information management and interoperability. The application of advanced concepts such as "type and instance" and supporting vertical integration and horizontal coordination overcomes the limitations of the existing asset descriptions. Additionally, VREDI designates elements for supporting six DT-based technical functionalities in the type and instance stages of the physical work center.																	0956-5515	1572-8145															10.1007/s10845-020-01586-x		MAY 2020											
J								Multi-step least squares support vector machine modeling approach for forecasting short-term electricity demand with application	NEURAL COMPUTING & APPLICATIONS										Sample entropy; Multi-objective sine cosine algorithm; Least squares support vector machine; Variational mode decomposition; Multi-step forecasting	ARTIFICIAL NEURAL-NETWORK; TIME-SERIES; MULTIOBJECTIVE OPTIMIZATION; APPROXIMATE ENTROPY; COMPLEXITY ANALYSIS; SAMPLE ENTROPY; PREDICTION; ALGORITHM; SYSTEM; CONSUMPTION	Electricity demand forecasting plays a crucial role in the operation of electrical power systems because it can provide management decisions related to load switching and power grid. Thus, there have been models developed to estimate the electricity demand. However, inaccurate demand forecasting may raise the operating cost of electric power sector, which means that it would waste considerable money. In this paper, a novel modeling framework was proposed for forecasting electricity demand. Sample entropy was developed to identify the nonlinearity and uncertainty in the original time series, after that redundant noise was removed through a decomposition technique. Besides, the most optimal modes of original series and the optimal input form of the model were determined by the feature selection method. Finally, electricity demand series can be conducted forecasting through least squares support vector machine tuned by multi-objective sine cosine optimization algorithm. The case studies of Australia demonstrated that the proposed framework can ensure high accuracy and strong stability. Thus, it can be considered as a useful tool for electricity demand forecasting.																	0941-0643	1433-3058															10.1007/s00521-020-04996-3		MAY 2020											
J								Determining fuzzy distance via coupled pair of operators in fuzzy metric space	SOFT COMPUTING										Best proximity point; Fixed point; Coupled fixed point; Common coupled fixed point; Fuzzy metric space	FIXED-POINT THEOREMS; PROXIMITY POINT; COMMON; CONTRACTIONS	In this present study, we introduce the new class of mappings called fuzzy proximally compatible mappings and we solve the common coupled global optimization problem of finding the fuzzy distance between two subsets of a fuzzy metric space for this class of non-self-fuzzy mappings. Further, we develop the notion CLRg property (CLRg common limit in the range of g) for non-self-fuzzy mappings, and by having this idea, we derive common global minimal solution to the fuzzy coupled fixed point equations F(x,y)=g(x)=x and F(y,x)=g(y)=y,where the pair (F, g) is proximally fuzzy weakly compatible mappings, without the assumption of continuity on g. Finally, we find a relation between, our extended notions, proximal fuzzy E.A property and proximal fuzzy CLRg property, and we find a unique solution to the common global optimization problem with the assumption of proximal fuzzy E.A property.																	1432-7643	1433-7479				JUL	2020	24	13					9403	9412		10.1007/s00500-020-05001-8		MAY 2020											
J								An efficient parameter estimation method for nonlinear high-order systems via surrogate modeling and cuckoo search	SOFT COMPUTING										Parameter estimation; Nonlinear high-order systems; Surrogate model; Cuckoo search algorithm	REDUCTION; STRATEGIES; EQUATION; STATE	This work developed an efficient parameter estimation method for nonlinear high-order systems using surrogate modeling and cuckoo search. Specifically, to address the heavy computational burden required for evaluating the candidate parameters, we utilized a low-dimensional surrogate model to approximate the original system. The surrogate model was constructed by employing the proper orthogonal decomposition and the discrete empirical interpolation method. Then, to obtain the parameters of the original system, we applied the cuckoo search algorithm to solve the optimization problem that was built on the surrogate model. The accuracy and efficiency of the proposed method were verified on two numerical experiments, dealing with the identification of parameters for the FitzHugh-Nagumo system and the predator-prey system. The results showed that our approach yields accurate results while significantly reducing the computational cost.																	1432-7643	1433-7479				NOV	2020	24	22					17065	17079		10.1007/s00500-020-04997-3		MAY 2020											
J								R-STDP Based Spiking Neural Network for Human Action Recognition	APPLIED ARTIFICIAL INTELLIGENCE											CLASSIFICATION; HISTOGRAMS; FEATURES; PATTERN	Video surveillance systems are omnipresent and automatic monitoring of human activities is gaining importance in highly secured environments. The proposed work explores the use of the bio-inspired third generation neural network called spiking neural network (SNN) in order to recognize the action sequences present in a video. The SNN used in this work carries the neural information in terms of timing of spikes rather than the shape of the spikes. The learning technique used herein is reward-modulated spike time-dependent plasticity (R-STDP). It is based on reinforcement learning that modulates or demodulates the synaptic weights depending on the reward or the punishment signal that it receives from the decision layer. The absence of gradient descent techniques and external classifiers makes the system computationally efficient and simple. Finally, the performance of the network is evaluated on the two benchmark datasets, viz., Weizmann and KTH datasets.																	0883-9514	1087-6545				JUL 28	2020	34	9					656	673		10.1080/08839514.2020.1765110		MAY 2020											
J								Hierarchical age estimation mechanism with adaBoost-based deep instance weighted fusion	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Age estimation; adaboost based deep instance weighted fusion mechanism (ADIWF); circulation iterative means clustering (CIMC); adaboost fusion	DISEASE; CLASSIFICATION; REGRESSION; TEETH	Age estimation can obtain biological age which is helpful for diagnosis of healthy status and disease. The current age estimation methods do not consider the deep relationships of instances, which limits the potential improvement of the age estimation performance. A hierarchical age estimation mechanism with adaboost-based deep instance weighted fusion is proposed to solve this problem. First, a circulation iterative means clustering (CIMC) algorithm is designed for constructing the hierarchical instance space (multiple-layer instance spaces) and obtain multiple trained base regression models. Second, an adaboost-based deep instance weighted fusion (ADIWF) mechanism is designed to fuse the results of the trained regression models. Several representative age-related datasets are used for verification of the proposed method. The experimental results show that the mean absolute error (MAE) can be decreased apparently, by 6.86% and 1.42% on the Heart and Diabetes Dataset, respectively. Besides, some factors that may influence the performance of the proposed mechanism are studied. In general, the proposed age estimation mechanism is effective. In addition, the mechanism is a kind of framework mechanism, so it can be used to construct different concrete age estimation algorithms, and is helpful for related studies.																	0952-813X	1362-3079															10.1080/0952813X.2020.1764633		MAY 2020											
J								A recurrent neural network for urban long-term traffic flow forecasting	APPLIED INTELLIGENCE										Learning long-term flows; Recurrent neural network; Weather information; Contextual information	REGRESSION; ALGORITHM; MODELS	This paper investigates the use of recurrent neural network to predict urban long-term traffic flows. A representation of the long-term flows with related weather and contextual information is first introduced. A recurrent neural network approach, named RNN-LF, is then proposed to predict the long-term of flows from multiple data sources. Moreover, a parallel implementation on GPU of the proposed solution is developed (GRNN-LF), which allows to boost the performance of RNN-LF. Several experiments have been carried out on real traffic flow including a small city (Odense, Denmark) and a very big city (Beijing). The results reveal that the sequential version (RNN-LF) is capable of dealing effectively with traffic of small cities. They also confirm the scalability of GRNN-LF compared to the most competitive GPU-based software tools when dealing with big traffic flow such as Beijing urban data.																	0924-669X	1573-7497				OCT	2020	50	10					3252	3265		10.1007/s10489-020-01716-1		MAY 2020											
J								Generalization of Dempster-Shafer theory: A complex mass function	APPLIED INTELLIGENCE										Generalized Dempster-Shafer evidence theory; Complex mass function; Complex basic belief assignment; Complex evidence theory; Complex number; Decision-making	EVIDENTIAL REASONING APPROACH; DECISION-MAKING; FUZZY NUMBERS; FAILURE MODE; BELIEF; UNCERTAINTY; SELECTION; ENTROPY; FRAMEWORK; SYSTEM	Dempster-Shafer evidence theory has been widely used in various fields of applications, because of the flexibility and effectiveness in modeling uncertainties without prior information. However, the existing evidence theory is insufficient to consider the situations where it has no capability to express the fluctuations of data at a given phase of time during their execution, and the uncertainty and imprecision which are inevitably involved in the data occur concurrently with changes to the phase or periodicity of the data. In this paper, therefore, a generalized Dempster-Shafer evidence theory is proposed. To be specific, a mass function in the generalized Dempster-Shafer evidence theory is modeled by a complex number, called as a complex basic belief assignment, which has more powerful ability to express uncertain information. Based on that, a generalized Dempster's combination rule is exploited. In contrast to the classical Dempster's combination rule, the condition in terms of the conflict coefficient between the evidences is released in the generalized Dempster's combination rule. Hence, it is more general and applicable than the classical Dempster's combination rule. When the complex mass function is degenerated from complex numbers to real numbers, the generalized Dempster's combination rule degenerates to the classical evidence theory under the condition that the conflict coefficient between the evidences is less than 1. In a word, this generalized Dempster-Shafer evidence theory provides a promising way to model and handle more uncertain information. Thanks to this advantage, an algorithm for decision-making is devised based on the generalized Dempster-Shafer evidence theory. Finally, an application in a medical diagnosis illustrates the efficiency and practicability of the proposed algorithm.																	0924-669X	1573-7497				OCT	2020	50	10					3266	3275		10.1007/s10489-019-01617-y		MAY 2020											
J								Selective attention to historical comparison or social comparison in the evolutionary iterated prisoner's dilemma game	ARTIFICIAL INTELLIGENCE REVIEW										Iterated prisoner's dilemma game; Evolution of cooperation; Selective attention; Historical comparison; Social comparison; Risk attitude adaptation	EMPIRICAL-EXAMINATION; PROSPECT-THEORY; RISK-TAKING; COOPERATION; PERFORMANCE; ASPIRATION; STRATEGIES; NETWORKS; RECIPROCITY; DYNAMICS	This paper investigates an evolutionary iterated prisoner's dilemma (IPD) model of multiple agents, in which agents interact in terms of the pair-wise IPD game while adapting their attitudes towards income stream risk. Specifically, agents will become more risk averse (or more risk seeking) if their game payoffs exceed (or fall below) their expectations. In particular, agents use their peers' average payoffs as expectations (social comparison) when their payoffs are lower than their peers' averages, but use their own historical payoffs as expectations (historical comparison) when their payoffs are higher than their peers' averages. Such selective attention to social comparison or historical comparison manifests a desire for continuous improvement of agents. Simulations are conducted to investigate the evolution of cooperation under the selective attention mechanism. Results indicate that agents can sustain a highly cooperative equilibrium when they consider selective attention in adjusting their risk attitudes. This holds true for both the well-mixed and the network-based games, even in the presence of uncertain game payoffs. The reason is that, selective attention can significantly induce agents to adhere to conditional cooperation as well as to identify uncertainty in payoffs, which enhances the risk-averse behavior of agents in the IPD game. As a result, high levels of cooperation can be attained.																	0269-2821	1573-7462				DEC	2020	53	8					6043	6078		10.1007/s10462-020-09842-5		MAY 2020											
J								Teammate-pattern-aware autonomy based on organizational self-design principles	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Human-robot coordination; Autonomy; Organizational design; Multiagent planning	COORDINATION	We describe an approach for constraining robot autonomy based on the robot's awareness of patterns of its human teammates' behaviors, rather than either ignoring its teammates (which is fast but dangerous) or inferring their plans (which is safer but slow). We explore the promise, and limitations, of this approach in a series of simulated problems where an unmanned ground vehicle and its human teammates must rapidly respond to a sudden context shift. Our results help us discern conditions under which a pattern-aware approach can be more effective than the alternatives, and our current efforts investigate how the manned-unmanned team can adopt biases to more readily establish such conditions that are more favorable to the pattern-aware approach.																	1387-2532	1573-7454				MAY 16	2020	34	2							39	10.1007/s10458-020-09462-x													
J								Multi-turn dialogue-oriented pretrained question generation model	COMPLEX & INTELLIGENT SYSTEMS										Multi-turn dialogue; Question generation; Pretrained language model; Text passage		In recent years, teaching machines to ask meaningful and coherent questions has attracted considerable attention in natural language processing. Question generation has found wide applications in areas such as education (testing knowledge) and chatbots (enhancing interaction). Following previous studies on conversational question generation, we propose a pretrained, encoder-decoder model that can incorporate the semantic information from both passage and hidden conversation representations. We adopt BERT as the encoder to combine external text and dialogue history, and we design a multi-head attention-based decoder to incorporate the semantic information from both text and hidden dialogue representations into the decoding process, thereby generating coherent questions. Experiments with conversational question generation and document-grounded dialogue response generation tasks indicate that the proposed model is superior to baseline models in terms of both standard metrics and human evaluations.																	2199-4536	2198-6053				OCT	2020	6	3					493	505		10.1007/s40747-020-00147-2		MAY 2020											
J								Topic discovery by spectral decomposition and clustering with coordinated global and local contexts	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Topic modeling; Spectral decomposition; Clustering; Global context; Local context		Topic modeling is an active research field due to its broad applications such as information retrieval, opinion extraction and authorship identification. It aims to discover topic structures from a collection of documents. Significant progress have been made by the latent dirichlet allocation (LDA) and its variants. However, the "bag-of-words" assumption is usually made for the whole document by conventional methods, which ignores the semantics of local context that play crucial roles in topic modeling and document understanding. In this paper, we propose a novel coordinated embedding topic model (CETM), which incorporates spectral decomposition and clustering technique by leveraging both global and local context information to discover topics. In particular, CETM learns coordinated embeddings by using spectral decomposition, capturing the word semantic relations effectively. To infer the topic distribution, we employ a clustering algorithm to capture semantic centroids of coordinated embeddings and derive a fast algorithm to obtain the topic structures. We conduct extensive experiments on three real-world datasets to evaluate the effectiveness of CETM. Quantitatively, compared to state-of-the-art topic modeling approaches, CETM achieves significantly better performance in terms of topic coherence and text classification. Qualitatively, CETM is able to learn more coherent topics and more accurate word distributions for each topic.																	1868-8071	1868-808X				NOV	2020	11	11					2475	2487		10.1007/s13042-020-01133-3		MAY 2020											
J								Improved resource allocation scheme for optimizing the performance of cell-edge users in LTE-A system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										LTE-A; UEs; QoS class identifier (QCI); Resource blocks (RBs); MCS; CA		Improved resource allocation scheme for optimizing the performance of cell edge users in Long Term Evolution-Advanced (LTE-A) system is proposed in this paper. The proposed algorithm optimally assigns and allocates Carrier Components (CCs), Radio Blocks (RBs) and Modulation and Coding Scheme (MCS) index for the users based on the Quality of Service (QoS) provision to meet the on demand service requests. The proposed algorithm divides the cell regions into cell centre and cell edge depending on the traffic load on the network. The available RBs are allocated to meet the on demands requests of the users among these two regions. RB usage ratio for optimizing the performance of the scheduling algorithm is used during resource allocation. As the RB usage ratio reaches more than 70%, throughput and data rates of the cell may not be reached in timely and reliable manner. The proposed algorithm takes RB usage ratio into consideration during scheduling to maintain minimum QoS provisions of the users for resource allocation. Simulation result shows that, performance of the cell edge users optimally met with minimum throughput and data rates for reliable communication in the LTE-A system.																	1868-5137	1868-5145															10.1007/s12652-020-02084-x		MAY 2020											
J								Content based satellite image retrieval system using fuzzy clustering	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Satellite; Clustering; Fuzzy logic	OBJECT DETECTION	Nowadays, Satellite image retrieval could be a huge issue to induce information for natural disaster management, military target detection, meteorology, urban designing, harm assessment and change detection, etc. Basis on the image substance, content based image retrieval extracts the images that are relevant to the user given query image from massive image databases. Most of the existing image retrieval methods are still incompetent of providing retrieval outcome with elevated retrieval accuracy and not as much of computational intricacy. This paper proposed Fuzzy multi-characteristic clustering technique to realize this goal that is based on Fuzzy logic and clustering. Fuzzy sets used to represent the vagueness occur in user query, similarity measure and image substance. Clustering is an unsupervised method of classification that provides a small amount of control to clustering and improves the clustering performance drastically. The tentative results reveal that our proposed method can achieve significant precision and recall rates with better computational efficiency.																	1868-5137	1868-5145															10.1007/s12652-020-02064-1		MAY 2020											
J								Single image super resolution via wavelet transform fusion and SRFeat network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Super resolution; Wavelet transform fusion; Generative adversarial network; Denoising		Image super resolution is a vital research topic in the field of computer vision. It aims to reconstruct high resolution images from low resolution images. Although the conventional image super resolution methods have achieved good performance and effect, there are still have some issues, e.g., the high-frequency details information is insufficient, and the reconstruction process will bring additional noise, and most basic interpolation techniques produce blurry results. To settle the problems mentioned above, we consider combining the deep learning method with the frequency domain fusion method. In this paper, a novel single image super resolution method based on SRFeat network and wavelet fusion is proposed. First, the training image is taken as the input of the backbone SRFeat network, then the generative adversarial network training is carried out. Then, the up-sampling is utilized to obtain the coarse super resolved image. Finally, the output image after the network training is combined with the up-sampling image of the low-resolution image by Wavelet fusion to obtain the final result. Without increasing the depth of the network and the redundant parameters, the proposed method can achieve better reconstruct result. The experimental results show that the proposed method can not only reduce the probability of image distortion, but recover the global information of the reconstructed image and remove the noise brought by the reconstruction process. The PSNR value of the proposed method is improved 0.3 dB, and the SSIM is improved 0.02.																	1868-5137	1868-5145															10.1007/s12652-020-02065-0		MAY 2020											
J								An energy-efficient fuzzy-based scheme for unequal multihop clustering in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster selection criteria; Competition radius; Energy balance; Fuzzy logic; Multihop method; Unequal clustering; Wireless sensor network	PARTICLE SWARM OPTIMIZATION; ALGORITHM; LOGIC; PROTOCOL	Currently, wireless sensor networks (WSNs) are providing practical solutions for various applications, including smart agriculture and healthcare, and have provided essential support by wirelessly connecting the numerous nodes or sensors that function in sensing systems needed for transmission to backends via multiple hops for data analysis. One key limitation of these sensors is the self-contained energy provided by the embedded battery due to their (tiny) size, (in) accessibility, and (low) cost constraints. Therefore, a key challenge is to efficiently control the energy consumption of the sensors, or in other words, to prolong the overall network lifetime of a large-scale sensor farm. Studies have worked toward optimizing energy in communication, and one promising approach focuses on clustering. In this approach, a cluster of sensors is formed, and its representatives, namely, a cluster head (CH) and cluster members (CMs), with the latter transmitting the sensing data within a short range to the CH. The CH then aggregates the data and forwards it to the base station (BS) using a multihop method. However, maintaining equal clustering regardless of key parameters such as distance and density potentially results in a shortened network lifetime. Thus, this study investigates the application of fuzzy logic (FL) to determine various parameters and membership functions and thereby obtain appropriate clustering criteria. We propose an FL-based clustering architecture consisting of four stages: competition radius (CR) determination, CH election, CM joining, and determination of selection criteria for the next CH (relaying). A performance analysis was conducted against state-of-the-art distributed clustering protocols, i.e., the multiobjective optimization fuzzy clustering algorithm (MOFCA), energy-efficient unequal clustering (EEUC), distributed unequal clustering using FL (DUCF), and the energy-aware unequal clustering fuzzy (EAUCF) scheme. The proposed method displayed promising performance in terms of network lifetime and energy usage.																	1868-5137	1868-5145															10.1007/s12652-020-02090-z		MAY 2020											
J								Fuzzy signal strength estimated Markov probabilistic graph for efficient handover and seamless data delivery in PAN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless network; Centralized mobile anchor node; Received signal strength; Fuzzy triangular membership function; Bearing angle; Markov graphical model; Adjacent mobile node	VERTICAL HANDOVER; MOBILITY; OPTIMIZATION	Seamless mobility management is an ability of the system to support the different services in personal area networks. A mobility management system is effectively designed for seamless mobile communication through the handover process. The handover is to transfer the data efficiently from one base station to another without any link failure. In order to improve the seamless data delivery with less handover delay, Fuzzy Signal Strength Estimation based on Stochastic Markov Graphical Model (FSSE-SMGM) is introduced in PAN. The PAN includes a number of mobile nodes. The mobile nodes are clustered in a more dynamic manner based on their communication range. Each cluster has a unique base station. When a mobile node moves out of its communication range, the centralized anchor node computes the Received Signal Strength (RSS) of the mobile nodes from the base station using two ray ground model. The model predicts the path losses between transmitting antenna and receiving antenna when they are in line of sight. FSSE-SMGM uses the fuzzy triangular membership function to evaluate the RSS with the threshold value. In addition, the direction angle's degree of each mobile node from the current position towards the available base station is computed. Based on the signal strength and direction angle, the centralized anchor node switches the mobile node to the best available base station. Followed by, greater signaling cost is achieved during randomness nature over seamless mobility. After that, Stochastic Markov Graphical Model is used in FSSE-SMGM to improve the seamless data delivery through adjacent mobile nodes using state transition probability. The nodes with minimum distance are formed a chain with Markov property. This in turn minimizes the packet loss and ensures seamless data delivery between the mobile nodes. The simulations of proposed FSSE-SMGM and existing methods are carried out in terms of handover delay, seamless data delivery rate and data packet loss rate with respect to a number of data packets, and mobile speed. The simulation result shows that FSSE-SMGM improves the seamless data delivery rate and minimizes the data packet loss as well as handover delay.																	1868-5137	1868-5145															10.1007/s12652-020-02034-7		MAY 2020											
J								Meta heuristic QoS based service composition for service computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Web service; Composition; Genetic; Tabu; Service computing; Quality of service		In Service Computing, Service selection plays a vital role in delivering an appropriate service to the end user based on the request. Service composition methodology is the major factor affecting the appropriate need for the user or the consumer. The proposed technique involve the use of hybrid meta heuristics genetic algorithm with tabu search to retrieve the best suitable web service to the end user based on the Quality of service parameters. The existing techniques use the parameters response time, cost and reliability. The technique used in hybrid algorithm is used computes the availability of service, response time, throughput and interoperability between the services. Hence the result of service composition gives high reliable service to the end user with maximum throughput and interoperability. The location based service selection mechanism is considered for service composition as almost 90% of the services are available in cloud.																	1868-5137	1868-5145															10.1007/s12652-020-02083-y		MAY 2020											
J								Photo-realistic dehazing via contextual generative adversarial networks	MACHINE VISION AND APPLICATIONS										Generative adversarial networks; Dehazing; Image restoration; Contextual network	OPTIMIZATION; ALGORITHM	Single image dehazing is a challenging task due to its ambiguous nature. In this paper we present a new model based on generative adversarial networks (GANs) for single image dehazing, called as dehazing GAN. In contrast to estimating the transmission map and the atmospheric light separately as most existing deep learning methods, dehazing GAN restores the corresponding hazy-free image directly from a hazy image via a generative adversarial network. Extensive experimental results on both synthetic dataset and real-world images show our model outperforms the state-of-the-art algorithms.																	0932-8092	1432-1769				MAY 16	2020	31	5							33	10.1007/s00138-020-01082-5													
J								Conditioning optimization of extreme learning machine by multitask beetle antennae swarm algorithm	MEMETIC COMPUTING										Extreme learning machine (ELM); Conditioning optimization; Beetle antennae search (BAS); Heuristic algorithm	CLASSIFICATION; RECOGNITION; COLONY	Extreme learning machine (ELM) as a simple and rapid neural network has been shown its good performance in various areas. Different from the general single hidden layer feedforward neural network (SLFN), the input weights and biases in hidden layer of ELM are generated randomly, so that it only takes a little computational overhead to train the model. However, the strategy of selecting input weights and biases at random may result in ill-conditioned problems. Aiming to optimize the conditioning of ELM, we propose an effective particle swarm heuristic algorithm called Multitask Beetle Antennae Swarm Algorithm (MBAS), which is inspired by the structures of artificial bee colony (ABC) algorithm and Beetle Antennae Search (BAS) algorithm. Then, the proposed MBAS is applied for optimizing the input weights and biases of ELM to solve its ill-conditioned problems. Experiment results show that the proposed method is capable of simultaneously reducing the condition number and regression error, and achieving good generalization performance.																	1865-9284	1865-9292				JUN	2020	12	2					151	164		10.1007/s12293-020-00301-w		MAY 2020											
J								A fused contextual color image thresholding using cuttlefish algorithm	NEURAL COMPUTING & APPLICATIONS										Image fusion; Local contrast; Masi entropy; Energy curve; Cuttlefish algorithm; Multi-level image segmentation	CUCKOO SEARCH ALGORITHM; OPTIMIZATION ALGORITHM; SEGMENTATION; ENTROPY; TSALLIS; RENYI	In this paper, we have proposed a fusion-based context-sensitive Masi energy curve model for multi-level thresholding exploiting cuttlefish algorithm (CFA). The proposed algorithm is simple and very efficient for the task of color image segmentation. Although Masi entropy exploits the additive/non-extensive information with the aid of a concordant entropic parameter, the performance is observed to be poor in the case of color image segmentation. Improved results can be obtained by using the concept of energy curve with Masi entropy at the cost of increased computational cost while selecting the suitable thresholds. To overcome the aforementioned drawbacks as well as to increase the quality of the segmented image, a simple multi-level thresholding method is proposed in this paper. The proposed color image segmentation scheme exploits the concept of local contrast fusion along with CFA to resolve the aforementioned issues. In order to prove the effectiveness of the proposed scheme, experimental evaluations on standard daily-life color images have been reported in this paper. The experimental outputs demonstrate that fusion-based multi-level thresholding is better than the existing dominant segmentation methods.																	0941-0643	1433-3058															10.1007/s00521-020-05013-3		MAY 2020											
J								Diagnosis of Alzheimer disease in MR brain images using optimization techniques	NEURAL COMPUTING & APPLICATIONS										Alzheimer disease; Clinical dementia rating (CDR); Deep learning; Hippocampus; MMSE score; Optimization technique	CUCKOO SEARCH ALGORITHM; SEGMENTATION; ATROPHY; HIPPOCAMPUS	Nature-inspired algorithms play a vital role in various applications, namely image processing, engineering, industrialized designs and business. Generally, these algorithms are inspired by the nature which is helpful in segmenting the brain internal regions, namely cerebrospinal fluid, grey matter HC, white matter, ventricle and so on. Segmentation of hippocampus (HC) is a very hectic process due to its anatomical structure of the brain. This work has been recommended for different optimization techniques such as lion optimization algorithm (LOA), genetic algorithm, BAT algorithm, particle swarm optimization and artificial bee colony optimization to segment HC region from the brain subregions. The comparison of these optimization methods has been evaluated, and it showed better performance in LOA due to its individualities of escaping from local optima. From the obtained results, it is witnessed that the LOA has ability to segment HC region with high accuracy of 95%. The LOA method showed the best classification accuracy compared to all other methods. Finally, the mini-mental state examination score validation has been attempted to reach the clinical targets as HC region is a major hallmarks for diagnosing AD. The overall process of the proposed work demonstrates the abnormalities in the brain natural history which provides the reliable and accurate indication to the clinician about AD progression.																	0941-0643	1433-3058															10.1007/s00521-020-04984-7		MAY 2020											
J								Research on tunnel engineering monitoring technology based on BPNN neural network and MARS machine learning regression algorithm	NEURAL COMPUTING & APPLICATIONS										BPNN; MARS; Regression algorithm; Tunnel engineering; Intelligent monitoring		Tunnel engineering is affected by a variety of factors, which results in large detection errors in tunnel engineering. In order to improve the monitoring effect of tunnel engineering, based on BPNN and MARS machine learning regression algorithm, this research constructs a tunnel engineering monitoring and prediction model. Moreover, the gray residual BP neural network designed in this study uses a series combination, and the residuals obtained from the gray model are used as the input data of the BP neural network, and the output of the combined model is used as the prediction result. By applying the monitoring data of the convergence of the upper surrounding of the tunnel surface section and deformation of the arch subsidence, it is verified that the proposed method based on the combined model of BPNN and MASR can predict and analyze the tunnel deformation monitoring data very well.																	0941-0643	1433-3058															10.1007/s00521-020-04988-3		MAY 2020											
J								Research on financial assets transaction prediction model based on LSTM neural network	NEURAL COMPUTING & APPLICATIONS										LSTM; Financial model; Transaction forecasting; Model analysis; Financial market	TIME-SERIES	In recent years, with the breakthrough of big data and deep learning technology in various fields, many scholars have begun to study the stock market time series by using deep learning technology. In the process of model training, the selection of training samples, model structure and optimization methods are often subjective. Therefore, studying these influencing factors is beneficial to provide scientific suggestions for the training of recurrent neural networks and is beneficial to improve the prediction accuracy of the model. In this paper, the LSTM deep neural network is used to model and predict the financial transaction data of Shanghai, and the three types of factors affecting the prediction accuracy of the model are systematically studied. Finally, a high-precision short-term prediction model of financial market time series based on LSTM deep neural network is constructed. In addition, this paper compares BP neural network, traditional RNN and RNN improved LSTM deep neural network. It proves that the LSTM deep neural network has higher prediction accuracy and can effectively predict the stock market time series.																	0941-0643	1433-3058															10.1007/s00521-020-04992-7		MAY 2020											
J								The mean chance conditional value at risk under interval type-2 intuitionistic fuzzy random environment	SOFT COMPUTING										Type-2 fuzzy theory; Interval type-2 intuitionistic fuzzy variable; Fuzzy random variable; Value at risk; Conditional value at risk	VALUE-AT-RISK; EXPECTED VALUE; VARIABLES; SETS	The interval type-2 intuitionistic fuzzy random variable is an extension of the intuitionistic fuzzy random variable such that it can be a effective tool to determine some high-uncertainty phenomena. In this paper, the interval type-2 intuitionistic fuzzy random variable is introduced for the first time, and then, a scalar expected value operator of interval type-2 intuitionistic fuzzy random variable is proposed. Moreover, the new concepts of mean chance value at risk and mean chance conditional value at risk are discussed for the interval type-2 intuitionistic fuzzy random variables which have application in uncertain optimization, like fuzzy inverse location problems. Finally, it is proven that mean chance value at risk and mean chance conditional value at risk fulfill the convex risk metric properties.																	1432-7643	1433-7479				JUL	2020	24	13					9361	9373		10.1007/s00500-020-04975-9		MAY 2020											
J								Multi-granular soft rough covering sets	SOFT COMPUTING										Soft rough covering; Multi-granularity; Rough set; Covering set	GROUP DECISION-MAKING; FUZZY-SETS	This paper presents a novel model that combines several interesting features in relation with rough sets, namely multi-granularity (which extends Pawlak's single-granular approach), soft rough sets (where the granulation structure is defined by soft sets), and coverings that induce rough sets. Optimistic and pessimistic multi-granular models are the outcome of this hybridization. Their properties, relationships, and links with existing models are thoroughly explored. Finally, an application of the model to multi-criteria group decision making is put forward. Examples and graphical discussions illustrate the performance of this criterion.																	1432-7643	1433-7479				JUL	2020	24	13					9391	9402		10.1007/s00500-020-04987-5		MAY 2020											
J								A new machine learning-based healthcare monitoring model for student's condition diagnosis in Internet of Things environment	SOFT COMPUTING										Internet of Things; Health monitoring system; Smart student care; Data mining; Support vector machine		Advancement in sensor technologies has resulted in rapid evolution of Internet of Things (IoT) applications for developing behavioral and physiological monitoring systems such as IoT-based student healthcare monitoring system. Nowadays, a growing number of students living alone scattered over wide geographical areas, and tracking their health function status is necessary. In this paper, an IoT-based student healthcare monitoring model is proposed to continuously check student vital signs and detect biological and behavioral changes via smart healthcare technologies. In this model, vital data are collected via IoT devices and data analysis is carried out through the machine learning methods for detecting the probable risks of student's physiological and behavioral changes. The experimental results reveal that the proposed model meets the efficiency and proper accuracy for detecting the students' condition. After evaluating the proposed model, the support vector machine has achieved the highest accuracy of 99.1% which is a promising result for our purpose. The results outperformed decision tree, random forest, and multilayer perceptron neural network algorithms as well.																	1432-7643	1433-7479				NOV	2020	24	22					17111	17121		10.1007/s00500-020-05003-6		MAY 2020											
J								A PSO-algorithm-based consensus model with the application to large-scale group decision-making	COMPLEX & INTELLIGENT SYSTEMS										Group decision-making (GDM); Pairwise comparison matrix (PCM ); Particle swarm optimization (PSO); Geometric consistency index (GCI); Emergency management	PARTICLE SWARM OPTIMIZATION; ANALYTIC HIERARCHY PROCESS; PREFERENCE RELATIONS; COMPARISON MATRIX; CONSISTENCY; INFORMATION; OPERATORS; ALLOCATION; WEIGHTS	Group decision-making (GDM) implies a process of extracting wisdom from a group of experts. In this study, a novel GDM model is proposed by applying the particle swarm optimization (PSO) algorithm to simulate the consensus process within a group of experts. It is assumed that the initial positions of decision-makers (DMs) are characterized by pairwise comparison matrices (PCMs). The minimum and maximum of the entries in the same locations of individual PCMs are supposed to be the constraints of DMs' opinions. The novelty comes with the construction of the optimization problem by considering the group consensus and the consistency degree of the collective PCM. The former is to minimize the distance between the collective PCM and each individual one. The latter is to make the collective PCM be acceptably consistent in virtue of the geometric consistency index. The fitness function used in the PSO algorithm is the linear combination of the two objectives. The proposed model is applied to solve a large-scale GDM problem arising in emergency management. Some comparisons with the existing methods reveal that the developed model has the advantages to decrease the order of an optimization problem and reach a fast yet effective solution.																	2199-4536	2198-6053				JUL	2020	6	2					287	298		10.1007/s40747-020-00144-5		MAY 2020											
J								Regression Neural Network segmentation approach with LIDC-IDRI for lung lesion	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Regression Neural Network; Lung lesion segmentation; Computed Tomography; Level set (LS); Skeleton graph cut; Toboggan Based Growing Automatic Segmentation	CHEST RADIOGRAPHS; NODULES	Segmenting precisely affected parts of the lungs from the output of CT (Computed Tomography) is critical in making inquiries on lung malignancy and can offer significant data for clinical conclusions. It plays a major and effective role in researches on lung diseases. The crux of the problem is developing automatic detection of lesion and segments them with perfect accuracy. Heterogeneity of lesion part makes segmentation a very difficult task. In TBGA (Toboggan Based Growing Automatic Segmentation Approach), the lack of degree of recognition results in difficulty in the boundary detection process during segmentation. To overcome the drawbacks, a Regression Neural Networks (RNN) Segmentation approach has been proposed in this paper. The degree of recognition is less in tissues which are associated to the neighboring lesion with pixels having same intensity. RNN provides a greater accuracy of recognition of the adjacent lesions with similar intensity when compared to other methods like Skeleton graph cut and Level set method. Segmentation is done based on the degree of recognition. So the RNN method proposed in this paper concentrates mainly on the precise detection of boundary for juxtapleural and juxtavascular lesions. The accuracy of segmenting lung parenchyma is a challenge in lesion segmentation. In RNN segmentation process, the result of parenchyma forms the basis of extracting lesion. RNN is a Learning Algorithm so the complexity of automatic lesion detection is avoided. RNN uses a trained set of data, so the resulting outcome is accurate.																	1868-5137	1868-5145															10.1007/s12652-020-02069-w		MAY 2020											
J								Uncertain vector autoregressive model with imprecise observations	SOFT COMPUTING										Uncertainty theory; Uncertain vector autoregressive model; Principle of least squares; Residual analysis; Confidence interval	TIME-SERIES	Prior uncertain autoregressive (UAR) model research has focused principally on a univariate time series. However, different variables tend to influence each other in reality. In order to fill this gap, this paper explores the interrelationships among different variables and proposes an exposition of uncertain vector autoregressive (UVAR) model. Furthermore, we choose the least squares principle to estimate the unknown parameters in the UVAR model and analyze the residual of disturbance term. Then, we present the point estimation and confidence interval of the variables in the next period. Finally, the empirical results show that essential improvements in forecasting can be obtained by adding relative variables.																	1432-7643	1433-7479				NOV	2020	24	22					17001	17007		10.1007/s00500-020-04991-9		MAY 2020											
J								From Near-Optimal Bayesian Integration to Neuromorphic Hardware: A Neural Network Model of Multisensory Integration	FRONTIERS IN NEUROROBOTICS										multisensory integration; spiking neural network; neural network; neuromorphic processing; Bayesian inference; audio-visual integration; computational modeling	CAT SUPERIOR COLLICULUS; UNISENSORY INTEGRATION; OUTPUT NEURONS; PROJECTIONS; CORTEX; LOCALIZATION; INFORMATION; ATTENTION; BEHAVIOR; FUSION	While interacting with the world our senses and nervous system are constantly challenged to identify the origin and coherence of sensory input signals of various intensities. This problem becomes apparent when stimuli from different modalities need to be combined, e.g., to find out whether an auditory stimulus and a visual stimulus belong to the same object. To cope with this problem, humans and most other animal species are equipped with complex neural circuits to enable fast and reliable combination of signals from various sensory organs. This multisensory integration starts in the brain stem to facilitate unconscious reflexes and continues on ascending pathways to cortical areas for further processing. To investigate the underlying mechanisms in detail, we developed a canonical neural network model for multisensory integration that resembles neurophysiological findings. For example, the model comprises multisensory integration neurons that receive excitatory and inhibitory inputs from unimodal auditory and visual neurons, respectively, as well as feedback from cortex. Such feedback projections facilitate multisensory response enhancement and lead to the commonly observed inverse effectiveness of neural activity in multisensory neurons. Two versions of the model are implemented, a rate-based neural network model for qualitative analysis and a variant that employs spiking neurons for deployment on a neuromorphic processing. This dual approach allows to create an evaluation environment with the ability to test model performances with real world inputs. As a platform for deployment we chose IBM's neurosynaptic chip TrueNorth. Behavioral studies in humans indicate that temporal and spatial offsets as well as reliability of stimuli are critical parameters for integrating signals from different modalities. The model reproduces such behavior in experiments with different sets of stimuli. In particular, model performance for stimuli with varying spatial offset is tested. In addition, we demonstrate that due to the emergent properties of network dynamics model performance is close to optimal Bayesian inference for integration of multimodal sensory signals. Furthermore, the implementation of the model on a neuromorphic processing chip enables a complete neuromorphic processing cascade from sensory perception to multisensory integration and the evaluation of model performance for real world inputs.																	1662-5218					MAY 15	2020	14								29	10.3389/fnbot.2020.00029													
J								Effective node selection technique towards sparse learning	APPLIED INTELLIGENCE										Optimization; Sparse learning; Node selection; Neuron activation; Information theory; Convolutional neural network		Neural networks are getting wider and deeper to achieve state-of-the-art results in various machine learning domains. Such networks result in complex structures, high model size, and computational costs. Moreover, these networks are failing to adapt to new data due to their isolation in the specific domain-target space. To tackle these issues, we propose a sparse learning method to train the existing network on new classes by selecting non-crucial parameters from the network. Sparse learning also manages to keep the performance of existing classes with no additional network structure and memory costs by employing an effective node selection technique, which analyzes and selects unimportant parameters by using information theory in the neuron distribution of the fully connected layers. Our method could learn up to 40% novel classes without notable loss in the accuracy of existing classes. Through experiments, we show how a sparse learning method competes with state-of-the-art methods in terms of accuracy and even surpasses the performance of related algorithms in terms of efficiency in memory, processing speed, and overall training time. Importantly, our method can be implemented in both small and large applications, and we justify this by using well-known networks such as LeNet, AlexNet, and VGG-16.																	0924-669X	1573-7497				OCT	2020	50	10					3239	3251		10.1007/s10489-020-01720-5		MAY 2020											
J								Smart-Green-Mult (SGM): overhear from topological kingpins in software defined wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy controller; IoT; SDN; Software defined wireless sensor networks; Topological Kingpins; WSN	MANAGEMENT; PROTOCOL	Wireless sensor networks (WSNs) are an important component of the Internet of Things (IoT). Each multicast group in a WSN consists of a set of multicast members. In a sparse network, multicast members belonging to the same group may not be very closely spaced. On the other hand, in a dense environment, more than one multicast member of a group may highly reside within downlink regions of the same router which may or may not be a multicast member. In that case, a multicast message can be delivered to more than one multicast member in a shot. This characteristic improves the status of a node from an ordinary router to a topological kingpin. If a good number of topological kingpins are included in a multicast tree, then a great amount of energy is saved increasing network throughput. The present article proposes one such energy efficient multicast scheme: Smart-Green-Mult (SGM), based on Software Defined Wireless Sensor Network (SD-WSN) framework. The network is divided into several zones. The shape of each zone is either circular or elliptical or polygonal. Each zone is under control of an Software Defined Network controller. SDN controller of each zone is aware of the topology of the zone and can compute energy efficient paths from any source to any destination inside or outside the zone. Overall this is a multicast protocol in SD-WSN.																	1868-5137	1868-5145															10.1007/s12652-020-01984-2		MAY 2020											
J								Design and implementation of hybrid integration of cognitive learning and chaotic countermeasures for side channel attacks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; Fixed encryption keys; Elliptical curve cryptography (ECC); Logistic maps; Chaotic countermeasures	POWER-ANALYSIS; MACHINE	Security in embedded systems is considered to be more important and needs to be a diagnosis for every minute. Also with the advent of the Internet of Things (IoT), security in the embedded system has reached its new peak of dimension. A Mathematically secure algorithm was formulated and runs on the cryptographic chips which are embedded in the systems, but secret keys can be at risk and even information can be retrieved by the prominent side-channel attacks. Fixed encryption keys, non-intelligent detection of side-channel attacks are some of the real-time challenges in an existing system of encryption. Following the limitations of existing systems, this research article focuses on the integration of powerful machine learning algorithms by retrieving the secret key information with countermeasures methodology using the chaotic logistic maps and includes the following contributions: (a) Preparation of Data Sets from the Power consumption traces captured from ARTIX-7 FPGA boards while running the Elliptical Curve Cryptography(ECC) on it (b) Implementation of High Speed and High Accurate Single feed-forward learning machines for the detection and classification of side-channel attacks (c) Design of Chaotic Countermeasures using 3-Dlogistic maps for attacked bits. The test_bed has been developed using the integration of FPGA along with Cortex-A57 architectures for experimentation of the proposed work and various evaluation parameters such as Accuracy, F-calls, Precision rates, sensitivity, and correlation co-efficient, entropy were calculated and analyzed. Moreover, the parameters of the proposed system which has been analyzed prove to outperform the other existing algorithms in terms of performance and detection.																	1868-5137	1868-5145															10.1007/s12652-020-02030-x		MAY 2020											
J								Using cloud computing technology to design and implementation of smart shop floor control system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Shop floor control system (SFCS); Smart manufacturing; Web services; Production process	X-ARCHITECTURE; ALGORITHMS	Smart manufacturing has evolved and become more automated, computerized and cloud computing. The current shop floor control system (SFCS) has been implemented for many years, and has gradually encountered many problems. It moves towards the flexible smart manufacturing owing to the multiplex customer demand. Therefore, the current SFCS has been customized in many versions, which makes the low maintenance efficiency and the version control difficult. This paper is using cloud computing as a platform of developing and integrating multiplex customer demand to establish the integrity smart system function, and using Team Server to centralize the program versions to solve version management. Besides, the maintenance efficiency is raised by using production parameter configuration of the service-oriented design. The cloud based method enables deploying to be performed in any place other than the production line, which also facilitates the promotion of maintenance efficiency. The subscription function offered by the reporting service provides reports at specific time. The result of this research is to develop the cloud-based smart SFCS, which can achieve the goal of integrating production process and the promotion of production efficiency. With the system report, cloud-based smart SFCS can support managers to make right decision to promote the product quality.																	1868-5137	1868-5145															10.1007/s12652-020-02040-9		MAY 2020											
J								Improved data transmission using Li-Fi technology for home automation application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Home automation; Li-fi communication; Local network management; Home gateway		In recent times, home automation is preferred to be an important addenda in modern homes for several purposes like safety, security etc. The home automation is a local network management for smaller enterprises and home, which are interconnected with tons of devices via wireless technologies. The major operation of the home automation application involves acquisition or rapid collection of data from the input devices. With increasing data transmission from the devices via local wireless network management, the overhead of data communication increases. Such conditions gets even worse in the low speed network. In order to ensure increased communication speed with low latency, the proposed method uses Li-Fi communication technology to transfer the data between the local wireless network management systems in home automation system. The proposed study develops a local Li-Fi network management framework with Li-Fi technology and its adaptability over the home automation network. The study hence improves operating speed of the network such that it achieves packet load balancing, energy efficiency and low latency.																	1868-5137	1868-5145															10.1007/s12652-020-02072-1		MAY 2020											
J								Neutrosophic Cognitive Maps (NCM) based feature selection approach for early leaf disease diagnosis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; Leaf disease; Neutrosophic cognitive maps; GLCM features; Neural network	CLASSIFICATION	Early diagnosis of leaf ailments is the most necessary and prominent way to increase agriculture production. In this paper, a computer-aided approach for classifying the ailments in plant leaf is proposed using the neutrosophic logic-based feature selection algorithm. Feature selection leads to better learning performance and lowers computational cost by choosing a small subset of features by eliminating noisy and redundant features thereby acting as a dimensionality reduction technique. Leaf disease classification is similar to other classification problems but varies significantly in the features that contribute to classification. In the proposed method, Neutrosophic Cognitive Maps (NCM) is used to select the best subsets from GLCM and statistical features that can effectively characterize the leaf ailments. Eight existing state-of-the-art feature selection techniques are compared with the proposed method in order to prove the ability of the proposed method on publicly available images from the PlantVillage repository. Further, the leaf diagnosis can be incorporated in a mobile computing system if needed using appropriate methods thereby enabling user-friendliness. The proposed feature selection method provides an overall classification accuracy of 99.8% while selecting just 11 features for leaf disease diagnosis																	1868-5137	1868-5145															10.1007/s12652-020-02070-3		MAY 2020											
J								Discovering the realistic paths towards the realization of patent valuation from technical perspectives: defense, implementation or transfer	NEURAL COMPUTING & APPLICATIONS										Neural network; Patent valuation; Path prediction	INDICATORS; INDEX	With the intense competition of global intellectual property, the number of authorized patents is increasing. However, the patent conversion rate is low and the patent valuation is hard. The realization of patent valuation faces some basic challenges including: (1) how to develop a patent valuation model in consideration of technical factors; (2) how to train/test the patent valuation model with the insufficient standard value data. To solve the above issues, we assume that the realization of patent valuation begins with selecting the realistic value-paths: defense, implementation or transfer. We explore a Bayesian neural network-based model to predict the paths toward the realization of patent valuation. In the model, a function-effect-based patent representation is proposed, from which some technical features are extracted. Given the patent features, we use Bayesian neural network to predict the value-paths toward the realization of patent valuation. The model is evaluated by precision, recall, F-measure. The results show our method can improve evaluation measurements significantly after the addition of technical features.																	0941-0643	1433-3058															10.1007/s00521-020-04964-x		MAY 2020											
J								A chaotic sequence-guided Harris hawks optimizer for data clustering	NEURAL COMPUTING & APPLICATIONS										Data mining; Data clustering; Harris hawks optimization; Metaheuristic	PARTICLE SWARM OPTIMIZATION; ALGORITHMS	Data clustering is one of the important techniques of data mining that is responsible for dividing N data objects into K clusters while minimizing the sum of intra-cluster distances and maximizing the sum of inter-cluster distances. Due to nonlinear objective function and complex search domain, optimization algorithms find difficulty during the search process. Recently, Harris hawks optimization (HHO) algorithm is proposed for solving global optimization problems. HHO has already proved its efficacy in solving a variety of complex problems. In this paper, a chaotic sequence-guided HHO (CHHO) has been proposed for data clustering. The performance of the proposed approach is compared against six state-of-the-art algorithms using 12 benchmark datasets of the UCI machine learning repository. Various comparative performance analysis and statistical tests have justified the effectiveness and competitiveness of the suggested approach.																	0941-0643	1433-3058															10.1007/s00521-020-04951-2		MAY 2020											
J								Pollution source intelligent location algorithm in water quality sensor networks	NEURAL COMPUTING & APPLICATIONS										Pollution source location; Water quality monitoring; Sensor networks; Poisson distribution; Simulation optimization; Genetic algorithm	CONTAMINANT SOURCE CHARACTERIZATION; SOURCE IDENTIFICATION; DISTRIBUTION-SYSTEMS; SWARM OPTIMIZATION; GENETIC ALGORITHM; LOCALIZATION; ENSEMBLE; MODELS	Water is the source of human life and water pollution is becoming more and more serious with the development of cities. The supervision and treatment of water resources have become a big problem of urban development. Water quality monitoring is not timely, flood warning is not timely is directly related to the livelihood of the people. And the development of smart water utilities can solve problems timely and accurately. By placing water quality sensors in the urban water supply network, real-time monitoring of water quality can be performed to prevent incidents of drinking water pollution. After an incident of drinking water pollution occurs, reverse locating the pollution source through the information detected by the water quality sensors represents a challenging problem because in the actual water supply network, the direction and speed of the water flow will change with the water demand of the residents, thus leading to uncertainty in this problem. In conventional studies of pollution source location problems, it is often assumed that the water demand is fixed. However, due to the variability of the water demand of residents, this problem is actually a dynamic change problem and thus can be considered as a dynamic optimization problem. In this study, a Poisson distribution model was used to simulate the change of water demand among urban residents. On this basis, we proposed an improved genetic algorithm to solve the pollution source location problem and implemented two different water supply networks to perform the simulation experiments, which could accurately locate the pollution sources. The simulation results were compared with the standard genetic algorithm to verify the accuracy and robustness of the proposed algorithm.																	0941-0643	1433-3058															10.1007/s00521-020-05000-8		MAY 2020											
J								PrivateDL: Privacy-preserving collaborative deep learning against leakage from gradient sharing	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										collaborative deep learning; gradient sharing; machine learning; privacy-preserving technique		Large-scale data training is vital to the generalization performance of deep learning (DL) models. However, collecting data directly is associated with increased risk of privacy disclosure, particularly in special fields such as healthcare, finance, and genomics. To protect training data privacy, collaborative deep learning (CDL) has been proposed to enable joint training from multiple data owners while providing reliable privacy guarantee. However, recent studies have shown that CDL is vulnerable to several attacks that could reveal sensitive information about the original training data. One of the most powerful attacks benefits from the leakage from gradient sharing during collaborative training process. In this study, we present a new CDL framework, PrivateDL, to effectively protect private training data against leakage from gradient sharing. Unlike conventional training process that trains on private data directly, PrivateDL allows effective transfer of relational knowledge from sensitive data to public data in a privacy-preserving way, and enables participants to jointly learn local models based on the public data with noise-preserving labels. This way, PrivateDL establishes a privacy gap between the local models and the private datasets, thereby ensuring privacy against the attacks launched to the local models through gradient sharing. Moreover, we propose a new algorithm called Distributed Aggregation Stochastic Gradient Descent, which is designed to improve the efficiency and accuracy of CDL, especially in the asynchronous training mode. Experimental results demonstrate that PrivateDL preserves data privacy with reasonable performance overhead.																	0884-8173	1098-111X				AUG	2020	35	8					1262	1279		10.1002/int.22241		MAY 2020											
J								Multi-view semi-supervised least squares twin support vector machines with manifold-preserving graph reduction	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-view semi-supervised learning; Least squares twin support vector machines; Semi-supervised learning; Manifold-preserving graph reduction	MAXIMUM-ENTROPY DISCRIMINATION	Multi-view semi-supervised support vector machines consider learning with multi-view unlabeled data to boost the learning performance. However, they have several defects. They need to solve the quadratic programming problem and the time complexity is quite high. Moreover, when a large number of multi-view unlabeled examples exist, it can generate more outliers and noisy examples and influence the performance. Therefore, in this paper, we propose two novel multi-view semi-supervised support vector machines called multi-view Laplacian least squares twin support vector machine and its improved version with the manifold-preserving graph reduction which can enhance the robustness of the algorithm. They can reduce the time complexity by changing the constraints to a series of equality constraints and lead to a pair of linear equations. The linear multi-view Laplacian least squares twin support vector machine and its improved version with manifold-preserving graph reduction are further generalized to the nonlinear case via the kernel trick. Experimental results demonstrate that our proposed methods are effective.																	1868-8071	1868-808X				NOV	2020	11	11					2489	2499		10.1007/s13042-020-01134-2		MAY 2020											
J								Spikes and Nets (S&N): A New Fast, Parallel Computing, Point Process Software for Multineuronal Discharge and Connectivity Analysis	NEURAL PROCESSING LETTERS										Spike trains; Discharge patterns; Functional connectivity; Spectral analysis; Large neuronal data files; Multi-electrode arrays; Time-dependent changes	SPINAL DORSAL-HORN; TRAIN ANALYSIS; FRACTAL ANALYSIS; PATTERNS; TOOLBOX; NEURONS; SYSTEM	S&N is a new multi-platform software for neuronal spike train analysis which offers a comprehensive set of methods to efficiently handle large numbers of neuronal spike train files with a user-friendly interface and automatic results archiving. Selection, grouping, archiving and results matching of point process sequential analysis of neuronal files is a complex and time-consuming task especially for multiple electrode array recordings. Relevant and useful software packages for spike train analysis are already available; however, the aim of this work was to develop an easy to use, fast, short learning curve, multi-platform and parallel computing software able to manage a large number of neuronal spike train files to detect discharge patterns, connectivity, and time-dependent changes. A set of the most used spike train methods to perform single and multi-neuronal discharge pattern recognition and functional connectivity analysis were implemented in an easy-to-use, standalone, Matlab-based software toolbox: spikes and nets (S&N). The methods included for single and multi-neuronal discharge pattern analysis are raster plot, interspike intervals distribution, multiparametric burst, auto-correlation, auto-spectral, fractal, poincare, and phases. For functional connectivity analysis, cross-correlation and joint interval scatter diagram were implemented. Additionally, time segmentation analysis is available to detect temporal changes for all methods. S&N efficiently handles large numbers of neuronal discharge files at once with fast and automatic archiving of both analytical and graphical results which makes it suitable for multi-electrode array data. S&N applies up to 11 different analytical methods, including automatic file segmentation for time-dependent changes detection, and generates publication quality graphs. The developed toolbox is multi-platform and reads universal spike train files with any temporal resolution, able to process also ECG, EEG or similar data files.																	1370-4621	1573-773X				AUG	2020	52	1			SI		385	402		10.1007/s11063-020-10242-7		MAY 2020											
J								GDTM: Graph-based Dynamic Topic Models	PROGRESS IN ARTIFICIAL INTELLIGENCE										Topic modeling; Dimensionality reduction; Distributional semantics; Language modeling; Graph partitioning		Dynamic Topic Modeling (DTM) is the ultimate solution for extracting topics from short texts generated in Online Social Networks (OSNs) like Twitter. It requires to be scalable and to be able to account for sparsity and dynamicity of short texts. Current solutions combine probabilistic mixture models like Dirichlet Multinomial or Pitman-Yor Process with approximate inference approaches like Gibbs Sampling and Stochastic Variational Inference to, respectively, account for dynamicity and scalability of DTM. However, these methods basically rely on weak probabilistic language models, which do not account for sparsity in short texts. In addition, their inference is based on iterative optimizations, which have scalability issues when it comes to DTM. We present GDTM, a single-pass graph-based DTM algorithm, to solve the problem. GDTM combines a context-rich and incremental feature representation method with graph partitioning to address scalability and dynamicity and uses a rich language model to account for sparsity. We run multiple experiments over a large-scale Twitter dataset to analyze the accuracy and scalability of GDTM and compare the results with four state-of-the-art models. In result, GDTM outperforms the best model by 11% on accuracy and performs by an order of magnitude faster while creating four times better topic quality over standard evaluation metrics.																	2192-6352	2192-6360				SEP	2020	9	3					195	207		10.1007/s13748-020-00206-2		MAY 2020											
J								Concurrent multiresponse multifactorial screening of an electrodialysis process of polluted wastewater using robust non-linear Taguchi profiling	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Wastewater; Electrodialysis; Multi-response optimization; Robust multi-factorial process profiling; Non-linear non-normal data; Data messiness	FACTORIAL-EXPERIMENTS; EXPERIMENTAL-DESIGN; OPTIMUM CONDITIONS; OPTIMIZATION; PURIFICATION; TECHNOLOGY; EXTRACTION; PARAMETERS; MEMBRANES; SCIENCE	Electrodialysis is an important chemical process that separates pollutants from wastewater pools to produce clean water for consumption and irrigation. Initial wastewater concentration of chemical elements always differs. Chemical components are strongly dependent on the efflux origin and treatment. To optimize an electrodialysis process is congruent to improved key water quality characteristics. To predict optimal electrodialysis performance there will always be a need to conduct a small number of structured experiments. This is because wastewater conditions are usually different in each situation thus requiring reliable evidence-based design decisions to be delivered timely and low-cost. We study a real example from crucial dessert wastewater operations that aim to supply clean water for irrigation. Several issues are scrutinized that are often overlooked when carrying out multiresponse multi-factorial statistical optimization in environmental screening. Programming fast-cycle trials with Taguchi-type factorial recipes reaps quick information for new development and improvement projects. But it also introduces phenomena such as saturation, unreplication and non-linearity that could undermine the optimization effort. The showcased paradigm uses popular Taguchi methods to organize a rapid and short round of trials in order to investigate the behavior of four electrodialysis controlling factors: 1) the dilute flow, 2) the cathode flow, 3) the anode flow and 4) the voltage. The three monitored water quality indices are: 1) the percentage of removed sodium cations, 2) the sodium adsorption ratio and 3) the sodium ratio. We discuss the intricacies that emerge from the synthetic type of the electrodialysis data: non-normality, non-linearity and messiness. We propose a robust and agile method to conduct the multi-response multi-factorial optimization for electrodialysis of polluted wastewater. It is based on super-ranking and distribution-free profiling. Comparison with other profiling methods is provided and main advantages are commented from a chemical engineering perspective.																	0169-7439	1873-3239				MAY 15	2020	200								103997	10.1016/j.chemolab.2020.103997													
J								Risk of false compliance/non-compliance decisions for sterility test due to false-negative and false-positive test results	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Conformity assessment; False decisions; False-negative; False-positive; Risk assessment; Sterility test	APPLICABILITY; UNCERTAINTY	The sterility conditions of pharmaceutical products should consider the manufacturing conditions and the result obtained from a sterility test. The statistical and analytical limitations of the sterility test may lead to false-negative or false-positive results. Experimental determination of the probability of false-negative and false-positive results requires a very large number of assays, which may be impracticable. Thus, the aim of this work was to estimate the risk of false decisions related to sterility test due to the false-negative or false-positive test results, using a theoretical approach. The probability of false-negative results was estimated by considering the expected level of contamination in the sample, the tested volume for each item, the total volume of each item, and the number of tested items. The probability of false-positive results due to environmental contamination was estimated by considering the microbial load per plate, the sampled air volume, the total volume of the safety cabinet, the number of air changes per hour, and the volume of the test tube used in the sterility test. A MS-Excel spreadsheet provided in supplementary material can be used to estimate the probability of false-negative or false-positive test results. The probabilities of false-negative and false-positive results considering the experimental conditions were found to be 0.0045% and 0.070%, respectively. The probabilities of false-negative or false-positive test results may be combined with previous knowledge about the probability of a non-sterile lot (Sterility Assurance Level - SAL) using the Bayesian approach. The Bayesian approach allows the risk of false compliance/non-compliance decisions to be estimated. Risks to consumer and producer are significantly affected by prior knowledge of the sterility assurance level.																	0169-7439	1873-3239				MAY 15	2020	200								104005	10.1016/j.chemolab.2020.104005													
J								ATR-MIR spectroscopy to predict commercial milk major components: A comparison between a handheld and a benchtop instrument	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										ATR-MIR; Milk; Milk major constituents; Fatty acids determination in milk; Proteins determination in milk; Carbohydrates determination in milk	RAPID-DETERMINATION; PLS-REGRESSION; RAW-MILK; PROTEIN; FTIR; FAT; QUALITY	There is a growing need of measurement technologies that can be used close to the sample source and optical spectroscopy is an excellent example of this genre of technology: from the lab to the field. This study investigates the possibility to quantify the major components and to detect the presence or absence of lactose in commercial milks with ATR-MIR spectroscopy. We explored the possibility to use a portable and economical ATR-MIR instrument, comparing the results with a benchtop system. Commercial milk samples from Italy, Switzerland and Spain were chosen covering the maximum range of variation for protein, carbohydrate and fat content. The analytical protocol was optimized to make it as fast and useable as possible for both instruments, from the sample pretreatment to the instrumental parameters. Multivariate calibration was used to correlate the recorded spectra to the content of the major milk components, while a classification was done in order to classify samples with or without lactose. A comparison was performed between the predictive capabilities of the models built with different data pretreatments, different variable selection methods and different validation systems to obtain the best results and to assure robust models.																	0169-7439	1873-3239				MAY 15	2020	200								103995	10.1016/j.chemolab.2020.103995													
J								Novel soft sensor development using echo state network integrated with singular value decomposition: Application to complex chemical processes	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Soft sensor; Echo state network; Singular value decomposition; Process industry; Complex chemical processes	EXTREME LEARNING-MACHINE; NEURAL-NETWORK; CLASSIFICATION; OPTIMIZATION; PERFORMANCE	It is of great importance to develop advanced soft sensors for ensuring the safety and stability of complex industrial processes. Unluckily, with the increasing scale of chemical processes, it becomes more and more demanding to develop soft sensor with high accuracy. In addition, most of industrial processes are dynamic. As a result, the soft sensors developed using static models cannot achieve acceptable performance. In order to handle this problem, the Echo state network (ESN) as a kind of recurrent neural network is selected. However, the output weights of ESN are calculated linearly. On one hand, the collinear in the reserve layer outputs may decrease the performance; on the other hand, the over-fining problem may occur. To enhance and improve the ESN performance, singular value decomposition based ESN (SVD-ESN) is presented. In the SVD-ESN method, the singular value decomposition instead of the traditional least square is adopted to calculate the weights between the output layer and the reserve layer. Through singular value analysis in the outputs of the reserve layer, appropriate defining parameters are selected to enhance the accuracy and ensure the computing speed. As a result, the collinearity and over-fining problem is solved; then the performance of ESN is enhanced. To test and validate the performance of SVD-ESN, the proposed SVD-ESN is developed as soft sensor for the High Density Polyethylene (HDPE) production process and Purified Terephthalic Acid (PTA) production process. Compared with the conventional ESN, Extreme Learning Machine (ELM), Dynamic Window based ELM (DW-ELM) and Long Short-Term Memory (LSTM), the simulation results show that the proposed SVD-ESN model obtains better performance in terms of prediction accuracy, which conforms that the proposed SVD-ESN can be used as an effective dynamic model for developing accurate soft sensors.																	0169-7439	1873-3239				MAY 15	2020	200								103981	10.1016/j.chemolab.2020.103981													
J								Quantitative models for detecting the presence of lead in turmeric using Raman spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Raman Spectroscopy; Lead; Heavy metals; PLSR; Herbal medicinal products	HEAVY-METALS; CHEMOMETRICS; INDUSTRIAL	The current study presents a novel methodology to quantify lead in Turmeric using Raman spectroscopy. In this study, Partial Least Squares Regression (PLSR) was used for the quantification of lead. For calibration purposes, different amounts of lead were added to Turmeric samples encompassing a concentration range between 4 and 25 mu g g(-1). Since lead does not show any Raman band, for the purposes of this study, a complex was formed, its solvent was evaporated and the complex solid samples were registered with a Raman instrument. Raman measurements were performed in two different modes, -diffuse reflectance and transmission-. The PLSR models developed from Raman spectra of two data acquisition modes were evaluated in order to determine the suitability of both acquisition modes for quantifying lead content. The results indicated that diffuse reflectance showed better performance in terms of accuracy and robustness with a bias of 0.55 mu g g(-1), a relative standard error of prediction (RSEP) of 8.5% and a correlation between the predicted and reference values (R-2) of 0.967. Despite the low lead concentration in the samples, the proposed model allows the quantification of the lead content in a fast and simple way.																	0169-7439	1873-3239				MAY 15	2020	200								103994	10.1016/j.chemolab.2020.103994													
J								CORAL: QSAR models of CB1 cannabinoid receptor inhibitors based on local and global SMILES attributes with the index of ideality of correlation and the correlation contradiction index	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										QSAR; SMILES; CORAL; CB1 receptor inhibitors; IIC; CCI	MONTE-CARLO METHOD; R(M)(2) METRICS; VALIDATION; DERIVATIVES; ANTAGONIST; DISCOVERY; CARCINOGENICITY; PARAMETERS; RIMONABANT; PREDICTION	Obesity has acquired notable attention due to its high occurrence and link with grievous health problems such as hypertension, diabetes and heart disease. It has been reported that the endocannabinoid system executes a pivotal part in the management of food absorption, fa augmentation, and energy balance. In the present manuscript, we report a detailed QSAR analysis for 165 CB1 cannabinoid receptor inhibitors employing the Monte Carlo optimization process incorporated within the CORAL software. Eight splits are made from the whole dataset and sixteen QSAR models are developed from these splits employing two target function TF1 (without index of ideality of correlation) and TF2 (with index of ideality of correlation). All the QSAR models developed with TF2 have better predictive potential than the models developed with TF1. The model built for split 5 using TF2 is the leading model due to the higher value of the determination coefficient of the validation set (R-valid(2) = 0.8518). The index of ideality of correlation (IIC) improves the statistical performance of CORAL-based QSAR-models and gives statistically robust predictive models of the investigated endpoint pIC(50). In the present manuscript, a novel criterion "Correlation Contradiction Index (CCI)" is also applied to know its predictive potential. The absolute value of CCI for calibration set is less when QSAR models are developed employing IIC. The promoters of increase and decrease endpoint pIC(50) are identified and these are applied to design seven new compounds. All the newly designed molecule were docked into in the active site of human cannabinoid receptor CB1 (PDB ID: 5tgz).																	0169-7439	1873-3239				MAY 15	2020	200								103982	10.1016/j.chemolab.2020.103982													
J								Metabonomics approach reveals the vital role of Huangqi in Huangqi Jianzhong tang against chronic atrophic gastritis coupled with molecular docking and BAWP	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Huangqi; Huangqi Jianzhong tang; Chronic atrophic gastritis; Metabonomics; systemsDock; BAWP	PEROXIDE PRESOAKING PRIOR; HYDROGEN-PEROXIDE; WEB SERVER; AMMONIA; ACID; PRETREATMENT; RELEASE	Huangqi (HQ) is a commonly Chinese herb that has been widely used in China for thousands of years. Many classic traditional Chinese medicine (TCM) prescriptions consist of HQ, which is known as "eight Huangqi out of ten prescriptions". However, its contribution to the whole formula remains unclear. In this study, a classical prescription Huangqi jianzhong tang (HQJZ) was selected to evaluate the "monarch" role of HQ against chronic atrophic gastritis (CAG) rats based on pharmacology and metabonomic strategy. Traditional pharmacology indexes, including body weight variety, pathological examination and biochemical indexes of gastric juice showed that the efficacy of HQJZ and HQ were better than HY (HQJZ without HQ). Serum metabonomics analysis also showed that the same separation of metabolic features among them. 9 differential endogenous metabolites were screened and involved into the metabolic alterations induced by CAG. Among them, 7, 6 and 4 metabolites were regulated by HQJZ, HQ and HY, respectively. Three important pathways, including pyruvate metabolism, glycolysis or gluconeogenesis, aminoacyl-tRNA biosynthesis, were further screened out as the most important pathways related to the efficacy of HQ contributing to HQJZ by integrating MetaboAnalyst, systemsDock and binding ability weighted polypharmacology index (BAWP). Our work provided a novel method to study the metabolic contribution of HQ to the whole HQJZ in the treatment of CAG, which might be beneficial to understand the fundamental idea behind formula construction of TCM.																	0169-7439	1873-3239				MAY 15	2020	200								103984	10.1016/j.chemolab.2020.103984													
J								Crystal texture recognition system based on image analysis for the analysis of agglomerates	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Crystal texture; Image analysis; I-BGLAM; Crystal agglomerate; Crystal overlap	PARTICLE CLASSIFICATION; SINGLE-CRYSTALS; CRYSTALLIZATION; DISCRIMINATION; MORPHOLOGY; FEATURES; RETINEX	In the process of chemical production and biopharmaceutical, with the complex reaction, the products will overlap or adhere to each other. Effectively distinguishing the overlap and adhesion of crystals is of great significance for the statistics of different morphological characteristics such as the number and size of crystals. This paper proposes a crystal texture recognition system based on image analysis, which mainly includes image preprocessing, feature extraction and texture classification. Firstly, the crystal images are pre-processed to eliminate the influence of water droplets, particle shadows and uneven illumination. Secondly, the Improved-Basic Gray Level Aura matrix (I-BGLAM) is used to extract texture features of the crystals to determine the focus state of crystals. Finally, the texture features are classified by back propagation neural network (BPNN) to effectively distinguish agglomerates and pseudo-agglomerates. The case study and experimental results of cooling crystallization of 1-glutamic acid show that the texture recognition system can effectively distinguish the adhesion and overlap of crystals, and effectively analyze the agglomerates, and has good experimental accuracy.																	0169-7439	1873-3239				MAY 15	2020	200								103985	10.1016/j.chemolab.2020.103985													
J								Evaluating different sparsity measures for resolving LC/GC-MS data in the context of multivariate curve resolution	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multivariate curve resolution; Sparsity constraint; Rotational ambiguity; Norm	CHROMATOGRAPHY-MASS SPECTROMETRY; ROTATIONAL AMBIGUITY; MCR-ALS; REGRESSION; SELECTION; TUTORIAL	Since mass spectra are sparse in nature, the "sparsity" has been recently proposed as a constraint in multivariate curve resolution-alternating least squares (MCR-ALS) methods for analyzing LC/GC-MS data. There are different ways for implementation of sparsity constraint, and the majority of such methods rely on imposing a penalty term on the norms of recovered mass spectra. However, the main question is which penalty method is more appropriate for implementation of sparsity constraint in MCR methods. In order to address this question, two- and three-component LC/GC-MS data were simulated and used for the case study, in the work. The areas of feasible solutions (AFS) were calculated using the grid search and Nelder-Mead simplex algorithms. Moreover, different measures of sparsity such as L-0-norm, L-1-norm, and L-2/L-1 for all mass spectra in the AFSs were calculated and visualized as contour plots. The results revealed that all mentioned measures of sparsity find the sparsest solution in AFS. However, from the optimization point of view, L-1-norm and L-2/L-1 are easier to implement than L-0 -norm. Approximation methods for solving L-0-norm problem can take advantage of both unique solution and fast optimization. The results of L-0-norm, L-1-norm, and L-2/L-1 criteria were compared with other sparsity measures such as Shannon entropy, Hoyer, Kurtosis, and Gini indices. The results revealed that L-1-norm sparsity measure coincides with L-0-norm, Hoyer, Kurtosis, and Gini indices from the accuracy point of view. Finally, the feasibility of least absolute shrinkage and selection operator (Lasso) was assessed for implementation of L-1-norm penalty and finding the sparsest solution in MCR. It was found that for small values of lambda parameter, Lasso is able to find the sparsest solution with the minimum sum of squares of errors. Thorough optimization of lambda is necessary for obtaining accurate results. The lasso-MCR-ALS algorithm was tested with the real GC-MS datasets related to the analysis of Iranian red jujube oil.																	0169-7439	1873-3239				MAY 15	2020	200								104004	10.1016/j.chemolab.2020.104004													
J								Quality by Design: Comparison of Design Space construction methods in the case of Design of Experiments	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Quality by Design (QbD); Design of Experiments (DoE); Design space (DS); Monte-carlo method; Bootstrap technique	BY-DESIGN; OPTIMIZATION; UNCERTAINTY; GRANULATION; QBD	Quality by Design is a recent concept from quality control which has led to new requirements demanded by the authorities, particularly in the pharmaceutical industry. Among these, the guideline ICHQ8 explains that quality cannot be tested into products but should be built in by design. Designs of Experiments have been used to establish the relationship between input and output parameters of a process or formula while minimizing the risk in the final decision. More precisely, modelling studies are carried out to better understand the process and to predict the outputs in the whole domain of interest. When there are many input and output parameters, the experimenter looks for a compromise zone in which all the output parameters comply with the specifications. In this area, we can define a region where all the inputs can vary without altering the quality of the product with a fixed probability that the objectives will be reached. This zone is called Design Space and the determination of their boundaries is recommended by the Food and Drug Administration. In this publication, we propose different methods such as Monte-Carlo method, Bootstrap technique, numerical approach and reliability method to determine the Design Space. A comparison study is achieved using a case study.																	0169-7439	1873-3239				MAY 15	2020	200								104002	10.1016/j.chemolab.2020.104002													
J								A note on spectral data simulation	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Simulation of spectral data; Subspaces; Near infrared spectroscopy; Partial least squares regression	REGRESSION; COMPONENTS	In chemometrics, it is common to simulate data to test new methods. However, it is difficult to find an article that only discusses the spectral data simulation in a global context. Most of the time, the simulation is performed specifically for one method. In this context, it is often difficult to understand the simulation choices and also to carry out a simulation adapted to the problem that one wishes to highlight. A generic simulation framework would allow a better understanding of the simulations carried out and also make them easier to carry out. In this article, a generic framework is proposed to simulate databases representing the problem that one wishes to simulate and facilitating the description of the simulation procedure. This method of simulation is based on the basic principles of chemometrics and allows a simple and fast simulation of data. This will be highlighted by three examples.																	0169-7439	1873-3239				MAY 15	2020	200								103979	10.1016/j.chemolab.2020.103979													
J								Combination of least absolute shrinkage and selection operator with Bayesian Regularization artificial neural network (LASSO-BR-ANN) for QSAR studies using functional group and molecular docking mixed descriptors	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Azine derivatives; QSAR; Molecular docking; LASSO; Artificial neural network	REVERSE-TRANSCRIPTASE INHIBITORS; VARIABLE SELECTION; APPLICABILITY DOMAIN; AUTOMATED DOCKING; NONLINEAR QSAR; DRUG DESIGN; DERIVATIVES; PREDICTION; DISCOVERY; PROTEIN	A combination of least absolute shrinkage and selection operator (LASSO) with Bayesian Regularization feedforward artificial neural network (LASSO-BR-ANN) was used as a new approach in the quantitative structureactivity relationship (QSAR) studies. A mixture of the docking derived descriptors with the simple functional group (structural) features was also introduced as a new ensemble of descriptors for accurate QSAR modeling. The performance of introduced approaches was tested with QSAR modeling of the biological activities (pEC(50)) of 73 azine derivatives as new non-nucleoside reverse transcriptase inhibitors (NNRTIs) for treatment of HIV disease. Molecular docking descriptors (MDDs) were generated from ligand-receptor interactions and functional group features derived using Dragon 5.5 software. The dataset was divided into three sets of training, validation, and test data. LASSO, as a penalized regression method, was applied to the training data set for the selection of the most relevant descriptors among the mixture of the structural and MDDs. LASSO selected descriptors were used as inputs in the construction of the Bayesian Regularization artificial neural network (BR-ANN) model. The results showed that the addition of functional group properties to the MDDs improves the accuracy of the model. Under the optimum conditions, LASSO-BR-ANN was successfully applied for the prediction of PEC50 values for compounds in the external test set with mean square error (MSE) and coefficient of determination (R-2) values of 0.07 and 0.88, respectively. Some of the prediction statistical parameters of the model were calculated and all of them were in their acceptable ranges, which confirm the validity of the proposed QSAR model.																	0169-7439	1873-3239				MAY 15	2020	200								103998	10.1016/j.chemolab.2020.103998													
J								Three-way PARAFAC decomposition of chromatographic data for the unequivocal identification and quantification of compounds in a regulatory framework	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										PARAFAC; PARAFAC2; GC-MS; HPLC-DAD; Unequivocal identification	PARALLEL FACTOR-ANALYSIS; OPTIMIZATION; CURVES	The growing demand for controls of foodstuffs, personal care products, medicines and the environment is unquestionable, as well as a better understanding of the toxicity of chemical products. This causes a growing need to propose methods of analysis for the unequivocal identification and quantification of analytes in complex samples. Several official organizations that regulate these aspects in pesticides, migrants or additives, have increased the requirements regarding the figures of merit, among others, for the unequivocal identification of the target analytes. The general recommendation is the use of the information provided by chromatographic techniques on the test sample, for example, the use of HPLC-DAD or GC-MS data. Therefore, for each sample, a data matrix formed by the response vector (absorbances or abundances) recorded at each retention time is available. A data array is obtained when the matrices corresponding to the calibration standards and the test samples are concatenated. There are several chemometric techniques with the second-order advantage that can handle data arrays, so target analytes can be identified and quantified using them even in the presence of interferents. In this work, PARAFAC has been considered as a good option. If the data array is trilinear, its analysis using PARAFAC/PARAFAC2 enables the unequivocal identification and quantification of the target analyze so that the result is valid according to the criteria imposed by the authorities. In this work, the chemometric methodology is explained through four different case studies related to the determination of analytes in complex matrices (bisphenol A migrated from polycarbonate, dichlobenil in onion, oxybenzone in sunscreen cosmetic creams and melamine migrated from melaware). This mull-way methodology solves the problems of the coelution of interferents that have a similar absorbance spectrum in HPLC-DAD (or share m/z ratios in GC-MS) with the target analyze or with the internal standard causing false-negative results with conventional identification methods. In addition, the PARAFAC decomposition of trilinear arrays enables the joint optimization of several analytical parameters (extraction, clean up, etc.) that control different sample pretreatments prior to the chromatographic determination of complex samples.																	0169-7439	1873-3239				MAY 15	2020	200								104003	10.1016/j.chemolab.2020.104003													
J								Sparse feature selection in multi-target modeling of carbonic anhydrase isoforms by exploiting shared information among multiple targets	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Sparse feature selection; l(2,)p-norm; Shared-subspace; Multi-target modeling; Carbonic anhydrase	SUPERVISED FEATURE-SELECTION; INHIBITORY-ACTIVITY; POTENT INHIBITORS; GRAPH LAPLACIAN; NEURAL-NETWORKS; QSAR MODELS; IX; XII; SULFONAMIDES; DERIVATIVES	Multi-target modeling can be used for inhibition prediction of CA isoforms, the essential zinc metalloenzymes involved in different biological processes such as tumorigenesis. In this study, the first multi-target model is developed for predicting the activities of inhibitors against CA-I, CA-II, CA-IX, and CA-XII. Structural similarity analysis is carried out for two cancer-related isoforms CA-IX and CA-XII. The mean TM-score value (0.935) reveals a marked similarity between the two structures. To select relevant descriptors for the developed multi-target model, we propose a novel feature selection method based on shared subspace learning, which considers correlation among different targets in multi-target modeling. The proposed shared subspace feature selection method uses the mixed convex and non-convex l2,p-norm (0 < p <= 1) minimization on both regularization and loss function to ensure that the loss function is robust to outliers and consider correlation among different descriptors for joint sparse feature selection. To solve the proposed shared subspace feature selection method for convex and non-convex cases, a unified Algorithm is presented. The study utilized a WA set to evaluate the performance of the proposed feature selection method with a multi-target kernel smoother model and compare to that of other feature selection methods with the multi-target kernel smoother models. The obtained results demonstrate the superiority of the proposed shared-subspace feature selection approach based on l(2,1/2) -norm in selecting the most relevant descriptors. Statistical results (RMSEtest = 0.5190, R-test(2) = 0.7613 and Q(ext)(2) = 0.7524) demonstrate that the model displays adequate quality for virtual screening. The results also represent the significance of using the shared subspace among different targets in the selection of relevant descriptors to predict the inhibition of CA isoforms.																	0169-7439	1873-3239				MAY 15	2020	200								104000	10.1016/j.chemolab.2020.104000													
J								DNNAce: Prediction of prokaryote lysine acetylation sites through deep neural networks with multi-information fusion	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Prokaryote lysine acetylation sites; Multi-information fusion; Group Lasso; Deep neural networks	SUBCELLULAR-LOCALIZATION; UBIQUITINATION SITES; ACCURATE PREDICTION; PROTEOMIC ANALYSIS; SEQUENCE FEATURES; PROTEIN; HISTONE; IDENTIFICATION; METHYLATION; ACETYLOME	As a reversible and widely existing post-translational modification of proteins, acetylation plays a crucial role in transcriptional regulation, apoptosis, and cytokine signaling. To better understand the molecular mechanism of acetylation, identification of acetylation sites is vital. The traditional experimental methods are time-consuming and cost-prohibitive, and the majority of acetylation sites remain unknown. It is necessary to develpoe an effective and accurate computational approach to predict the acetylation sites, especially utilizing the advanced deep learning technique. In this study, we propose a prokaryote acetylation sites prediction method based on the deep neural networks, named DNNAce. We extract the protein features from sequence information, physicochemical information, and evolution information of amino acid residues and obtain the initial feature set All. Moreover, we use Group Lasso to remove the irrelevant features that are not effective for classification in the sites prediction area. Compared to other machine learning methods, we use deep neural networks to predict acetylation sites. The 10-fold cross-validation on independent test datasets indicates that DNNAce has the highest values of accuracy for predicting acetylation sites. That DNNAce accurately identifies acetylation sites assists us in understanding its molecular mechanism, and provides related theoretical foundation for the development of drug targets for various diseases. The source code and all datasets are available at https://github.com/QUST-AIBBDRC/DNNAce/.																	0169-7439	1873-3239				MAY 15	2020	200								103999	10.1016/j.chemolab.2020.103999													
J								A deep learning based regression method on hyperspectral data for rapid prediction of cadmium residue in lettuce leaves	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Stacked auto-encoders; Hyperspectral imaging; Lettuce; Heavy metal; Nondestructive testing	TVB-N CONTENT; MOISTURE-CONTENT; BIOACCESSIBILITY; BIOAVAILABILITY; IMAGE; TIME	In order to effectively realize the spectral detection of heavy metal content, a deep learning method which consisted of stacked auto-encoders (SAE) and partial least squares support vector machine regression (LSSVR) is proposed to obtain depth features and establish cadmium (Cd) detection model. The Vis-NIR hyperspectral images of 1120 lettuce leaf samples were obtained and the whole region of lettuce leaf sample spectral data was collected and preprocessed with different spectral pre-treatment methods. Successive projections algorithm (SPA), partial least squares regression (PLSR) and SAE were used to acquire the optimum wavelength, respectively. Besides, the characteristic wavelengths were used to build partial least squares support vector machine regression (LSSVR) models. Furthermore, the best prediction performance for detecting Cd content in lettuce leaves was obtained by Savitzky-Golay combined with first derivative (SG-1st) pre-processing method, with R-p(2) of 0.9487, RMSEP of 0.01049 mg/kg and RPD of 3.330 using SAE-LSSVR method. The results of this study indicated that deep learning method coupled with hyperspectral imaging technique has great potential for detecting heavy metal content in lettuce leaves.																	0169-7439	1873-3239				MAY 15	2020	200								103996	10.1016/j.chemolab.2020.103996													
J								BP neural network modeling with sensitivity analysis on monotonicity based Spearman coefficient	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Spearman coefficient; Monotonicity; Soft sensing; BP neural network	EXTREME LEARNING-MACHINE; SOFT SENSOR; PREDICTION	This paper proposes a new monotonicity extraction method which is used to constrain the modeling process of a neural network. The main contributions of this paper are the sensitivity analysis on monotonicity based on Spearman coefficient, and the application of monotonicity on neural network modeling. This study uses scatter plots of bivariate variables and the Spearman coefficient to extract the monotonic information. To weaken the influence of noise, binary 0-1 integer linear program is applied to filter the scatter diagram. Based on the monotonicity information, a constraint optimization problem is proposed to obtain the BP neural network model and an Alopex-based evolutionary algorithm (AEA) is used to search for the optimal weights and thresholds. The results of a numeral example and an ethylene cracking furnace show that the proposed approach can achieve a good predicting performance in the two cases.																	0169-7439	1873-3239				MAY 15	2020	200								103977	10.1016/j.chemolab.2020.103977													
J								Hyper-heuristic method for multilevel thresholding image segmentation	EXPERT SYSTEMS WITH APPLICATIONS										Image segmentation; Multilevel thresholding; Hyper-Heuristic; Image processing; Meta-heuristic algorithms	OPTIMIZATION ALGORITHM; ENTROPY	In digital image processing, one of the most relevant tasks is to classify pixels depending on their intensity level. To perform this process there exist different traditional methods as Otsu or Kapur, such methods are used to compute the thresholds that divide the histogram of the image into different groups. These methods are easy to implement for a single threshold; however, the computational effort is affected when more thresholds are required. Therefore, different meta-heuristic based approaches have been proposed, but each of them has its properties and limitations. So, this paper introduces an alternative concept to the image segmentation which is called hyper-heuristic that at each iteration determines the optimal execution sequence of meta-heuristic algorithms that provides the optimal thresholds. The proposed method consists of two layers, in the first layer, the genetic algorithm (GA) is used to determine the execution sequence of the meta-heuristic algorithms. While the second layer contains the set of four meta-heuristic algorithms that executed in a specific order, assigned by the current solution of GA, to update the threshold population. In order to evaluate the performance of the proposed approach, it has been tested over a set of benchmark images and the results provide a good performance in terms of quality of segmentation. Moreover, experimental comparisons support that the proposed hyper-heuristic is able to find more accurate solutions than other algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113201	10.1016/j.eswa.2020.113201													
J								A-Stacking and A-Bagging: Adaptive versions of ensemble learning algorithms for spoof fingerprint detection	EXPERT SYSTEMS WITH APPLICATIONS										Stacking; Bagging; Ensemble learning; Spoof fingerprint detection	CLASSIFIERS; PATTERN; MODELS	Stacking and bagging are widely used ensemble learning approaches that make use of multiple classifier systems. Stacking focuses on building an ensemble of heterogeneous classifiers while bagging constructs an ensemble of homogenous classifiers. There exist some applications where it is essential for learning algorithms to be adaptive towards the training data. We propose A-Stacking and A-Bagging; adaptive versions of stacking and bagging respectively that take into consideration the similarity inherently present in the dataset. One of the main motives of ensemble learning is to generate an ensemble of multiple "experts" that are weakly correlated. We achieve this by producing a set of disjoint experts where each expert is trained on a different subset of the dataset. We show the working mechanism of the proposed algorithms on spoof fingerprint detection. The proposed versions of these algorithms are adaptive as they conform to the features extracted from the live and spoof fingerprint images. From our experimental results, we establish that A-Stacking and A-Bagging give competitive results on both balanced and imbalanced datasets. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113160	10.1016/j.eswa.2019.113160													
J								ReDMark: Framework for residual diffusion watermarking based on deep networks	EXPERT SYSTEMS WITH APPLICATIONS										Blind watermarking; CNN; Data diffusion; Deep convolutional networks; FCN; Transparency	NEURAL-NETWORKS; IMAGE CLASSIFICATION; BLIND WATERMARKING; ROBUST; SCHEME; MACHINE; SECURE; TRANSFORM	Due to the rapid growth of machine learning tools and specifically deep networks in various computer vision and image processing areas, applications of Convolutional Neural Networks for watermarking have recently emerged. In this paper, we propose a deep end-to-end diffusion watermarking framework (ReDMark) which can learn a new watermarking algorithm in any desired transform space. The framework is composed of two Fully Convolutional Neural Networks with residual structure which handle embedding and extraction operations in real-time. The whole deep network is trained end-to-end to conduct a blind secure watermarking. The proposed framework simulates various attacks as a differentiable network layer to facilitate end-to-end training. The watermark data is diffused in a relatively wide area of the image to enhance security and robustness of the algorithm. Comparative results versus recent state-of-the-art researches highlight the superiority of the proposed framework in terms of imperceptibility, robustness and speed. The source codes of the proposed framework are publicly available at Githubl. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113157	10.1016/j.eswa.2019.113157													
J								Classification of user competency levels using EEG and convolutional neural network in 3D modelling application	EXPERT SYSTEMS WITH APPLICATIONS										Deep neural networks; EEG; Competency; CNN; Novice; Entropy	AUTOMATED DETECTION; NOVICE; EXPERT; SEGMENTATION; PERFORMANCE; EXPERIENCE; STRESS; CAD	Competency classification is one of the main challenging tasks for the development of state-of-the-art next generation computer-aided design (CAD) system. To develop a futuristic system that can accommodate the lack of competency, the system needs to adapt to the competency level of the user. To solve this problem, we have presented a deep convolutional neural network (CNN) model that uses the Electroencephalography (EEG) of the user to classify the level of competency in 3D modeling task. The five competency levels were defined based on the task completion time, final 3D model rating and previous modeling experience. This is the first study that classifies user competency and employs the CNN model for the analysis of EEG signals in the design application. In this work, a 14-layer deep CNN model was implemented to classify competency into five different levels. The proposed technique achieved an accuracy, specificity, and sensitivity of > 88%, > 90% and > 70% respectively with 5-fold cross-validation. The results showed the applicability of a CNN model to classify the user competency and can be used as a first step in developing state-of-the-art adaptive 3D modeling systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113202	10.1016/j.eswa.2020.113202													
J								Data-driven construction of SPARQL queries by approximate question graph alignment in question answering over knowledge graphs	EXPERT SYSTEMS WITH APPLICATIONS										Knowledge graph; Answering natural language questions; Disambiguation of interpretations; Pairwise graph alignment	SEMANTIC WEB	As increasingly more semantic real-world data is stored in knowledge graphs, providing intuitive and effective query methods for end-users is a fundamental and challenging task. Since there is a gap between the plain natural language question (NLQ) and structured data, most RDF question/answering (QJA) systems construct SPARQL queries from NLQs and obtain precise answers from knowledge graphs. A major challenge is how to disambiguate the mapping of phrases and relations in a question to the dataset items, especially in complex questions. In this paper, we propose a novel data-driven graph similarity framework for RDF Q/A to extract the query graph patterns directly from the knowledge graph instead of constructing them with semantically mapped items. An uncertain question graph is presented to model the interpretations of an NLQ based on which our problem is reduced to a graph alignment problem. In formulating the alignment, both the lexical and structural similarity of graphs are considered, hence, the target RDF subgraph is used as a query graph pattern to construct the final query. We create a pruned entity graph dynamically based on the complexity of an input question to reduce the search space on the knowledge graph. Moreover, to reduce the calculating cost of the graph similarity, we compute the similarity scores only for same-distance graph elements and equip the process with an edge association aware surface form extraction method. Empirical studies over real datasets indicate that our proposed approach is flexible and effective as it outperforms state-of-the-art methods significantly. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113205	10.1016/j.eswa.2020.113205													
J								An efficient binary social spider algorithm for feature selection problem	EXPERT SYSTEMS WITH APPLICATIONS										Social spider algorithm; Feature selection; Classifiers	PARTICLE SWARM OPTIMIZATION; FEATURE SUBSET-SELECTION; GENETIC ALGORITHM; CLASSIFICATION; REDUCTION	The social spider algorithm (SSA) is a heuristic algorithm created on spider behaviors to solve continuous problems. In this paper, firstly a binary version of the social spider algorithm called binary social spider algorithm (BinSSA) is proposed. Currently, there is insufficient focus on the binary version of SSA in the literature. The main part of the binary version is the transfer function. The transfer function is responsible for mapping continuous search space to binary search space. In this study, eight of the transfer functions divided into two families, S-shaped and V-shaped, are evaluated. BinSSA is obtained from SSA, by transforming constant search space to binary search space with eight different transfer functions (S-Shapes and V-Shaped). Thus, eight different variations of BinSSA are formed as BinSSA1, BinSSA2, BinSSA3, BinSSA4, BinSSA5, BinSSA6, BinSSA7, and BinSSA8. For increasing, exploration and exploitation capacity of BinSSA, a crossover operator is added as BinSSA-CR. In secondly, the performances of BinSSA variations are tested on feature selection task. The optimal subset of features is a challenging problem in the process of feature selection. In this paper, according to different comparison criteria (mean of fitness values, the standard deviation of fitness values, the best of fitness values, the worst of fitness values, accuracy values, the mean number of the selected features, CPU time), the best BinSSA variation is detected. In the feature selection problem, the K-nearest neighbor (K-NN) and support vector machines (SVM) are used as classifiers. A detailed study is performed for the fixed parameter values used in the fitness function. BinSSA is evaluated on low-scaled, middle-scaled and large-scaled twenty-one well-known UCI datasets and obtained results are compared with state-of-art algorithms in the literature. Obtained results have shown that BinSSA and BinSSA-CR show superior performance and offer quality and stable solutions. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113185	10.1016/j.eswa.2020.113185													
J								A grid-quadtree model selection method for support vector machines	EXPERT SYSTEMS WITH APPLICATIONS										Support vector machine; Parameter determination; Quadtree; Grid search	PARAMETER DETERMINATION; SVM ALGORITHM; OPTIMIZATION; CLASSIFICATION	In this paper, a new model selection approach for Support Vector Machine (SVM), which integrates the quadtree technique with the grid search, denominated grid-quadtree (GQ) is proposed. The developed method is the first in the literature to apply the quadtree for the SVM parameters optimization. The SVM is a machine-learning technique for pattern recognition whose performance relies on its parameters determination. Thus, the model selection problem for SVM is an important field of study and requires expert and intelligent systems to solve it. Real classification data sets involve a huge number of instances and features, and the greater is the training data set dimension, the larger is the cost of a recognition system. The grid search (GS) is the most popular and the simplest method to select parameters for SVM. However, it is time-consuming, which limits its application for big-sized problems. With this in mind, the main idea of this research is to apply the quadtree technique to the GS to make it faster. Hence, this may lower computational time cost for solving problems such as bio-identification, bank credit risk and cancer detection. Based on the asymptotic behaviors of the SVM, it was noticeably observed that the quadtree is able to avoid the GS full search space evaluation. As a consequence, the GQ carries out fewer parameters analysis, solving the same problem with much more efficiency. To assess the GQ performance, ten classification benchmark data set were used. The obtained results were compared with the ones of the traditional GS. The outcomes showed that the GQ is able to find parameters that are as good as the GS ones, executing 78.8124% to 85.8415% fewer operations. This research points out that the adoption of quadtree expressively reduces the computational time of the original GS, making it much more efficient to deal with high dimensional and large data sets. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113172	10.1016/j.eswa.2019.113172													
J								Combining novelty and popularity on personalised recommendations via user profile learning	EXPERT SYSTEMS WITH APPLICATIONS										Recommender systems; Machine learning; Data sparsity; Diffusion-based algorithms; User profile	ACCURACY; DIFFUSION; MODEL	Recommender systems have been widely used by large companies in the e-commerce segment as aid tools in the search for relevant contents according to the user's particular preferences. A wide variety of algorithms have been proposed in the literature aiming at improving the process of generating recommendations; in particular, a collaborative, diffusion-based hybrid algorithm has been proposed in the literature to solve the problem of sparse data, which affects the quality of recommendations. This algorithm was the basis for several others that effectively solved the sparse data problem. However, this family of algorithms does not differentiate users according to their profiles. In this paper, a new algorithm is proposed for learning the user profile and, consequently, generating personalised recommendations through diffusion, combining novelty with the popularity of items. Experiments performed in well-known datasets show that the results of the proposed algorithm outperform those from both diffusion-based hybrid algorithm and traditional collaborative filtering algorithm, in the same settings. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113149	10.1016/j.eswa.2019.113149													
J								Ensemble learning with label proportions for bankruptcy prediction	EXPERT SYSTEMS WITH APPLICATIONS										Bankruptcy prediction; Ensemble learning; Learning with label proportion; Proportion-SVM	FINANCIAL RATIOS; CREDIT RISK; DECISION TREE; PERFORMANCE; MODELS	Corporate bankruptcy prediction is an interesting and important research topic that can be conceived in many practical applications. Recently, machine learning based methods have been widely proposed to solve the problem of bankruptcy prediction. However, the existing models do not consider that large amounts of instance-level labeled training data are hard to be obtained in practice. In this paper, we propose to address bankruptcy prediction problem from the perspective of learning with label proportions, where the unlabeled training data are provided in different bags and only giving the bag-level proportion of instances belonging to a particular class. Then, we contribute two novel prediction methods, termed as Bagged-pSVM and Boosted-pSVM, based on proportion support vector machines and ensemble strategies including bagging and boosting. The proposed methods can not only explicitly model the unknown instance-level labels and the known label proportions under a large-margin framework, but also improve the performance through introducing ensemble learning strategies. Extensive experiments on the benchmark datasets demonstrate their efficiency and superiority on solving the problem of bankruptcy prediction. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113155	10.1016/j.eswa.2019.113155													
J								SHyFTOO, an object-oriented Monte Carlo simulation library for the modeling of Stochastic Hybrid Fault Tree Automaton	EXPERT SYSTEMS WITH APPLICATIONS										Discrete event simulation; Multi-state systems; Repairable systems; Dynamic reliability; Matlab (R); Dynamic fault tree	DYNAMIC RELIABILITY; RISK ANALYSIS; PETRI NETS; SYSTEMS; PERFORMANCE; DEPENDABILITY; FAILURES; TOOL	Dependability assessment is a crucial activity to ensure the correct operation of complex systems. The output of dependability assessment activities include the quantification of reliability, availability, maintenance and safety related metrics. These metrics can assist in the identification of the system weak points or in the conception of mitigation strategies to increase the system dependability level. The development of advanced computer-aided methodologies to support dependability assessment activities is essential to automate and reduce the efforts implied by this process and similarly, the development of accurate dependability assessment methods is very important to increase the quality of the results. In this context, it is possible to identify different contributions that improve the dependability assessment through general-purpose modeling methodologies. However, existing solutions are ad-hoc applications specified with low-level stochastic formalisms and this complicates their adoption in the industry. Accordingly, this paper presents Stochastic Hybrid Fault Tree Automaton (SHyFTA) based simulation algorithm that allows the accurate dependability analysis of repairable multi-state systems. SHyFTA integrates the stochastic and deterministic operation of the system under study as well as their interactions. The algorithm is formalized through an object-oriented software architecture, which is developed as a software library for the modeling and simulation of repairable SHyFTA models. Following the proposed architecture, a Matlab implementation of this library, SHyFTOO, has been developed and validated with a thorough test campaign. In order to provide a guideline to the end-users and show the potential of the SHyFTOO library, the case study of a feed-water pumping system is implemented in detail and it is used to evaluate different preventive maintenance policies. The SHyFTOO library can open the way to further investigations that address the interactions between the failure behavior and the functional operation of a system and their combined effect on system dependability. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113139	10.1016/j.eswa.2019.113139													
J								A bi-phased multi-objective genetic algorithm based classifier	EXPERT SYSTEMS WITH APPLICATIONS										Classification rules mining; Elitist Multi-Objective Genetic Algorithm; Pareto approach; Statistical test	EVOLUTIONARY ALGORITHMS; RULE EVALUATION; SYSTEMS; OPTIMIZATION; INDUCTION; DISCOVERY; ACCURACY; INTERVALS; SELECTION; DESIGN	This paper presents a novel Bi-Phased Multi-Objective Genetic Algorithm (BPMOGA) based classification method. It is a Learning Classifier System (LCS) designed for supervised learning tasks. Here we have used Genetic Algorithms (GAs) to discover optimal classifiers from data sets. The objective of the work is to find out a classifier or Complete Rule (CR) which comprises of several Class Specific Rules (CSRs). Phase-I of BPMOGA extracts optimized CSRs in IF - THEN form by following Michigan approach, without considering interaction among the rules. Phase-II of BPMOGA builds optimized CRs from CSRs by following Pittsburgh way. It combines the advantages of both approaches. Extracted CRs help to build CSRs for the next run of phase-I. Hence, phase-I and phase-Il are cyclically related, which is one of the uniqueness of BPMOGA. With the help of twenty one benchmark data sets from the University of California at Irvine (UCI) machine learning repository we have compared performance of BPMOGA based classifier with fourteen GA and non-GA based classifiers. Statistical test shows that the performance of the proposed classifier is either superior or comparable to other classifiers. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113163	10.1016/j.eswa.2019.113163													
J								DPDnet: A robust people detector using deep learning with an overhead depth camera	EXPERT SYSTEMS WITH APPLICATIONS										People detection; Depth camera information; interest regions estimation; Overhead depth camera; Feature extraction	TRACKING PEOPLE; COUNTING PEOPLE; TIME; RGB	This paper proposes a deep learning-based method to detect multiple people from a single overhead depth image with high precision. Our neural network, called DPDnet, is composed by two fully-convolutional encoder-decoder blocks built with residual layers. The main block takes a depth image as input and generates a pixel-wise confidence map, where each detected person in the image is represented by a Gaussian-like distribution, The refinement block combines the depth image and the output from the main block, to refine the confidence map. Both blocks are simultaneously trained end-to-end using depth images and ground truth head position labels. The paper provides a rigorous experimental comparison with some of the best methods of the state-of-the-art, being exhaustively evaluated in different publicly available datasets. DPDnet proves to outperform all the evaluated methods with statistically significant differences, and with accuracies that exceed 99%. The system was trained on one of the datasets (generated by the authors and available to the scientific community) and evaluated in the others without retraining, proving also to achieve high accuracy with varying datasets and experimental conditions. Additionally, we made a comparison of our proposal with other CNN-based alternatives that have been very recently proposed in the literature, obtaining again very high performance. Finally, the computational complexity of our proposal is shown to be independent of the number of users in the scene and runs in real time using conventional GPUs. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113168	10.1016/j.eswa.2019.113168													
J								A novel wrapper feature selection algorithm based on iterated greedy metaheuristic for sentiment classification	EXPERT SYSTEMS WITH APPLICATIONS										Sentiment classification; Feature selection; Iterated greedy; Metaheuristic; Machine learning		In recent years, sentiment analysis is becoming more and more important as the number of digital text resources increases in parallel with the development of information technology. Feature selection is a crucial sub-stage for the sentiment analysis as it can improve the overall predictive performance of a classifier while reducing the dimensionality of a problem. In this study, we propose a novel wrapper feature selection algorithm based on Iterated Greedy (IG) metaheuristic for sentiment classification. We also develop a selection procedure that is based on pre-calculated filter scores for the greedy construction part of the IG algorithm. A comprehensive experimental study is conducted on commonly-used sentiment analysis datasets to assess the performance of the proposed method. The computational results show that the proposed algorithm achieves 96.45% and 90.74% accuracy rates on average by using Multi-nomial Naive Bayes classifier for 9 public sentiment and 4 Amazon product reviews datasets, respectively. The results also reveal that our algorithm outperforms state-of-the-art results for the 9 public sentiment datasets. Moreover, the proposed algorithm produces highly competitive results with state-of-the-art feature selection algorithms for 4 Amazon datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113176	10.1016/j.eswa.2020.113176													
J								ILWAANet: An Interactive Lexicon-Aware Word-Aspect Attention Network for aspect-level sentiment classification on social networking	EXPERT SYSTEMS WITH APPLICATIONS										Social media; Deep learning; Aspect-level sentiment analysis		An Interactive Lexicon-Aware Word-Aspect Attention Network (ILWAAN) is proposed for aspect-level sentiment classification which deals with identifying the sentiment polarity of a specific aspect in its context and have potential application on social networking. In this model, effective multiple attention mechanisms (intra-attention and interactive-attention mechanisms) integrated with sentiment lexicon information are developed to form an aspect-specific representation at two levels: Phrase-level and Aggregation-level information. Specifically, an aspect and its context are fused with the sentiment lexicon information and learn their relationship representations by lexicon-aware attention operations. This allows the model to tries to incorporate the aspect information into the deep neural networks and learn to attend the correct sentiment context words conditioned on the informative aspect words. To evaluate the performance, we evaluate our model in three benchmark data: Twitter, Laptop, and Restaurant. The experimental results indicate that our models improve the performance for aspect-level sentiment classification. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113065	10.1016/j.eswa.2019.113065													
J								Evidential two-step tree species recognition approach from leaves and bark	EXPERT SYSTEMS WITH APPLICATIONS										Tree species recognition; Two-step classification; Theory of belief functions; Mass estimation; k nearest neighbors	BELIEF FUNCTIONS; CLASSIFICATION; IMAGE; RULE	The contribution of this paper is twofold. First, this paper aims at developing an intelligent system that emulates the decision-making ability of a botanist expert in the recognition of tree species from their leaves and bark. The main challenges of this recognition problem are related to the high diversity of trees in nature, the interspecies similarity and the intra-species variability. Therefore, similarities between species cause several confusions during recognition. The proposed decision system is designed to solve this complex problem of tree species recognition by reasoning with knowledge sets where the inference engine is based on belief functions theory, which reduces confusion between species and achieves greater accuracy. Secondly, this paper proposes a practical solution that can be embedded in the user's smartphone without any need for an internet connection. Therefore, our approach is adapted for smartphone limits, i.e. limits related to memory and computation capacity. Once in nature, everybody should appreciate the idea of having a mobile application that reflects the skills and know-how of a botanist. Building an application to make the potential of tree species recognition accessible and easy to use is a challenging problem. From methodological perspectives, the suggested method is a two-step recognition approach that identifies the leaf in a first step and refines the results using the bark in the second step. In fact, the first step is used to reduce the dimensionality of the problem through the identification of a subset of most probable species. The second step is performed using a modified evidential k Nearest Neighbors (EkNN) algorithm that recognizes the bark from the output of the first step. A set of experiments on real-world data is presented in order to study the accuracy of the proposed solution against existing ones. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113154	10.1016/j.eswa.2019.113154													
J								Model validation failure in class imbalance problems	EXPERT SYSTEMS WITH APPLICATIONS										Class imbalance; Model validation; Absolute rarity; Performance evaluation	CLASSIFICATION; ACCURACY; SMOTE	For a classification task, multiple classification models can be built from the training set in various ways. In general, the best-performing model is selected for deployment through a model validation procedure. However, even if the dataset is sufficiently large, model validation is difficult when the minority class is too rare in an absolute sense in the validation set. Under such an extreme absolute rarity condition, the validation performance of a model is more affected by randomness in the model so that it would misleadingly estimate the generalization ability of the model. In this regard, even a random guessing model, which will eventually fail to accurately classify new data, can yield a considerably high validation performance by chance. This implies that the selected model may not perform well during its deployment. In this study, the effect of absolute rarity on the inherent difficulty of model validation is investigated. We demonstrate that the higher degree of absolute rarity in the validation set as well as comparing a larger number of models during model validation contribute to an increased likelihood of model validation failure. Finally, a practical guideline is suggested to evaluate model validation results. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113190	10.1016/j.eswa.2020.113190													
J								Multiplex community detection in complex networks using an evolutionary approach	EXPERT SYSTEMS WITH APPLICATIONS										Community detection; Multiplex networks; Evolutionary algorithms; Social network analysis	GENETIC ALGORITHM; ORGANIZATION; MULTISCALE; MOEA/D	Multiplex networks are the general representative of complex systems composed of distinct interactions between the same entities on multiple layers. Community detection in the multiplex networks is the problem of finding a shared structure under all layers, which combines the information of the entire network. Most of the existing methods for community detection in the single-layer networks cannot be well applied to detect shared communities in multiplex networks. In this paper, we employ a multi objective evolutionary approach, namely Multi-Objective Evolutionary Algorithm based on Decomposition with Tabu Search (MOEA/D-TS), to detect shared communities in multiplex networks. Also, we have improved the MOEA/D-TS using a social networks analysis measure named Clustering Coefficient (CC) in terms of the generation of the initial population. This hybrid algorithm employs the parallel computing capacity of the Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) along with the neighborhood search authority of Tabu Search (TS) for discovering Pareto optimal solutions. Extensive experiments on a variety of single-layer and multiplex real-world data sets show the superiority of the proposed method in comparison to state-of-the-art algorithms and its capability for producing improved results. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113184	10.1016/j.eswa.2020.113184													
J								Fingerprint indexing for wrinkled fingertips immersed in liquids	EXPERT SYSTEMS WITH APPLICATIONS										Fingerprint indexing; Contactless fingerprint recognition; Wrinkled fingertips; Minutiae; Clustering; Ellipse	CONTACTLESS	Wrinkled fingerprint recognition has been a challenging problem because of changing the position of fingertip features. This change significantly degrades the fingerprint recognition accuracy. Contactless dry three-dimensional (3D) fingerprints have the advantages of reducing the position change of fingertip features presented in both contact-based and contactless dry 2D fingerprints. Unfortunately, in contrast to the contactless dry 3D fingerprints, the position of features in the contactless wrinkled 3D fingerprints will be changed. Furthermore, identifying a fingerprint in a voluminous database is another challenge. With an increasing the number of individuals and inserting their fingerprints into the enrollment database, the cost of identification will increase and can become critical. Fingerprint indexing is a prominent method to reduce the response time of a probe in a large-scale database. The indexing approaches powerfully boost the recognition efficiency of dry fingerprints, but the accuracy of wrinkled fingerprints cannot be guaranteed. Moreover, previous indexing approaches only focused on dry 2D fingerprints and did not consider 3D fingerprints and the problems of wrinkled fingerprints. This paper proposes a 3D fingerprint reconstruction technique based on multi-view contactless wrinkled fingerprint images. In the proposed system, we use two cameras to acquire the frontal image. A dual camera can get more details of an image and is useful to acquire wrinkled fingerprints. In this paper, we propose a rectification technique for wrinkled 2D and 3D fingerprints as well. This paper also proposes a wrinkled fingerprint indexing approach to overcome the problems of wrinkled fingerprints. Our proposal employs minutiae quadruplets, ellipse properties, and a k-means clustering to index and retrieve fingerprints. Finally, a voting technique based on minutiae quality is proposed. Experimental results validate our approach and demonstrate the effectiveness of proposed method. Some new outlooks on the topics of 3D fingerprint acquisition, rectification and 3D reconstruction of wrinkled fingerprints, and fingerprint indexing are revealed. The main impact of this work is that more researchers will be attracted in related areas for wrinkled fingerprint recognition. The main significance is that the diversity of expert systems should be well promoted in addressing reconstruction of 3D shape and rectification of deformed fingerprints. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113153	10.1016/j.eswa.2019.113153													
J								Robust FCM clustering algorithm with combined spatial constraint and membership matrix local information for brain MRI segmentation	EXPERT SYSTEMS WITH APPLICATIONS										Brain MRI segmentation; Membership matrix; Local information; FCM; Spatial constraint	FUZZY C-MEANS; IMAGE SEGMENTATION; TUMOR SEGMENTATION	This paper presents a robust fuzzy clustering algorithm for the segmentation of brain tissues in magnetic resonance imaging (MRI). The proposed method incorporates context-aware spatial constraint and local information of the membership matrix into the fuzzy c-means (FCM) clustering algorithm. Based upon this approach, an FCM clustering algorithm with joint spatial constraint and membership matrix local information (FCMS-MLI) for brain MRI segmentation is presented, which is more robust against noise and other artifacts. The proposed spatial constraint considers both local spatial and gray-level information adaptively, and to the best of the authors' knowledge for the first time, the membership matrix local information (MLI) of fuzzy clustering is extracted to be utilized besides the spatial constraint. The proposed method solves two significant drawbacks of spatial constraint-based FCM approaches, which are ineffectiveness in preserving image details as well as confronting noise and intensity non-uniformity (INU) simultaneously. These problems are caused due to utilizing spatial constraints solely. The presented context-aware spatial constraint makes the method robust against a high level of noise while preserving image details. Furthermore, employing the MLI technique improves segmentation results in the presence of noise concurrently with INU. In contrast to spatial constraint-based methods, which just use local information in the image domain, the FCMS-MLI technique utilizes information in both image and coefficient domains. Hence, the proposed method benefits from two different sources of information. Finally, several types of images, including synthetic images, simulated and real brain MR images are utilized to make a comparison among the performances of popular FCMS types (i.e. FCM algorithms with spatial constraint), some new methods and the proposed algorithm. Experimental results prove efficiency and robustness of the FCMS-MLI algorithm confronting different levels of noise and INU. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113159	10.1016/j.eswa.2019.113159													
J								A stochastic approximation approach to spatio-temporal anchorage planning with multiple objectives	EXPERT SYSTEMS WITH APPLICATIONS										Anchorage planning; Spatio-temporal planning; Planning expert system; Stochastic approximation; Multi-objective optimization	CAPACITY; STRATEGY	Globalization and subsequent increase in seaborne trade have necessitated efficient planning and management of world's anchorage areas. These areas serve as a temporary stay area for commercial vessels for various reasons such as waiting for passage or port, fuel services, and bad weather conditions. The research question we consider in this study is how to place these vessels inside a polygon-shaped anchorage area in a dynamic fashion as they arrive and depart, which seems to be the first of its kind in the literature. We specifically take into account the objectives of (1) anchorage area utilization, (2) risk of vessel collisions, and (3) fuel consumption performance. These three objectives define our objective function in a weighted sum scheme. We present a spatio-temporal methodology for this multi-objective anchorage planning problem where we use Monte Carlo simulations to measure the effect of any particular combination of planning metrics (measured in real time for an incoming vessel) on the objective function (measured in steady state). We resort to the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm for identifying the linear combination of the planning metrics that optimizes the objective function. We present computational experiments on a major Istanbul Straight anchorage, which is one of the busiest in the world, as well as synthetic anchorages. Our results indicate that our methodology significantly outperforms comparable algorithms in the literature for daily anchorage planning. For the Istanbul Straight anchorage, for instance, reduction in risk was 42% whereas reduction in fuel costs was 45% when compared the best of the current state-of-the-art methods. Our methodology can be utilized within a planning expert system that intelligently places incoming vessels inside the anchorage so as to optimize multiple strategic goals. Given the flexibility of our approach in terms of the planning objectives, it can easily be adapted to more general variants of multi-objective spatio-temporal planning problems where certain objects need to be dynamically placed inside two or even-three dimensional spaces in an intelligent manner. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113170	10.1016/j.eswa.2019.113170													
J								Informative top-k class associative rule for cancer biomarker discovery on microarray data	EXPERT SYSTEMS WITH APPLICATIONS										Microarray gene expression; Associative classification; Information gain; Biomarker discovery; Colorectal cancer; Breast cancer	GENE-EXPRESSION; FEATURE-SELECTION; CLASSIFICATION; INTEGRATION; SIGNATURE; KNOWLEDGE; NETWORKS; DISEASES	The discovery of reliable cancer biomarkers is crucial for accurate early detection and clinical diagnosis. One of the strategies is by identifying expression-based cancer biomarkers through integrative microarray data analysis. Microarray is a powerful high-throughput technology, which allows a genome-wide analysis of human genes with various biological information. Nevertheless, more studies are needed on improving the predictability of the discovered gene biomarkers, as well as their reproducibility and interpretability, to qualify them for clinical use. This paper proposes an informative top-k class associative rule (iTCAR) method in an integrative framework for identifying candidate genes of specific cancers. iTCAR introduces an enhanced associative classification algorithm that integrates microarray data with biological information from gene ontology, KEGG pathways, and protein-protein interactions to generate informative class associative rules. A new interestingness measurement is used to rank and select class associative rules for building accurate classifiers. The experimental results show that ITCAR has excellent predictability by achieving the average classification accuracy above 90% and the average area under the curve above 0.8. Besides, iTCAR has significant reproducibility and interpretability through functional enrichment analysis and retrieval of meaningful cancer terms. These promising results suggest the proposed method has great potential in identifying candidate genes, which can be further investigated as biomarkers for cancer diseases. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113169	10.1016/j.eswa.2019.113169													
J								Improving consensus clustering with noise-induced ensemble generation	EXPERT SYSTEMS WITH APPLICATIONS										Consensus clustering; Attribute noise; Ensemble generation	ATTRIBUTE NOISE	Because of the negative perception towards noise, it is commonly eliminated in the process of data cleansing prior to the analysis process. Some studies attempt to employ tolerant or robust algorithms to achieve a reliable outcome. One way or another, the impact of noise might be minimized, thus preserving the integrity of discovered knowledge. On the other hand, making good use of noise has recently been investigated and exploited in different contexts, such as in privacy-preserving data mining, single clustering and consensus clustering. Given our initial study of employing uniform random noise in the process of ensemble generation as a way to increase diversity within an ensemble, improved clustering goodness can be obtained at specific levels of noise. To consolidate the aforementioned finding, this paper investigates a rich collection of random noise functions, which can be used to form perturbed data variation within the framework of noise-induced ensemble generation. The effectiveness of this approach which uses different cases for random noise is demonstrated over benchmark datasets from the UCI repository. The results suggest that the noise-induced strategy is generally better than the baseline counterpart, whilst showing uneven improvement with different data patterns. As such, a guideline is provided to make the best use of the proposed method with any new set of data. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113138	10.1016/j.eswa.2019.113138													
J								An improved exact algorithm for a territory design problem with p-center-based dispersion minimization	EXPERT SYSTEMS WITH APPLICATIONS										Territory design; p-center problem; Integer programming; Model reformulation	LOCATION; DISTRICTS; GRASP; MODEL	Territory design deals with the discrete assignment of geographical units into territories with restrictions defined by planning criteria. We propose an exact solution method based on an integer programming model with the objective of minimizing a p-center dispersion measure. The solution approach is an iterative algorithm that uses different subproblems to validate if, for given values of the objective function of the original problem, it is possible to find feasible solutions with at most p territories. This change allows testing various candidate distance values as lower bounds on the optimal solution of the original problem. The aim is to improve these lower bounds at each iteration as we add the necessary constraints to reach a feasible solution. The proposed algorithm performs significantly faster than the best-known exact solution method for this model. Tests for both methodologies were done using a new set of instances with up to 300 nodes. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113150	10.1016/j.eswa.2019.113150													
J								Towards automatically filtering fake news in Portuguese	EXPERT SYSTEMS WITH APPLICATIONS										Fake news; Text categorization; Natural language processing; Machine learning; Corpus construction	DECEPTION DETECTION; ACCURACY	In the last years, the popularity of smartphones and social networks has been contributing to the spread of fake news. Through these electronic media, this type of news can deceive thousands of people in a short time and cause great harm to individuals, companies, or society. Fake news has the potential to change a political scenario, to contribute to the spread of diseases, and even to cause deaths. Despite the efforts of several studies on fake news detection, most of them only cover English language news. There is a lack of labeled datasets of fake news in other languages and, moreover, important questions still remain open. For example, there is no consensus on what are the best classification strategies and sets of features to be used for automatic fake news detection. To answer this and other important open questions, we present a new public and real dataset of labeled true and fake news in Portuguese, and we perform a comprehensive analysis of machine learning methods for fake news detection. The experiments were performed using different sets of features and employing different types of classification methods. A careful analysis of the results provided sufficient evidence to respond appropriately to the open questions. The various evaluated scenarios and the drawn conclusions from the results shed light on the potentiality of the methods and on the challenges that fake news detection presents. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113199	10.1016/j.eswa.2020.113199													
J								Ensemble belief rule base modeling with diverse attribute selection and cautious conjunctive rule for classification problems	EXPERT SYSTEMS WITH APPLICATIONS										Belief rule base; Attribute selection; Classification; Ensemble learning; Cautious conjunctive rule	EXPERT-SYSTEM; DECISION-ANALYSIS; OPTIMIZATION; INFERENCE; METHODOLOGY; PARAMETER; ACTIVATION	Belief rule-based systems have demonstrated its advantages in solving complicated problems with uncertain information. However, the rule combinatorial explosion problem is still a great challenge for belief rule bases (BRBs) when a problem involves a large number of attributes, because existing attempts have not addressed this challenge adequately, e.g., utilization of single attribute selection method to downsize BRBs without considering its inherent weakness, or adjustment of referential values to optimize BRBs without attribute selection. Thus, inspired by ensemble learning, the objective of this paper is to propose an ensemble BRB modeling method to deal with classification problems. First, six attribute selection methods that have different advantages are introduced to select diverse sets of antecedent attributes for constructing multiple BRBs, and all of these BRBs are further trained by parameter learning for diverse belief rule-based systems. Second, due to the fact that each belief rule-based system has different importance and hardly satisfies the assumption of independence, a weight learning method is proposed to determine the weight of each belief rule-based system, and a new analytical cautious conjunctive rule (CCR) is deduced from the recursive CCR, that is suitable for the combination of non- independent individuals, to combine the outputs of all belief rule-based systems. Eight classification datasets from the well- known UCI database are adopted to verify the effectiveness of the proposed BRB modeling method in comparison with the belief rule-based systems constructed by single attribute selection, conventional fuzzy rule-based classifiers, and machine learning-based classifiers. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113161	10.1016/j.eswa.2019.113161													
J								Automatic multiple moving humans detection and tracking in image sequences taken from a stationary thermal infrared camera	EXPERT SYSTEMS WITH APPLICATIONS										Infrared imagery; Human detection; Objects tracking; Particle filter; Similarity measure	PEDESTRIAN DETECTION; VIDEO SURVEILLANCE; VISUAL TRACKING; PEOPLE; FUSION; CLASSIFICATION; FRAMEWORK; PATTERNS; MOTION; SHAPE	Several Particle Filter (PF)-based methods for human tracking in thermal IR image sequences have been proposed in the literature. Unfortunately, the majority of these methods are developed for tracking only a single human. Moreover, this human is manually pre-selected in the first frame of the image sequence, which is not practical for the real case of intelligent and efficient video surveillance system that needs tracking more than one human and without any external operator intervention. To contribute to addressing this need, in this paper, we propose a novel PF-based method that detects and tracks multiple moving humans using a thermal IR camera, without prior knowledge about their number and initial locations in the monitored scene. This method consists of three main parts. In the first one, all the moving objects are extracted from the image sequence by using the Gaussian Mixture Model (GMM) and then, for each extracted object, a combined shape, appearance, spatial and temporal-based similarity function that allows us to detect a human without any prior training of a mathematical model is calculated. The second part consists in tracking the human previously detected by using a PF and an adaptive combination of spatial, intensity, texture and motion velocity cues. In each cue, a model for the detected human is created, and when new observations arrive in the next frames, the similarity distances between each created model and the observed moving regions are calculated. The human tracking is achieved by combining individual similarity distances using adaptive weights, into a PF algorithm. The third part is devoted to detect and handle occlusions by using simple heuristic rules and grayscale Vertical Projection Histogram (VPH). Each part of the proposed method was separately tested on a set of real-world thermal IR image sequences containing background clutters, appearance and disappearance of multiple moving objects, occlusions, illumination and scales changes. A comparative study with several state-of-the-art methods has shown that the proposed method performs consistently better in terms of Center Location Error (CLE) and the Success Rate (SR), and it can also run at speed of about 15 Frame Per Second per human, which is considerably enough for real-time applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113171	10.1016/j.eswa.2019.113171													
J								The path cover problem: Formulation and a hybrid metaheuristic	EXPERT SYSTEMS WITH APPLICATIONS										Vehicle routing problem; Variable neighborhood search; Tabu search; Hybrid variable neighborhood search and tabu search; Vertex-disjoint path cover problem	VEHICLE-ROUTING PROBLEM; PARTICLE SWARM OPTIMIZATION; TABU SEARCH ALGORITHM; GENETIC ALGORITHM; NEIGHBORHOOD SEARCH; HETEROGENEOUS FLEET; SYSTEM; PICKUP; CAR; VRP	This paper introduces the path cover problem (PCP) that is a variant of the capacitated vehicle routing problem (CVRP). In contrast to CVRP, a vehicle route in PCP involves a set of vertex-disjoint paths where a vehicle starts at a customer and finishes at another customer without traveling to the depot. PCP is defined to find a set of service paths to serve a set of customers that aiming to minimize the total traveling cost and hiring cost. This paper develops a hybrid approach that integrates variable neighborhood search (VNS) and tabu search (TS) to solve the problem. This algorithm presents an approach to explore the solution space by utilizing multi-set neighborhood and applying the systematic changing neighborhood of VNS. TS is incorporated into the algorithm to guide the search toward diverse regions. The computational results indicate that the proposed algorithm efficiently solves PCP. (C) 2019 Published by Elsevier Ltd.																	0957-4174	1873-6793				MAY 15	2020	146								113107	10.1016/j.eswa.2019.113107													
J								A hybrid discrete water wave optimization algorithm for the no-idle flowshop scheduling problem with total tardiness criterion	EXPERT SYSTEMS WITH APPLICATIONS										Water wave optimization; No-idle flowshop scheduling problem; Total tardiness; Variable neighborhood search	VARIABLE NEIGHBORHOOD SEARCH; ITERATED GREEDY ALGORITHM; DIFFERENTIAL EVOLUTION; WAIT	The no-idle flowshop has attracted enormous attention owing to its widespread application in the manufacturing industry domain. In this paper, a hybrid discrete water wave optimization algorithm, named HWWO, is presented to solve the NIFSP with total tardiness. In order to improve the quality of a population, an initialize method based on a new priority rule combined with the modified NEH method is proposed to generate a population. In the propagation phase, a self-adaption selection neighborhood search structure is introduced to amplify the search range of waves and balance the exploration and exploitation ability of the HWWO. Afterwards, a variable neighborhood search is adopted to strengthen the local search and maintain the diversity of the population in the breaking phase. In the refraction operation, a perturbation sequence is generated and combined with the local optimal solution found by the breaking operation, in order to generate a new solution, and prevent the algorithm from becoming trapped in the local optimum. Furthermore, the control parameters and time complexity are analyzed. The experimental results and comparisons with the other state-of-the-art algorithms evaluated on Taillard's and Ruiz's benchmark sets reveal that the effectiveness and efficiency of the HWWO outperformed the compared algorithms for solving the NIFSP. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 15	2020	146								113166	10.1016/j.eswa.2019.113166													
J								Automatic Generation of Object Shapes With Desired Affordances Using Voxelgrid Representation	FRONTIERS IN NEUROROBOTICS										affordance; generative design; computer aided design (CAD) algorithms; voxel grids; shape generation; affordance testing	ROBOTICS; FORM	3D objects (artifacts) are made to fulfill functions. Designing an object often starts with defining a list of functionalities or affordances (action possibilities) that it should provide, known asfunctional requirements. Today, designing 3D object models is still a slow and difficult activity, with few Computer-Aided Design (CAD) tools capable to explore the design solution space. The purpose of this study is to explore shape generation conditioned on desired affordances. We introduce an algorithm for generating voxelgrid object shapes which afford the desired functionalities. We follow the principleform follows function, and assume that object forms are related to affordances they provide (their functions). First, we use an artificial neural network to learn a function-to-form mapping from a dataset of affordance-labeled objects. Then, we combine forms providing one or more desired affordances, generating an object shape expected to provide all of them. Finally, we verify in simulation whether the generated object indeed possesses the desired affordances, by defining and executing affordance tests on it. Examples are provided using the affordances contain-ability, sit-ability, and support-ability.																	1662-5218					MAY 14	2020	14								22	10.3389/fnbot.2020.00022													
J								Cooperative spectrum sensing in wireless sensor networks using forager bee's intelligence	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Cognitive radio; Co-operative spectrum sensing; Swam intelligence; Foraging bees		Increasing utilization of wireless sensor networks Leads scarcity problems in spectrum allocation channels. The development of cognitive radio networks over wireless spectrum technology based on both licensed and unlicensed user. The maximum interference of users are mitigated to use unlicensed bandwidth. Due the high interference of bandwidth sharing enormously by secondary users (SUs). To sense the unlicensed user on over problematic condition remains the overhead problems on cognitive radio networks. In this paper, Co-operative spectrum sensing as well as spectrum allocation with Forager Bee's Intelligence algorithm is used. The idle spectrum band in licensed band is identified and allocated to sensor nodes, which reduces the interference and collisions in WSN and avoids packet retransmission. This ensures fair energy in WSNs and Spectrum resources are utilized efficiently.																	1868-5137	1868-5145															10.1007/s12652-020-02067-y		MAY 2020											
J								Deep nonlinear regression least squares polynomial fit to detect malicious attack on IoT devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; Information security; Malware; API calls; Machine learning attack; Least squares polynomial fit; Gadgets attack		The explosion of IoT gadgets which be able to more effortlessly conceded than PCs has prompted an expansion in the existence of IoT-dependent botnet attacks. So as to alleviate this newfangled danger there remains a necessity to grow innovative techniques designed for identifying attacks propelled from conceded IoT gadgets in addition to distinguish among hour as well as millisecond elongated IoT-dependent attacks. Now we suggest and experimentally estimate a Deep Nonlinear Regression Least Squares Polynomial Fit to recognize peculiar system traffic originating as of conceded IoT gadgets. On the way to estimate our strategy, we contaminated 9 business IoT gadgets in our lab through 2 of the most generally acknowledged IoT-dependent botnets, Mirai and BASHLITE. Our estimated outcomes showed our suggested strategy's capacity to precisely and rapidly recognize the attacks as they were being propelled from the conceded IoT gadgets which remained a piece of a botnet. The tests show a truly accuracy 98.75%.																	1868-5137	1868-5145															10.1007/s12652-020-02075-y		MAY 2020											
J								A Decision-centric approach for secure and energy-efficient cyber-physical systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cyber-physical systems; Security; Markov decision process; Energy-efficiency; MATLAB; Modeling	WIRELESS SENSOR NETWORKS; RELIABILITY; PERFORMANCE; CHALLENGES; MANAGEMENT; COST	Cyber-Physical Systems (CPSs) integrate the interdisciplinary fields of computing, networking and control to perform tasks in the real world. CPSs have recently found applications in many battery-powered devices with stringent energy consumption requirements. To ensure secure operation, CPS necessitates sufficient security mechanisms to be incorporated against cyber attacks. However, maximizing energy efficiency and improving security are desirable but contrasting requirements. Towards reducing energy consumption, the optimal strategy for CPS is to initialize the security mechanism dynamically, at the onset of cyberattacks. In the absence of attacks, CPS can deactivate the security mechanism to minimize energy consumption. In the case of CPS, this approach is novel and contrary to the traditional approach of long-term, continual operation of the security mechanism. Towards this goal, we use a decision-centric approach based on Markov Decision Process (MDP) to estimate a threshold upon which the system initiates its security mechanism. We evaluate our proposed mechanism using MATLAB based TrueTime simulator. Evaluation shows that our proposed MDP-based approach achieves maximum energy-savings of 8.26 and 11.05% in defending against Denial- of-Service and Deception attacks, respectively. Further, our approach can be used to develop sustainable CPS designs that balance the trade-off between energy-efficiency and security.																	1868-5137	1868-5145															10.1007/s12652-020-01995-z		MAY 2020											
J								Secure storage allocation scheme using fuzzy based heuristic algorithm for cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Fuzzy system; Heuristic algorithm; K-nearest neighbors; RSA algorithm	SEARCH	Cloud computing is a paradigm in the modern era for the development of secure storage and secure job scheduling process in a cloud-based system. The cloud technology provides a user interface in the sense of raw data management with the relational databases to improve flexibility and scalability. Fuzzy-based Heuristic algorithm is proposed for secure storage allocation in a cloud environment. In this work, client systems are clustered using a fuzzy-based rule mechanism. The server generates a distributed public key using RSA for ensuring security and resolve memory recycling issues. It enhances the usage of the client over the cloud data storage in secure manner. It also improves the efficiency of data storage globally by implementing heuristic techniques. The K-nearest neighbors approach is used for efficient query searching for data storage in the clusters by the client. The proposed load-balanced scheduling approach enhances power consumption and efficiency. In the simulation, the results reveal that the proposed scheme provides better performance than existing approaches in terms of load balancing with security.																	1868-5137	1868-5145															10.1007/s12652-020-02082-z		MAY 2020											
J								Prediction of students' procrastination behaviour through their submission behavioural pattern in online learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Educational data mining; Predication of students' performance; Submission behavioural pattern; Higher education; Procrastination behaviour; Online learning	ACADEMIC PROCRASTINATION; PERFORMANCE; TIME	Prediction of students' performance has been reported as a vital task which enables educators to take necessary actions to improve students' learning. Numerous studies have concluded that students with lower procrastination tendencies archive more compared to those with higher procrastination tendencies. In this study, a new method is proposed to predict students' procrastination tendencies discerned from their submission behavioural patterns in online learning. In this method, feature vectors signifying students' submission patterns on homework are firstly drafted. Next, an ensemble clustering method is employed to optimally sort students into various categories of procrastination: procrastinator, procrastinator candidate, and non-procrastinator. Lastly, various classification methods are assessed to discern which one best predicts students' procrastination tendencies. The efficacy of this approach is assessed through the data from a course comprised of 242 students at the University of Tartu in Estonia. Our study found that our method correctly identifies student procrastination from submission pattern data with 97% accuracy, and that the best performing classifier is linear support vector machine. Investigating the effect of different number of features (homework) on performance of clustering and classification methods indicate that finding the optimal number of feature to use in both clustering and classification methods is a vital task as it could potentially affect prediction power of our approach. More specifically, the results show that in our proposed approach, unlike clustering methods that show a better performance with lower number of features, classification methods mostly tend to show a better performance with larger number of features.																	1868-5137	1868-5145															10.1007/s12652-020-02041-8		MAY 2020											
J								Extreme learning machine-based super-twisting repetitive control for aperiodic disturbance, parameter uncertainty, friction, and backlash compensations of a brushless DC servo motor	NEURAL COMPUTING & APPLICATIONS										Aperiodic disturbance; Backlash; Chattering; Coulomb friction; Extreme learning machine; Neural network; Nonlinear control; Parameter uncertainty; Periodic signal; Repetitive control; Super-twisting control; Viscous friction	NONLINEAR-SYSTEMS; MECHANICAL SYSTEMS; DESIGN	This paper presents an extreme learning machine-based super-twisting repetitive control (ELMSTRC) to improve the tracking accuracy of periodic signal with less chattering. The proposed algorithm is robust against the plant uncertainties caused by mass and viscous friction variations. Moreover, it compensates the nonlinear friction and the backlash by using extreme learning machine based super-twisting algorithm. Firstly, a repetitive control is designed to track the periodic reference and compensate the viscous friction. Then, a stable extreme learning machine-based super-twisting control is constructed to compensate the aperiodic disturbance, nonlinear friction, backlash and plant uncertainties. The stability of ELMSTRC system is analysed based on Lyapunov stability criteria. The proposed algorithm is verified on a brushless DC servo motor with various loading, backlash and friction conditions. The simulation and experimental comparisons highlight the advantages of the proposed algorithm.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14483	14495		10.1007/s00521-020-04965-w		MAY 2020											
J								Attentive gated neural networks for identifying chromatin accessibility	NEURAL COMPUTING & APPLICATIONS										Gated convolutional networks; Gated recurrent units; Attention mechanism; Chromatin accessibility	GENOME; ELEMENTS	Accessible chromatin is associated strongly with active gene regulatory regions. Enhancers and promoters commonly occur in accessible chromatin, and systematically discovering functional sites is indispensable at the whole genome level. However, biological experiments are expensive and time-consuming, and currently, computational methods could not completely learn the hidden key regulatory patterns of genomic contexts. Moreover, the feature encoding methods of genetic sequences often ignore position information among sequences, and accurately identifying accessibility regions greatly depends on capturing more informative sequence features. To address the issues, we first encode the DNA sequences by using position embeddings, which are produced by integrating position information of the original sequences into embedding vectors and then propose a novel deep learning framework, called attentive gated neural networks (AGNet), to automatically extract complex patterns for predicting chromatin accessibility from DNA sequences. Specifically, we combine gated neural networks (GNNs) with dual attention to extract multiple patterns and long-term associations merely from DNA sequences. Experimental results on five cell-type datasets show that AGNet obtains the best performance than the published methods for the accessibility prediction. Furthermore, the results not only reveal that AGNet can learn more regulatory patterns that underlie DNA sequences, but also validate the significance of position embeddings for the accessibility prediction.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15557	15571		10.1007/s00521-020-04879-7		MAY 2020											
J								Digitized rotations of 12 neighbors on the triangular grid	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Digital geometry; Digital image processing; Discretized rotations; Discrete motions; Non-traditional grid; Neighborhood maps	TOMOGRAPHY; ALGORITHMS	There are various geometric transformations, e.g., translations, rotations, which are always bijections in the Euclidean space. Their digital counterpart, i.e., their digitized variants are defined on discrete grids, since most of our pictures are digital nowadays. Usually, these digital versions of the transformations have different properties than the original continuous variants have. Rotations are bijective on the Euclidean plane, but in many cases they are not injective and not surjective on digital grids. Since these transformations play an important role in image processing and in image manipulation, it is important to discover their properties. Neighborhood motion maps are tools to analyze digital transformations, e.g., rotations by local bijectivity point of view. In this paper we show digitized rotations of a pixel and its 12-neighbors on the triangular grid. In particular, different rotation centers are considered with respect to the corresponding main pixel, e.g. edge midpoints and corner points. Angles of all locally bijective and non-bijective rotations are described in details. It is also shown that the triangular grid shows better performance in some cases than the square grid regarding the number of lost pixels in the neighborhood motion map.																	1012-2443	1573-7470				AUG	2020	88	8					833	857		10.1007/s10472-019-09688-w		MAY 2020											
J								Predictive intelligence of reliable analytics in distributed computing environments	APPLIED INTELLIGENCE										Predictive intelligence; Exploration query prediction; Centroid refinement; Machine learning		Lack of knowledge in the underlying data distribution in distributed large-scale data can be an obstacle when issuing analytics & predictive modelling queries. Analysts find themselves having a hard time finding analytics/exploration queries that satisfy their needs. In this paper, we study how exploration query results can be predicted in order to avoid the execution of 'bad'/non-informative queries that waste network, storage, financial resources, and time in a distributed computing environment. The proposed methodology involves clustering of a training set of exploration queries along with the cardinality of the results (score) they retrieved and then using query-centroid representatives to proceed with predictions. After the training phase, we propose a novel refinement process to increase the reliability of predicting the score of new unseen queries based on the refined query representatives. Comprehensive experimentation with real datasets shows that more reliable predictions are acquired after the proposed refinement method, which increases the reliability of the closest centroid and improves predictability under the right circumstances.																	0924-669X	1573-7497				OCT	2020	50	10					3219	3238		10.1007/s10489-020-01712-5		MAY 2020											
J								Multistage Decision Framework for the Selection of Renewable Energy Sources Based on Prospect Theory and PROMETHEE	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Renewable energy sources; Picture linguistic fuzzy sets; PROMETHEE II; Prospect theory; Multi-criteria decision making	PROJECT PERFORMANCE EVALUATION; MAKING APPROACH; ELECTRICITY-GENERATION; MULTICRITERIA; SUSTAINABILITY; SYSTEMS; TECHNOLOGIES; RESOURCES; NUMBERS; TURKEY	The selection of appropriate renewable energy sources (RESs) for a region is a complex multi-criteria decision-making problem because the RES selection process involves many factors, such as economic, environmental, technological and societal. Hence, to address this problem, this paper proposes a multistage framework for selecting a suitable RES alternative by integrating picture linguistic fuzzy numbers (PLFNs), preference ranking organization method for enrichment evaluations II (PROMETHEE II) and prospect theory (PT). First, PLFNs are used to describe the evaluation information. Second, using the proposed picture linguistic fuzzy weighed Heronian distance measurement, this paper proposes an extended maximizing deviation method that can capture the interrelationships among criteria. Third, an extended PROMETHEE II, which considers the bounded rationality of decision-makers, combined with PT is developed. Finally, the proposed framework solves a RES selection problem in northwest of China. The result shows solar energy is the best choice, followed by wind, hydro and biomass energy. Sensitivity analysis is conducted to explore the effects of the parameters on the results. The advantages of the proposed method are verified through a comparative analysis.																	1562-2479	2199-3211				JUL	2020	22	5					1535	1551		10.1007/s40815-020-00858-1		MAY 2020											
J								Low power area optimized and high speed carry select adder using optimized half sum and carry generation unit for FIR filter	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Area efficient VLSI circuit; Binary adder; Carry select adder; High speed adder; Low power adder		In VLSI system design, one of the most significant areas of on-going research is high speed with low power system design. Usually the speed of the VLSI circuits like binary adders can be improved by more parallelism but, this more parallelism idea will increase the power consumption and area of the circuit. Therefore there is a trade-off between speed and power/area. A carry select adder is one of the fast binary adders but it consumes more power and area. In this paper, we proposed a low power and area efficient and high speed binary carry select adder (CSLA). The half sum and carry generator (HSCG) of the proposed CSLA is designed with a new methodology, that the HSCG output are estimated using input A and B not with carry-in hence it is faster and results in less area leads to low power consumption. Moreover, the NAND gate based circuit is used to compute the intermediate carries within the block and carry-out of the block which are derived from HSCG output along with carry-in. Similarly, the full sum of the proposed adder is generated from XNOR gate based circuitry by utilizing the intermediate carries of the block. The existing and proposed CSLAs are synthesized with the Synopsys EDA tool using 32 nm CMOS technology. The performance of the existing and proposed designs are analyzed and the results shows that the proposed 64-bit CSLA exhibits average reduction of 43% in power and average of 25% less utilization in terms of area compared to existing design. This proposed CSLA is used in a FIR Filter and obtained significant reduction in ADP and PDP.																	1868-5137	1868-5145															10.1007/s12652-020-02062-3		MAY 2020											
J								Data mining for fast and accurate makespan estimation in machining workshops	JOURNAL OF INTELLIGENT MANUFACTURING										Makespan estimation; Ensemble of BPNN; Gene expression programming; Clustering; Genetic algorithm	SHOP SCHEDULING PROBLEM; CYCLE TIME; ENSEMBLE; PREDICTION; OPTIMIZATION; REGRESSION; SVM	The fast and accurate estimation of makespan is essential for the determination of the delivery date and the sustainable development of the enterprise. In this paper, a high-quality training dataset is constructed and an adaptive ensemble model is proposed to achieve fast and accurate makespan estimation. First, both the logistics features extracted by the Pearson correlation coefficient and the new meaningful nonlinear combination features dug out by gene expression programming are first involved in this paper for constructing a high-quality dataset. Secondly, an improved clustering with elbow criterion and a resampling operation are applied simultaneously to generate representative subsets; and correspondingly, several back propagation neural network (BPNN) with the architecture optimized by genetic algorithm are trained by these subsets respectively to generate effective diverse learners; and then, a K-nearest neighbor based dynamic weight combination strategy which is sensitive to current testing sample is proposed to make full use of the learner's positive effects and avoid its negative effects. Finally, the results of effective experiments prove that both the newly involved features and the improvements in the proposed ensemble are effective. In addition, comparison experiments confirm that the proposed enhanced ensemble of BPNNs outperforms significantly the prevailing approaches, including single, ensemble and hybrid models. And hence, the proposed model can be utilized as a convenient and reliable tool to support customer order acceptance.																	0956-5515	1572-8145															10.1007/s10845-020-01585-y		MAY 2020											
J								Shape Analysis of Surfaces Using General Elastic Metrics	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Shape spaces; Vector valued one-forms; Elastic metrics; SRNF metric; Surface registration	SOBOLEV METRICS; SPACE; PARAMETERIZATION	In this article, we introduce a family of elastic metrics on the space of parametrized surfaces in 3D space using a corresponding family of metrics on the space of vector-valued one-forms. We provide a numerical framework for the computation of geodesics with respect to these metrics. The family of metrics is invariant under rigid motions and reparametrizations; hence, it induces a metric on the "shape space" of surfaces. This new class of metrics generalizes a previously studied family of elastic metrics and includes in particular the Square Root Normal Field (SRNF) metric, which has been proven successful in various applications. We demonstrate our framework by showing several examples of geodesics and compare our results with earlier results obtained from the SRNF framework.																	0924-9907	1573-7683				OCT	2020	62	8					1087	1106		10.1007/s10851-020-00959-4		MAY 2020											
J								Label similarity-based weighted soft majority voting and pairing for crowdsourcing	KNOWLEDGE AND INFORMATION SYSTEMS										Crowdsourcing; Label aggregation; Specific quality; Overall quality; Label similarity	QUALITY	Crowdsourcing services provide an efficient and relatively inexpensive approach to obtain substantial amounts of labeled data by employing crowd workers. It is obvious that the labeling qualities of crowd workers directly affect the quality of the labeled data. However, existing label aggregation strategies seldom consider the differences in the quality of workers labeling different instances. In this paper, we argue that a single worker may even have different labeling qualities on different instances. Based on this premise, we propose four new strategies by assigning different weights to workers when labeling different instances. In our proposed strategies, we first use the similarity among worker labels to estimate the specific quality of the worker on different instances, and then we build a classifier to estimate the overall quality of the worker across all instances. Finally, we combine these two qualities to define the weight of the worker labeling a particular instance. Extensive experimental results show that our proposed strategies significantly outperform other existing state-of-the-art label aggregation strategies.																	0219-1377	0219-3116				JUL	2020	62	7					2521	2538		10.1007/s10115-020-01475-y		MAY 2020											
J								crowd: A Visual Tool for Involving Stakeholders into Ontology Engineering Tasks	KUNSTLICHE INTELLIGENZ										Ontology engineering; Semantic web tools; Conceptual modelling	WEB	We present crowd, a web-based visual tool for ontology engineering tasks. Its aim is to involve ontology developers and domain experts into a collaborative comprehension and design of conceptual models, enhancing the communication between them and assessing their quality by fully integrating automatic reasoning in the tool. In this paper we briefly describe the initial requirements, architecture and user interface, and make an evaluation based on a use case and a comparison with related tools.																	0933-1875	1610-1987				SEP	2020	34	3			SI		365	371		10.1007/s13218-020-00657-8		MAY 2020											
J								Extension of labeled multiple attribute decision making based on fuzzy neighborhood three-way decision	NEURAL COMPUTING & APPLICATIONS										Labeled multiple attribute decision making; Three-way decision; Fuzzy neighborhood; Attribute selection	WEIGHT ASSIGNMENT; BAYES RISK; REDUCTION; DISTANCE; SETS	Weight assignment of attribute is considered as a key part of multiple attribute decision making (MADM), and this is also applicable to labeled multiple attribute decision making (LMADM) that is a decision theory specially proposed for the dataset with labels. However, regarding the decision making of massive data characterized by redundancy and uncertainty, more means including attribute selection and uncertainty processing should be considered to solve these problems. Based on the traditional framework of LMADM, this paper deduces a new framework to adapt to the decision making of massive data. With respect to the uncertainty generated from data and decision process, a fuzzy neighborhood three-way decision model (FN3WD) is proposed, in which the fuzzy neighborhood relationship can address the uncertainty of data and the three-way decision theory can deal with the uncertainty of decision process. Finally, the experimental results illustrate the superiority of FN3WD and verify the effectiveness of the proposed framework of the extended LMADM by using some benchmarked datasets and the Commercial Modular Aero-Propulsion System Simulation dataset.																	0941-0643	1433-3058															10.1007/s00521-020-04946-z		MAY 2020											
J								Hybrid channel based pedestrian detection	NEUROCOMPUTING										Pedestrian detection; Handcrafted features channels; CNN feature channels; Rol-pooling; Feature combination		Pedestrian detection has achieved great improvements with the help of Convolutional Neural Networks (CNNs). CNN can learn high-level features from input images, but the insufficient spatial resolution of CNN feature channels (feature maps) may cause a loss of information, which is harmful especially to small instances. In this paper, we propose a new pedestrian detection framework, which extends the successful RPN+BF framework to combine handcrafted features and CNN features. RoI-pooling is used to extract features from both handcrafted channels (e.g. HOG+LUV, CheckerBoards or RotatedFilters) and CNN channels. Since handcrafted channels always have higher spatial resolution than CNN channels, we apply RoI-pooling with larger output resolution to handcrafted channels to keep more detailed information. Our ablation experiments show that the developed handcrafted features can reach better detection accuracy than the CNN features extracted from the VGG-16 net, and a performance gain can be achieved by combining them. Experimental results on Caltech pedestrian dataset with the original annotations and the improved annotations demonstrate the effectiveness of the proposed approach. When using a more advanced RPN in our framework, our approach can be further improved and get competitive results on both benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						1	8		10.1016/j.neucom.2019.12.110													
J								6D object pose estimation via viewpoint relation reasoning	NEUROCOMPUTING										6D object pose estimation; Keypoints detection; Convolutional neural networks; Geometric reasoning		Estimating the 6D object pose is a very challenging task in computer vision. The main difficulty is mapping the object from RGB images to 3D space. In this paper, we present a novel two-stage method for estimating the 6D object pose by using the 2D keypoints of an object and its 2D bounding box. There are two stages in our method. The first stage detects the 2D keypoints and 2D bounding boxes of objects by a stable end-to-end framework. During the training phase, this framework uses viewpoint transformation information and object saliency regions to learn geometrically and semantically consistent keypoints. Then the 6D poses of objects are calculated by a series of geometric reasoning algorithms in the second stage. Experiments show that our method achieves accurate pose estimation and robust to occluded and cluttered scenes. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						9	17		10.1016/j.neucom.2019.12.108													
J								A memristor-based circuit design for generalization and differentiation on Pavlov associative memory	NEUROCOMPUTING										Memristor; Generalization; Differentiation; Pavlov associative memory; Circuit design	SYSTEMS; MODEL	The traditional Pavlov associative memory contains the process of learning and forgetting, which correspond to the reinforcement and extinction in classical conditional reflex respectively. In fact, besides reinforcement and extinction, classical conditional reflex also contains generalization and differentiation. In this paper, a memristor-based circuit is designed to implement generalization and differentiation based on Pavlov associative memory. This circuit can respond to one kind of conditional stimulus after the initial learning, then when another similar stimulus is applied to the circuit, the circuit responds similarily, this is the phenomenon of generalization. To make the circuit be sufficiently cognizant of these two similar stimuli and ultimately distinguish them, a special training method, which selectively reinforces one stimulus while inhibits another similar stimulus, is used to train the whole circuit. As a result, when these two similar stimuli are applied to the circuit separately again, it can respond to them differently. This is the phenomenon of differentiation. Finally, the correctness of implementing these functions described above are demonstrated by simulation results in PSPICE. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						18	26		10.1016/j.neucom.2019.12.106													
J								A three-level Multiple-Kernel Learning approach for soil spectral analysis	NEUROCOMPUTING										Multiple Kernel Learning (MKL); Kernel alignment; Heterogeneous source combination; Soil organic carbon; Soil texture; VNIR-SWIR spectroscopy	ORGANIC-MATTER; REFLECTANCE SPECTROSCOPY; PREDICTION; CARBON; REGRESSION; ALGORITHMS; CHEMISTRY; MODEL	To ensure the sustainability of the soil ecosystem, which is the basis for food production, efficient large-scale baseline predictions and trend assessments of key soil properties are necessary. In that regard, visible, near-infrared, and shortwave infrared (VNIR-SWIR) spectroscopy can provide an alternative for the expensive wet chemistry. In this paper, we examined the application of the Multiple-Kernel Learning (MKL) approach to soil spectroscopy by integrating the information from heterogeneous features. In particular, the proposed three-level MKL framework acts in the following way: at the first level, it uses multiple kernels at each spectral feature (wavelength) to maximize the information of each band. At the second level, it performs implicit feature selection at the spectral source level, enabling it to provide interpretable results. Finally, at the third level of integration it combines the complementary information contained within a pool of spectral sources, each derived from its own set of pre-processing techniques. Additionally, at this stage, the proposed approach is also capable of fusing heterogeneous sources of information, such as auxiliary predictors, which can assist the spectral predictions. The experimental analysis was conducted using the pan-European LUCAS (Land Use/Cover Area frame statistical Survey) topsoil database, with a goal to predict from the VNIR-SWIR spectra the concentration of soil organic carbon (SOC), a key indicator for agricultural productivity and environmental resilience. The particle size distribution which describes the soil texture was selected as the set of auxiliary predictors. The proposed MKL framework was compared with other state-of-the-art approaches, and the results indicated that it attains the best performance in terms of accuracy, whilst at the same time producing interpretable results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						27	41		10.1016/j.neucom.2020.01.008													
J								A Multilayer Interval Type-2 Fuzzy Extreme Learning Machine for the recognition of walking activities and gait events using wearable sensors	NEUROCOMPUTING										Multilayer Neural Networks (ML-NNs); Fuzzy Autoencoders (FAEs); Interval Type-2 fuzzy logic system (IT2FLSs); Wearable sensors; Kernel-based ELM; Direct-defuzzification method; Extreme learning machine	NEURAL-NETWORK; LOGIC SYSTEMS; REDUCTION	In this paper, a novel Multilayer Interval Type-2 Fuzzy Extreme Learning Machine (ML-IT2-FELM) for the recognition of walking activities and Gait events is presented. The ML-IT2-FELM uses a hierarchical learning scheme that consists of multiple layers of IT2 Fuzzy Autoencoders (FAEs), followed by a final classification layer based on an IT2-FELM architecture. The core building block in the ML-IT2-FELM is an IT2-FELM, which is a generalised model of the Interval Type-2 Radial Basis Function Neural Network (IT2-RBFNN) and that is functionally equivalent to a class of simplified IT2 Fuzzy Logic Systems (FLSs). Each FAE in the ML-IT2-FELM employs an output layer with a direct-defuzzification process based on the Nie-Tan algorithm, while the IT2-FELM classifier includes a Karnik-Mendel type-reduction method (KM). Real data was collected using three inertial measurements units attached to the thigh, shank and foot of twelve healthy participants. The validation of the ML-IT2-FELM method is performed with two different experiments. The first experiment involves the recognition of three different walking activities: Level-Ground Walking (LGW), Ramp Ascent (RA) and Ramp Descent (RD). The second experiment consists of the recognition of stance and swing phases during the gait cycle. In addition, to compare the efficiency of the ML-IT2-FELM with other ML fuzzy methodologies, a kernel-based ML-IT2-FELM that is inspired by kernel learning and called KML-IT2-FELM is also implemented. The results from the recognition of walking activities and gait events achieved an average accuracy of 99.98% and 99.84% with a decision time of 290.4ms and 105ms, respectively, by the ML-IT2-FELM, while the KML-IT2-FELM achieved an average accuracy of 99.98% and 99.93% with a decision time of 191.9ms and 94ms. The experiments demonstrate that the ML-IT2-FELM is not only an effective Fuzzy Logic-based approach in the presence of sensor noise, but also a fast extreme learning machine for the recognition of different walking activities. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						42	55		10.1016/j.neucom.2019.11.105													
J								A novel patch-based nonlinear matrix completion algorithm for image analysis through convolutional neural network	NEUROCOMPUTING										Nonlinear matrix completion; Laplace distribution; Patch-based completion algorithm; Deep convolutional neural network	FACTORIZATION; RECOVERY; MODEL; NORM	Matrix completion is extensively studied due to its wide applications in science and technology. In this paper, we concentrate our study on the matrix completion problem for image analysis tasks due to their immense importance and pervasive use in many fields. A rich collection of models has been proposed to capture both linear and nonlinear relationships latent in a matrix. Even though nonlinear models possess more powerful matrix completion capabilities than their linear counterparts, these models generally carry higher model complexity and tuning difficulties. To take advantage of the superior discriminative power offered by nonlinear models while curbing its deployment overheads, this paper proposes a novel nonlinear matrix completion model utilizing a deep learning-based approach. In contrast to existing nonlinear models, the new model carefully explores and exploits spatial locality among adjacent matrix elements exhibited in patches of various sizes and locations in a target matrix. Building upon this idea, a new patch-based nonlinear matrix completion algorithm is designed. The algorithm leverages a convolutional neural network to learn the predictive relationship between a matrix element and its surrounding elements through an end-to-end trainable fashion, leading to a capable and easy-to-deploy nonlinear matrix completion solution. To identify an optimal patch size suited for tackling a given matrix completion task without exhaustively enumerating all candidate patch sizes, the new algorithm is coupled with a fast stochastic search procedure, yielding a good trade-off between computational efficiency and accuracy. Extensive experiments are conducted to validate the effectiveness and advantages of the proposed algorithm for nonlinear matrix completion problem in comparison with a series of state-of-the-art algorithms. Experimental results consistently demonstrate the superiority of the new algorithm in completing images with a variety of random and textual noises. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 14	2020	389						56	82		10.1016/j.neucom.2020.01.037													
J								Near-optimal neural-network robot control with adaptive gravity compensation	NEUROCOMPUTING										Nonlinear optimal control; Direct adaptive control; Neural-adaptive control; Overlearning; Weight drift; Bursting; Cerebellar Model Articulation Controller; Elastic-joint robot	OPTIMAL-CONTROL DESIGN; NONLINEAR-SYSTEMS; TIME-SYSTEMS	Adaptive nonlinear optimal control methods, as proposed in the literature, give rise to some questions around practical implementation in robotics, especially how to find a solution in a reasonable time and how to deal with gravity. This paper proposes a method to solve these problems by using a neural network with local basis-function domains, specifically the Cerebellar Model Articulation Controller (CMAC). The algorithm uses the local domains in order to keep track of the value of local cost-functionals. In this way, it can freeze the learning of the network's weights in a feedforward-like component in the CMAC when the bias has been overcome identified by using an error-based cost-functional e.g. automatic gravity compensation in a robot. After the feedforward component has been established, the algorithm then starts to learn another set of weights which contribute to feedback-like terms in the CMAC and these weights get frozen when they no longer reduce a cost-functional that includes additional control effort e.g. in a robot the control effort beyond that needed to compensate for gravity is penalized. Lyapunov methods guarantee uniformly ultimately bounded signals and ensure weight drift and bursting do not occur. One advantage is that the training time for finding a near-optimal control does not increase over previous neural-adaptive methods. Another advantage is that penalizing the control effort in a search for optimization does result in any steady-state error due to gravity. Simulations show that the proposed method significantly outperforms a standard adaptive-CMAC control using e-modification, without increasing control effort or training time. An experimental flexible-joint robot verifies that the adaptive method significantly outperforms a linear quadratic regulator. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 14	2020	389						83	92		10.1016/j.neucom.2020.01.026													
J								Building interactive sentence-aware representation based on generative language model for community question answering	NEUROCOMPUTING										Community questions answering; Semantic matching; Representation learning; Recurrent neural network; Attention mechanism		Semantic matching between question and answer sentences involves recognizing whether a candidate answer is relevant to a particular input question. Given the fact that such matching does not examine a question or an answer individually, context information outside the sentence should be considered equally important to the within-sentence syntactic context. This motivates us to design a new question-answer matching model, built upon a cross-sentence, context-aware, bi-directional long short-term memory architecture. The interactive attention mechanisms are proposed which automatically select salient positional sentence representations, that contribute more significantly towards the relevance between two question and answer. A new quantity called context information jump is proposed to facilitate the formulation of the attention weights, and is computed via the joint states of adjacent words. An interactive-aware sentence representation is constructed by connecting a combination of multiple sentence positional representations to each hidden representation state. In the experiments, the proposed method is compared with existed models, using four public community datasets, and the evaluations show that it is very competitive. In particular, it offers 0.32%-1.8% improvement over the best performing model for three out of four datasets, while for the remaining one performance is around 0.2% of the best performer. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						93	107		10.1016/j.neucom.2019.12.107													
J								Single image dehazing based on learning of haze layers	NEUROCOMPUTING										Image processing; Deep learning; Image dehazing; Convolutional neural network	VISIBILITY RESTORATION; REMOVAL; NETWORK; WEATHER	This paper proposes a new haze layer based single image dehazing algorithm. The residual images, which exists between the hazy images and the clear images, will be firstly obtained by haze layers through an end-to-end mapping from the original hazy images. Then the designed convolutional neural network can remove the residual image from the given hazy image to obtain a recovered dehazed image. In order to remove the halos and block artefacts in the dehazed images, a guided filter is applied before generating the final dehazed images, which will also increase the reality of the result images. Since the proposed haze layer based dehazing algorithm directly learns the residual images, it enjoys a relatively high learning rate and low computation. We tested and compared the proposed algorithm with other dehazing methods in various variables, such as the density of the fog and the testing scenes, e.g. real-world images or synthetic images. Experimental results showed the advancement of the proposed method in many metrics. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						108	122		10.1016/j.neucom.2020.01.007													
J								Stability-driven non-negative matrix factorization-based approach for extracting dynamic network from resting-state EEG	NEUROCOMPUTING										Dynamic functional networks; Interpretable subnetworks; Matrix decomposition; Resting-state EEG; Stability selection	GRAPH-THEORETICAL ANALYSIS; FUNCTIONAL CONNECTIVITY; BRAIN NETWORK; PHASE SYNCHRONIZATION; FMRI DATA; AUTISM; RECONFIGURATION; ORGANIZATION; COMPONENTS; CHILDREN	Human behavior in health and disease is associated with dynamic functional networks, which are characterized as hierarchical ensembles with segregated spatial architecture and corresponding temporal architecture. How best to extract dynamical functional network structure and coefficient time series from neural data remains an open problem. We developed a new method, which called stability-driven non-negative matrix factorization (staNMF), that combines non-negative matrix factorization with a novel data-driven model selection criterion to decompose dynamic functional networks into sets of superposed subnetworks. To evaluate our method, we first use simulated noisy connectivity networks to demonstrate that staNMF is able to find the correct number of subnetworks, obtain the connectivity matrix of each subnetwork, and recover the corresponding activation coefficient time series. We then apply staNMF to a resting state EEG dataset recorded from 41 five-year-old children with autism spectrum disorder (ASD) and 44 age-matched typically-developing (TD) children. StaNMF extracted several robust and biologically interpretable subnetworks for which significant differences between ASD and TD could be found. Specifically, the strength of the activation coefficient time series of one identified local connections subnetwork was higher in ASD children than in TD. Several subnetworks that had interhemispheric, long-range connections had lower energy of activation coefficient time series in ASD compared to TD. In conclusion, the analyses on simulated and real EEG data demonstrate that staNMF can be used to characterize dynamic functional networks, making it a promising tool for investigating disorders using resting-state EEG. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						123	131		10.1016/j.neucom.2020.01.071													
J								Imaging brain extended sources from EEG/MEG based on variation sparsity using automatic relevance determination	NEUROCOMPUTING										EEG/MEG source imaging; Variation sparsity; Automatic relevance determination (ARD); ADMM	CORTICAL CURRENT-DENSITY; SOURCE RECONSTRUCTION; ELECTROMAGNETIC TOMOGRAPHY; QUANTITATIVE-ANALYSIS; SOURCE LOCALIZATION; BAYESIAN FRAMEWORK; EEG; MEG; ALGORITHM; INFERENCE	Estimating the extents and localizations of extended sources from noninvasive EEG/MEG signals is challenging. In this paper, we have proposed a fully data driven source imaging method, namely Variation Sparse Source Imaging based on Automatic Relevance Determination (VSSI-ARD), to reconstruct extended cortical activities. VSSI-ARD explores the sparseness of current sources on the variation domain by employing ARD prior under empirical Bayesian framework. With convex analysis, the sources are efficiently obtained by solving a series of reweighting L-21-norm regularization problems with ADMM. By virtue of the iterative reweighting process and sparse signal processing techniques, VSSI-ARD gets rid of the small amplitude dipoles that are more probably outside the extent of underlying sources. With the sparsity enforced on the edges using ARD prior, the estimations show clear boundaries between active and background regions without subjective thresholds. Validation with both simulated and human experimental data indicates that VSSI-ARD not only estimates the localizations of sources, but also provides relatively useful and accurate information about the extents of cortical activities. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						132	145		10.1016/j.neucom.2020.01.038													
J								Effective and scalable causal partitioning based on low-order conditional independent tests	NEUROCOMPUTING										Causal inference; High-dimensionality; Constraint-based method; Conditional independence test	DECOMPOSITION	Recovering causal relationships from observed data is crucial to a variety of applications. Due to the curse of dimensionality, general causal discovery methods such as constraint-based methods and functional model based methods are not quite effective and efficient for large and high-dimensional data sets. Thus, some causal partitioning methods have been proposed to handle this problem. However, existing causal partitioning methods rely on high-order conditional independent (CI) tests, which makes them inefficient in handling dense causal graphs. Therefore, high-dimensionality is still a big challenge to these methods. In this work, we propose a new split-and-merge strategy to enable effective and scalable causality discovery. Different from the existing methods, our method uses only low-order CI tests, can get more accurate results and is applicable to various scenarios. We provide both theoretic analysis and empirical evaluation on the proposed method. Experiments on various real-world causal graphs show that the proposed method outperforms the stat-of-the-art method in terms of accuracy, efficiency and scalability. For high-dimensional cases, our method is much faster than the counterpart by one to three orders of magnitudes. (C) 2020 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 14	2020	389						146	154		10.1016/j.neucom.2020.01.021													
J								Multitarget prediction using an aim-object-based asymmetric neuro-fuzzy system: A novel approach	NEUROCOMPUTING										Aim-object-based asymmetric; Neuro-fuzzy system; Aim-object layer; Sphere complex fuzzy set; Multitarget prediction; Hybrid learning algorithm	TIME-SERIES; FUNCTION APPROXIMATION; OPTIMIZATION; NETWORKS	This paper proposes an aim-object-based asymmetric neuro-fuzzy system that is different from conventional models in two ways. First, this system has an asymmetric structure with different numbers of neurons in the premise and consequent layers. Secondly, with the assistance of the sphere complex fuzzy set, depending on the application, our model can alter the number of outputs. In addition, a hybrid learning algorithm combining the whale optimization algorithm and the recursive least-square estimator is proposed to optimize the proposed model. The results of the experiment show that the proposed model can simultaneously predict multiple targets with fewer parameters and maintain a performance level similar to that of the conventional neuro-fuzzy system. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						155	169		10.1016/j.neucom.2019.12.113													
J								Recurrent reverse attention guided residual learning for saliency object detection	NEUROCOMPUTING										Salient object detection; Deep learning; Deep convolutional neural networks; Attention module		Deep convolutional neural networks (CNNs) have been widely applied to saliency object detection (SOD) with promising performance. However, the coarse feature resolutions due to a series of pooling and convolutional operations in CNNs considerably decrease the original image resolution, resulting in losing spatial details and fine structures, especially on salient object boundaries. To address this issue, we present a recurrent reverse attention based residual learning network for SOD. We first construct a pair of low-and high-level integrated feature representations by aggregating two groups of low-and high-level feature maps, respectively, among which we design a novel joint feature pyramid pooling module to increase the high-level feature resolution. Afterwards, we progressively learn to refine the residual between each side-output saliency prediction and the ground-truth in a cascaded fashion by alternatively using the high-level and low-level integrated features. Each residual learning module contains a recurrent reverse attention module that focuses on the residual region outside the current predicted salient regions, guiding the network to learn the missing salient parts and details. Finally, we develop a simple yet effective boundary detection loss to compensate the detailed texture loss. Extensive evaluations on six popular SOD benchmark datasets demonstrate remarkable performance boosting of our proposed approach compared with state-of-the-art methods. Especially, our approach runs in real-time with a speed of 32 fps. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						170	178		10.1016/j.neucom.2019.12.109													
J								Crafting adversarial example with adaptive root mean square gradient on deep neural networks	NEUROCOMPUTING										Adversarial example; Adaptive gradient; Root mean square; Perturbation		Deep Neural Networks have achieved remarkable success in computer vision, natural language, and audio tasks. It shows excellent ability in dealing with specific tasks with surpassing efficiency and accuracy. However, researches indicated that deep neural models are extremely vulnerable to crafted adversarial perturbation. In image classification domain, crafted images with adversarial perturbation can fool deep neural models into misclassifying. Specific researches revealed that adversarial examples crafted by attack methods show substantial pixel modification strength, which causes lower similarity between the clean and corresponding sample and makes the change in crafted samples visible. To address the issues mentioned above, we propose an adversarial attack method, which generates adversarial perturbation based on adaptive root mean square gradient strategy. In our proposed approach, we formulate adversarial perturbation based on an adaptive gradient at root mean square level during crafting adversarial sample. Due to the adaptive strategy, the proposed method searches the decision boundary between the original and the adversarial classes in latent space directly by searching the extremum of loss. It helps to generate adversarial samples with higher image quality and better transferability on fooling multiple deep neural models. We evaluate several state-of-the-art attack methods with proposed methods. Experimental results show that our approaches outperform modern techniques in crafting adversarial sample with slight pixel modification, and excellent efficiency in fooling classifiers in both no-targeted and targeted attack strategies. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						179	195		10.1016/j.neucom.2020.01.084													
J								Effects of repetitive SSVEPs on EEG complexity using multiscale inherent fuzzy entropy	NEUROCOMPUTING										EEG; SSVEP; Complexity; Multiscale inherent fuzzy entropy	APPROXIMATE ENTROPY; HABITUATION; NETWORK	Multiscale inherent fuzzy entropy is an objective measurement of electroencephalography (EEG) complexity, reflecting the habituation of brain systems. Entropy dynamics are generally believed to reflect the ability of the brain to adapt to a visual stimulus environment. In this study, we explored repetitive steady-state visual evoked potential (SSVEP)-based EEG complexity by assessing multiscale inherent fuzzy entropy with relative measurements. We used a wearable EEG device with Oz and Fpz electrodes to collect EEG signals from 40 participants under the following three conditions: a resting state (closed-eyes (CE) and open-eyes (OE) stimulation with five 15-Hz CE SSVEPs and stimulation with five 20-Hz OE SSVEPs. We noted monotonic enhancement of occipital EEG relative complexity with increasing stimulus times in CE and OE conditions. The occipital EEG relative complexity was significantly higher for the fifth SSVEP than for the first SSEVP (FDR-adjusted p < 0.05). Similarly, the prefrontal EEG relative complexity tended to be significantly higher in the OE condition compared to that in the CE condition (FDR-adjusted p < 0.05). The results also indicate that multiscale inherent fuzzy entropy is superior to other competing multiscale-based entropy methods. In conclusion, EEG relative complexity increases with stimulus times, a finding that reflects the strong habituation of brain systems. These results suggest that multiscale inherent fuzzy entropy is an EEG pattern with which brain complexity can be assessed using repetitive SSVEP stimuli. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						198	206		10.1016/j.neucom.2018.08.091													
J								A hierarchical meta-model for multi-class mental task based brain-computer interfaces	NEUROCOMPUTING										Brain computer interface; Mental tasks classification; Feature extraction; Feature selection; Support vector machine	SINGLE-TRIAL EEG; CLASSIFICATION; SELECTION	In the last few years, many research works have been suggested on Brain-Computer Interface (BCI), which assists severely physically disabled persons to communicate directly with the help of electroencephalogram (EEG) signal, generated by the thought process of the brain. Thought generation inside the brain is a dynamic process, and plenty thoughts occur within a small time window. Thus, there is a need for a BCI device that can distinguish these various ideas simultaneously. In this research work, our previous binary-class mental task classification has been extended to the multi-class mental task problem. The present work proposed a novel feature construction scheme for multi mental task classification. In the proposed method, features are extracted in two phases. In the first step, the wavelet transform is used to decompose EEG signal. In the second phase, each feature component obtained is represented compactly using eight parameters (statistical and uncertainty measures). After that, a set of relevant and non-redundant features is selected using linear regression, a multivariate feature selection approach. Finally, optimal decision tree based support vector machine (ODT-SVM) classifier is used for multi mental task classification. The performance of the proposed method is evaluated on the publicly available dataset for 3-class, 4-class, and 5-class mental task classification. Experimental results are compared with existing methods, and it is observed that the proposed plan provides better classification accuracy in comparison to the existing methods for 3-class, 4-class, and 5-class mental task classification. The efficacy of the proposed method encourages that the proposed method may be helpful in developing BCI devices for multi-class classification. Keywords: Brain computer interface Mental tasks classification Feature extraction Feature selection Support vector machine (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 14	2020	389						207	217		10.1016/j.neucom.2018.07.094													
J								Fuzzy knowledge based performance analysis on big data	NEUROCOMPUTING										Incremental clustering algorithms; Big data; Apache spark framework; Parallel processing; Very large data; Internet of things	COMPLEXITY; KERNEL	Due to the various emerging technologies, an enormous amount of data, termed as Big Data, gets collected every day and can be of great use in various domains. Clustering algorithms that store the entire data into memory for analysis become unfeasible when the dataset is too large. Many clustering algorithms present in the literature deal with the analysis of huge amount of data. The paper discusses a new clustering approach called an Incremental Random Sampling with Iterative Optimization Fuzzy c-Means (IRSIO-FCM) algorithm. It is implemented on Apache Spark, a framework for Big Data processing. Sparks works really well for iterative algorithms by supporting in-memory computations, scalability, etc. IRSIO-FCM not only facilitates effective clustering of Big Data but also performs storage space optimization during clustering. To establish a fair comparison of IRSIO-FCM, we propose an incremental version of the Literal Fuzzy c-Means (LFCM) called ILFCM implemented in Apache Spark framework. The experimental results are analyzed in terms of time and space complexity, NMI, ARI, speedup, sizeup, and scaleup measures. The reported results show that IRSIO-FCM achieves a significant reduction in run-time in comparison with ILFCM. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 14	2020	389						218	228		10.1016/j.neucom.2018.10.088													
J								An improved algorithm for finding the generators of the solution space for A circle times x >= x	SOFT COMPUTING										Max-plus algebra; System of inequalities; Supereigenvector; Algorithm; Generator	MAX ALGEBRA; SYSTEM	This paper deals with the problem of finding the generators of the solution space for a system of inequalities A circle times x >= x in max-plus algebra. It provides an improved algorithm which can be used to find a smaller set of generators for the solution space by skipping a large number of invalid generators.																	1432-7643	1433-7479				JUL	2020	24	13					9383	9390		10.1007/s00500-020-04978-6		MAY 2020											
J								Forecasting hybrid neural network with variational learning rate and q-DSCID synchronization evaluation for energy market	SOFT COMPUTING										Hybrid neural network; Empirical wavelet transform; Random inheritance formula; Variational learning rate; Crude oil futures price; q-order dyadic scales complexity invariant distance	TIME-SERIES; PRICE; VOLATILITY; PREDICTION; DECOMPOSITION; MODEL; CONTROLLER; ALGORITHM; FUTURES; MOTION	Because of the nonlinearity, uncertainty, and dynamics of crude oil price, its price forecasting has continuously been a burdensome international research issue. To better implement the prediction of the energy market by machine learning algorithms, premeditating the influence factors of historical data in different periods on prediction consequence, random inheritance formula error correction algorithm is proposed in this work. The empirical wavelet transform and reconstruction are applied to extract data features simultaneously. A novel hybrid neural network model is constructed, which integrates empirical wavelet transform, Elman recurrent neural network, and random inheritance formula. Variational learning rate is proposed and used to ameliorate the selection of parameters for the network training procedure. In this paper, the proposed model is applied in crude oil futures price forecasting. Further, a variety of evaluation indicators are introduced to contrast and evaluate the predictions. An original representative synchronization evaluation arithmetic q-order dyadic scales complexity invariant distance is put forward and utilized. Demonstration results suggest that the proposed model has superior preciseness among comparison models.																	1432-7643	1433-7479				NOV	2020	24	22					16811	16828		10.1007/s00500-020-04977-7		MAY 2020											
J								Voltage unbalance evaluation in the intelligent recognition of induction motor rotor faults	SOFT COMPUTING										Rotor fault; Induction motor; Artificial neural networks	SUPPORT VECTOR MACHINE; SIGNATURE ANALYSIS; DIAGNOSIS; VIBRATION; CLASSIFICATION; IDENTIFICATION; DECOMPOSITION; ALGORITHM; SEVERITY; MODEL	Induction motors are widely used in several industrial applications due to their factors of favouritism already consolidated, such as robustness, low cost and high reliability. Early detection and proper fault diagnosis reduce the maintenance cost and also increase process effectiveness. Therefore, this paper presents a method for fast classification of rotor faults in line-connected induction motors operating at steady state, under unbalanced voltages and load conditions. Hence, the amplitude of the stator's current signal in the time domain is presented as input to intelligent computational models for the classification of rotor's faults. After a proper discretization of the current signal, the points extraction technique is applied allowing a reduction in the classifier's complexity. Results from 900 experimental tests are provided and compared to validate this study. The results indicate that this approach can be employed to proper classify rotor broken bars in induction motors operating under unbalanced voltage and different load conditions.																	1432-7643	1433-7479				NOV	2020	24	22					16935	16946		10.1007/s00500-020-04986-6		MAY 2020											
J								Water management using genetic algorithm-based machine learning	SOFT COMPUTING										Fitness function; Objective function; Decision-making system; Prediction system		Water scarcity is the major problem presently being faced globally; it is to be managed in an efficient manner. The water management procedure is one of the techniques to condense necessary water. The objective of water management system is that water supply agency to collect, distribute quality water without any delay and scarcity. An intelligent system is necessary for efficient production, collection and distribution. The proposed intelligent system consists of genetic operations with fitness value and neural network for training. The fitness function is used to make new intelligent members from existing population of water resources for water collection and distribution. The system is applicable for prediction about water consumption, distribution using decision-making algorithms to increase optimization performance by calculation of objective function of various population types. The regression performance of proposed intelligent system is calculated and compared with other algorithms.																	1432-7643	1433-7479				NOV	2020	24	22					17153	17165		10.1007/s00500-020-05009-0		MAY 2020											
J								Intention-Related Natural Language Grounding via Object Affordance Detection and Intention Semantic Extraction	FRONTIERS IN NEUROROBOTICS										intention-related natural language grounding; object affordance detection; intention semantic extraction; multi-visual features; attention-based dynamic fusion		Similar to specific natural language instructions, intention-related natural language queries also play an essential role in our daily life communication. Inspired by the psychology term "affordance" and its applications in Human-Robot interaction, we propose an object affordance-based natural language visual grounding architecture to ground intention-related natural language queries. Formally, we first present an attention-based multi-visual features fusion network to detect object affordances from RGB images. While fusing deep visual features extracted from a pre-trained CNN model with deep texture features encoded by a deep texture encoding network, the presented object affordance detection network takes into account the interaction of the multi-visual features, and reserves the complementary nature of the different features by integrating attention weights learned from sparse representations of the multi-visual features. We train and validate the attention-based object affordance recognition network on a self-built dataset in which a large number of images originate from MSCOCO and ImageNet. Moreover, we introduce an intention semantic extraction module to extract intention semantics from intention-related natural language queries. Finally, we ground intention-related natural language queries by integrating the detected object affordances with the extracted intention semantics. We conduct extensive experiments to validate the performance of the object affordance detection network and the intention-related natural language queries grounding architecture.																	1662-5218					MAY 13	2020	14								26	10.3389/fnbot.2020.00026													
J								Some Criteria of the Knowledge Representation Method for an Intelligent Problem Solver in STEM Education	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING											MODEL; REQUIREMENTS; SYSTEMS	Nowadays, building intelligent systems for science, technology, engineering, and math (STEM) education is necessary to support the studying of learners. Intelligent problem solver (IPS) is a system that can be able to solve or tutor how to solve the problems automatically. Learners only declare hypothesis and goal of problems based on a sufficient specification language. They can request the program to solve it automatically or to give instructions that help them to solve it themselves. Knowledge representation plays a vital role in these kinds of intelligent systems. There are various methods for knowledge representation; however, they do not meet the requirements of an IPS in STEM education. In this paper, we propose the criteria of a knowledge model for an IPS in education. These criteria orient to develop a method for knowledge representation to meet actual requirements in practice, especially pedagogical requirements. For proving the effectiveness of these criteria, a knowledge model is also constructed. This model can satisfy these criteria and be applied to build IPS for courses, such as mathematics and physics.																	1687-9724	1687-9732				MAY 13	2020	2020								9834218	10.1155/2020/9834218													
J								STRATA: unified framework for task assignments in large teams of heterogeneous agents	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multi-agent systems; Task assignment; Heterogeneous agents	ALLOCATION; STRATEGIES; DIVERSITY; TAXONOMY	Large teams of heterogeneous agents have the potential to solve complex multi-task problems that are intractable for a single agent working independently. However, solving complex multi-task problems requires leveraging the relative strengths of the different kinds of agents in the team. We present Stochastic TRAit-based Task Assignment (STRATA), a unified framework that models large teams of heterogeneous agents and performs effective task assignments. Specifically, given information on which traits (capabilities) are required for various tasks, STRATA computes the assignments of agents to tasks such that the trait requirements are achieved. Inspired by prior work in robot swarms and biodiversity, we categorize agents into different species (groups) based on their traits. We model each trait as a continuous variable and differentiate between traits that can and cannot be aggregated from different agents. STRATA is capable of reasoning about both species-level and agent-level variability in traits. Further, we define measures of diversity for any given team based on the team's continuous-space trait model. We illustrate the necessity and effectiveness of STRATA using detailed experiments based in simulation and in a capture-the-flag game environment.																	1387-2532	1573-7454				MAY 13	2020	34	2							38	10.1007/s10458-020-09461-y													
J								Intelligent environment for advanced brain imaging: multi-agent system for an automated Alzheimer diagnosis	EVOLUTIONARY INTELLIGENCE										Multi-agent system (MAS); 2D; 3D image segmentation; 2D; 3D image processing; Alzheimer disease (AD); Healthcare	MR-IMAGES; SEGMENTATION; TUMOR; DISEASE; CLASSIFICATION; ALGORITHM; NEGOTIATION; FRAMEWORK; ACCURACY; AGENT	Over decades Alzheimer's disease (AD) researches presented increasing challenges. However, various methods were proposed to detect AD including image processing. This paper presents a concrete solution to diagnose AD based on a multi-agent system (MAS). This approach highlights the importance of the cooperation paradigm within a robust system, in which all agents cooperate to accomplish the segmentation tasks. The exchanges between agents remain an essential part of the segmentation process. The original contribution of this paper is twofold: (1) To present an agent-based segmentation methodology by highlighting the main characteristics, advantages, and disadvantages of MAS. (2) To provide a usable solution by facilitating the detection of AD while taking into account both the expertise and the requirements of specialists in the application domain. Ensuring a cooperative segmentation using the multi-agent system offers a strong point in terms of system stability as well as clarity of the process for physicians. For this, several tests have been carried out to prove the effectiveness of our work. The results ensure that the performance indices in our proposed method were higher.																	1864-5909	1864-5917															10.1007/s12065-020-00420-w		MAY 2020											
J								Entropy-controlled deep features selection framework for grape leaf diseases recognition	EXPERT SYSTEMS										best features selection; CNN; feature extraction; fruit diseases; fusion	SKIN-LESION DETECTION; SEGMENTATION; CLASSIFICATION; FUSION; IDENTIFICATION; AGRICULTURE; EXTRACTION; SUPERPIXEL; STRATEGY; CANCER	Several countries are most reliant on agriculture either in terms of employment opportunities, national income, availability of a raw material, food production, to name but a few. However, it faces a big challenge such as climate changes, diseases, pets, weeds etc. Therefore, last decade has provided a machine learning-based solution to the agricultural community, which helped farmers to identify the diseases at the early stages. In this article, our focus is on grape diseases, and proposes a novel framework to identify and classify the selected diseases at the early stages. A deep learning-based solution is embedded into a conventional architecture for optimal performance. Three primary steps are involved; (a) feature extraction after applying transfer learning on pre-trained deep models, AlexNet and ResNet101, (b) selection of best features using proposed Yager Entropy along with Kurtosis (YEaK) technique, (c) fusion of strong features using proposed parallel approach and later subject to classification step using least squared support vector machine (LS-SVM). The simulations are performed on infected grape leaves obtained from the plant village dataset to achieving an accuracy of 99%. From the simulation results, we sincerely believe that our proposed approach performed exceptionally compared to several existing methods.																	0266-4720	1468-0394															10.1111/exsy.12569		MAY 2020											
J								On the use of semantic technologies for video analytics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computer vision; Semantic web; Video analysis; OWL	STRUCTURAL DESCRIPTION; ACTIVITY RECOGNITION; CONCEPT ONTOLOGY; IMAGE RETRIEVAL; BIG DATA; KNOWLEDGE; ANNOTATION; WEB; FRAMEWORK; TIME	The rapid proliferation of smart devices, surveillance cameras, infrastructures and buildings enhanced with the Internet of Things (IoT) technologies has led to a huge explosion of contents, especially in the video domain, determining an ever increasing interest towards the development of methods and tools for automatic analysis and interpretation of video sequences. Through the years, the availability of contextual knowledge has proven to improve video analysis performances in several ways, although the formal representation of semantic content in a shareable and fusion oriented manner is still an open problem, also considering the wide diffusion of Fog and Edge computing architectures for video analytics lately. In this context, an interesting answer has come from Semantic Web (SW) technologies, that opened a new perspective for the so-called Knowledge Based Computer Vision (KBCV), adding novel analytics opportunities, improving accuracy, and facilitating data exchange between video analysis systems in an open extensible manner. In this work, we propose a survey of the papers from the last eighteen years, back when first applications of semantic technologies to video analytics have appeared. The papers, analyzed under different perspectives to give a comprehensive overview of the technologies involved, reveal an interesting trend towards the adoption of SW technologies for video analytics scopes. As a result of our work, some insights about future challenges are also provided.																	1868-5137	1868-5145															10.1007/s12652-020-02021-y		MAY 2020											
J								Congestion control load balancing adaptive routing protocols for random waypoint model in mobile ad-hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile ad-hoc network; CCLBARP; Protocol; Traffic load	SCHEME	Congestion is the main issue for routing in the mobile system which overall degrading performance in the network due to inadequate accessibility of resources of the wireless network of the nature, node mobility and dynamic topology of the wireless network. Congestion causes packet loss, time and energy wastage by connection failure and by mobile node failure. Congestion could be recognized as a base of deployment, link capacity node and routing routes of the network. The congestion detected on node-link by a source node along with the node of the path, which distributes traffic flow over the alternative node paths by considering routing path availability of the network. This paper has proposed a congestion control load balancing adaptive routing protocol (CCLBARP) algorithms on the direction to reduce delay, system routing overhead, congestion and enhances the life of network in MANET. The proposed congestion control technique is compared with original existing routing protocols such as DYMO and DSR of MANET in terms of throughput, end-to-end delay, packet drops average jitter, packet delivery ratio and normalized routing overhead of the network and this CCLBARP get outperformed the DSR and DYMO.																	1868-5137	1868-5145															10.1007/s12652-020-02059-y		MAY 2020											
J								Multi-agent deep neural networks coupled with LQF-MWM algorithm for traffic control and emergency vehicles guidance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial neural network; Convolution neural network; Longest queue first algorithm; Multi-agent systems; Traffic signal control systems; Emergency vehicle	REAL-TIME; SIGNALIZED INTERSECTIONS; BUS PRIORITY; TRAVEL-TIME; PREEMPTION; PREDICTION; SYSTEM	Authorities in modern cities are facing daily challenges related to traffic control. Due to the problem complexity caused by the urbanization growth, investing in developing traffic signal control systems (TSCS) to guarantee better mobility has taken more attention by these authorities. In the existing literature, the majority of TSCS offers only a real-time control for a detected traffic problem without considering early prediction and estimation of its occurrence. Furthermore, traffic problems related to the arrival and guidance of emergency vehicles are rarely considered. Based on these gaps, we rely on concepts and mechanisms from both, the Artificial and the convolution neural networks (ANN and CNN), coupled with the longest queue first maximal weight matching algorithm (LQF-MWM), to develop PANNAL, a predictive and reactive TSCS. PANNAL is a Multi-Agent based System, where each individual agent has ANN, CNN, and LQF-MWM to adapt signal sequences and durations and favor the crossing of emergency vehicles. Agents have a heterarchical architecture considered for coordination. We leant on VISSIM, a state-of-the-art traffic simulation software for simulation and evaluation. We adopted algorithms, scenarios, key performance indicators, and evaluation results from the recent literature for benchmarking. These algorithms are pre-emptive and have a high performance and competitive results in traffic control of disturbed traffic condition.																	1868-5137	1868-5145															10.1007/s12652-020-01921-3		MAY 2020											
J								A secure medical image transmission algorithm based on binary bits and Arnold map	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine interaction; Image transmission; Security; Arnold maps		The responsibility of maintaining patient's records is with medical personnel. The medical personnel are supposed not to disclose any kind of medical information related to the patient. This is also applicable for the medical information discovered by medical personnel in connection with the treatment of the patient. With the advent of technology and its penetration into the medical field in the form of telemedicine and e-health, the challenge of maintaining confidentiality is becoming complex. The confidentiality needs to be protected from storage of medical image or transmission of image from a medical database center to other. Nowadays, the information of patient is printed in the corner of the medical image. This can be accessed by anybody or even the machine can access and store the information. During an electronic transmission, the patient information may be intercepted by a third party. This may lead to big lawsuit. Apart from these security issues, for scenarios such as medical research, the image should be used but patient information should be hidden. Also for diagnostic purposes, the information of the patient should be readily accessible to the medical personnel. These constraints lead to the necessity of suitable security techniques for medical image storage and transmission. If suitable security techniques are not applied, privacy of patients will be stake. Hence, numerous methods are being implemented by individuals, governments and businesses for secured transmission of patient's data. For patient's privacy protection, secured transmission requires techniques like cryptography and watermarking. These techniques achieve confidentiality and integrity. In this work, a new approach has been developed for secured transmission of medical images.																	1868-5137	1868-5145															10.1007/s12652-020-02028-5		MAY 2020											
J								DEODORANT: a novel approach for early detection and prevention of polycystic ovary syndrome using association rule in hypergraph with the dominating set property	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Polycystic ovary syndrome; Media sources data; Association rule; Hypergraph; Dominating set; Spectral clustering		Present online health discussion forums generate the bountiful amount of digitized data through health-blogs, posts, tweets, and chats in the social media. People post queries, health issues they undergo along with the symptoms, diagnosis, and clinical reports to get direction for preventive measures and medical relief. As a case study, this work focus on detection of Polycystic Ovary Syndrome, a prevalent condition that affects a woman's hormone levels. This PCOS problem has been investigated as it forms high-risk factor for infertility, heart disease, diabetes, stroke and many such diseases. We propose a novel model named as DEODORANT (Detection and prEvention of polycystic Ovary synDrome using assOciation rule hypeRgrAph and domiNating set properTy) to derive prospective use of real-time mining data. The unstructured data collected from various media sources are preprocessed using NLTK and association rules are derived by applying apriori algorithm. These association rules are represented in hypergraph and then regenerated as line graph to make it suitable for cluster construction. Spectral clustering is performed on line graph to partition into clusters of hypergraphs. By applying dominating set property on the resultant hypergraph, required inferences can be elicited. From the experimental results, support value of the outcome derived from the dominating set of each cluster has exhibited the symptoms and the causes with percentage ranking. It is evident that they get aligned with precise result portraying real statistics. This type of analysis will empower doctors and health organizations to keep track of the diseases, their symptoms for early detection and safe recovery.																	1868-5137	1868-5145															10.1007/s12652-020-01990-4		MAY 2020											
J								On developing dynamic and efficient cryptosystem for safeguarding healthcare data in public clouds	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cryptosystems; High secure encryption standard; Medical cloud	SEARCH; SECURE	With the greater development in internet, in medical field, all the confidential information about the patients and their test results are maintained in any of the remote servers around the world. Because of the confidentiality to be maintained in case of handling the patient related data, it is very crucial that the data is handled only by the authorized personnel. In the conventional method of data storage, the data are normally encrypted in the patient's side and then they are kept securely in remote cloud server. So, the patient is empowered to decide the visibility of his data, hence becomes the data owner. But, the main disadvantage of this approach is its restricted data availability for the doctors who need to access the medical data. In this paper we propose two dynamic and efficient cryptosystems for secure storage and access of medical data. First algorithm exploits the High Secure Encryption Standard scheme, which has the advantage of making an encrypted data stored in the remote cloud is available for searching. Moreover, this approach ensures security and privacy both in patient's side and the doctor's side. Also, we propose another approach for developing a key management system, which will effectively handle the allocation of encryption and decryption keys for all the people who are handling the sensitive medical data of the patients. The experimental results show that our proposed model of cryptosystem ensures that the data has been stored and retrieved with higher security with relatively optimized searching time. Moreover, this approach is very efficient in terms of space complexity, search indexing and secret access of data comparing with many of the existing algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02033-8		MAY 2020											
J								A hybrid approach for mortality prediction for heart patients using ACO-HKNN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Heart disease prediction; Feature selection; Hybrid KNN; Mortality prediction; Data analysis		Heart disease is the major cause of mortality in the world. The heart disease prediction from the clinical data is deliberate as the most important subject in clinical data analysis. Especially the size of data in health care is vast. Data mining (DM) assists decision and prediction from the raw health care data. DM converts the large collection into useful information. Several existing studies utilize the data mining approaches in heart disease prediction. There is only little research focused on selecting the important features which play a significant role in predicting heart disease is less. The aim of this study is to provide an enhanced approach with novel feature selection and classification technique to predict mortality in congestive heart failure patients. Through this approach the death rate due to heart disease will be decreased gradually. The ant colony optimization (ACO) algorithm is utilized for selecting the best feature for hybrid K-nearest neighbor (KNN) classifier. The proposed approach is compared with the prior classification techniques such as the Support vector machine, Naive Bayes, KNN, C4.5, and decision tree. UCI Cleveland dataset is utilized for our implementation. Using the Netbeans IDE an experimental was conducted and the result shows that the heart disease prediction model provides a better result with accuracy of 99.2%.The present study shows the efficiency of the HKNN in heart disease prediction system. Initially important features are analyzed and then classification is utilized to obtain a better result.																	1868-5137	1868-5145															10.1007/s12652-020-02027-6		MAY 2020											
J								Speaker age and gender classification using GMM supervector and NAP channel compensation method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Speaker age and gender classification; Gaussian mixture model (GMM); Nuisance attribute projection (NAP); Support vector machine (SVM); Maximum-A-posteriori (MAP)	AUTOMATIC SPEAKER; FORECAST ENGINE; VERIFICATION	One of the most important factors affecting the performance of speech-based recognition systems is the differences between training and test conditions. The Nuisance attribute projection (NAP) is an effective method for eliminating these differences, called channel effects. In this study, the effects of the NAP approach in determining age and gender groups are investigated. Mel-frequency cepstral coefficients and delta coefficients are used as a feature and Gaussian mixture models (GMM) adapted from the universal background model by maximum-a-posteriori method are used for the modeling of age and gender classes. After the GMMs corresponding to each speech are converted into mean supervectors, they are applied to a Support Vector Machine (SVM), and speeches are classified according to the age and gender group of the speakers. While linear GMM kernel based on Kullback-Leibler divergence is used instead of standard SVM kernels, the NAP channel subspace size is changed between 20 and 200 and the number of GMM components is changed between 32 and 512 to determine the optimum values for these parameters. In the tests on the aGender database, the optimum number of components is determined as 128, and the optimum NAP channel subspace size is determined as 45. The age and gender classification accuracy of the system, which is developed using these optimum parameters, is increased from 60.52 to 62.03% with the use of NAP. In addition, age classification accuracy is increased from 60.23 to 61.82% and gender classification accuracy is increased from 91.71 to 92.30%.																	1868-5137	1868-5145															10.1007/s12652-020-02045-4		MAY 2020											
J								Incorporating communities' structures in predictions of missing links	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Community detection; Link prediction; Social network analysis; Similarity measures; Network features	NETWORKS; ORGANIZATION	This article introduces a community-based approach to link prediction that identifies the links likely to be seen in the near future in a network. The proposed method incorporates community structure as a feature in the predictions of missing links in a network. We design a feature-based similarity measure that considers the impact of community structure in addition to other network features in link prediction. We analyze the performance of the devised approach in terms of precision, recall, accuracy, and area-under-the-curve (AUC) metrics on real-world datasets. Further, we examine the performance of the devised method in terms of execution time against real-world and synthetic datasets. The proposed approach outperforms the other existing approaches, as will be shown experimentally later.																	0925-9902	1573-7675				AUG	2020	55	1					183	205		10.1007/s10844-020-00603-y		MAY 2020											
J								A fuzzy-based adaptive multi-input-output scheme in lieu of diabetic and hypertension management for post-operative patients: an human-machine interface approach with its continuum	NEURAL COMPUTING & APPLICATIONS										Diabetics; Fuzzy logic; Hypertension; Glucose control; Optimal insulin analysis; Optimal SNP infusion	CHALLENGES; THERAPY	Here, a multi-input-output control strategy-based adaptive system is implemented to regulate the glycaemic and hypertension (HT) level concurrently for post-operative patients. The current medical research identifies the relationship between diabetes and HT, and these two diseases have possible overlap in their disease aetiology. Based on the continuum, the blood glucose and blood pressure (BP) have to be measured independently and control the infusion to maintain optimally based on the variations of diabetic mellitus and HT to avoid the major complications for the perioperative condition. Based on the analysis, the post-operative strain may increase HT and may increase the glycaemic level, and this uncertainty of disparity may lead to BP, hyperinsulinemia, cardio diseases, and osmotic diuresis along with hyperglycaemia. The proposed adaptive cascade control strategy illustrates two different types of control loops independently, and these loops adopt an adaptive control strategy with parametric compensation. This adaptive control algorithm integrates the cascade methodology along with expert knowledge to treat these diseases by using adaptation laws with the help of fuzzy logic to regulate the proper insulin and sodium nitroprusside (SNP) infusion. The output response of the plasma glucose concentration and HT regulation was shown along with insulin infusion rate and SNP infusion rate based on the extensive simulation readings. The attained simulation results demonstrate that the adaptive control strategy shows better outcomes for the infusion and may achieve potentially better control on HT and glycaemic levels for post-operative patients.																	0941-0643	1433-3058															10.1007/s00521-020-04975-8		MAY 2020											
J								Adaptive fuzzy fault-tolerant control using Nussbaum-type function with state-dependent actuator failures	NEURAL COMPUTING & APPLICATIONS										Adaptive control; Fuzzy logic systems; Actuator faults; Nussbaum-type function	NONLINEAR-SYSTEMS; TRACKING CONTROL; NEURAL-CONTROL; DESIGN; APPROXIMATION; COMPENSATION; SPACECRAFT; SENSOR	This paper presents an adaptive fuzzy fault-tolerant tracking control for a class of unknown multi-variable nonlinear systems, with external disturbances, unknown control sign, and actuator faults. By employing fuzzy logic systems, the unknown nonlinear dynamics and the state-dependent actuator faults are approximated, and by utilizing a Nussbaum-type function, the issue of unknown control sign is solved. The proposed control scheme is based on two forms, an adaptive fuzzy controller along with a robust controller that is equipped with a Nussbaum-type gain function, which guarantees stability with the boundedness of all signals involved in the closed-loop system. To prove the accuracy, and the effectiveness of the proposed control scheme, a simulation example on two-inverted pendulums system is carried out.																	0941-0643	1433-3058															10.1007/s00521-020-04977-6		MAY 2020											
J								Learning Stable Robust Adaptive NARMA Controller for UAV and Its Application to Twin Rotor MIMO Systems	NEURAL PROCESSING LETTERS										NARMA model; Stable robust adaptive control; Nonlinear controller; Unmanned air vehicle; Twin rotor MIMO system	ALTITUDE CONTROL; ACTUATOR	This study presents a nonlinear auto-regressive moving average (NARMA) based online learning controller algorithm providing adaptability, robustness and the closed loop system stability. Both the controller and the plant are identified by the proposed NARMA based input-output models of Wiener and Hammerstein types, respectively. In order to design the NARMA controller, not only the plant but also the closed loop system identification data are obtained from the controlled plant during the online supervised learning mode. The overall closed loop model parameters are determined in suitable parameter regions to provide Schur stability. The identification and controller parameters are calculated by minimizing the einsensitive error functions. The proposed controller performances are not only tested on two simulated models such as the quadrotor and twin rotor MIMO system (TRMS) models but also applied to the real TRMS with having severe cross-coupling effect between pitch and yaw. The tracking error performances of the proposed controller are observed better compared to the conventional adaptive and proportional-integral-derivative controllers in terms of the mean squared error, integral squared error and integral absolute error. The most noticeable superiority of the developed NARMA controller over its linear counterpart, namely the adaptive auto-regressive moving average (ARMA) controller, is observed on the TRMS such that the NARMA controller shows a good tracking performance not only for the simulated TRMS model but also the real TRMS. On the other hand, it is seen that the adaptive ARMA is incapable of producing feasible control inputs for the real TRMS whereas it works well for the simulated TRMS model.																	1370-4621	1573-773X				AUG	2020	52	1			SI		353	383		10.1007/s11063-020-10265-0		MAY 2020											
J								A generalized belief interval-valued soft set with applications in decision making	SOFT COMPUTING										Intuitionistic fuzzy set; Soft set; Generalized belief interval-valued soft set; Multi-attribute decision making (MADM); Dempster-Shafer Theory	EVIDENTIAL REASONING APPROACH; FUZZY; UNCERTAINTY; FRAMEWORK; ENTROPY; MODEL	The belief interval-valued soft set (BIVSS) combines soft set theory and belief interval value (Dempster-Shafer theory). In this study, we propose a generalized belief interval-valued soft set (GBIVSS) approach and explore the associated properties of this approach in decision-making applications. Using the score function, the scoring function and similarity measure used to compare the relationships between GBIVSS are proposed. Then, we applied the GBIVSS to deal with multi-attribute decision making (MADM) problems. Furthermore, we used a case study of car purchase to illustrate the rationality of the proposed approach. In addition, we compare the effectiveness and advantages of our proposed approach and other existing models, which show superior performance in our proposed approach. GBIVSS provides a solution for multi-attribute problems.																	1432-7643	1433-7479				JUL	2020	24	13					9339	9350		10.1007/s00500-020-04949-x		MAY 2020											
J								Video trajectory analysis using unsupervised clustering and multi-criteria ranking	SOFT COMPUTING										Unsupervised clustering; Object trajectory; Motion analysis	ONLINE	Surveillance camera usage has increased significantly for visual surveillance. Manual analysis of large video data recorded by cameras may not be feasible on a larger scale. In various applications, deep learning-guided supervised systems are used to track and identify unusual patterns. However, such systems depend on learning which may not be possible. Unsupervised methods relay on suitable features and demand cluster analysis by experts. In this paper, we propose an unsupervised trajectory clustering method referred to as t-Cluster. Our proposed method prepares indexes of object trajectories by fusing high-level interpretable features such as origin, destination, path, and deviation. Next, the clusters are fused using multi-criteria decision making and trajectories are ranked accordingly. The method is able to place abnormal patterns on the top of the list. We have evaluated our algorithm and compared it against competent baseline trajectory clustering methods applied to videos taken from publicly available benchmark datasets. We have obtained higher clustering accuracies on public datasets with significantly lesser computation overhead.																	1432-7643	1433-7479				NOV	2020	24	21					16643	16654		10.1007/s00500-020-04967-9		MAY 2020											
J								Tukey's biweight estimation for uncertain regression model with imprecise observations	SOFT COMPUTING										Uncertain regression; Tukey biweight function; Uncertain variable; Uncertainty theory	FUZZY; SERIES	The purpose of regression analysis is to study how a response variable has a relation to a vector of explanatory variables. Traditionally, statisticians assume that the observation data are precise, and we can get some exact values. However, in many cases, the imprecise observation data are available. We assume that these data are uncertain variables in the sense of uncertainty theory. In this paper, the Tukey biweight or bisquare family of loss functions is applied to estimate unknown parameters satisfying the uncertain regression model. First, the Tukey biweight estimations of three types of regression models are given, namely linear, asymptotic and Michaelis-Menten. Then an empirical study is presented to verify the feasibility of this approach. Finally, the effectiveness of this method in weakening the outliers influence is shown by the comparative analysis.																	1432-7643	1433-7479				NOV	2020	24	22					16803	16809		10.1007/s00500-020-04973-x		MAY 2020											
J								A complete online-SVM pipeline for case-based reasoning system: a study on pipe defect detection system	SOFT COMPUTING										Defect detection; Case-based reasoning; Online learning; SVM; Expert system	KERNEL METHODS; ADAPTATION	Recent developments in case-based reasoning system (CBR) have led to an interest in favoring machine learning (ML) approaches as a replacement for traditional weighted distance methods. However, valuable information obtained through a training process was relinquished as transferring to other phases. This paper proposed a complete pipeline integration of CBR using kernel method designated with support vector machine (SVM) as the main engine. Since the system requires learning SVM model to be invoked in every phase, the online learning mechanism is nominated to effectively update the model when a new case adjoins. The proposed full SVM-CBR integration has been successfully built into a pipe defect detection. The achieved result indicates a substantial improvement by transferring learning information accurately.																	1432-7643	1433-7479				NOV	2020	24	22					16917	16933		10.1007/s00500-020-04985-7		MAY 2020											
J								Deep learning-based sequential pattern mining for progressive database	SOFT COMPUTING										Sequential pattern mining; Wavelet analysis; CNN; LSTM; Progressive database	ALGORITHM; WAVELET	Sequential pattern mining (SPM) is one of the main application areas in the field of online business, e-commerce, bioinformatics, etc. The traditional approaches in SPM are unable to accurately mine the huge volume of data. Therefore, the proposed work employs a sequential mining model based on deep learning to minimize complexity in handling huge data. Application areas such as online retailing, finance, and e-commerce face a dynamic change in data, which results in non-stationary data. Therefore, our proposed work uses discrete wavelet analysis to convert non-stationary data into time series. In the proposed SPM, a reformed hybrid combination of convolutional neural network (CNN) with long short-term memory (LSTM) is designed to find out customer behavior and purchasing patterns in terms of time. CNN is used to find the concerned itemsets (frequent) at the end of the pattern and LSTM for finding the time interval among each pair of successive itemsets. The proposed work mines the sequential pattern from a progressive database that removes the obsolete data. Finally, the accuracy of the proposed work is compared with some traditional algorithms to demonstrate its robustness.																	1432-7643	1433-7479				NOV	2020	24	22					17233	17246		10.1007/s00500-020-05015-2		MAY 2020											
J								Optimal design for SCAP/battery power management applied in electric vehicle (EV) applications: a KHO-RDF technique	SOFT COMPUTING										Battery; Energy management; HESS; Load dynamics; Power loss; SCAP; State of charge (SoC)	ENERGY-STORAGE SYSTEM; FUEL-CELL; OPTIMIZATION; STRATEGY; BATTERY; PERFORMANCE	This dissertation proposes power management of optimal control scheme for hybrid energy storage system (HESS) like super capacitor and (SCAP) battery in electric vehicles. The proposed technique is a parallel performance of both the random decision forest (RDF) and krill herd optimization (KHO), and thus, it is called as KHO-RDF method. The main objective is to minimize the difference between the actual and reference power in the battery and SCAP. Here, the HESS framework comprises of two sections: (1) figuring the SCAP reference voltage dependent on load dynamics. (2) Maximizing the power flow through HESS. The reference voltage of SCAP by evaluating real-time load dynamics is computed at first, i.e., the vehicle dynamic, motor characteristics, regenerative braking systems and driving conditions. Furthermore, at the same time the magnitude variety of battery power was minimized and the power loss will occur. The input parameters of SCAP are load current, battery current and state of charge. In proposed technique, possible control signals dataset of HESS is fused to produce KHO. By utilizing the practiced dataset of KHO, the RDF is trained and predicts the optimal parameters of HESS. Moreover, the proposed technique advances the SCAP voltage, battery current magnitude, battery current varieties and battery power. With the proposed approach, the parameter of HESS is optimized and it provides certain solutions. The proposed technique is executed in Matrix Laboratory (MATLAB)/Simulink working platform. By using the comparison analysis with the existing procedures, the performance of the HESS is surveyed.																	1432-7643	1433-7479				NOV	2020	24	22					17247	17263		10.1007/s00500-020-05016-1		MAY 2020											
J								A deep learning approach for effective intrusion detection in wireless networks using CNN	SOFT COMPUTING										Convolutional neural network; Conditional random field; Correlation coefficient; Feature selection; Classification and intrusion detection system	KRILL HERD ALGORITHM; FEATURE-SELECTION; DETECTION SYSTEMS; NEURAL-NETWORKS; COMBINATION; CLASSIFIER	Security is playing a major role in this Internet world due to the rapid growth of Internet users. The various intrusion detection systems were developed by many researchers in the past to identify and detect the intruders using data mining techniques. However, the existing systems are not able to achieve sufficient detection accuracy when using the data mining. For this purpose, we propose a new intrusion detection system to provide security in data communication by identifying and detecting the intruders effectively in wireless networks. Here, we propose a new feature selection algorithm called conditional random field and linear correlation coefficient-based feature selection algorithm to select the most contributed features and classify them using the existing convolutional neural network. The experiments have been conducted for evaluating the proposed intrusion detection system that achieves 98.88% as overall detection accuracy. The tenfold cross-validation has been done for evaluating the performance of the proposed model.																	1432-7643	1433-7479				NOV	2020	24	22					17265	17278		10.1007/s00500-020-05017-0		MAY 2020											
J								A design of information granule-based under-sampling method in imbalanced data classification	SOFT COMPUTING										Imbalanced data; Information granule; Support vector machine (SVM); K-nearest-neighbor (KNN); Under-sampling	KRILL HERD ALGORITHM; NEAREST-NEIGHBOR; DATA-SETS; PREDICTION; SMOTE; RECOGNITION; SCHEME	In numerous real-world problems, we are faced with difficulties in learning from imbalanced data. The classification performance of a "standard" classifier (learning algorithm) is evidently hindered by the imbalanced distribution of data. The over-sampling and under-sampling methods have been researched extensively with the aim to increase the predication accuracy over the minority class. However, traditional under-sampling methods tend to ignore important characteristics pertinent to the majority class. In this paper, a novel under-sampling method based on information granules is proposed. The method exploits the concepts and algorithms of granular computing. First, information granules are built around the selected patterns coming from the majority class to capture the essence of the data belonging to this class. In the sequel, the resultant information granules are evaluated in terms of their quality and those with the highest specificity values are selected. Next, the selected numeric data are augmented by some weights implied by the size of information granules. Finally, a support vector machine and a K-nearest-neighbor classifier, both being regarded here as representative classifiers, are built based on the weighted data. Experimental studies are carried out using synthetic data as well as a suite of imbalanced data sets coming from the public machine learning repositories. The experimental results quantify the performance of support vector machine and K-nearest-neighbor with under-sampling method based on information granules. The results demonstrate the superiority of the performance obtained for these classifiers endowed with conventional under-sampling method. In general, the improvement of performance expressed in terms of G-means is over 10% when applying information granule under-sampling compared with random under-sampling.																	1432-7643	1433-7479				NOV	2020	24	22					17333	17347		10.1007/s00500-020-05023-2		MAY 2020											
J								On the identification and analysis of citation pattern irregularities among journals	EXPERT SYSTEMS										anomalous citation; cartel; citation stacking; impact factor manipulation; unsupervised machine learning	COERCIVE CITATION; SELF-CITATIONS; AUTHOR; DOCUMENTS; EDITORS; INDEX	Recent studies report that few journals are adopting unethical citation practices to inflate Impact Factor (IF) artificially. "Clarivate Analytics" has started to blacklist such journals since 2006. As reported in the literature, evaluation of journals individually, to detect anomalies from vast and dynamically changing citation network is not efficient. The primary purpose of this work is to define a diverse feature set that can identify such cases of extreme outliers and reason them. The sample size is narrowed down using an unsupervised clustering algorithm in the absence of a labeled training dataset. Next, time-series IF data is analyzed to detect point outliers. Furthermore, microscopic features are identified to reason them. Results reflected from the F-value after ANOVA analysis reveals that geometrical patterns (self-loop, pairwise and group mutual-citation) among journals, an abrupt increase in the paper count of donor and corresponding IF inflation of recipient are some of the essential features. Microscopic features include social factor (calculation of revised IF after removing directed self or mutual-citation), impact of the field of study, impact of publication house and author factor that includes author self and mutual-citation. The significance of this work is to ensure that the quality of a journal is withheld without compromising research integrity by controlling or auditing individual features periodically.																	0266-4720	1468-0394														e12561	10.1111/exsy.12561		MAY 2020											
J								The old doom of a new technology	AI & SOCIETY										Imitation; Art; Robotics; Roboaesthetics; Natural; Artificial		Although robots are largely designed with industrial, scientific or military aims in mind, many designers seek to endow them with anthropomorphic forms, even at the risk of compromising their functionality. Why do they not content themselves with constructing useful machines, rather than also incorporating human-like features? What drives them to cover their machines with a latex coating to simulate human skin? The utopia of the creation of a double appears also in the world of art, the history of which shows, above all, an outcome that coincides with the abandonment of the naturalistic imperative, and the inauguration of various periods of exploration and innovation. That is why a possible roboaesthetics-i.e. robotics and aesthetics combined could give rise to a new scenario. This view would acknowledges the radical novelty of these 'species' and induce the adoption of a new observation level, constructed on the basis of a common project.																	0951-5666	1435-5655															10.1007/s00146-020-00986-0		MAY 2020											
J								Consent for targeted advertising: the case of Facebook	AI & SOCIETY										Targeted advertising; Social media; Consent mechanisms; Privacy; GDPR; Privacy harms		The EU General Data Protection Regulation (GDPR) recognizes the data subject's consent as one of the legal grounds for data processing. Targeted advertising, based on personal data processing, is a central source of revenue for data controllers such as Google and Facebook. At present, the implementation of consent mechanisms for such advertisements are often not well developed in practice and their compliance with the GDPR requirements can be questioned. The absence of consent may mean an unlawful data processing and a lack of control of the user (data subject) on his personal data. However, consent mechanisms that do not fully satisfy GDPR requirements can give users a false sense of control, encouraging them to allow the processing of more personal data than they would have otherwise. In this paper, we identify the features, originating from GDPR requirements, of consent mechanisms. For example, the GDPR specifies that a consent must be informed and freely given, among other requirements. We then examine the Ad Consent Mechanism of Facebook that is based on processing of user activity data off Facebook Company Products provided by third parties with respect to these features. We discuss to what extent this consent mechanism respects these features. To the best of our knowledge, our evaluation of Facebook's Ad Consent Mechanism is the first of its kind.																	0951-5666	1435-5655															10.1007/s00146-020-00981-5		MAY 2020											
J								Testing and unpacking the effects of digital fake news: on presidential candidate evaluations and voter support	AI & SOCIETY										Fake news; Priming; Framing effects; Voting; Candidate preferences	PROCESSING NEWS; MEDIA; INFORMATION; CITIZENS; EXPOSURE; OPINION	There is growing worldwide concern that the rampant spread of digital fake news (DFN) via new media technologies is detrimentally impacting Democratic elections. However, the actual influence of this recent Internet phenomenon on electoral decisions has not been directly examined. Accordingly, this study tested the effects of attention to DFN on readers' Presidential candidate preferences via an experimental web-survey administered to a cross-sectional American sample (N = 552). Results showed no main effect of exposure to DFN on participants' candidate evaluations or vote choice. However, the perceived believability of DFN about the Democratic candidate negatively mediated evaluations of that candidate-especially amongst far-right ideologues. These results suggest that DFN may at worst reinforce the partisan dispositions of mostly politically conservative Internet users, but does not cause or induce conversions in these dispositions. Overall, this study contributes novel experimental evidence, indicating that the potential electoral impact of DFN, although concerning, is strongly conditional on a reciprocal interaction between message receptibility and a pre-existing right-wing ideological orientation. The said impact is, therefore, likely narrow in scope.																	0951-5666	1435-5655															10.1007/s00146-020-00980-6		MAY 2020											
J								Agent programming in the cognitive era	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Agent programming languages; Belief-desire-intention; Artificial intelligence; Machine learning	INTENTION SELECTION; FORMAL SEMANTICS; BDI; ARCHITECTURE; MODEL; GOAL; REPRESENTATION; SPECIFICATION; ENVIRONMENT; LANGUAGE	It is claimed that, in the nascent 'Cognitive Era', intelligent systems will be trained using machine learning techniques rather than programmed by software developers. A contrary point of view argues that machine learning has limitations, and, taken in isolation, cannot form the basis of autonomous systems capable of intelligent behaviour in complex environments. In this paper, we explore the contributions that agent-oriented programming can make to the development of future intelligent systems. We briefly review the state of the art in agent programming, focussing particularly on BDI-based agent programming languages, and discuss previous work on integrating AI techniques (including machine learning) in agent-oriented programming. We argue that the unique strengths of BDI agent languages provide an ideal framework for integrating the wide range of AI capabilities necessary for progress towards the next-generation of intelligent systems. We identify a range of possible approaches to integrating AI into a BDI agent architecture. Some of these approaches, e.g., 'AI as a service', exploit immediate synergies between rapidly maturing AI techniques and agent programming, while others, e.g., 'AI embedded into agents' raise more fundamental research questions, and we sketch a programme of research directed towards identifying the most appropriate ways of integrating AI capabilities into agent programs.																	1387-2532	1573-7454				MAY 12	2020	34	2							37	10.1007/s10458-020-09453-y													
J								struc2gauss: Structural role preserving network embedding via Gaussian embedding	DATA MINING AND KNOWLEDGE DISCOVERY										Gaussian embedding; Structural similarity; Uncertainty modeling		Network embedding (NE) is playing a principal role in network mining, due to its ability to map nodes into efficient low-dimensional embedding vectors. However, two major limitations exist in state-of-the-art NE methods: role preservation and uncertainty modeling. Almost all previous methods represent a node into a point in space and focus on local structural information, i.e., neighborhood information. However, neighborhood information does not capture global structural information and point vector representation fails in modeling the uncertainty of node representations. In this paper, we propose a new NE framework, struc2gauss, which learns node representations in the space of Gaussian distributions and performs network embedding based on global structural information. struc2gauss first employs a given node similarity metric to measure the global structural information, then generates structural context for nodes and finally learns node representations via Gaussian embedding. Different structural similarity measures of networks and energy functions of Gaussian embedding are investigated. Experiments conducted on real-world networks demonstrate that struc2gauss effectively captures global structural information while state-of-the-art network embedding methods fail to, outperforms other methods on the structure-based clustering and classification task and provides more information on uncertainties of node representations.																	1384-5810	1573-756X				JUL	2020	34	4					1072	1103		10.1007/s10618-020-00684-x		MAY 2020											
J								Choosing function sets with better generalisation performance for symbolic regression models	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Symbolic regression; Genetic Programming; Machine learning; Generalisation; Overfitting; Data-driven modelling	REGULARIZATION APPROACH; BLOAT CONTROL; PREDICTION; ENSEMBLE; GP	Supervised learning by means of Genetic Programming (GP) aims at the evolutionary synthesis of a model that achieves a balance between approximating the target function on the training data and generalising on new data. The model space searched by the Evolutionary Algorithm is populated by compositions of primitive functions defined in a function set. Since the target function is unknown, the choice of function set's constituent elements is primarily guided by the makeup of function sets traditionally used in the GP literature. Our work builds upon previous research of the effects of protected arithmetic operators (i.e. division, logarithm, power) on the output value of an evolved model for input data points not encountered during training. The scope is to benchmark the approximation/generalisation of models evolved using different function set choices across a range of 43 symbolic regression problems. The salient outcomes are as follows. Firstly, Koza's protected operators of division and exponentiation have a detrimental effect on generalisation, and should therefore be avoided. This result is invariant of the use of moderately sized validation sets for model selection. Secondly, the performance of the recently introduced analytic quotient operator is comparable to that of the sinusoidal operator on average, with their combination being advantageous to both approximation and generalisation. These findings are consistent across two different system implementations, those of standard expression-tree GP and linear Grammatical Evolution. We highlight that this study employed very large test sets, which create confidence when benchmarking the effect of different combinations of primitive functions on model generalisation. Our aim is to encourage GP researchers and practitioners to use similar stringent means of assessing generalisation of evolved models where possible, and also to avoid certain primitive functions that are known to be inappropriate.																	1389-2576	1573-7632															10.1007/s10710-020-09391-4		MAY 2020											
J								Weakly-supervised Semantic Guided Hashing for Social Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION										Hashing; Image retrieval; Matrix factorization; Social image; Discrete code	QUANTIZATION	Hashing has been widely investigated for large-scale image retrieval due to its search effectiveness and computation efficiency. In this work, we propose a novel Semantic Guided Hashing method coupled with binary matrix factorization to perform more effective nearest neighbor image search by simultaneously exploring the weakly-supervised rich community-contributed information and the underlying data structures. To uncover the underlying semantic information from the weakly-supervised user-provided tags, the binary matrix factorization model is leveraged for learning the binary features of images while the problem of imperfect tags is well addressed. The uncovered semantic information enables to well guide the discrete hash code learning. The underlying data structures are discovered by adaptively learning a discriminative data graph, which makes the learned hash codes preserve the meaningful neighbors. To the best of our knowledge, the proposed method is the first work that incorporates the hash code learning, the semantic information mining and the data structure discovering into one unified framework. Besides, the proposed method is extended to one deep approach for the optimal compatibility of discriminative feature learning and hash code learning. Experiments are conducted on two widely-used social image datasets and the proposed method achieves encouraging performance compared with the state-of-the-art hashing methods.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2265	2278		10.1007/s11263-020-01331-0		MAY 2020											
J								An efficient privacy preserving on high-order heterogeneous data using fuzzy K-prototype clustering	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Privacy-preserving; Heterogeneous data; MEKPFCM; Cloud computing; Map reduce; Elliptical curve cryptography	MAPREDUCE	In this paper, we propose a privacy-preserving high order large amount of heterogeneous data using distributed high order fuzzy k-prototype framework named distributed multiple exponential kernel possibilistic fuzzy clustering (MEKPFCM), incorporates kernel fuzzy c-means and possibilistic fuzzy clustering algorithms. The privacy-preserving high order MEKPFCM, cluster the heterogeneous dataset by representing each heterogeneous data object as a tensor. In this paper, the cloud server directly performs clustering over encrypted datasets, while achieving maximum accuracy. The fully homomorphic encryption algorithm (FHE) is utilized to protect the high order large amount of heterogeneous data. Moreover, we design a secure integration of map-reduce into our proposed work, which makes our proposed work enormously appropriate for cloud computing environment. Detailed security analysis and experimental results show that proposed MEKPFCM method can effectively cluster a large amount of heterogeneous data. The experimentation of the proposed technique was carried out using UCI machinery skin dataset and the performance was compared with the previous techniques using accuracy and encryption time. Furthermore, MEKPFCM can cluster bigdata by using the cloud computing technology without disclosing privacy.																	1868-5137	1868-5145															10.1007/s12652-020-01987-z		MAY 2020											
J								A survey of neural networks usage for intrusion detection systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neural network; Deep learning; Machine learning; Intrusion detection system	EXTREME LEARNING-MACHINE; ATTACKS; SVM	In recent years, advancements in the field of the artificial intelligence (AI) gained a huge momentum due to the worldwide appliance of this technology by the industry. One of the crucial areas of AI are neural networks (NN), which enable commercial utilization of functionalities previously not accessible by usage of computers. Intrusion detection system (IDS) presents one of the domains in which neural networks are widely tested for improving overall computer network security and data privacy. This article gives a thorough overview of recent literature regarding neural networks usage in intrusion detection system area, including surveys and new method proposals. Short tutorial descriptions of neural network architectures, intrusion detection system types and training datasets are also provided.																	1868-5137	1868-5145															10.1007/s12652-020-02014-x		MAY 2020											
J								Improved teaching-learning based optimization algorithm using Lyapunov stability analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Convergence; Lyapunov stability; Teaching-learning-based optimization; Swarm intelligence algorithms	PARTICLE SWARM	Teaching-learning-based optimization (TLBO) algorithms is one of the swarm-based optimization search algorithms. It develops based on the teaching-learning procedures at a classroom to solve multi-dimensional and nonlinear problems. In this paper, convergence and stability analysis of TLBO are studied. The stability of individual dynamics is analyzed by Lyapunov stability theorem and the concept of system dynamics. Stability conditions are achieved and utilized for adapting parameters of the TLBO. The TLBO algorithm is modified based on stability analysis. The modified TLBO is compared with the standard TLBO, particle swarm optimization (PSO), real genetic algorithm (RGA), and gravitational search algorithm (GSA). Simulation results confirm the validity and feasibility of the proposed modified TLBO. The appropriate performance is achieved for multi-dimensional and nonlinear standard bench functions.																	1868-5137	1868-5145															10.1007/s12652-020-02012-z		MAY 2020											
J								Feature reduced blind steganalysis using DCT and spatial transform on JPEG images with and without cross validation using ensemble classifiers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cross validation; Feature extraction; Classifier; SVM; SVM-PSO; PCA; Sampling; Kernels		The paper discuss the outcome evaluation of JPEG images in both spatial and DCT transform and a comparative study is being done. There are four distinct steganographic algorithms-LSB matching, LSB replacement, pixel value differencing and F5 are used. The embedding performed on the images are 25% with text. The idea of cross validation is used to validate the classifier better and a comparative analysis is performed on results with and without cross validation. Features removed for investigation are the first order, second order, extended features and Markov features. Relevant features are chosen by feature reduction. This process is done using principal component analysis (PCA). This is done to eliminate redundant feature that can hamper the efficiency of classification. The classifiers used are support vector machine (SVM) and support vector machine with particle swarm optimisation (SVM-PSO). The classification is done based on six kernels like radial, dot, multiquadratic, epanechnikov and ANOVA and four sampling techniques like shuffled, linear, stratified and automatic sampling. The existing techniques had always used radial as kernel without sampling for a classification. The proposed system make use of this imperfection and has formulated the result.																	1868-5137	1868-5145															10.1007/s12652-020-02001-2		MAY 2020											
J								Behavior based fuzzy security protocol for wireless networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Behavior pattern; Fuzzy logic; Pattern analyzer; Security protocol; Wireless network		Wireless networks are inevitable constituents of the present communication world. The applications of wireless networks are numerous from fundamental communication establishment to high secured financial transactions. Providing security to communication and network transactions is an essential factor to protect the network environment from prying eyes of intruders. This work is targeted to provide a fast and reliable security protocol against intruder attacks based on the network behavior pattern analysis. Fuzzy logic is incorporated in this work to enable the swift security policy selection based on the behavioral pattern changes. Proposed functional blocks such as Node Behavior Analyzer (NBA), Cluster Behavior Analyzer (CBA), Integrated Network Behavior Tracker (INBT), Inversed Elegant Pairing Key Exchange (IEPKE) and Fuzzy based Security Policy Selector (FSPS) are commingled in the name "Behavior based Fuzzy Security Protocol for Wireless Networks" (BFSPWN). Improvements in standard wireless networks' performance metrics such as Throughput, Latency, Jitter, End-to-End Delay and Security are considered in the evaluation process of proposed method.																	1868-5137	1868-5145															10.1007/s12652-020-02060-5		MAY 2020											
J								Reliability evaluation method for warm standby embryonic cellular array	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Embryonic; Warm standby; Reliability; Fault detection; Self-repairing; k-out-of-n system; Non-homogeneous continuous time Markov model	HARDWARE; SYSTEMS	In this paper, a reliability evaluation method for warm standby embryonic cellular array, based on k-out-of-n warm standby system reliability model and non-homogeneous continuous time Markov model, is proposed in order to evaluate the reliability more accurately. In reliability evaluation process, spare cells are in warm standby mode, and embryonic cell fault detection coverage and fault self-repairing success rate are considered. Experimental results show that the proposed reliability evaluation method can evaluate the reliability of embryonic cellular array effectively and improve its accuracy. Based on the proposed reliability evaluation method, the effects of parameters change on embryonic cellular array reliability are researched. By improving embryonic cell fault detection coverage and fault self-repairing success rate, and reducing embryonic cell failure rate, the reliability can be improved effectively. The higher the fault detection coverage and fault self-repairing success rate, the larger the growth rate of reliability. In addition, by increasing the scale of embryonic cellular array, the reliability increases to the maximum first, then it will decrease continuously. The reliability variation law can not only provide theoretical guidance for embryonic cellular array optimization design, but also point out the direction for further research.																	1868-5137	1868-5145															10.1007/s12652-020-02044-5		MAY 2020											
J								Prediction model for stock price trend based on recurrent neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recurrent neural network (RNN); Attention mechanism; Stock price trend prediction		Stock data have a long memory, that is, changes in stock prices are closely related to historical transaction data. Also, Recurrent Neural Networks have good time series feature extraction capabilities. The paper proposed prediction models based on RNN/LSTM/GRU respectively. The attention mechanism has the ability to select and focus "key information". Therefore, based on the conventional Recurrent Neural Network, this paper introduced the attention mechanism and proposed a prediction model based on AT-RNN/AT-LSTM/AT-GRU. And the paper modeled and experimented with it. The results showed that: (1) In the most basic comparison test of RNN-M, LSTM-M, and GRU-M prediction models, the GRU-M and LSTM -M was significantly better than the RNN-M and the GRU-M was slightly better than the LSTM-M; (2) The introduction of the attention mechanism layer was helpful to improve the accuracy of the stock fluctuation prediction model;(3) Deeper neural networks did not necessarily achieve better results.																	1868-5137	1868-5145															10.1007/s12652-020-02057-0		MAY 2020											
J								A Bio-Inspired Goal-Directed Visual Navigation Model for Aerial Mobile Robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Bio-inspired navigation; Active navigation; Topological navigation; Aerial robots	ANT MELOPHORUS-BAGOTI; PLACE CELLS; HOMING STRATEGIES; PATH-INTEGRATION; SLAM; MAP; REPRESENTATIONS; HIPPOCAMPUS; FEATURES; SYSTEM	Reliably navigating to a distant goal remains a major challenge in robotics. In contrast, animals such as rats and pigeons can perform goal-directed navigation with great reliability. Evidence from neural science and ethology suggests that various species represent the spatial space as a topological template, with which they can actively evaluate future navigation uncertainty and plan reliable/safe paths to distant goals. While topological navigation models have been deployed in mobile robots, relatively little inspiration has drawn upon biology in terms of topological mapping and active path planning. In this paper, we propose a novel bio-inspired topological navigation model, which consists of topological map construction, active path planning and path execution, for aerial mobile robots with visual landmark recognition and compass orientation capability. To mimic the topological spatial representation, the model firstly builds the topological nodes based on the reliability of visual landmarks, and constructs the edges based on the compass accuracy. Then a reward diffusion algorithm akin to animals' path evaluation process is developed. The diffusion process takes the topological structure and landmark reliability into consideration, which helps the agent to construct the path with visually reliable nodes. In the path execution process, the agent combines orientation guidance and landmark recognition to estimate its position. To evaluate the performance of the proposed navigation model, a systematic series of experiments were conducted in a range of challenging and varied real-world visual environments. The results show that the proposed model generates animal-like navigation behaviours, which avoids travelling across large visually aliased areas, such as forest and water regions, and achieves higher localization accuracy than navigating on the shortest paths.																	0921-0296	1573-0409				OCT	2020	100	1					289	310		10.1007/s10846-020-01190-4		MAY 2020											
J								A Data-Driven Model for Evaluating the Survivability of Unmanned Aerial Vehicle Routes	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Data-driven; Relation learning; Threat assessment; Route survivability	TARGET PURSUIT; MULTIPLE UAVS; PLANNER; FUSION	Evaluating unmanned aerial vehicle (UAV) survivability is crucial when UAVs are required to perform missions in hostile areas. There are complex spatiotemporal interactions among entities in hostile areas; therefore, evaluation of the survivability of a UAV flying along a specific route needs to effectively fuse spatiotemporal information. It is difficult to clarify how information is fused and how threats accumulate along the route. We present a novel solution for building a learnable evaluation model that can extract the required knowledge directly from the data. In this approach, hostile scenarios are decomposed into various threat entities, threat relations (TRs) and UAVs, where a TR is the relation between a threat entity and a UAV. We propose a data-driven evaluation model named the sequential threat inference network (STIN), which can learn TRs and perform spatiotemporal fusion to evaluate survivability. We validate the model in multiple scenarios that contain threat entities of different types, quantities and attributes. The results show that the STIN is superior to the baseline models in various situations. Specifically, the STIN can automatically generalize learned knowledge to scenarios with different numbers of threat entities without retraining. In the generalization experiment, the error increases little when the STIN is directly used in the new scenarios where the number of entities is larger than in the training scenarios.																	0921-0296	1573-0409				NOV	2020	100	2					629	646		10.1007/s10846-020-01197-x		MAY 2020											
J								Semantic Image Segmentation with Improved Position Attention and Feature Fusion	NEURAL PROCESSING LETTERS										Semantic image segmentation; Spatial pooling pyramid; Improved position attention; Feature fusion; Dense feature map		Encoder-decoder structure is an universal method for semantic image segmentation. However, some important information of images will lost with the increasing depth of convolutional neural network (CNN), and the correlation between arbitrary pixels will get worse. This paper designs a novel image segmentation model to obtain dense feature maps and promote segmentation effects. In encoder stage, we employ ResNet-50 to extract features, and then add a spatial pooling pyramid (SPP) to achieve multi-scale feature fusion. In decoder stage, we provide an improved position attention module to integrate contextual information effectively and remove the trivial information through changing the construction way of attention matrix. Furthermore, we also propose the feature fusion structure to generate dense feature maps by preforming element-wise sum operation on the upsampling features and corresponding encoder features. The simulation results illustrate that the average accuracy and mIOU on CamVid dataset can reach 90.7% and 63.1% respectively. It verifies the effectiveness and reliability of the proposed method.																	1370-4621	1573-773X				AUG	2020	52	1			SI		329	351		10.1007/s11063-020-10240-9		MAY 2020											
J								A case study of conditional deep convolutional generative adversarial networks in machine fault diagnosis	JOURNAL OF INTELLIGENT MANUFACTURING										Generative adversarial network; Convolutional neural network; Conditional model; Data augmentation; Machine fault diagnosis	NEURAL-NETWORKS	Due to the real working conditions, the collected mechanical fault datasets are actually limited and always highly imbalanced, which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative model of conditional-deep convolutional generative adversarial network (C-DCGAN) and provide a study in detail. Deep convolutional generative adversarial network (DCGAN), based on traditional generative adversarial networks (GAN), introduces convolutional neural network into the training for unsupervised learning to improve the effect of generative networks. Conditional generative adversarial network (CGAN) is a conditional model obtained through introducing conditional extension into GAN. C-DCGAN is a combination of DCGAN and CGAN. In C-DCGAN, based on the feature extraction ability of convolutional networks, through the structural optimization, conditional auxiliary generative samples are used as augmented data and applied in machine fault diagnosis. Two datasets (Bearing dataset and Planetary gear box dataset) are carried out to validate. The simulation experiments showed that the improved performance is mainly due to the generated signals from C-DCGAN to balance the dataset. The proposed method can deal with imbalanced fault classification problem much more effectively. This model could improve the accuracy of fault diagnosis and the generalization ability of the classifier in the case of small samples and display better fault diagnosis performance.																	0956-5515	1572-8145															10.1007/s10845-020-01579-w		MAY 2020											
J								Weld defect classification in radiographic images using unified deep neural network with multi-level features	JOURNAL OF INTELLIGENT MANUFACTURING										Non-destructive testing; Weld defect classification; Deep neural network; Multi-level features fusion; Stacked auto-encoder	OF-THE-ART; AUTOMATIC CLASSIFICATION	Deep neural network (DNN) exhibits state-of-the-art performance in many fields including weld defect classification. However, there is still a large room for improving the classification performance over the generic DNN models. In this paper, a unified deep neural network with multi-level features is proposed for weld defect classification. Firstly, we define 11 weld defect features as inputs of our proposed classification model. Not limited to geometric and intensity features, 4 features based on the intensity contrast between weld defect and its background are proposed in this paper. Secondly, we construct a novel deep learning framework: a unified deep neural network, where multi-level features of each hidden layer are fused by the last hidden layer to predict the type of weld defect comprehensively. In addition, we investigate pre-training and fine-turning strategies to get better generalization performance with small dataset. Comparing with other classification methods like SVM and generic DNN model, our framework takes full advantage of multi-level features extracted from each hidden layer, an outstanding performance is shown where the classification accuracy is improved by 3.18% and 4.33% on the test dataset, to reach 91.36%.																	0956-5515	1572-8145															10.1007/s10845-020-01581-2		MAY 2020											
J								A novel technique to self-adapt parameters in parallel/distributed genetic programming	SOFT COMPUTING										Genetic programming; Neural networks; Evolutionary computing; Parallel computing; Distributed computing		This paper introduces the Supervisor Evolutionary Algorithm, a novel technique that allows for self-adapt almost all the internal parameters in parallel distributed client-server genetic programming. This novel adapting mechanism, is itself of an evolutionary nature, so we have a double evolutionary tool. The upper level, as is usual in evolutionary computing, has its own customized selection, crossover, and mutation mechanisms. The lower stage used here is the Brain Project a parallel-distributed software tool for formal modelling of numerical data using a hybrid neural-genetic programming technique. As demonstrated by the experiment reported in this paper, our approach works well adapting continuously its internal parameters.																	1432-7643	1433-7479				NOV	2020	24	22					16885	16894		10.1007/s00500-020-04982-w		MAY 2020											
J								Uncertain nonlinear system identification using Jaya-based adaptive neural network	SOFT COMPUTING										Neural nonlinear auto-regressive exogenous (NNARX) model; Jaya algorithm; Nonlinear system identification; Piezoelectric actuator; Nonlinear benchmark test function	PARTICLE SWARM OPTIMIZATION; HYSTERESIS; MODEL; ALGORITHM; SEARCH; DESIGN	The piezoelectric actuator has been receiving tremendous interest in the past decade, due to its broad applications in areas of micro-robotics, neurosurgical robot, MEMS, exoskeleton, medical applications, and other applications. However, the hysteresis nonlinearity widely existing in smart materials yields undesirable responses, which make the hysteresis control problem even more challenging. Therefore, many studies based on artificial neural networks have been developed to cope with the hysteresis nonlinearity. However, the back-propagation algorithm which is popular in training a neural network model often performs local optima with stagnation and slow convergence speed. To overcome these drawbacks, this paper proposes a new training algorithm based on the Jaya algorithm to optimize the weights of the neural NARX model (called Jaya-NNARX). The performance and efficiency of the proposed method are tested on identifying two typical nonlinear benchmark test functions and are compared with those of a classical BP algorithm, particle swarm optimization algorithm, and differential evolution algorithm. Forwardly, the proposed Jaya-NNARX method is applied to identify the nonlinear hysteresis behavior of the piezoelectric actuator. The identification results demonstrate that the proposed algorithm can successfully identify the highly uncertain nonlinear system with perfect precision.																	1432-7643	1433-7479				NOV	2020	24	22					17123	17132		10.1007/s00500-020-05006-3		MAY 2020											
J								Gravitational search algorithm-based optimization of hybrid wind and solar renewable energy system	COMPUTATIONAL INTELLIGENCE										gravitational search algorithm; hybrid renewable energy; nature inspired algorithm; optimization	METHODOLOGY; POWER; PERFORMANCE; MODEL	Due to the issue of environmental protection coupled with high energy demand, there was an initiation for exploration of different renewable energy sources. This article aims to optimize the total annual cost of hybrids of wind and solar renewable energy system to satisfy the predesigned load. Minimization of the total annual cost of the system by determining appropriate numbers of the components, so that the desired load can be economically and reliably satisfied under the given constraints. Gravitational Search Algorithm (GSA) was employed for the optimization process. GSA is a recently proposed metaheuristic algorithm which is based on Newton's universal gravitational law of gravity and mass interactions. It uses stochastic rules to escape local optima and find the global optimal solutions. MATLAB codes were designed for the developed fitness function and employed algorithm. The proposed methodology was run for the fitness function through the code and the results were discussed. The result was compared with the results of Particle Swarm Optimization (PSO) and also shown that: GSA has some advantage over PSO algorithm. Even though, the algorithm has several parameters to be adjusted, it is strong in both local and global optimal searches.																	0824-7935	1467-8640															10.1111/coin.12336		MAY 2020											
J								Integration of AI with reduced order generalized integrator controller for power system harmonic reduction	COMPUTATIONAL INTELLIGENCE										fuzzy logic; harmonic distortion; proportional resonant; reduced order generalized integrator (ROGI); shunt active filter; space vector modulation	VOLTAGE; FILTER; DESIGN	The increased use of electronics for control and use of nonlinear type loads by consumers in the present power system network are injecting harmonics into the power signal, due to which the power quality issue has become more challenge for the present researchers. In this work, the reduced order generalized integrator (ROGI) and fuzzy logic control (FLC) are collectively used to reduce the current harmonics in the power system. FLC-SVM technique is used to maintain the process of SAF-Shunt active filter with fixed switching frequency. This proposed control technique consists of control loop for current to have fast and effective control. The Proportional Resonant controller is used to have control on voltage for a slow and effective control process which is also used to compensate reactive power. The shunt active filter is designed with the help of ROGI by integrating PI controller for fuzzy-based SVM to reduce the current harmonics on the load side. The results are achieved in MATLAB/SIMULINK software.																	0824-7935	1467-8640															10.1111/coin.12335		MAY 2020											
J								Artificial intelligence-based wind forecasting using variational mode decomposition	COMPUTATIONAL INTELLIGENCE										adaptive neuro fuzzy inference system; intrinsic mode functions; mean absolute percentage error; variational mode decomposition; wind speed	ARCHITECTURES	Intermittency in wind offers the major challenge in accomplishing the wind energy as a dependable sustainable energy resource in power grid. Fluctuations in wind speed occur seasonally over a year and if this seasonality is considered, the prediction of the speed of wind can be made more accurate. In this paper, an attempt is made to apply a signal decomposition technique called Variational Mode Decomposition (VMD), which decomposes series of wind speed data into several intrinsic mode functions (IMFs) to make the data more regular thereby enhancing the accuracy of the wind speed forecast model. Then, artificial intelligence technique, Adaptive neuro fuzzy inference system (ANFIS) is applied for the wind speed prediction by combining the obtained modes from VMD. Here, wind data of two sites in India, Jogimatti and Lamba are taken for the study. Each site data is grouped into high and low wind speed months and later, this series is decomposed into regular modes using VMD. Later, ANFIS is applied for training and predicting the wind speed for different time horizons.																	0824-7935	1467-8640															10.1111/coin.12331		MAY 2020											
J								Meta-heuristic firefly approach to multi-servers load balancing with independent and dependent server availability consideration	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud; Load balancing; Dependent task; Independent task; Iterative proximal algorithm (IP); Meta heuristic firefly optimization algorithm (MFOA)	SERVICE AVAILABILITY; OPTIMIZATION	Load balancing is the foremost confront in a cloud environment. Load balancing is assisted to disseminate dynamic workload across many nodes to guarantee that not a single node gets overloaded. In the existing work Iterative Proximal Algorithm is introduced for the load balancing. This current work concentrates on request migration criteria between multiple servers for load balancing. It varies from common load balancing crisis, assume that it is under a disseminated, competitive environment, and is non-cooperative. For every server, its projected response time is taken to be a dis-utility function and its value is reduced. But load balancing with dependent and independent servers is a confronting errand. In order to resolve this challenge, Meta-heuristic scheme is carried out based on firefly algorithm to balance load on multiple servers. The main contribution of this research work is to perform the load balancing to improve the computational efficiency of the task submitted by the users. The anticipated multi-server load balancing is carried out based on dependent and independent tasks. The jobs comprise of various interdependent errands in which independent tasks might be processed in multiple cores of the VM or multiple VMs. The errands come through the server's run-time in arbitrary time intervals for different loads. This crisis is resolved by using variation inequality (VI) theory and confirming that there prevails Nash equilibrium resolution set for the devised game. After this, a Nash equilibrium resolution for multi-server load balancing is calculated by using an Iterative Proximal algorithm (IP) is anticipated with Meta heuristic Firefly Optimization Algorithm. Convergence of IPA algorithm is analyzed so that it gets converged to Nash equilibrium. At last, many numerical computations are performed to confirm theoretical analysis.																	1868-5137	1868-5145															10.1007/s12652-020-02032-9		MAY 2020											
J								An improved Jaya algorithm with a modified swap operator for solving team formation problem	SOFT COMPUTING										Jaya algorithm; Team formation problem; Social networks; Genetic algorithm; Modified swap operator	ECONOMIC OPTIMIZATION; DESIGN OPTIMIZATION	Forming a team of experts that can match the requirements of a collaborative task is an important aspect, especially in project development. In this paper, we propose an improved Jaya optimization algorithm for minimizing the communication cost among team experts to solve team formation problem. The proposed algorithm is called an improved Jaya algorithm with a modified swap operator (IJMSO). We invoke a single-point crossover in the Jaya algorithm to accelerate the search, and we apply a new swap operator within Jaya algorithm to verify the consistency of the capabilities and the required skills to carry out the task. We investigate the IJMSO algorithm by implementing it on two real-life datasets (i.e., digital bibliographic library project and StackExchange) to evaluate the accuracy and efficiency of proposed algorithm against other meta-heuristic algorithms such as genetic algorithm, particle swarm optimization, African buffalo optimization algorithm and standard Jaya algorithm. Experimental results suggest that the proposed algorithm achieves significant improvement in finding effective teams with minimum communication costs among team members for achieving the goal.																	1432-7643	1433-7479				NOV	2020	24	21					16627	16641		10.1007/s00500-020-04965-x		MAY 2020											
J								The first twenty years of agent-based software development with JADE	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										JADE; Agent-oriented software engineering; Software agents	SYSTEMS; FIPA; COMPONENTS; LANGUAGES; ZEUS	A recent survey provides convincing evidence that JADE is among the most widely used tools to develop agent-based software systems. It finds application in industrial settings and to support research, and it has been used to introduce students to software agents in various universities. This paper offers a perspective on the current state of JADE by first presenting a chronicle of the relevant events that contributed to make JADE what it is today. Then, this paper enumerates some of the abstractions that JADE helped to identify and that are now commonly adopted in the community of researchers and practitioners interested in software agents and agent-based software development. Such abstractions have been successfully applied to construct relevant software systems, and among them, this paper reports on a mission-critical system that uses the abstractions that JADE contributed to identify to serve millions of users every day. Finally, this paper discusses an outlook on the near future of JADE by sketching a recent project that could contribute to provide a new perspective on the use of JADE.																	1387-2532	1573-7454				MAY 11	2020	34	2							36	10.1007/s10458-020-09460-z													
J								Modelling monthly mean air temperature using artificial neural network, adaptive neuro-fuzzy inference system and support vector regression methods: A case of study for Turkey	NETWORK-COMPUTATION IN NEURAL SYSTEMS										Artificial neural networks; adaptive neuro-fuzzy inference system; support vector regression; monthly mean air temperature	GLOBAL SOLAR-RADIATION; EXTREME LEARNING-MACHINE; DEW-POINT TEMPERATURE; ELECTRICITY DEMAND; MINIMUM TEMPERATURE; MOISTURE-CONTENT; TIME-SERIES; PREDICTION; ANFIS; MAXIMUM	The accurate modelling and prediction of air temperature values is an exceptionally important meteorological variable that affects in many areas. The present study is aimed at developing models for the prediction of monthly mean air temperature values in Turkey using ANN, ANFIS and SVMr methods. In developing the models, the monthly data derived from eight stations of the TSMS for the 1963-2015 period were used, including latitude, longitude, elevation, month, and minimum, maximum and mean air temperatures. The performances of the ANN, ANFIS and SVMr models were compared using R-2, MSE, MAPE and RRMSE. In order to verify the differences between the predicted temperature values provided by the ANN, ANFIS and SVMr models and the observed temperature values derived from the stations, a t-test analysis was conducted, and the best ANN, ANFIS and SVMr models were determined according to the statistical performance values. These models were then used to make air temperature predictions for the cities. Manova was carried out to determine the effects of the differences temperature predictions and RRMSE values of the models. Generally, the statistical performance values of the ANFIS models were found to be slightly better than those of the ANN and SVMr models.																	0954-898X	1361-6536															10.1080/0954898X.2020.1759833		MAY 2020											
J								Cancer molecular subtype classification from hypervolume-based discrete evolutionary optimization	NEURAL COMPUTING & APPLICATIONS										Classification; Multiobjective optimization; Animal migration optimization algorithm; Gene expression data	PARTICLE SWARM OPTIMIZATION; FEATURE-SELECTION; MICROARRAY DATA; GENE SELECTION; ALGORITHM; CRITERIA	High dimensionality and sample imbalance of gene expression data promote the development of effective algorithms for classifying gene expression data. To improve the ability to distinguish different subtypes of gene expression data, we devise a hypervolume-based discrete evolutionary optimization algorithm (HYBDEOA) in this paper. Four objectives, namely the number of genes, the accuracy, the relevance, and the redundancy, are optimized simultaneously to guide the evolution. Firstly, binary encoding is used to choose some features, projecting data onto different subspaces. After that, a discrete neighborhood operation is conducted to generate a new binary-mapped population. Combining the new population with the current population, we employ the hypervolume-based mechanism to select the Pareto solutions. Finally, a discrete mutation method is proposed to find promising solutions in the binary search space. To demonstrate the performance of HYBDEOA, we apply HYBDEOA to 55 synthetic datasets and 35 cancer gene expression datasets. Extensive experiments are also conducted to reveal the effectiveness and efficiency of HYBDEOA. The experimental results demonstrate that our proposed method is a parameter-less and robust algorithm, which can group gene expression data with a finer and more informative classification.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15489	15502		10.1007/s00521-020-04846-2		MAY 2020											
J								Machine learning and data analytics for the IoT	NEURAL COMPUTING & APPLICATIONS										Cybersecurity; Internet of Things; Intelligent systems; Machine learning	DATA INJECTION ATTACKS; BIG DATA ANALYTICS; CONNECTED VEHICLES; DETECTION SYSTEM; SEMANTIC WEB; INTERNET; THINGS; REVOLUTION; PATTERNS; EDGE	The Internet of Things (IoT) applications have grown in exorbitant numbers, generating a large amount of data required for intelligent data processing. However, the varying IoT infrastructures (i.e., cloud, edge, fog) and the limitations of the IoT application layer protocols in transmitting/receiving messages become the barriers in creating intelligent IoT applications. These barriers prevent current intelligent IoT applications to adaptively learn from other IoT applications. In this paper, we critically review how IoT-generated data are processed for machine learning analysis and highlight the current challenges in furthering intelligent solutions in the IoT environment. Furthermore, we propose a framework to enable IoT applications to adaptively learn from other IoT applications and present a case study in how the framework can be applied to the real studies in the literature. Finally, we discuss the key factors that have an impact on future intelligent applications for the IoT.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16205	16233		10.1007/s00521-020-04874-y		MAY 2020											
J								A novel hybrid multi-verse optimizer with K-means for text documents clustering	NEURAL COMPUTING & APPLICATIONS										Multi-verse optimizer (MVO); Hybridization; Text clustering; k-Means clustering	ARTIFICIAL BEE COLONY; KRILL HERD ALGORITHM; SWARM OPTIMIZATION; BAT ALGORITHM; CUCKOO SEARCH	Text clustering has been widely utilized with the aim of partitioning specific document collection into different subsets using homogeneity/heterogeneity criteria. It has also become a very complicated area of research, including pattern recognition, information retrieval, and text mining. Metaheuristics are typically used as efficient approaches for the text clustering problem. The multi-verse optimizer algorithm (MVO) involves a stochastic population-based algorithm. It has been recently proposed and successfully utilized to tackle many hard optimization problems. However, a recently applied research trend involves hybridizing two or more algorithms with the aim of obtaining a superior solution regarding the problems of optimization. In this paper, a new hybrid of MVO algorithm with the K-means clustering algorithm is proposed, i.e., the H-MVO algorithm with the aims of enhancing the quality of initial candidate solutions, as well as the best solution, which is produced by MVO at each iteration. This hybrid algorithm aims at improving the global (diversification) ability of the search and finding a better cluster partition. The proposed H-MVO effectiveness was tested on five standard datasets, which are used in the domain of data clustering, as well as six standard text datasets, which are utilized in the domain of text document clustering, in addition to two scientific articles' datasets. The experiments showed that K-means hybridized MVO improves the results in terms of high convergence rate, accuracy, error rate, purity, entropy, recall, precision, and F-measure criteria. In general, H-MVO has outperformed or at least proven to be highly competitive compared to the original MVO algorithm and with well-known optimization algorithms like KHA, HS, PSO, GA, H-PSO, and H-GA and the clustering techniques like K-mean, K-mean++, DBSCAN, agglomerative, and spectral clustering techniques.																	0941-0643	1433-3058															10.1007/s00521-020-04945-0		MAY 2020											
J								Lung nodules detection using semantic segmentation and classification with optimal features	NEURAL COMPUTING & APPLICATIONS										Computer-aided detection (CAD) system; Computerized tomography (CT) scan; Acquisition; Segmentation; Classification; Principal components analysis (PCA)	COMPUTER-AIDED DETECTION; PULMONARY NODULES; AUTOMATIC DETECTION; CT IMAGES	Lung cancer is a deadly disease if not diagnosed in its early stages. However, early detection of lung cancer is a challenging task due to the shape and size of its nodules. Radiologists use automated tools for more precise opinion. Automated detection of the affected lung nodules is complicated because of the shape similarity among healthy and unhealthy tissues. Over the years, several expert systems have been developed that help radiologists to diagnose lung cancer effectively. In this article, we have proposed a framework to precisely detect lungs cancer to classify the benign and malignant nodules. The proposed framework is tested using the subset of the publicly available dataset, i.e., the Lung Image Database Consortium image collection (LIDC-IDRI). We applied filtering and noise removal in the pre-processing phase. Furthermore, the adaptive thresholding technique (OTSU) and the semantic segmentation are used to accurately detect the unhealthy lung nodules. Overall, 13 nodules features have extracted using principal components analysis algorithm. In addition, four optimal features are selected based on the classification performance. In the classification phase, 9 different classifiers are employed for the experimentation. Empirical analysis shows that the proposed system outperformed other techniques and provides 99.23% accuracy using a logit boost classifier.																	0941-0643	1433-3058															10.1007/s00521-020-04870-2		MAY 2020											
J								Privacy preserving distributed training of neural networks	NEURAL COMPUTING & APPLICATIONS										Decentralized neural network training; Data privacy; Weight averaging; Distributed ledger technology; IPFS; IOTA		Learnae is a system aiming to achieve a fully distributed way of neural network training. It follows a "Vires in Numeris" approach, combining the resources of commodity personal computers. It has a full peer-to-peer model of operation; all participating nodes share the exact same privileges and obligations. Another significant feature of Learnae is its high degree of fault tolerance. All training data and metadata are propagated through the network using resilient gossip protocols. This robust approach is essential in environments with unreliable connections and frequently changing set of nodes. It is based on a versatile working scheme and supports different roles, depending on processing power and training data availability of each peer. In this way, it allows an expanded application scope, ranging from powerful workstations to online sensors. To maintain a decentralized architecture, all underlying tech should be fully distributed too. Learnae's coordinating algorithm is platform agnostic, but for the purpose of this research two novel projects have been used: (1) IPFS, a decentralized filesystem, as a means to distribute data in a permissionless environment and (2) IOTA, a decentralized network targeting the world of low energy "Internet of Things" devices. In our previous work, a first approach was attempted on the feasibility of using distributed ledger technology to collaboratively train a neural network. Now, our research is extended by applying Learnae to a fully deployed computer network and drawing the first experimental results. This article focuses on use cases that require data privacy; thus, there is only exchanging of model weights and not training data.																	0941-0643	1433-3058															10.1007/s00521-020-04880-0		MAY 2020											
J								Fast simultaneous image super-resolution and motion deblurring with decoupled cooperative learning	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image super-resolution; Motion deblurring; Decoupled cooperative learning	SPARSE REPRESENTATION; SALIENCY DETECTION	In recent years, deep convolutional neural networks (CNNs) have been widely applied to handle low-level vision problems. However, most existing CNN-based approaches can either handle single degeneration each time or treat them jointly through feature entangling, thus likely leading to poor performance when the actual degradation is inconsistent with hypothetical degradation condition. Furthermore, feature coupling will bring a large amount of computation, which may make the methods impractical to real-time mobile scenarios. In order to address these problems, we propose a deep decoupled cooperative learning model which can not only develop the corresponding recover network to deal with each degradation, but also flexibly handle multiple degradations at the same time. Thus, our approach can achieve disentangling and synthesizing single image super-resolution and motion deblurring, which has high practicability. We evaluate the proposed approach on various benchmark datasets, covering both natural images and synthetic images. The results demonstrate its superiority, compared to the state-of-the-art, where image SR and motion deblurring can be accomplished effectively concurrently. The source code of the work is available at https://github.com/hengliusky/Cooperative-Learning-Deblur-SR..																	1861-8200	1861-8219															10.1007/s11554-020-00976-x		MAY 2020											
J								Soft computing approach for multi-objective task allocation problem in wireless sensor network	EVOLUTIONARY INTELLIGENCE										WSN; PSO; Binary PSO; NSGA-II; Multi-objective optimization; Pareto dominance		Sensor nodes of a wireless sensor network (WSN) are resource constrained and the real time applications of WSN may exceed the computational capacity of a particular sensor node. Thus, such real-time applications of WSN cannot be completed by a single sensor node in many cases, but the problem can be solved by distributing the task among multiple sensor nodes. Thus, given a set of sensor nodes and a computationally heavy task to be executed, to find best suitable set of sensor nodes from the available sensor nodes to complete the assigned task is an important research problem in the WSN domain. This allows the system to utilize the resources of a sensor node in a better way and to enhance the parallel processing capacity of WSN. The sensor nodes should be selected for a task such that, with the selected set of nodes, the task can be completed in an efficient manner in terms of resource consumption. The problem of task allocation is to select best suitable set of sensor nodes for a task considering the energy consumption, communication over head, network life time and computational requirements. In this paper, we propose two methods for this problem, namely modified multi-objective binary particle swarm optimization (MOMBPSO) and non-dominated sorting genetic algorithm-II (NSGA-II) for task allocation in WSN. We carried out extensive simulation experiments with varying number of iterations, sensor nodes and number of tasks. Simulation results show that modified binary PSO performs better in terms of energy consumption and NSGA-II is performing better in terms of spread of solutions compared to MOMBPSO.																	1864-5909	1864-5917															10.1007/s12065-020-00412-w		MAY 2020											
J								Integration of Facial Thermography in EEG-based Classification of ASD	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Autism spectrum disorder; facial thermography; EEG signal processing; machine learning; decision support system; ASDGenus	DIAGNOSTIC OBSERVATION SCHEDULE; AUTISM; SPECTRUM; METHODOLOGY; FEVER; ALGORITHMS; FRACTALITY; RESPONSES; SEVERITY; CHILDREN	Autism spectrum disorder (ASD) is a neurodevelopmental disorder affecting social, communicative, and repetitive behavior. The phenotypic heterogeneity of ASD makes timely and accurate diagnosis challenging, requiring highly trained clinical practitioners. The development of automated approaches to ASD classification, based on integrated psychophysiological measures, may one day help expedite the diagnostic process. This paper provides a novel contribution for classifing ASD using both thermographic and EEG data. The methodology used in this study extracts a variety of feature sets and evaluates the possibility of using several learning models. Mean, standard deviation, and entropy values of the EEG signals and mean temperature values of regions of interest (ROIs) in facial thermographic images were extracted as features. Feature selection is performed to filter less informative features based on correlation. The classification process utilizes Naive Bayes, random forest, logistic regression, and multi-layer perceptron algorithms. The integration of EEG and thermographic features have achieved an accuracy of 94% with both logistic regression and multi-layer perceptron classifiers. The results have shown that the classification accuracies of most of the learning models have increased after integrating facial thermographic data with EEG.																	1476-8186	1751-8520															10.1007/s11633-020-1231-6		MAY 2020											
J								Fuzzy Logic in Dynamic Parameter Adaptation of Harmony Search Optimization for Benchmark Functions and Fuzzy Controllers	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Harmony search algorithm; Dynamic parameter adaptation; Fuzzy logic; Benchmark mathematical functions; Fuzzy controller	MULTIMODAL OPTIMIZATION; ALGORITHM; COMPETITION; OPERATOR	Nowadays the use of fuzzy logic has been increasing in popularity, and this is mainly due to the inference mechanism that allows simulating human reasoning in knowledge-based systems. The main contribution of this work is using the concepts of fuzzy logic in a method for dynamically adapting the main parameters of the harmony search algorithm during execution. Dynamical adaptation of parameters in metaheuristics has been shown to improve performance and accuracy in a wide range of applications. For this reason, we propose and approach for fuzzy adaptation of parameters in harmony search. Two case studies are considered for testing the proposed approach, the optimization of mathematical functions, which are unimodal, multimodal, hybrid, and composite functions and a control problem without noise and when noise is considered. A statistical comparison between the harmony search algorithm and the fuzzy harmony search algorithm is presented to verify the advantages of the proposed approach.																	1562-2479	2199-3211				JUN	2020	22	4					1198	1211		10.1007/s40815-020-00860-7		MAY 2020											
J								Invariant packet feature with network conditions for efficient low rate attack detection in multimedia networks for improved QoS	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multimedia networks; Network conditions; Routing; Invariant features; Low rate attacks; QoS	DDOS ATTACKS; ENTROPY	The problem of low rate attack detection has been well studied in different situations. However the methods suffer to achieve higher performance in low rate attack detection. The multimedia transmission is focused on transmitting video and audio which claims higher bandwidth conditions. There exists no such algorithm in detecting low rate attacks for invariant network conditions. To solve this issue, an invariant feature based approach is presented in this paper. The method maintains the network features like the routes, bandwidth conditions and traffic. Based on these features, a set of routes has been identified for each data transmission. Here, low rate attack detection is performed at the reception of any packet and the data transmission is performed using cooperative routing. From the packet features, and the route being followed, the method identifies the class of route, traffic and bandwidth conditions of the route. Using these features, the method computes Network Transmission Support measure. Based on the NTS value, the method performs low rate attack detection and improves the performance.																	1868-5137	1868-5145															10.1007/s12652-020-02056-1		MAY 2020											
J								Efficiently Testing Digital Convexity and Recognizing Digital Convex Polygons	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Digital geometry; Digital convexity; Convex; Digital polyhedron; Digital polyhedron recognition; Quickhull; Polygonal separation	ALGORITHM; HULL; STRAIGHTNESS; DIMENSION; POINTS; TIME	A set S subset of Z(2) of integer points is digital convex if conv(S) boolean AND Z(2) = S, where conv(S) denotes the convex hull of S. In this paper, we consider the following two problems. The first one is to test whether a given set S of n lattice points is digital convex. If the answer to the first problem is positive, then the second problem is to find a polygon P subset of Z(2) with minimum number of edges and whose intersection with the lattice P boolean AND Z(2) is exactly S. We provide linear-time algorithms for these two problems. The algorithm is based on the well-known quickhull algorithm. The time to solve both problems is O(n + h log r) = O(n + n(1/3) log r), where h = min(vertical bar conv(S)vertical bar, n(1/3)) and r is the diameter of S.																	0924-9907	1573-7683				JUN	2020	62	5			SI		693	703		10.1007/s10851-020-00957-6		MAY 2020											
J								Transfer learning by mapping and revising boosted relational dependency networks	MACHINE LEARNING										Transfer learning; Statistical relational learning; Theory revision	CLAUSE	Statistical machine learning algorithms usually assume the availability of data of considerable size to train the models. However, they would fail in addressing domains where data is difficult or expensive to obtain. Transfer learning has emerged to address this problem of learning from scarce data by relying on a model learned in a source domain where data is easy to obtain to be a starting point for the target domain. On the other hand, real-world data contains objects and their relations, usually gathered from noisy environments. Finding patterns through such uncertain relational data has been the focus of the Statistical Relational Learning (SRL) area. Thus, to address domains with scarce, relational, and uncertain data, in this paper, we propose TreeBoostler, an algorithm that transfers the SRL state-of-the-art Boosted Relational Dependency Networks learned in a source domain to the target domain. TreeBoostler first finds a mapping between pairs of predicates to accommodate the additive trees into the target vocabulary. After, it employs two theory revision operators devised to handle incorrect relational regression trees aiming at improving the performance of the mapped trees. In the experiments presented in this paper, TreeBoostler has successfully transferred knowledge between several distinct domains. Moreover, it performs comparably or better than learning from scratch methods in terms of accuracy and outperforms a transfer learning approach in terms of accuracy and runtime.																	0885-6125	1573-0565				JUL	2020	109	7			SI		1435	1463		10.1007/s10994-020-05871-x		MAY 2020											
J								Dual-Path Part-Level Method for Visible-Infrared Person Re-identification	NEURAL PROCESSING LETTERS										Cross-modality person re-identification; Visible-infrared; Human part-level; Loss function		Visible-infrared cross-modality person re-identification is a realistic problem of person re-identification. Under poor illumination scenario, general methods of visible-visible person re-identification can not solve the problem well. If we directly compare the visible images of pedestrians captured under dark lighting with the visible images of pedestrians captured under normal light, this extreme color deviation will greatly reduce the recognition ability of the learned representations. In this paper, we propose a dual-path framework for visible-infrared cross-modality person re-identification based human part level features. Feature learning module contains modality-specific dual-path layers and modality-shared human part-level layers, which achieve discriminative global and local representations. In order to better optimize the proposed network, we design a global loss function and a local loss function for the global features and local features, respectively. The two loss functions are integrated together to train the network. We verify the effectiveness of our method on the challenging benchmarks: SYSU-MM01 and RegDB. Experimental results show that, compared with other cross-modality methods, our method has better effect in improving visible-infrared cross-modality person re-identification tasks.																	1370-4621	1573-773X				AUG	2020	52	1			SI		313	328		10.1007/s11063-020-10239-2		MAY 2020											
J								DeepBot: a time-based botnet detection with deep learning	SOFT COMPUTING										Botnet; Deep learning; RNN		Over the decades, as the technology of Internet thrives rapidly, more and more kinds of cyber-attacks are blasting out around the world. Among them, botnet is one of the most noxious attacks which has always been challenging to overcome. The difficulties of botnet detection stem from the various forms of attack since the viruses keep evolving to avoid themselves from being found. Rule-based botnet detection has its shortcoming of detecting dynamically changing features. On the other hand, the more the Internet functionalities are developed, the severer the impacts botnets may cause. In recent years, many network devices have suffered from botnet attacks as the Internet of things technology prospers, which caused great damage in many industries. Consequently, botnet detection has always been a critical issue in computer security field. In this paper, we introduce a method to detect potential botnets by inspecting the behaviors of network traffics from network packets. In the beginning, we sample the given packets by a period of time and extract the behavioral features from a series of packets. By analyzing these features with proposed deep learning models, we can detect the threat of botnets and classify them into different categories.																	1432-7643	1433-7479				NOV	2020	24	21					16605	16616		10.1007/s00500-020-04963-z		MAY 2020											
J								Adaptive neural fuzzy inference system-based scheduler for cyber-physical system	SOFT COMPUTING										Cyber-physical system; Dynamic scheduler; Fuzzy logic; ANFIS; EDF; ELST		Task scheduling is one of the challenging research problems in distributed computing, especially in a complex scenario like a cyber-physical system. The cyber-physical system consists of a physical system and cyber system which are operated on the different domain of response time. The performance of the scheduling algorithm under the cyber-physical system depends on both cyber and physical factors. But both the factors are unpredictable one in reality which makes the scheduling a challenging one. This paper proposes a fuzzy logic controller based on an efficient scheduler which tackles the above problem. The proposed dynamic scheduler involves three scheduling algorithms which will be selected by the fuzzy controller based on the dynamic behavior of a cyber-physical system. A neural network is incorporated into the fuzzy controller to provide the learning capability, and an adaptive neural fuzzy inference system (ANFIS) is designed. The simulation results demonstrate the superiority of the proposed mechanism in comparison with the existing one.																	1432-7643	1433-7479				NOV	2020	24	22					17309	17318		10.1007/s00500-020-05020-5		MAY 2020											
J								A hybrid genetic algorithm for scientific workflow scheduling in cloud environment	NEURAL COMPUTING & APPLICATIONS										Cloud computing; Genetic algorithm; Scientific workflow; Workflow scheduling; Deadline; Budget	EXECUTION; SYSTEM; TASKS	Nowadays, we live an unprecedented evolution in cloud computing technology that coincides with the development of the vast amount of complex interdependent data which make up the scientific workflows. All these circumstances developments have made the issue of workflow scheduling very important and of absolute priority to all overlapping parties as the provider and customer. For that, work must be focused on finding the best strategy for allocating workflow tasks to available computing resources. In this paper, we consider the scientific workflow scheduling in cloud computing. The main role of our model is to optimize the time needed to run a set of interdependent tasks in cloud and in turn reduces the computational cost while meeting deadline and budget. To this end, we offer a hybrid approach based on genetic algorithm for modelling and optimizing a workflow-scheduling problem in cloud computing. The heterogeneous earliest finish time (HEFT), an heuristic model, intervenes in the generation of the initial population. Based on results obtained from our simulations using real-world scientific workflow datasets, we demonstrate that the proposed approach outperforms existing HEFT and other strategies examined in this paper. In other words, experiments show high efficiency of our proposed approach, which makes it potentially applicable for cloud workflow scheduling. For this, we develop a GA-based module that was integrated to the WorkflowSim framework based on CloudSim.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15263	15278		10.1007/s00521-020-04878-8		MAY 2020											
J								Enhanced pedestrian detection using optimized deep convolution neural network for smart building surveillance	SOFT COMPUTING										Pedestrian detection; Deep learning; Convolution neural network; Machine learning	COMPUTER VISION; EFFICIENT; TRACKING	Pedestrian detection and tracking is a critical task in the area of smart building surveillance. Due to advancements in sensors, the architects concentrate in construction of smart buildings. Pedestrian detection in smart building is greatly challenged by the image noises by various external environmental parameters. Traditional filter-based techniques for image classification like histogram of oriented gradients filters and machine learning algorithms suffer to perform well for huge volume of pedestrian input images. The advancements in deep learning algorithms perform exponentially good in handling the huge volume of image data. The current study proposes a pedestrian detection model based on deep convolution neural network (CNN) for classification of pedestrians from the input images. Proposed optimized version of VGG-16 architecture is evaluated for pedestrian detection on the INRIA benchmarking dataset consisting of 227 x 227 pixel images. The proposed model achieves an accuracy of 98.5%. It was found that proposed model performs better than the other pretrained CNN architectures and other machine learning models. Pedestrians are reasonably detected and the performance of the proposed algorithm is validated.																	1432-7643	1433-7479				NOV	2020	24	22					17081	17092		10.1007/s00500-020-04999-1		MAY 2020											
J								Flow driven attention network for video salient object detection	IET IMAGE PROCESSING										image motion analysis; video signal processing; neural nets; object detection; image segmentation; image sequences; feature extraction; attention mechanism; fusing optical flow; appearance features; feature map; state-of-the-art method flow-guided recurrent neural encoder; Densely Annotated Video Segmentation; Freiburg-Berkeley Motion Segmentation; flow driven attention network; video salient object detection; convolutional neural network; still-image based saliency detectors; flow-driven attention network; FDAN; motion information; appearance feature extractor; motion-guided attention module; saliency map regression module; infers; ultimate saliency map	MODEL	Salient object detection has been revolutionised by convolutional neural network (CNN) recently. However, it is hard to transfer the state-of-the-art still-image based saliency detectors to videos directly, owing to the neglect of temporal contexts between frames. In this study, the authors propose a flow-driven attention network (FDAN) to exploit motion information for video salient object detection. FDAN consists of an appearance feature extractor, a motion-guided attention module and a saliency map regression module. It extracts the appearance feature per frame, refines appearance feature with optical flow and infers the ultimate saliency map, respectively. Motion-guided attention module is the core of FDAN, which extracts motion information in the form of attention. This attention mechanism is a two-branch CNN, fusing optical flow and appearance features. In addition, a shortcut connection is applied to the attention multiplied feature map for noise suppression intensively. Experimental results show that the proposed method can achieve performance on par with the state-of-the-art method flow-guided recurrent neural encoder on challenging benchmarks of Densely Annotated Video Segmentation and Freiburg-Berkeley Motion Segmentation while being two times faster in detection.																	1751-9659	1751-9667				MAY 11	2020	14	6					997	1004		10.1049/iet-ipr.2019.0836													
J								Lossless digital image watermarking in sparse domain by using K-singular value decomposition algorithm	IET IMAGE PROCESSING										discrete cosine transforms; iterative methods; singular value decomposition; image watermarking; data encapsulation; image representation; image coding; lossless digital image watermarking; K-singular value decomposition algorithm; watermarking technique; sparse elements; host image; DCT coefficients; sparse coefficients; sparse domain orthogonal matching pursuit algorithm; inverse DCT; hidden secret message; robust lossless sparse domain-based watermarking approach; discrete cosine transform; sparse representation-based dictionary learning process; regularised parameters; secret message extraction stage; peak signal-to-noise ratio; structural similarity; normal correlation; feature similarity; Gaussian attack; salt and pepper attacks; speckle attacks; rotate attacks; crop attacks; fold attacks; blur attack	SAR IMAGES; SEGMENTATION; RECOGNITION; ENERGY	The crucial hurdle faced by the watermarking technique is to maintain the steadiness corresponding to several attacks while assisting a sufficient level of security. In this study, a robust lossless sparse domain-based watermarking approach combined with discrete cosine transform (DCT) is introduced to hide the secret message in the selected significant sparse elements of the host image. The proposed method takes advantage of a sparse representation-based dictionary learning process. To enhance the security of the original image, the authors first apply the DCT on a secret message. These DCT coefficients with some regularised parameters will be inserted into the selected significant sparse coefficients. At the extraction stage, the secret message is extracted from those significant sparse coefficients by employing the sparse domain orthogonal matching pursuit algorithm. Finally, the inverse DCT is applied to extract the secret message without any information loss. To show the effectiveness of the proposed method, different commonly used attacks are simulated. Simulation results in terms of peak signal-to-noise ratio, structural similarity, normal correlation, and feature similarity indicate that the proposed method can recover the hidden secret message accurately against seven different types of attacks including speckle, Gaussian, salt and pepper, rotate, crop, fold, and blur attack.																	1751-9659	1751-9667				MAY 11	2020	14	6					1005	1014		10.1049/iet-ipr.2018.6040													
J								Parallel strength Pareto evolutionary algorithm-II based image encryption	IET IMAGE PROCESSING										chaos; cryptography; genetic algorithms; evolutionary computation; Pareto optimisation; image processing; search problems; parallel strength Pareto evolutionary algorithm-II based image encryption; chaotic maps; poor value assignments; chaotic map un-chaotic; hyper-parameter tuning; meta-heuristic based image encryption approaches; strength Pareto evolutionary algorithm-II based meta-heuristic approach; four-dimensional chaotic map	HYBRID GENETIC ALGORITHM; ONLY MULTIMEDIA CIPHERS; QUANTITATIVE CRYPTANALYSIS; MEDICAL IMAGES; CHAOTIC SYSTEM; STREAM CIPHER; SCHEME; SECURE; EFFICIENT; CLOUD	In recent years, many image encryption approaches have been proposed on the basis of chaotic maps. The various types of chaotic maps such as one-dimensional and multi-dimensional have been used to generate the secret keys. Chaotic maps require some parameters and value assignment to these parameters is very crucial. Because, poor value assignments may make the chaotic map un-chaotic. Therefore, hyper-parameter tuning of chaotic maps is required. Recently, meta-heuristic based image encryption approaches have been designed by researchers to resolve this issue. However, the majority of the techniques suffer from poor computational speed and stuck in local optima problems. Therefore, in this study, a strength Pareto evolutionary algorithm-II based meta-heuristic approach is proposed to tune the hyper-parameters of the four-dimensional chaotic map. The proposed approach is also implemented in a parallel fashion to enhance the computational speed. The effectiveness of the proposed approach is evaluated through extensive experiments. Comparative analyses show that the proposed approach outperforms the competitive approaches in terms of entropy, NPCR, UACI, and PSNR by $0.9834$0.9834, $ 1.0728$1.0728, $ 0.9134$0.9134, and $ 0.8971\%$0.8971%, respectively.																	1751-9659	1751-9667				MAY 11	2020	14	6					1015	1026		10.1049/iet-ipr.2019.0587													
J								Blind quality assessment for 3D synthesised video with binocular asymmetric distortion	IET IMAGE PROCESSING										rendering (computer graphics); video signal processing; statistical analysis; image texture; distortion; cameras; binocular asymmetric distortion; blind quality assessment method; 3D synthesised video; local edge deformations; objective quality score; texture information; asymmetry 3D-SV database	IMAGE; VIEWS; INFORMATION	During the process of watching 3D synthesised video (3D-SV) and switching viewpoints, there is a case of asymmetric distortion, the left(right) viewpoint is a synthesised video generated by rendering technique, and the right(left) viewpoint is a real video taken by the camera. How to accurately estimate the quality of 3D-SV with binocular asymmetric distortions is a new and challenging problem. Aiming at this problem, a blind quality assessment method for 3D-SV with binocular asymmetric distortions is proposed. Firstly, the local edge deformations of synthesised videos at different scales are measured by calculating their standard deviations. Secondly, the global naturalness of synthesised videos is computed by analysing their natural statistical characteristics. Thirdly, a strategy for fusing left and right quality scores is proposed, which considers their texture information in different directions. Finally, the random forest is used to obtain an objective quality score. The experimental results show the superiority of the proposed method on asymmetry 3D-SV database.																	1751-9659	1751-9667				MAY 11	2020	14	6					1027	1034		10.1049/iet-ipr.2019.1085													
J								Inhomogeneous morphological PDEs for robust and adaptive image shock filters	IET IMAGE PROCESSING										image enhancement; mathematical morphology; edge detection; iterative methods; adaptive filters; partial differential equations; image filtering; equivalent sup-inf-based formulations; robust filters; adaptive filters; PDE-based methods; synthetic colour images; robust image shock filters; adaptive image shock filters; classical morphological filters; noisy environment; intrinsic image structures; robust shock filters; multiscale morphological operators; image edge functions; local weights; inhomogeneous Hamiltonians; partial differential equations; inhomogeneous morphological PDE; greyscale; colour images	MATHEMATICAL MORPHOLOGY; DIFFERENTIAL-EQUATIONS; ENHANCEMENT; SEGMENTATION; OPERATORS	Classical morphological filters suffer from well performing in a noisy environment, and intrinsic image structures are not taken into account. The authors propose here an alternative to overcome such weaknesses, by properly using robust shock filters and inhomogeneity. Thus, they obtain multiscale morphological operators by using image edge functions as local weights in inhomogeneous Hamiltonians in classical multiscale dilations/erosions formulated with partial differential equations (PDEs). They provide the equivalent sup-inf-based formulations, and derive sharpening/enhancement methods. In addition, they establish the PDE associated with the asymptotical iterations of the proposed robust and adaptive filters. The good behaviours of the proposed sup-inf and PDE-based methods are illustrated on synthetic, greyscale, and colour images; results are analysed both qualitatively and quantitatively.																	1751-9659	1751-9667				MAY 11	2020	14	6					1035	1046		10.1049/iet-ipr.2019.0086													
J								Image reflection removal using end-to-end convolutional neural network	IET IMAGE PROCESSING										computer vision; learning (artificial intelligence); object detection; convolutional neural nets; end-to-end convolutional neural network; single image reflection removal; deep learning strategies; mixed reflection image cascaded edges; deep convolutional encoder-decoder network; identical encoder-decoder network structure; reflection edge; image reflection layer; network model; image dataset; synthetic mixed reflection image; RRnet	TRANSPARENT LAYERS; SEPARATION	Single image reflection removal is an ill-posed problem. To solve this problem, this study develops a network structure based on a deep encoder-decoder RRnet. Unlike most deep learning strategies applied in this context, the authors find that redundant information increases the difficulty of predicting images on the network; thus, the proposed method uses mixed reflection image cascaded edges as input to the network. The proposed network structure is divided into two parts: the first part is a deep convolutional encoder-decoder network. Its function uses the mixed reflection image and the target edge as input to predict the target layer. The second part is an identical encoder-decoder network structure. Its function uses the mixed reflection image and the reflection edge as input to predict the image reflection layer. In addition, the authors use joint loss to optimise the network model. To train the neural network, they also create an image dataset for reflection removal, which includes a true mixed reflection image and a synthetic mixed reflection image. They use four evaluation indicators to evaluate the proposed method and the other six methods. The experimental results indicate that the proposed method is superior to previous methods.																	1751-9659	1751-9667				MAY 11	2020	14	6					1047	1058		10.1049/iet-ipr.2019.0247													
J								Dual attention convolutional network for action recognition	IET IMAGE PROCESSING										image sequences; video signal processing; image motion analysis; feature extraction; image recognition; convolutional neural nets; image colour analysis; action recognition; spatial features; temporal features; two-stream ConvNets; Dual Attention ConvNet; dual attention mechanism; spatial attention; temporal attention; 2D ConvNet; two-stream convolutional networks; RGB frames; multiple video frames; UCF-101 benchmark; HMDB-51 benchmark	HISTOGRAMS	Action recognition has been an active research area for many years. Extracting discriminative spatial and temporal features of different actions plays a key role in accomplishing this task. Current popular methods of action recognition are mainly based on two-stream Convolutional Networks (ConvNets) or 3D ConvNets. However, the computational cost of two-stream ConvNets is high for the requirement of optical flow while 3D ConvNets takes too much memory because they have a large amount of parameters. To alleviate such problems, the authors propose a Dual Attention ConvNet (DANet) based on dual attention mechanism which consists of spatial attention and temporal attention. The former concentrates on main motion objects in a video frame by using ConvNet structure and the latter captures related information of multiple video frames by adopting self-attention. Their network is entirely based on 2D ConvNet and takes in only RGB frames. Experimental results on UCF-101 and HMDB-51 benchmarks demonstrate that DANet gets comparable results among leading methods, which proves the effectiveness of the dual attention mechanism.																	1751-9659	1751-9667				MAY 11	2020	14	6					1059	1065		10.1049/iet-ipr.2019.0963													
J								End-to-end learning interpolation for object tracking in low frame-rate video	IET IMAGE PROCESSING										video signal processing; learning (artificial intelligence); object tracking; interpolation; mobile computing; low frame rates; implicit video frame interpolation sub-network; object tracking; low frame-rate video; high frame-rate latent video; effective end-to-end optimisation; frame rate; tracking accuracy; semantic video analytics; end-to-end learning interpolation; subsequent semantic analytics; bandwidth constraints; analytics performance	MOTION ESTIMATION	In many scenarios, where videos are transmitted through bandwidth-limited channels for subsequent semantic analytics, the choice of frame rates has to balance between bandwidth constraints and analytics performance. Faced with this practical challenge, this study focuses on enhancing object tracking at low frame rates and proposes a learning Interpolation for tracking framework. This framework embeds an implicit video frame interpolation sub-network, which is concatenated and jointly trained with another object tracking sub-network. Once a low frame-rate video is an input, it is first mapped into a high frame-rate latent video, based on which the tracker is learned. Novel strategies and loss functions are derived to ensure the effective end-to-end optimisation of the authors' network. On several challenging benchmarks and settings, their method achieves a highly competitive tradeoff between frame rate and tracking accuracy. As is known, the implications of interpolation on semantic video analytics and tracking remain unexplored, and the authors expect their method to find many applications in mobile embedded vision, Internet of Things and edge computing.																	1751-9659	1751-9667				MAY 11	2020	14	6					1066	1072		10.1049/iet-ipr.2019.0944													
J								Cooperation: A new force for boosting generative adversarial nets with dual-network structure	IET IMAGE PROCESSING										learning (artificial intelligence); image resolution; solid modelling; given data distribution; generative model; discriminative model; mode collapse; dual generators; cooperation mechanism; single-generator network; dual-generator network; dual networks; network parameters; dual-generator structure; multiple generators; high-resolution image generation; 3D model generation; generative adversarial net; dual-network structure		The principle of generative adversarial net is to fit the given data distribution by combining a generative model and discriminative model. There are two major challenges to conventional systems - they are difficult to train and they easily fall into 'mode collapse'. To improve it, this study describes a novel network structure with dual generators. A 'cooperation' mechanism is introduced to help the generators work together. During training, generators not only learn from discriminative feedback but also from each other (like a study group). Compared with a single-generator network, a dual-generator network could capture many more 'modes' and eventually reduce the impact of 'mode collapse.' Dual networks also require extra computational resources. However, our experiment shows that even with network parameters of similar size, dual networks still achieved better results. Additionally, a dual-generator structure could be extended to multiple generators. The proposed network structure is also very robust and flexible. It can be adapted to various application scenarios, such as high-resolution image generation, domain adaptation and 3D model generation. The experimental results showed that with the same computing resources, multiple generators can generate better quality synthetic data, including 2D images, 3D objects, style transferring etc.																	1751-9659	1751-9667				MAY 11	2020	14	6					1073	1080		10.1049/iet-ipr.2019.0149													
J								CPGAN: Conditional patch-based generative adversarial network for retinal vessel segmentation	IET IMAGE PROCESSING										medical image processing; biomedical optical imaging; image segmentation; blood vessels; learning (artificial intelligence); eye; sensitivity analysis; retinal blood vessels; diagnostic biomarker; unified loss function; patch-based generative adversarial network-based technique; generator network; conditional patch-based generative adversarial network; retinal vessel segmentation; CPGAN; deep learning methods; diabetic retinopathy; ophthalmologic retinopathy; spatial features; biased distribution; fundoscopic images; receiver operating characteristic curves	BLOOD-VESSELS; MATCHED-FILTER; IMAGES; EXTRACTION; MODEL	Retinal blood vessels, the diagnostic bio-marker of ophthalmologic and diabetic retinopathy, utilise thick and thin vessels for diagnostic and monitoring purposes. The existing deep learning methods attempt to segment the retinal vessels using a unified loss function. However, a difference in spatial features of thick and thin vessels and a biased distribution creates an imbalanced thickness, rendering the unified loss function to be useful only for thick vessels. To address this challenge, a patch-based generative adversarial network-based technique is proposed which iteratively learns both thick and thin vessels in fundoscopic images. It introduces an additional loss function that allows the generator network to learn thin and thick vessels, while the discriminator network assists in segmenting out both vessels as a combined objective function. Compared with state-of-the-art techniques, the proposed model demonstrates the enhanced accuracy, sensitivity, specificity, and area under the receiver operating characteristic curves on STARE, DRIVE, and CHASEDB1 datasets.																	1751-9659	1751-9667				MAY 11	2020	14	6					1081	1090		10.1049/iet-ipr.2019.1007													
J								DRAN: Deep recurrent adversarial network for automated pancreas segmentation	IET IMAGE PROCESSING										computerised tomography; medical image processing; cancer; biological organs; image segmentation; deep recurrent adversarial network; automated pancreas segmentation; abdominal computed tomography scans; pancreas cancer diagnosis; previous segmentation methods; contextual spatial correlation; short-term memory module; spatial interaction; clinical segmentation tool; convolution autoencoder module; CT scan patches		Automated pancreas segmentation in abdominal computed tomography (CT) scans is of high clinical relevance (i.e. pancreas cancer diagnosis and prognosis), but extremely difficult because the pancreas is a soft, small, and flexible abdominal organ with high anatomical variability, which causes the previous segmentation methods to result in low precision. In this study, the authors present a new deep recurrent adversarial network (DRAN) to tackle this challenge. DRAN contains three steps: (i) preserving global resolution of CT scans and modifying the receptive field of kernel adaptively through a dilated convolution autoencoder module; (ii) modelling contextual spatial correlation between neighbouring CT scan patches benefits from a specially designed local long short-term memory module; and (iii) improving the performance and generalisation by leveraging an adversarial module, which can constrain the spatial smoothness consistency between continuous CT scans based on the long-range spatial interaction. The system is evaluated on a dataset of 80 manually segmented CT volumes, using four-fold cross-validation. Its performance surpasses other state-of-the-art methods, with the Dice similarity coefficient of $89.87\pm 3.17\%$89.87 +/- 3.17% and pixel-wise accuracy of $95.85\pm 3.04\%$95.85 +/- 3.04%. Also, they perform a qualitative evaluation by an expert further revealing the effectiveness and potential of their DRAN as a clinical segmentation tool.																	1751-9659	1751-9667				MAY 11	2020	14	6					1091	1100		10.1049/iet-ipr.2019.0399													
J								Writer identification with n-tuple direction feature from contour	IET IMAGE PROCESSING										handwriting recognition; feature extraction; natural language processing; text-independent writer identification; contour-hinge feature; correct writer identification rate; n-tuple direction feature; Farsi database; English database	CODEBOOK	This study introduces an effective solution for text-independent writer identification by generalising contour-hinge feature, which is called n-tuple direction feature. For extracting n-tuple direction feature, the authors first obtain all contours from connected components, then n + 1 points are considered on the contour with a certain distance apart, and next, the directions of the fragments connecting two successive points are computed. The n + 1 points move on the contour and the n-dimensional histogram of directions is computed. The proposed method is evaluated on large Farsi and English databases. A correct writer identification rate of 92.2% for English handwritings from 900 persons and 97.7% for Farsi handwritings from 600 persons are achieved. Comparison between the proposed method and other studies shows the promising performance and superiority of the proposed method.																	1751-9659	1751-9667				MAY 11	2020	14	6					1101	1109		10.1049/iet-ipr.2018.6391													
J								New adaptive histogram equalisation heuristic approach for contrast enhancement	IET IMAGE PROCESSING										statistical distributions; image enhancement; adaptive parameter; modified CDF; low contrast images; bright contrast images; image contrast; artificial effects; input images; new adaptive histogram equalisation heuristic approach; contrast enhancement; immense brightness change; probability distribution function; PDF; cumulative distribution function; simple histogram equalisation technique; adaptive heuristic HE approach; image enhancement	MODIFICATION SCHEME; IMAGE-CONTRAST	Contrast enhancement of an image can be performed by using a simple histogram equalisation (HE) technique. However, there are some drawbacks of HE like immense brightness change, artificial effects, over-enhancement, which make it unsuitable to be used in many applications. To resolve these issues a new adaptive heuristic HE approach is proposed in this study. First, probability distribution function (PDF) of the image is calculated. Second, an adaptive parameter is calculated based on the mean and maximum values of that PDF. Thereafter, PDF and cumulative distribution function (CDF) are modified by applying a threshold limit to that adaptive parameter. Finally, another adaptive parameter is finding out by using modified CDF and a new CDF is obtained by using this second adaptive parameter. Traditional HE is then applied with the new CDF to getting the enhanced image. The visual and quantitative results of the proposed method outperform all other state-of-the-art papers and works well both for low and bright contrast images simultaneously. After rigorous experiment, it is concluded that the authors' method enhances the image contrast very well with no over-enhancement or artificial effects in the images and also preserves the original characteristics of the input images.																	1751-9659	1751-9667				MAY 11	2020	14	6					1110	1119		10.1049/iet-ipr.2019.0106													
J								Image encryption using a combination of Grain-128a algorithm and Zaslavsky chaotic map	IET IMAGE PROCESSING										chaos; cryptography; image sequences; image coding; data protection; image encryption; chaotic systems; design algorithms; secret key; two-dimensional Zaslavsky chaotic map; bit confusion; diffusion process; medical images; grain-128a algorithm; encoding message process; confidential image data protection; grain-128a stream cipher algorithm; fixed length 256-bit secret key; current 128; 0 A	SYSTEM	Encryption is a very important way to secure data in storage and communication, and it is a process of encoding messages or information in such a manner that only authorised persons can access it. Different techniques are used to protect confidential image data against illicit access. In image encryption using chaotic systems, most authors use or design algorithms to generate the initial parameters' values from the secret key. However, as the key size depends on the number of these parameters, the used algorithms show little sensitivity to small changes in the key. To enhance both security and sensitivity in the choice of the initial parameters, this work combines the use of the Grain-128a stream cipher algorithm with two-dimensional Zaslavsky chaotic map. Firstly, the Grain-128a algorithm is applied to generate the required parameters of Zaslavsky's chaotic map from a fixed length 256-bit secret key. Secondly, the sequences generated by the chaotic map are used to encrypt the image using a bit confusion and diffusion process. The simulation results on greyscale, colour, binary, indexed, and medical images together with the scores obtained in the evaluation of the algorithm show that the proposed method is very sure and effective in encrypting images of any size and any type.																	1751-9659	1751-9667				MAY 11	2020	14	6					1120	1131		10.1049/iet-ipr.2019.0671													
J								Two-stage image smoothing based on edge-patch histogram equalisation and patch decomposition	IET IMAGE PROCESSING										minimisation; image colour analysis; image texture; feature extraction; smoothing methods; gradient methods; image segmentation; image enhancement; edge detection; content-aware image resizing; edge-patch histogram equalisation; structural edges; two-stage image smoothing method; texture region; image segmentation; edge map; image patches; edge distribution; nonedge-patch; edge pixel rate; smooth component; patch boundaries; image processing; edge detection; image abstraction; L-0 gradient minimisation	NOISE REMOVAL	Part of important structural edges in the image is smoothed due to the small gradients, while the others are preserved with greater gradients. Therefore, the authors propose a two-stage image smoothing method based on edge-patch histogram equalisation and patch decomposition. The authors' purpose is to increase the gradient of important structural edges while reducing the gradient of the texture region. Therefore, they divide the image into edge-patches where the structural edges are concentrated or non-edge-patches where the texture details are concentrated by image segmentation. The edge-patch needs to be equalised by the histograms for increasing the gradient of the edge pixels. All patches are decomposed to extract the smooth component for reducing the gradient of pixels. The smooth component of each patch is smoothed via $L_0$L0 gradient minimisation. In order to ensure the continuity of the patch boundaries, the edge-patch is inversely equalised. Finally, the whole image is smoothed via $L_0$L0 gradient minimisation for removing residual textures and seams. Experimental results demonstrate that the proposed method is more competitive in maintaining important structural edges and removing texture details than the state-of-the-art approaches. The proposed method can be applied to many areas of image processing.																	1751-9659	1751-9667				MAY 11	2020	14	6					1132	1140		10.1049/iet-ipr.2019.0484													
J								Smoke detection in ship engine rooms based on video images	IET IMAGE PROCESSING										support vector machines; feature extraction; object detection; computer vision; image classification; ships; video signal processing; smoke; smoke detectors; fire safety; image motion analysis; ship engine rooms; fire detection systems; heat detection; video smoke detection systems; VSD system; smoke generator; motion detection; support vector machine classifier; nonsmoke region; real-time smoke detection systems; video frames; video images; local binary pattern descriptor; feature vector extraction; machine vision	MICROSCOPIC IMAGES; CLASSIFICATION; EXTRACTION; FEATURES; MODEL	Fire detection systems in ships are based on smoke and heat detection in accordance with safety regulations. The rapid advancement of machine vision technology has led to the development of video smoke detection (VSD) systems. In this study, a VSD system is applied to smoke detection within the engine room of the ship. A dataset for a range of scenarios was created with a smoke generator. The method for smoke detection was based on motion detection and a support vector machine classifier, which was used to make candidate regions and perform classification. A local binary pattern descriptor was used to extract the feature vector. A training set was made from a variety of video frames, randomly. Experimental results seldom produced false positive windows in the non-smoke region. However, if the greyscale value of difference image between background and the smoke is lower than the setting value for motion detection, the system could not detect smoke. Processing time is sufficiently fast for use in real-time smoke detection systems. To install a VSD system on-board a vessel, the authors recommend a performance standard of the system which must be met.																	1751-9659	1751-9667				MAY 11	2020	14	6					1141	1149		10.1049/iet-ipr.2018.5305													
J								Joint rain and atmospheric veil removal from single image	IET IMAGE PROCESSING										image colour analysis; visibility; rain; Gaussian processes; image enhancement; image restoration; learning (artificial intelligence); joint rain; atmospheric veil removal; single image; natural rainy scenes; nearby individual rain streaks; atmospheric veiling effect; distant accumulated rain; rain accumulation; generalised rain model; atmospheric light; natural heavy rain scenario	MODEL	In natural rainy scenes, visibility is significantly degraded by two types of phenomena: specular highlights of nearby individual rain streaks and atmospheric veiling effect caused by distant accumulated rain. However, most existing deraining methods only take the first kind of degradation into consideration, which limits their potential application in heavy rain. In this study, a joint rain and atmospheric veil removal framework is proposed to address this problem. Since rain streaks and rain accumulation are entangled with each other, which is intractable to simulate, causing clean/rainy image pairs of real-world are hard to generate. Hence, after introducing a generalised rain model, which can represent both rain streaks and atmospheric veil physically, the authors do not learn the mapping function between image pairs using deep-learning architecture, but estimate the rain streaks, transmission, and atmospheric light via Gaussian mixture model patch prior and dark channel prior to solve the rain model instead. According to the comprehensive experimental evaluations, the proposed method outperforms other state-of-the-art methods in terms of both high visibility and vivid colour, especially in natural heavy rain scenario.																	1751-9659	1751-9667				MAY 11	2020	14	6					1150	1156		10.1049/iet-ipr.2019.0952													
J								Efficient symmetric image encryption by using a novel 2D chaotic system	IET IMAGE PROCESSING										image processing; transforms; white noise; chaos; image coding; cryptography; white noise image; two-dimensional chaotic system; 2D chaotic maps; different 1D chaotic maps; wider chaotic ranges; complex chaotic behaviours; existing 1D chaotic maps; novel image encryption algorithm; chaotic sequences; scrambled image; efficient symmetric image encryption; novel 2D chaotic system; digital images security	ALGORITHM; MAP; TRANSFORM	The authors know that a common and effective way to protect digital images security are to encrypt these images into white noise image. In this study, the authors have designed a new two-dimensional (2D) chaotic system which is derived from two existing one-dimensional (1D) chaotic maps. The simulation results show that the new 2D chaotic system is able to produce many 2D chaotic maps by selecting different 1D chaotic maps, and which have the wider chaotic ranges and more complex chaotic behaviours compared with the existing 1D chaotic maps. In order to investigate its applications, using the proposed 2D chaotic maps, the authors design a novel image encryption algorithm. First of all, the original image is scrambled by using the chaotic sequences which are generated by new 2D chaotic maps, Arnold transform and Hilbert curve. Then the scrambled image is confused and diffused by chaotic sequences. Finally, the performance of the proposed encryption algorithm is simulated and the experimental results show that the validity and reliability of the proposed algorithm is validated.																	1751-9659	1751-9667				MAY 11	2020	14	6					1157	1163		10.1049/iet-ipr.2019.0551													
J								Image DAEs based on residual entropy maximum	IET IMAGE PROCESSING										image denoising; maximum entropy methods; mean square error methods; Gaussian noise; image restoration; learning (artificial intelligence); convolutional neural nets; image DAEs; residual entropy maximum; image denoising; low signal-to-noise ratio; nonGaussian noise; image processing; improved convolution neural network auto-encoders; denoising auto-encoders; end-to-end mappings; noisy images; target images; image restoration quality; maximum entropy principle; training loss function; ordinary DAEs; residual statistics; improved training algorithm; augmented Lagrange function method; image information; improved DAEs; original mean-square-error loss function DAEs; priors based methods; peak SNR; Riesz feature similarity metric indexes; image residual statistical features	SPARSE; REPRESENTATIONS	Image denoising under low signal-to-noise ratio (SNR) and non-Gaussian noise is still a challenging problem in image processing. In this study, the authors prose a kind of improved convolution neural network auto-encoders for image denoising. Different from other priors based methods, the denoising auto-encoders (DAEs) can learn end-to-end mappings from noisy images to the target ones. This study research statistical features of image residual between the restored images and target images. According to the maximum entropy principle, the training loss function of the ordinary DAEs was modified with residual statistics as the constraint condition, and an improved training algorithm was proposed based on augmented Lagrange function method. Thus, the quality of restored image can be improved through removing image information from residual more efficiently. Experiments show not only the denoising effects of improved DAEs is superior to the original mean-square-error loss function DAEs in both peak SNR and Riesz feature similarity metric indexes, but also has the ability to suppress the different types of noises with different levels through a single model.																	1751-9659	1751-9667				MAY 11	2020	14	6					1164	1169		10.1049/iet-ipr.2018.5929													
J								Empirical wavelet transform-based fog removal via dark channel prior	IET IMAGE PROCESSING										object detection; inverse transforms; image enhancement; image restoration; equalisers; wavelet transforms; fog; image colour analysis; fog removal technique; DCP; empirical wavelet transformation coefficients; foggy input image; wavelet coefficients; embedded wavelet transformed image; output image; adaptive histogram equalisation technique; inverse transformed image; sharp contrast output; high contrast output; contrast-enhanced image; S-channel; V-channel gain adjustment; dark channel; image processing; surveillance system; defogging techniques; colour-line model; dense fog		Haze and fog removing from videos and images has got massive concentration in the field of video and image processing because videos and images are severely affected by fog in tracking and surveillance system, object detection. Different defogging techniques proposed so far are based on polarisation, colour-line model, anisotropic diffusion, dark channel prior (DCP) etc. However, these methods are unable to produce output image with desirable quality in the presence of dense fog and sky region. In this study, the authors have proposed a novel fog removal technique where DCP is applied on the low-frequency component of empirical wavelet transformation coefficients of the foggy input image. They apply unsharp masking on wavelet coefficients of the embedded wavelet transformed image for improving the sharpness of the output image. Later contrast limited adaptive histogram equalisation technique is used as a post-processing task to the inverse transformed image for producing the sharp and high contrast output. Finally, the colour and intensity of the contrast-enhanced image are uplifted through S-channel and V-channel gain adjustment. The proposed method provides significant improvement to the overall quality of the output image compared to contemporary techniques. The quantitative and qualitative measurements confirm the claims.																	1751-9659	1751-9667				MAY 11	2020	14	6					1170	1179		10.1049/iet-ipr.2019.0496													
J								Robust random walk for leaf segmentation	IET IMAGE PROCESSING										image segmentation; probability; biology computing; robust random walk; imaging conditions; leaf shapes; pairwise pixels; illumination; leaf surface; nonlocal pixels; specified connected pixels; smoothed leaf segmentation; robust leaf segmentation; unconstrained leaf images; smooth segmentation edges	OF-THE-ART; SHAPE PRIOR; IMAGE; OPTIMIZATION; LEAVES	In this study, the authors focus on the task of leaf segmentation under different imaging conditions (e.g. backgrounds and shadows). A new method - robust random walk (RW) is proposed to propagate the prior of user's specified pixels. Specifically, they first employ RWs to take the relationship of pairwise pixels into consideration. A superpixel-consistent constraint is added to make the edges of segmentation smooth. Owing to the effect of illumination, some parts of a leaf surface are brighter than others and it may further harm the subsequent label propagation. To address this problem, they learn a common subspace by taking into account the illumination of local and non-local pixels. By doing so, it has good adaptability to process noise interfering and non-uniform illumination. In addition, since RW only considers the pairwise relationship of pixels, it will be sensitive to the specified and connected pixels. Thus, they further employ a log-likelihood ratio to predict the probability of a pixel belonging to the background and use it to guide the label propagation. Based on the proposed method, they can obtain a smoothed and robust leaf segmentation. Experimental results on unconstrained leaf images demonstrate the efficiency of their algorithm.																	1751-9659	1751-9667				MAY 11	2020	14	6					1180	1186		10.1049/iet-ipr.2018.6255													
J								Compressive sensed video recovery via iterative thresholding with random transforms	IET IMAGE PROCESSING										compressed sensing; filtering theory; image reconstruction; image denoising; medical image processing; transforms; discrete wavelet transforms; iterative methods; video signal processing; discrete cosine transforms; compressive; video recovery; random transforms; iterative thresholding algorithm; fixed sparsifying; iteration; different transforms; resulting pixel value; simple example; block-based 2D discrete cosine; random selection; fixed block size; random shift; random 2D discrete wavelet; video block-matching; frame residual computation algorithm	IMAGE; RECONSTRUCTION	The authors consider the problem of compressive sensed video recovery via iterative thresholding algorithm. Traditionally, it is assumed that some fixed sparsifying transform is applied at each iteration of the algorithm. In order to improve the recovery performance, at each iteration the thresholding could be applied for different transforms in order to obtain several estimates for each pixel. Then the resulting pixel value is computed based on obtained estimates using simple averaging. However, calculation of the estimates leads to significant increase in reconstruction complexity. Therefore, the authors propose a heuristic approach, where at each iteration only one transform is randomly selected from some set of transforms. First, they present simple examples, when block-based 2D discrete cosine transform is used as the sparsifying transform, and show that the random selection of the block size at each iteration significantly outperforms the case when fixed block size is used. Second, building on these simple examples, they apply the proposed approach when video block-matching and 3D filtering (VBM3D) is used for the thresholding and show that the random transform selection within VBM3D allows to improve the recovery performance as compared with the recovery based on VBM3D with fixed transform.																	1751-9659	1751-9667				MAY 11	2020	14	6					1187	1200		10.1049/iet-ipr.2019.0661													
J								Generalised deep learning framework for HEp-2 cell recognition using local binary pattern maps	IET IMAGE PROCESSING										medical image processing; learning (artificial intelligence); image texture; image classification; biological techniques; deep learning based image classification; ensemble approach; texture information; local binary patterns maps; effective cell image classifier; generalised deep learning framework; HEp-2 cell recognition; HEp-2 cell image classifier; local binary pattern mapping; serum evaluation	INDIRECT IMMUNOFLUORESCENCE IMAGES; CLASSIFICATION; FEATURES; ANA; AUTOANTIBODIES; SEGMENTATION; VARIABILITY; NETWORK	The authors propose a novel HEp-2 cell image classifier to improve the automation process of patients' serum evaluation. The authors' solution builds on the recent progress in deep learning based image classification. They propose an ensemble approach using multiple state-of-the-art architectures. They incorporate additional texture information extracted by an improved version of local binary patterns maps, $\alpha $alpha LBP-maps, which enables to create a very effective cell image classifier. This innovative combination is trained on three publicly available datasets and its general applicability is demonstrated through the evaluation on three independent test sets. The presented results show that their approach leads to a general improvement of performance on average on the three public datasets.																	1751-9659	1751-9667				MAY 11	2020	14	6					1201	1208		10.1049/iet-ipr.2019.0705													
J								Securing DICOM images by a new encryption algorithm using Arnold transform and Vigenere cipher	IET IMAGE PROCESSING										image processing; data compression; cryptography; image colour analysis; image coding; novel encryption method; medical applications; size 16 x 16 pixels; image block-by-block; Vigenere cipher algorithm; Arnold; colour images; JPEG compression; typical image encryption algorithm; recent image encryption algorithm; DICOM images	SCHEME; SYSTEM	This study presents a novel encryption method for the security of DICOM (Digital Imaging and Communications in Medicine) images used in medical applications. The proposed algorithm splits the original image into blocks of size 16 x 16 pixels, then encrypts it through three steps. Firstly, the keys k(1), k(2), k(3) and k(4) are transformed from four vectors of 16 pixels to a matrix of 16 x 16 pixels. Then, the proposed algorithm encrypts the image block-by-block using the Vigenere cipher algorithm. For each block, the proposed algorithm modifies the key using Arnold transform. The proposed encryption algorithm is scalable with colour images and JPEG (Joint Photographic Experts Group) compression. The cryptanalysis of the proposed algorithm demonstrates that it passed the cryptography attacks tests with success. Its running time shows that it is faster than a typical and recent image encryption algorithm.																	1751-9659	1751-9667				MAY 11	2020	14	6					1209	1216		10.1049/iet-ipr.2019.0042													
J								An information theoretic approach to quantify the stability of feature selection and ranking algorithms	KNOWLEDGE-BASED SYSTEMS										Feature selection; Feature ranking; Stability; Robustness; Jensen-Shannon divergence	CANCER; CLASSIFICATION; PREDICTION; DIVERGENCE	Feature selection is a key step when dealing with high-dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information-theoretic approach based on the Jensen-Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and being able to give more importance to the disagreements that appear at the top of the list. Moreover, it possesses desirable properties including correction for change, upper/lower bounds and conditions for a deterministic selection. We illustrate the use of this stability metric with data generated in a fully controlled way and compare it with popular metrics including the Spearman's rank correlation and the Kuncheva's index on feature ranking and selection outcomes, respectively. Additionally, experimental validation of the proposed approach is carried out on a real-world problem of food quality assessment showing its potential to quantify stability from different perspectives. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105745	10.1016/j.knosys.2020.105745													
J								Political Optimizer: A novel socio-inspired meta-heuristic for global optimization	KNOWLEDGE-BASED SYSTEMS										Optimization; Political Optimizer (PO); Meta-heuristics; Socio-inspired algorithm; Past-based position updating; Human behavior-based optimization	PARTICLE SWARM OPTIMIZATION; LEARNING-BASED OPTIMIZATION; ENGINEERING OPTIMIZATION; METAHEURISTIC ALGORITHM; DIFFERENTIAL EVOLUTION; DESIGN OPTIMIZATION; SEARCH	This paper proposes a novel global optimization algorithm called Political Optimizer (PO), inspired by the multi-phased process of politics. PO is the mathematical mapping of all the major phases of politics such as constituency allocation, party switching, election campaign, inter-party election, and parliamentary affairs. The proposed algorithm assigns each solution a dual role by logically dividing the population into political parties and constituencies, which facilitates each candidate to update its position with respect to the party leader and the constituency winner. Moreover, a novel position updating strategy called recent past-based position updating strategy (RPPUS) is introduced, which is the mathematical modeling of the learning behaviors of the politicians from the previous election. The proposed algorithm is benchmarked with 50 unimodal, multimodal, and fixed dimensional functions against 15 state of the art algorithms. We show through experiments that PO has an excellent convergence speed with good exploration capability in early iterations. Root cause of such behavior of PO is incorporation of RPPUS and logical division of the population to assign dual role to each candidate solution. Using Wilcoxon rank-sum test, PO demonstrates statistically significant performance over the other algorithms. The results show that PO outperforms all other algorithms, and consistency in performance on such a comprehensive suite of benchmark functions proves the versatility of the algorithm. Furthermore, experiments demonstrate that PO is invariant to function shifting and performs consistently in very high dimensional search spaces. Finally, the applicability on real-world applications is demonstrated by efficiently solving four engineering optimization problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105709	10.1016/j.knosys.2020.105709													
J								Bayesian optimisation in unknown bounded search domains	KNOWLEDGE-BASED SYSTEMS										Bayesian optimisation; Experimental design; Hyperparameter tuning	GLOBAL OPTIMIZATION	Bayesian optimisation (BO) is one of the most sample efficient methods for determining the optima of expensive, noisy black-box functions. Despite its tremendous success in scientific discovery and hyperparameter tuning, it still requires a bounded search space. The search spaces boundaries are, however, often chosen heuristically with an educated guess. If the boundaries are misspecified, then the search space may either be unnecessarily large and hence more expensive to optimise, or it may simply not contain the global optimum. In this paper, we introduce a method for dynamically determining the bound directly from the data. This is done using a distribution of the bound derived in a Bayesian setting. The prior is chosen by the user and the likelihood is derived with Thompson sampling. This results in a bound that is both cheap to optimise and has a high probability of containing the global optimum. We compare the performance of our method with the alternative methods on a range of synthetic and real-world problems and demonstrate that our method achieves consistently superior results. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105645	10.1016/j.knosys.2020.105645													
J								Classifying emotions in Stack Overflow and JIRA using a multi-label approach	KNOWLEDGE-BASED SYSTEMS										Multi-label classification; Emotion classification; Stack Overflow; Jira Issue Tracker	SMOTE	A forum or social media post can express multiple emotions, such as love, joy or anger. Emotion classification has been proven useful for measuring aspects such as user satisfaction. Despite its usefulness, research in emotion classification is limited, because the task is multi-label and publicly available data sets and lexica are very limited. A number of emotion classifiers for general-domain text have been proposed recently, but only a few for text in the domain of Open Source Software (OSS), such as EmoTxt. In this paper, we explore different lexica and two multi-label algorithms for classifying emotions in text related to OSS. We trained various multi-label classifiers using HOMER and RAkEL on a data set of Stack Overflow posts and a data set of JIRA Issue Tracker comments. The classifiers have been enriched with features derived from different state-of-the-art lexica. We achieved multi-label Micro F scores up to 0.811 and Subset 0/1 Loss of 0.290. These results represent a statistically significant improvement over the state-of-the-art. (C) 2020 The Authors. Published by Elsevier B.V.																	0950-7051	1872-7409				MAY 11	2020	195								105633	10.1016/j.knosys.2020.105633													
J								Multi-view semi-supervised learning for classification on dynamic networks	KNOWLEDGE-BASED SYSTEMS										Semi-supervised learning; Multi-view learning; Dynamic networks; Total variation	ALGORITHM	In recent decades, the task of graph-based multi-view learning has become a fundamental research problem, which could integrate data from multiple sources to improve performance. The dynamic networks could be treated as one kind of multi-view network, but it is continually evolving and leads to entirely different observations at multiple epochs. In this paper, we treat these observations as multiple views and seek a semi-supervised multi-view approach to address the classification problem. Therefore, we propose Multi-view Semi-supervised learning for Classification on Dynamic networks (MSCD). With the aid of total variation regularization, MSCD can obtain a sparse and smooth combination of the views and a better classification result. From the theoretical point of view, the MSCD model is decomposed into simpler sub-problems, which can be effectively solved under the Alternating Direction Method of Multipliers (ADMM) framework. Extensive experiments on both synthetic and real-world datasets show that our model can outperform the state-of-the-art approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105698	10.1016/j.knosys.2020.105698													
J								Stable sparse subspace embedding for dimensionality reduction	KNOWLEDGE-BASED SYSTEMS										Dimensionality reduction; Feature projection; Random projection; Sparse; Stable		Sparse random projection (RP) is a popular tool for dimensionality reduction that shows promising performance with low computational complexity. However, in the existing sparse RP matrices, the positions of non-zero entries are usually randomly selected. Although they adopt uniform sampling with replacement, due to large sampling variance, the number of non-zeros is uneven among rows of the projection matrix which is generated in one trial, and more data information may be lost after dimension reduction. To break this bottleneck, based on random sampling without replacement in statistics, this paper builds a stable sparse subspace embedded matrix (S-SSE), in which non-zeros are uniformly distributed. It is proved that the S-SSE is stabler than the existing matrix, and it can maintain Euclidean distance between points well after dimension reduction. Our empirical studies corroborate our theoretical findings and demonstrate that our approach can indeed achieve satisfactory performance. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105639	10.1016/j.knosys.2020.105639													
J								SMLBoost-adopting a soft-margin like strategy in boosting	KNOWLEDGE-BASED SYSTEMS										Boosting; Classification; Noise sensitivity; SMLBoost; Soft margin	ENSEMBLE; CLASSIFICATION; SMOTE	Boosting is well known for its excellence on regular datasets, nevertheless, boosting can also easily overfit the wrong patterns when the training set contains noise. This paper aims at tackling boosting's noise sensitivity via simulating a soft-margin like procedure. In contrast to the previous studies, which build soft-margin via introducing some regularization terms into their loss functions, this paper aims to build a clean marginal (border) region near the classification hyperplane, thereby, the principle of margin can be rebuilt for boosting in the noisy scenario. The key ingredients of the proposed method consist in a noise detection strategy and a weight updating mechanism, which progressively assigns higher weight to the un-noisy instances that have been pushed into the marginal region defined by an unsupervised margin. We conducted experiments on 20 UCI datasets and 44 imbalanced datasets from KEEL. Experimental results not only demonstrate the superiority of the proposed method over other robust boostings on noisy datasets, they also confirm when combined with oversampling technique that might erroneously generate noisy samples to balance the class distribution, the proposed method can deliver promising imbalanced learning ability. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105705	10.1016/j.knosys.2020.105705													
J								Defining and detecting k-bridges in a social network: The Yelp case, and more	KNOWLEDGE-BASED SYSTEMS										k-bridge; k-bridge detection algorithm; Multi-network scenario; Influencers; Analysis of co-reviewers; Yelp; Reddit; PATSTAT-ICRIOS	COMMUNITY DISCOVERY; IMPACT; MEDIA	In this paper, we introduce the concept of k-bridge (i.e., a user who connects k sub-networks of the same network or k networks of a multi-network scenario) and propose an algorithm for extracting k-bridges from a social network. Then, we analyze the specialization of this concept and algorithm in Yelp and we extract several knowledge patterns about Yelp k-bridges. In particular, we investigate how some basic characteristics of Yelp k-bridges vary against k (i.e., against the number of macro-categories which the businesses reviewed by them belong to). Then, we verify if there exists an influence exerted by k-bridges on their friends and/or on their co-reviewers. We also analyze the relationship between k-bridges and power users. In addition, we investigate the relationship between k-bridges and the main centrality measures in the macro-categories of Yelp. We also propose two further specializations of k-bridges, regarding Reddit and the network of patent inventors, to prove that the knowledge on k-bridges we initially found in Yelp is not limited to this social network. Finally, we present two use cases that can highly benefit from the knowledge on k-bridges detected through our approach. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105721	10.1016/j.knosys.2020.105721													
J								Decomposition-based Bayesian network structure learning algorithm using local topology information	KNOWLEDGE-BASED SYSTEMS										Bayesian network; Structure learning; Model decomposition; Local neighborhood structure	PRIME SUBGRAPH DECOMPOSITION; SEARCH; EFFICIENT	Hybrid learning algorithms which integrate the merits of the constraint-based methods and the search-and-score methods are used to cope with Bayesian network (BN) structure estimation problem. However, such simple and crude synthesis techniques always consider the global topology information during the learning process and attempt to directly search for the optimal network structure in the enormous solution space for large-scale BNs, resulting in prohibitive computational cost as well as low learning accuracy. Therefore, we propose a novel hybrid structure learning algorithm based on the idea of model decomposition, which takes into account the knowledge of local neighborhood structures. The proposed method works in four stages. We first draft an undirected independence graph by using an efficient Markov blanket discovery approach, and then split the entire network into a series of subgraphs. After learning the small BNs from the observed data, the resultant topology can be obtained by combining these small BNs. Experiments on different benchmark BNs and the varying data sets demonstrate that the proposed algorithm generally gains the better performance of structure recovery than other representative methods, especially for large-scale BNs. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105602	10.1016/j.knosys.2020.105602													
J								CRGA: Homographic pun detection with a contextualized-representation	KNOWLEDGE-BASED SYSTEMS										Homographic pun detection; Ambiguity; Contextualized representation; Gated attention; Bi-GRU; CNN; Self-attention		Detecting a homographic pun is one of the fundamental research tasks in natural language processing. A homographic pun is able to produce humor through the latent relationship between the pun and its semantically similar target. Puns have been widely applied in written and spoken forms of human language and have a long history. However, the ambiguity of a homographic pun is still a large challenge that cannot be addressed well with current methods. To alleviate this problem, we present a novel contextualized-representation gated attention (CRGA) network for the detection of homographic puns. This architecture has several advantages that can be exploited: one is that a contextual representation is used across varying linguistic contexts to address the polysemy of homographic puns; another other is that the CRGA model is able to detect homographic puns by combining the global semantic understanding, local script understanding, pun characteristic self-attention and gated mechanism. With this design, the CRGA model can effectively capture the polysemy information, which is helpful for homographic pun detection. The experimental results based on the common SemEval2017 Task? and Pun of the Day datasets demonstrate the effectiveness and advancement of our proposed CRGA model. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				MAY 11	2020	195								105056	10.1016/j.knosys.2019.105056													
J								Unsupervised bin-wise pre-training: A fusion of information theory and hypergraph	KNOWLEDGE-BASED SYSTEMS										Deep neural network; Mutual information; Information theory; Partial information decomposition; Hypergraph	DEEP NEURAL-NETWORKS; WEIGHT INITIALIZATION; MUTUAL INFORMATION; FEATURE-SELECTION; BACKPROPAGATION; SYSTEM; SPEED	Pre-training is considered to be a triggering point' for Deep Neural Networks which gains attraction of the researchers. Though recent research works focus on designing efficient pre-training models, they often fail to capture the relevant information representations across the layers with minimum turns and to maintain the stability of the learning model. This research article presents a novel unsupervised bin-wise pre-training model which fuses Information Theory and Hypergraph that acts as an effective optimizer: speed up the learning process & minimize generalization loss, and also as an impelling regularizer: maintain the stability of the Deep Neural Network. The proposed model is evaluated using three different benchmark datasets and the experimental results confirm the supremacy of the proposed model over the state-of-the-art approaches in speeding up the learning process and minimizing generalization loss without deteriorating the stability of Deep Neural Network. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105650	10.1016/j.knosys.2020.105650													
J								Discovering knowledge combinations in multidimensional collaboration network: A method based on trust link prediction and knowledge similarity	KNOWLEDGE-BASED SYSTEMS										Knowledge combination; Knowledge transfer; Knowledge similarity; Trust link prediction; Multilayered network	GROUP DECISION-MAKING; FEEDBACK MECHANISM; CONSENSUS MODEL; SOCIAL NETWORK; CREATIVITY; COMMERCE; DESIGN	Discovering knowledge combination has been considered an effective strategy for knowledge retrieval and knowledge discovery. Generally, knowledge combination driven by close-cooperation can be achieved via modeling the process of knowledge transfer. However, the existing studies seldom built connections between knowledge transfer and the identification of knowledge combination, especially in the existing knowledge transfer models, less attention is paid to the effects of trust and knowledge similarity. Therefore, the research motivations of this paper are to model the process of knowledge transfer and to further discover knowledge combinations. To minimize the risks of knowledge transfer, both the knowledge similarity and the trust embodied within need to be taken into account, thereby proposing a bi-layered network regarding knowledge similarity and trust. First, a trust network is obtained novelly whereby the proposed method of trust link prediction. Accordingly, a directed knowledge flow network is constructed through a proposed knowledge transfer model endowed with trust scores. Second, knowledge combinations in a knowledge flow network are therefore acquired by adopting a community detection method. Third, various probabilities of knowledge combinations based on the maximum network modularity are calculated with respect to the influence of knowledge similarity on cooperation probability. The key contributions of this paper are summarized as an effective approach to identifying knowledge combinations conducted to improve the efficiencies of knowledge management. Related experiments and comparisons are presented to illustrate the practicalities of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105701	10.1016/j.knosys.2020.105701													
J								Graph neural entity disambiguation	KNOWLEDGE-BASED SYSTEMS										Entity disambiguation; Graph neural entity disambiguation; Entity-word graph; Graph convolutional networks		Entity Disambiguation (ED) aims to automatically resolve mentions of entities in a document to corresponding entries in a given knowledge base. State-of-the-art ED methods typically utilize local contextual information for obtaining mention embeddings which will be compared to candidate entity embeddings and then apply Conditional Random Field (CRF) for collective ED, considering global coherence. An inherent drawback of these methods is that, the global semantic relationships among the candidate entities in the same document are not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the global coherence effect. In this paper, to address the issue, we propose a novel end-to-end graph neural entity disambiguation model which fully exploits the global semantic information. In particular, a heterogeneous entity-word graph is first constructed for each document to model the global semantic relationships among candidate entities in a same document. Then graph convolutional network (GCN) is applied on the entity-word graph to generate enhanced entity embeddings encoding global semantics, which are fed to a CRF for collective ED. Extensive experiments have demonstrated the efficiency and effectiveness of our method over a few state-of-the-art ED methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105620	10.1016/j.knosys.2020.105620													
J								Improved Binary Grey Wolf Optimizer and Its application for feature selection	KNOWLEDGE-BASED SYSTEMS										Grey Wolf Optimizer; Discrete; Binary; Transfer function; Feature selection	PARTICLE SWARM OPTIMIZATION; ALGORITHM; CLASSIFICATION; COMBINATION; QUALITY; SYSTEM	Grey Wolf Optimizer (GWO) is a new swarm intelligence algorithm mimicking the behaviours of grey wolves. Its abilities include fast convergence, simplicity and easy realization. It has been proved its superior performance and widely used to optimize the continuous applications, such as, cluster analysis, engineering problem, training neural network and etc. However, there are still some binary problems to optimize in the real world. Since binary can only be taken from values of 0 or 1, the standard GWO is not suitable for the problems of discretization. Binary Grey Wolf Optimizer (BGWO) extends the application of the GWO algorithm and is applied to binary optimization issues. In the position updating equations of BGWO, the a parameter controls the values of A and D, and influences algorithmic exploration and exploitation. This paper analyses the range of values of AD under binary condition and proposes a new updating equation for the a parameter to balance the abilities of global search and local search. Transfer function is an important part of BGWO, which is essential for mapping the continuous value to binary one. This paper includes five transfer functions and focuses on improving their solution quality. Through verifying the benchmark functions, the advanced binary GWO is superior to the original BGWO in the optimality, time consumption and convergence speed. It successfully implements feature selection in the UCI datasets and acquires low classification errors with few features. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105746	10.1016/j.knosys.2020.105746													
J								Feature selection for hierarchical classification via joint semantic and structural information of labels	KNOWLEDGE-BASED SYSTEMS										Feature selection; Hierarchical classification; Label semantic similarity; Label hierarchical structure	ANNOTATION; PREDICTION; RELIEFF; GRAPH	Hierarchical Classification is widely used in many real-world applications, where the label space is exhibited as a tree or a Directed Acyclic Graph (DAG) and each label has rich semantic descriptions. Feature selection, as a type of dimension reduction technique, has proven to be effective in improving the performance of machine learning algorithms. However, many existing feature selection methods cannot be directly applied to hierarchical classification problems since they ignore the hierarchical relations and take no advantage of the semantic information in the label space. In this paper, we propose a novel feature selection framework based on semantic and structural information of labels. First, we transform the label description into a mathematical representation and calculate the similarity score between labels as the semantic regularization. Second, we investigate the hierarchical relations in a tree structure of the label space as the structural regularization. Finally, we impose two regularization terms on a sparse learning based model for feature selection. Additionally, we adapt the proposed model to a DAG case, which makes our method more general and robust in many real-world tasks. Experimental results on real-world datasets demonstrate the effectiveness of the proposed framework for hierarchical classification domains. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105655	10.1016/j.knosys.2020.105655													
J								Community detection in complex networks with an ambiguous structure using central node based link prediction	KNOWLEDGE-BASED SYSTEMS										Complex network; Community detection; Link prediction; Central node	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; LABEL PROPAGATION; MODULARITY; OPTIMIZATION; PROBABILITY; SIMILARITY	Community detection in complex networks has aroused wide attention, since it can find some useful information hidden in the networks. Many different community detection algorithms have been proposed to detect the communities in a variety of networks. However, as the ratio of each node connecting with the nodes in other communities increases, namely, the community structure of networks becomes unclear, the performance of most existing community detection algorithms will considerately deteriorate. As a method of finding missing information, link prediction can predict undiscovered edges in the networks. However, the existing link prediction based community detection algorithms cannot deal with the networks with an ambiguous community structure, namely, the networks having a mixing parameter greater than 0.5. In this paper, we design a new strategy of link prediction and propose a community detection algorithm based on this strategy to detect the communities in complex networks, especially for the networks with an ambiguous community structure. Experimental results on synthetic benchmark networks and real-world networks indicate that the proposed community detection algorithm outperforms five state-of-the-art community detection algorithms, especially for those without a clear community structure. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105626	10.1016/j.knosys.2020.105626													
J								K-Means-based isolation forest	KNOWLEDGE-BASED SYSTEMS										Anomaly detection; Isolation forest; K-Means; Search tree; Spatio-temporal datasets	TIME-SERIES DATA; ANOMALY DETECTION; FEATURE-SELECTION; NEURAL-NETWORKS; ALGORITHM; OUTLIERS; SUPPORT	The task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods. Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data. In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach. This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method. k-Means clustering is used to predict the number of divisions on each decision tree node. As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. Moreover, it returns more intuitively appealing anomaly score values. (C) 2020 The Authors. Published by Elsevier B.V.																	0950-7051	1872-7409				MAY 11	2020	195								105659	10.1016/j.knosys.2020.105659													
J								LDA-based data augmentation algorithm for acoustic scene classification	KNOWLEDGE-BASED SYSTEMS										Acoustic scene classification; Topic model; LDA; Key audio event; Non-key audio event	NEURAL-NETWORKS	Deep neural network needs large amount of data for training, to obtain more data, many simple data augmentation algorithms have been proposed. In this paper, we propose a LDA-based data augmentation algorithm to extend the training set. The proposed LDA-based data augmentation algorithm uses the topic model LDA to detect the key audio words in the recordings, and further to detect the key audio events and non-key audio events for each recording; with the detected keyaudio-event segments, for each acoustic scene class, the probability distribution of key-audio-event's occurrence numbers, the probability distribution of key-audio-event's locations under each occurrence number and the probability distribution of key-audio-event's durations under each occurrence number is counted, and then the new recordings are generated according to these probability distributions. Experiments are done on the public TUT acoustic scenes 2016 dataset, and the experimental results show that compared with the other simple data augmentation algorithms, the proposed LDA-based data augmentation algorithm is more stable and effective, it can get better generalization ability for different kinds of neural network on different datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105600	10.1016/j.knosys.2020.105600													
J								Boosted K-nearest neighbor classifiers based on fuzzy granules	KNOWLEDGE-BASED SYSTEMS										Granule computing; Fuzzy sets; Machine learning	INFORMATION GRANULATION; ADABOOST; KNN; CLASSIFICATION; RECOGNITION; SYSTEMS; SVM; PSO	K-nearest neighbor (KNN) is a classic classifier, which is simple and effective. Adaboost is a combination of several weak classifiers as a strong classifier to improve the classification effect. These two classifiers have been widely used in the field of machine learning. In this paper, based on information fuzzy granulation, KNN and Adaboost, we propose two algorithms, a fuzzy granule K-nearest neighbor (FGKNN) and a boosted fuzzy granule K-nearest neighbor (BFGKNN), for classification. By introducing granular computing, we normalize the process of solving problem as a structured and hierarchical process. Structured information processing is focused, so the performance including accuracy and robust can be enhanced to data classification. First, a fuzzy set is introduced, and an atom attribute fuzzy granulation is performed on samples in the classified system to form fuzzy granules. Then, a fuzzy granule vector is created by multiple attribute fuzzy granules. We design the operators and define the measure of fuzzy granule vectors in the fuzzy granule space. And we also prove the monotonic principle of the distance of fuzzy granule vectors. Furthermore, we also give the definition of the concept of K-nearest neighbor fuzzy granule vector and present FGKNN algorithm and BFGKNN algorithm. Finally, we compare the performance among KNN, Back Propagation Neural Network (BPNN), Support Vector Machine (SVM), Logistic Regression (LR), FGKNN and BFGKNN on UCI data sets. Theoretical analysis and experimental results show that FGKNN and BFGKNN have better performance than that of the methods mentioned above if the appropriate parameters are given. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105606	10.1016/j.knosys.2020.105606													
J								Learning-based elephant herding optimization algorithm for solving numerical optimization problems	KNOWLEDGE-BASED SYSTEMS										Elephant herding optimization; Swarm intelligence; Velocity strategy; Learning strategy; Separation strategy; Elitism strategy; Benchmark function	ARTIFICIAL BEE COLONY; PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; KRILL HERD; FIREFLY ALGORITHM; SEARCH ALGORITHM; CRYPTANALYSIS; SELECTION; MEMORY	The elephant herding optimization (EHO) is a recent swarm intelligence algorithm. This algorithm simulates the clan updating and separation behavior of elephants. The EHO method has been successfully deployed in various fields. However, a more reliable implementation of the standard EHO algorithm still requires improving the control and selection of the parameters, convergence speed, and efficiency of the optimal solutions. To cope with these issues, this study presents an improved EHO algorithm terms as IMEHO. The proposed IMEHO method uses a global velocity strategy and a novel learning strategy to update the velocity and position of the individuals. Furthermore, a new separation method is presented to keep the diversity of the population. An elitism strategy is also adopted to ensure that the fittest individuals are retained at the next generation. The influence of the parameters and strategies on the IMEHO algorithm is fully studied. The proposed method is tested on 30 benchmark functions from IEEE CEC 2014. The obtained results are compared with other eight metaheuristic algorithms and evaluated according to Friedman rank test. The results imply the superiority of the IMEHO algorithm to the standard EHO and other existing metaheuristic algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105675	10.1016/j.knosys.2020.105675													
J								A novel random forest approach for imbalance problem in crime linkage	KNOWLEDGE-BASED SYSTEMS										Crime linkage; Classification; Class imbalance; Information granule; Random forest	MODUS-OPERANDI; SCENE BEHAVIOR; LINKING; DECISION; BURGLARIES; REGRESSION; SMOTE	Crime linkage is a challenging task in crime analysis, which is to find serial crimes committed by the same offenders. It can be regarded as a binary classification task detecting serial case pairs. However, most case pairs in the real world are nonserial, so there is a serious class imbalance in the crime linkage. In this paper, we propose a novel random forest based on the information granule. The approach does not resample the minority class or the majority class but concentrates on indistinguishable case pairs at the classification boundary. The information granule is used to identify case pairs that are difficult to distinguish in the dataset and constructs a nearly balanced dataset in the uncertainty region to deal with the imbalanced problem. In the proposed approach, random trees come from the original dataset and the above mentioned nearly balanced dataset. A real-world robbery dataset and some public imbalanced datasets are employed to measure the performance of the approach. The results show that the proposed approach is effective in dealing with class imbalances, and it can be extended to combine with other methods solving class imbalances. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105738	10.1016/j.knosys.2020.105738													
J								Incremental learning imbalanced data streams with concept drift: The dynamic updated ensemble algorithm	KNOWLEDGE-BASED SYSTEMS										Data stream classification; Concept drift; Class imbalance; Ensemble	RESAMPLING ENSEMBLE; SMOTE	Learning nonstationary data streams has been well studied in recent years. However, most of the researches assume that the class imbalance of data streams is relatively balanced. Only a few approaches tackle the joint issue of concept drift and class imbalance due to its complexity. Meanwhile, the existing chunk ensembles for classifying imbalanced nonstationary data streams always need to store previous data, which consumes plenty of memory usage. To overcome these issues, we propose a chunk-based incremental ensemble algorithm called Dynamic Updated Ensemble (DUE) for learning imbalanced data streams with concept drift. Compared to the existing techniques, its merits are fivefold: (1) it learns one chunk at a time without requiring access to previous data; (2) it emphasizes misclassified examples in the model update procedure; (3) it can timely react to multiple kinds of concept drifts; (4) it can adapt to the new condition when switching majority class to minority class; (5) it keeps a limited number of classifiers to ensure high efficiency. Experiments on synthetic and real datasets demonstrate the effectiveness of DUE in learning nonstationary imbalanced data streams. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105694	10.1016/j.knosys.2020.105694													
J								Multiple attribute group decision making based on nucleolus weight and continuous optimal distance measure	KNOWLEDGE-BASED SYSTEMS										Distance measure; Cooperative game; Continuous operator; Group decision making	AVERAGING DISTANCE; OPERATORS; AGGREGATION	Continuous distance measure is an effective tool to describe the relation of alternatives in group decision making under uncertain environments. Firstly, a novel aggregation operator, i.e., continuous optimal ordered weighted quasi-averaging (COOWQA) operator, is proposed. Some special cases and desirable properties of the COOWQA operator are analyzed. Then, the COOWQD measure is developed based on the COOWQA operator. To consider the interaction between decision makers, the nucleolus weight is presented to integrate the positive and negative ideal COOWQD measure. lambda-fuzzy measure is adopted to obtain the measure values on decision maker sets. Finally, the GDM-CN algorithm is developed to group decision making with interval decision information. The application in selecting knowledge management systems is provided to illustrate the rationality and effectiveness of GDM-CN algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105719	10.1016/j.knosys.2020.105719													
J								Decision making with a sequential modeling of pairwise comparison process	KNOWLEDGE-BASED SYSTEMS										Decision analysis; Pairwise comparisons; Sequential modeling; Analytic hierarchy process (AHP); Acceptable consistency	ANALYTIC HIERARCHY PROCESS; FUZZY COGNITIVE MAPS; PRIORITY VECTORS; CONSISTENCY; REVERSAL; AHP; INCONSISTENCY; PRESERVATION; MATRICES	The Analytic Hierarchy Process (AHP) is considered to be the natural way to realize decision making with pairwise comparisons of alternatives. Within the framework of the AHP model, the full process of forming comparison matrices is considered as an indivisible whole; and the irrational and uncertain behavior of decision makers has been identified and highlighted. In practice, the process of comparing alternatives is complex, especially when the number of alternatives is large. In this study, we propose a sequential modeling of wise comparison process for alternatives, which subsumes the typical AHP model as a special case. A new mathematical framework based on the leading principal submatrices is formed. The process of quantifying priorities of alternatives and the inconsistency-degree process of judgements are proposed to capture the irrationality and uncertainty experienced by decision makers in pairwise comparisons. It is found that the rank preservation is natural according to the comparison process. The acceptable consistency of multiplicative reciprocal matrices is redefined under the principle of weak rationality. A feedback mechanism is proposed to help decision makers avoid irrational behavior and to produce a better outcome of the decision procedure. Some comparisons with the AHP model demonstrate that the proposed model offers a clear way to capture the irrational and uncertain behavior of decision makers when evaluating the importance of alternatives. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105642	10.1016/j.knosys.2020.105642													
J								A personalized diagnosis method to detect faults in gears using numerical simulation and extreme learning machine	KNOWLEDGE-BASED SYSTEMS										Personalized fault diagnosis; Gears; Finite element method; Numerical simulation; Extreme learning machine	DYNAMIC-ANALYSIS; ELEMENT; DAMAGE; WAVELETS; ENTROPY	Fault classification methods are a long-term research focus in both science society and engineering application. Generally, every real-world running mechanical system is exhibit personalized vibration behaviors and the corresponding fault samples of such systems are difficult to be obtained. Therefore, extreme learning machine (ELM), a typical fault classification method is failed to attain agreeable fault detection results. In this paper, a personalized fault diagnosis method using finite element method (FEM) simulation and ELM is proposed to detect faults in gears. The method includes three steps. Firstly, The FEM model of gears with faults is constructed to obtain fault samples (simulation signals). Secondly, to achieve ELM training process, the meshing frequency components of each simulation signal is separated into sub-signals and the corresponding time and time-frequency domains indicators are served as training samples. Finally, the measured vibration signals of gear transmission systems are employed as testing samples of trained ELM to recognize its fault types. The classification accuracy ratios of gear states in a cracked teeth of driving gear, a peeled teeth of driving gear, a broken teeth of driving gear, a peeled teeth of driving gear and a broken teeth of driven gear, a broken teeth of driving gear and a broken teeth of driven gear are 85%, 90%, 92.5%, 90% and 85% respectively. It is expect that the proposed personalized fault diagnosis method can set up a bridge between fault classification methods and real-world applications. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105653	10.1016/j.knosys.2020.105653													
J								Semi-supervised stochastic blockmodel for structure analysis of signed networks	KNOWLEDGE-BASED SYSTEMS										Network data mining; Signed networks; Semi-supervised learning; Stochastic blockmodel	COMMUNITY DETECTION	Finding hidden structural patterns is a critical problem for all types of networks, including signed networks. Among all of the methods for structural analysis of complex network, stochastic blockmodel (SBM) is an important research tool because it is flexible and can generate networks with many different types of structures. However, most existing SBM learning methods for signed networks are unsupervised, leading to poor performance in terms of finding hidden structural patterns, especially when handling noisy and sparse networks. Learning SBM in a semi-supervised way is a promising avenue for overcoming the above difficulty. In this type of model, a small number of labelled nodes and a large number of unlabelled nodes, coupled with their network structures, are simultaneously used to train SBM. We propose a novel semi-supervised signed stochastic blockmodel and its learning algorithm based on variational Bayesian inference, with the goal of discovering both assortative (the nodes connect more densely in same clusters than that in different clusters) and disassortative (the nodes link more sparsely in same clusters than that in different clusters) structures from signed networks. The proposed model is validated through a number of experiments wherein it compared with the state-of-the-art methods using both synthetic and real-world data. The carefully designed tests, allowing to account for different scenarios, show our method outperforms other approaches existing in this space. It is especially relevant in the case of noisy and sparse networks as they constitute the majority of the real-world networks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105714	10.1016/j.knosys.2020.105714													
J								An affinity propagation clustering based particle swarm optimizer for dynamic optimization	KNOWLEDGE-BASED SYSTEMS										Affinity propagation clustering; Optimal particles relocation; Dynamic optimization problems; Particle swarm optimizer	DIFFERENTIAL EVOLUTION; GENETIC ALGORITHMS; MEMORY; ENVIRONMENTS; SCHEME; MODEL	Multipopulation methods, which can enhance the population diversity, are well suited for dynamic optimization. However, there are still some challenges need to be tackled when multipopulation methods are employed, namely, how to avoid sensitive parameters when creating sub-populations, and how to effectively adapt to the changing optima continuously during the search process. Therefore, a novel multipopulation algorithm based on the affinity propagation clustering is proposed to address the above challenges. In the proposed method, affinity propagation clustering is applied for automatically creating sub-populations by message-passing process, which can avoid some extra parameters. Moreover, a simple but effective strategy, denoted as optimal particles relocation, is proposed for responding to environmental changes. In this strategy, the best particles in each sub-population are first stored in a memory. Then, local search is applied for helping the memory to quickly locate new peaks, if the environmental change has occurred. To validate the performance of the proposed algorithm, a variety of experiments have been conducted. The experimental results have demonstrated that the proposed algorithm performs robustly and competitively under different environments. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105711	10.1016/j.knosys.2020.105711													
J								Compact geometric representation of qualitative directional knowledge	KNOWLEDGE-BASED SYSTEMS										Qualitative spatial representation; Cardinal Direction Calculus; Compact representation	POLYGONS	To effectively and efficiently deal with large-scale spatial data is critical for applications in the age of information technology. Compact representation of spatial knowledge is one of the emerging research techniques that contribute to this capability. In this article, we consider the problem of compactly representing qualitative directional relations between extended objects, modelled in the Cardinal Direction Calculus (CDC) of Goyal and Egenhofer. For a large dataset of regions, this approach first constructs a simplified geometry for each region, which preserves CDC relations between regions, and then represents each simplified geometry compactly, so that the storage size is small while retrieving CDC relations from the representation is still reasonably fast. More specifically, the method called necessary cut is used to construct simple geometries, and the two methods, viz. the polygon representation and the rectangle representation, are devised to compactly represent the constructed geometries in cubic time w.r.t. the size of the corresponding simple geometry. Theoretical analyses demonstrate that the two representations, especially the rectangle representation, are promising to have small storage size. Moreover, our empirical evaluations on real-world datasets show that, for each dataset the new approach can produce a rectangle representation that has dominant performance against the state of the art techniques in reducing the storage size of the relations, while the average efficiency of retrieving CDC relations based on the rectangle representation is about the same as the fastest method in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105616	10.1016/j.knosys.2020.105616													
J								A concise peephole model based transfer learning method for small sample temporal feature-based data-driven quality analysis	KNOWLEDGE-BASED SYSTEMS										Concise peephole model; Transfer learning; Small sample learning; Quality analysis	FAULT-DIAGNOSIS	Insufficient samples and low analysis efficiency are two main problems for data-driven quality analysis. To avoid negative transfer from source data to target data and improve transfer efficiency for temporal feature extraction, in this paper, a novel transfer learning model and algorithm with feature mapping, feature learning and domain adaptation was proposed based on concise peephole model (TLCPM). A feature mapping model based on deep convolution network was firstly presented by establishing deep convolution network to automatically learn and minimize the feature distance of source data and target data. The feature learning method was then constructed by concise peephole model to extract feature of temporal sample, which has less training parameters and more concise network structure than traditional peephole model. And domain adaptation combining one-layer fully connected network with TLCPM is used to realize the transfer learning ability of CPM model and minimize the probability output distribution distance. Finally, a bolt tightening test bench was set up and a tiny bolt-tightening data set was acquired to validate the proposed method. The results showed the TLCPM can be properly applied to analyze small sample temporal feature-based data and achieved good comprehensive performance. It took 0.5 min to obtain high test accuracy (95.14%), which was better than the results of other state of the art algorithms. Moreover, additional validation experiments were carried on. The results revealed that TLCPM also had better prediction accuracy on learning different categories of small samples. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105665	10.1016/j.knosys.2020.105665													
J								A novel intrusion detection system based on an optimal hybrid kernel extreme learning machine	KNOWLEDGE-BASED SYSTEMS										Intrusion detection system; Extreme learning machine; Gravitational search algorithm; Differential evolution; Kernel principal component analysis	DIFFERENTIAL EVOLUTION; FEATURE-SELECTION; COMPONENT ANALYSIS; NEAREST NEIGHBORS; ALGORITHM; OPTIMIZATION; CLASSIFIER; ENSEMBLE; REGRESSION; DEPSO	Intrusion detection is a challenging technology in the area of cyberspace security for protecting a system from malicious attacks. A novel accurate and effective misuse intrusion detection system that relies on specific attack signatures to distinguish between normal and malicious activities is therefore presented to detect various attacks based on an extreme learning machine with a hybrid kernel function (HKELM). First, the derivation and proof of the proposed hybrid kernel are given. A combination of the gravitational search algorithm (GSA) and differential evolution (DE) algorithm is employed to optimize the parameters of HKELM, which improves its global and local optimization abilities during prediction attacks. In addition, the kernel principal component analysis (KPCA) algorithm is introduced for dimensionality reduction and feature extraction of the intrusion detection data. Then, a novel intrusion detection approach, KPCA-DEGSA-HKELM, is obtained. The proposed approach is eventually applied to the classic benchmark KDD99 dataset, the real modern UNSW-NB15 dataset and the industrial intrusion detection dataset from the Tennessee Eastman process. The numerical results validate both the high accuracy and the time-saving benefit of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105648	10.1016/j.knosys.2020.105648													
J								An integrated parallel big data decision support tool using the W-CLUS-MCDA: A multi-scenario personnel assessment	KNOWLEDGE-BASED SYSTEMS										Data-Driven Decision-Making (DDDM); CLUS-MCDA; Best-Worst Method (BWM); Big data; Parallel Decision-Making (PDM); Multi-scenario decision making; Personnel selection problem	SELECTION; MODEL; EXTENSION; INDUSTRY; SYSTEM; NUMBER; RISK; ENVIRONMENTS; TOPSIS; SWARA	One of the most primary issues that organizations have to deal with is incorporating massive structured data problems, simultaneously. Additionally, a vital division in any organization is the department of human resources (HR), which is in charge of the recruitment and personnel selection procedures. Due to the nature of the personnel assessment problems, which include multiple candidates as alternatives along with various complex evaluating criteria, these types of problems can be tackled by the aid of multi-attribute decision making (MADM) techniques. Moreover, in mega-structured organizations, the procedure of personnel selection contains massive structures of data due to the number of potential candidates for job positions in various sub-divisions and departments. Therefore, the personnel selection problem in such environments can be subjected as a big data problem which should be handled prudently to save time and cost. The main objective of the current study is to extend the CLUS-MCDA approach (CLUSter analysis for improving Multiple Criteria Decision Analysis) and integrate it with the Best-Worst Method (BWM) and a specific structure to solve multi-scenario big data decision-making problems. In this study, to validate the practicality and reliability of the W-CLUSMCDA approach, multiple personnel selection and risk assessment problems have been investigated with various scenarios within several departments, simultaneously. This study has also introduced the concept of multi-scenario parallel decision making (PDM) within the context of MADM methodology using a data-driven decision-making approach solving various big data problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105749	10.1016/j.knosys.2020.105749													
J								A dynamic group decision making process for high number of alternatives using hesitant Fuzzy Ontologies and sentiment analysis	KNOWLEDGE-BASED SYSTEMS										Group decision making; Fuzzy ontologies; Sentiment analysis; Computing with words; Type-2 hesitant fuzzy sets	SUPPORT-SYSTEM; CONSENSUS MODEL; ENVIRONMENTS; INFORMATION; WORDS	The high spread of Internet and social networks have completely changed the way that Group Decision Making methods are designed, developed and implemented. Experts now operate in environments where a large amount of information is available and new ideas and participants can appear at any time; this results in a dynamically changing decision environment. In this paper, a novel group decision making method for dynamic contexts with a high number of decision alternatives is presented. As the main component of the proposal, a perceptual computing scheme is used in order to extract information from the experts. In the process, sentiment analysis is used when analyzing the debate texts in order to obtain information for selecting the best alternatives on each round. Moreover, interval type-2 hesitant Fuzzy Ontologies are used in order to store the information related to alternatives. By combining interval type-2 and hesitant fuzzy sets, imprecise information can be represented in a comfortable and intuitive way within the ontology. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105657	10.1016/j.knosys.2020.105657													
J								TDP: Two-dimensional perceptron for image recognition	KNOWLEDGE-BASED SYSTEMS										Convolutional neural network; Multilayer perceptron; Two-dimensional perceptron; Recognition performance; F1 score	CLASSIFICATION	Convolutional neural network (CNN) is widely applied to different areas due to good recognition performance. However, convolution operation is a complex computation and consumes the bulk of processing time for CNN. It is still a hot problem how to develop a novel model with good recognition performance for deep learning. Here, we propose a novel model, namely, two-dimensional perceptron (TDP), to get direct input of two-dimensional data for further processing. A TDP has a new network architecture and an innovative computation process of hidden neurons. In cases with the same number of hidden neurons, compared with multilayer perceptron (MLP), TDP achieves good recognition performance with 1 x -36 x speedup and a decrease of parameters by exceeding 97% on MNIST and COIL-20 datasets. Meanwhile, TDP obtains 1%-32% improvement of recognition accuracy in comparison to CNN on CIFAR-10 and SVHN datasets. Furthermore, on INFUSE dataset, TDP has an increase of F1 score by up to almost 11% in comparison with MLP and CNN. The results indicate that TDP is a promising and novel model with excellent recognition performance. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105615	10.1016/j.knosys.2020.105615													
J								Joint low-rank representation and spectral regression for robust subspace learning	KNOWLEDGE-BASED SYSTEMS										Low-rank representation; Subspace learning; Spectral regression; Joint learning; Robustness	NONLINEAR DIMENSIONALITY REDUCTION; PRESERVING PROJECTIONS; SPARSE; GRAPH; ALGORITHM	Subspace learning represents a group of algorithms to project high dimensional data onto a low dimensional subspace in order to retain the desirable properties and simultaneously reduce the dimensionality. In graph-based subspace learning algorithms, the quality of graph affects the performance of projection matrix learning a lot. Especially when data is noisy or even grossly corrupted, we cannot guarantee that the constructed graph can accurately depict the inner structure of data. Additionally, the widely used two-stage paradigm of learning the projection matrix on a given graph isolates the connection between these two stages. In this paper, we propose a general framework to get rid of these disadvantages by the inspiration of low-rank matrix recovery and spectral regression-based subspace learning. Concretely, by jointly optimizing the objectives of low-rank representation and spectral regression, on one hand we can automatically get the recovered data from noise and the graph affinity matrix and on the other hand we can perform the projection matrix learning. The formulated joint low-rank and subspace learning (JLRSL) framework can be efficiently optimized by the augmented Lagrange multiplier method. We evaluate the performance of JLRSL by conducting extensive experiments on representative benchmark data sets and the results show that the low-rank learning can greatly facilitate the process of subspace learning, leading to robust feature extraction. Moreover, comparison with the state-of-the-arts is performed. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105723	10.1016/j.knosys.2020.105723													
J								A Meta-learning approach for recommending the number of clusters for clustering algorithms	KNOWLEDGE-BASED SYSTEMS										Meta-learning; Recommendation; Number of clusters; Clustering	INSTANCE SELECTION; DATA SET; RANKING; SYSTEMS; VALIDATION	One of the main challenges in Clustering Analysis is choosing the optimal number of clusters. A typical methodology is to evaluate a validity index over the data and to optimize it as a function of the number of clusters. However, this process can have a high computational cost. In this work, we introduce a new approach for recommending the number of clusters for a particular dataset by using Meta-learning. As the predictive performance of the meta-models induced by Meta-learning is affected by how datasets are described by meta-features, we propose a new set of meta-features able to improve the predictive performance of meta-models used for recommending the number of clusters. Experimental results show that the proposed approach provides a good recommendation of the number of clusters. Additionally, the proposed meta-feature obtains better results than meta-features for clustering tasks found in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105682	10.1016/j.knosys.2020.105682													
J								Mutual information-based label distribution feature selection for multi-label learning	KNOWLEDGE-BASED SYSTEMS										Feature selection; Multi-label data; Granular computing; Label enhancement; Mutual information	STREAMING FEATURE-SELECTION; ATTRIBUTE REDUCTION; MISSING LABELS; GRAPH; CLASSIFICATION; ACCELERATOR; ALGORITHM; DECISION; SPARSE	Feature selection used for dimensionality reduction of the feature space plays an important role in multi-label learning where high-dimensional data are involved. Although most existing multi-label feature selection approaches can deal with the problem of label ambiguity which mainly focuses on the assumption of uniform distribution with logical labels, it cannot be applied to many practical applications where the significance of related label for every instance tends to be different. To deal with this issue, in this study, label distribution learning covered with a certain real number of labels is introduced to design a model for the labeling-significance. Nevertheless, multi-label feature selection is limited to handling only labels consisting of logical relations. In order to solve this problem, combining the random variable distribution with granular computing, we first propose a label enhancement algorithm to transform logical labels in multi-label data into label distribution with more supervised information, which can mine the hidden label significance from every instance. On this basis, to remove some redundant or irrelevant features in multi-label data, a label distribution feature selection algorithm using mutual information and label enhancement is developed. Finally, the experimental results show that the performance of the proposed method is superior to the other state-of-the-art approaches when dealing with multi-label data. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105684	10.1016/j.knosys.2020.105684													
J								Incorporating expert prior in Bayesian optimisation via space warping	KNOWLEDGE-BASED SYSTEMS										Bayesian optimisation; Gaussian process; Black-box function; Probability integrity transform; Space warping		Bayesian optimisation is a well-known sample-efficient method for the optimisation of expensive black-box functions. However when dealing with big search spaces the algorithm goes through several low function value regions before reaching the optimum of the function. Since the function evaluations are expensive in terms of both money and time, it may be desirable to alleviate this problem. One approach to subside this cold start phase is to use prior knowledge that can accelerate the optimisation. In its standard form, Bayesian optimisation assumes the likelihood of any point in the search space being the optimum is equal. Therefore any prior knowledge that can provide information about the optimum of the function would elevate the optimisation performance. In this paper, we represent the prior knowledge about the function optimum through a prior distribution. The prior distribution is then used to warp the search space in such a way that space gets expanded around the high probability region of function optimum and shrinks around low probability region of optimum. We incorporate this prior directly in function model (Gaussian process), by redefining the kernel matrix, which allows this method to work with any acquisition function, i.e. acquisition agnostic approach. We show the superiority of our method over standard Bayesian optimisation method through optimisation of several benchmark functions and hyperparameter tuning of two algorithms: Support Vector Machine (SVM) and Random forest. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105663	10.1016/j.knosys.2020.105663													
J								A constrained optimization algorithm for learning GloVe embeddings with semantic lexicons	KNOWLEDGE-BASED SYSTEMS										GloVe; Word embeddings; Semantic lexicons; Constrained optimization	WORD EMBEDDINGS; SIMILARITY; FRAMEWORK	GloVe representations of words as vector embeddings in continuous spaces are learned from matrix factorization of the words' co-occurrences matrix constructed from large corpora. Due to their high quality as textual features, GloVe embeddings have been extensively utilized for many text mining and natural language processing tasks with considerable success. Further improvements of these word representations can be obtained by also taking into account the valuable information of the semantic properties of the words and the complex relationships between them as provided by semantic lexicons. In this paper we adopt optimization techniques from the domain of machine learning with constrained optimization in order to leverage the relational knowledge between words, and we propose an efficient algorithm that produces word embeddings enhanced by the semantic information. The proposed algorithm outperforms other related approaches that utilize semantic information either during training or as a post-processing step. Our claims are validated by experiments on popular text mining and natural language processing tasks, including word similarities, word analogies, and sentiment analysis, which demonstrate that our proposed model can significantly improve the quality of word vector representations. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105628	10.1016/j.knosys.2020.105628													
J								A learning path recommendation model based on a multidimensional knowledge graph framework for e-learning	KNOWLEDGE-BASED SYSTEMS										Learning path recommendation; Knowledge graph; e-learning; Learning needs	SYSTEM; CREATIVITY	E-learners face a large amount of fragmented learning content during e-learning. How to extract and organize this learning content is the key to achieving the established learning target, especially for non-experts. Reasonably arranging the order of the learning objects to generate a well-defined learning path can help the e-learner complete the learning target efficiently and systematically. Currently, knowledge-graph-based learning path recommendation algorithms are attracting the attention of researchers in this field. However, these methods only connect learning objects using single relationships, which cannot generate diverse learning paths to satisfy different learning needs in practice. To overcome this challenge, this paper proposes a learning path recommendation model based on a multidimensional knowledge graph framework. The main contributions of this paper are as follows. Firstly, we have designed a multidimensional knowledge graph framework that separately stores learning objects organized in several classes. Then, we have proposed six main semantic relationships between learning objects in the knowledge graph. Secondly, a learning path recommendation model is designed for satisfying different learning needs based on the multidimensional knowledge graph framework, which can generate and recommend customized learning paths according to the e-learner's target learning object. The experiment results indicate that the proposed model can generate and recommend qualified personalized learning paths to improve the learning experiences of e-learners. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105618	10.1016/j.knosys.2020.105618													
J								Words are important: A textual content based identity resolution scheme across multiple online social networks	KNOWLEDGE-BASED SYSTEMS										Content information; Identity matching; Identity resolution; Natural language processing; Online social networks; Text mining	SITES; FRAMEWORK; FACEBOOK; TWITTER; USERS	Identity resolution of a person using various online social networks can enable an interested party to have a better and holistic understanding of former's behavior and personality. Major challenges in developing a reliable and scalable matching scheme for online identities include non-availability of required information or having contradictory information for the same user across these networks. In this study, we present a scheme for identity matching which utilizes important features extracted from contents generated by or shared with users across one's online social networks. With the help of natural language processing and text mining techniques, we extract and process parts-of-speech, symbols, emoticons, numbers, and high frequency words in user's posts, tweets, retweets, and URLs. On the basis of experiments with ground truth Twitter-Facebook real datasets, this method achieved 91.2 percent accuracy in matching user's identity across the user's profiles. The main contribution of this paper is that this proposes a novel method for identity matching, which utilizes only the publicly available content information of online social network users. This method can be used alone for identity matching, or can be used along with other identity resolution frameworks to enhance their accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105624	10.1016/j.knosys.2020.105624													
J								BECKEY: Understanding, comparing and discovering keys of different semantics in knowledge bases	KNOWLEDGE-BASED SYSTEMS										Semantic Web; Key discovery; Data linking; Key semantics; RDF; Semantic agnostic approach	INTERLINKING	Integrating data coming from different knowledge bases has been one of the most important tasks in the Semantic Web the last years. Keys have been considered to be very useful in the data linking task. A set of properties is considered a key if it uniquely identifies every resource in the data. To cope with the incompleteness of the data, three different key semantics have been proposed so far. We propose BECKEY, a semantic agnostic approach that discovers keys for all three semantics, succeeding to scale on large datasets. Our approach is able to discover keys under the presence of erroneous data or duplicates (i.e., almost keys). A formalisation of the three semantics along with the relations among them is provided. An extended experimental comparison of the three key semantics has taken place. The results allow a better understanding of the three semantics, providing insights on when each semantic is more appropriate for the task of data linking. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105708	10.1016/j.knosys.2020.105708													
J								A coarsening method for bipartite networks via weight-constrained label propagation	KNOWLEDGE-BASED SYSTEMS										Complex networks; Multilevel method; Large-scale networks; Bipartite networks; Community detection; Network coarsening; Network visualization	CLASSIFICATION	A multilevel method is a scalable strategy to solve optimization problems in large bipartite networks, which operates in three stages. Initially the input network is iteratively coarsened into a hierarchy of gradually smaller networks. Coarsening implies in collapsing vertices into so-called super-vertices which inherit properties of their originating vertices. An initial solution is obtained executing the target algorithm in the coarsest network. Finally, this solution is successively projected back over the inverse sequence of coarsened networks, up to the initial one, yielding an approximate final solution. Despite its potential applicability, the strategy faces several theoretical and practical limitations. Coarsening is usually attained following a user-defined policy to match vertices pairwise. However, the network reduction process is extremely slow and may yield degraded solutions due to propagation of poor matches. Additionally, proper parameterization of coarsening algorithms is difficult, as well as ensuring the super-vertices preserve the relevant properties. We address these issues with a near-linear complexity coarsening strategy based on weight-constrained label propagation. Our strategy collapses groups of vertices, rather than pairs, yielding faster and more extensive network reduction. Moreover, users may specify the desired size of the coarsest network and control super-vertex weights. The applicability of our solution is illustrated in multiple scenarios, namely: multilevel implementation of an existing high-cost community detection algorithm; as a direct community detection algorithm; finally, network visualization, in connection with force-directed graph drawing algorithms. Results provide empirical evidence on the potential of our proposal to foster novel applications of the multilevel method in bipartite networks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105678	10.1016/j.knosys.2020.105678													
J								MCFS: Min-cut-based feature-selection	KNOWLEDGE-BASED SYSTEMS										Machine-learning; Feature-selection; Nearest-neighbour; Correlations; Max-flow min-cut; Classification	STATISTICAL COMPARISONS; CLASSIFICATION; CLASSIFIERS; ALGORITHMS; REGRESSION	In this paper, MCFS (Min-Cut-based feature-selection) is presented, which is a feature-selection algorithm based on the representation of the features in a dataset by means of a directed graph. The main contribution of our work is to show the usefulness of a general graph-processing technique in the feature-selection problem for classification datasets. The vertices of the graphs used herein are the features together with two special-purpose vertices (one of which denotes high correlation to the feature class of the dataset, and the other denotes a low correlation to the feature class). The edges are functions of the correlations among the features and also between the features and the classes. A classic max-flow min-cut algorithm is applied to this graph. The cut returned by this algorithm provides the selected features. We have compared the results of our proposal with well-known feature-selection techniques. Our algorithm obtains results statistically similar to those achieved by the other techniques in terms of number of features selected, while additionally significantly improving the accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105604	10.1016/j.knosys.2020.105604													
J								Fault tolerating multi-tenant service-based systems with dynamic quality	KNOWLEDGE-BASED SYSTEMS										Cloud computing; Fault tolerance; Multi-tenancy; Redundancy; Cloud model	QOS; FRAMEWORK; SELECTION; ALGORITHM	The rapid development of multi-tenant service-based systems (SBSs) is mainly due to the popularity of cloud computing. SBS combines the existing services in the form of business processes to meet the needs of multiple tenants. There are two kinds of services: functional and non-functional. Multiple services can have the same functionality while changing their non-functional properties, or QoS (Quality of Service). QoS value is an important criterion for service selection or recommendation. In the existing work, QoS value in service selection or recommendation is regarded as a constant. However, in the dynamic and volatile cloud environment, the performance of services may fluctuate due to various factors (e.g. techniques of service studied, network conditions, etc.). A constant value does not reflect the consistency of the service. This paper proposes FT4MTS-D (fault-tolerant strategies for multi-tenant service-based systems with dynamic quality), a novel method that evaluates the criticality of dynamic cost-effective fault tolerance strategies for multi-tenant service-based systems. First, the consistency of each given service is evaluated based on the feature vector of the cloud model. Second, this paper measures the consistency of given service by the sensitivity of probability degradation of an SBS in comparison to the probability degradation of that service. The goal is to find the critical services in the SBS and then determine the optimal fault-tolerant strategy for the given service. The experimental results show that FT4MTS-D has obvious advantages compared with several existing representative methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105715	10.1016/j.knosys.2020.105715													
J								FMDBN: A first-order Markov dynamic Bayesian network classifier with continuous attributes	KNOWLEDGE-BASED SYSTEMS										Time-series data; Dynamic Bayesian network classifier; Time-delayed transformation; Dislocated transformation; Evolutionary learning	PROBABILITY DENSITY-ESTIMATION; FAULT-DETECTION	With the development of data driven decision making and prediction, time-series data are ubiquitous and the demand for its classification is vast. Although a large body of research has been reported in the literature, it is mainly oriented to situations in which class and attributes are changing simultaneously. In practice however, those class and attributes changes are not always synchronous. This means that further studies for asynchronous classifier problems are necessary. In this paper, a first-order Markov dynamic Bayesian network classifier is proposed to address the asynchronous issue, by combing time-series data preprocessing, time-delayed and dislocated transformation of variables, initial and evolutionary learning. The attribute density in this classifier is estimated based on Gaussian function, and the classification accuracy criterion for time-series progressiveness is also considered. This classifier has a relatively simple structure, which can avoid the problem of overfitting. In addition, data can effectively be classified by utilizing three kinds of classification information, namely time-delayed, non-time-delayed and mixed information in multivariate time-series datasets. The proposed method is also able to accumulate classification information via iterative evolution and thus improve the generalization of classifiers. Experiments were carried out by using standard time-series datasets from UCI, financial and macroeconomic domains. The experimental results show that the proposed first-order Markov dynamic Bayesian network classifier is more accurate in dealing with these dynamic classification problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105638	10.1016/j.knosys.2020.105638													
J								Monitoring online reviews for reputation fraud campaigns	KNOWLEDGE-BASED SYSTEMS										Fake review detection; Reputation fraud campaign; Online detection; Conditional random field	FAKE	Online reviews are critical for both purchasers and sellers in the era of E-commerce. Praiseful reviews and/or 5-star ratings can yield remarkable profit gains, on the other hand, a bad-mouth review or a low rating score often incurs sales decrease. Therefore, fake review detection has attracted lots of research interests in recent years. While most existing approaches detect fake reviews in an offline fashion, i.e., finding suspicious reviews from a large volume of historical data, few efforts have been made to detect fake reviews in an online fashion, i.e., detecting suspicious reviews in review data streams. Online detecting fake reviews has many more benefits than offline detection in that the damages of fake reviews can be significantly reduced by removing them as early as possible. In this paper, we propose a novel online monitoring technique for detecting reputation fraud campaigns in product reviews. The technique includes two phases. First, it monitors online reviews to generate the most abnormal review subsequences (MARSs), which can be considered as candidate reputation fraud campaigns. Second, conditional random fields are exploited to label each review in a MARS as fake or genuine. Experiments show that our proposed methods are highly effective and efficient, with many advantages compared with existing online detection approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105685	10.1016/j.knosys.2020.105685													
J								Vital spreaders identification in complex networks with multi-local dimension	KNOWLEDGE-BASED SYSTEMS										Complex network; Vital spreaders identification; Multi-local dimension	IDENTIFYING INFLUENTIAL NODES; COMMON-CAUSE FAILURE; SOCIAL NETWORKS; INFLUENCE MAXIMIZATION; GENERALIZED DIMENSIONS; RELIABILITY ASSESSMENT; NEURAL-NETWORK; SYSTEMS; PREDICTION; CENTRALITY	The important nodes identification has been an interesting problem in this issue. Several centrality methods have been proposed to solve this problem, but most previous methods have their own limitations. To address this problem more effectively, multi-local dimension (MLD) which is based on the fractal property is proposed to identify the vital spreaders in this paper. This proposed method considers the information contained in the box and q plays a weighting coefficient for this partition information. MLD would have different expressions with different values of q, and it would degenerate to local information dimension and variant of local dimension when q = 1 when q = 0 respectively, both of which have been effective identification methods for influential nodes. Thus, MLD would be a more general method which can degenerate to some existing centrality methods. In addition, different from classical methods, the node with low MLD would be more important in the network. Some real-world and theoretical complex networks and comparison methods are applied in this paper to show the effectiveness and reasonableness of this proposed method. The experiment results show the superiority of this proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105717	10.1016/j.knosys.2020.105717													
J								Hierarchical multi-task learning with CRF for implicit discourse relation recognition	KNOWLEDGE-BASED SYSTEMS										Multi-level implicit discourse relation recognition; Hierarchical multi-task neural network; Conditional random field layer; Semantic hierarchy; Mapping relationship		Implicit discourse relation recognition (IDRR) remains an ongoing challenge. Recently, various neural network models have been proposed for this task, and have achieved promising results. However, almost all of them predict multi-level discourse senses separately, which not only ignores the semantic hierarchy of and mapping relationships between senses, but also may result in inconsistent predictions at different levels. In this paper, we propose a hierarchical multi-task neural network with a conditional random field layer (HierMTN-CRF) for multi-level IDRR. Specifically, a HierMTN component is designed to jointly model multi-level sense classifications, with these senses as supervision signals at different feature layers. Consequently, the hierarchical semantics of senses are explicitly encoded into features at different layers. To further exploit the mapping relationships between adjacent-level senses, a CRF layer is introduced to perform collective sense predictions. In this way, our model infers a sequence of multi-level senses rather than separate sense predictions in previous models. In addition, our model can be easily constructed based on existing IDRR models. Experimental results and in-depth analyses on the benchmark PDTB data set show that our model achieves significantly better and more consistent results over several competitive baselines on multi-level IDRR, without additional time overhead. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105637	10.1016/j.knosys.2020.105637													
J								Link prediction of time-evolving network based on node ranking	KNOWLEDGE-BASED SYSTEMS										Dynamic network; Time-evolving SF network; Node ranking; Link prediction; Time series forecasting	RECONSTRUCTION; SIMILARITY; REGRESSION	Many real-world networks belong to the kind that evolves over time. So it is very meaningful and challenging to predict whether the link will occur in the network of future time. In this paper, both time-evolving scale-free (SF) network and real-world dynamic network are taken into consideration first and then two kinds of methods are respectively proposed for link prediction. Different from many existing similarity-based dynamic network link prediction methods, many of which adopt node-pair similarity such as common neighbors (CN), Adamic-Adar (AA), and so on, we measure the similarity between nodes from a new perspective. With further research into node ranking, some eigenvector-based methods, such as PageRank (PR), Cumulative Nomination (CuN) and so on, can compute the values of node importance which can be regarded as the stationary distribution of Markov chain for all nodes iteratively. Therefore, from a statistical point of view, the importance of a node is like the probability of attracting other nodes to connect with it and the derivative value of a node pair is like the probability of attracting each other. These node-ranking-based approaches are very novel in the field of link prediction in that few researches have paid enough attention to them before. In addition, an adaptively time series forecasting method is proposed in this paper, and it uses the historical similarity series to predict the future similarity between each node pair adaptively. Experimental results show that our proposed algorithms can predict the future links not only for the growing SF network but also for the dynamic networks in the real-world. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105740	10.1016/j.knosys.2020.105740													
J								A dummy-based user privacy protection approach for text information retrieval	KNOWLEDGE-BASED SYSTEMS										Text retrieval; Privacy protection; Feature distribution; Topic significance	WEB SEARCH; QUERIES; SCHEME	Text retrieval enables people to efficiently obtain the desired data from massive text data, so has become one of the most popular services in information retrieval community. However, while providing great convenience for users, text retrieval results in a serious issue on user privacy. In this paper, we propose a dummy-based approach for text retrieval privacy protection. Its basic idea is to use well-designed dummy queries to cover up user queries and thus protect user privacy. First, we present a client-based system framework for the protection of user privacy, which requires no change to the existing algorithm of text retrieval, and no compromise to the accuracy of text retrieval. Second, we define a user privacy model to formulate the requirements that ideal dummy queries should meet, i.e., (1) having highly similar feature distributions with user queries, and (2) effectively reducing the significance of user query topics. Third, by means of the knowledge derived from Wikipedia, we present an implementation algorithm to construct a group of ideal dummy queries that can well meet the privacy model. Finally, we demonstrate the effectiveness of our approach by theoretical analysis and experimental evaluation. The results show that by constructing dummy queries that have similar feature distributions but unrelated topics with user queries, the privacy behind users' textual queries can be effectively protected, under the precondition of not compromising the accuracy and usability of text retrieval. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				MAY 11	2020	195								105679	10.1016/j.knosys.2020.105679													
J								An adaptive integral separated proportional-integral controller based strategy for particle swarm optimization	KNOWLEDGE-BASED SYSTEMS										Particle swarm optimization; Integral separated strategy; Overshoot problem; Search direction; Proportional-integral controller	CONTROL-SYSTEM; DESIGN; PSO; ALGORITHMS	Particle swarm optimization algorithm (PSO), which updates the particle by the linear summation of the particle's past momentum and current search direction, has demonstrated its power in many optimization applications. However, few researches have focused on the overshoot problem of PSO caused by past momentum, which may result in an oscillation search and slow convergence speed on complex optimization problems. Based on the connection between the PSO optimization process and the PID controller based control system, we first analyze the effect of the momentum in PSO, then we find that the oscillation problem relates to the gathering of the momentum term and the current search direction. Inspired by the conditional integration in automatic control, we propose an adaptive search direction learning approach for PSO, namely ISPSO (Integral Separated PI controllerbased PSO). The ISPSO separates momentum term adaptively when the current search direction is consistent with historical momentum direction, and then the IS strategy will guide particles to fly to better and steady directions by eliminating the integral gathering of historical momentum. We select seven main-stream approaches as control group, and conduct experiments on benchmark CEC2013 test suite. The experimental results of ISPSO, providing the faster steady global convergence and higher solution accuracy, show a significant improvement on PSO algorithm. Compared with the results of the seven methods, the performance of ISPSO is also promising in general, especially for unimodal and composition functions. Furthermore, our results validate the generalization and effectiveness of IS strategy by the designed experiments of PSO variants with and without IS strategy, which also indicates that the IS strategy can be applied to PSO variants with any topological structures. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105696	10.1016/j.knosys.2020.105696													
J								Distributed semi-supervised learning algorithms for random vector functional-link networks with distributed data splitting across samples and features	KNOWLEDGE-BASED SYSTEMS										Distributed learning (DL); Semi-supervised learning (SSL); Manifold regularization (MR); Alternating direction method of multipliers (ADMM); Random vector functional link (RVFL); Distributed features	NEURAL-NETWORK	In this paper, we propose two manifold regularization (MR) based distributed semi-supervised learning (DSSL) algorithms using the random vector functional link (RVFL) network and alternating direction method of multipliers (ADMM) strategy. In DSSL problems, training data consisting of labeled and unlabeled samples are often large-scale or high-dimension and split across samples or features. These distributed data separately stored over a communication network where each node has only access to its own data and can only communicate with its neighboring nodes. In many scenarios, centralized algorithms cannot be applied to solve DSSL problems. In our previous work, we proposed a MR based DSSL algorithm, denoted as the D-LapWNN algorithm, to solve DSSL problems with distributed samples. It has been proved that the D-LapWNN algorithm, combining the wavelet neural network (WNN) with the zero-gradient-sum (ZGS) strategy, is an efficient DSSL algorithm with distributed samples or horizontally partitioned data. The drawback of the D-LapWNN algorithm is that the loss function of each node or agent over the communication network must be twice continuously differentiable. In order to extend our previous work and settle the corresponding drawback, we propose a horizontally DSSL (HDSSL) algorithm to solve DSSL problems with distributed samples. Then, we novelly propose a vertically DSSL (VDSSL) algorithm to solve DSSL problems with distributed features or vertically partitioned data. As far as we know, the VDSSL algorithm is the first work focusing on DSSL problems with distributed features. During the learning process of the proposed algorithms, nodes over the communication network only exchange coefficients rather than raw data. It means that the proposed algorithms are privacy-preserving methods. Finally, some simulations are given to show the efficiency of the proposed algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105577	10.1016/j.knosys.2020.105577													
J								An end-to-end functional spiking model for sequential feature learning	KNOWLEDGE-BASED SYSTEMS										Spiking neural network; Neuromorphic system; Sequential feature learning; Temporal encoding	NETWORKS; NEURONS	Spiking Neural Network (SNN) has recently gained significant momentum in the neuromorphic low-power systems. However, the existing SNN models have limited use in time-sequential feature learning, and the exhausting spike encoding and decoding make the SNNs not straightforward to use. Inspired by the functional organization in the primate visual system, we propose an end-to-end functional spiking model in this paper to address these issues. Specifically, we propose the functional spike response to make each neuron special, and the dynamic synaptic efficiency to make the transmission of each input signal controllable. We represent inputs by a simple two-tuple set instead of conventional complex encoding, which achieves end-to-end learning. Experiments on synthetic datasets demonstrate that employing the two-tuple encoding strategy, our method improves the accuracy of the traditional SNN model significantly. In addition, we apply our method to seven real-world datasets and one human motion prediction dataset to investigate its performance. Experimental results show that the proposed functional spike response organization saves the running time of our model compared with the LSTM, GRU and one of the state-of-the-art time series processing methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105643	10.1016/j.knosys.2020.105643													
J								Locating the propagation source in complex networks with a direction-induced search based Gaussian estimator	KNOWLEDGE-BASED SYSTEMS										Complex networks; Propagation source locating; Gaussian estimator (GE); Direction-induced search (DIS); Direction-induced search based Gaussian estimator (DISGE)	EPIDEMIC; RUMORS	Locating the propagation source is crucial for developing strategies to control the spreading process taking on complex networks. Gaussian estimator (GE) is one of the most effective methods for propagation source locating in networks with limited observers. However, on general graphs, due to GE makes an approximation that the actual diffusion tree in spreading process is assumed to be a breadth-first search (BFS) tree, and thus ignores the effect of direction information recorded in observers. Therefore, the accuracy of GE is affected. In this paper, by utilizing the direction information in observers, we define, for the first time, a novel direction-induced search (DIS). Further, a direction-induced search based Gaussian estimator (DISGE) is proposed by combining DIS and the original GE. Experimental results on a series of synthetic and real networks show that the DISGE is feasible and effective in locating the propagation source with limited observers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105674	10.1016/j.knosys.2020.105674													
J								High order discriminant analysis based on Riemannian optimization	KNOWLEDGE-BASED SYSTEMS										Classification; Clustering; Dimensionality reduction; Discriminant analysis; Product manifold; Riemannian optimization; Stiefel manifold	TENSOR DECOMPOSITIONS; QUALITY	Supervised learning of linear discriminant analysis is a well-known algorithm in machine learning, but most of the discriminant relevant algorithms are generally fail to discover the nonlinear structures in dimensionality reduction. To address such problem, thus we propose a novel method for dimensionality reduction of high-dimensional dataset, named manifold-based high order discriminant analysis (MHODA). Transforming the optimization problem from the constrained Euclidean space to a restricted search space of Riemannian manifold and employing the underlying geometry of nonlinear structures, it takes advantage of the fact that matrix manifold is actually of low dimension embedded into the ambient space. More concretely, we update the projection matrices for optimizing over the Stiefel manifold, and exploit the second order geometry of trust-region method. Moreover, in order to validate the efficiency and accuracy of the proposed algorithm, we conduct clustering and classification experiments by using six benchmark datasets. The numerical results demonstrate that MHODA is superiority to the most state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105630	10.1016/j.knosys.2020.105630													
J								Robust visual tracking with correlation filters and metric learning	KNOWLEDGE-BASED SYSTEMS										Visual tracking; Correlation filters; Metric learning; Hard negative mining	OBJECT TRACKING; NETWORKS; REGION	Discriminative correlation filters (DCFs) have been widely used in the visual tracking community in recent years. The DCFs-based trackers determine the target location through a response map generated by the correlation filters and determine the target scale by a fixed scale factor. However, the response map is vulnerable to noise interference and the fixed scale factor also cannot reflect the real scale change of the target, which can obviously reduce the tracking performance. In this paper, to solve the aforementioned drawbacks, we propose to learn a metric learning model in correlation filters framework for visual tracking (called CFML). This model can use a metric learning function to solve the target scale problem. In particular, we adopt a hard negative mining strategy to alleviate the influence of the noise on the response map, which can effectively improve the tracking accuracy. Extensive experimental results demonstrate that the proposed CFML tracker achieves competitive performance compared with the state-of-the-art trackers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105697	10.1016/j.knosys.2020.105697													
J								Augmented visual feature modeling for matching in low-visibility based on cycle-labeling of Superpixel Flow	KNOWLEDGE-BASED SYSTEMS										Feature modeling; Superpixel; Low-visibility; Optical flow; Illumination variation; Outlier removal		There are many automation systems that are required to work under poor visual conditions, such as auto-navigation in foggy or underwater environment. The low-visibility poses challenges for traditional feature modeling methods, which commonly contribute as a key component to such autonomous systems. It can thus negatively impact the performance of those computing systems. For example, the matching precision (matching quality) and the successfully identified matches (matching quantity) can both drop dramatically in low-visibility. On the other hand, human vision system can robustly identify visual features correctly despite the variations in lighting conditions. Inspired by human knowledge of perceiving visual features, this paper presents a novel feature modeling solution under poor visual conditions. Based on a color constancy enhanced illumination alignment, a new concept called Superpixel Flow (SPF) is proposed to model the visual features in images. SPF is generated considering the content motions across frame pairs, which make it easier to track across frames compared with classic Superpixels. The matching is achieved by a cycle-labeling strategy using Markov Random Field (MRF) with energy functions composed according to human knowledge of compare visual features. An outlier removal follows to further improve the matching accuracy. Competitive performance is demonstrated in the experiments compared with state-of-the-art approaches. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				MAY 11	2020	195								105699	10.1016/j.knosys.2020.105699													
J								A survey on multi-modal social event detection	KNOWLEDGE-BASED SYSTEMS										Multi-modality; Social event detection; Feature learning; Event inference	IMAGE RETRIEVAL; CROSS-MEDIA; CLASSIFICATION; FEATURES; ROBUST; IDENTIFICATION; SEGMENTATION; INFORMATION; RECOGNITION; HISTOGRAM	Due to the prevalence of social media sites, users are allowed to conveniently share their ideas and activities anytime and anywhere. Therefore, these sites hold substantial real-world event related data. Different from traditional social event detection methods which mainly focus on single-media, multimodal social event detection aims at discovering events in vast heterogeneous data such as texts, images and video clips. These data denote real-world events from multiple dimensions simultaneously so that they can provide comprehensive and complementary understanding of social event. In recent years, multi-modal social event detection has attracted intensive attentions. This paper concentrates on conducting a comprehensive survey of extant works. Two current attempts in this field are firstly reviewed: event feature learning and event inference. Particularly, event feature learning is a prerequisite because of its ability on translating social media data into computer-friendly numerical form. Event inference aims at deciding whether a sample belongs to a social event. Then, several public datasets in the community are introduced and the comparison results are also provided. At the end of this paper, a general discussion of the insights is delivered to promote the development of multi-modal social event detection. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAY 11	2020	195								105695	10.1016/j.knosys.2020.105695													
J								Extracting Time Expressions and Named Entities with Constituent-Based Tagging Schemes	COGNITIVE COMPUTATION										Inconsistent tag assignment; Position-based tagging scheme; Constituent-based tagging scheme; Named entities; Time expressions; Intrinsic characteristics		Time expressions and named entities play important roles in data mining, information retrieval, and natural language processing. However, the conventional position-based tagging schemes (e.g., the BIO and BILOU schemes) that previous research used to model time expressions and named entities suffer from the problem of inconsistent tag assignment. To overcome the problem of inconsistent tag assignment, we designed a new type of tagging schemes to model time expressions and named entities based on their constituents. Specifically, to model time expressions, we defined a constituent-based tagging scheme termed TOMN scheme with four tags, namely T, O, M, and N, indicating the defined constituents of time expressions, namely time token, modifier, numeral, and the words outside time expressions. To model named entities, we defined a constituent-based tagging scheme termed UGTO scheme with four tags, namely U, G, T, and O, indicating the defined constituents of named entities, namely uncommon word, general modifier, trigger word, and the words outside named entities. In modeling, our TOMN and UGTO schemes model time expressions and named entities under conditional random fields with minimal features according to an in-depth analysis for the characteristics of time expressions and named entities. Experiments on diverse datasets demonstrate that our proposed methods perform equally with or more effectively than representative state-of-the-art methods on both time expression extraction and named entity extraction.																	1866-9956	1866-9964				JUL	2020	12	4					844	862		10.1007/s12559-020-09714-8		MAY 2020											
J								Interactive Transfer Learning in Relational Domains	KUNSTLICHE INTELLIGENZ												We consider the problem of interactive transfer learning where a human expert provides guidance to the transfer learning algorithm that aims to transfer knowledge from a source task to a target task. One of the salient features of our approach is that we consider cross-domain transfer, i.e., transfer of knowledge across unrelated domains. We present an intuitive interface that allows for an expert to refine the knowledge in target task based on his/her expertise. Our results show that such guided transfer can effectively reduce the search space thus improving the efficiency and effectiveness of the transfer process.																	0933-1875	1610-1987				JUN	2020	34	2			SI		181	192		10.1007/s13218-020-00659-6		MAY 2020											
J								A Mechanistic Account of Stress-Induced Performance Degradation	COGNITIVE COMPUTATION										Performance degradation; Clarion; Cognitive architecture; Motivation; Implicit; Explicit	WORKING-MEMORY; MOTIVATION; IMPLICIT; CHOKING; ANXIETY; PERSONALITY; COMPONENTS; PREJUDICE; FRAGILITY; PRESSURE	Stress-induced performance degradation in high-pressure situations has been documented empirically and generated different explanations. The existing theories often assume the distinction between implicit and explicit processing but speculate differently on the impact that high-pressure situations have on their interaction. Although few attempts have been made so far at clarifying these underlying processes mechanistically (e.g., computationally), this paper proposes a detailed, mechanistic, and process-based account based on the Clarion cognitive architecture. This account incorporates facets of existing theories, but explores motivation, metacognition, and their effects on performance degradation. This account has been applied to different tasks that have previously suggested different explanations. These tasks were simulated within the Clarion cognitive architecture and results matched well with human data. Utilizing data from different tasks, we come up with a unified model of stress-induced performance degradation in high-pressure situations, which shows a unified, motivation-based, mechanistic account of these phenomena is possible, thus shedding light on the phenomena and pointing to mechanistic explanations of other related phenomena.																	1866-9956	1866-9964															10.1007/s12559-020-09725-5		MAY 2020											
J								High-speed and area-efficient Sobel edge detector on field-programmable gate array for artificial intelligence and machine learning applications	COMPUTATIONAL INTELLIGENCE										field-programmable gate array; hardware description language; high-level synthesis; MATLAB HDL coder; register transfer language; Sobel edge detection; Vivado	HIGH-LEVEL SYNTHESIS	Sobel edge detector is an algorithm commonly used in image processing and computer vision to extract edges from input images using derivative of image pixels in x and y directions against surrounding pixels. Most artificial intelligence and machine learning applications require image processing algorithms running in real time on hardware systems like field-programmable gate array (FPGAs). They typically require high throughput to match real-time speeds and since they run alongside other processing algorithms, they are required to be area efficient as well. This article proposes a high-speed and low-area implementation of the Sobel edge detection algorithm. We created the design using a novel high-level synthesis (HLS) design method based on application specific bit widths for intermediate data nodes. Register transfer level code was generated using MATLAB hardware description language (HDL) coder for HLS. The generated HDL code was implemented on Xilinx Kintex 7 field programmable gate array (FPGA) using Xilinx Vivado software. Our implementation results are superior to those obtained for similar implementations using the vendor library block sets as well as those obtained by other researchers using similar implementations in the recent past in terms of area and speed. We tested our algorithm on Kintex 7 using real-time input video with a frame resolution of 1920 x 1080. We also verified the functional simulation results with a golden MATLAB implementation using FPGA in the loop feature of HDL Verifier. In addition, we propose a generic area, speed, and power improvement methodology for different HLS tools and application designs.																	0824-7935	1467-8640															10.1111/coin.12334		MAY 2020											
J								Machine learning-based charge scheduling of electric vehicles with minimum waiting time	COMPUTATIONAL INTELLIGENCE										charge scheduling; electric vehicle; fast charging port; zigbee		In order to reduce the greenhouse gas emission and limit the rise in global temperature, the trend in automotive industry is changing rapidly and most of the manufacturers are moving towards the electrification of vehicles. Computational intelligence and machine learning play a very important role in the field of electric vehicles (EVs) due to the necessity of automatic control in battery charging and port accessibility. Due to the limited ranges of EVs, they have to be charged periodically during their travels and its charging will take more time. As the number of EVs increases, suitable charging infrastructure having many charging stations and co-ordination of scheduling the charging vehicles from charging stations are necessary. As charging stations have less number of fast charging ports, accessing these fast charging ports needs proper planning. The major challenge of an EV is to identify the charging station with a fast charging port which is on route to the destination with minimum waiting time. This article deals with the application of machine learning in selecting a charging station with available fast charging port and minimum waiting time.																	0824-7935	1467-8640															10.1111/coin.12333		MAY 2020											
J								A novel method for the classification of Alzheimer's disease from normal controls using magnetic resonance imaging	EXPERT SYSTEMS										Alzheimer's disease (AD); computer-aided diagnosis (CAD); FreeSurfer; LSTSVM; mild cognitive impairment (MCI); RELS-TSVM; TSVM	SUPPORT VECTOR MACHINES; DIMENSIONAL PATTERN-CLASSIFICATION; BRAIN ATROPHY; LEAST-SQUARES; COMPONENT ANALYSIS; MCI PATIENTS; DIAGNOSIS; MRI; AD; CONVERSION	Alzheimer's disease (AD) is the most prevalent form of dementia. Although fewer people, who suffer from AD are correctly and promptly diagnosed, due to a lack of knowledge of its cause and unavailability of treatment, AD is more manageable if the symptoms of mild cognitive impairment (MCI) are in an early stage. In recent years, computer-aided diagnosis has been widely used for the diagnosis of AD. The main motive of this paper is to improve the classification and prediction accuracy of AD. In this paper, a novel approach is developed to classify MCI, normal control (NC), and AD using structural magnetic resonance imaging (sMRI) from the Alzheimer's disease Neuroimaging Initiative (ADNI) dataset (50 AD, 50 NC, 50 MCI subjects). FreeSurfer is used to process these MRI data and obtain cortical features such as volume, surface area, thickness, white matter (WM), and intrinsic curvature of the brain regions. These features are modified by normalizing each cortical region's features using the absolute maximum value of that region's features from all subjects in each group of MCI, NC, and AD independently. A total of 420 features are obtained. To address the curse of dimensionality, the obtained features are reduced to 30 features using a sequential feature selection technique. Three classifiers, namely the twin support vector machine (TSVM), least squares TSVM (LSTSVM), and robust energy-based least squares TSVM (RELS-TSVM), are used to evaluate the classification accuracy from the obtained features. Five-fold and 10-fold cross-validation are used to validate the proposed method. Experimental results show an accuracy of 100% for the studied database. The proposed approach is innovative due to its higher classification accuracy compared to methods in the existing literature.																	0266-4720	1468-0394															10.1111/exsy.12566		MAY 2020											
J								Three-dimensional reconstruction of the dribble track of soccer robot based on heterogeneous binocular vision	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Heterogeneous binocular vision; Soccer robot; Dribble trajectory; Three-dimensional reconstruction	IMPLEMENTATION; SYSTEM	With the rapid development of science and technology, as an important branch of the computer. Binocular vision technology has also been greatly improved and improved. It has been applied to many fields of production and life, medical information, industrial manufacturing, and service industries. Heterogeneous binocular vision typically uses a camera to simulate the sensory acquisition process of humans. To process the target information, multiple viewpoints observe the same motion scene. Finally, multiple pairs of images are obtained from different viewpoints. Three-dimensional reconstruction has long been one of the important research fields of computer vision. It has great application value in robot autonomous navigation vision, self-driving vehicle, and automatic detection field and freedom degree mechanical device control. In the robot application of visual positioning technology, with the development needs, the level of intelligence of the robot is also gradually increasing. The three-dimensional reconstruction technology combined with the principle of heterogeneous binocular vision has important practical significance for completing the technique of the dribble track of soccer robots. Based on the RoboCup soccer robot competition, this paper uses the heterogeneous binocular vision technology to analyze the kinematics of the soccer robot in detail. By analyzing the basic motion characteristics of soccer robots, a three-dimensional reconstruction technology model of its dribble trajectory is established, and the reliability, practicability and rationality of the design are verified.																	1868-5137	1868-5145															10.1007/s12652-020-02039-2		MAY 2020											
J								Similarity-aware neural machine translation: reducing human translator efforts by leveraging high-potential sentences with translation memory	NEURAL COMPUTING & APPLICATIONS										Neural machine translation; Translation memory; High-potential sentences; Human translator efforts		In computer-aided translation tasks, reducing the time of reviewing and post-editing on translations is meaningful for human translators. However, existing studies mainly aim to improve overall translation quality, which only reduces post-editing time. In this work, we firstly identify testing sentences which are highly similar to training set (high-potential sentences) to reduce reviewing time, then we focus on improving corresponding translation quality greatly to reduce post-editing time. From this point, we firstly propose two novel translation memory methods to characterize similarity between sentences on syntactic and template dimensions separately. Based on that, we propose a similarity-aware neural machine translation (similarity-NMT) which consists of two independent modules: (1) Identification Module, which can identify high-potential sentences of testing set according to multi-dimensional similarity information; (2) Translation Module, which can integrate multi-dimensional similarity information of parallel training sentence pairs into an attention-based NMT model by leveraging posterior regularization. Experiments on two Chinese English domains have well-validated the effectiveness and universality of the proposed method of reducing human translator efforts.																	0941-0643	1433-3058															10.1007/s00521-020-04939-y		MAY 2020											
J								Optimized deployment method and performance evaluation of gas sensor network based on field experiment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gas sensor; Field experimentation; Deployment optimization; Gas characteristic; Meteorological features; MATLAB software	STOCHASTIC-PROGRAMMING APPROACH; PLACEMENT; LIFETIME	In order to prevent gas leakage, gas sensors are routinely deployed in chemical industrial parks to monitor it. However, because of the effects of weather conditions, such as wind speed and direction, these gas sensors may not be able to accurately monitor the concentration of the gas leak. Toxic and harmful gases not only do harm to the surrounding environment, but also to the health of nearby residents. In order to effectively monitor the occurrence of gas leakage, in this paper, a plan of setting up wireless sensor network on site is designed, which is based on the meteorological and gas characteristics. What's more, the plan includes the rectangle plan, the fan plan and the annular plan. Also, it stipulates the optimization standard of sensor node position and deployment scheme and field experiments were carried out in the chemical industrial park, whereby the monitoring values of gas concentration in different schemes were obtained. According to the standards, the optimal deployment scheme and the optimal location of sensor nodes in different schemes are also procured. The results show that the method is feasible and effective. It can optimize the deployment position of different elevation under certain wind speed and wind direction as well as optimize different solutions.																	1868-5137	1868-5145															10.1007/s12652-020-02055-2		MAY 2020											
J								Hyper-parameter optimization of deep learning model for prediction of Parkinson's disease	MACHINE VISION AND APPLICATIONS										Parkinson's disease; Hyperparameter optimization; Deep learning model; Grid search optimization		Neurodegenerative disorder such as Parkinson's disease (PD) is among the severe health problems in our aging society. It is a neural disorder that affects people socially as well as economically. It occurs due to the failure of the brain's dopamine-producing cells to produce enough dopamine to enable the motor movement of the body. This disease primarily affects vision, speech, movement problems, and excretion activity, followed by depression, nervousness, sleeping problems, and panic attacks. The onset of Parkinson's disease is diagnosed with the help of speech disorders, which are the earliest symptoms of it. The essential goal of this paper is to build up a viable clinical decision-making system that helps the doctor in diagnosing the PD influenced patients. In this paper, a specific framework based on grid search optimization is proposed to develop an optimized deep learning Model to predict the early onset of Parkinson's disease whereby multiple hyperparameters are to be set and tuned for evaluation of the deep learning model. The grid search optimization consists of three main stages, i.e., the optimization of the deep learning model topology, the hyperparameters, and its performance. An evaluation of the proposed approach is done on the speech samples of PD patients and healthy individuals. The results of the approach proposed are finally analyzed, which shows that the fine-tuning of the deep learning model parameters result in the overall test accuracy of 89.23% and the average classification accuracy of 91.69%.																	0932-8092	1432-1769				MAY 9	2020	31	5							32	10.1007/s00138-020-01078-1													
J								Transport electrification based on embedded electrical network and smart synchronization device	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Transport electrification; Embedded electrical network; Embedded devices; Semantic web; Remote control	SYSTEM; CHALLENGES; FUTURE	This paper provides to solve a problem about the transport communications and parallel alternators synchronization. Around this suggestion, the fully-electric idea attracts the researchers and developers attention, by changing the pneumatic and hydraulic devices to electrical systems. Moreover, this transport electrification based on embedded electrical network, which can be described as a parallel multi turbo-alternators connecting in the same bus-bar, to produce the electricity to various loads. In this paper, remote control and monitoring of embedded electrical network in real time is discussed. Indeed, the embedded electrical network applied on transport electrification depends on conditions that should be respected in parallel alternators, to avoid disturbances in estimation values and energy crisis. In this order, a smart synchronization device will be achieved. It is based on embedded device "Raspberry Pi" and ethernet protocol for communication. The synchronization device sends all estimations data to application web, by using a local database in "Raspberry Pi". Therefore, any change in data estimations or unbalance in parallel alternators conditions, the smart device sends a real-time alert and warning to smartphone application.																	1868-5137	1868-5145															10.1007/s12652-020-02050-7		MAY 2020											
J								Discrete-time Markov chain for prediction of air quality index	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Discrete-time Markov chain; Air quality; Prime air pollutant; AQI prediction	TAIWAN; OZONE; SUMMER; TAIPEI; CITY; SO2	Together with water and land, air is a fundamental necessity of life. Nevertheless, the ambient air quality is deteriorating around the world because of rapid urbanization and industrialization. The problem of air pollution has become a prominent issue for the public and academia. In fact, the public is more interested in being informed about the possibility of occurrence of air pollution episodes than the accurate forecasting of a specific pollutant. Therefore, this study proposes a process based upon discrete-time Markov chains (DTMC), to predict the air quality index (AQI) and identify the prime air pollutants in a specific area. This study utilizes online air quality monitoring data retrieved from the Taiwan Environment Protection Administration, to demonstrate the application of the process. The findings of the study revealed that there are three prime air pollutants, namely ozone (O-3), nitrogen dioxide (NO2), and fine particulate matter (PM10), which frequently contaminate the ambient air in Taipei city. Furthermore, this study used data for three time periods to verify the proposed process and found that the performance of the process in predicting the AQI values for 7 days is better than the prediction for 30 days and 62 days.																	1868-5137	1868-5145															10.1007/s12652-020-02036-5		MAY 2020											
J								Research of improving semantic image segmentation based on a feature fusion model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Semantic image segmentation; Feature fusion model; Atrous convolutions; Context information; Conditional random field		The context information of images had been lost due to the low resolution of features, and due to repeated combinations of max-pooling layer and down-sampling layer. When the feature extraction process had been performed using a convolutional network, the result of semantic image segmentation loses sensitivity to the location of the object. The semantic image segmentation based on a feature fusion model with context features layer-by-layer had been proposed. Firstly, the original images had been pre-processed by the Gaussian Kernel Function to generate a series of images with different resolutions to form an image pyramid. Secondly, inputting an image pyramid into the network structure in which the plurality of fully convolutional network was been combined in parallel to obtain a set of initial features with different granularities by expanding receptive fields using Atrous Convolutions, and the initialization of feature fusion with different layer-by-layer granularities in a top-down method. Finally, the score map of feature fusion model had been calculated and sent to the conditional random field, modeling the class correlations between image pixels of the original image by the fully connected conditional random field, and the spatial position information and color vector information of image pixels were jointed to optimize and obtain results. The experiments on the PASCAL VOC 2012 and PASCAL Context datasets had achieved better mean Intersection Over Union than the state-of-the-art works. The proposed method has about 6.3% improved to the conventional methods.																	1868-5137	1868-5145															10.1007/s12652-020-02066-z		MAY 2020											
J								Extraction and analysis of brain functional statuses for early mild cognitive impairment using variational auto-encoder	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Brain functional network; Early mild cognitive impairment (eMCI); Variational auto-encoder (VAE); Common functional network (CFN); Functional status	FRONTOTEMPORAL DEMENTIA; EFFECTIVE CONNECTIVITY; NETWORK; PATTERNS; ALEXNET	Deep Auto-Encoders (DAE) have been widely used in dimensionality reduction and feature extraction of brain functional networks. However, the features in aggregation matrices of functional networks obtained by DAE dimensionality reduction might lose part of time-varying information, so that DAE cannot learn the distribution of original features well. To solve these problems, we extracted and analyzed brain functional statuses for early Mild Cognitive Impairment (eMCI) based on Variational Auto-Encoder (VAE). The high dimensions of features in aggregation matrices of dynamic functional networks were reduced by VAE to obtain the corresponding hidden variable matrices. Gaussian Mixture Model (GMM) was used to cluster the features in the hidden variable matrices to form several Common Functional Networks (CFNs) representing different functional statuses. We analyzed the similarities and differences of functional statuses between eMCI subjects and normal subjects in different sub-segments, as well as the switching of functional statuses in the entire time series. The experimental results show that there are similarities between the most frequent functional statuses of the two types of subjects and differences between the least frequent functional statuses. The proposed method can more significantly reveal the similarity and difference of functional statuses between eMCI subjects and normal subjects than the comparable methods, and the switching rule of functional statuses can help better understand the dynamic characteristics of brain functional networks for eMCI patients.																	1868-5137	1868-5145															10.1007/s12652-020-02031-w		MAY 2020											
J								An enhanced approach on distributed accountability for shared data in cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Accountability; Homomorphic encryption; Data sharing; Cloud computing; Key abuse attack		Cloud computing contribute huge business opportunities. Cloud data has always hacked by the cyber attackers. The cloud user's major concerns in cloud are lack of clarity and loss of control over their data. Data accountability is the way to ensure the trust on cloud service provider which allows to monitor the cloud user data usage. These computation and storage are distributed in nature. To address this issue we are utilizing a framework called Cloud Information Accountability. To provide security to the log recording encryption is performed that leads to lack of time and space. Further the untrusted cloud service provider cause key abuse attacks. To overcome the above limitation a novel information accountability system is proposed to ensure the tracking of user data usage in cloud. The Homomorphic encryption is utilized on user data which is uploaded on the cloud. This feature is called "strong binding" which means both data and policies travel with the data thus providing distributed accountability which is very essential because of distributed nature of the cloud. Hence this framework provides end to end accountability for user's data at both Cloud Service Providers (CSP) and Cloud users by providing the log records for their data. Furthermore a protocol is designed based on objects for data access that prevents the key abuse attack. The method improves the security performance up to 95%.																	1868-5137	1868-5145															10.1007/s12652-020-02029-4		MAY 2020											
J								A dedicated hardware accelerator for real-time acceleration of YOLOv2	JOURNAL OF REAL-TIME IMAGE PROCESSING										Hardware accelerator; FPGA; Convolutional neural network; Object detection; YOLOv2		In recent years, dedicated hardware accelerators for the acceleration of the convolutional neural network (CNN) have been extensively studied. Although many studies have presented efficient designs on FPGAs for image classification neural network models such as AlexNet and VGG, there are still little implementations for CNN-based object detection applications. This paper presents an OpenCL-based high-throughput FPGA accelerator for the YOLOv2 object detection algorithm on Arria-10 GX1150 FPGA. The proposed hardware architecture adopts a scalable pipeline design to support multi-resolution input image and full 8-bit fixed-point datapath to improve hardware resource utilization. Layer fusion technology that merges the convolution, batch normalization and Leaky-ReLU is also developed to avoid transmission of intermediate data between FPGA and external memory. Experimental results show that the final design achieves a peak throughput of 566 GOP/s under the working frequency of 190 MHz. The accelerator can execute YOLOv2 inference computation (288x288 resolution) and tiny YOLOv2 (416x416resolution) at the speed of 35 and 71 FPS, respectively.																	1861-8200	1861-8219															10.1007/s11554-020-00977-w		MAY 2020											
J								Optimization of geometry quality model for wire and arc additive manufacture based on adaptive multi-objective grey wolf algorithm	SOFT COMPUTING										Wire and arc additive manufacture; Intelligent algorithm; Multi-objective grey wolf algorithm; TOPSIS	WELDING PROCESS PARAMETERS	In order to obtain good geometry quality in the wire and arc additive manufacture, it is important to select the appropriate process parameters. Firstly, based on the 4-factor and 5-level experiments, the multi-objective mathematical model of process parameters and geometry quality is established by response surface methodology. Secondly, an adaptive grey wolf algorithm for solving multi-objective problems is proposed. The algorithm introduces external Archive, adaptive hunting mechanism, and fusion polynomial mutation mechanism to improve the search ability of the grey wolf algorithm. Experiments show that the Pareto set obtained by the adaptive multi-objective grey wolf algorithm is more diverse and convergent than the other five well-known algorithms. Meanwhile, in order to obtain the desired geometry quality, the TOPSIS algorithm is used to analyze the Pareto set obtained to get the optimal process parameters.																	1432-7643	1433-7479				NOV	2020	24	22					17401	17416		10.1007/s00500-020-05027-y		MAY 2020											
J								Forecasting the unit cost of a DRAM product using a layered partial-consensus fuzzy collaborative forecasting approach	COMPLEX & INTELLIGENT SYSTEMS										Fuzzy collaborative forecasting; Dynamic random access memory; Layered partial consensus	HYBRID FUZZY; INTELLIGENCE APPROACH; LINEAR-REGRESSION; C-MEANS; TIME; OPTIMIZATION; MODELS; SYSTEM	A layered partial-consensus fuzzy collaborative forecasting approach is proposed in this study to forecast the unit cost of a dynamic random access memory (DRAM) product. In the layered partial-consensus fuzzy collaborative forecasting approach, the partial-consensus fuzzy intersection (PCFI) operator is applied instead of the prevalent fuzzy intersection (FI) operator to aggregate the fuzzy forecasts by experts. In this way, some meaningful information, such as the suitable number of experts, can be obtained through observing changes in the PCFI result when the number of experts varies. After applying the layered partial-consensus fuzzy collaborative forecasting approach to a real case, the experimental results revealed that the layered partial-consensus fuzzy collaborative forecasting approach outperformed three existing methods. The most significant advantage was up to 13%.																	2199-4536	2198-6053				OCT	2020	6	3					479	492		10.1007/s40747-020-00146-3		MAY 2020											
J								A simple detection and generation algorithm for simple circuits in directed graph based on depth-first traversal	EVOLUTIONARY INTELLIGENCE										Directed graph; Simple circuit; Trivial graph; Strongly connected component; Depth-first traversal		This paper proposes a new algorithm for detecting and generating simple circuits within a directed graph. Before generating all simple circuits in a directed graph, the algorithm first detects whether there is a simple circuit in directed graph, and the detection can divide the directed graph into trivial graphs or strongly connected components. In the process of detecting the existence of simple circuit, the algorithm repeatedly reduces the number of vertices in vertex set along with edge pruning. Based on depth-first traversal, we can get all simple circuits of the directed graph. Especially, instead of proceeding depth-first traversals based on all vertices, the algorithm starts with a random vertex in a strongly connected component to reduce the number of depth-first traversals, and obtain all simple circuits of this strongly connected component. The difficulty of the algorithm is to detect and delete the repeated circuits in the process of generating all simple circuits, which can reduce both the storage space and time complexity. As a result, the output of the algorithm does not contain any repeated circuits. The algorithm simplifies the edge set of directed graphs and greatly reduces the number of depth-first traversals, thereby reducing the computational complexity and improving the computational efficiency.																	1864-5909	1864-5917															10.1007/s12065-020-00416-6		MAY 2020											
J								On the cryptanalysis of S-DES using nature inspired optimization algorithms	EVOLUTIONARY INTELLIGENCE										Cryptanalysis; Simplified data encryption standard; Cuckoo search algorithm; Firefly algorithm; Black-hole optimization; Genetic algorithm; Memetic algorithm; Levy flight		Cryptanalysis has emerged to be an important topic in the era of modern emerging technologies. The cryptanalysis of Simplified Data Encryption Standard (S-DES) is a NP-Hard combinatorial problem. This paper has two goals. Firstly, we study the cryptanalysis of S-DES via nature-inspired meta-heuristic algorithms namely Cuckoo Search, Firefly and Black-Hole Optimization Algorithms. Each of these algorithms is based on fascinating natural phenomena and exploits the inherent uniqueness of such phenomena to solve optimization problems. Secondly, we present a comparative study on the efficiency of these three fairly new algorithms with that of previously established Memetic Algorithm and Genetic Algorithms in regard to S-DES cryptanalysis. Through experimentations and extensive tests, it has been shown that the proposed algorithm based on Cuckoo Search proves to be most efficient with respect to accuracy and execution time.																	1864-5909	1864-5917															10.1007/s12065-020-00417-5		MAY 2020											
J								Flat random forest: a new ensemble learning method towards better training efficiency and adaptive model size to deep forest	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Ensemble learning; Flat random forest; Training efficiency; Size-adaptive model	ALGORITHM	The known deficiencies of deep neural networks include inferior training efficiency, weak parallelization capability, too many hyper-parameters etc. To address these issues, some researchers presented deep forest, a special deep learning model, which achieves some significant improvements but remain poor training efficiency, inflexible model size and weak interpretability. This paper endeavors to solve the issues in a new way. Firstly, deep forest is extended to the densely connected deep forest to enhance the prediction accuracy. Secondly, to perform parallel training with adaptive model size, the flat random forest is proposed by achieving the balance between the width and depth of densely connected deep forest. Finally, two core algorithms are respectively presented for the forward output weights computation and output weights updating. The experimental results show, compared with deep forest, the proposed flat random forest acquires competitive prediction accuracy, higher training efficiency, less hyper-parameters and adaptive model size.																	1868-8071	1868-808X				NOV	2020	11	11					2501	2513		10.1007/s13042-020-01136-0		MAY 2020											
J								Development of a speed invariant deep learning model with application to condition monitoring of rotating machinery	JOURNAL OF INTELLIGENT MANUFACTURING										Maintenance; Long short-term memory; Convolutional neural network; Machine condition monitoring; Mechanical imbalance	WAVELET TRANSFORM; FAULT-DETECTION; PREDICTION	The application of cutting-edge technologies such as AI, smart sensors, and IoT in factories is revolutionizing the manufacturing industry. This emerging trend, so called smart manufacturing, is a collection of various technologies that support decision-making in real-time in the presence of changing conditions in manufacturing activities; this may advance manufacturing competitiveness and sustainability. As a factory becomes highly automated, physical asset management comes to be a critical part of an operational life-cycle. Maintenance is one area where the collection of technologies may be applied to enhance operational reliability using a machine condition monitoring system. Data-driven models have been extensively applied to machine condition data to build a fault detection system. Most existing studies on fault detection were developed under a fixed set of operating conditions and tested with data obtained from that set of conditions. Therefore, variability in a model's performance from data obtained from different operating settings is not well reported. There have been limited studies considering changing operational conditions in a data-driven model. For practical applications, a model must identify a targeted fault under variable operational conditions. With this in mind, the goal of this paper is to study invariance of model to changing speed via a deep learning method, which can detect a mechanical imbalance, i.e., targeted fault, under varying speed settings. To study the speed invariance, experimental data obtained from a motor test-bed are processed, and time-series data and time-frequency data are applied to long short-term memory and convolutional neural network, respectively, to evaluate their performance.																	0956-5515	1572-8145															10.1007/s10845-020-01578-x		MAY 2020											
J								A discrete differential evolution algorithm for flow shop group scheduling problem with sequence-dependent setup and transportation times	JOURNAL OF INTELLIGENT MANUFACTURING										Flow shop group scheduling; Sequence-dependent setup time; Round-trip transportation time; Discrete differential evolution; Cooperative-oriented optimization	MANUFACTURING CELL; HYBRID ALGORITHM; OPTIMIZATION; MAKESPAN	This study investigates a flow shop group scheduling problem where both sequence-dependent setup time between groups and round-trip transportation time between machines are considered. The objective is to minimize makespan. To solve the problem, we first develop a mixed integer linear programming model and then propose an efficient co-evolutionary discrete differential evolution algorithm (CDDEA). In the CDDEA, several problem-specific heuristic rules are generated to construct initial population. A novel discrete differential evolution mechanism and a cooperative-oriented optimization strategy are proposed to synergistically evolve both the sequence of jobs in each group and the sequence of groups. In addition, two lower bounds are developed to evaluate the solution quality of CDDEA. Extensive computational experiments are carried out. The results show that the proposed CDDEA is effective in solving the studied problem.																	0956-5515	1572-8145															10.1007/s10845-020-01580-3		MAY 2020											
J								Attractor landscapes in Boolean networks with firing memory: a theoretical study applied to genetic networks	NATURAL COMPUTING										Discrete dynamical systems; Boolean networks; Biological network modeling	BIOLOGICAL REGULATORY NETWORKS; CELL-CYCLE NETWORK; ARABIDOPSIS-THALIANA; DYNAMICAL BEHAVIOR; FEEDBACK LOOPS; ROBUSTNESS; MODEL; STABILITY; AUTOMATA; MOTIFS	In this paper we study the dynamical behavior of Boolean networks with firing memory, namely Boolean networks whose vertices are updated synchronously depending on their proper Boolean local transition functions so that each vertex remains at its firing state a finite number of steps. We prove in particular that these networks have the same computational power than the classical ones, i.e. any Boolean network with firing memory composed of m vertices can be simulated by a Boolean network by adding vertices. We also prove general results on specific classes of networks. For instance, we show that the existence of at least one delay greater than 1 in disjunctive networks makes such networks have only fixed points as attractors. Moreover, for arbitrary networks composed of two vertices, we characterize the delay phase space, i.e. the delay values such that networks admits limit cycles or fixed points. Finally, we analyze two classical biological models by introducing delays: the model of the immune control of the lambda\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\lambda $$\end{document}-phage and that of the genetic control of the floral morphogenesis of the plant Arabidopsis thaliana.																	1567-7818	1572-9796				JUN	2020	19	2			SI		295	319		10.1007/s11047-020-09789-0		MAY 2020											
J								Building an efficient OCR system for historical documents with little training data	NEURAL COMPUTING & APPLICATIONS										CNN; FCN; Historical documents; LSTM; Neural network; OCR; Porta fontium; Synthetic data	IMAGE	As the number of digitized historical documents has increased rapidly during the last a few decades, it is necessary to provide efficient methods of information retrieval and knowledge extraction to make the data accessible. Such methods are dependent on optical character recognition (OCR) which converts the document images into textual representations. Nowadays, OCR methods are often not adapted to the historical domain; moreover, they usually need a significant amount of annotated documents. Therefore, this paper introduces a set of methods that allows performing an OCR on historical document images using only a small amount of real, manually annotated training data. The presented complete OCR system includes two main tasks: page layout analysis including text block and line segmentation and OCR. Our segmentation methods are based on fully convolutional networks, and the OCR approach utilizes recurrent neural networks. Both approaches are state of the art in the relevant fields. We have created a novel real dataset for OCR from Porta fontium portal. This corpus is freely available for research, and all proposed methods are evaluated on these data. We show that both the segmentation and OCR tasks are feasible with only a few annotated real data samples. The experiments aim at determining the best way how to achieve good performance with the given small set of data. We also demonstrate that obtained scores are comparable or even better than the scores of several state-of-the-art systems. To sum up, this paper shows a way how to create an efficient OCR system for historical documents with a need for only a little annotated training data.																	0941-0643	1433-3058															10.1007/s00521-020-04910-x		MAY 2020											
J								An Efficient Mammogram Image Retrieval System Using an Optimized Classifier	NEURAL PROCESSING LETTERS										Modified Weiner Filter; Mammogram; Euclidean distance; Principal component analysis; MANFIS; Artificial bee colony algorithm	BREAST-CANCER; BURDEN	The computerized examination of mammograms in the breast cancer prevention is gaining much importance. The paper which is introduced proposes an adequate mammogram image retrieval methodology utilizing the optimized classifier. At first, the info mammogram image is brought as of the database and it is then pre-handled by utilizing the Modified Weiner. This filtered image undergoes the pectoral removal. This is followed with the method of Feature extraction. The extorted features are then categorized to 3 classes namely benign, malignant and normal by utilizing an optimized classifier. The 'Modified Adaptive Neuro-Fuzzy Inference System' is optimized using the ABC Algorithm ('Artificial Bee Colony'). Score values of such classified images are determined utilizing the 'Principal Component Analysis'. Then repeat a similar process for query images and finally, the minimal distance is evaluated betwixt these 2 images This is finished using the Euclidian distance and in this way the image having less distance on diverged from the question is recovered. The proposed mammogram image retrieval methodology is implemented on the stage called MATLAB and it is evaluated by utilizing disparate database images.																	1370-4621	1573-773X															10.1007/s11063-020-10254-3		MAY 2020											
J								Applying the fuzzy CESTAC method to find the optimal shape parameter in solving fuzzy differential equations via RBF-meshless methods	SOFT COMPUTING										Fuzzy differential equation (FDE); Radial basis function (RBF); Stochastic arithmetic; CESTAC method; CADNA library	DATA APPROXIMATION SCHEME; NUMERICAL-SOLUTIONS; MULTIQUADRICS; CADNA	In this paper, by using the CESTAC method and the CADNA library a procedure is proposed to solve a fuzzy initial value problem based on RBF-meshless methods under generalized H-differentiability. So a reliable approach is presented to determine optimal shape parameter and number of points for RBF-meshless methods. The results reveal that the proposed method is very effective and simple. Also, the numerical accuracy of the method is shown in the tables and figures, and algorithms are given based on the stochastic arithmetic. The examples illustrate the efficiency and importance of using the stochastic arithmetic in place of the floating-point arithmetic.																	1432-7643	1433-7479				OCT	2020	24	20					15655	15670		10.1007/s00500-020-04890-z		MAY 2020											
J								A Prosthetic Shank With Adaptable Torsion Stiffness and Foot Alignment	FRONTIERS IN NEUROROBOTICS										lower limb prosthesis; bioinspiration; elastic actuation; user-specific interaction; torsion adapter; impedance control	GAIT EVENT DETECTION; TRANSVERSE PLANE ADAPTER; TRANSTIBIAL AMPUTEE; STRAIGHT-LINE; ROTATION; WALKING; PARAMETERS; GYROSCOPE; ANKLE; TASK	Torsion adapters in lower limb prostheses aim to increase comfort, mobility and health of users by allowing rotation in the transversal plane. A preliminary study with two transtibial amputees indicated correlations between torsional stiffness and foot alignment to increase comfort and stability of the user depending on the gait situation and velocity. This paper presents the design and proof-of-concept of an active, bio-inspired prosthetic shank adapter and a novel approach to create a user-specific human-machine interaction through adapting the device's properties. To provide adequate support, load data and subjective feedback of subjects are recorded and analyzed regarding defined gait situations. The results are merged to an user individual preference-setting matrix to select optimal parameters for each gait situation and velocity. A control strategy is implemented to render the specified desired torsional stiffness and transversal foot alignment values to achieve situation-dependent adaptation based on the input of designed gait detection algorithms. The proposed parallel elastic drive train mimics the functions of bones and muscles in the human shank. It is designed to provide the desired physical human-machine interaction properties along with optimized actuator energy consumption. Following test bench verification, trials with five participants with lower limb amputation at different levels are performed for basic validation. The results suggest improved movement support in turning maneuvers. Subjective user feedback confirmed a noticeable reduction of load at the stump and improved ease of turning.																	1662-5218					MAY 8	2020	14								23	10.3389/fnbot.2020.00023													
J								Using Bionics to Restore Sensation to Reconstructed Breasts	FRONTIERS IN NEUROROBOTICS										embodiment; tactile feedback; bionics; breast; mastectomy; sensation	MASTECTOMY; CANCER; TOUCH; EXPECTATIONS; HEALTH; SENSOR; WOMEN	Mastectomy often leads to a complete desensitization of the chest, which in turn can give rise to diminished sexual function and to disembodiment of the breasts. One approach to mitigate the sensory consequences of mastectomy is to leverage technology that has been developed for the restoration of sensation in bionic hands. Specifically, sensors embedded under the skin of the nipple-areolar complex can be used to detect touches. The output of the sensors then drives electrical stimulation of the residual intercostal nerves, delivered through chronically implanted electrode arrays, thereby eliciting tactile sensations experienced on the nipple-areolar complex. The hope is that the bionic breast will restore a woman's sense that her breast belongs to her body so she can experience the pleasure of an embrace and derive the benefit of the sensual touch of her partner.																	1662-5218					MAY 8	2020	14								24	10.3389/fnbot.2020.00024													
J								Aggregating neighborhood information for negative sHarm probabilityampling for knowledge graph embedding	NEURAL COMPUTING & APPLICATIONS										Negative sampling; Knowledge graph; Neighborhood; NKSGAN; Link prediction	ATTENTION	Knowledge graphs, as linked data, can be extracted from texts in triple form that illustrate the structure of "entity-relation-entity." Knowledge graph embedding (KGE) models are used to map entities and relations into a continuous vector space with semantic constraints so as to learn a knowledge graph with fact triples. In the KGE model training process, both positive and negative triples are necessarily provided. Thus, negative sampling methods are meaningful in generating negative samples based on the representations of entities and relations. This paper proposes an innovative neighborhood knowledge selective adversarial network (NKSGAN), which leverages the representation of aggregating neighborhood information to generate high-quality negative samples for enhancing the performances of the discriminator. Experiments are conducted on widely used standard datasets such as FB15k, FB15k-237, WN18 and WN18RR to evaluate our model for link prediction task. The results present the superiority of our proposed NKSGAN than other baseline methods, indicating that the negative sampling process in NKSGAN is effective in generating high-quality negative samples for boosting KGE models.																	0941-0643	1433-3058															10.1007/s00521-020-04940-5		MAY 2020											
J								Assessment of relative impacts of various geo-mining factors on methane dispersion for safety in gassy underground coal mines: an artificial neural networks approach	NEURAL COMPUTING & APPLICATIONS										Methane layering; Methane dispersion; Safety of underground coal mines; Artificial neural networks (ANN); Principal component analysis (PCA); Geo-mining parameters	PREDICTION	Dispersing methane to a safer level is crucial for mines safety as methane has been the greatest contributor of explosion hazard in underground coal mines worldwide. Methane dispersion is affected by several geo-mining factors. This study is first of its kind, which makes an attempt to develop a model for predicting methane concentration in underground coal mines based on seven different geo-mining factors using multi-layered artificial neural networks. The main objective is to quantify the relative influences of these factors on methane dispersion in underground coal mines and identify the significant factors through sensitivity analysis. Three different architectures of neural networks were trained using the methane dispersion data generated through computational fluid dynamics simulations conducted at varied geo-mining conditions. Principal component analysis on the input set was done for dimensionality reduction, which reduced the number of variables to seven from eight while maintaining a variance of 99%. All the models performed very well, and the best model yielded mean square error of 0.0304 and R-2 of 0.942. The study unveiled some new facts on the relative effects of ventilation type and surface roughness on methane dispersion. It established that air velocity is the most significant and surface roughness of mine galley is the least significant factor affecting methane dispersion in underground coal mines with relative importance of 0.25 and 0.01, respectively. The outcome of this study will be useful in design of mine ventilation system for effective coal mine methane management and enhancing mines safety.																	0941-0643	1433-3058															10.1007/s00521-020-04974-9		MAY 2020											
J								TileGAN: category-oriented attention-based high-quality tiled clothes generation from dressed person	NEURAL COMPUTING & APPLICATIONS										Generative adversarial network (GAN); Image-to-image translation; Attention; Clothes generation		During the past decades, applying deep learning technologies on fashion industry are increasingly the mainstream. Due to the different gesture, illumination or self-occasion, it is hard to directly utilize the clothes images in real-world applications. In this paper, to handle this problem, we present a novel multi-stage, category-supervised attention-based conditional generative adversarial network by generating clear and detailed tiled clothing images from certain model images. This newly proposed method consists of two stages: in the first stage, we generate the coarse image which contains general appearance information (such as color and shape) and category of the garment, where a spatial transformation module is utilized to handle the shape changes during image synthesis and an additional classifier is employed to guide coarse image generated in a category-supervised manner; in the second stage, we propose a dual path attention-based model to generate the fine-tuned image, which combines the appearance information of the coarse result with the high-frequency information of the model image. In detail, we introduce the channel attention mechanism to assign weights to the information of different channels instead of connecting directly. Then, a self-attention module is employed to model long-range correlation making the generated image close to the target. In additional to the framework, we also create a person-to-clothing data set containing 10 categories of clothing, which includes more than 34 thousand pairs of images with category attribute. Extensive simulations are conducted, and experimental result on the data set demonstrates the feasibility and superiority of the proposed networks.																	0941-0643	1433-3058															10.1007/s00521-020-04928-1		MAY 2020											
J								Extraction and prioritization of product attributes using an explainable neural network	PATTERN ANALYSIS AND APPLICATIONS										Attribute extraction; Attribute prioritization; Grad-CAM; Explainable neural network; Convolutional neural network; Transfer learning	REVIEWS	Identification of product attributes is an important matter in real-world business environments because customers generally make purchase decisions based on their evaluation of the attributes of the product. Numerous studies on product attribute extraction have been performed on the basis of user-generated textual reviews. However, most of them focused only on the attribute extraction process itself and not on the relative importance of the extracted attributes, which are critical information that can be utilized for the promotion or development of specification sheets. Thus, in this study, we focused on the development of an attribute set for a product by considering the relative importance of the extracted attributes. First, we extracted the aspects by utilizing convolutional neural network-based approaches and transfer learning. Second, we propose a novel approach, consisting of variants of the Gradient-weighted class activation mapping (Grad-CAM) algorithm, one of the explainable neural network frameworks, to capture the importance score of each extracted aspect. Using a sentimental prediction model, we calculated the weight of each aspect that affects the sentiment decision. We verified the performance of our proposed method by comparing the similarity of the product attributes that it extracted and their relative importance with the product attributes that customers consider to be the most important and by comparing the attributes used to develop the specification sheet of an existing major commercial site.																	1433-7541	1433-755X				NOV	2020	23	4					1767	1777		10.1007/s10044-020-00878-5		MAY 2020											
J								Interactively shaping robot behaviour with unlabeled human instructions	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Interactive machine learning; Human-robot interaction; Shaping; Reinforcement learning; Unlabeled instructions		In this paper, we propose a framework that enables a human teacher to shape a robot behaviour by interactively providing it with unlabeled instructions. We ground the meaning of instruction signals in the task-learning process, and use them simultaneously for guiding the latter. We implement our framework as a modular architecture, named TICS (Task-Instruction-Contingency-Shaping) that combines different information sources: a predefined reward function, human evaluative feedback and unlabeled instructions. This approach provides a novel perspective for robotic task learning that lies between Reinforcement Learning and Supervised Learning paradigms. We evaluate our framework both in simulation and with a real robot. The experimental results demonstrate the effectiveness of our framework in accelerating the task-learning process and in reducing the number of required teaching signals.																	1387-2532	1573-7454				MAY 8	2020	34	2							35	10.1007/s10458-020-09459-6													
J								A performance simulation and verification method of packet scheduling algorithms for data stream based on QoS	EVOLUTIONARY INTELLIGENCE										IP QoS; Information service; Military priority; Packet scheduling		In the process of network-based data flow transmission for various applications of information services, there are some problems, such as limited bandwidth and unclear priority level of business data, which affect the end-to-end reliable transmission efficiency because the system cannot perceive the link state. Traditional parallel link redundancy schemes have not fundamentally solved the problems of invulnerability and reliability. On the basis of sorting out the demand of service data flow transmission, this paper proposes a performance verification method of data flow packet scheduling algorithm based on QoS to solve the problems of network delay and congestion, and explores a reliable end-to-end service data flow transmission mode.																	1864-5909	1864-5917															10.1007/s12065-020-00418-4		MAY 2020											
J								A novel energy estimation model for constraint based task offloading in mobile cloud computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile cloud computing; Task offloading; Energy estimation; Constraint-based rule generation; Task scheduling; Prioritization		The utility of mobile applications has been increased enormously due to the advancements in science and technology to assist the users for various purposes. The main intention of integrating cloud services and resources with the mobile application is to reduce battery usage and to improve the efficiency of mobile devices. Thus the process of shifting a task that can be run on the cloud resources for assistance is referred to as task offloading. The process of task offloading is critical in the field of Mobile Cloud Computing. The major issue that pairs with task offloading are the communication cost estimation of the devices. To overcome the above-mentioned issues and to create an effective task offloading model, A Novel Mobile Cloud Computing framework called Rule Generation based Energy Estimation Model (RG-EEM) is designed. The energy required for executing the task in the local mobile device and cloud is estimated by implementing an energy estimation algorithm. Then a novel constraints specific rule generation algorithm is used to estimate the task execution time and memory utilization of the task, from which the decision has to be taken for offloading or local execution by considering all the possible affecting characteristics. Further, a novel task clustering and scheduling algorithm are implemented to execute the task in the cloud server effectively which will help in allocating similar tasks to a particular virtual machine in the cloud. The RG-EEM algorithm will also help in partitioning the task and parallel execution. The effectiveness of the RG-EEM technique is evaluated using the parametric measures and compared with the existing techniques.																	1868-5137	1868-5145															10.1007/s12652-020-01903-5		MAY 2020											
J								Multilevel sentiment analysis using domain thesaurus	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cross domain; Multidomain classification; Sensitive embedding; Domain thesaurus; Aspect ranking; Opinion mining; Multilevel sentiment analysis; Spectral feature alignment; HotelRecommendation; Product rating		The main aspects or features of products are identified through aspect extraction. Sentiment analysis is performed for the source domain, which is referred as the product. Then, the source domain is mapped to target domain, which is referred to as the hotel, via a process called multidomain sentiment classification. A sentiment sensitive domain thesaurus makes the words alignment for the words that are expressing the same sentiments from two different domains. Then a rank is given for products as well as hotels- with the help of features that are present in history of reviews of consumers. The user may easily get the idea for shopping ideas by seeing star ratings on a website. Product recommendations and service recommendations for hotels will be given to the user. Based on each feature or aspect of the product positive and negative opinions are identified. Health-related problems are identified with the help of the patient's history. For the purpose of medical treatment, 47% of online users search for treatment procedures on the Internet based on the field of bioinformatics. The purpose of opinion mining is to separatepositive and negative opinions regarding the aspects of the object. The important decision here is, how to select the correct source domain to have an adaptation to a given target domain. The source and target domain share same sentiment words. For example, the electronics domain can be adapted for the kitchen Domain. Electronics products, such as televisions, fans, grinders, blenders, refrigerators, washing machines, are considered under the electronics domain. The cutting aspect of blenders is sharp under electronics domain can be related to knives under the kitchen domain. The kitchen domain is the target domain. If the above kitchen domain is the target domain, then the ranking is performed based on the history of reviews from online users for kitchen based products.																	1868-5137	1868-5145															10.1007/s12652-020-01941-z		MAY 2020											
J								From a quantum theory to a classical one	SOFT COMPUTING										Quantum-classical crossover; Open quantum systems; Generalized Coherent states; Global symmetries	COHERENT STATES; REPRESENTATIONS	We present and discuss a formal approach for describing the quantum to classical crossover based on the group-theoretic construction of generalized coherent states. The method was originally introduced by Yaffe (Rev Mod Phys 54:407, 1982) in 1982 for tackling large-N quantum field theories and has been recently used for studying open quantum systems whose environment, while becoming macroscopic, may or may not display a classical behaviour (Liuzzo-Scorpo et al. in EPL (Europhys Lett) 111(4):40008, 2015; Rossi et al. in Phys Rev A 96:032116, 2017; Foti et al. in Quantum 3:179, 2019; Coppo in Schwarzschild black holes as macroscopic quantum systems, Universita degli studi di Firenze, Florence, 2019). Referring to these recent developments, in this paper we provide the essential elements of Yaffe's approach in the framework of standard quantum mechanics, so as to clarify how the approach can be used without referring to quantum field theory. Moreover, we address the role played by a possible global symmetry in making the large-N limit of the original quantum theory to flow into a formally well-defined classical theory, and we specifically consider the quantum-to-classical crossover of angular momentum. We also give details of a paradigmatic example, namely that of N free one-dimensional spinless particles. Finally, we discuss upon the foundational requirement that any classical description should ultimately be derived from an underlying quantum theory, that, however, is not, and should never be confused with, the one obtained via some quantization procedure of the classical description itself.																	1432-7643	1433-7479				JUL	2020	24	14					10315	10325		10.1007/s00500-020-04934-4		MAY 2020											
J								Cubic bipolar fuzzy set with application to multi-criteria group decision making using geometric aggregation operators	SOFT COMPUTING										Bipolar fuzzy set; Cubic set; Cubic bipolar fuzzy sets; Operations for CBFSs; Geometric aggregation operators for CBFSs; Multi criteria group decision making	ALGORITHM; SELECTION	Bipolar irrational emotions are implicated in a broad variety of individual actions. For example, two specific elements of decision making are the benefits and adverse effects. The harmony and respectful coexistence between these two elements is viewed as a cornerstone to a healthy social setting. For bipolar fuzzy characteristics of the universe of choices that rely on a small range of degrees, a bipolar fuzzy decision making method utilizing different techniques is accessible. The idea of a simplistic bipolar fuzzy set is ineffective in supplying consistency to the details about the frequency of the rating due to minimal knowledge. In this respect, we present cubic bipolar fuzzy sets (CBFSs) as a generalization of bipolar fuzzy sets. The plan of this research is to establish an innovative multi-criteria group decision making (MCGDM) based on cubic bipolar fuzzy set (CBFS) by unifying aggregation operators under geometric mean operations. The geometric mean operators are regarded to be a helpful technique, particularly in circumstances where an expert is unable to fuse huge complex unwanted information properly at the outset of the design of the scheme. We present some basic operations for CBFSs under dual order, i.e., P-Order and R-Order. We introduce some algebraic operations on CBFSs and some of their fundamental properties for both orders. We propose P-cubic bipolar fuzzy weighted geometric (P-CBFWG) operator and R-cubic bipolar fuzzy weighted geometric (R-CBFWG) operator to aggregate cubic bipolar fuzzy data. We also discuss the useability and efficiency of these operators in MCGDM problem. In human decisions, the second important part is ranking of alternatives obtained after evaluation. In this regard, we present an improved score and accuracy function to compare the cubic bipolar fuzzy elements (CBFEs).We also discuss a set theoretic comparison of proposed set with other theories as well as method base comparison of the proposed method with some existing techniques of bipolar fuzzy domain.																	1432-7643	1433-7479				NOV	2020	24	21					16111	16133		10.1007/s00500-020-04927-3		MAY 2020											
J								An efficient soft computing approach for securing information over GAMEOVER Zeus Botnets with modified CPA algorithm	SOFT COMPUTING										Client computer system; Peer-to-peer networks; Botnets; Botmaster; CPA	P2P BOTNET	In recent times, security threats are predominant in almost all digital applications over the internet. One such threatening effect comes from GAMEOVER Zeus Botnet. These Botnets enter inside online networks and affect the Peer client list, through Internet Rely Chat (IRC). IRC includes applications like Tactical chat and On Chat. Zeus botnets gather the secret information of these users, such as bank data and military secrets. Moreover, the architecture is P2P and hence the location of GAMEOVER Zeus Botnet varies from the one to another botnets models. The threat of Zeus botnets loom large as they operate through the processes of identification of users, through which, they establish control over the hosts via P2P server. An intelligent method of identification of Zeus botnets in a distributed computing system using cryptographic schemes to defend against such attacks is the prime issue of research in this paper. An advanced Botnet capturing algorithm, named as modified cryptographic prefix IP anonymize (CPA) algorithm is proposed in this research paper to effectively counter the threats of Zeus bots. Essential features of the proposed CPA algorithm include prefixed IP mapped anonymizing in Internet Rely Chat, inbuilt adaptive security algorithm, which helps in network cleansing. If left unchecked, these Zeus botnets may result in consequent shutdown of the system. Hence, identification of the location of these botnets with the help of IP-prefix mapped method through modified CPA algorithm have been systematically presented in this paper.																	1432-7643	1433-7479				NOV	2020	24	21					16499	16507		10.1007/s00500-020-04956-y		MAY 2020											
J								Fuzzy modeling of refractory cement viscosity to improve thermocouples manufacturing process	SOFT COMPUTING										Refractory cement viscosity; Nonlinear modeling; Fuzzy Weibull regression; Thermocouples manufacturing; Quality and reliability	SYSTEMS; UNCERTAINTY; CONCRETE; LOGIC; TIME; LIFE	Refractory cement is one of the elementary materials for the thermocouples manufacture. It is important pointing out that viscosity greatly affects its quality and functionality. In this sense, there is a viscosity range in which refractory cement must be applied; this range is known as "pot life." For this reason, the cement setting process (viscosity behavior) should be modeled in order to predict its life (useful time). Like this, some operational factors must be considered, among them: temperature and humidity as well as the fact that pot life behavior is nonlinear and must be performed by a growth model. At the modeling process, it is necessary considering the uncertainty which is not considered in the properties of the bodies studied in the rheological models for real materials. This work proposes an inverse prediction method for measuring the prediction error time. Furthermore, it is suggested performing the estimated times by a triangular fuzzy number with the purpose of considering all uncertainty information and providing a reliable prediction. Therefore, the fuzzy theory to the Weibull analysis was adapted in order to estimate some faculties: the fuzzy reliability, the fuzzy useful time with a desired reliability and the fuzzy mean pot life. Results show accuracy and useful pot life predictions.																	1432-7643	1433-7479				NOV	2020	24	22					17035	17050		10.1007/s00500-020-04995-5		MAY 2020											
J								A new effective robust nonlinear controller based on PSO for interleaved DC-DC boost converters for fuel cell voltage regulation	SOFT COMPUTING										Interleaved boost converter; Proton-exchange membrane fuel cell; PSO algorithm; Nonlinear control; PSO-PI controller; Current ripples	SYSTEM; DESIGN; FAULTS; OPTIMIZATION; ALGORITHM	Output voltage regulation of DC-DC converters has recently gained an increasing attention to face the many system nonidealities. The fast switching behavior is nonlinear time varying, the presence of model and measurement uncertainties, and large variations, are all inherited challenges. The aim of the present work is to design a robust nonlinear controller that ensures satisfactory and robust output voltage regulation for a proton-exchange membrane fuel cell (PEMFC) based on a DC-DC Interleaved Boost Converter (IBC). A state-space model of the DC-DC IBC is first derived using the state-space averaging technique, and a mathematical model is constructed for the PEFMC. In this regard, a robust nonlinear controller and a proportional integral controller are proposed. The controllers are tuned though particle swarm optimization algorithm to estimate their good parameters assuring the desired performance is met. The integral of absolute error criterion is used to improve the dynamic performance of the overall controlled system. Furthermore, the closed-loop stability is analyzed using the Lyapunov stability theorem, and the effectiveness of the closed-loop system is validated under various operating conditions of the PEMFC and load perturbations. Compared to other methods, the obtained results demonstrate a superior performance of the proposed control strategy in terms of its robustness to variations and uncertainties, smooth tracking of a varying set-point and faster transients.																	1432-7643	1433-7479				NOV	2020	24	22					17051	17064		10.1007/s00500-020-04996-4		MAY 2020											
J								Generalized two-dimensional PCA based on l(2,p)-norm minimization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										2-D principal component analysis center dot; l(2,p)-norm; Sample mean; Feature extraction	PRINCIPAL-COMPONENT ANALYSIS; DIMENSIONALITY REDUCTION; ROBUST; NORM; L1-NORM; REPRESENTATION; CLASSIFICATION	To exploit the information from two-dimensional structured data, two-dimensional principal component analysis (2-DPCA) has been widely used for dimensionality reduction and feature extraction. However, 2-DPCA is sensitive to outliers which are common in real applications. Therefore, many robust 2-DPCA methods have been proposed to improve the robustness of 2-DPCA. But existing robust 2-DPCAs have several weaknesses. First, these methods cannot be robust enough to outliers. Second, to center a sample set mixed with outliers using the L2-norm distance is usually biased. Third, most methods do not preserve the nice property of 2-DPCA (rotational invariance), which is important for learning algorithm. To alleviate these issues, we present a generalized robust 2-DPCA, which is named as 2-DPCA with l2,p-norm minimization (l2,p-2-DPCA), for image representation and recognition. In l2,p-2-DPCA, l2,p-norm is employed as the distance metric to measure the reconstruction error, which can alleviate the effect of outliers. Therefore, the proposed method is robust to outliers and preserves the desirable property of 2-DPCA which is invariant to rotational and well characterizes the geometric structure of samples. Moreover, most existing robust PCA methods estimate sample mean from database with outliers by averaging, which is usually biased. Sample mean are treated as an unknown variable to remedy the bias of computing sample mean in l2,p-2-DPCA, we propose an iterative algorithm, which has a closed-form solution in each iteration. Experimental results on several benchmark databases demonstrate the effectiveness and advantages of our method.																	1868-8071	1868-808X				NOV	2020	11	11					2421	2438		10.1007/s13042-020-01127-1		MAY 2020											
J								Artificial Color Constancy via GoogLeNet with Angular Loss Function	APPLIED ARTIFICIAL INTELLIGENCE												Color constancy is the ability of the human visual system to perceive colors unchanged independently of illumination. Giving a machine this feature will be beneficial in many fields where chromatic information is used. Particularly, it significantly improves scene understanding and object recognition.In this article, we propose a transfer learning-based algorithm, which has two main features: accuracy higher than many state-of-the-art algorithms and simplicity of implementation. Despite the fact that GoogLeNet was used in the experiments, the given approach may be applied to any convolutional neural networks. Additionally, we discuss the design of a new loss function oriented specifically to this problem and propose a few of the most suitable options.																	0883-9514	1087-6545				JUL 28	2020	34	9					643	655		10.1080/08839514.2020.1730630		MAY 2020											
J								Enhanced forensic speaker verification performance using the ICA-EBM algorithm under noisy and reverberant environments	EVOLUTIONARY INTELLIGENCE										Independent component analysis; ICA-EBM; Noisy and reverberant environments; Forensic speaker verification systems	INDEPENDENT COMPONENT ANALYSIS; BLIND SEPARATION; RECOGNITION	Forensic speaker verification performance reduces significantly under high levels of noise and reverberation. Multiple channel speech enhancement algorithms, such as independent component analysis by entropy bound minimization (ICA-EBM), can be used to improve noisy forensic speaker verification performance. Although the ICA-EBM was used in previous studies to separate mixed speech signals under clean conditions, the effectiveness of using the ICA-EBM for improving forensic speaker verification performance under noisy and reverberant conditions has not been investigated yet. In this paper, the ICA-EBM algorithm is used to separate the clean speech from noisy speech signals. Features from the enhanced speech are obtained by combining the feature-warped mel frequency cepstral coefficients with similar features extracted from the discrete wavelet transform. The identity vector (i-vector) length normalized Gaussian probabilistic linear discriminant analysis is used as a classifier. The Australian Forensic Voice Comparison and QUT-NOISE corpora were used to evaluate forensic speaker verification performance under noisy and reverberant conditions. Simulation results demonstrate that forensic speaker verification performance based on ICA-EBM improves compared with that of the traditional independent component analysis under different types of noise and reverberation environments. For surveillance recordings corrupted with different types of noise (CAR, STREET and HOME) at - 10 dB signal to noise ratio, the average equal error rate of the proposed method based on ICA-EBM is better than that of the traditional ICA by 12.68% when the interview recordings are kept clean, and 7.25% when the interview recordings have simulated room reverberations.																	1864-5909	1864-5917															10.1007/s12065-020-00406-8		MAY 2020											
J								Harmony search algorithm for simultaneous minimization of bi-objectives in multi-row parallel machine layout problem	EVOLUTIONARY INTELLIGENCE										Multi-row parallel machine layout problem; Multi-product environment; Flow distance of products; Area of the layout; Simple heuristic; Harmony search algorithm	DESIGN; OPTIMIZATION	The manufacturing cost of products and productivity mainly depends on the arrangements of manufacturing facilities on the shop floor. The process of designing a good layout ensures the relative positions of different types of machines to satisfy the objectives of the manufacturers. Probably, the arrangement of machines in a single row with considering duplicate machines and forward flow of materials under a multi-product environment is providing better solutions to the manufacturers in view of satisfying the objectives. Since the usage of duplicate machines in a single row machine layout, the investment cost on machines is high along with the requirement of a lengthy space. In addition to that, the forward flow of materials is resulting from an increase in the flow distance of products. In view of eliminating these two constraints, the laying of machines in multi-row with forward and backward flow of products is being considered. The proposed work is discussed about the effective way of laying parallel machines in multiple rows to satisfy the minimization of bi-objective namely flow distance of products and area of the layout. A simple heuristic is developed to evaluate the bi-objectives for the given sequence of machines placed on the constrained multiple rows. Further, a harmony search algorithm is utilized to identify the best sequence of machines to be arranged in multi-rows to simultaneously minimize the bi-objectives. The effectiveness of the proposed algorithm is tested on the problems associated with single row machine layout and the problems dealt by Vitayasak and Pongcharoen (Expert Syst Appl 98:129-152, 2018). The results ensured that the proposed algorithm was capable of providing the best solution to the multi-row parallel machine layout problem by significantly minimizing the bi-objectives simultaneously.																	1864-5909	1864-5917															10.1007/s12065-020-00419-3		MAY 2020											
J								GIS-based hierarchical fuzzy multicriteria decision-making method for urban planning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										GIS; Fuzzy sets; MCDA; AHP; Land parcels; Urban planning	AGGREGATION; MODEL; AHP	We present a new GIS-based fuzzy hierarchical MCDA for multiple assessments of land parcels in urban planning. In our model the criteria are decomposed in a hierarchical structure in which the leaf nodes are modelled with trapezoidal and triangular fuzzy sets on the universe of a specific characteristic of the parcel. The fuzzy set of a criterion is constructed as intersection of the fuzzy sets of the subcriteria in next hierarchical level that compose it. This approach has the advantage of managing complex evaluations of land parcels and facilitating the attribution of the degrees with which land parcels meet criteria. The proposed model has been experimented on a study area constituted by the municipality of Pozzuoli (Italy) in which the land parcels are the microzones in which the municipality is divided; we apply our method to assess which microzones are most suitable to increase the accommodation services of public primary school near them. Comparison of the results with expert assessments show that out method turns out to be reliable and consistent with the expert's evaluations.																	1868-5137	1868-5145															10.1007/s12652-020-02043-6		MAY 2020											
J								A secure and reliable transmission scheme for low loss high performance wireless communication system based on IoT	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy carrying communication; Internet of things; Dynamic switching; Opportunity safe transmission; Energy constraint	POWER TRANSFER; INFORMATION; INTERNET; DESIGN; TIME	In order to solve the problem of energy limitation and security for wireless network transmission, an opportunistic secure transmission scheme based on dynamic handover optimization is proposed. Firstly, according to the network model of the information energy joint transmission system, the algorithm flow of the dynamic signal energy switching dips transmission scheme is determined. Then, the operation mechanism and constraints of the proposed transmission scheme are comprehensively judged from four aspects: energy transmission, information transmission based on dynamic switching, performance optimization and analysis. Finally, the advantages of the proposed scheme in energy conversion and information transmission performance are verified by experiments. Compared with several other advanced schemes, the proposed opportunistic secure transmission scheme based on dynamic handover optimization has greater throughput, and the system has better reliability and security, which has proved the validity of the proposed scheme.																	1868-5137	1868-5145															10.1007/s12652-020-01982-4		MAY 2020											
J								EEG-based emotion recognition using an improved radial basis function neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Emotion recognition; EEG signals; Improved; Radial basis function neural network (RBF-NN)	CLASSIFICATION; IMPLEMENTATION; SEGMENTATION; COMBINATION; RETRIEVAL; SPEECH	The most advanced human-computer interaction is to make computers, like humans, capable of intelligent perception, judgment, and feedback. In the interaction, the emotions of the interactors can be identified to make intelligent measures. Emotion recognition mainly includes the recognition of speech, facial expressions, text, gestures, and physiological signals. Among them, emotional recognition in physiological signals is the most authentic. Since the EEG signal is a comprehensive reflection of the activities of many neurons in the brain in the cerebral cortex and can directly reflect brain activity, the EEG is rich in useful information. Therefore, this article uses EEG signals for the study of emotion recognition. First, the EEG is collected, preprocessed, and feature extracted; then an improved radial basis function neural network (I-RBF-NN) algorithm is used to process the EEG data; finally, the experimental results obtained by different classification models are compared and analyzed. The experimental results show that the I-RBF-NN proposed in this paper is better than other comparison algorithms for emotion recognition of EEG.																	1868-5137	1868-5145															10.1007/s12652-020-02049-0		MAY 2020											
J								Arbitrary-oriented object detection via dense feature fusion and attention model for remote sensing super-resolution image	NEURAL COMPUTING & APPLICATIONS										Object detection; Arbitrary oriented; Rotation proposals; Remote sensing image; Attention model; Dense feature pyramid network; Super-resolution		In this paper, we aim at developing a new arbitrary-oriented end-to-end object detection method to further push the frontier of object detection for remote sensing image. The proposed method comprehensively takes into account multiple strategies, such as attention mechanism, feature fusion, rotation region proposal as well as super-resolution pre-processing simultaneously to boost the performance in terms of localization and classification under the faster RCNN-like framework. Specifically, a channel attention network is integrated for selectively enhancing useful features and suppressing useless ones. Next, a dense feature fusion network is designed based on multi-scale detection framework, which fuses multiple layers of features to improve the sensitivity to small objects. In addition, considering the objects for detection are often densely arranged and appear in various orientations, we design a rotation anchor strategy to reduce the redundant detection regions. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 and scene text dataset ICDAR2015 demonstrate that the proposed method can be competitive with or even superior to the state-of-the-art ones, like R2CNN and R2CNN++.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14549	14562		10.1007/s00521-020-04893-9		MAY 2020											
J								Fractional chaos maps with flower pollination algorithm for chaotic systems' parameters identification	NEURAL COMPUTING & APPLICATIONS										Fractional chaos maps; Flower pollination optimization algorithm; Chaotic systems; Chaotic systems with noise	DIFFERENTIAL EVOLUTION; OPTIMIZATION ALGORITHM; SEARCH ALGORITHM; MODELS	Meta-heuristic optimization algorithms are the new gate in solving most of the complicated nonlinear systems. So, improving their robustness, reliability, and convergence speed is the main target to meet the requirements of various optimization problems. In the current work, three different fractional-order chaos maps (FC-maps), which have been introduced recently, are incorporated with the fundamental flower pollination algorithm to tune its parameters adaptively. These maps are fractional logistic map, fractional sine map, fractional tent map, and their integer-order versions. As a result, fractional chaotic FPA (FC-FPA) is proposed. The FC-FPA has been mathematically tested over 10-, 30-, 50-, and 100-dimensional CEC 2017 benchmark functions. Moreover, the influence of merging FC-maps with FPA is investigated in case of increasing the number of maximum evaluation functions based on the ten functions of CEC 2020. Additionally, to assess the superiority of the proposed FC-FPA algorithm for more complicated optimization problems, it has been tested to extract the parameters of different chaotic systems with and without added noise. In addition, it is tested on the identification of the corresponding parameters for the chaotic behavior in brush-less DC motor. The results of the fractional version of CFPA are compared with that of integer CFPA and standard FPA via an extensive statistical analysis. Furthermore, a nonparametric statistical test is employed to affirm the superiority of the proposed fractional variants of CFPA. It is evident that the performance of FPA is highly influenced by integrating the fractional-order chaos maps as the introduced FC-FPA variants provide a better accurate and more consistent results as well as a higher speed of convergence especially upon using the fractional sine map.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16291	16327		10.1007/s00521-020-04906-7		MAY 2020											
J								Double graphs-based discriminant projections for dimensionality reduction	NEURAL COMPUTING & APPLICATIONS										Dimensionality reduction; Graph embedding; Graph construction; Pattern classification	PRESERVING PROJECTION; FACE RECOGNITION; SPARSE; CLASSIFICATION	Graph embedding plays an important role in dimensionality reduction for processing the high-dimensional data. In graph embedding, its keys are the different kinds of graph constructions that determine the performance of dimensionality reduction. Inspired by this fact, in this article we propose a novel graph embedding method named the double graphs-based discriminant projections (DGDP) by integrating two designed discriminative global graph constructions. The proposed DGDP can well discover the discriminant and geometrical structures of the high-dimensional data through the informative graph constructions. In two global graph constructions, we consider the geometrical distribution of each point on each edge of the graphs to define the adjacent weights with class information. Moreover, in the weight definition of one graph construction, we further strengthen pattern discrimination among all the classes to design the weights of the corresponding adjacent graph. To demonstrate the effectiveness of the proposed DGDP, we experimentally compare it with the state-of-the-art graph embedding methods on several data sets. The experimental results show that the proposed graph embedding method outperforms the competing methods with more power of data representation and pattern discrimination in the embedded subspace.																	0941-0643	1433-3058															10.1007/s00521-020-04924-5		MAY 2020											
J								Robust mixed-norm constrained regression with application to face recognitions	NEURAL COMPUTING & APPLICATIONS										Mixed-norm; Low rank; Recovered image; Alternating direction method of multipliers	SPARSE; ALGORITHM	Most existing regression-based classification methods cope with pixelwise noise via l(1)-norm or l(2)-norm, but neglect the structural information between pixels. To the best of our knowledge, nuclear norm-based matrix regression approaches have achieved great success for addressing imagewise noise, but may result in unreasonable regression and incorrect classification, especially when test images are extremely corrupted by larger occlusions and severe illumination variations, since they apply the corrupted test images to reconstruction process directly, and the influence of noise will be unavoidable. To overcome this limitation, this paper presents a robust mixed-norm constrained regression model to deal with the structural noise corruption. To be more specific, nuclear norm of the error between corrupted test image and its corresponding recovered image is exploited as a regular term for characterizing the low rank noise structure, and Frobenius norm is utilized to depict the difference between the recovered image and restructured image on account of the less noise of recovered image. Then, we adopt the alternating direction method of multipliers to settle our proposed approaches efficiently. Furthermore, the theoretical convergence proof and detailed analysis of computational complexity are provided to assess our algorithms. Eventually, extensive experiments on five well-known face databases have manifested that the proposed methods outperform some state-of-the-art regression-based approaches for primarily addressing noise caused by occlusion and illumination changes.																	0941-0643	1433-3058															10.1007/s00521-020-04925-4		MAY 2020											
J								A novel selection mechanism for evolutionary algorithms with metameric variable-length representations	SOFT COMPUTING										Metameric representations; Evolutionary algorithms; Variable-length algorithms	GENETIC ALGORITHM; NEURAL NETWORKS; SENSOR NETWORK; OPTIMIZATION; DESIGN; CLASSIFIER; PLACEMENT	Metameric problems are variable-length optimization problems whose representations take on an at least partially segmented structure. This is referred to as a metameric representation. Frequently, each of these segments defines one of a number of analogous components in the solution. Examples include the nodes in a coverage network or turbines in a wind farm. Locating optimal solutions requires, in part, determining the optimal number of components. Evolutionary algorithms can be applied but require modifications to the traditional fixed-length operators. This study proposes a new selection operator for metameric problems: length niching selection. First, the population is partitioned into several niches based on solution length. A window function determines at which lengths a niche is formed. Local selection is then applied within each niche independently, resulting in a new parent population formed by a diverse set of solution lengths. A coverage and a wind farm problem are used to demonstrate the effectiveness of the new operator.																	1432-7643	1433-7479				NOV	2020	24	21					16439	16452		10.1007/s00500-020-04953-1		MAY 2020											
J								An effective compensation of power quality issues using MPPT-based cuckoo search optimization approach	SOFT COMPUTING										Power quality; Unified power flow controller; Active power filter; Voltage swell; Voltage sag; Total harmonic distortion		Power quality becomes the most significant concerns for many reasons as the dependence of electricity is increasing in this modern society. In fact that less quality of power will generate some considerable losses in terms of economy in few moments. The potential power quality issues were flicker, voltage dips, supply and harmonic interruptions. The quality of power might be enhanced with the use of some filtering approaches and some compensators. This work presents a unique framework of optimal utilization of the unified power flow controller (UPFC). The UPFC's inverter series is then controlled so as to perform the following functions: load reactive power sharing with the shunt inverter and compensation of voltage sag/swell. A cuckoo search optimization-based maximum power point tracking algorithm is employed for providing best fitness function for varying duty cycles. The approach of active power filter is widely employed in the harmonic compensation of voltage swell/sag. The proposed system also provides low rate of total harmonic distortion. MATLAB/Simulink-based simulation results are discussed to support the developed concept.																	1432-7643	1433-7479				NOV	2020	24	22					16719	16725		10.1007/s00500-020-04966-w		MAY 2020											
J								Evolutionary optimization of artificial neural network using an interactive phase-based optimization algorithm for chaotic time series prediction	SOFT COMPUTING										Time series prediction; Feed-forward neural network; Phase based optimization; Evolutionary optimization		The prediction of chaotic time series is an important issue in nonlinear information procession. Due to the multi-modal, high-dimensional and non-differentiable or discontinuous characteristics of chaotic systems, global optimization techniques are required to avoid from falling into local optima for the prediction of chaotic time series. Phase-based optimization is recently proposed as a global search algorithm inspired by natural phenomena. In this paper, an improved phase-based optimization algorithm integrating stochastic interaction strategy and global optimal interaction strategy, termed interactive phase-based optimization (IPBO), is proposed to train feed-forward neural networks (FNNs) for chaotic time series prediction. The combination of stochastic interaction strategy and global optimal interaction strategy can balance the capability of exploration and exploitation in the global optimization process. To demonstrate the searching capability, sixteen widely used benchmark functions are firstly used to investigate its optimization performance. Then, the prediction effectiveness of FNNs trained by IPBO has been illustrated using classical chaotic time series of Lorenz, Box-Jenkins and Mackey-Glass. The training and testing performances of IPBO and other state-of-the-art optimization algorithms have been compared for predicting these time series. Conducted numerical experiments indicate that IPBO is not only competitive in functions optimization and has also a better learning ability in training FNNs among other state-of-the-art optimization algorithms.																	1432-7643	1433-7479				NOV	2020	24	22					17093	17109		10.1007/s00500-020-05002-7		MAY 2020											
