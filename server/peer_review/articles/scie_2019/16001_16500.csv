PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Enhancing Game Agent Pathfinding Through Dynamic Graph Reweighting	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Knowledge-based systems; multiagent systems; multiplayer; pathfinding		This paper proposes a computationally inexpensive algorithm that utilizes player data to optimize nonplayer character pathfinding in a competitive, multiplayer environment and focuses on imitating the player behavior. The algorithm's input consists of player statistics gathered during the current and previous matches with additional time and space context, similar in design to influence maps. The input is then enriched with two additional, novel variables, allowing easy online fine-tuning of the output. The obtained result influences the final edge values of the map graph. Any known pathfinding algorithm that works with digraphs can then be utilized to control the agent. This paper contains exemplary results obtained when analyzing input on a map modeled after an existing map in the video game Unreal Tournament.																	0218-0014	1793-6381				APR	2020	34	4							2059010	10.1142/S0218001420590107													
J								An Efficient Fully Homomorphic Encryption Scheme for Private Information Retrieval in the Cloud	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Fully homomorphic encryption; homomorphic operation; ciphertext refreshing; private information retrieval	KEY	Information retrieval in the cloud is common and convenient. Nevertheless, privacy concerns should not be ignored as the cloud is not fully trustable. Fully Homomorphic Encryption (FHE) allows arbitrary operations to be performed on encrypted data, where the decryption of the result of ciphertext operation equals that of the corresponding plaintext operation. Thus, FHE schemes can be utilized for private information retrieval (PIR) on encrypted data. In the FHE scheme proposed by Ducas and Micciancio (DM), only a single homomorphic NOT AND (NAND) operation is allowed between consecutive ciphertext refreshings. Aiming at this problem, an improved FHE scheme is proposed for efficient PIR where homomorphic additions and multiplications are based on linear operations on ciphertext vectors. Theoretical analysis shows that when compared with the DM scheme, the proposed scheme allows multiple homomorphic additions and a single homomorphic multiplication to be performed. The number of allowed homomorphic additions is determined by the ratio of the ciphertext modulus to the upper bound of initial ciphertext noise. Moreover, simulation results show that the proposed scheme is significantly faster than the DM scheme in the homomorphic evaluation for a series of algorithms.																	0218-0014	1793-6381				APR	2020	34	4							2055008	10.1142/S0218001420550083													
J								Weighted Densely Connected Convolutional Networks for Reinforcement Learning	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Dense convolutional networks; deep reinforcement learning; Q-learning; weighted connection		A weighted densely connected convolution network (W-DenseNet) is proposed for reinforcement learning in this work. The W-DenseNet can maximize the information flow between all layers in the network by cross layer connection, which can reduce the phenomenon of gradient vanishing and degradation, and greatly improves the speed of training convergence. The weight coefficient introduced in W-DenseNet, the current layer received all the previous layers' feature maps with different initial weights, which can extract feature information of different layers more effectively according to tasks. According to the weight adjusted by learning, the cross-layer connection is pruned to remove the cross-layer connection with smaller weight, so as to reduce the number of cross-layer. In this work, GridWorld and FlappyBird games are used for simulation. The simulation results of deep reinforcement learning based on W-DenseNet are compared with the traditional deep reinforcement learning algorithm and reinforcement learning algorithm based on DenseNet. The simulation results show that the proposed W-DenseNet method can make the results more convergent, reduce the training time, and obtain more stable results.																	0218-0014	1793-6381				APR	2020	34	4							2052001	10.1142/S0218001420520011													
J								WiCLoc: A Novel CSI-based Fingerprint Localization System	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Indoor localization; WiFi; fingerprint; channel state information		With the rapid development of smart devices and WiFi networks, WiFi-based indoor localization is becoming increasingly important in location-based services. Among various localization techniques, the fingerprint-based method has attracted much interest due to its high accuracy and low equipment requirement. Traditional fingerprint-based indoor localization systems mostly obtain positioning by measuring the received signal strength indicator (RSSI). However, the RSSI is affected by environmental influences, thereby limiting the precision of positioning. Therefore, we propose a new indoor fingerprint localization system based on channel state information (CSI). We adopt a novel method, in which the amplitude and phase of the CSI are fused to generate fingerprints in the training phase and apply a weighted k-nearest neighbor (KNN) algorithm for fingerprint matching during the estimation phase. The system is validated in an exhibition hall and laboratory and we also compare the results of the proposed system with those of two CSI-based and an RSSI-based fingerprint localization systems. The results show that the proposed system achieves a minimum mean distance error of 0.85 m in the exhibition hall and 1.28 m in the laboratory, outperforming the other systems.																	0218-0014	1793-6381				APR	2020	34	4							2058003	10.1142/S0218001420580033													
J								Algorithm for Curved Surface Mesh Generation Based on Delaunay Refinement	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Surface mesh; mesh generation; delaunay refinement; sizing function	RECONSTRUCTION	Curved surface mesh generation is a key step for many areas. Here, a mesh generation algorithm for closed curved surface based on Delaunay refinement is proposed. We focus on improving the shape quality of the meshes generated and making them conform to 2-manifold. The Delaunay tetrahedralization of initial sample is generated first, the initial surface mesh which is a subset of the Delaunay tetrahedralization can be achieved. A triangle is refined by inserting a new point if it is large or of bad quality. For each sample, we also check the triangles that adjoin it whether from a topological disk. If not, the largest triangle will be refined. Finally, the surface mesh is updated after a new point is inserted into the sample. The definition of mesh size function for surface mesh generation is also put in this paper. Meshing experiments of some models demonstrate that the new algorithm is advantageous in generating high quality surface mesh, the count of mesh is suitable and can well approximate the curved surface. The presented method can be used for a wide range of problems including computer graphics, computer vision and finite element method.																	0218-0014	1793-6381				APR	2020	34	4							2050007	10.1142/S021800142050007X													
J								Addressing data sparsity for neural machine translation between morphologically rich languages	MACHINE TRANSLATION										Neural machine translation; Factored models; Deep learning		Translating between morphologically rich languages is still challenging for current machine translation systems. In this paper, we experiment with various neural machine translation (NMT) architectures to address the data sparsity problem caused by data availability (quantity), domain shift and the languages involved (Arabic and French). We show that the Factored NMT (FNMT) model, which uses linguistically motivated factors, is able to outperform standard NMT systems using subword units by more than 1 BLEU point even when a large quantity of data is available. Our work shows the benefits of applying linguistic factors in NMT when faced with low- and high-resource conditions.																	0922-6567	1573-0573				APR	2020	34	1					1	20		10.1007/s10590-019-09242-9													
J								An Investigation into the Origin of Autopoiesis	ARTIFICIAL LIFE										Autopoiesis; origin of life; statistical mechanics; cellular automata	GAME; PHYSICS; MODELS; LIFE	Using a glider in the Game of Life cellular automaton as a toy model of minimal persistent individuals, this article explores how questions regarding the origin of life might be approached from the perspective of autopoiesis. Specifically, I examine how the density of gliders evolves over time from random initial conditions and then develop a statistical mechanics of gliders that explains this time evolution in terms of the processes of glider creation, persistence, and destruction that underlie it.																	1064-5462	1530-9185				APR	2020	26	1			SI		5	22		10.1162/artl_a_00307													
J								How Computational Experiments Can Improve Our Understanding of the Genetic Architecture of Common Human Diseases	ARTIFICIAL LIFE										Genetics; complexity; epistasis; simulation; genetic programming	QUANTITATIVE LIPID TRAITS; GENOME-WIDE ASSOCIATION; STATISTICAL EPISTASIS; COMPLEX; DISCOVERY; BIOLOGY	Susceptibility to common human diseases such as cancer is influenced by many genetic and environmental factors that work together in a complex manner. The state of the art is to perform a genome-wide association study (GWAS) that measures millions of single-nucleotide polymorphisms (SNPs) throughout the genome followed by a one-SNP-at-a-time statistical analysis to detect univariate associations. This approach has identified thousands of genetic risk factors for hundreds of diseases. However, the genetic risk factors detected have very small effect sizes and collectively explain very little of the overall heritability of the disease. Nonetheless, it is assumed that the genetic component of risk is due to many independent risk factors that contribute additively. The fact that many genetic risk factors with small effects can be detected is taken as evidence to support this notion. It is our working hypothesis that the genetic architecture of common diseases is partly driven by non-additive interactions. To test this hypothesis, we developed a heuristic simulation-based method for conducting experiments about the complexity of genetic architecture. We show that a genetic architecture driven by complex interactions is highly consistent with the magnitude and distribution of univariate effects seen in real data. We compare our results with measures of univariate and interaction effects from two large-scale GWASs of sporadic breast cancer and find evidence to support our hypothesis that is consistent with the results of our computational experiment.																	1064-5462	1530-9185				APR	2020	26	1			SI		23	37		10.1162/artl_a_00308													
J								The Complexity Ratchet: Stronger than Selection, Stronger than Evolvability, Weaker than Robustness	ARTIFICIAL LIFE										Evolution; complexity; epistasis; robustness; evolvability; in silico experimental evolution	EVOLUTION; ORGANIZATION; EPISTASIS	Using the in silico experimental evolution platform Aevol, we have tested the existence of a complexity ratchet by evolving populations of digital organisms under environmental conditions in which simple organisms can very well thrive and reproduce. We observed that in most simulations, organisms become complex although such organisms are a lot less fit than simple ones and have no robustness or evolvability advantage. This excludes selection from the set of possible explanations for the evolution of complexity. However, complementary experiments showed that selection is nevertheless necessary for complexity to evolve, also excluding non-selective effects. Analyzing the long-term fate of complex organisms, we showed that complex organisms almost never switch back to simplicity despite the potential fitness benefit. On the contrary, they consistently accumulate complexity in the long term, meanwhile slowly increasing their fitness but never overtaking that of simple organisms. This suggests the existence of a complexity ratchet powered by negative epistasis: Mutations leading to simple solutions, which are favorable at the beginning of the simulation, become deleterious after other mutations-leading to complex solutions-have been fixed. This also suggests that this complexity ratchet cannot be beaten by selection, but that it can be overthrown by robustness because of the constraints it imposes on the coding capacity of the genome.																	1064-5462	1530-9185				APR	2020	26	1			SI		38	57		10.1162/artl_a_00312													
J								Interpreting the Tape of Life: Ancestry-Based Analyses Provide Insights and Intuition about Evolutionary Dynamics	ARTIFICIAL LIFE										Phylogeny; lineage; metrics; evolutionary dynamics; phylogenetic metrics; data visualization; digital evolution		Fine-scale evolutionary dynamics can be challenging to tease out when focused on the broad brush strokes of whole populations over long time spans. We propose a suite of diagnostic analysis techniques that operate on lineages and phylogenies in digital evolution experiments, with the aim of improving our capacity to quantitatively explore the nuances of evolutionary histories in digital evolution experiments. We present three types of lineage measurements: lineage length, mutation accumulation, and phenotypic volatility. Additionally, we suggest the adoption of four phylogeny measurements from biology: phylogenetic richness, phylogenetic divergence, phylogenetic regularity, and depth of the most-recent common ancestor. In addition to quantitative metrics, we also discuss several existing data visualizations that are useful for understanding lineages and phylogenies: state sequence visualizations, fitness landscape overlays, phylogenetic trees, and Muller plots. We examine the behavior of these metrics (with the aid of data visualizations) in two well-studied computational contexts: (1) a set of two-dimensional, real-valued optimization problems under a range of mutation rates and selection strengths, and (2) a set of qualitatively different environments in the Avida digital evolution platform. These results confirm our intuition about how these metrics respond to various evolutionary conditions and indicate their broad value.																	1064-5462	1530-9185				APR	2020	26	1			SI		58	79		10.1162/artl_a_00313													
J								Autonomously Moving Pine-Cone Robots: Using Pine Cones as Natural Hygromorphic Actuators and as Components of Mechanisms	ARTIFICIAL LIFE										Autonomous; pine-cone robots; pine-cone hygromorph; natural hygromorphic actuator; artificial life; natural materials; natural energy		We have developed autonomously moving pine-cone robots, which are made of multiple joined pine-cone scales for outdoor natural environments. We achieved these natural robots by using pine cones as both natural hygromorphic actuators and components of the mechanisms. When they are put in outdoor places where moist periods (e.g., rain) and dry periods repeatedly occur, they can move up and down on the spot or move forward. This article describes the motivation behind our research, the design and implementation of three different hygromorphic actuators, and applications for autonomously moving robots in nature.																	1064-5462	1530-9185				APR	2020	26	1			SI		80	89		10.1162/artl_a_00310													
J								Death and Progress: How Evolvability is Influenced by Intrinsic Mortality	ARTIFICIAL LIFE										Intrinsic mortality; evolvability; evolutionary computation; spatial models; senescence	EVOLUTION	Many factors influence the evolvability of populations, and this article illustrates how intrinsic mortality (death induced through internal factors) in an evolving population contributes favorably to evolvability on a fixed deceptive fitness landscape. We test for evolvability using the hierarchical if-and-only-if (h-iff) function as a deceptive fitness landscape together with a steady state genetic algorithm (SSGA) with a variable mutation rate and indiscriminate intrinsic mortality rate. The mutation rate and the intrinsic mortality rate display a relationship for finding the global maximum. This relationship was also found when implementing the same deceptive fitness landscape in a spatial model consisting of an evolving population. We also compared the performance of the optimal mutation and mortality rate with a state-of-the-art evolutionary algorithm called age-fitness Pareto optimization (AFPO) and show how the two approaches traverse the h-iff landscape differently. Our results indicate that the intrinsic mortality rate and mutation rate induce random genetic drift that allows a population to efficiently traverse a deceptive fitness landscape. This article gives an overview of how intrinsic mortality influences the evolvability of a population. It thereby supports the premise that programmed death of individuals could have a beneficial effect on the evolvability of the entire population.																	1064-5462	1530-9185				APR	2020	26	1			SI		90	111		10.1162/artl_a_00311													
J								Evolution Towards Criticality in Ising Neural Agents	ARTIFICIAL LIFE										Evolution; artificial life; criticality; neural network; Ising model; Boltzmann machine		Criticality is thought to be crucial for complex systems to adapt at the boundary between regimes with different dynamics, where the system may transition from one phase to another. Numerous systems, from sandpiles to gene regulatory networks to swarms to human brains, seem to work towards preserving a precarious balance right at their critical point. Understanding criticality therefore seems strongly related to a broad, fundamental theory for the physics of life as it could be, which still lacks a clear description of how life can arise and maintain itself in complex systems. In order to investigate this crucial question, we model populations of Ising agents competing for resources in a simple 2D environment subject to an evolutionary algorithm. We then compare its evolutionary dynamics under different experimental conditions. We demonstrate the utility that arises at a critical state and contrast it with the behaviors and dynamics that arise far from criticality. The results show compelling evidence that not only is a critical state remarkable in its ability to adapt and find solutions to the environment, but the evolving parameters in the agents tend to flow towards criticality if starting from a supercritical regime. We present simulations showing that a system in a supercritical state will tend to self-organize towards criticality, in contrast to a subcritical state, which remains subcritical though it is still capable of adapting and increasing its fitness.																	1064-5462	1530-9185				APR	2020	26	1			SI		112	129		10.1162/artl_a_00309													
J								Neural Autopoiesis: Organizing Self-Boundaries by Stimulus Avoidance in Biological and Artificial Neural Networks	ARTIFICIAL LIFE										Autopoiesis; homeostasis; neural adaptation; cultured neurons; spiking neural networks; STDP	TIMING-DEPENDENT PLASTICITY; FREE-ENERGY PRINCIPLE; ACTIVE INFERENCE; SPIKING NEURONS; ADAPTATION; DYNAMICS; MODEL; PATTERNS; CORTEX; FIELD	Living organisms must actively maintain themselves in order to continue existing. Autopoiesis is a key concept in the study of living organisms, where the boundaries of the organism are not static but dynamically regulated by the system itself. To study the autonomous regulation of a self-boundary, we focus on neural homeodynamic responses to environmental changes using both biological and artificial neural networks. Previous studies showed that embodied cultured neural networks and spiking neural networks with spike-timing dependent plasticity (STDP) learn an action as they avoid stimulation from outside. In this article, as a result of our experiments using embodied cultured neurons, we find that there is also a second property allowing the network to avoid stimulation: If the agent cannot learn an action to avoid the external stimuli, it tends to decrease the stimulus-evoked spikes, as if to ignore the uncontrollable input. We also show such a behavior is reproduced by spiking neural networks with asymmetric STDP. We consider that these properties are to be regarded as autonomous regulation of self and nonself for the network, in which a controllable neuron is regarded as self, and an uncontrollable neuron is regarded as nonself. Finally, we introduce neural autopoiesis by proposing the principle of stimulus avoidance.																	1064-5462	1530-9185				APR	2020	26	1			SI		130	151		10.1162/artl_a_00314													
J								Conditional density estimation and simulation through optimal transport	MACHINE LEARNING										Conditional density estimation; Optimal transport; Wasserstein barycenter; Explanation of variability; Confounding factors; Sampling; Uncertainty quantification		A methodology to estimate from samples the probability density of a random variable x conditional to the values of a set of covariates {zl}\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\{z_{l}\}$$\end{document} is proposed. The methodology relies on a data-driven formulation of the Wasserstein barycenter, posed as a minimax problem in terms of the conditional map carrying each sample point to the barycenter and a potential characterizing the inverse of this map. This minimax problem is solved through the alternation of a flow developing the map in time and the maximization of the potential through an alternate projection procedure. The dependence on the covariates {zl}\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\{z_{l}\}$$\end{document} is formulated in terms of convex combinations, so that it can be applied to variables of nearly any type, including real, categorical and distributional. The methodology is illustrated through numerical examples on synthetic and real data. The real-world example chosen is meteorological, forecasting the temperature distribution at a given location as a function of time, and estimating the joint distribution at a location of the highest and lowest daily temperatures as a function of the date.																	0885-6125	1573-0565				APR	2020	109	4					665	688		10.1007/s10994-019-05866-3													
J								Distributed block-diagonal approximation methods for regularized empirical risk minimization	MACHINE LEARNING										Distributed optimization; Large-scale learning; Empirical risk minimization; Dual method; Inexact method	DESCENT METHODS; ERROR-BOUNDS; COMPLEXITY; CODES	In recent years, there is a growing need to train machine learning models on a huge volume of data. Therefore, designing efficient distributed optimization algorithms for empirical risk minimization (ERM) has become an active and challenging research topic. In this paper, we propose a flexible framework for distributed ERM training through solving the dual problem, which provides a unified description and comparison of existing methods. Our approach requires only approximate solutions of the sub-problems involved in the optimization process, and is versatile to be applied on many large-scale machine learning problems including classification, regression, and structured prediction. We show that our framework enjoys global linear convergence for a broad class of non-strongly-convex problems, and some specific choices of the sub-problems can even achieve much faster convergence than existing approaches by a refined analysis. This improved convergence rate is also reflected in the superior empirical performance of our method.																	0885-6125	1573-0565				APR	2020	109	4					813	852		10.1007/s10994-019-05859-2													
J								On cognitive preferences and the plausibility of rule-based models	MACHINE LEARNING										Inductive rule learning; Interpretable models; Cognitive bias	CONJUNCTION FALLACY; NOVELTY DETECTION; NEURAL-NETWORKS; KNOWLEDGE; UNCERTAINTY; HEURISTICS; DISCOVERY; INFORMATION; PSYCHOLOGY; FREQUENCY	It is conventional wisdom in machine learning and data mining that logical models such as rule sets are more interpretable than other models, and that among such rule-based models, simpler models are more interpretable than more complex ones. In this position paper, we question this latter assumption by focusing on one particular aspect of interpretability, namely the plausibility of models. Roughly speaking, we equate the plausibility of a model with the likeliness that a user accepts it as an explanation for a prediction. In particular, we argue that-all other things being equal-longer explanations may be more convincing than shorter ones, and that the predominant bias for shorter models, which is typically necessary for learning powerful discriminative models, may not be suitable when it comes to user acceptance of the learned models. To that end, we first recapitulate evidence for and against this postulate, and then report the results of an evaluation in a crowdsourcing study based on about 3000 judgments. The results do not reveal a strong preference for simple rules, whereas we can observe a weak preference for longer rules in some domains. We then relate these results to well-known cognitive biases such as the conjunction fallacy, the representative heuristic, or the recognition heuristic, and investigate their relation to rule length and plausibility.																	0885-6125	1573-0565				APR	2020	109	4					853	898		10.1007/s10994-019-05856-5													
J								Fast core pricing algorithms for path auction	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Path auction; Core; Pricing algorithm; Constraint set	COMBINATORIAL; VCG	Path auction is held in a graph, where each edge stands for a commodity and the weight of this edge represents the prime cost. Bidders own some edges and make bids for their edges. The auctioneer needs to purchase a sequence of edges to form a path between two specific vertices. Path auction can be considered as a kind of combinatorial reverse auctions. Core-selecting mechanism is a prevalent mechanism for combinatorial auction. However, pricing in core-selecting combinatorial auction is computationally expensive, one important reason is the exponential core constraints. The same is true of path auction. To solve this computation problem, we simplify the constraint set and get the optimal set with only polynomial constraints in this paper. Based on our constraint set, we put forward two fast core pricing algorithms for the computation of bidder-Pareto-optimal core outcome. Among all the algorithms, our new algorithms have remarkable runtime performance. Finally, we validate our algorithms on real-world datasets and obtain excellent results.																	1387-2532	1573-7454				APR	2020	34	1							18	10.1007/s10458-019-09440-y													
J								Effects of direct input-output connections on multilayer perceptron neural networks for time series prediction	SOFT COMPUTING										Time series prediction; BPNN-DIOC; Linear relationship; Prediction accuracy		Feedforward neural network prediction is the most commonly used method in time series prediction. In view of the low prediction accuracy of the conventional BPNN model when the time series data contain a certain linear relationship, this paper describes a neural network approach for time series prediction, that is BPNN-DIOC (back-propagation neural network with direct input-to-output connections). Eight different datasets were used to verify the validity of BPNN-DIOC model in time series prediction. In this paper, the BPNN was extended to four variants based on the presence or absence of output layer bias and input-to-output connections firstly, and the prediction accuracy of eight datasets are analyzed by statistic method secondly. Finally, the experimental results demonstrate that the BPNN-DIOC has better prediction accuracy compared to the conventional BPNN while the output layer bias has no significant effect. Therefore, the input-to-output connections can significantly improve the prediction ability of time series.																	1432-7643	1433-7479				APR	2020	24	7					4729	4738		10.1007/s00500-019-04480-8													
J								An ANFIS-TLBO criterion for shear failure of rock joints	SOFT COMPUTING										Direct shear test; Rock joints; ANFIS; Barton's criterion; TLBO	FUZZY INFERENCE SYSTEM; AGGREGATION OPERATORS; BAYESIAN INVERSION; STRENGTH; BEHAVIOR; PREDICTION	The strength behavior of a rock mass is often controlled by different discontinuities therein, along which the structural failures may threaten rock structures. Thus, the shear strength criteria for rock joints have always been a fundamental topic in rock mechanics. Knowing the different aspects of the rock joint shear strength based on the previously presented criteria, it is time to try different approaches other than the regression analysis in order to model the nonlinear behavior of rock joints. This research focuses on the estimation of the shear strength of rock joints using the computational intelligence. A total of 84 direct shear tests were first performed on replicas of natural rock fractures with various mechanical and morphological characteristics under several normal stress levels. Then, an adaptive neuro-fuzzy inference system (ANFIS) combined with a teaching-learning-based optimization (TLBO) algorithm was used to establish a new shear strength criterion. The results demonstrated that the ANFIS-TLBO criterion provided an accurate estimation of rock joint shear strength. A comparison between the ANFIS-TLBO criterion and the Barton's empirical equation revealed a better performance of the suggested model in mapping the experimental data. The ANFIS-TLBO model's residual indicated a random pattern, and its histogram exhibited a symmetric bell-shaped distribution around zero, supporting the appropriateness of the model.																	1432-7643	1433-7479				APR	2020	24	7					4759	4773		10.1007/s00500-019-04230-w													
J								Modeling, simulation, estimation and boundedness analysis of discrete event systems	SOFT COMPUTING										Discrete event systems; Petri nets; FCF systems; Fastest non-decreasing trajectory; State estimator; Boundedness	PETRI NETS; MARKING ESTIMATION; STATE ESTIMATION	In this paper, a particular state model allowing describing the system evolution in time is proposed. This state model contains two inequalities describing the evolution in time of the system state and input. The system simulation is based on the resolution of this state model. After that, a state estimator is proposed in order to estimate the whole system state and inputs. The state model and the observer are both proposed following count and dater approaches successively. In thiswork, the considered state is the number of transition firing if a count approach is followed and the dates of firing if the dater approach is considered. It is proved, using an illustrative example, that the proposed observer estimates well the system state by comparing the simulated and the estimated states. A boundedness analysis of the system trajectory is proposed in the case of FCF Petri nets. This analysis is based on an algorithm which gives the bounded transitions knowing the system input. Some particular tables are elaborated to describe the proposed algorithm. Theses tables give the bounded transitions after each iteration of the algorithm.																	1432-7643	1433-7479				APR	2020	24	7					4775	4789		10.1007/s00500-019-04231-9													
J								Identification in the delta domain: a unified approach via GWOCFA	SOFT COMPUTING										Hammerstein model; Wiener model; Chaotic map; Delta operator; Grey wolf optimizer-based chaotic firefly algorithm (GWOCFA)	OPTIMIZATION ALGORITHM; SYSTEMS	The identification of linear dynamic systems in the delta domain has been proposed in this paper with the help of a hybrid metaheuristic algorithm combining chaotic firefly algorithm (CFA) and grey wolf optimiser (GWO). GWO performs the global search, while CFA fine-tunes the solutions through its local search abilities, thereby balancing exploration and exploitation features. Linear systems with static nonlinearities at the input are termed as the Hammerstein model, whereas linear systems with static nonlinearities at the output are known as the Wiener model. A test case with continuous polynomial nonlinearities has been taken up for Hammerstein and Wiener system identification in the delta domain. Delta operator parameterisation unifies identification of continuous-time systems with the discrete domain at a higher sampling rate. Pseudo-random binary sequence (PRBS), polluted with white Gaussian noise of fixed signal-to-noise ratio (SNR), has been considered as the input signal to estimate the unknown model parameters as well as static nonlinear coefficients. The hybrid algorithm not only supersedes the parent heuristics of which it is constituted but also proves better in comparison with some standard and latest heuristic approaches reported in the literature. Nonparametric statistical tests are performed to validate the results. The plots of fitness function (normalised value) against the number of iterations also support the convergence speed and accuracy of the results.																	1432-7643	1433-7479				APR	2020	24	7					4791	4808		10.1007/s00500-019-04232-8													
J								m-MBOA: a novel butterfly optimization algorithm enhanced with mutualism scheme	SOFT COMPUTING										Optimization algorithm; Butterfly optimization algorithm (BOA); Mutualism phase; Symbiosis organisms search (SOS); m-MBOA; Hybrid method; Benchmark function	SYMBIOTIC ORGANISMS SEARCH; DIFFERENTIAL EVOLUTION	The simplicity and effectiveness of a recently proposed metaheuristic, butterfly optimization algorithm (BOA) have gained huge popularity among research community and are being used to solve optimization problems in various disciplines. However, the algorithm is suffering from poor exploitation ability and has a tendency to show premature convergence to local optima. On the other hand, the mutualism phase of another popular metaheuristic symbiosis organisms search (SOS) is known for its exploitation capability. In this paper, a novel hybrid algorithm, namely m-MBOA is proposed to enhance the exploitation ability of BOA with the help of mutualism phase of SOS. To evaluate the effectiveness of m-MBOA, thirty-seven (37) classical benchmark functions are considered and the performance of m-MBOA is compared with the performance of ten (10) state-of-the-art algorithms. Statistical tools have been employed to observe the efficiency of the m-MBOA qualitatively, and obtained results confirm the superiority of the proposed algorithm compared to the state-of-the-art metaheuristic algorithms. Finally, four real-life optimization problem, namely gear train design problem, gas compressor design problem, cantilever beam design problem and three-bar truss design problem are solved with the help of the newly proposed algorithm, and the results are compared with the obtained results of different popular state-of-the-art optimization techniques and found that the proposed algorithm is more efficient than the compared algorithms.																	1432-7643	1433-7479				APR	2020	24	7					4809	4827		10.1007/s00500-019-04234-6													
J								Dynamic economic dispatch incorporating renewable energy sources and pumped hydroenergy storage	SOFT COMPUTING										Tent equation; Solar-wind-thermal system; Pumped-storage hydraulic unit; Ramp rate limits	DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; POWER-SYSTEM; OPTIMIZATION; WIND; PSO	Due to mounting infiltration of solar and wind energy sources, it becomes essential to investigate its brunt on the dynamic economic dispatch. Here, solar-wind-thermal system integrating pumped-storage hydraulic unit has been considered. This work recommends chaotic fast convergence evolutionary programming (CFCEP) rooted in Tent equation for solving dynamic economic dispatch problem incorporating renewable energy sources and pumped-storage hydraulic unit. Chaotic sequences increase the exploitation ability in the searching space and enhance the convergence property. In the recommended technique, chaotic sequences have been pertained for acquiring the dynamic scaling factor setting in fast convergence evolutionary programming (FCEP). The efficiency of the recommended technique is revealed on two test systems. Simulation outcomes of the suggested technique have been matched up to those acquired by FCEP, differential evolution and particle swarm optimization. It has been observed from the comparison that the recommended CFCEP technique has the capability to confer with better quality solution.																	1432-7643	1433-7479				APR	2020	24	7					4829	4840		10.1007/s00500-019-04237-3													
J								A soft computing model based on asymmetric Gaussian mixtures and Bayesian inference	SOFT COMPUTING										Asymmetric Gaussian mixture; RJMCMC; Intrusion detection; Spam filtering; Image categorization; Dimensionality reduction	GENERALIZED DIRICHLET MIXTURE; FEATURE-SELECTION; UNKNOWN NUMBER; COMPONENTS	A novel unsupervised Bayesian learning framework based on asymmetric Gaussian mixture (AGM) statistical model is proposed since AGM is shown to be more effective compared to the classic Gaussian mixture model. The Bayesian learning framework is developed by adopting sampling-based Markov chainMonte Carlo (MCMC) methodology. More precisely, the fundamental learning algorithm is a hybrid Metropolis-Hastings within Gibbs sampling solution which is integrated within a reversible jump MCMC learning framework, a self-adapted sampling-based implementation, that enables model transfer throughout the mixture parameters learning process and therefore automatically converges to the optimal number of data groups. Furthermore, in order to handle high-dimensional vectors of features, a dimensionality reduction algorithm based on mixtures of distributions is included to tackle the irrelevant and extraneous features. The performance comparison between AGM and other popular models is given, and both synthetic and real datasets extracted from challenging applications such as intrusion detection, spam filtering and image categorization are evaluated to show the merits of the proposed approach.																	1432-7643	1433-7479				APR	2020	24	7					4841	4853		10.1007/s00500-019-04238-2													
J								GuASPSO: a new approach to hold a better exploration-exploitation balance in PSO algorithm	SOFT COMPUTING										Particle swarm optimization; Meta-heuristic algorithms; Swarm-intelligence techniques; Premature convergence; Compromise programming; Self-organizing map (SOM)	PARTICLE SWARM OPTIMIZATION; HYBRID DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; GENETIC ALGORITHMS	This paper presents a new variant of particle swarm optimization (PSO) algorithm named guided adaptive search-based particle swarm optimizer (GuASPSO). In this algorithm, the personal best particles are all divided into a linearly decreasing number of clusters. Then, the unique global best guide of a given particle located at a cluster is obtained as the weighted average calculated over other clusters' best particles. Since the clustered particles are being well-distributed over the whole search space in the clustering process, there would be a moderate distance between each particle and its unique global best guide, contributing the particles neither to be trapped in local optima nor engaged in a drift leading to lose diversity in the search space. In this approach, the number of clusters is high at the early iterations and is gradually decreased by lapse of iterations to less stress the diversity factor and further stress the fitness role to cause the particles to better converge to the optimal point. Holding this balance between global and personal bests' role to attract the particles, on the one hand and between convergence and diversity, on the other hand, can hold a better exploration-exploitation balance in the proposed algorithm. To test the performance of GuASPSO, four popular meta-heuristic algorithms, including genetic algorithm, gravitational search algorithm, gray wolf optimizer, and PSO algorithm as well as 23 standard benchmark functions as the test beds, are employed. The experimental results validated GuASPSO as a robust well-designed algorithm to handle various optimization problems.																	1432-7643	1433-7479				APR	2020	24	7					4855	4875		10.1007/s00500-019-04240-8													
J								Cuckoo search and firefly algorithms in terms of generalized net theory	SOFT COMPUTING										Cuckoo search algorithm; Firefly algorithm; Generalized net; Metaheuristic; Model; Population-based algorithms; Benchmark functions		In the presented paper, the functioning and the results of the work of two metaheuristic algorithms, namely cuckoo search algorithm (CS) and firefly algorithm (FA), are described using the apparatus of generalized nets (GNs), which is an appropriate and efficient tool for describing the essence of various optimization methods. The two developed GN-models mimic the optimization processes based on the nature of cuckoos and fireflies, respectively. The proposed GN-models execute the two considered metaheuristic algorithms conducting basic steps and performing optimal search. Building upon these two GN-models, a universal GN-model is constructed that can be used for describing and simulating both the CS and the FA by setting different characteristic functions of the GN-tokens. Moreover, the universal GN-model itself can be transformed to each of the herewith presented GN-models by applying appropriate hierarchical operators. In order to validate the proposed universal GN-model, numerical experiments are performed for the operating of the universal GN-model (CS and FA) on benchmark mathematical functions. The obtained results are compared with the results of the GNmodel of CS, GN-model of FA, as well as the results of the standard CS and FA.																	1432-7643	1433-7479				APR	2020	24	7					4877	4898		10.1007/s00500-019-04241-7													
J								Multi-criteria decision making approach based on PROMETHEE with probabilistic simplified neutrosophic sets	SOFT COMPUTING										Probabilistic simplified neutrosophic set; Group decision making; PROMETHEE method; Projection measure	CORRELATION-COEFFICIENT; AGGREGATION OPERATORS; SIMILARITY; CRITERIA; ENTROPY; WEIGHT	Probabilistic simplified neutrosophic set (PSNS) is an important tool to describe the vagueness existing in the real life. In this study, we define a PSNS and discuss some of theoretical set operations of PSNSs. Also we propose the concepts of module on PSNSs, as well as inner product and projection operator between two PSNSs. In relation to this new set, we introduce a probabilistic simplified neutrosophic number (PSNS). A PSNN has three components which is called probabilistic-valued truth membership degree, probabilistic-valued indeterminacy membership degree and probabilistic-valued falsity membership degree, respectively. We give some of algebraic operational rules, a score function and an accuracy function on PSNNs. Furthermore, we introduce two aggregation operators called the probabilistic simplified neutrosophic weighted arithmetic average operator and the probabilistic simplified weighted geometric average operator on PSNNs. Furthermore, we determine weights of criteria with a method that is based on fuzzy measure and develop a method based on preference function to determine weight of each decision maker. We present an extended PROMETHEE method based on PSNSs for group decision problems. Finally, as an application of this theory, we give a practice on a multi-criteria group decision making problem based on PSNNs by using extended PROMETHEE method to ensure stability of the proposed method.																	1432-7643	1433-7479				APR	2020	24	7					4899	4915		10.1007/s00500-019-04244-4													
J								A novel cuckoo search algorithm under adaptive parameter control for global numerical optimization	SOFT COMPUTING										Cuckoo search algorithm; Adaptive parameter control; Global numerical optimization; Fractional-order chaotic systems	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; CHAOS; DYNAMICS	Cuckoo search (CS) is a well-known population-based stochastic search technique for solving global numerical optimization problems. At each iteration process, CS searches for new solutions by Levy flights random walk together with a local random walk (LRW). For LRW, mutation proceeds with a uniformly distributed random number in the interval [0, 1] as its mutation factor, which plays an important role in controlling the population diversity and the explorative power of the algorithm. However, this mutation factor generally results in sensitivity to the given optimization problem and thus fails to balance well these two aspects. In view of this consideration, we introduce a simple adaptive parameter control mechanism to LRW, and propose a novel adaptive cuckoo search (CSAPC) algorithm in this paper to improve the optimization performance of CS. The adaptive parameter control mechanism dynamically updates the control parameters based on a Cauchy distribution and the Lehmer mean during the iteration. To verify the performance of CSAPC, simulations and comparisons are conducted on 48 benchmark functions from two well-known test suites. In order to further test its efficacy, CSAPC is applied to solve the problem of parameter estimation of two typical uncertain fractional-order chaotic systems. The numerical, statistical and graphical analysis demonstrates the great competency of CSAPC, and hence can be regarded as an efficient and promising tool for solving the real-world complex optimization problems besides the benchmark problems.																	1432-7643	1433-7479				APR	2020	24	7					4917	4940		10.1007/s00500-019-04245-3													
J								A novel switching function approach for data mining classification problems	SOFT COMPUTING										Classification; Rule induction; Logic minimization; Prime cube; Data mining; Switching function	RULE-INDUCTION; ALGORITHM; SELECTION	Rule induction (RI) is one of the known classification approaches in data mining. RI extracts hidden patterns from instances in terms of rules. This paper proposes a logic-based rule induction (LBRI) classifier based on a switching function approach. LBRI generates binary rules by using a novel minimization function, which depends on simple and powerful bitwise operations. Initially, LBRI generates instance codes by encoding the dataset with standard binary code and then generates prime cubes (PC) for all classes from the instance codes by the proposed reduced offset method. Finally, LBRI selects the most effective PC of the current classes and adds them into the binary rule set that belongs to the current class. Each binary rule represents an If-Then rule for the rule induction classifiers. The proposed LBRI classifier is based on basic logic functions. It is a simple and effective method, and it can be used by intelligent systems to solve real-life classification/ prediction problems in areas such as health care, online/financial banking, image/voice recognition, and bioinformatics. The performance of the proposed algorithm is compared to six rule induction algorithms; decision table, Ripper, C4.5, REPTree, OneR, and ICRM by using nineteen different datasets. The experimental results show that the proposed algorithm yields better classification accuracy than the other rule induction algorithms on ten out of nineteen datasets.																	1432-7643	1433-7479				APR	2020	24	7					4941	4957		10.1007/s00500-019-04246-2													
J								Utilizing data science techniques to analyze skill and demand changes in healthcare occupations: case study on USA and UAE healthcare sector	SOFT COMPUTING										Health care; Jobs and skills analysis; Data science; Factorization; Natural-language processing Automation		New technologies are emerging on a continual basis with drastic trajectories and wider penetration into the job market. Health care is among the top ten sectors in terms of talent turnover rates. Hence, being proactive-by predicting what skills will be in demand, is essential to be prepared for these changes. To predict such transitions, this paper aims to develop a job analysis system with an example from the healthcare field in two countries; United States of America and United Arab Emirates. This empirical research consists of using data science with the help of multiple techniques. To study changes in job demand, we deployed Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) models; while for the skill changes we used techniques Factor Analysis and Non-Negative Matrix Factorization. Using different heatmaps visualizations of the LSI and LDA weights, results provided significant insights into the skill sets and demand changes in both job markets. The study concludes that low-skilled jobs are constantly being replaced by automated systems, while some of the high skill sets are also at risk.																	1432-7643	1433-7479				APR	2020	24	7					4959	4976		10.1007/s00500-019-04247-1													
J								The grey generalized Verhulst model and its application for forecasting Chinese pig price index	SOFT COMPUTING										Grey Verhulst model; Grey generalized Verhulst model; Pig price index; Intelligent algorithm models	CONSUMPTION	The Verhulst model is used in many natural and social systems. When simulating and predicting system sequences with an inverted U shape or a signal peak feature by the grey Verhulst model (abbreviated as GVM), such as the pig price index, a drift phenomenon happens sometimes. We introduce a grey generalized Verhulst model named as GGVM to address this problem. Compared with the GVM, the GGVM contains a constant as the grey action quantity. Besides, the multivariable grey generalized Verhulst model is also given. Three cases containing Glipizide tablets blood drug concentration series, the number of traffic deaths, and consumer price index (abbreviated as CPI) sequences are utilized to demonstrate that GGVM can eliminate the drift phenomenon effectively. Given that the pork is the most consumed meat for Chinese residents and the fluctuation of its price is closely related to the interests of residents and pig-breeding enterprises, it is important to predict the pork price. So the prediction of pork price index whose time series possesses an inverted U shape is carried out by GGVM, GVM, and intelligent algorithm models, including LSSVR, e-SVR, and RBF in the empirical part. The results show that GGVM produces higher accurate simulation and prediction than the models as given above.																	1432-7643	1433-7479				APR	2020	24	7					4977	4990		10.1007/s00500-019-04248-0													
J								A novel method for day-ahead solar power prediction based on hidden Markov model and cosine similarity	SOFT COMPUTING										Solar power prediction; Day-ahead forecasting; Hidden Markov Model; Cosine similarity	NEURAL-NETWORK; HYBRID METHOD; FUZZY MODEL; OUTPUT; FORECAST; TERM; OPTIMIZATION; GENERATION; SYSTEM	Nowadays, with the emergence of new technologies such as smart grid and increasing the use of renewable energy in the grid, energy prediction has become more important in the electricity industry. Furthermore, with growing the integration of power generated from renewable energy sources into grids, an accurate forecasting tool for the reduction in undesirable effects of this scenario is essential. This study has developed a novel approach based on the hidden Markov model (HMM) for forecasting day-ahead solar power. The aim is to find a pattern of solar power changes at a given time in consecutive days. The proposed approach consists of two steps. In the first step, the cosine similarity is used to determine the similarity of solar power variations on consecutive days to a particular vector. In the second step, the obtained information from the first step is fed to HMM as a feature vector. These data are used for training and forecasting day-ahead solar power. After obtaining the preliminary results of the prediction, two known filters are utilized as post-processing to remove spikes and smooth the results. Finally, the performance of the proposed method is tested on real NREL data. No meteorological data (even solar radiation) are used; moreover, the model is fed only from the solar power of the past 23 days. To evaluate the proposed method, a feed-forward neural network and a simple HMM are examined with the same data and conditions. All three methods are tested with and without the post-processing. The results show that the proposed model is superior to other examined methods in terms of accuracy and computational time.																	1432-7643	1433-7479				APR	2020	24	7					4991	5004		10.1007/s00500-019-04249-z													
J								A novel distance-based multiple attribute decision-making with hesitant fuzzy sets	SOFT COMPUTING										Hesitant fuzzy set; Distance measure; Multiple criteria decision-making	SIMILARITY MEASURES; INFORMATION MEASURES; ASSESSMENTS	Up to now, various types of distance measures have been developed and investigated in-depth for hesitant fuzzy sets (HFSs). The analytical study of the existing distance measures for HFSs shows that they have still some limitations. In an attempt to overcome the limitations, this study develops a class of Hausdorff-based distances to measure the distance among HFSs which are not restricted to the same length of their hesitant fuzzy elements (HFEs) and of course the arranging order of values in the HFEs. Furthermore, these HFS distance measures do satisfy all well-known and essential axioms, specially, the triangle inequality property. Eventually, we present some examples to illustrate the efficiency of the new developed HFS distance measures together with a comparative analysis with other existing ones.																	1432-7643	1433-7479				APR	2020	24	7					5005	5017		10.1007/s00500-019-04250-6													
J								Effective collaborative strategies to setup tuners	SOFT COMPUTING										Tuning methods; ParamILS; I-Race; Evoca; Collaborative approach; Parameter search space	AUTOMATIC ALGORITHM CONFIGURATION; DESIGN	Parameter setting problem has demonstrated being a relevant problem related to the use of metaheuristics. ParamILS and I-Race are sophisticated tuning methods that can provide valuable information for designers as well as manage conditional parameters. However, the quality of parameter configurations they can find strongly depends on a proper definition of parameter search space. Evoca is a recently proposed tuner which has demonstrated being less sensitive to the setup of parameters search space. In this paper, we propose an effective collaborative approach that combines Evoca and I-Race as well as Evoca and ParamILS. In both collaborative strategies, Evoca is used to define a proper parameter search space for each tuner. Results demonstrated that the collaborative approaches studied are able to find good parameter configurations reducing the effort required to properly define the parameter search space.																	1432-7643	1433-7479				APR	2020	24	7					5019	5041		10.1007/s00500-019-04252-4													
J								An improved flower pollination algorithm to the urban transit routing problem	SOFT COMPUTING										NP-Hard; Flower pollination algorithm; Urban transit routing problem	NETWORK DESIGN; OPTIMIZATION	The urban transit routing problem is NP-Hard, referring to the design of effective bus routes on the existing road networks. Current studies mainly focus on the models and the application of algorithms, and the improvements in the operation process in the algorithm such as the construction of initial solutions and the transformation methods are not investigated in detail. In order to optimize bus routes, the initial bus route set generation method, the local search, and the global search of the flower pollination algorithm were improved. Taking the average travel time of passengers and the proportion of the number of transfers as the optimization objective, an improved initial population generative method and an improved framework of flower pollination algorithm were applied to obtain a better set of bus routes. Finally, the effectiveness of the improved algorithm was verified based on some experimental results and compared to the previous bus networks such as Mandl's Switzerland network.																	1432-7643	1433-7479				APR	2020	24	7					5043	5052		10.1007/s00500-019-04253-3													
J								Classification in the multiple instance learning framework via spherical separation	SOFT COMPUTING										Classification; Multiple instance learning; Spherical separation; DC functions; Bundle methods	BUNDLE METHOD; DC	We consider a multiple instance learning problem where the objective is the binary classifications of bags of instances, instead of single ones. We adopt spherical separation as a classification tool and come out with an optimization model which is of difference-of-convex type. We tackle the model by resorting to a specialized nonsmooth optimization algorithm, recently proposed in the literature which is based on objective function linearization and bundling. The results obtained by applying the proposed approach to some benchmark test problems are also reported.																	1432-7643	1433-7479				APR	2020	24	7					5071	5077		10.1007/s00500-019-04255-1													
J								An R2 indicator and weight vector-based evolutionary algorithm for multi-objective optimization	SOFT COMPUTING										R2 indicator; Weight vector adaption; Multi-objective optimization; Evolutionary algorithm	DOMINANCE RELATION; DECOMPOSITION; SELECTION	A two-stage R2 indicator-based evolution algorithm (TS-R2EA) was proposed in the recent years. A good balance between convergence and diversity can be achieved, due to the R2 indicator and reference vector-guided selection strategy. However, TSR2-EA is sensitive to problem geometries. In order to address this issue, a weight vector-based selection strategy is introduced, and a weight vector adaptive strategy based on population partition is proposed. In the selection strategy, each candidate solution is ranked according to the scalarizing function values in the corresponding neighbor, and the candidate solutions with good performance can be selected. In the adaptive strategy, the population is partitioned by associating each individual with its closest weight vector, and the weight vectors with a worse performance are adjusted. Similar to TS-R2EA, these strategies are combined with the R2 indicator to solve multi-objective optimization problems. The performance of proposed algorithm has been validated and compared with four related algorithms on a variety of benchmark test problems. The experimental results have demonstrated that the proposed algorithm has high competition and is less sensitive to problem geometries.																	1432-7643	1433-7479				APR	2020	24	7					5079	5100		10.1007/s00500-019-04258-y													
J								The influences of channel subsidy on consumers in a dual-channel supply chain	SOFT COMPUTING										Channel subsidy; Dual-channel supply chain; Consumer surplus; Consumption decision; Pricing decision	PROMOTIONS; CUSTOMERS; DEPTH	This paper examines the influence of subsidies provided by traditional (e.g., a bricks-and-mortar retail mall) and electronic (e.g., Alibaba and Amazon) retail platforms to consumers. We look at the competition between traditional retailers, who sell in a shopping mall, and e-tailers, who sell on the electronic platform. Our focus is on analyzing and demonstrating how these channel subsidies influence (1) pricing behaviors for the e-tailer and the traditional retailer and (2) consumers utilities and channel choices. The main findings are as follows. Whether from the perspective of consumers utilities or from the perspective of consumer channel choice, consumers with different WTP are influenced differently by channel subsidies. While both types of subsidy help the two channels attract some low willingness to pay consumers, the electronic channel subsidy can help the electronic channel attract consumers who are willing to pay more from the traditional channel. The results can help retailers and channel subsidy providers to understand how channel subsidies work, and the differences in the WTP of consumers who purchase products before and after subsidies. This understanding informs retailers' segmentation, targeting and pricing decisions and helps subsidy providers to make channel subsidy- related decisions.																	1432-7643	1433-7479				APR	2020	24	7					5101	5110		10.1007/s00500-019-04260-4													
J								The verification model of multi-focus image fusion by simulating subjective evaluation	SOFT COMPUTING										Multi-focus image; All-focus image; Image fusion; Performance evaluation; SSIM	QUALITY ASSESSMENT; DECOMPOSITION	We present a model to simulate the subjective evaluation and compare various fusion algorithms. First, we produce an allfocus image and two multi-focus images by the help of two filters. Second, we fuse two multi-focus images with two representative algorithms WT and CT. Third, we decide the optimal fusion rules of WT and CT by comparing the fused images with an all-focus image. Finally, we improve the performances of fused images by combining several algorithms. Simulation shows the verification model can compare various fusion algorithms or rules. Meanwhile, our fusion model can get better performances than other algorithms or rules.																	1432-7643	1433-7479				APR	2020	24	7					5111	5118		10.1007/s00500-019-04263-1													
J								Certain indices of graphs under bipolar fuzzy environment with applications	SOFT COMPUTING										Connectivity index of a BFG; Boundedness of connectivity index of a BFG; Average connectivity index of a BFG; Women's football league; Decision making	ARCS	Connectivity index of graph plays a significant role in chemistry, pharmacology, etc. This paper brings in connectivity index of a bipolar fuzzy graph (BFG) with its boundedness. We examine the changes of connectivity index for a BFG when a vertex or an edge is removed. Some theorems related to these are established. The parameter, average connectivity index of a BFG, is introduced with some theorems. Some special types of nodes like bipolar fuzzy connectivity-enhancing node, bipolar fuzzy connectivity-reducing node, bipolar fuzzy connectivity-neutral node with their properties are introduced. Keeping in mind bipolar judgemental thinking, two applications of these thoughts are bestowed to increase the popularity of women football league in India and another is to determine the order of the places to build colleges in a town.																	1432-7643	1433-7479				APR	2020	24	7					5119	5131		10.1007/s00500-019-04265-z													
J								Modelling and simulation of coal gases in a nano-porous medium: a biologically inspired stochastic simulation	SOFT COMPUTING										Coal gases; Unsteady gas equation; Artificial neural network; Firefly algorithm	RADIAL BASIS FUNCTION; DIFFERENTIAL-EQUATIONS; NUMERICAL-SOLUTION; NEURAL-NETWORK; UNSTEADY-FLOW; POROSITY	This paper aims to study the dynamics of the unsteady pressure flow of coal gases caused by the temperature conditions and compressibility in the presence of a nano-porous medium using soft computing technique. To immaculately understand the mechanism, a novelty in the partial differential equation is augmented by considering the fractional-order Caputo derivative, which produces theoretically significant and accurate approximation. Subsequently, the constructed model is experimentally simulated by means of artificial neural network (ANN) and a stochastic process based on a firefly algorithm (FFA). ANN has the ability to approximate and transform the differential equation into an error minimization problem, while FFA efficiently minimizes the error function and optimizes the unknown weights of the constructed network. Furthermore, two error measuring tools; mean absolute error and root mean square error, is also formulated to evaluate the performance index of the designed scheme. Accordingly, the designed scheme is systematically elaborated to assess the pressure sorption of coal gases such as nitrogen (N-2) and carbon dioxide (CO2). The accuracy of the obtained approximation shows the competitiveness of the considered scheme. Notably, the deliberation provides substantial indications about the dynamical behaviour of coal gases, which can be implemented significantly on various dynamical problems.																	1432-7643	1433-7479				APR	2020	24	7					5133	5150		10.1007/s00500-019-04267-x													
J								A structure evolution-based design for stable IIR digital filters using AMECoDEs algorithm	SOFT COMPUTING										IIR digital filters; Structure design; Evolutionary algorithms	DIFFERENTIAL EVOLUTION; SEARCH ALGORITHM; OPTIMIZATION; ENSEMBLE	This paper proposes a subsystem-based structure evolution algorithm for stable IIR digital filter design. The method designs the IIR digital filter through optimizing the filter structure. A filter structure is defined as the connection of the subsystems. A subsystem is a 2-order IIR digital structure with uncertain parameters. Subsystems are randomly connected with the constraints of no feedback branches between subsystems. The subsystem's parameters and the connections between subsystems are optimized by evolutionary algorithms. An adaptive multiple-elites-guided composite differential evolution algorithm (AMECoDEs) is used to optimize this problem. Four classic types of filters, lowpass, highpass, bandpass and bandstop filters are designed. Five state-of-the-art evolutionary algorithms are compared. The simulation results show that AMECoDEs holds the first place on comprehensive performance and convergence rate. At the same time, the poles of filters all reside within the unit circle, which validates the stability of the IIR digital filters.																	1432-7643	1433-7479				APR	2020	24	7					5151	5163		10.1007/s00500-019-04268-w													
J								Partial label learning via low-rank representation and label propagation	SOFT COMPUTING										Partial label learning; Low-rank representation; Label propagation; Sparse constraint; Adaboost.R2	GRAPH	In partial label learning, each training instance is assigned with a set of candidate labels, among which only one is correct. An intuitive strategy to learn from such ambiguous data is disambiguation. Existing methods following such strategy either identify the ground-truth label via treating each candidate label equally or disambiguate the candidate label set via assuming latent variable and optimizing it iteratively. In this paper, we propose a novel two-stage method called partial label learning via low-rank representation and label propagation, where instance similarity and label confidence are taken into consideration to improve the disambiguation ability of the model. In the first stage, we first build the global instance-similarity relationship via low rank representation and sparse constraint and then obtain the accurate instance-label assignments via iterative label propagation strategy. In the second stage, we utilize the Adaboost.R2 to make prediction for unseen instances, where CART is incorporated as the weak classifier. Extensive experiments on the artificial and real-world data sets demonstrate that the proposed method can achieve superior or comparable performance than the comparing state-of-the-art approaches.																	1432-7643	1433-7479				APR	2020	24	7					5165	5176		10.1007/s00500-019-04269-9													
J								Supply chain management under uncertainty with the combination of fuzzy multi-objective planning and real options approaches	SOFT COMPUTING										Supply chains; Uncertainty; Stochastic programming; Fuzzy; Real options	PARALLEL SEMICONTINUOUS PROCESSES; MULTIPRODUCT BATCH PLANTS; INVENTORY MANAGEMENT; PROGRAMMING APPROACH; CAPACITY EXPANSION; GENETIC ALGORITHM; FACILITY LOCATION; OPTIMAL POLICIES; DECISION-MAKING; SERVICE-LEVEL	The concentration of this paper is to measure and break supply chain planning decisions under market and/or technical uncertainty. A three-level supply chain with manufacturers-distributors-customer's loops is considered that customer demands, percent of back products from customers and shipping time of products from distributors to customers are considered as fuzzy variables. The main approach considers suppliers and distributor selection and determines the affected customers in the system problems simultaneously under uncertain conditions. The objects of the proposed model are maximizing the quality of products and income and minimizing the total cost and the shipping time from distributors to customers. Also, in the proposed model we consider some constraints such as lack of orders, production capacity and customer demands. Also, a two-organize, stochastic programming methodology is proposed for joining request vulnerability in multisite midterm store network arranging issues. The assessment of the normal second-stage expenses is accomplished by expository reconciliation yielding an equivalent convex mixed-integer nonlinear problem. At long last, a real options-based valuation (ROV) system for supporting under vulnerability is produced. Multistage stochastic writing computer programs are utilized to fuse vulnerability and a quantitative correlation of the ROV approach, and the customary net-present-value approach is given.																	1432-7643	1433-7479				APR	2020	24	7					5177	5198		10.1007/s00500-019-04271-1													
J								A Niftymean chart method based onmedian ranked set sampling design	SOFT COMPUTING										Skewed bivariate distributions; Big data; Mean charts; Median ranked set sampling		In the case of contamination for skewed distributions, the modified Shewhart, modified weighted variance, and modified skewness correction methods are newly introduced by Karagoz (Hacet J Math Stat 47(1):223-242, 2018). In this study, we propose to modify these methods by considering simple random sampling (SRS), ranked set sampling (RSS) and median ranked set sampling (MRSS) designs under the contaminated type I Marshall-Olkin bivariate Weibull and lognormal distributions. These bivariate distributions are chosen since they can represent a wide variety of shapes from nearly symmetric to highly skewed. We evaluate the performance of proposed modified methods based on different ranked set sampling designs by using Monte Carlo Simulation. The type I risks of (X) over bar charts for existing and newly proposed modified methods by using SRS, RSS and MRSS designs in the case of contamination for these distributions are obtained via simulation study. The proposed modified methods using RSS and MRSS designs for the (X) over bar chart can be a favorable substitute in process monitoring when the distribution is highly skewed and contaminated.																	1432-7643	1433-7479				APR	2020	24	7					5199	5216		10.1007/s00500-019-04272-0													
J								A distance-similaritymethod to solve fuzzy sets and fuzzy soft sets based decision-making problems	SOFT COMPUTING										Fuzzy set; Fuzzy soft set; Distance measure; Similarity measure; Decision-making problem	RISK ANALYSIS	Similarity measure plays an important role in fuzzy environment. Motivating from usual Euclidean distance measure, it introduces a new distance-similarity approach to get a solution of a fuzzy sets and fuzzy soft sets based maximization decision-making problems. Also, three algorithms have been proposed connected to fuzzy sets and fuzzy soft sets. Then using these algorithms, different types of decision-making problems can be solved. To check the efficiency of our approach, we consider an example and solve it by different existing methods, and the results are compared.																	1432-7643	1433-7479				APR	2020	24	7					5217	5229		10.1007/s00500-019-04273-z													
J								Toward recursive spherical harmonics issued bi-filters: Part II: an associated spherical harmonics entropy for optimal modeling	SOFT COMPUTING										Spherical harmonics; Filters; Bi-filters; Recursive methods; Image processing; Entropy	MAXIMUM-ENTROPY; WAVELET ENTROPY; INFORMATION; PRINCIPLE; EQUATIONS	Image processing continues to be a challenging topic in many scientific fields such as medicine, computational physics and informatics especially with the discovery and development of 3D cases. Therefore, development of suitable tools that guarantee a best treatment is a necessity. Spherical shapes are a big class of 3D images whom processing necessitates adoptable tools. This encourages researchers to develop special mathematical bases suitable for 3D spherical shapes. The present work lies in this whole topic with the application of special spherical harmonics bases. In Jallouli et al. (Soft Comput 2018. https://doi.org/10.1007/s00500-018-3596-9), theoretical framework of spherical harmonics filters adapted to image processing has been developed. In the present paper, new approach based on Jallouli et al. (Soft Comput 2018. https://doi.org/ 10.1007/s00500-018-3596-9) is proposed for the reconstruction of images provided with spherical harmonics Shannon-type entropy to evaluate the order/disorder of the reconstructed image. Efficiency and accuracy of the approach are demonstrated by a simulation study on several spherical models.																	1432-7643	1433-7479				APR	2020	24	7					5231	5243		10.1007/s00500-019-04274-y													
J								A greedy screening test strategy to accelerate solving LASSO problems with small regularization parameters	SOFT COMPUTING										LASSO; Screening; Greedy; Dynamic screening	SHRINKAGE-THRESHOLDING ALGORITHM; SELECTION	In the era of big data remarked by high dimensionality and large sample size, the least absolute shrinkage and selection operator (LASSO) problems demand efficient algorithms. Both static and dynamic strategies based on screening test principle have been proposed recently, in order to safely filter out irrelevant atoms from the dictionary. However, such strategies only work well for LASSO problems with large regularization parameters, and lose their efficiency for those with small regularization parameters. This paper presents a novel greedy screening test strategy to accelerate solving LASSO problems with small regularization parameters, as well as its effectiveness through adoption of a relatively larger regularization parameter which filters out irrelevant atoms in every iteration. Further more, the convergence proof of the greedy strategy is given, and the computational complexity of LASSO solvers integrated with this strategy is investigated. Numerical experiments on both synthetic and real data sets support the effectiveness of this greedy strategy, and the results show it outperforms both the static and dynamic strategies for LASSO problems with small regularization parameters.																	1432-7643	1433-7479				APR	2020	24	7					5245	5253		10.1007/s00500-019-04275-x													
J								Forecasting the multifactorial interval grey number sequences using grey relational model and GM (1, N) model based on effective information transformation	SOFT COMPUTING										Grey numbers; Grey system theory; Grey relational analysis; GM (1, N); Traffic congestion; China	GM(1,1); OPTIMIZATION; OUTPUT	In the context of data eruption, the data often show a short-term pattern and change rapidly which makes it difficult to use a single real value to express. For this kind of small-sample and interval data, how to analyze and predict multi-factor sequences efficiently becomes a problem. By this means, grey system theory (GST) is developed in which the interval grey numbers, as a typical object of GST, characterize the range of data and the grey relational and prediction models analyze the relations of multiple grey numbers and forecast the future. However, traditional grey relative relational model has some limitations: the results obtained always show low resolution, and there are no extractions for the interval feature information from the interval grey number sequence. In this paper, the grey relational analysis model (GRA) based on effective information transformation of interval grey numbers is established, which contains comprehensive information of area differences and slope variances and optimizes the resolution of traditional grey degree. Then, according to the relational results, the multivariable GM model (GM (1, N)) is proposed to forecast the interval grey number sequence. To verify the effectiveness of this novel model, it is established to analyze the relationship between the degree of traffic congestion and its relevant factors in the Yangtze River Delta of China and predict the development of urban traffic congestion degrees in this area over the next 5 years. In addition, some traditional statistical methods (principal component analysis, multiple linear regression models and curve regression models) are established for comparisons. The results show high performances of the novel GRA model and GM (1, N) model, which means the models proposed in this paper are suitable for interval grey numbers from regional data. The strengths which recommend the use of this novel method lie in its high recognition mechanism and multi-angle information transformation for interval grey numbers as well as its characteristic of timeliness in information processing.																	1432-7643	1433-7479				APR	2020	24	7					5255	5269		10.1007/s00500-019-04276-w													
J								An information set-based robust text-independent speaker authentication	SOFT COMPUTING										Text-independent speaker recognition; Information set theory; Twofold information set features	IDENTIFICATION; RECOGNITION	This paper presents a method for the extraction of twofold information set (TFIS) features for the text-independent speaker recognition. The method takes the Mel frequency cepstral coefficients from the frames of a sample speech signal and forms a matrix. From this, both spatial and temporal information components are derived based on the information set concept using the entropy framework. The TFIS features comprising their combination of two components are less in number thus reducing the computational time, complexity and improving the performance under the noisy environment. The proposed approach is tested on three datasets namely NIST-2003, VoxForge 2014 speech corpus and VCTK speech corpus in terms of speed, computational complexity, memory requirement and accuracy. Its performance is validated under different noisy environments at different signal-to-noise ratios.																	1432-7643	1433-7479				APR	2020	24	7					5271	5287		10.1007/s00500-019-04277-9													
J								A New Teaching-Learning-based Chicken Swarm Optimization Algorithm	SOFT COMPUTING										Algorithm; Benchmark; Chicken Swarm Optimization; Function; Hybrid; Teaching-Learning-based Optimization	CHEMICAL-REACTION OPTIMIZATION; PERFORMANCE	Chicken Swarm Optimization (CSO) is a novel swarm intelligence-based algorithm known for its good performance on many benchmark functions as well as real-world optimization problems. However, it is observed that CSO sometimes gets trapped in local optima. This work proposes an improved version of the CSO algorithm with modified update equation of the roosters and a novel constraint-handling mechanism. Further, the work also proposes synergy of the improved version of CSO with Teaching-Learning-based Optimization (TLBO) algorithm. The proposed ICSOTLBO algorithm possesses the strengths of both CSO and TLBO. The efficacy of the proposed algorithm is tested on eight basic benchmark functions, fifteen computationally expensive benchmark functions as well as two real-world problems. Further, the performance of ICSOTLBO is also compared with a number of state-of-the-art algorithms. It is observed that the proposed algorithm performs better than or as good as many of the existing algorithms.																	1432-7643	1433-7479				APR	2020	24	7					5313	5331		10.1007/s00500-019-04280-0													
J								A unified method for Pythagorean fuzzy multicriteria group decision-making using entropy measure, linear programming and extended technique for ordering preference by similarity to ideal solution	SOFT COMPUTING										Multicriteria group decision-making; Pythagorean fuzzy sets; Distance measure; Entropy weight model; Linear programming; TOPSIS	AGGREGATION OPERATORS; TODIM APPROACH; MODELS; TOPSIS	This paper presents an innovative method for solving Pythagorean fuzzy (PF) multicriteria group decision-making problems with completely unknown weight information about criteria using entropy weight model, linear programming (LP) and modified technique for ordering preference by similarity to ideal solution (TOPSIS). At first, a new distance measure for PF sets is defined considering their degree of hesitancy and based on weighted Hamming distance and Hausdorff metric. To handle the fuzziness in criteria weights, PF entropy weight model is used to find the initial weights of the criteria in PF format. Following the concept of TOPSIS, an LP model is constructed on the basis of the view point that the chosen alternative should have the smallest distance from the positive ideal solution and the largest distance from the negative ideal solution. Then, the LP model is utilized to find optimal weights of the criteria. Using the newly defined distance measure, entropy weight model and LP model, TOPSIS is extended in PF environments. The existing methods are able to find criteria weights in the form of crisp values only, whereas proposed method is able to obtain those weights in PF format. Thus, the proposed method can overcome the drawback in computing criteria weight for multicriteria group decision-making in PF environments and reduce the information loss significantly. Several numerical examples are considered and solved to validate the superiority of the proposed methodology.																	1432-7643	1433-7479				APR	2020	24	7					5333	5344		10.1007/s00500-019-04282-y													
J								Hybrid wind speed prediction model based on recurrent long short-term memory neural network and support vector machine models	SOFT COMPUTING										LSTM network; SVM model; Particle swarm optimization; Ant lion optimization algorithm; Wind speed; Prediction accuracy	SINGULAR SPECTRUM ANALYSIS; WAVELET PACKET DECOMPOSITION; POWER-GENERATION; MULTISTEP; ALGORITHM; FRAMEWORK; SYSTEM; OPTIMIZATION; COMBINATION; ENSEMBLE	Renewable energy has gained its significance in the recent years due to the increasing power demand and the requirement in various distribution and utilization sectors. To meet the energy demand, renewable energy resources which include wind and solar have attained significant attractiveness and remarkable expansions are carried out all over the world to enhance the power generation using wind and solar energy. This research paper focuses on predicting the wind speed so that it results in forecasting the possible wind power that can be generated from the wind resources which facilitates to meet the growing energy demand. In this work, a recurrent neural network model called as long short-term memory network model and variants of support vector machine models are used to predict the wind speed for the considered locations where the windmill has been installed. Both these models are tuned for the weight parameters and kernel variational parameters using the proposed hybrid particle swarm optimization algorithm and ant lion optimization algorithm. Experimental simulation results attained prove the validity of the proposed work compared with the methods developed in the early literature.																	1432-7643	1433-7479				APR	2020	24	7					5345	5355		10.1007/s00500-019-04292-w													
J								Computational intelligence-based model for diarrhea prediction using Demographic and Health Survey data	SOFT COMPUTING										Public health; Diarrhea disease; Incidence forecast; Prevention and control; Artificial intelligence; Neural network	CLIMATE VARIABILITY; NEURAL-NETWORKS; DRINKING-WATER; DISEASE; IMPACT; DETERMINANTS; ASSOCIATION; SANITATION; KNOWLEDGE; ACCESS	Diarrhea is one of the leading public health problems and the third main cause of death among young children in developing countries. Solutions to tackling the infectious disease require both preventive and control efforts. However, efforts toward improving the control measures require comprehending the factors associated with diarrhea incidence and the ability to accurately forecast the incidence of the disease. Therefore, the present study develops a diarrhea incidence prediction model based on the 2013 Nigeria Demographic and Health Survey data using artificial neural network. The empirical results of the model indicate that, by using only 44 demographic, socioeconomic and environmental variables, diarrhea incidence can be predicted with high accuracy of 95.78 and 95.63% during training and testing phases, respectively. The model is useful for health policymakers in devising proactive intervention measures, including preparing healthcare systems and improving diarrhea prevention and control capabilities. It could also benefit future studies in predicting epidemics that are affected by similar variables.																	1432-7643	1433-7479				APR	2020	24	7					5357	5366		10.1007/s00500-019-04293-9													
J								Constraint programming model for resource-constrained assembly line balancing problem	SOFT COMPUTING										Assembly line balancing; Constraint programming; Resource constraints; Type-2 problem	ALGORITHM	The literature studies assume that resources used to be perform the tasks are certain and homogenous in any assembly line. However, tasks may need to be processed by general resource requirements in real life. These general resources could be classified by usage of resources such as simple or multiple, alternative and concurrent. The problem which is related to assignment of the task to any workstation and assignment of resources needed by the task simultaneously is defined as resource-constrained assembly line balancing problems (RCALBPs). In this study, a multiobjective model with minimization of cycle time and resource usage for a given number of stations is modeled to solve the RCALBP for the first time. Alternative and general resource types for tasks and using more than two resource type requirements are also considered. A constraint programming model is developed and solved to find the optimal solutions of these problems. The proposed models are tested with sample scenarios to show the effectiveness of the model.																	1432-7643	1433-7479				APR	2020	24	7					5367	5375		10.1007/s00500-019-04294-8													
J								Partial belong relation on soft separation axioms and decision-making problem, two birds with one stone	SOFT COMPUTING										Partial belong relation; Total non-belong relation; e-soft T-i-space; Decision-making problem	TOPOLOGICAL-SPACE; SETS; SEMIOPEN	Here, we employ partial belong and total non-belong relations to construct a study consisting of two parts: one of them is related to soft topologies, and the other one is concerned with real-life applications. In the first part, we define a new class of soft separation axioms, namely e-soft T-i-spaces (i = 0, 1, 2, 3, 4). We formulate these spaces with respect to distinct ordinary points using partial belong and total non-belong relations. The merits of using these two relations are that they help to generate a wider class of soft spaces and open up the way for more real-life applications. With the help of examples, we show the relationships between them and investigate the interrelations between them and their parametric topological spaces. Also, we study under what condition the concepts of soft T-i, p-soft Ti and e-soft Ti are equivalent. Furthermore, we scrutinize their behaviours in terms of soft subspaces, soft topological properties and finite soft product spaces. In the second part, we introduce an algorithm using partial belong relations in a decision-making problem in order to bring out the optimal choices.																	1432-7643	1433-7479				APR	2020	24	7					5377	5387		10.1007/s00500-019-04295-7													
J								A low-carbon-orient product design schemes MCDM method hybridizing interval hesitant fuzzy set entropy theory and coupling network analysis	SOFT COMPUTING										Low-carbon design; MCDM; Coupling network analysis; Type-2 fuzzy number; Interval hesitant fuzzy entropy theory	GROUP DECISION-MAKING; CROSS-ENTROPY; OPERATORS; RANKING; VIKOR; ART	The product low-carbon design is the key to decrease carbon emissions in manufacturing. The multiple criteria decision-making (MCDM) method has been widely used in solving the design schemes preference choosing problems. However, the existed MCDM method has two primary problems facing the product low-carbon design cases: (i) How to clarify the coupling relationship between low-carbon decision criteria? (ii) How to fuzzily express the low-carbon-orient product design schemes? To solve these problems, we proposed a novel MCDM method for product low-carbon design. It combines the coupling network analysis with the interval hesitant fuzzy set entropy theory into MCDM process. We used a case study of injection molding machine low-carbon design to verify the proposed MCDM method. It turns out that the proposed MCDM method can help us make more rational and equitable decisions among alternative low-carbon schemes.																	1432-7643	1433-7479				APR	2020	24	7					5389	5408		10.1007/s00500-019-04296-6													
J								Forensic document examination system using boosting and bagging methodologies	SOFT COMPUTING										Document forensics; Printer forensics; KPNF; SURF; ORB; AdaBoost; Bagging	WATERMARKING; CLASSIFICATION; SIZE	Document forgery has increased enormously due to the progression of information technology and image processing software. Critical documents are protected using watermarks or signatures, i.e., active approach. Other documents need passive approach for document forensics. Most of the passive techniques aim to detect and fix the source of the printed document. Other techniques look for the irregularities present in the document. This paper aims to fix the document source printer using passive approach. Hand-crafted features based on key printer noise features (KPNF), speeded up robust features (SURF) and oriented FAST rotated and BRIEF (ORB) are used. Then, feature-based classifiers are implemented using K-NN, decision tree, random forest and majority voting. The document classifier proposed model can efficiently classify the questioned documents to their respective printer class. Further, adaptive boosting and bootstrap aggregating methodologies are used for the improvement in classification accuracy. The proposed model has achieved the best accuracy of 95.1% using a combination of KPNF + ORB + SURF with random forest classifier and adaptive boosting methodology.																	1432-7643	1433-7479				APR	2020	24	7					5409	5426		10.1007/s00500-019-04297-5													
J								Tampering detection using hybrid local and global features in wavelet-transformed space with digital images	SOFT COMPUTING										Altered image; Forensics; Square block; Circular block; Passive method	COPY-MOVE FORGERY; LOCALIZATION; EFFICIENT; REGION	The widespread availability of advanced digital image technology and powerful image editing tools has made it extremely easy to manipulate image content. One popular technique that is challenging for tampering detection methods is the copy-move forgery. Here, one part of the image is copied and is pasted into another part of the same image. Image tampering is very difficult to detect with the naked eye. The authenticity of digital images is critical when they are used as evidence in court, for news reports or insurance claims, as they have the power to influence decisions and outcomes. Hence, this paper presents an efficient method for copy-move forgery detection by means of a HOG descriptor and local binary pattern variance algorithms. The copy-move forgery detection (CMFD) approach is introduced and is applied to the forged region by determining suitable post-processing techniques. The proposed technique is evaluated using MICC-F220, MICC-F2000, UCID, CoMoFoD and CASIA TIDE data sets in which translation, flipping, rotation, scaling, color reduction, brightness change and JPEG compression are applied to an image. The experimental performance of the proposed technique is assessed in terms of the true- and false-detection rates. Ultimately, our proposed method is highly suitable for detecting the altered region, and accurate CMFD results are obtained in forensic image applications.																	1432-7643	1433-7479				APR	2020	24	7					5427	5443		10.1007/s00500-019-04298-4													
J								Two-stage three-machine assembly scheduling problem with sum-of-processing-times-based learning effect	SOFT COMPUTING										Two-stage assembly scheduling; Learning effect; Genetic algorithm; Makespan	TOTAL COMPLETION-TIME; SINGLE-MACHINE; FLOW-SHOP; GENETIC ALGORITHM; BOUND ALGORITHM; OPTIMIZATION ALGORITHM; MAKESPAN; MINIMIZE; HEURISTICS; FABRICATION	Researchers claim that the processing of most products can be formulated as a two-stage assembly scheduling model. The literature states that cumulative learning experience is neglected in solving two-stage assembly scheduling problems. The sum-of-processing-times-based learning effect means that the actual processing time of a job becomes shorter when it is scheduled later, which depends on the sum of processing time of the jobs already processed. Motivated by this observation, we investigate a novel two-stage assembly scheduling with three machines and sum-of-processing-times-based learning effect to minimize the makespan criterion, where two machines operate at the first stage and an assembly machine operates at the second stage. To solve this NP-hard problem, a branch-and-bound method incorporating with ten dominance properties and a lower bound procedure is first derived to obtain an optimal solution. Three heuristics based on Johnson's rule with and without improvement are then applied separately to a genetic algorithm and a cloud theory-based simulated annealing algorithm, which are further modified with an interchange pairwise method for finding near-optimal solutions. Finally, the numerical results obtained using all proposed algorithms are reported and evaluated.																	1432-7643	1433-7479				APR	2020	24	7					5445	5462		10.1007/s00500-019-04301-y													
J								Deep neural network based framework for complex correlations in engineering metrics	ADVANCED ENGINEERING INFORMATICS										Deep neural networks; Machine learning; Regression modelling; Soil shear strength; Index properties of soil	CLAY PARAMETERS; SHEAR-STRENGTH; PREDICTION; ARCHITECTURES; FOUNDATIONS	Linear or polynomial regression and artificial neural networks are often adopted to obtain correlation models between various attributes in engineering fields. Although these are straightforward, they may not perform well for datasets that involve complex correlations among multiple attributes, and overfitting can occur when highorder polynomials are used to match the data from one scenario but fail to produce accurate predictions elsewhere. This paper presents a Deep Neural Networks (DNN) based framework for obtaining complex correlations in engineering metrics and provides guidelines to assess data adequacy, remove outliers and to identify and resolve overfitting problems. Moreover, a clear and concise set of procedures for tuning hyperparameters of DNN is discussed. As an illustration, a DNN model was trained to predict the undrained shear strength of clays based on liquid limit, plastic limit, water content, vertical effective stress, and preconsolidation stress. This analysis is conducted with 1101 samples gathered from different sites all over the world. Prediction of soil strengths often involved significant uncertainties due to the natural variations in earth materials and site conditions, contributing to complex relationships among various material properties. Our results show that the proposed framework performs better than conventional correlation models established from previous studies. The developed framework and accompanying Python script can be readily applied to the prediction of clay properties at other sites, and also to other types of engineering metrics.																	1474-0346	1873-5320				APR	2020	44								101058	10.1016/j.aei.2020.101058													
J								Deep learning-based method for vision-guided robotic grasping of unknown objects	ADVANCED ENGINEERING INFORMATICS										Collaborative robotics; Deep learning; Vision-guided robotic grasping; Industry 4.0		Nowadays, robots are heavily used in factories for different tasks, most of them including grasping and manipulation of generic objects in unstructured scenarios. In order to better mimic a human operator involved in a grasping action, where he/she needs to identify the object and detect an optimal grasp by means of visual information, a widely adopted sensing solution is Artificial Vision. Nonetheless, state-of-art applications need long training and fine-tuning for manually build the object's model that is used at run-time during the normal operations, which reduce the overall operational throughput of the robotic system. To overcome such limits, the paper presents a framework based on Deep Convolutional Neural Networks (DCNN) to predict both single and multiple grasp poses for multiple objects all at once, using a single RGB image as input. Thanks to a novel loss function, our framework is trained in an end-to-end fashion and matches state-of-art accuracy with a substantially smaller architecture, which gives unprecedented real-time performances during experimental tests, and makes the application reliable for working on real robots. The system has been implemented using the ROS framework and tested on a Baxter collaborative robot.																	1474-0346	1873-5320				APR	2020	44								101052	10.1016/j.aei.2020.101052													
J								Predictive maintenance using cox proportional hazard deep learning	ADVANCED ENGINEERING INFORMATICS										Predictive maintenance; Deep learning; Reliability analysis; Fleet maintenance; Machine learning	RELIABILITY-ANALYSIS; REMAINING LIFE; MODEL; PROGNOSTICS; ENGINES; SYSTEM	Predictive maintenance (PdM) has become prevalent in the industry in order to reduce maintenance cost and to achieve sustainable operational management. The core of PdM is to predict the next failure so corresponding maintenance can be scheduled before it happens. The purpose of this study is to establish a Time-Between-Failure (TBF) prediction model through a data-driven approach. For PdM, data sparsity is regarded as a critical issue which can jeopardize algorithm performance for the modelling based on maintenance data. Meanwhile, data censoring has imposed another challenge for handling maintenance data because the censored data is only partially labelled. Furthermore, data sparsity may affect algorithm performance of existing approaches when addressing the data censoring issue. In this study, a new approach called Cox proportional hazard deep learning (CoxPHDL) is proposed to tackle the aforementioned issues of data sparsity and data censoring that are common in the analysis of operational maintenance data. The idea is to offer an integrated solution by taking advantage of deep learning and reliability analysis. To start with, an autoencoder is adopted to convert the nominal data into a robust representation. Secondly, a Cox proportional hazard model (Cox PHM) is researched to estimate the TBF of the censored data. A long-short-term memory (LSTM) network is then established to train the TBF prediction model based on the pre-processed maintenance data. Experimental studies using a sizable real-world fleet maintenance data set provided by a UK fleet company have demonstrated the merits of the proposed approach where the algorithm performance based on the proposed LSTM network has been improved respectively in terms of MCC and RMSE.																	1474-0346	1873-5320				APR	2020	44								101054	10.1016/j.aei.2020.101054													
J								Appearance-driven conversion of polygon soup building models with level of detail control for 3D geospatial applications	ADVANCED ENGINEERING INFORMATICS										3D geospatial; Model conversion; Level of detail; 3D polygon-soup building; Appearance; Two-manifold	RECONSTRUCTION; CITYGML	In many 3D applications, building models in polygon-soup representation are commonly used for the purposes of visualization, for example, in movies and games. Their appearances are fine, however geometry-wise, they may have limited information of connectivity and may have internal intersections between their parts. Therefore, they are not well-suited to be directly used in 3D geospatial applications, which usually require geometric analysis. For an input building model in polygon-soup representation, we propose a novel appearance-driven approach to interactively convert it to a two-manifold model, which is more well-suited for 3D geospatial applications. In addition, the level of detail (LOD) can be controlled interactively during the conversion. Because a model in polygon-soup representation is not well-suited for geometric analysis, the main idea of the proposed method is extracting the visual appearance of the input building model and utilizing it to facilitate the conversion and LODs generation. The silhouettes are extracted and used to identify the features of the building. After this, according to the locations of these features, horizontal cross-sections are generated. We then connect two adjacent horizontal cross-sections to reconstruct the building. We control the LOD by processing the features on the silhouettes and horizontal cross-sections using a 2D approach. We also propose facilitating the conversion and LOD control by integrating a variety of rasterization methods. The results of our experiments demonstrate the effectiveness of our method.																	1474-0346	1873-5320				APR	2020	44								101049	10.1016/j.aei.2020.101049													
J								Automated text classification of near-misses from safety reports: An improved deep learning approach	ADVANCED ENGINEERING INFORMATICS										Deep learning; Near-miss; Safety; Text classification	BIG-DATA; CONSTRUCTION; MANAGEMENT	Examining past near-miss reports can provide us with information that can be used to learn about how we can mitigate and control hazards that materialise on construction sites. Yet, the process of analysing near-miss reports can be a time-consuming and labour-intensive process. However, automatic text classification using machine learning and ontology-based approaches can be used to mine reports of this nature. Such approaches tend to suffer from the problem of weak generalisation, which can adversely affect the classification performance. To address this limitation and improve classification accuracy, we develop an improved deep learning-based approach to automatically classify near-miss information contained within safety reports using Bidirectional Transformers for Language Understanding (BERT). Our proposed approach is designed to pre-train deep bidirectional representations by jointly extracting context features in all layers. We validate the effectiveness and feasibility of our approach using a database of near-miss reports derived from actual construction projects that were used to train and test our model. The results demonstrate that our approach can accurately classify 'near misses', and outperform prevailing state-of-the-art automatic text classification approaches. Understanding the nature of near-misses can provide site managers with the ability to identify work-areas and instances where the likelihood of an accident may occur.																	1474-0346	1873-5320				APR	2020	44								101060	10.1016/j.aei.2020.101060													
J								Automated discovery of scientific concepts: Replicating three recent discoveries in mechanics	ADVANCED ENGINEERING INFORMATICS										Interdisciplinary engineering knowledge genome; Knowledge discovery; Scientific discovery; Truss mechanism duality; Knowledge-based discovery	SYSTEMS; DESIGN	Throughout history, humanity cherished new scientific discoveries; many people devoted their lives to find them in laborious manual processes. Since the emergence of artificial intelligence, about 60 years ago, the possibility of automated scientific discovery has become conceivable and has attracted significant and growing interest. Many studies of automated scientific discovery have been presented over the years. This article presents a new automated discovery approach that crosses disciplines and transfers knowledge between them. The approach requires rich and formal background knowledge to find concepts, methods, or laws not known in one discipline by using their counterpart in other disciplines, specifically disciplinary knowledge that is represented in the Interdisciplinary Engineering Knowledge Genome. Three recent discoveries in mechanics were replicated automatically through software execution, demonstrating the validity of the approach. In future studies, the goal is to discover new knowledge in mechanics and/or electronics, as well as venture into other disciplines including outside engineering.																	1474-0346	1873-5320				APR	2020	44								101080	10.1016/j.aei.2020.101080													
J								Dynamic probabilistic analysis of accidents in construction projects by combining precursor data and expert judgments	ADVANCED ENGINEERING INFORMATICS										Construction safety; Dynamic analysis; Probability model; Incident occurrence frequency; Copula method	LEADING INDICATORS; SAFETY	Construction accident occurrences are essentially rare, stochastic, and dynamic. This study proposes a method for accident prediction that fully captures these natures based on historical data and prior knowledge. The method utilizes the relatively high occurrence frequency of precursor events and the dependency between precursors and accidents. The modeling approach consists of three steps: (1) characterize the stochastic occurrences of precursor events over time based on precursor data; (2) estimate the failure rate of the Poisson model which is assumed to be a prior distribution of accident occurrences; and (3) elicit the expert knowledge about the stochastic dependency between near miss occurrences and accident occurrences. A copula-based Markov model is used to develop the time series model of precursors while a copula-based protocol is proposed to aid expert judgment elicitation and quantification. The probability of accident occurrence is then dynamically updated according to the observed historical near miss numbers. The proposed method is applied to a metro construction project. A five-year long near miss data were collected and used as accident precursor data, while three experts were invited to provide relevant information. The developed accident model is used to predict the accident-prone periods, which are consistent with the months that the observed near miss occurrence frequency deviates significantly from normality. Thus, the model can be used to support the planning of necessary safety improvement programs before the accident risk increased.																	1474-0346	1873-5320				APR	2020	44								101062	10.1016/j.aei.2020.101062													
J								Agent-based management of support systems for distributed brainstorming	ADVANCED ENGINEERING INFORMATICS										Multi-agent system; Resource-oriented architecture; Distributed brainstorming; Flexibility; Usability	INTERNET; ARCHITECTURE; IOT	In this paper, we describe the design and the management of an agent-based system that supports distributed brainstorming activities. The support system is a highly coordinated IoT application composed of many locally installed interface devices, multimedia communication functions, and cloud functions that process application logic and store meeting data. The system is designed to support a variety of brainstorming sessions, so its functionalities must be modifiable and enable the system to be adapted to different environments and user requirements without any loss of performance. System accessibility should be also ensured from any location for any user. These constraints require a flexible and usable support system. We further discuss the aspects of flexibility and usability that are important in a support system for distributed brainstorming, from which we propose a conceptual schema for flexible and usable support systems. To realize this schema, we present a resource-oriented architecture that can modify the brainstorming support system's structure and functions. Flexibility is achieved thanks to an agent-based system that manages resources and operates on them according to users' requests. We also describe the system architecture, which is organized around a set of channels dedicated to different services proposed to the users. We present in detail a video channel that ensures user awareness during synchronized activities. We then conduct several experiments verifying the usability of important channels in the architecture and present the results of these experiments. Finally, we discuss experimental scenarios that show how the system owes its adaptability to management based on an agent organization that supports distributed brainstorming and other activities.																	1474-0346	1873-5320				APR	2020	44								101050	10.1016/j.aei.2020.101050													
J								Constructing an exactly periodic subspace for enhancing SSVEP based BCI	ADVANCED ENGINEERING INFORMATICS										Exactly periodic spatial filter (EPSD); Steady-state visual evoked potential (SSVEP); Electroencephalogram (EEG); Brain-computer interface (BCI)	CANONICAL CORRELATION-ANALYSIS; FREQUENCY RECOGNITION; BRAIN	A novel exactly periodic spatial filtering (EPSD) approach, that provides a robust detection performance, is introduced and evaluated in this study. The proposed method exploits the temporal properties of the steady-state visual evoked potential (SSVEP) response to construct an orthogonal and exactly periodic mapping that enhances the signal to noise ratio (SNR) of the SSVEP embedded in the electroencephalogram (EEG) data. The subspace of interest is constructed via the elimination of the signals spaces that does not constitute the exact period of the target frequency. The EPSD is evaluated on a 35 subject benchmark dataset collected using a 40 target SSVEP BCI system. The results reveal that the proposed EPSD spatial filter significantly enhances the performance of target detection. Further statistical tests also confirm that the EPSD is a potential alternative to the existing SSVEP spatial filters for realizing an efficient BCI system.																	1474-0346	1873-5320				APR	2020	44								101046	10.1016/j.aei.2020.101046													
J								SAFE: An EEG dataset for stable affective feature selection	ADVANCED ENGINEERING INFORMATICS										EEG dataset; Emotion recognition; Affective features; Stable feature selection; Long-term aBCI performance	EMOTION RECOGNITION; TIME-SERIES	An affective brain-computer interface (aBCI) is a direct communication pathway between human brain and computer, via which the computer tries to recognize the affective states of its user and respond accordingly. As aBCI introduces personal affective factors into human-computer interaction, it could potentially enrich the user's experience during the interaction. Successful emotion recognition plays a key role in such a system. The state-of-the-art aBCIs leverage machine learning techniques which consist in acquiring affective electroencephalogram (EEG) signals from the user and calibrating the classifier to the affective patterns of the user. Many studies have reported satisfactory recognition accuracy using this paradigm. However, affective neural patterns are volatile over time even for the same subject. The recognition accuracy cannot be maintained if the usage of aBCI prolongs without recalibration. Existing studies have overlooked the performance evaluation of aBCI during long-term use. In this paper, we propose SAFE-an EEG dataset for stable affective feature selection. The dataset includes multiple recording sessions spanning across several days for each subject. Multiple sessions across different days were recorded so that the long-term recognition performance of aBCI can be evaluated. Based on this dataset, we demonstrate that the recognition accuracy of aBCIs deteriorates when re-calibration is ruled out during long-term usage. Then, we propose a stable feature selection method to choose the most stable affective features, for mitigating the accuracy deterioration to a lesser extent and maximizing the aBCI performance in the long run. We invite other researchers to test the performance of their aBCI algorithms on this dataset, and especially to evaluate the long-term performance of their methods.																	1474-0346	1873-5320				APR	2020	44								101047	10.1016/j.aei.2020.101047													
J								Real-time anomaly detection framework using a support vector regression for the safety monitoring of commercial aircraft	ADVANCED ENGINEERING INFORMATICS										Aviation safety; Real-time monitoring; Anomaly detection; Support vector machine; Feature selection	FAULT-DETECTION; DATA-DRIVEN; DIAGNOSIS; FUSION; SERIES; FILTER; MOTOR; SVM	The development of an automated health monitoring framework is critical for aviation system safety, especially considering the expected increase in air traffic over the next decade. Conventional approaches such as model-based and exceedance methods have a low detection accuracy and are limited to specific applications. This paper proposes a robust real-time health monitoring framework for detecting performance anomalies, which may impact system safety during flight operations, with high accuracy and generalized applicability. The proposed monitoring framework utilizes sensor data from commercial flight data recorders to predict possible flight performance anomalies. Decimation, a signal processing technique, in conjunction with Savitzky-Golay filtering is utilized to preprocess the dataset and mitigate sampling rate and noise issues that prevent direct usage of historical flight data. Correlation-based feature subset selection is subsequently performed, and these features are used to train a support vector machine that predicts flight performance. With this model, performance anomalies in the test data are automatically detected based on deviations from the predicted flight behavior. The proposed monitoring framework was demonstrated to detect performance anomalies in real-time and exhibited accurate detection capabilities with high computational efficiency.																	1474-0346	1873-5320				APR	2020	44								101071	10.1016/j.aei.2020.101071													
J								A novel axle temperature forecasting method based on decomposition, reinforcement learning optimization and neural network	ADVANCED ENGINEERING INFORMATICS										Axle temperature forecasting; Hybrid model; Empirical wavelet transform; Q-learning algorithm; Parameter optimization; Q-BPNN network	EMPIRICAL WAVELET TRANSFORM; FAULT-DIAGNOSIS; WIND; ALGORITHMS; PREDICTION; MODEL	Axle temperature forecasting technology is important for monitoring the status of the train bogie and preventing the hot axle and other dangerous accidents. In order to achieve high-precision forecasting of axle temperature, a hybrid axle temperature time series forecasting model based on decomposition preprocessing method, parameter optimization method, and the Back Propagation (BP) neural network is proposed in this study. The modeling process consists of three phases. In stage I, the empirical wavelet transform (EWT) method is used to preprocess the original axle temperature series by decomposing them into several subseries. In stage II, the Q-learning algorithm is used to optimize the initial weights and thresholds of the BP neural network. In stage III, the Q-BPNN network is used to build the forecasting model and complete predicting all subseries. And the final forecasting results are generated by combining all prediction results of subseries. By comparing all results over three case predictions, it can be concluded that: (a) the proposed Q-learning based parameter optimization method is effective in improving the accuracy of the BP neural network and works better than the traditional population-based optimization methods; (b) the proposed hybrid axle temperature forecasting model can get accurate prediction results in all cases and provides the best accuracy among eight general models.																	1474-0346	1873-5320				APR	2020	44								101089	10.1016/j.aei.2020.101089													
J								Psychophysiological evaluation of seafarers to improve training in maritime virtual simulator	ADVANCED ENGINEERING INFORMATICS										Electroencephalogram; Psychophysiological evaluation; Human factors; Maritime simulator; Assessment; Maritime training		Over the years, safety in maritime industries has been reinforced by many state-of-the-art technologies. However, the accident rate hasn't dropped significantly with the advanced technology onboard. The main cause of this phenomenon is human errors which drive researchers to study human factors in the maritime domain. One of the key factors that contribute to human performance is their mental states such as cognitive workload and stress. In this paper, we propose and implement an Electroencephalogram (EEG)-based psychophysiological evaluation system to be used in maritime virtual simulators for monitoring, training and assessing the seafarers. The system includes an EEG processing part, visualization part, and an evaluation part. By using the processing part of the system, different brain states including cognitive workload and stress can be identified from the raw EEG data recorded during maritime exercises in the simulator. By using the visualization part, the identified brain states, raw EEG signals, and videos recorded during the maritime exercises can be synchronized and displayed together. By using the evaluation part of the system, an indicative recommendation on "pass", "retrain", or "fail" of the seafarers' performance can be obtained based on the EEG-based cognitive workload and stress recognition. Detailed analysis of the demanding events in the maritime tasks is provided by the system for each seafarer that could be used to improve their training. A case study is presented using the proposed system. EEG data from 4 pilots were recorded when they were performing maritime tasks in the simulator. The data are processed and evaluated. The results show that one pilot gets a "pass" recommendation, one pilot gets a "retrain" recommendation, and the other two get "fail" results regarding their performance in the simulator.																	1474-0346	1873-5320				APR	2020	44								101048	10.1016/j.aei.2020.101048													
J								AICF: Attention-based item collaborative filtering	ADVANCED ENGINEERING INFORMATICS										Collaborative filtering; Item-based CF; Recommender systems; Attention mechanisms	MODEL	Item-to-item collaborative filtering (short for ICF) has been widely used in ecommerce websites due to his interpretability and simplicity in real-time personalized recommendation. The focus of ICF is to calculate the similarity between items. With the rapid development of machine learning in recent years, it takes similarity model instead of cosine similarity and Pearson coefficient to calculate the similarity between items in recommendation. However, the existing similarity models can not sufficient to express the preferences of users for different items. In this work, we propose a novel attention-based item collaborative filtering model(AICF) which adopts three different attention mechanisms to estimate the weights of historical items that users have interacted with. Compared with the state-of-the-art recommendation models, the AICF model with simple attention mechanism Self-Attention can better estimate the weight of historical items on non-sparse data sets. Due to depth models can model complex connection between items, our model with the more complex Transformer achieves superior recommendation performance on sparse data. Extensive experiments on ML-1M and Pinterest-20 show that the proposed model greatly outperforms other novel models in recommendation accuracy and provides users with personalized recommendation list more in line with their interests.																	1474-0346	1873-5320				APR	2020	44								101090	10.1016/j.aei.2020.101090													
J								Real-time detection of wildfire risk caused by powerline vegetation faults using advanced machine learning techniques	ADVANCED ENGINEERING INFORMATICS										Wildfire; Ignition process; Machine learning; Powerline vegetation faults; XGBoost	ENERGY USE INTENSITY; BUSHFIRE FATALITIES; IDENTIFICATION; PREDICTION; AUSTRALIA; SELECTION; BEHAVIOR; POLICY	Wildfires, also known as bushfires, happened more and more frequently in the last decades. Especially in countries like Australia, the dry and warm climate there make bushfire become one of the most frequent local hazards. Among different kinds of causes of bushfires, overhead powerline vegetation fault is one of the most common causes that relate to human activities. Reducing the bushfire risk from this perspective has attracted many scholars to study efficient strategies and systems. However, most of them started their research from the angle of powerline faults, while limited literature has explored the characteristics of the vegetations and their ignition features. The objective of this study is to explore and discover the numerical patterns from the contact to the ignition process between different upper story vegetations and the powerlines. Those patterns can not only help provide real-time warnings of bushfire caused by powerline vegetation faults but also avoid false alarm. To achieve this, we collected the voltage and current records of 188 ignition field tests that simulated the powerline vegetation faults. To explore the numerical patterns behind and develop a real-time alarming system, this study proposed a machine learning-based model, namely Hybrid Step XGBoost. According to the tests, the model could identify the safe contacts or the danger contacts between the powerlines and the upper story vegetation with an accuracy of 98.17%. Its performance also surpassed some advanced deep learning networks in our experiments.																	1474-0346	1873-5320				APR	2020	44								101070	10.1016/j.aei.2020.101070													
J								Transfer learning for long-interval consecutive missing values imputation without external features in air pollution time series	ADVANCED ENGINEERING INFORMATICS										Air quality; Deep learning; Long-interval consecutive missing values; Long short-term memory (LSTM); Neural network; Transfer learning	ENERGY USE INTENSITY; SHORT-TERM-MEMORY; NEURAL-NETWORK; QUALITY; CREDITS; CLASSIFICATION; PREDICTION; SELECTION; MODEL	Air pollution has become one of the world's largest health and environmental problems. Studies focusing on air quality prediction, influential factors analysis, and control policy evaluation are increasing. When conducting these studies, valid and high-quality air pollution data are necessarily required to generate reasonable results. Missing data, which is frequently contained in the collected raw data, therefore, has become a significant barrier. Existing methods on missing data either cannot effectively capture the temporal and spatial mechanism of air pollution or focus on sequences with low missing rates and random missing positions. To address this problem, this paper proposes a new imputation methodology, namely transferred long short-term memory-based iterative estimation (TLSTM-IE) to impute consecutive missing values with large missing rates. A case study is conducted in New York City to verify the effectiveness and priority of the proposed methodology. Long-interval consecutive missing PM2.5 concentration data are filled. Experimental results show that the proposed model can effectively learn from long-term dependencies and transfer the learned knowledge. The imputation accuracy of the TLSTM-IE model is 25-50% higher than other commonly seen methods. The novelty of this study lies in two aspects. First is that we target at long-interval consecutive missing data, which has not been addressed before by existing studies in atmospheric research. Second is the novel application of transfer learning on missing values imputation. To our best knowledge, no research on air quality has implemented this technique on this problem before.																	1474-0346	1873-5320				APR	2020	44								101092	10.1016/j.aei.2020.101092													
J								From form features to semantic features in existing MCAD: An ontological approach	ADVANCED ENGINEERING INFORMATICS										Feature; MCAD; Ontological analysis; Semantic representation; Design rational; Functional feature	DESIGN	Today's product development processes rely on Mechanical Computer-Aided Design (MCAD) systems that implement a geometric-centered perspective in design. The development of long discussed feature-based MCAD has not yet led to systems that truly support semantic and functional representation of features, which hampers also the use of these models for functional reasoning. This paper investigates the present feature-based MCAD limitations. It illustrates, through simple examples, how to use ontological analysis and feature re-classification to introduce software extensions in existing MCAD that achieve a newer level of semantic representation of features, and enhance the cognitive understanding of the final model. The proposal also shows how to automatically validate these features from the functional viewpoint.																	1474-0346	1873-5320				APR	2020	44								101088	10.1016/j.aei.2020.101088													
J								Ontological emergence scheme in self-organized and emerging systems	ADVANCED ENGINEERING INFORMATICS										Emerging systems; Self-organized systems; Ontology mining; Web semantic; Ontological emergence; Knowledge discovery	MODEL	This paper presents the concept of "Ontological Emergence", a process that seeks to adapt an ontology to the changes and new components in a self-organized and emergent system, through the application of a set of rules that allows the emergence of a new conceptualization (emerging concepts). The Ontological Emergence provides the structuration of the information and knowledge that could be generated in the system, creating conceptual models that can adequately represent the new behavior that is emerging. It arises from the need to represent ontologically a conceptualization of a reality that is dynamic, which cannot be pre-defined or pre-determined, in order to generate emerging knowledge models that follows the scalability and the evolution of it. In that sense, in this paper is proposed an "Ontological Emergence Scheme" based on a set of processes of registration, monitoring, analysis and adaptation of the various conceptual models that interact in the system, as well as on some processing rules in regard to requirements and information of the context, in order to allow the ontological emergence. In this proposal scheme, the Meta-ontologies guide the ontological emergence process through the definition of general categories, to facilitate the integration of concepts from different ontologies or data sources. Finally, the paper presents some case studies, showing its utility in self-organized and emergent systems.																	1474-0346	1873-5320				APR	2020	44								101045	10.1016/j.aei.2020.101045													
J								A virtual speller system using SSVEP and electrooculogram	ADVANCED ENGINEERING INFORMATICS										Human computer interface (HCI); Steady state visual evoked potential (SSVEP); Electrooculogram (EOG); Virtual speller/keyboard system	BRAIN-COMPUTER-INTERFACE; FREQUENCY STIMULATION METHOD	In this study a novel hybrid speller/keyboard system that combines electro-oculogram (EOG) with steady state visual evoked potential (SSVEP) is designed. Conventional EOG based speller system needs continuous eye movements for selecting a single target and it has a limitation on total number of targets. In this proposed speller, 36 targets are divided into nine groups, which includes alphabets, numbers and special characters. Target selection consists of two stages. Various eye movements (gaze, blinks, winks) are used for selecting the target groups and SSVEP is used to identify the target from the selected group. The cue guided online and free spelling tasks were performed on 10 subjects to validate the proposed system. The average classification accuracy of the proposed system is 94.16% with the information transfer rate (ITR) of 70.99 (+/- 9.95) bits/min. For validating the proposed speller system, the classification accuracy and ITR were compared with conventional speller systems.																	1474-0346	1873-5320				APR	2020	44								101059	10.1016/j.aei.2020.101059													
J								An elastic manifold learning approach to beat-to-beat interval estimation with ballistocardiography signals	ADVANCED ENGINEERING INFORMATICS										Ballistocardiogram; Phase space; Manifold learning; Elastic map	HEART-RATE ESTIMATION; MODEL; SYSTEM; BED	Continuous monitoring of heart rate variation is an important measure to diagnose cardiovascular problems and reduce related morbidity and mortality. The recent advances in wearable sensors have enabled the collection of ballistocardiographic (BCG) records over a long period without sacrificing the user's normal daily life. However, there are multiple interferences that severely impact the BCG sampling process and thus degrade the signal quality. In this paper, we introduce a novel approach to estimating the beat-to-beat intervals by applying an unsupervised manifold learning framework in a hybrid phase space. First, we map the BCG time series into the three-dimensional space within which the desired BCG sample points are expected to form a low-dimensional manifold. This manifold is then reconstructed by its local linear property to remove the high-frequency noise; and overlapping manifold segments are projected to a low-dimensional principal subspace before aligned to mitigate the low-frequency non-stationary center shifts and amplitude variations. After we take the statistics to analyze the period indicators, the heartbeat intervals can be inferred. The proposed approach was tested with the BCG data collected from 10 subjects in different genders, ages, heights, and weights. We compare the estimates with the ground truth ECG references, and the results show that the proposed algorithm is able to provide reliable and accurate estimates for heart rates and beat-to-beat intervals, with the standard deviation of the interval estimate error of 22 ms.																	1474-0346	1873-5320				APR	2020	44								101051	10.1016/j.aei.2020.101051													
J								Daily plan-do-check-act (PDCA) cycles with level of development (LOD) 400 objects for foremen	ADVANCED ENGINEERING INFORMATICS										Building Information Modeling; Level of development 400; Daily bill of materials; Plan-do-check-act cycle; Daily performance metrics; Daily productivity; Field control	MANAGEMENT; SYSTEM; PERFORMANCE	Level of development (LOD) 400 objects have the required information for definitively defining a list of materials and their quantities for a certain scope of construction work. Generating this information each day and operating LOD400 objects through the plan-do-check-act (PDCA) cycle on a daily basis could provide the quantitative basis for proactive and precise day-to-day field control. However, the high frequency of the cycle and the granularity of LOD400 make the process time-consuming and onerous for a foreman. The contribution of this research is an ontology that enables a foreman to go through the PDCA cycle on a daily basis with LOD400 objects and generate six information items (actual date of work completed, products built, location of products built, quantities of materials installed, number of workers that were on site, and performance metric) after the check step C of the PDCA cycle. The ontology was developed into a software prototype for field experiments and the results show that foremen can go through the daily PDCA cycle with LOD400 objects in 2 min and 47 s on average. The average usability score of the prototype was 95, which falls between "excellent" and "best imaginable" on the System Usability Scale (SUS).																	1474-0346	1873-5320				APR	2020	44								101091	10.1016/j.aei.2020.101091													
J								A novel architecture: Using convolutional neural networks for Kansei attributes automatic evaluation and labeling	ADVANCED ENGINEERING INFORMATICS										Product design; Kansei attributes; Convolutional neural network; Evaluation automation	DESIGN; PATTERNS	Kansei evaluation is crucial to the process of Kansei engineering. However, traditional methods are subjective and random. In order to eliminate the differences of individual evaluation criteria in product Kansei attributes evaluation, and further improve the evaluation efficiency, a novel automatic evaluation and labeling architecture for product Kansei attributes was proposed in this paper based on Convolutional Neural Networks (CNNs). The architecture consists of two modules: (1) Target detection module (Faster R-CNN was taken as an example), (2) Fine-Grained classification module (DFL-CNN was taken as an example). A case study was provided to validate the proposed architecture. The proposed architecture transformed design evaluation tasks into the recognition and classification tasks. The experiments achieved 98.837%, 96.899%, 86.047%, and 81.008% accuracy in the binary, triple, and two five-classification tasks, respectively. Our results proved the feasibility of using computer vision to mimic human vision for the automatic evaluation of Kansei attributes.																	1474-0346	1873-5320				APR	2020	44								101055	10.1016/j.aei.2020.101055													
J								A BIM-based simulation framework for fire safety management and investigation of the critical factors affecting human evacuation performance	ADVANCED ENGINEERING INFORMATICS										Fire safety management; Statistical analysis; Building information modeling; Fire dynamics simulator; Agent-based modeling; Human evacuation performance	AGENT-BASED SIMULATION; SYSTEM; MODEL; BEHAVIORS	Fire hazards are a big threat to human life and property safety. The U.S. fire statistics reveal that, in 2017 alone, 1,319,500 fires caused 3400 deaths and 14,670 injuries, which resulted in a loss of $23 billion [1]. Effective evacuation planning in densely occupied buildings should be primarily put in place if both the number of injuries/fatalities and the level of property loss are to be minimized. However, it is not realistic, and is unethical to study human evacuation performance under a burning building. For this reason, computational tools tend to be the best approach for simulating fire growth as well as human response to fire hazards. This study aims to develop a BIM-based simulation framework that implements the Fire Dynamic Simulator (FDS) and agent-based modeling (ABM) for simulating fire growth and evacuation performance for different building layout scenarios. An experimental implementation is conducted to validate the proposed framework, which verified the benefits of (1) using BIM to offer a platform for conducting simulation design and visualizing the simulation results of (a) hazardous fire zones and (b) effective escape routes; (2) simulating fire growth using the FDS tool; (3) developing an agent-based model that accounts for the critical factors affecting evacuation performance; and (4) applying a statistical analysis for investigating the effects of influential parameters from the proposed model. As a result, the simulation outputs can be used to optimize the building design and to investigate the influential factors on human evacuation efficiency. The proposed framework contributes to building fire safety management by enabling to minimize both injuries/fatalities and property loss.																	1474-0346	1873-5320				APR	2020	44								101093	10.1016/j.aei.2020.101093													
J								Prediction of interface yield stress and plastic viscosity of fresh concrete using a hybrid machine learning approach	ADVANCED ENGINEERING INFORMATICS										Interface yield stress; The plastic viscosity; Least Squares Support Vector Machine; Particle Swarm Optimization; Hybrid machine learning	SUPPORT VECTOR MACHINE; COMPRESSIVE STRENGTH; ELASTIC-MODULUS; PIPE-FLOW; BEHAVIOR; SYSTEM; LAYER	The interface yield stress and the plastic viscosity of concrete mixes critically influence their pumpability. This study constructs and verifies a data-driven method for predicting these two important parameters. The proposed method is a hybridization of Least Squares Support Vector Machine (LSSVM) and Particle Swarm Optimization (PSO). The LSSVM is employed to infer the mapping function between the two concrete mix's parameters and their influencing factors. Moreover, in order to overcome the challenging task of fine-tuning the LSSVM model hyper-parameters, the PSO algorithm, a swarm intelligence based metaheuristic, is utilized to optimize the LSSVM prediction model. A data set including 142 experimental tests has been collected in this study to construct and verify the proposed hybrid method. Experimental results supported by the Wilcoxon signed-rank test point out that the hybridization of LSSVM and PSO (with coefficients of determination = 0.71 and 0.77 for interface yield stress and plastic viscosity predictions, respectively) can deliver predictive results superior to those of benchmark models. Hence, the hybrid model of PSO and LSSVM can be a promising alternative to assist engineers in the task of concrete structure construction.																	1474-0346	1873-5320				APR	2020	44								101057	10.1016/j.aei.2020.101057													
J								SNA-based multi-criteria evaluation of multiple construction equipment: A case study of loaders selection	ADVANCED ENGINEERING INFORMATICS										Construction equipment selection; 2DULVs; SNA; AHP; EDAS; Loaders	GROUP DECISION-MAKING; ENVIRONMENTAL EMISSIONS; CONSENSUS MODEL; RISK-ASSESSMENT; SOCIAL NETWORK; FUZZY; OPERATORS; CRITERIA; IDENTIFICATION; OPTIMIZATION	Selecting the proper construction equipment is a challenging task owing to a wide range of available types as well as a host of criteria to be considered during decision making. To deal with this, a heterogeneous group decision-making framework to evaluate multiple purchasing choices of construction equipment is proposed with two data forms, i.e., 2-dimension uncertain linguistic variables (2DULVs) and real numbers. Firstly, a novel way to derive weights of experts by social network analysis (SNA) is applied considering trust degrees among experts in a social trust network. Secondly, after evaluation index system is established, 2DULVs that include both the linguistic evaluations on alternatives and decision makers' appraisals on the given evaluation results are applied to represent subjective fuzzy evaluation information, while real numbers are used to represent quantitative values. Thirdly, 2-dimension uncertain linguistic power generalized weighted aggregation (2DULPGWA) operator is applied to aggregate evaluation values among experts. Fourthly, analytic hierarchy process (AHP) and entropy method are utilized to derive combined weights of sub-criteria before the rank can be obtained by the evaluation based on distance from average solution (EDAS) method. Finally, a case study to evaluate multiple choices of loaders is proposed and a comparative analysis is conducted to prove the effectiveness of the proposed framework. This study provides a meaningful guidance for the optimal selection among various types of construction equipment.																	1474-0346	1873-5320				APR	2020	44								101056	10.1016/j.aei.2020.101056													
J								A decision-support method for information inconsistency resolution in direct modeling of CAD models	ADVANCED ENGINEERING INFORMATICS										Computer-aided design; Direct modeling; Parametric modeling; Information inconsistency; Decision-making	BEAUTIFICATION	Direct modeling is a very recent CAD paradigm that can provide unprecedented modeling flexibility. It, however, lacks the parametric capability, which is indispensable to modern CAD systems. For direct modeling to have this capability, an additional associativity information layer in the form of geometric constraint systems needs to be incorporated into direct modeling. This is no trivial matter due to the possible inconsistencies between the associativity information and geometry information in a model after direct edits. The major issue of resolving such inconsistencies is that there often exist many resolution options. The challenge lies in avoiding invalid resolution options and prioritizing valid ones. This paper presents an effective method to support the user in making decisions among the resolution options. In particular, the method can provide automatic information inconsistency reasoning, avoid invalid resolution options completely, and guide the choice among valid resolution options. Case studies and comparisons have been conducted to demonstrate the effectiveness of the method.																	1474-0346	1873-5320				APR	2020	44								101087	10.1016/j.aei.2020.101087													
J								A production inventory supply chain model with partial backordering and disruption under triangular linguistic dense fuzzy lock set approach	SOFT COMPUTING										Production inventory; Partial backlogging; Disruption; Supply chain; Linguistic triangular dense fuzzy lock set; Optimization	GROUP DECISION-MAKING; ORDER QUANTITY MODEL; IMPERFECT QUALITY; EOQ MODEL; DETERMINISTIC EOQ; EPQ; DEMAND; INFORMATION; PROPAGATION; CONSENSUS	This paper focuses on a three-level distribution process in a supply chain (SC) modeling where the raw materials are received batchwise with imperfect quality. Defective batches are rejected instantly under ``all or none'' policy. Allowing partial backlogging and random disruption in supply, we develop an expected average cost function of the production inventory SC model first. Then, considering the several cost components of the model as linguistic triangular dense fuzzy lock set, the cost function itself has been fuzzified according to the needs of the problem defined at case study. Utilizing the proper application (growth) of key vectors, the objective function has been solved under crisp, general fuzzy, dense fuzzy, dense fuzzy lock of single and double keys environment, respectively. For managerial importance, numerical results and graphical illustrations are made to justify the novelty.																	1432-7643	1433-7479				APR	2020	24	7					5053	5069		10.1007/s00500-019-04254-2													
J								A novel multi-criteria analysis model for the performance evaluation of bank regions: an application to Turkish agricultural banking	SOFT COMPUTING										Banking; Performance evaluation; Simulation; Hesitant fuzzy sets; AHP; GRA	DATA ENVELOPMENT ANALYSIS; LINGUISTIC TERM SETS; DYNAMIC NETWORK DEA; FUZZY MCDM APPROACH; DECISION-MAKING; FINANCIAL PERFORMANCE; MANAGERIAL EFFICIENCY; COMPONENT ANALYSIS; REVERSE LOGISTICS; FIRM PERFORMANCE	The banks serve in a highly dynamic and competitive environment and need to systematically evaluate their performance to improve their competitiveness. Performance evaluation is an important and complex process that requires flexible and analytic methods while handling the multidimensionality of the problem. This study presents a hybrid multi-criteria performance evaluation model for banking sector which combines two multi-criteria decision making methods that are simulation-integrated hesitant fuzzy linguistic term sets-based analytic hierarchy process method to determine the importance level of each criterion according to the decision makers' subjective judgements and grey relational analysis method to rank bank regions according to their performance values. The presented model is based on both probability theory and fuzzy sets theory and thus better represents all the dimensions of the uncertainty inherent in decision making process. A real-life application of the proposed performance evaluation model for a private bank operating in agricultural banking sector in Turkey is also given to illustrate the effectiveness and the applicability of the model.																	1432-7643	1433-7479				APR	2020	24	7					5289	5311		10.1007/s00500-019-04279-7													
J								Center Manifold Analysis of Plateau Phenomena Caused by Degeneration of Three-Layer Perceptron	NEURAL COMPUTATION											DYNAMICS	A hierarchical neural network usually has many singular regions in the parameter space due to the degeneration of hidden units. Here, we focus on a three-layer perceptron, which has one-dimensional singular regions comprising both attractive and repulsive parts. Such a singular region is often called a Milnor-like attractor. It is empirically known that in the vicinity of a Milnor-like attractor, several parameters converge much faster than the rest and that the dynamics can be reduced to smaller-dimensional ones. Here we give a rigorous proof for this phenomenon based on a center manifold theory. As an application, we analyze the reduced dynamics near the Milnor-like attractor and study the stochastic effects of the online learning.																	0899-7667	1530-888X				APR	2020	32	4					683	710		10.1162/neco_a_01268													
J								Neural Model of Coding Stimulus Orientation and Adaptation	NEURAL COMPUTATION											PRIMARY VISUAL-CORTEX; LINE-ORIENTATION; RECEPTIVE-FIELDS; TILTED LINES; SELECTIVITY; INHIBITION; PLASTICITY; CONTRAST; NORMALIZATION; PERCEPTION	The coding of line orientation in the visual system has been investigated extensively. During the prolonged viewing of a stimulus, the perceived orientation continuously changes (normalization effect). Also, the orientation of the adapting stimulus and the background stimuli influence the perceived orientation of the subsequently displayed stimulus: tilt after-effect (TAE) or tilt illusion (TI). The neural mechanisms of these effects are not fully understood. The proposed model includes many local analyzers, each consisting of two sets of neurons. The first set has two independent cardinal detectors (CDs), whose responses depend on stimulus orientation. The second set has many orientation detectors (OD) tuned to different orientations of the stimulus. The ODs sum up the responses of the two CDs with respective weightings and output a preferred orientation depending on the ratio of CD responses. It is suggested that during prolonged viewing, the responses of the CDs decrease: the greater the excitation of the detector, the more rapid the decrease in its response. Thereby, the ratio of CD responses changes during the adaptation, causing the normalization effect and the TAE. The CDs of the different local analyzers laterally inhibit each other and cause the TI. We show that the properties of this model are consistent with both psychophysical and neurophysiological findings related to the properties of orientation perception, and we investigate how these mechanisms can affect the orientation's sensitivity.																	0899-7667	1530-888X				APR	2020	32	4					711	740		10.1162/neco_a_01269													
J								Feature Extraction of Surface Electromyography Based on Improved Small-World Leaky Echo State Network	NEURAL COMPUTATION											SEPARATION; FRAMEWORK; WAVELET; SIGNALS	Surface electromyography (sEMG) is an electrophysiological reflection of skeletal muscle contractile activity that can directly reflect neuromuscular activity. It has been a matter of research to investigate feature extraction methods of sEMG signals. In this letter, we propose a feature extraction method of sEMG signals based on the improved small-world leaky echo state network (ISWLESN). The reservoir of leaky echo state network (LESN) is connected by a random network. First, we improved the reservoir of the echo state network (ESN) by these networks and used edge-added probability to improve these networks. That idea enhances the adaptability of the reservoir, the generalization ability, and the stability of ESN. Then we obtained the output weight of the network through training and used it as features. We recorded the sEMG signals during different activities: falling, walking, sitting, squatting, going upstairs, and going downstairs. Afterward, we extracted corresponding features by ISWLESN and used principal component analysis for dimension reduction. At the end, scatter plot, the class separability index, and the Davies-Bouldin index were used to assess the performance of features. The results showed that the ISWLESN clustering performance was better than those of LESN and ESN. By support vector machine, it was also revealed that the performance of ISWLESN for classifying the activities was better than those of ESN and LESN.																	0899-7667	1530-888X				APR	2020	32	4					741	758		10.1162/neco_a_01270													
J								Online Learning Based on Online DCA and Application to Online Classification	NEURAL COMPUTATION											RELATIVE LOSS BOUNDS; ALGORITHMS; GRADIENT	We investigate an approach based on DC (Difference of Convex functions) programming and DCA (DC Algorithm) for online learning techniques. The prediction problem of an online learner can be formulated as a DC program for which online DCA is applied. We propose the two so-called complete/approximate versions of online DCA scheme and prove their logarithmic/sublinear regrets. Six online DCA-based algorithms are developed for online binary linear classification. Numerical experiments on a variety of benchmark classification data sets show the efficiency of our proposed algorithms in comparison with the state-of-the-art online classification algorithms.																	0899-7667	1530-888X				APR	2020	32	4					759	793		10.1162/neco_a_01266													
J								Optimal Multivariate Tuning with Neuron-Level and Population-Level Energy Constraints	NEURAL COMPUTATION											FISHER INFORMATION	Optimality principles have been useful in explaining many aspects of biological systems. In the context of neural encoding in sensory areas, optimality is naturally formulated in a Bayesian setting as neural tuning which minimizes mean decoding error. Many works optimize Fisher information, which approximates the minimum mean square error (MMSE) of the optimal decoder for long encoding time but may be misleading for short encoding times. We study MMSE-optimal neural encoding of a multivariate stimulus by uniform populations of spiking neurons, under firing rate constraints for each neuron as well as for the entire population. We show that the population-level constraint is essential for the formulation of a well-posed problem having finite optimal tuning widths and optimal tuning aligns with the principal components of the prior distribution. Numerical evaluation of the two-dimensional case shows that encoding only the dimension with higher variance is optimal for short encoding times. We also compare direct MMSE optimization to optimization of several proxies to MMSE: Fisher information, maximum likelihood estimation error, and the Bayesian Cramer-Rao bound. We find that optimization of these measures yields qualitatively misleading results regarding MMSE-optimal tuning and its dependence on encoding time and energy constraints.																	0899-7667	1530-888X				APR	2020	32	4					794	828		10.1162/neco_a_01267													
J								Unsupervised feature selection by self -paced learning regularization	PATTERN RECOGNITION LETTERS										Feature selection; Self-paced learning; Robust statistic	FRAMEWORK	Previous feature selection methods equivalently consider the samples to select important features. However, the samples are often diverse. For example, the outliers should have small or even zero weights while the important samples should have large weights. In this paper, we add a self-paced regularization in the sparse feature selection model to reduce the impact of outliers for conducting feature selection. Specifically, the proposed method automatically selects a sample subset which includes the most important samples to build an initial feature selection model, whose generalization ability is then improved by involving other important samples until a robust and generalized feature selection model has been established or all the samples have been used. Experimental results on eight real datasets show that the proposed method outperforms the comparison methods. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		4	11		10.1016/j.patrec.2018.06.029													
J								Rating prediction via generative convolutional neural networks based regression	PATTERN RECOGNITION LETTERS										Generative convolutional neural network; Rating Prediction		Ratings are an essential criterion for evaluating the quality of movies and a critical indicator of whether a customer would watch a movie. Therefore, an important related research challenge is to predict the rating of a movie before it is released in cinema or even before it is produced. Many existing approaches fail to address this challenge because they predict movie ratings based on post-production factors such as review comments from social media. Consequently, they are generally inapplicable until a movie has been released for a certain period of time when a sufficient number of review comments have become available. In this paper, we propose a regression model based on generative convolutional neural networks for movie rating prediction. Instead of post-production factors widely used by previous work, this model learns from movies' intrinsic pillars such as genres, budget, cast, director and plot information, which are obtainable before the production of movies. In particular, the model explores the correlations between the rating of a movie and its intrinsic attributes to predict its rating. The results can serve as a reference for investors and movie studios to determine an optimal portfolio for movie production and a guidance to the interested users to choose the movie to watch. Extensive experiments on a real dataset are benchmarked against a set of baselines and state of the art approaches. The results demonstrate the effectiveness of our approach. The proposed model is also general to be extended to handle other prediction tasks. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		12	20		10.1016/j.patrec.2018.07.028													
J								Opinion fraud detection via neural autoencoder decision forest	PATTERN RECOGNITION LETTERS										Autoencoder; Neural decision forest; Opinion fraud detection		Online reviews play an important role in influencing buyers' daily purchase decisions. However, fake and meaningless reviews, which cannot reflect users' genuine purchase experience and opinions, widely exist on the Web and pose great challenges for users to make right choices. Therefore, it is desirable to build a fair model that evaluates the quality of products by distinguishing spamming reviews. We present an end-to-end trainable unified model to leverage the appealing properties from Autoencoder and random forest. A stochastic decision tree model is implemented to guide the global parameter learning process. Extensive experiments were conducted on a large Amazon review dataset. The proposed model consistently outperforms a series of compared methods. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		21	29		10.1016/j.patrec.2018.07.013													
J								Supervised feature selection by self -paced learning regression	PATTERN RECOGNITION LETTERS										Feature selection; Self-paced learning; Regression analysis; Supervised learning; Sparse learning		Feature selection is an effective method to reduce the feature dimension and improve the efficiency of data mining algorithms, it can be used to solve the "curse of dimensionality" problem. Most existing feature selection methods do not adequately consider the effects of noise and outliers, thereby reducing the performance of regression. The paper proposed a new feature selection model and supervised learning method, using self-paced regularizer (SP-regularizer) terms and l(2), 1 -norm to constrain the model. Specifically, the feature selection model is trained firstly using the most high-confidence samples base on the self-paced learning theory, and then adds the more high-confidence training samples in the remaining samples to increase the generalization ability of the initial feature selection model until the generalization ability of the feature selection model is not improved or all training samples are used up. Then use l 2, 1 -norm remove redundant feature to improve algorithm efficiency. Experimental results on nine datasets show that our proposed method is superior to the comparison algorithms. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		30	37		10.1016/j.patrec.2018.08.029													
J								Embedded conformal deep low -rank auto -encoder network for matrix recovery	PATTERN RECOGNITION LETTERS										Low-rank matrix; Auto-encoder network; Conformal constraint; Image restoration	SPARSE REPRESENTATION; DIMENSIONALITY	We present a novel embedded conformal deep low-rank auto-encoder (ECLAE) neural network architecture for matrix recovery and it can be utilized for image restoration and clustering. Traditionally, robust principal component analysis based methods attempt to decompose the raw matrix into two components: low-rank part and sparse part. For image data as an example, the primary information of the raw data is gathered in the low-rank component and other information, such as noise, exists in the sparse part. The principal components of a data matrix can be recovered even though a positive fraction of its elements are arbitrarily corrupted. However, these methods neglect the non-linear structure information of the original data. Many recent researches pay more attention to the structure of deep learning to extract the nonlinear relationship of data. The deep auto-encoder as a classical deep structure has performed many splendid results. Hence, we propose ECLAE to integrate the advantage of auto-encoder and low-rank representation. And the conformal local structure of data is perfectly embedded into the novel deep frame. Our key idea includes two folds. The first fold is to adaptively obtain latent layer learning the neighbor structure of data with the conformal constraint. The other is to embed the global information into the network by appending the low-rank constraint over the network outputs. To verify the ability of matrix decompose, our method is used for image restoration. And to evaluate the structure of low-dimensional space, the latent representation is exploited for cluster. Extensive experimental results illustrate the efficiency of the proposed algorithm. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		38	45		10.1016/j.patrec.2018.08.025													
J								Competitive and collaborative representation for classification	PATTERN RECOGNITION LETTERS										Competitive environment; Competitive weight; l(2) -Norm regularization; Collaborative representation	ROBUST FACE RECOGNITION; SPARSE REPRESENTATION; DISCRIMINATIVE DICTIONARY	Deep network recently has achieved promising performance for classification task with massive training samples. The behavior of this model, however, would be diminished obviously when the training set is small. Meanwhile, linear representation based classifiers have widely applied into many fields. These classifiers mostly attempt to take advantage of the correct class to code the test sample through appending l 1 -norm or nuclear norm which highly takes computation. Under these observations, we present a novel competitive and collaborative representation classification (Co-CRC) that employs the properties of training data with l 2 -norm regularization to create a competitive environment which enables the correct class to make more contribution to coding. Additionally, the proposed competitive weight in this paper enhances the competitive relation among all classes and is beneficial for the classifier to find the correct class. Extensive experimental results over popular benchmarks including object, scene and face images database indicate that the proposed algorithm totally based on l 2 -norm regularization takes less computation to obtain rather sparse coding and mostly outperforms several state-of-the-art approaches. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		46	55		10.1016/j.patrec.2018.06.019													
J								Multi -view predictive latent space learning	PATTERN RECOGNITION LETTERS										Multi-view learning; Predictive latent space learning; Unsupervised clustering; Unsupervised dimension reduction	DIMENSIONALITY REDUCTION	In unsupervised circumstances, multi-view learning seeks a shared latent representation by taking the consensus and complementary principles into account. However, most existing multi-view unsupervised learning approaches do not explicitly lay stress on the predictability of the latent space. In this paper, we propose a novel multi-view predictive latent space learning (MVP) model and apply it to multi-view clustering and unsupervised dimension reduction. The latent space is forced to be predictive by maximizing the correlation between the latent space and feature space of each view. By learning a multiview graph with adaptive view-weight learning, MVP effectively combines the complementary information from multi-view data. Experimental results on benchmark datasets show that MVP outperforms the state-of-the-art multi-view clustering and unsupervised dimension reduction algorithms. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		56	61		10.1016/j.patrec.2018.06.022													
J								Movie fill in the blank by joint learning from video and text with adaptive temporal attention	PATTERN RECOGNITION LETTERS										Video question answering; Adaptive temporal attention; Text information fusion		Video understanding is a challenging problem and it attracts a lot of research attention. Lately, a new task called movie fill in the blank (MovieFIB) is proposed. In this task, given a movie clip and a description which has one blank, we need to predict the word in the blank accurately. Previous studies make many contributions to tackling this problem. However, some of them do not utilize the relationship between words and video frames, and some others treat visual information as essential elements for blank word prediction, which fail to distinguish the effects of texts before and after the blank. To overcome the limitations, in this paper we propose to use adaptive temporal attention and fuse text information with attention. We first extract video and word features. Then, adaptive temporal attention is used to update original description. For the updated description, we extract its text information. Attention mechanism is applied to fuse text information. Finally, we use adaptive temporal attention to predict the blank word. Extensive experiments demonstrate that our model achieves satisfactory performance. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		62	68		10.1016/j.patrec.2018.06.030													
J								Self -paced Learning for K -means Clustering Algorithm	PATTERN RECOGNITION LETTERS											FEATURE-SELECTION; SPARSE; REGRESSION; ROBUST	The traditional K-means clustering algorithm is easily affected by the noise, outliers and falling into local optimal solution. This paper proposes a K-means clustering algorithm based on self-paced learning. Firstly, a best training subset is selected to construct the initial cluster model base on self-paced learning theory, and then enhances the generalization ability of the initial clustering model by adding sub-good subsets of samples one by one until the model performance is optimal or all training samples are used up. By analyzing the experimental results, the clustering algorithm proposed in this paper achieves better performance than the compare algorithms on the five real data sets. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		69	75		10.1016/j.patrec.2018.08.028													
J								Spatial-temporal multi -task learning for salient region detection	PATTERN RECOGNITION LETTERS											ATTENTION; MOTION	This paper proposes a novel multi-task learning based salient region detection method by fusing spatial and temporal features. Salient region detection has been widely used in various computer vision tasks, being as a general preprocessor to identify interest objects. Despite the recent successes, existing saliency models still lag behind the performance of human when visually perceives dynamic scenes. Most of the existing models largely rely on various spatial features. However, these spatial feature based methods have several deficiencies: (i) they can hardly adapt to the situation where moving objects are included, and (ii) they cannot model the human vision process in dynamic scenes. Recently, some saliency models introduce temporal features in their detecting process, such as the optical flow and stacking frames. The potential of temporal features for saliency optimization has been demonstrated. However, since temporal features in these models are merely used as a compensation to static features, the advantages of temporal features have not yet been fully explored. Aiming to comprehensively address these issues above, our method fuses spatial and temporal features, and learns the mapping relationship from various features to salient regions using our multi-task learning framework. The final salient region is generated by our unified Bayesian framework. The experimental results demonstrated that our proposed approach outperforms previous methods. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		76	83		10.1016/j.patrec.2018.10.019													
J								Omni -supervised joint detection and pose estimation for wild animals	PATTERN RECOGNITION LETTERS										Semi-supervised; Object detection; Animal		Monitoring wildlife populations and activities have significance for biology and ecology. With the rapid development of computer vision and deep learning techniques, it is possible to employ state-of-the-art convoluntional neural network (CNN) based detectors to process the big data from field surveillance cameras and assist in the following studies. However, data labelling during the training stage is a very timeconsuming, labour intensive and expensive task. In this paper, we detect multiple animals (Kangaroo, emu, dingo, bird and wildcat) in the wild in an Omni-supervised learning setting. The unlabeled data from the surveillance cameras will be filtered and used for training via data distillation approach. Moreover, we also perform joint pose estimation and detection for Kangaroo which has the most samples in the dataset. To study the feasibility, we also built a large full high definition (HD) wild animal surveillance image dataset from collected data from several national parks across the State of Queensland in Australia and this dataset will be made publicly available. Extensive experiments show that the detection and pose estimation results can be further improved by using unlabeled data wisely. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		84	90		10.1016/j.patrec.2018.11.002													
J								Robust rigid registration algorithm based on pointwise correspondence and correntropy	PATTERN RECOGNITION LETTERS										Iterative closest point (ICP); Correntropy; Outliers; Noises; Rigid registration	GRAPH	The iterative closest point (ICP) algorithm is fast and accurate for rigid point set registration, but it works badly when handling noisy data or point clouds with outliers. This paper instead proposes a novel method based on the ICP algorithm to deal with this problem. Firstly, correntropy is introduced into the rigid registration problem which could handle noises and outliers well, and then a new energy function based on maximum correntropy criterion is proposed. After that, a new ICP algorithm based on correntropy is proposed, which performs well in dealing with rigid registration with noises and outliers. This new algorithm converges monotonically from any given parameters, which is similar to the ICP algorithm. Experimental results demonstrate its accuracy and robustness compared with the traditional ICP algorithm. (c) 2018ElsevierB.V. Allrightsreserved.																	0167-8655	1872-7344				APR	2020	132				SI		91	98		10.1016/j.patrec.2018.06.028													
J								Multi -task image set classification via joint representation with class -level sparsity and intra-task low -rankness	PATTERN RECOGNITION LETTERS										Multi-task recognition; Image set classification; Class-level sparsity; Low-rankness	FACE RECOGNITION; NEAREST POINTS; APPEARANCE; MANIFOLD	Image set classification has recently attracted great attention due to its widespread applications in computer vision and pattern recognition. The great challenges lie in effectively and efficiently measuring the similarity among image sets with high inter-class ambiguity and large intra-class variability. In this paper, we propose a joint representation based approach to image set classification, in which class-level sparse and globally low rank constraints are imposed on the representation coefficients to embody inter-set discrimination and intra-set commonality respectively. Furthermore, sometimes the small size of image sets or improper usage of a single kind of features causes useful information limited and lacking in discriminability. To address this problem, we extend the traditional image set classification to a multi-task version, i.e., modify the proposed model to incorporate multiple kinds of features. Fortunately, on the total multi-task representation coefficients, both the total class-level sparsity and the intra-task low-rankness constraints still apply. The proposed method is optimized as a non-smooth convex optimization problem by employing an alternating optimization technique. Experiments on five public datasets demonstrate that the proposed method surpasses existing joint representation models with various regularizations for image set classification and compares favorably with other state-of-the-art methods. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		99	105		10.1016/j.patrec.2018.11.009													
J								A general extensible learning approach for multi -disease recommendations in a telehealth environment	PATTERN RECOGNITION LETTERS										Dual-Tree complex wavelet transformation; Machine learning ensemble; Recommender system; Chronicle heart disease; Time series prediction; Telehealth environment	HEART-DISEASE; ENSEMBLE; EFFICIENT; SYSTEM; RISK; CLASSIFICATION; PREDICTION; DIAGNOSIS; BOOTSTRAP; FAILURE	In a telehealth environment, intelligent technologies are rapidly evolving toward improving the quality of patients' lives and providing better clinical decision-making especially those who suffer from chronic diseases and require continuous monitoring and chronic-related medical measurements. A short-term disease risk prediction is a challenging task but is a great importance for teleheath care systems to provide accurate and reliable recommendations to patients. In this work, a general extensible learning approach for multi-disease recommendations is proposed to provide accurate recommendations for patients with chronic diseases in a telehealth environment. This approach generates appropriate recommendations for patients suffering from chronic diseases such as heart failure and diabetes about the need to take a medical test or not on the coming day based on the analysis of their medical data. The statistical features extracted from the sub-bands obtained after a four-level decomposition of the patient's time series data are classified using a machine learning ensemble model. A combination of three classifiers - Least Squares-Support Vector Machine, Artificial Neural Network, and Naive Bayes - are utilized to construct the bagging-based ensemble model used to produce the final recommendations for patients. Two reallife datasets collected from chronic heart and diabetes disease patients are used for experimentations and evaluation. The experimental results show that the proposed approach yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations as well as reduces the workload for chronic diseases patients who undergo body tests most days. Thus, the proposed approach is considered one of a promising tool for analyzing time series medical data of multi diseases and providing accurate and reliable recommendations to patients suffering from different types of chronic diseases. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		106	114		10.1016/j.patrec.2018.11.006													
J								Multi -task learning using variational auto -encoder for sentiment classification	PATTERN RECOGNITION LETTERS										Sentiment classification; Opinion mining; Deep learning; Multi-task learning; Variational auto-encoder; LSTM Big data	REPRESENTATIONS	With the rapid growth of the big data, many approaches in the representation of text for sentiment classification have been successfully proposed in natural language processing. However, these approaches remedy this problem based on single-task supervised objectives learning and do not consider their relative of multiple tasks. Based on these defects, in this work, we consider these tasks are relative, and use weight-shared parameters for learning the representation of text in neural network model, we introduce and study a multi-task approach with variational auto-encoder generative model (MTVAE) by jointly learning them. Experimental results on six subsets of Amazon review data show that the proposed approach can effectively improve the sentiment classification accuracy by other relative tasks. (c) 2018 Published by Elsevier B.V.																	0167-8655	1872-7344				APR	2020	132				SI		115	122		10.1016/j.patrec.2018.06.027													
J								A new nested ensemble technique for automated diagnosis of breast cancer	PATTERN RECOGNITION LETTERS										Data mining and machine learning; Breast cancer; Nested ensemble technique; BayesNet classifier; Naive Bayes classifier	FEATURE-SELECTION; CLASSIFICATION; HYBRID; CLASSIFIERS; ALGORITHMS; ROBUST; RULES	Nowadays, breast cancer is reported as one of most common cancers amongst women. Early detection of this cancer is an essential to aid in informing subsequent treatments. This study investigates automated breast cancer prediction using machine learning and data mining techniques. We proposed the nested ensemble approach which used the Stacking and Vote (Voting) as the classifiers combination techniques in our ensemble methods for detecting the benign breast tumors from malignant cancers. Each nested ensemble classifier contains "Classifiers" and "MetaClassifiers". MetaClassifiers can have more than two different classification algorithms. In this research, we developed the two-layer nested ensemble classifiers. In our two-layer nested ensemble classifiers the MetaClassifiers have two or three different classification algorithms. We conducted the experiments on Wisconsin Diagnostic Breast Cancer (WDBC) dataset and K-fold Cross Validation technique are used for the model evaluation. We compared the proposed two-layer nested ensemble classifiers with single classifiers (i.e., BayesNet and Naive Bayes) in terms of the classification accuracy, precision, recall, F 1 measure, ROC and computational times of training single and nested ensemble classifiers. We also compared our best model with previous works reported in the literatures in terms of accuracy. The results demonstrate that the proposed two-layer nested ensemble models outperformance the single classifiers and most of the previous works. Both SV-BayesNet-3MetaClassifier and SV-Naive Bayes-3-MetaClassifier achieved accuracy 98.07% (K = 10). However, SV-Naive Bayes-3-MetaClassifier is more efficiency as it needs less time to build the model. (c) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				APR	2020	132				SI		123	131		10.1016/j.patrec.2018.11.004													
J								Leveraging unpaired out -of -domain data for image captioning	PATTERN RECOGNITION LETTERS										Image captioning; Out-of-domain data; Deep learning		Image captioning is a focal point in the realm of computer vision due to its scientific and practical values. Although recent encoder-decoder models have achieved promising performance, they only leverage data from standard datasets, and the performance is limited to the specific datasets. There're still large amounts of data without any additional annotations on the internet which can't be fully utilized. In this paper, we propose a novel approach to using external unpaired images and texts to enhance the performance of image captioning system. Our method can utilize image and text data scraped from the internet respectively to improve the performance limited in concepts-decoder framework. Our approach can transfer the knowledge learned from web data to the standard dataset. We conduct extensive experiments on MS COCO and Flickr30K datasets. The result demonstrates the effectiveness of our method. On both datasets, our metrics scores are significantly improved compared with other methods. (c) 2019ElsevierB.V. Allrightsreserved.																	0167-8655	1872-7344				APR	2020	132				SI		132	140		10.1016/j.patrec.2018.12.018													
J								Ensemble learning via constraint projection and undersampling technique for class-imbalance problem	SOFT COMPUTING										Ensemble learning; Constraint projection; Undersampling technique; Class-imbalance	SMOTE; ALGORITHMS	Ensemble learning is an effective technique for the class-imbalance problem, and the key for obtaining a successful ensemble is to create individual base classifiers with high accuracy and diversity. In this paper, we propose a novel ensemble learning method via constraint projection and undersampling technique, constructing each base classifier through the following two steps: 1) constructing a set of pairwise constraints by undersampling examples from the minority/majority class set and learning a projection matrix from the pairwise constraint set and 2) undersampling the original training set to obtaining a new training set on which a base classifier is constructed in the new feature space defined by the projection matrix. For the first step, the projection matrix is mainly used to enhance the separability between the diverse class examples and thus to improve the performance of the base classifier, and the undersampling technique is used to create diverse sets of pairwise constraints to train diverse projection matrices, thus introducing diversity to base classifiers. For the second step, the undersampling technique aims to improve the performance of base classifiers on the minority class and further increase the diversity between the individual base classifiers. The experimental results showthat the proposed method shows significantly better performance on the measures of recall, g-mean, f-measure and AUC than other state-of-the-art methods for 29 datasets with various data distributions and imbalance ratios.																	1432-7643	1433-7479				APR	2020	24	7					4711	4727		10.1007/s00500-019-04501-6													
J								An application of soft computing for the earth stress analysis in hydropower engineering	SOFT COMPUTING										Soft computing; Earth stress identification; Hydraulic fracturing; Artificial neural network; Genetic algorithm	IN-SITU STRESSES; TARIM BASIN; FIELD; PARAMETERS; MECHANICS; SYSTEM; ROCKS	This paper presents a soft computing of integrating artificial neural networks (ANNs) and genetic algorithms (GAs) to back analyze the earth stress field based on hydraulic fracturing. In this method, the ANN model is employed to map the relationship between the earth stress parameters and hydraulic fracturing behavior instead of numerical computation, and the advantage of this work is that it can conveniently conduct the integration of ANN and optimization algorithm and effectively reduce the workload of numerical computation by using directly the field-measured information to build learning samples. In addition, this can also improve accuracy of earth stress determination from field test data sets for ANN model. The GA is applied to implement multi-objective earth stress parameters optimization on the basis of the objective function. The field monitoring information in a practical project of hydropower engineering is used to verify the proposed soft computing in this study. Investigation results demonstrate that the proposed methodology is capable and valuable in addressing geomechanical parameters determination in hydropower engineering.																	1432-7643	1433-7479				APR	2020	24	7					4739	4749		10.1007/s00500-019-04542-x													
J								Grayscale images colorization with convolutional neural networks	SOFT COMPUTING										Colorization; Computer vision; Convolutional neural network; Self-supervised learning	COLOR; ALGORITHM	Previous approaches to the colorization of grayscale images rely on human manual labor and often produce desaturated results that are not likely to be true colorizations. Inspired by Matias Richart's paper, we proposed an automatic approach based on deep neural networks to color the image in grayscale. We have studied several models, approaches and loss functions to understand the best practices for producing a plausible colorization. By noting that some loss functions work better than others, we used the VGG-16 CNN model based on the classification with the loss of cross-entropy. The experiment shows that our model can produce a plausible colorization.																	1432-7643	1433-7479				APR	2020	24	7					4751	4758		10.1007/s00500-020-04711-3													
J								Lyapunov stability-Dynamic Back Propagation-based comparative study of different types of functional link neural networks for the identification of nonlinear systems	SOFT COMPUTING										Functional link neural network; Nonlinear systems; Dynamic back propagation algorithm; Identification; Lyapunov stability analysis; Adaptive learning rate	MODEL SELECTION; CLASSIFICATION	In this paper, the performance comparison of various types of functional link neural networks (FLNNs) has been done for the nonlinear system identification. The FLNNs being compared in the present study are: trigonometry FLNN, Legendre FLNN (LeFLNN), Chebyshev FLNN, power series FLNN (PSFLNN) and Hermite FLNN. The recursive weights adjustment equations are derived using the combination of Lyapunov stability criterion and dynamic back propagation algorithm. In the simulation study, a total of three nonlinear systems (both static and dynamic systems) are considered for testing and comparing the approximation ability and computational complexity of the above-mentioned FLNNs. From the simulation results, it is observed that the LeFLNN has given better approximation accuracy and PSFLNN offered least computational load as compared to the rest models.																	1432-7643	1433-7479				APR	2020	24	7					5463	5482		10.1007/s00500-019-04496-0													
J								Robustness control in bilinear modeling based on maximum correntropy	JOURNAL OF CHEMOMETRICS										maximum correntropy; multivariate calibration; normal distribution; outliers; robust models	REGRESSION	We present the development of a bilinear regression model for multivariate calibration on the basis of maximum correntropy criteria (MCC) whose robustness can be easily controlled. MCC regression methods can be more effective when the assumption of normality does not hold or when data are contaminated with outliers. These methods are competitive when the degree of robustness against outliers should be controlled. By controlling the robustness, information from candidate outliers can be partially retained rather than completely included or discarded during calibration. Within the context of bilinear regression models, an MCC approach using statistically inspired modification of the partial least squares (SIMPLS) is proposed, which is named maximum correntropy-weighted partial least squares (MCW-PLS). Thanks to the controllable robustness of MCC models, observations are upweighted or downweighted during the calibration process, rendering robust models with soft discrimination of samples. Such a weighting represents an important advantage, especially for cases when samples are not drawn from a normal distribution. Applications to three real case studies are presented. These applications uncovered three main features of MCW-PLS: robustness control between SIMPLS and robust SIMPLS (RSIMPLS), improvements in prediction performance of bilinear calibration models, and the possibility to detect the most informative samples in a calibration set.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3215	10.1002/cem.3215													
J								Mixtures of QSAR models: Learning application domains of pK(a) predictors	JOURNAL OF CHEMOMETRICS										ensemble models; mixture of models; model selection; pKa prediction	QUANTITATIVE STRUCTURE-ACTIVITY; APPLICABILITY DOMAIN; NEURAL-NETWORK; DRUG DESIGN; EXPERTS	Quantitative structure-activity relationship models (QSAR models) predict the physical properties or biological effects based on physicochemical properties or molecular descriptors of chemical structures. Our work focuses on the construction of optimal linear and nonlinear weighted mixes of individual QSAR models to more accurately predict their performance. How the splitting of the application domain by a nonlinear gating network in a "mixture of experts" model structure is suitable for the determination of the optimal domain-specific QSAR model and how the optimal QSAR model for certain chemical groups can be determined is highlighted. The input of the gating network is arbitrarily formed by the various molecular structure descriptors and/or even the prediction of the individual QSAR models. The applicability of the method is demonstrated on the pK(a) values of the OASIS database (1912 chemicals) by the combination of four acidic pK(a) predictions of the OECD QSAR Toolbox. According to the results, the prediction performance was enhanced by more than 15% (root-mean-square error [RMSE] value) compared with the predictions of the best individual QSAR model.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3223	10.1002/cem.3223													
J								Comparison of sensory evaluation techniques for Hungarian wines	JOURNAL OF CHEMOMETRICS										ANOVA; PLS-DA; sensometrics; sensory analysis; wine	POLYPHENOLS; OXYGENATION; ASTRINGENCY; PERCEPTION; COLOR; TASTE	The aim of this study was to compare different Hungarian Kadarka, Kekfrankos, and Cabernet franc wines produced and aged by the same methods and to compare two types of sensory analysis methods as well: the 100-point OIV system and quantitative descriptive analysis (QDA). Both tests were conducted by 12 assessors of the University of Pecs, Institute for Regional Development, Faculty of Horticulture and Oenology. This study provides conclusions about the use of sensory analysis methods, highlighting the advantages and disadvantages of QDA and the OIV system. Principal component analysis, analysis of variance (ANOVA), multiple factor analysis, and partial least squares dicriminant analysis were used for the evaluation of the data. Our results showed that the sensory panel was able to discriminate the samples by both sensory methods; however, the information provided by them was significantly different. ANOVA clearly showed that the two methods have different sensitivity when comparing wines (commercial and produced wine samples) and QDA proved to be the more sensitive, as well as more detailed, method. Partial least squares discriminant analysis augmented the findings in the classification part of the different type of wine samples. In general, OIV is able to show the general quality of the wines, while QDA coupled with proper chemometric methods is able to describe why the given samples received good or bad OIV scores.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3219	10.1002/cem.3219													
J								Modified PCA and PLS: Towards a better classification in Raman spectroscopy-based biological applications	JOURNAL OF CHEMOMETRICS										factor methods; feature extraction; PCA; PLSR; Raman spectroscopy		Raman spectra of biological samples often exhibit variations originating from changes of spectrometers, measurement conditions, and cultivation conditions. Such unwanted variations make a classification extremely challenging, especially if they are more significant compared with the differences between groups to be separated. A classifier is prone to such unwanted variations (ie, intragroup variations) and can fail to learn the patterns that can help separate different groups (ie, intergroup differences). This often leads to a poor generalization performance and a degraded transferability of the trained model. A natural solution is to separate the intragroup variations from the intergroup differences and build the classifier based on merely the latter information, for example, by a well-designed feature extraction. This forms the idea of this contribution. Herein, we modified two commonly applied feature extraction approaches, principal component analysis (PCA) and partial least squares (PLS), in order to extract merely the features representing the intergroup differences. Both of the methods were verified with two Raman spectral datasets measured from bacterial cultures and colon tissues of mice, respectively. In comparison to ordinary PCA and PLS, the modified PCA was able to improve the prediction on the testing data that bears significant difference to the training data, while the modified PLS could help avoid overfitting and lead to a more stable classification.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3202	10.1002/cem.3202													
J								Comparison of supervised learning statistical methods for classifying commercial beers and identifying patterns	JOURNAL OF CHEMOMETRICS										beer; error estimation; fruit beer; learning algorithms; multiple factor analysis (MFA)	CHEMOMETRICS; PERFORMANCE; PREDICTION	In this study, 13 properties (alcohol-, real extract-, flavonoid-, anthocyanin, glucose, fructose, maltose, sucrose content, EBC [European Brewery Convention] and L*a*b* color, bitterness) of 21 beers (alcohol-free pale lagers, alcohol-free beer-based mixed drinks, beer-based mixed drinks, international lagers, wheat beers, stouts, fruit beers) were determined. In the first step, multiple factor analysis (MFA) was performed for the whole data and five clusters (target classes) were determined; then, a bootstrapping was applied to establish a balanced data so as every cluster should contain 100 samples and the total sample size is 500. In the second step, 12 supervised learning algorithms (random trees [RND], Quinlan's C4.5 decision tree algorithm [C4.5], Iterative Dichotomiser 3 algorithm [ID3], cost-sensitive decision tree algorithm [CSMC4], cost-sensitive classification tree [CSCRT], k-nearest neighbors algorithm [KNN], radial basis function [RBF], multilayer perceptron neural network [MLP], prototype nearest neighbor [PNN], linear discriminant analysis [LDA], naive Bayes with continuous variables [NBC], partial least squares discriminant analysis [PLS-DA]) were applied to classify each brand into the target classes. Furthermore, several error rates were calculated: re-substitution error rate (RER), cross-validated error rate (CV), bootsrap error (BOOT), leave-one-out (LOO), and train-test error rate (TRAIN). The MFA could discriminate five groups, which can be characterized by some analytical parameters, and the other multivariate methods performed similarly. The methods can be discriminated best based on the BOOT, CV, and LOO. The best estimation methods are the C4.5, CSMC4, and CSCRT; these performed best along the flavonoid content and EBC color. It identified that the methods most sensitive to the properties are the NBC. The classification ability fluctuated greatly in the case of three properties (glucose, maltose, sucrose). A remarkable fluctuation has been experienced in the case of L*a*b* color parameters, flavonoid content, EBC color, and bitterness by NBC method.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3216	10.1002/cem.3216													
J								Variable importance: Comparison of selectivity ratio and significance multivariate correlation for interpretation of latent-variable regression models	JOURNAL OF CHEMOMETRICS										biomarkers; selectivity ratio; significance multivariate correlation; variable importance; variable selection	PARTIAL LEAST-SQUARES; MASS-SPECTRAL PROFILES; PROJECTION VIP; BIOCHEMOMETRICS; CLASSIFICATION; COMPONENTS	This work examines the performance of significance multivariate correlation (sMC) and selectivity ratio (SR) for ranking variables according to their importance in latent-variable regressions (LVRs) models. Both indices are based on target projection (TP) of a validated LVR model obtained by partial least squares (PLS). The matrix of explanatory x-variables is projected on the normalized regression vector to obtain a score vector that is proportional to the vector of predicted values for the response variable y. sMC for each x-variable is calculated by dividing the squared variance explained by the decomposition obtained from these two vectors on the squared residuals. This is similar to how SR is calculated except that for SR, the regression vector is replaced by the loading matrix obtained by projecting the data matrix of x-variables onto the score matrix obtained by TP. The two indices for variable importance are compared for three different applications with data representing instrumental profiles from liquid chromatography, infrared spectroscopy, and proton nuclear magnetic spectroscopy. Results show that SR outperforms sMC for interpretation and biomarker selection. The main drawback of sMC appears to be the mixing of predictive and orthogonal variation resulting from the direct use of the normalized regression vector in the calculation. SR uses a loading vector that is proportional to the covariances between x-variables and the predicted response variable.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3211	10.1002/cem.3211													
J								A procedure for calibration transfer of DOSY NMR measurements: An example of molecular weight of heparin preparations	JOURNAL OF CHEMOMETRICS										partial least squares (PLS) regression; piecewise direct standardization (PDS); standardization; 2D NMR	INSTRUMENTAL TRANSFER METHODS; 2D NMR; CHEMOMETRIC MODELS; QUANTIFICATION; MIXTURES	Calibration transfer is commonly used for spectra obtained on different spectrometers or other conditions. This paper reports a multivariate transfer approach for 2D diffusion-ordered spectroscopy (DOSY) measurements among high-resolution nuclear magnetic resonance (NMR) spectrometers on the basis of partial least squares (PLS) regression. As the test system previously published quantitative model to predict molecular weight of heparin, low molecular weight heparin (LMWH) was used. For multivariate modeling, piecewise direct standardization (PDS) and direct standardization (DS) were employed. PDS showed the best performance for estimating heparin molecular weight resulting in a significant decrease in root mean square error of prediction (RMSEP) from 647 and 393 Da without standardization to 513 and 135 Da when PDS was applied for heparin and LMWH, respectively. DS is not recommended for high-resolution NMR data. The study showed the success of standardization procedure in cases when multivariate models have to be applied to the 2D NMR spectra recorded on different NMR spectrometers. The proposed approach appears to be a valuable achievement towards application of transfer models also to 2D NMR data.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3210	10.1002/cem.3210													
J								Process capability indices when two sources of variability present, a tolerance interval approach	JOURNAL OF CHEMOMETRICS										process capability indices; process performance; tolerance interval; balanced one-way ANOVA; proportion of non-conforming parts	LIMITS; MODELS	The sound tolerance interval-based method and two P-p-based approximations are compared on the proportion of nonconforming parts. As output distribution of the process, one possible model is examined here: two sources of variations are in a one-way structure. It was found that the uncertainty of variance components estimates plays the major role in the goodness of the three calculation methods. Summary: Statistical indices - like process capability (C-P) or process performance (P-P) index - make the relationship between the width of the specification interval and the extent of the process variability illustrative. The latter is characterized by the tolerance interval, which contains the major part of the population with high confidence. In the original concept this tolerance interval is calculated using oversimplified models. In practice this model is not conform with reality. As improvement two sources of variation is assumed in a one-way structure. The proportion of non-conforming parts of the population is the quantity of interest. Non-conforming means that the characteristic is beyond the specification limits. According to this, the quantile of the distribution shall be determined that is equal to the specification limits. Thus, the task is to calculate the tolerance interval for the N(mu, sigma(2)(A) + sigma(2)(e)) distribution. In practical cases the variance components are unknown and are to be estimated. To estimate the ratio of non-conforming parts, two approximate calculation methods which are coherent with the definition of P-P are investigated, as well. The aim of this work is to compare the results of the two P-P based approximate methods with the tolerance interval based (theoretically sound) calculation method. Two situations from the practice are considered - datasets from process validation and process monitoring environment are evaluated during which the effect of the number of experiments is given. Our study indicated that the main difference of the goodness of the approximations is from the uncertainty of parameter estimates.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3213	10.1002/cem.3213													
J								Ranking and multicriteria decision making in optimization of raspberry convective drying processes	JOURNAL OF CHEMOMETRICS										bioactive compounds; deep ranking analysis by power eigenvectors; freeze-drying; physicomechanical properties; sum of ranking differences	ANTIOXIDANT ACTIVITY; BIOACTIVE COMPOUNDS; DEGRADATION KINETICS; AIRBORNE ULTRASOUND; OSMOTIC DEHYDRATION; RED RASPBERRY; ANTHOCYANIN; COLOR; BLACKBERRY; STRAWBERRY	Serbia is one of the leading producers and exporters of raspberry in the world, and considering the short shelf life of raspberry, the processing, storage, and transport are some of the main issues to be addressed. A comparative experiment was conducted in order to find the suitable process parameters for convective drying that may be considered as the alternatives to freeze-drying, which is a widely used preservation method for raspberry even though it is a costly and energy-consuming method. Twelve convective drying regimens were applied with a combination of three influencing factors: air temperature (60 degrees C, 70 degrees C, and 80 degrees C), air rate (0.5 and 1.5 m.s(-1)), and stage of raspberry (fresh and frozen). The final product, a dried raspberry, was assessed for chemical, physical, and mechanical properties and rehydration capacity. Deep ranking analysis by power eigenvectors (DRAPE) and sum of ranking differences (SRD) were used to uncover the differences and similarities between the applied drying methods. SRD showed that convective drying of fresh raspberries proved to be more similar to freeze-dried raspberries than convective drying of frozen ones. Fresh samples dried at 60 degrees C air temperature and 1.5 m.s(-1) air flow proved to be the most similar to the reference freeze-drying method. This convective regimen gives samples with the lowest color change, shrinkage, and shape deformation. With the mechanical and chemical properties of these samples being observed, statistical Duncan's test show that there is no significant difference (P < .05) in terms of hardness, shear force resistance, total phenolic, and total flavonoid preservation, compared with freeze-dried samples. DRAPE gave similar results, but it added the variable importance in ranking as well, and total phenol reduction was defined as the most important variable. These results can help practitioners to develop cheaper and simpler drying methods that would replace the freeze-drying but keep the same quality of the dried products.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3224	10.1002/cem.3224													
J								Composition of cometary particles collected during two periods of the Rosetta mission: multivariate evaluation of mass spectral data	JOURNAL OF CHEMOMETRICS										comet 67P/Churyumov-Gerasimenko; KNN classification; random forest classification; time-of-flight secondary ion mass spectrometry; variable importance	DUST; COSIMA; SPECTROMETER; COMA	The instrument COSIMA (COmetary Secondary Ion Mass Analyzer) onboard of the European Space Agency mission Rosetta collected and analyzed dust particles in the neighborhood of comet 67P/Churyumov-Gerasimenko. The chemical composition of the particle surfaces was characterized by time-of-flight secondary ion mass spectrometry. A set of 2213 spectra has been selected, and relative abundances for CH-containing positive ions as well as positive elemental ions define a set of multivariate data with nine variables. Evaluation by complementary chemometric techniques shows different compositions of sample groups collected during two periods of the mission. The first period was August to November 2014 (far from the Sun); the second period was January 2015 to February 2016 (nearer to the Sun). The applied data evaluation methods consider the compositional nature of the mass spectral data and comprise robust principal component analysis as well as classification with discriminant partial least squares regression, k-nearest neighbor search, and random forest decision trees. The results indicate a high importance of the relative abundances of the secondary ions C+ and Fe+ for the group separation and demonstrate an enhanced content of carbon-containing substances in samples collected in the period with smaller distances to the Sun.																	0886-9383	1099-128X				APR	2020	34	4			SI				e3218	10.1002/cem.3218													
J								Image Set-Oriented Dual Linear Discriminant Regression Classification and Its Kernel Extension	NEURAL PROCESSING LETTERS										Image set classification; Linear regression; Dimensionality reduction; Discriminative projection	FACE RECOGNITION; REPRESENTATION; ROBUST	Along with the rapid development of computer and image processing technology, it is definitely convenient to obtain various images for subjects, which can be more robust to classification as more feature information is contained. However, how to effectively exploit the rich discriminative information within image sets is the key problem. In this paper, based on the concept of dual linear regression classification method for image set classification, we propose a novel discriminative framework to exploit the superiority of discriminant regression mechanism. We aim to learn a projection matrix to force the represented image points from the same class to be close and those from different class are better separated. The feature extraction strategy in our discriminative framework can appropriately work with the corresponding classification strategy, thus, better classification performance can be achieved. Moreover, we propose a kernel discriminative extension method to address the non-linearity problem by adopting the kernel trick. From the experimental results, our proposed method can obtain competitive recognition rates on face recognition tasks via mapping the original image sets into a more discriminative feature space. Besides, it also shows the effectiveness for object classification task with small image sizes and different number of frames.																	1370-4621	1573-773X				APR	2020	51	2					1061	1079		10.1007/s11063-019-10133-6													
J								Joint Robust Transfer Metric and Adaptive Transfer Function Learning	NEURAL PROCESSING LETTERS										Source domain; Target domain; Marginalized de-noising; Low-rank constraint; Empirical risk; Manifold structure	REGULARIZATION; ADAPTATION; FRAMEWORK	Finding the right distance metric is one of the main challenges in machine learning and computer vision procedures. On the other hand, adapting a good classifier with the learned metric has the same importance. In traditional machine learning problems both training and test data come from a same distribution, however it is not common for many real-world data sets. Therefore, they suffer from poor performance where there is inconsistency and dissimilarity between the training (source) data and the test (target) data domain distributions. In this paper, we present a method to overcome this issue efficiently. To this end, a projection matrix is found by learning a cross-domain metric to map the samples of the source and target datasets to a new feature space where the distance of two domains is reduced by utilizing marginal and conditional adaptation terms. Moreover, the metric becomes powerful by employing marginalized de-noising and low-rank strategies. Also, parallel to learning the projection matrix, an adaptive decision function is learned by minimizing the empirical risk while maximizing the consistency of the manifold structure of data with the classifier. In addition, a distribution adaptation term is incorporated into the learning procedure of the classifier to lessen the distance between two domains by minimizing it. The validity of the proposed technique is tested on different image categorization datasets and the experimental results demonstrate that it performs well compared to the state-of-the-art methods. Graphic																	1370-4621	1573-773X				APR	2020	51	2					1411	1443		10.1007/s11063-019-10152-3													
J								Semi-supervised Fuzzy Min-Max Neural Network for Data Classification	NEURAL PROCESSING LETTERS										Semi-supervised; Fuzzy min-max neural network; Data classification		Learning from the lack of labeled data is a challenging task which often limits the performance of the classifier. Since the unlabeled data is easy to obtain, using both of the labeled and unlabeled data in the training process provide a way to solve this problem. In this paper, a semi-supervised classification method based on fuzzy min-max neural network (SS-FMM) is proposed. In SS-FMM, the network has been modified for handling both of the labeled and unlabeled data. In addition, the staged feedback process is designed to modify the network structure of the traditional fuzzy min-max neural network. A staged-threshold function designed in SS-FMM, the hyperbox pruning process and the hyperbox relabeling process can be started dynamically. Moreover, the hyperboxes relabeling process and the hyperbox pruning process are designed to maximize using the unlabeled data and control the amount of the hyperboxes. In order to testify the effectiveness of SS-FMM, various experiments are carried out with several benchmark data sets. In addition, SS-FMM has been applied on the internal inspection data of our system. The results show that SS-FMMM has got good performance.																	1370-4621	1573-773X				APR	2020	51	2					1445	1464		10.1007/s11063-019-10142-5													
J								Synchronization Control of Quaternion-Valued Neural Networks with Parameter Uncertainties	NEURAL PROCESSING LETTERS										Quaternion-valued; Lexicographical ordering; Uncertain terms; Synchronization; Quasi-synchronization	FIXED-TIME SYNCHRONIZATION; EXPONENTIAL SYNCHRONIZATION; EQUATIONS; DELAYS	In this paper, by starting from basic quaternion algebra properties and algorithms, we develop a comprehensive set of properties to ensure the uncertain quaternion-valued neural networks can receive synchronization and quasi-synchronization goals. By endowing the classic Lyapunov technique, several sufficient criteria for the synchronization and quasi-synchronization analysis of the addressed model are proposed by means of two simple and rigorous control strategies. Particularly, lexicographical ordering approach is proposed in this paper, which can be employed to determine the "magnitude" of two different quaternion-valued. Finally, we have numerical evidences that the mathematical model and the conclusions presented are validate.																	1370-4621	1573-773X				APR	2020	51	2					1465	1484		10.1007/s11063-019-10153-2													
J								Finite-Time Mittag-Leffler Stability of Fractional-Order Quaternion-Valued Memristive Neural Networks with Impulses	NEURAL PROCESSING LETTERS										Quaternion-valued; Memristor; Fractional-order neural networks; Finite-time stability	GLOBAL ASYMPTOTICAL PERIODICITY; O(T(-ALPHA)) STABILITY; LEAKAGE DELAY; SYNCHRONIZATION; DISCRETE	The finite-time Mittag-Leffler stability for fractional-order quaternion-valued memristive neural networks (FQMNNs) with impulsive effect is studied here. A new mathematical expression of the quaternion-value memductance (memristance) is proposed according to the feature of the quaternion-valued memristive and a new class of FQMNNs is designed. In quaternion field, by using the framework of Filippov solutions as well as differential inclusion theoretical analysis, suitable Lyapunov-functional and some fractional inequality techniques, the existence of unique equilibrium point and Mittag-Leffler stability in finite time analysis for considered impulsive FQMNNs have been established with the order 0<beta<1 Then, for the fractional order beta satisfying 1<beta<2$$1 and by ignoring the impulsive effects, a new sufficient criterion are given to ensure the finite time stability of considered new FQMNNs system by the employment of Laplace transform, Mittag-Leffler function and generalized Gronwall inequality. Furthermore, the asymptotic stability of such system with order 1<beta<2 $$1 have been investigated. Ultimately, the accuracy and validity of obtained finite time stability criteria are supported by two numerical examples.																	1370-4621	1573-773X				APR	2020	51	2					1485	1526		10.1007/s11063-019-10154-1													
J								Maintenance Personnel Detection and Analysis Using Mask-RCNN Optimization on Power Grid Monitoring Video	NEURAL PROCESSING LETTERS										Mask-RCNN; Power grid monitoring; TLD tracking algorithm		In recent years, deep learning theory and applications have been grown rapidly. Its application aspects has been widely extended to medical care, unmanned driving, intelligent monitoring and other fields. In this paper, we focus on detecting and analyzing the movements of maintenance personnel based on power grid surveillance videos by using MASK-RCNN. Firstly, we detect the maintenance personnel in the video data using optimized MASK-RCNN network. Then, we plot the corresponding personnel path image using segmentation and centroid detection, which can accurately count the personnel trajectory with in-and-out information. Secondly, this paper introduce a tracking-learning-detection algorithm to further track and analyze interested feature events of power grid video. The experimental results show that our algorithm can accurately detect multiple personnel and obtain the key features of the video contents.																	1370-4621	1573-773X				APR	2020	51	2					1599	1610		10.1007/s11063-019-10159-w													
J								A Design Strategy for the Efficient Implementation of Random Basis Neural Networks on Resource-Constrained Devices	NEURAL PROCESSING LETTERS										Random basis networks; Threshold function; Digital architectures; FPGA; CPLD	COMPUTATIONAL INTELLIGENCE; CLASSIFICATION; PROCESSOR; SYSTEM	The deployment of connectionist models on resource-constrained, low-power embedded systems brings about specific implementation issues. The paper presents a design strategy, aimed at low-end reconfigurable devices, for implementing the prediction operation supported by a single hidden-layer feedforward neural network (SLFN). The paper first shows that a considerable efficiency can be obtained when hard-limiter thresholding operators support the activation functions of the neurons. Secondly, the analysis highlights the advantages of using random basis networks, thanks to their limited memory requirements. Finally, the paper presents a pair of different architectural approaches to the effective support of SLFNs on CPLDs and low-end FPGAs. The alternatives differ in the specific trade-off strategy between area utilization and latency. Experiments confirm the effectiveness of both schemes, yielding a pair of viable implementation options for satisfying the respective constraints, namely effective area utilization or low latency.																	1370-4621	1573-773X				APR	2020	51	2					1611	1629		10.1007/s11063-019-10165-y													
J								Quantitative Analysis in Delayed Fractional-Order Neural Networks	NEURAL PROCESSING LETTERS										Self-connection delay; Stability; Hopf bifurcation; Fractional order; Neural networks	BIFURCATION-ANALYSIS; STABILITY	This paper mainly investigates the influence of self-connection delay on bifurcation in a fractional neural network. The bifurcation criteria for the proposed systems with self-connection delay or without self-connection delay is figured out using time delay as a bifurcation parameter, respectively. The effects of self-connection delay on bifurcation in a fractional neural network are ascertained in this paper. Comparative analysis indicates that the stability performance of the proposed fractional neural networks is overly undermined by self-connection delay, which cannot be disregarded. In addition, the impact of fractional order on the bifurcation point is revealed. To highlight the proposed original results, two numerical examples are finally presented.																	1370-4621	1573-773X				APR	2020	51	2					1631	1651		10.1007/s11063-019-10161-2													
J								Growing Self-Organizing Maps for Nonlinear Time-Varying Function Approximation	NEURAL PROCESSING LETTERS										Function approximation; Self-organizing map; Incremental learning; Locally weighted learning; Nonlinear time-varying Function	NEURAL-NETWORKS; IDENTIFICATION; PREDICTION; WIND	Function approximation may be described as the task of modeling the input-output relation and therefore yielding an estimation of the real output function value. In many domains, an ideal learning algorithm needs to approximate nonlinear time-varying functions from a high-dimensional input space and avoid problems from irrelevant or redundant input data. Therefore, the method has to meet three requirements, namely, it must: allow incremental learning to deal with changing functions and changing input distributions; keep the computational cost low; and achieve accurate estimations. In this paper, we explore different approaches to perform function approximation based on the Local Adaptive Receptive Fields Self-Organizing Map (LARFSOM). Local models are built by calculating between the output associated with the winning node and the difference vector between the input vector and the weight vector. These models are combined by using a weighted sum to yield the final approximate value. The topology is adapted in a self-organizing way, and the weight vectors are adjusted in a modified unsupervised learning algorithm for supervised problems. Experiments were carried out on synthetic and real-world datasets. Experimental results indicate that the proposed approaches perform competitively against Support Vector Regression (SVR) and can improve function approximation accuracy and computational cost against the locally weighted interpolation (LWI), a state-of-the-art interpolating algorithm for self-organizing maps.																	1370-4621	1573-773X				APR	2020	51	2					1689	1714		10.1007/s11063-019-10168-9													
J								General and Improved Five-Step Discrete-Time Zeroing Neural Dynamics Solving Linear Time-Varying Matrix Equation with Unknown Transpose	NEURAL PROCESSING LETTERS										Zeroing neural dynamics; General discretization formula; Linear time-varying matrix equation; Unknown transpose; Truncation error	SYLVESTER EQUATION; NETWORKS; DESIGN; STABILITY; SYSTEMS	In this paper, a general five-step discrete-time zeroing neural dynamics (DTZND) model is proposed to solve linear time-varying matrix equation with unknown transpose. Specifically, the explicit continuous-time zeroing neural dynamics (CTZND) model is derived from the time-varying matrix equation with unknown transpose via Kronecker product and vectorization technique. Furthermore, a general five-step discretization formula is designed to approximate the first-order derivative of the target point, and the convergence condition is given. Thus, the general five-step DTZND model is obtained by using the general five-step discretization formula to discretize the CTZND model. Theoretical analyses present the stability and convergence of the proposed general five-step DTZND model. Numerical experiment results substantiate that the proposed DTZND model for solving linear time-varying matrix equation is stable and convergent with the theoretically analyzed errors. In addition, the improved DTZND models are provided in terms of accuracy and computational complexity, and verified by numerical experiments.																	1370-4621	1573-773X				APR	2020	51	2					1715	1730		10.1007/s11063-019-10181-y													
J								Community Detection in Complex Networks Using Nonnegative Matrix Factorization and Density-Based Clustering Algorithm	NEURAL PROCESSING LETTERS										Community detection; Nonnegative matrix factorization; Density peak clustering; NNDSVD	FAST SEARCH; PEAKS; TREE; FIND	Community detection is a critical issue in the field of complex networks. Capable of extracting inherent patterns and structures in high dimensional data, the non-negative matrix factorization (NMF) method has become one of the hottest research topics in community detection recently. However, this method has a significant drawback; most community detection methods using NMF require the number of communities to be preassigned or determined by searching for the best community structure among all candidates. To address the problem, in this paper, we use an improved density peak clustering to obtain the number of cores as the pre-defined parameter of nonnegative matrix factorization. Then we adopt nonnegative double singular value decomposition initialization which can rapidly reduce the approximation error of nonnegative matrix factorization. Finally, we compare and analyze the performance of different algorithms on artificial networks and real-world networks. Experimental results indicate that the proposed method is superior to the state-of-the-art methods.																	1370-4621	1573-773X				APR	2020	51	2					1731	1748		10.1007/s11063-019-10170-1													
J								Sp Almost Periodic Solutions of Clifford-Valued Fuzzy Cellular Neural Networks with Time-Varying Delays	NEURAL PROCESSING LETTERS										Clifford-valued fuzzy cellular neural networks; <mml; math><mml; msup><mml; mi>S</mml; mi><mml; mi>p</mml; mi></mml; msup></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$S<^>{p}$$; end{document}<inline-graphic xlink; href="11063_2019_10176_Article_IEq6; gif"; >-almost periodic solution; Global exponential stability	GLOBAL EXPONENTIAL STABILITY; FUNCTIONAL-DIFFERENTIAL EQUATIONS; NICHOLSONS BLOWFLIES MODEL; LIMIT-CYCLES; EXISTENCE; CONVERGENCE; SYNCHRONIZATION; STEPANOV	In this paper, we consider Clifford-valued fuzzy cellular neural networks with time-varying delays. In order to avoid the inconvenience caused by the non-commutativity of the multiplication of Clifford numbers, we first decompose the considered n-dimensional Clifford-valued systems into 2mn-dimensional real-valued systems. Then by using the Banach fixed point theorem and a proof by contradiction, we establish sufficient conditions ensuring the existence, the uniqueness and the global exponential stability of Sp-almost periodic solutions for the considered neural networks. Finally, we give an example to illustrate the effectiveness of the obtained results. Our results are new even when the considered neural networks degenerates to real-valued, complex-valued and quaternion-valued neural networks.																	1370-4621	1573-773X				APR	2020	51	2					1749	1769		10.1007/s11063-019-10176-9													
J								Daily Activity Feature Selection in Smart Homes Based on Pearson Correlation Coefficient	NEURAL PROCESSING LETTERS										Activity recognition; Feature selection; Pearson Correlation Coefficient; Smart home	ACTIVITY RECOGNITION; BUG REPORT; ONTOLOGY; ALGORITHM; PATTERN	In the case of a smart home, the ability to recognize daily activities depends primarily on the strategy used for selecting the appropriate features related to these activities. To achieve the goal, this paper presents a daily activity feature selection strategy based on the Pearson Correlation Coefficient. Firstly, a daily activity feature is viewed as a vector in Pearson Correlation Coefficient formula. Secondly, the relation degree between daily activity features is obtained according to weighted Pearson Correlation Coefficient formula. At last, redundant features are removed by the relation degree between daily activity features. Two distinct datasets are adopted to mitigate the effects of the coupling of the dataset used and the sensor configuration. Three different machine learning techniques are employed to evaluate the performance of the proposed approach in activity recognition. The experiment results show that the proposed approach yields higher recognition rates and achieves average improvement F-measures of 1.56% and 2.7%, respectively.																	1370-4621	1573-773X				APR	2020	51	2					1771	1787		10.1007/s11063-019-10185-8													
J								A Novel Model Predictive Runge-Kutta Neural Network Controller for Nonlinear MIMO Systems	NEURAL PROCESSING LETTERS										Adaptive nonlinear MIMO controller; Nonlinear model predictive control; Runge-Kutta based system identification; Runge-Kutta EKF; Runge-Kutta neural network controller	CONTROL MECHANISM; IDENTIFICATION; DESIGN; ANFIS	In this paper, a novel model predictive Runge-Kutta neural network (RK-NN) controller based on Runge-Kutta model is proposed for nonlinear MIMO systems. The proposed adaptive controller structure incorporates system model which provides to approximate the K-step ahead future behaviour of the controlled system, nonlinear controller where Runge-Kutta neural network (RK-NN) controller is directly deployed and adjustment mechanism based on Levenberg-Marquardt optimization method so as to optimize the weights of the Runge-Kutta neural network (RK-NN) controller. RBF neural network is employed as constituent network in order to identify the changing rates of the controller dynamics. So, the learning ability of RBF neural network and Runge Kutta integration method are combined in the MIMO nonlinear controller block. The control performance of the proposed MIMO RK-NN controller has been examined via simulations performed on a nonlinear three tank system and Van de Vusse benchmark system for different cases, and the obtained results indicate that the RK-NN controller and Runge-Kutta model achieve good control and modeling performances for nonlinear MIMO dynamical systems.																	1370-4621	1573-773X				APR	2020	51	2					1789	1833		10.1007/s11063-019-10167-w													
J								Lagrange Stability for Delayed-Impulses in Discrete-Time Cohen-Grossberg Neural Networks with Delays	NEURAL PROCESSING LETTERS										Delayed impulse; Discrete-time neural networks; Exponential Lagrange stability; Cohen-Grossberg neural network	NICHOLSONS BLOWFLIES MODEL; LIMIT-CYCLES; EXPONENTIAL CONVERGENCE; SYSTEMS; EXISTENCE	In this paper, the problem of exponential Lagrange stability for delayed-impulses in discrete-time Cohen-Grossberg neural networks (CGNNs) with delays is considered. By establishing a novel convergent difference inequation, combining with inductive method and Lyapunov theory, some sufficient conditions are obtained to ensure the exponential Lagrange stability for delayed-impulses in discrete-time CGNNs. Meanwhile, the exponential convergent domain for network is given. Finally, some examples with their simulations are given to verify the effectiveness of our results.																	1370-4621	1573-773X				APR	2020	51	2					1835	1848		10.1007/s11063-020-10190-2													
J								Global Lagrange Stability of Inertial Neutral Type Neural Networks with Mixed Time-Varying Delays	NEURAL PROCESSING LETTERS										Inertial neural network; Neutral type; Mixed time-varying delays; Lagrange stability; Global exponential attractive set; Inequality	EXPONENTIAL STABILITY; DISSIPATIVITY ANALYSIS; PERIODIC-SOLUTIONS; ROBUST STABILITY; HOPF-BIFURCATION; SYNCHRONIZATION; SENSE; CHAOS	This paper deals with the Lagrange stability of inertial neutral type neural networks with mixed time-varying delays. Two different types of activation functions are considered, including bounded and general unbounded activation functions. Under a proper variable transformation, the original inertial system is converted to a first order differential network. Based on Lyapunov method and applying inequality techniques and analytical method, some sufficient criteria are derived to ensure the global Lagrange exponential stability of the addressed neural networks. Moreover, the global exponential attractive sets are established. These results here generalize and improve the earlier publications on inertial neural networks. Finally, some numerical examples with simulations are given to demonstrate the effectiveness of our theoretical results.																	1370-4621	1573-773X				APR	2020	51	2					1849	1867		10.1007/s11063-019-10177-8													
J								Performance Evaluation of a New BP Algorithm for a Modified Artificial Neural Network	NEURAL PROCESSING LETTERS										Modified MANN; Adaptive activation function; Improved BP algorithm; New ANN structure	BACKPROPAGATION ALGORITHM; HYBRID MODEL; CONVERGENCE; ENSEMBLE; PCA; VMD	In a conventional artificial neural model, the nonlinear activation function (AF) follows the weight sum operation. In this paper, the AF is placed before the connecting weights of each artificial neuron and hence modified artificial neural network (MANN) is proposed and the corresponding backpropagation (BP) learning algorithm is derived. Further, the slope of AF which is conventionally fixed during the training phase is adjusted for achieving better and faster training of the multi-layer artificial neural network (MANN). In this case, also both the weights and slope update algorithms are derived. To assess and compare the performance of these two ANN models, standard applications such as classification, nonlinear system identification (direct modeling), nonlinear channel equalization (inverse modeling) are implemented through simulation and compared with that obtained by conventional BP based MANN. The simulation results demonstrate that the adaptive slope based MANN outperforms the fixed slope as well as conventional MANN in terms of three different performance measures such as the number of iterations to converge, mean of the square of residual error and mean absolute deviation.																	1370-4621	1573-773X				APR	2020	51	2					1869	1889		10.1007/s11063-019-10172-z													
J								Memristor Crossbar Array Based ACO For Image Edge Detection	NEURAL PROCESSING LETTERS										Memristor; Ant colony optimization; Image edge detection; Crossbar array		Memristor provides an available way to design and deploy swarm intelligence. As a typical swarm intelligence algorithm, ant colony optimization is implemented by the memristor crossbar array to make image edge detection in this paper. Firstly, a non-linear voltage-controlled memristor model with a relaxation term is proposed. Then, an improved ant colony optimization with padding strategy is designed. Thirdly, a memristor crossbar array with external control circuits is designed to implement ant colony optimization for image edge detection, which offers high device density and parallel computing. In the course of ant colony optimization based image edge detection deployed by memristor crossbar array, the threshold to generating edges can be directly chosen as the mean of the final conductance matrix. On the one hand, experiment results show that more delicate edges can be detected by proposed method compared to holistically-nested edge detection based on neural networks. On the other hand, Figure of merit of proposed method is better than that of Sobel operator.																	1370-4621	1573-773X				APR	2020	51	2					1891	1905		10.1007/s11063-019-10179-6													
J								Feature Enhancement for Multi-scale Object Detection	NEURAL PROCESSING LETTERS										Object detection; Deep learning; Oriented gradient features; Dilated convolution		Recently, deep learning has brought great progress in object detection. However, we believe that traditional hand-crafted features may still contain valuable human knowledge complementary to features learned from raw data. Besides, almost all top-performing object detection methods extract features by using backbones originally designed for image classification. The generated features are often highly semantic, which is beneficial to global image classification, but may lose details useful for object localization and recognition under various scales. To alleviate the problems mentioned above, a feature enhancement method is proposed in this paper. Inspired by the success of histograms of oriented gradients in traditional object detection research, we construct feature channels based on oriented gradients as input to convolutional neural networks to capture discriminative local orientations. The oriented gradients and RGB features are stacked as input of network to enhance the input feature representation. For accurate object localization and recognition, we employ dilated convolutions to increase spatial resolutions of output feature maps while maintaining their respective receptive fields. Hierarchical feature maps with different receptive fields are aggregated into the final feature representation for multi-scale object detection without extra upsampling. Experimental results on PASCAL VOC 2007 and 2012 demonstrate superiority of the proposed method compared with state-of-the-art methods for multi-scale object detection.																	1370-4621	1573-773X				APR	2020	51	2					1907	1919		10.1007/s11063-019-10182-x													
J								Finite-Time Synchronization of Coupled Inertial Memristive Neural Networks with Mixed Delays via Nonlinear Feedback Control	NEURAL PROCESSING LETTERS										Finite-time synchronization; Fixed-time synchronization; Nonlinear feedback control; Time delays	DYNAMICAL NETWORKS; SYSTEMS; STABILIZATION; DISSIPATIVITY; STABILITY	The finite-time synchronization of coupled inertial memristive neural networks (IMNNs) systems is discussed in this paper. Firstly, a mathematical model of IMNNs with time-varying delays is given, then the original system is transformed into a first-order differential equation by selecting a suitable variable substitution. Secondly, by using two different controllers and the definition of the upper right-hand derivative, it can be guaranteed that finite-time and fixed-time synchronization between response system and drive system based on finite time stability and fixed time theory. Finally, two numerical simulations are given to illustrate the effectiveness of the main results.																	1370-4621	1573-773X				APR	2020	51	2					1921	1938		10.1007/s11063-019-10180-z													
J								H infinity\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$H_{\infty }$$\end{document} Filtering for Markov Jump Neural Networks Subject to Hidden-Markov Mode Observation and Packet Dropouts via an Improved Activation Function Dividing Method	NEURAL PROCESSING LETTERS										Activation function dividing method; Hidden Markov model (HMM); Markov jump neural networks; <mml; math><mml; msub><mml; mi>H</mml; mi><mml; mi>infinity</mml; mi></mml; msub></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$H_{; infty }$$; end{document}<inline-graphic xlink; href="11063_2019_10175_Article_IEq6; gif"; > filtering	TIME-VARYING DELAYS; OUTPUT-FEEDBACK STABILIZATION; STATE ESTIMATION; NONLINEAR-SYSTEMS; H-INFINITY; MISSING MEASUREMENTS; STABILITY; DISCRETE	This paper is devoted to investigating the H infinity filtering problem for Markov jump neural networks with hidden-Markov mode observation and packet dropouts, in which the information regarding to the Markov state can not be completely acquired. To address this circumstance, a hidden Markov model (HMM)-based technique is established. That is employing a detector to detect the information of the Markov state and then giving an estimated signal of the Markov state for the filter design. Some H infinity performance analysis criteria for filtering error systems and the corresponding HMM-based filter design procedure are given. An improved activation function dividing method (AFDM) is presented for neural networks to reduce the conservatism of the obtained results. The superiority of the improved AFDM and the validity of obtained results are verified by an illustrative example.																	1370-4621	1573-773X				APR	2020	51	2					1939	1955		10.1007/s11063-019-10175-w													
J								ANN Based Solution of Uncertain Linear Systems of Equations	NEURAL PROCESSING LETTERS										Artificial neural network (ANN); Interval arithmetic; Linear systems of equations (LSEs); Fully interval linear systems of equations	NEURAL-NETWORKS; FUZZY SYSTEM; LAG-SYNCHRONIZATION; NUMERICAL-SOLUTION; INTERVAL	Linear systems of equations have many applications in the area of engineering sciences, mathematics, operations research and statistics. It is worth mentioning that the coefficient matrix of the linear systems of equations may not be always crisp due to various uncertainties. These uncertainties may be in the form of interval. Likewise, the solution set and right hand side vector may also be in interval. In this respect, a fully interval linear system of equations (P similar to z similar to=q similar to) is one where the coefficient matrix, the unknown vector and the right hand side vector all are in the form of interval. Although various authors proposed different methods to handle the fully linear systems of equations but those are sometimes problem specific etc. As such, in this paper nxnfully interval linear systems of equations has been solved based on artificial neural network (ANN) model. In this regard, step by step algorithm has been included. Further, a convergence theorem has also been discussed for choosing suitable learning parameter. Few numerical examples and an application problem related to electrical circuit have been solved using the proposed method. Detail procedure has been discussed with numerical results to show the efficacy and powerfulness of the method.																	1370-4621	1573-773X				APR	2020	51	2					1957	1971		10.1007/s11063-019-10183-w													
J								An Image Clustering Auto-Encoder Based on Predefined Evenly-Distributed Class Centroids and MMD Distance	NEURAL PROCESSING LETTERS										Auto-encoder; Clustering; Predefined evenly-distributed class centroids (PEDCC); Data augmentation; Maximum mean discrepancy (MMD)	NETWORK	In this paper, we propose a novel, effective and simpler end-to-end image clustering auto-encoder algorithm: ICAE. The algorithm uses predefined evenly-distributed class centroids (PEDCC) as the clustering centers, which ensures the inter-class distance of latent features is maximal, and adds data distribution constraint, data augmentation constraint, auto-encoder reconstruction constraint and Sobel smooth constraint to improve the clustering performance. Specifically, we perform one-to-one data augmentation to learn the more effective features. The data and the augmented data are simultaneously input into the autoencoder to obtain latent features and the augmented latent features whose similarity are constrained by an augmentation loss. Then, making use of the maximum mean discrepancy distance, we combine the latent features and augmented latent features to make their distribution close to the PEDCC distribution (uniform distribution between classes, Dirac distribution within the class) to further learn clustering-oriented features. At the same time, the MSE of the original input image and reconstructed image is used as reconstruction constraint, and the Sobel smooth loss to build generalization constraint to improve the generalization ability. Finally, extensive experiments on three common datasets MNIST, Fashion-MNIST, COIL20 are conducted. The experimental results show that the algorithm has achieved the best clustering results so far. In addition, we can use the predefined PEDCC class centers, and the decoder to clearly generate the samples of each class. The code can be downloaded at																	1370-4621	1573-773X				APR	2020	51	2					1973	1988		10.1007/s11063-020-10194-y													
J								A new insight into implementing Mamdani fuzzy inference system for dynamic process modeling: Application on flash separator fuzzy dynamic modeling	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Mamdani fuzzy inference; Flash separation; Dynamic modeling; Heuristic-based modeling; Rule reduction	DISTILLATION-COLUMNS; GENERALIZED-MODEL; IDENTIFICATION	In this work, a novel approach to model the dynamic behavior of the flash separation process (as a main building block of non-reacting stage-wise operations) based on Mamdani Fuzzy Inference Systems is proposed. This model surmounts the need to solve various types of mathematical equations governing the system and does not require thermodynamic properties which are either not available or computationally demanding. Hence it can be easily used in dynamic simulation of multi-phase flow in distributed systems. In the proposed approach the overall model is broken into several simple sub-models based on intuitive analysis of an expert. Moreover, a new fuzzy concept, named ``Linguistic Composition Variable'' is introduced to represent components mole fractions of each phase as a fuzzy variable. Accordingly, large number of rules which is the main shortcoming of the Mamdani Fuzzy system is significantly reduced. The performance of the proposed dynamic model is evaluated through comparing its results against their corresponding counterparts for a flash separator of crude oil. Overall MAPE (Mean Absolute Percentage Error) values of 7.17% for the gas molar fractions, 3.06% for liquid molar fractions, 10.16% for the temperature, 0.63% for the pressure and 16.44% for the liquid level of the separator have been achieved showing that the proposed fuzzy model can effectively capture the general trends of process data of the dynamic process.																	0952-1976	1873-6769				APR	2020	90								103485	10.1016/j.engappai.2020.103485													
J								Enhancing semantic segmentation with detection priors and iterated graph cuts for robotics	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Semantic scene understanding; Object detection; Segmentation and categorization; Mapping	MAPS	To foster human-robot interaction, autonomous robots need to understand the environment in which they operate. In this context, one of the main challenges is semantic segmentation, together with the recognition of important objects, which can aid robots during exploration, as well as when planning new actions and interacting with the environment. In this study, we extend a multi-view semantic segmentation system based on 3D Entangled Forests (3DEF) by integrating and refining two object detectors, Mask R-CNN and You Only Look Once (YOLO), with Bayesian fusion and iterated graph cuts. The new system takes the best of its components, successfully exploiting both 2D and 3D data. Our experiments show that our approach is competitive with the state-of-the-art and leads to accurate semantic segmentations.																	0952-1976	1873-6769				APR	2020	90								103467	10.1016/j.engappai.2019.103467													
J								Toward optimum fuzzy support vector machines using error distribution	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Classification; Support vector machine (SVM); Fuzzy support vector machine (FSVM); Social media data; 500px images; Loss function	DIAGNOSIS; CLASSIFICATION	Support vector machines (SVM) is one of the prevalent techniques in the machine learning which is applicable in many of the real world classification problems. However, these problems are sensitive to noise and the presence of the outliers in training data. For this reason, other methods like fuzzy SVM (FSVM) were introduced to solve this challenge. Similar to SVM, these methods look for finding an optimal hyperplane that separates classes with the maximum possible margin. The main difference is the allocation of fuzzy membership degree to each training sample based on its significance so that leads to the resistance of the method. In this paper to overcome this problem, we proposed a robust FSVM based on prior knowledge related to error distribution (PKED-FSVM) to classify synthetic and real data. The optimal coefficient in our proposed PKED-FSVM method is introduced from the viewpoint of minimum Bayesian risk. Our proposed method is compared with SVM and other FSVM methods and results are obtained on several types of synthetic data from UCI. Finally, we collected the social media images from 500px based on user-tagged and created two data sets; Coastal-Non-Coastal, and the Rural-urban, and used the proposed method to classify these two data sets and separating coastal areas from non-coastal areas, and urban areas from rural areas, respectively. The results show that our proposed method has acceptable performance and works better than other competing methods.																	0952-1976	1873-6769				APR	2020	90								103545	10.1016/j.engappai.2020.103545													
J								Dynamic shuffled frog-leaping algorithm for distributed hybrid flow shop scheduling with multiprocessor tasks	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Distributed scheduling; Hybrid flow shop scheduling; Multiprocessor tasks; Shuffled frog-leaping algorithm; Dynamic strategy	MINIMIZING MAKESPAN; SEARCH ALGORITHM; FLOWSHOPS; OPTIMIZATION; TIME; HEURISTICS; MACHINES	Distributed scheduling problems have attracted much attention in recent years; however, distributed hybrid flow shop scheduling problem (DHFSP) is seldom investigated. In this paper, DHFSP with multiprocessor tasks is studied and a dynamic shuffled frog-leaping algorithm (DSFLA) is proposed to minimize makespan. Dynamic search process is executed in each memeplex with at least two improved solutions. Global search and dynamic multiple neighborhood search are applied, in which neighborhood structure is chosen based on its optimization effect. A new destruction-construction process is hybridized with DSFLA and population shuffling is done when shuffling condition is met. Lower bound is obtained and proved. A number of experiments are conducted on a set of instances. The computational results validate the effectiveness of the new strategies of DSFLA and the competitive performances on solving the considered DHFSP.																	0952-1976	1873-6769				APR	2020	90								103540	10.1016/j.engappai.2020.103540													
J								Time series segmentation for state-model generation of autonomous aquatic drones: A systematic framework	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Time series segmentation; Situation assessment; State-model generation; Autonomous surface vessels; Activity recognition; Water monitoring; Model interpretation/explanation; Sensor data analysis	HUMAN ACTIVITY RECOGNITION; SITUATION ASSESSMENT	Autonomous surface vessels are becoming increasingly important for water monitoring. Their aim is to navigate rivers and lakes with limited intervention of human operators, to collect real-time data about water parameters. To reach this goal, these intelligent systems must interact with the environment and act according to the situations they face. In this work we propose a framework based on the integration of recent time-series clustering/segmentation methods and cluster validity indices, for detecting, modeling and evaluating aquatic drone states. The approach is completely data-driven and unsupervised. It takes unlabeled multivariate time series of sensor traces and returns both a set of statistically significant state-models (generated by different mathematical approaches) and a related segmentation of the dataset. We test the approach on a real dataset containing data of six campaigns, two in rivers and four in lakes, in different countries for about 5.6 h of navigation. Results show that the methodology is able to recognize known states and to discover unknown states, enabling novelty detection. The approach is therefore an easy-to-use tool for discovering and interpreting significant states in sensor data, that enables improved data analysis and drone autonomy.																	0952-1976	1873-6769				APR	2020	90								103499	10.1016/j.engappai.2020.103499													
J								A generalized matrix profile framework with support for contextual series analysis	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Time series; Anomaly detection; Matrix Profile; Distance matrix; Series Distance Matrix; Contextual Matrix Profile	TIME-SERIES; DISCOVERY	The Matrix Profile is a state-of-the-art time series analysis technique that can be used for motif discovery, anomaly detection, segmentation and others, in various domains such as healthcare, robotics, and audio. Where recent techniques use the Matrix Profile as a preprocessing or modeling step, we believe there is unexplored potential in generalizing the approach. We derived a framework that focuses on the implicit distance matrix calculation. We present this framework as the Series Distance Matrix (SDM). In this framework, distance measures (SDM-generators) and distance processors (SDM-consumers) can be freely combined, allowing for more flexibility and easier experimentation. In SDM, the Matrix Profile is but one specific configuration. We also introduce the Contextual Matrix Profile (CMP) as a new SDM-consumer capable of discovering repeating patterns. The CMP provides intuitive visualizations for data analysis and can find anomalies that are not discords. We demonstrate this using two real world cases. The CMP is the first of a wide variety of new techniques for series analysis that fits within SDM and can complement the Matrix Profile.																	0952-1976	1873-6769				APR	2020	90								103487	10.1016/j.engappai.2020.103487													
J								A competency question-oriented approach for the transformation of semi-structured bioinformatics data into linked open data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Semi-structured bioinformatics data; Linked open data; Stepwise transformation approach; Competency questions	ONTOLOGY; UNIPROT	Bioinformatics data obtained using different molecular biology techniques must be processed through different analysis tools to discover new biological knowledge. Since plain processed data have no explicit semantic value, the extraction of additional knowledge through data exploration would benefit from the transformation of bioinformatics data into Linked Open Data (LOD). Different approaches have been proposed to support the transformation of different types of biomedical data into LOD. However, these approaches are not flexible enough so they can be easily adapted for the transformation of semi-structured bioinformatics data into LOD. Thus, this paper proposes a novel approach to support such transformation. According to this approach, a set of competency questions drive not only the definition of transformation rules, but also the data transformation and exploration afterwards. The paper also presents a support toolset and describes the successful application of the proposed approach in the functional genomics domain.																	0952-1976	1873-6769				APR	2020	90								103495	10.1016/j.engappai.2020.103495													
J								An ontological approach for pathology assessment and diagnosis of tunnels	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Tunnel diagnosis; Ontology; Intelligent decision support systems; Linear transport structures	DECISION-SUPPORT-SYSTEM; SEMANTIC WEB TECHNOLOGIES; INTEGRATION	Tunnel maintenance requires complex decision making, which involves pathology diagnosis and risk assessment, to ensure full safety while optimising maintenance and repair costs. A Decision Support System (DSS) can play a key role in this process by supporting the decision makers in identifying pathologies based on disorders present in various tunnel portions and contextual factors affecting a tunnel. Another key aspect is to identify which spatial stretches within a tunnel contain pathologies of similar kinds within neighbouring tunnel segments. This paper presents PADTUN, a novel intelligent decision support system that assists with pathology diagnosis and assessment of tunnels with respect to their disorders and diagnosis influencing factors. It utilises semantic web technologies for knowledge capture, representation, and reasoning. The core of PADTUN is a family of ontologies which represent the main concepts and relations associated with pathology assessment, and capture the decision process concerning tunnel maintenance. Tunnel inspection data is linked to these ontologies to take advantage of inference capabilities offered by semantic technologies. In addition, an intelligent mechanism is presented which exploits abstraction and inference capabilities. Thus PADTUN provides the world's first semantically based intelligent DSS for tunnel maintenance. PADTUN was developed by an interdisciplinary team of tunnel experts and knowledge engineers in real-world settings offered by the NeTTUN EU Project. An evaluation of the PADTUN system is performed using real-world tunnel data and diagnosis tasks. We show how the use of semantic technologies allows addressing the complex issues of tunnel pathology inferencing, aiding in, and matching transportation experts' expectations of decision support. The methodology is applicable to any linear transport structures, offering intelligent ways to aid with complex decision processes related to diagnosis and maintenance.																	0952-1976	1873-6769				APR	2020	90								103450	10.1016/j.engappai.2019.103450													
J								Large-scale monitoring of operationally diverse district heating substations: A reference-group based approach	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										District heating substations; Return temperature; Reference-group based operational monitoring; Fault detection; Outlier detection	FAULT-DETECTION; SUPPORT VECTOR; LOAD; CONSUMPTION; MODEL	A typical district heating (DH) network consists of hundreds, sometimes thousands, of substations. In the absence of a well-understood prior model or data labels about each substation, the overall monitoring of such large number of substations can be challenging. To overcome the challenge, an approach based on the collective operational monitoring of each substation by a local group (i.e., the reference-group) of other similar substations in the network was formulated. Herein, if a substation of interest (i.e., the target) starts to behave differently in comparison to those in its reference-group, then it was designated as an outlier. The approach was demonstrated on the monitoring of the return temperature variable for atypical(1) and faulty operational behavior in 778 substations associated with multi-dwelling buildings. The choice of an appropriate similarity measure along with its size kappa were the two important factors that enables a reference-group to effectively detect an outlier target. Thus, different similarity measures and size kappa for the construction of the reference-groups were investigated, which led to the selection of the Euclidean distance with kappa = 80. This setup resulted in the detection of 77 target substations that were outliers, i.e., the behavior of their return temperature changed in comparison to the majority of those in their respective reference-groups. Of these, 44 were detected due to the local construction of the reference-groups. In addition, six frequent patterns of deviating behavior in the return temperature of the substations were identified using the reference-group based approach, which were then further corroborated by the feedback from a DH domain expert.																	0952-1976	1873-6769				APR	2020	90								103492	10.1016/j.engappai.2020.103492													
J								Reliable blood supply chain network design with facility disruption: A real-world application	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Logistics; Blood supply chain network; Location-allocation analysis; Disruption risks and disaster; Robust optimization; Performance analysis	IMPERIALIST COMPETITIVE ALGORITHM; ROBUST OPTIMIZATION; LOCATION; MANAGEMENT; MODEL	The blood supply of hospitals in disasters is a crucial issue in supply chain management. In this paper, a dynamic robust location-allocation model is presented for designing a blood supply chain network under facility disruption risks and uncertainty in a disaster situation. A scenario-based robust approach is adapted to the model to tackle the inherent uncertainty of the problem, such as a great deal of a periodic variation in demands and facilities disruptions. It is considered that the effect of disruption in facilities depends on the initial investment level for opening them, which are affected by the allocated budget. The usage of the model is implemented by a real-world case example that addresses the demand and disruption probability as uncertain parameters. For large-scale problems, two meta-heuristic algorithms, namely the self-adaptive imperialist competitive algorithm and invasive weed optimization, are presented to solve the model. Furthermore, several numerical examples of managerial insights are evaluated.																	0952-1976	1873-6769				APR	2020	90								103493	10.1016/j.engappai.2020.103493													
J								Combined weighted multi-objective optimizer for instance reduction in two-class imbalanced data problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Instance reduction; Imbalanced classes; Between-class distribution; Evolutionary algorithm; Chaotic function; Decision surface	IMPERIALIST COMPETITIVE ALGORITHM; KRILL HERD ALGORITHM; PROTOTYPE SELECTION; DATA SETS; CLASSIFICATION; ENSEMBLE; IMPROVE; NOTION; KERNEL; SMOTE	Instance reduction from class-balanced data has been investigated in much research. However, there is a lack of studies on class-imbalanced data. Learning from imbalanced data lately has attracted a lot of attention due to the practical applications. In the case of two-class imbalanced data, the instances from one class, majority class, are more numerous than the instances from the other class, which is a minority class. The present paper aims to introduce a new instance reduction method that preserves between-class distributions in the balanced data and handles minority class instance reduction in two-class imbalanced data, efficiently. The proposed method solves the instance reduction issue from an unconstrained multi-objective optimization problem aspect. Accordingly, a new combined weighted optimizer is designed. By employing the chaotic krill herd evolutionary algorithm, both the minority and majority class spaces with the accelerated convergence are explored. Through this method, the original data set is purged of those instances that decrease accuracy, and Gmean. The performance has been evaluated on both imbalanced and balanced data sets collected from the UCI repository by the 10-fold cross-validation method. Evaluations show that the proposed method outperforms state-of-the-art methods in terms of classification accuracy, Gmean, reduction rates, and computational time.																	0952-1976	1873-6769				APR	2020	90								103500	10.1016/j.engappai.2020.103500													
J								Altered mineral segmentation in thin sections using an incremental-dynamic clustering algorithm	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Thin section; Alborz Mineralogical Database; Altered minerals; Intelligent mineral segmentation; Incremental-dynamic clustering; NASA Mars rover exploration	GRAIN-BOUNDARY DETECTION; PETROGRAPHIC IMAGES; EDGE-DETECTION; IDENTIFICATION; SYSTEM	Intelligent mineral segmentation in thin section images of rocks still remains a challenging task in modern computational mineralogy. The objective of the paper is segmenting minerals in geological thin section's images with special attention on altered mineral segmentation. In this paper, an efficient incremental-dynamic clustering algorithm is developed for segmentation of minerals in thin sections containing altered and non-altered minerals. In the clustering algorithm, there is no need for determining the number of clusters (minerals) existed in thin section images, and also it is able to deal with color changing and non-evident boundaries in altered minerals. We have solved two main existing limitations: segmentation of mineral pixels that are frequently labeled as background pixels, and segmentation of thin sections containing altered minerals. Moreover, we created an open database (Alborz Mineralogical Database), as a benchmark database in computational geosciences regarding image studies of mineral. The proposed method is validated based on the results provided by the segmentation maps, and experimental results indicate that the proposed method is very efficient and outperforms previous segmentation methods for altered minerals in thin section images. The proposed method can be applied in mining engineering, rock mechanics engineering, geotechnique engineering, mineralogy, petrography, and applications such as NASA's Mars Rover Explorations (MRE).																	0952-1976	1873-6769				APR	2020	90								103466	10.1016/j.engappai.2019.103466													
J								Holistic design for deep learning-based discovery of tabular structures in datasheet images	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep learning; Image processing; Document processing; Table detection; Tabular data extraction; Page object detection; Structure detection	EXTRACTION	Extracting data from tabular structures contained within product datasheets is crucial in many contexts, particularly in the management and optimization of supply chains that serve various industries. In order to minimize human intervention, table detection and table structure detection form the essential functionality. However, a self-contained holistic solution to extract the tables as well as their columns and rows in not readily available. To address this challenge, This study presents a new formal procedure that consists of the following sequence: table detection, structure segmentation and holistic tabular structure detection on documents. The proposed table detection model outperforms the state-of-the-art solutions by achieving a recall value of 1.0 and a precision of more than 0.99 on public competition datasets. Furthermore, this work introduces a judging mechanism and an agreement-based post-processing procedure to incorporate hand-crafted rules into the deep learning models. Though the individual components achieve a new state-of-the-art F1-Score, when integrated the best achieved F-measure for the holistic system is 0.89.																	0952-1976	1873-6769				APR	2020	90								103551	10.1016/j.engappai.2020.103551													
J								Tunicate Swarm Algorithm: A new bio-inspired based metaheuristic paradigm for global optimization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Metaheuristics; Constrained optimization; Unconstrained optimization; Engineering design problems; Swarm intelligence	SPOTTED HYENA OPTIMIZER; DESIGN	This paper introduces a bio-inspired metaheuristic optimization algorithm named Tunicate Swarm Algorithm (TSA). The proposed algorithm imitates jet propulsion and swarm behaviors of tunicates during the navigation and foraging process. The performance of TSA is evaluated on seventy-four benchmark test problems employing sensitivity, convergence and scalability analysis along with ANOVA test. The efficacy of this algorithm is further compared with several well-regarded metaheuristic approaches based on the generated optimal solutions. In addition, we also executed the proposed algorithm on six constrained and one unconstrained engineering design problems to further verify its robustness. The simulation results demonstrate that TSA generates better optimal solutions in comparison to other competitive algorithms and is capable of solving real case studies having unknown search spaces. Note that the source codes of the proposed TSA algorithm are available at http://dhimangaurav.com/																	0952-1976	1873-6769				APR	2020	90								103541	10.1016/j.engappai.2020.103541													
J								Semisupervised classification of remote sensing images using efficient neighborhood learning method	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Remote sensing; Land cover classification; Neural networks; Semisupervised learning	MODEL	Efficiency of a classification model can be enhanced with more number of accurately labeled samples, which are difficult to obtain in remote sensing imagery. To mitigate this issue, semisupervised learning methodologies can be suitably used that exploit both unlabeled and labeled samples in the learning process and lead to the performance improvement of a classification model. With this reasoning, the present article proposes a self-learning semisupervised classification model using a neural network (NN) as the base classifier. The model uses an efficient neighborhood information learning method for the NN to overcome the demerits of existing conventional approaches. It is very crucial and challenging to find the true and the most relevant neighborhood information of unlabeled samples. Using two different approaches, we propose the generation of similarity matrixes for extracting neighborhood information that eventually improve the learning process of NN. The first method considers mutual neighborhood information and the second method uses the class-map of unlabeled samples. Class labels of the unlabeled samples are predicted by a classifier, i.e., trained with the available labeled samples. Finally, the collaborative neighborhood information is derived from these two matrixes and used for the development of the proposed semisupervised classification model. Experimental demonstration on three multispectral and one hyperspectral remote sensing images justified the superiority of the proposed model compared to the existing state-of-the-art methods. For comparative analysis, various performance measures, such as overall accuracy, kappa coefficient, precision, recall, dispersion score,.., and Davies-Bouldin (DB) scores are used.																	0952-1976	1873-6769				APR	2020	90								103520	10.1016/j.engappai.2020.103520													
J								A novel bat algorithm with double mutation operators and its application to low-velocity impact localization problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Novel bat algorithm; Cauchy mutation; Gaussian mutation; Fiber Bragg grating sensor; Low-velocity impact localization	FLAME OPTIMIZATION ALGORITHM; ARTIFICIAL BEE COLONY; GRAVITATIONAL SEARCH; COMPOSITE PLATES; DIFFERENTIAL EVOLUTION; STRUCTURAL DESIGN; DAMAGE EVALUATION; SWARM ALGORITHM; FBG SENSORS; HYBRID BAT	The low-velocity impact localization in the plate structure of the ship is a critical problem which can be considered as a nonlinear optimization problem. The bat algorithm (BA) has been widely used to solve nonlinear optimization problems. However, the standard BA exhibits poor performance on complex problems because of its premature convergence. In this study, a novel bat algorithm with double mutation operators (TMBA), in which a modified time factor and two mutation operators are integrated, is proposed to enhance BA's performance on nonlinear optimization problems. Classical benchmark functions are employed to analyze the contributions of the three modifications and demonstrate the significant improvement of TMBA. For the low-velocity impact localization problem, the low-velocity impact localization system based on fiber Bragg grating (FBG) sensors is utilized to receive the impact signals. The wavelet threshold de-noising method and the generalized cross-correlation method are both applied to the extraction of time differences between the impact signals. Then, the proposed algorithm and several well-known optimization algorithms are adopted to solve the minimization fitness function which is established using the triangulation method. The statistical results indicate that TMBA is more feasible and effective for solving the low-velocity impact localization problem.																	0952-1976	1873-6769				APR	2020	90								103505	10.1016/j.engappai.2020.103505													
J								A novel policy gradient algorithm with PSO-based parameter exploration for continuous control	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Continuous control; Model-free reinforcement learning; Convergence improvement; Gradient-free algorithm; Policy gradient; Parameter exploration	PARTICLE SWARM OPTIMIZATION; REINFORCEMENT	Continuous control has attracted enormous attention due to its essential role in real-world applications. However, it is considerably difficult to be addressed through explicitly modeling in practice. As promising approaches, model-free policy gradient (PG) based methods in reinforcement learning (RL), however, suffer from slow convergence and complex computation owing to the high variance of gradient estimating and sophisticated backpropagation. Therefore, in this paper, a gradient-free policy gradient algorithm with PSO-based parameter exploration (PG-PSOPE) is proposed for continuous control tasks. To reduce variance and improve convergence rate, the PSO is combined with PG to provide a novel way for training policy network in RL. Experimental results of simulated physical control tasks verify the effectiveness of the proposed algorithm. Besides, the PG-PSOPE is superior in both convergence speed and final performance to the typical on-policy PG and the off-policy deep RL method. Furthermore, the PG-PSOPE exhibits the simplicity and high effectiveness by comparison of training time under different tasks, and its running time is reduced by 58 times compared with other gradient-based methods for the best case.																	0952-1976	1873-6769				APR	2020	90								103525	10.1016/j.engappai.2020.103525													
J								A rational and consensual method for group decision making with interval-valued intuitionistic multiplicative preference relations	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										GDM; IVIMPR; Model; Consistency and consensus	CONSISTENCY ANALYSIS; MODELS	Interval-valued intuitionistic multiplicative variables (IVIMVs) can conveniently and effectively represent the uncertain multiplicative preferred and non-preferred judgements of decision makers, this paper studies group decision making (GDM) with interval-valued intuitionistic multiplicative preference relations (IVIMPRs). To calculate the interval-valued intuitionistic multiplicative priority weight vector reasonably, a new consistency concept is introduced that satisfies robustness and upper triangular property. Following this concept, models to judge the consistency and obtain consistent IVIMPRs from inconsistent ones are constructed, respectively. To address the problem of incomplete preferences, consistency-based model to determine missing values is built. Furthermore, we study the consensus for GDM with IVIMPRs and provide a new consensus approach. When an acceptable consensus level is not achieved, an interactive and automatic adjustment method is applied to reach a better consensus level. Following discussion about consistency and consensus, an algorithm for GDM is offered that can address inconsistent and incomplete IVIMPRs. Finally, a practical problem about selecting the steel supplier is selected to show the application of the new method.																	0952-1976	1873-6769				APR	2020	90								103514	10.1016/j.engappai.2020.103514													
J								A novel fractional-order type-2 fuzzy control method for online frequency regulation in ac microgrid	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										General type-2 fuzzy neural network; AC microgrid; Frequency regulation; Learning algorithm	LOAD; DESIGN; SYSTEM; ALGORITHM; PSO	In this paper, a novel adaptive fractional-order fuzzy control method is developed for frequency control in an ac microgrid (MG). A sequential general type-2 fuzzy system based on the radial basis neural network is presented for online modeling of the frequency response of the MG. Then, the parameters of the type-2 fuzzy controller based on the online estimated model are online tuned, such that the frequency deviation is minimized. The consequent parameters, i.e., centers of membership functions (MFs), the values of alpha-cuts, and the type-reduction parameters are optimized based on the proposed algorithm, which is inspired from the particle swarm optimization and artificial bee colony algorithm (PSO-ABC). The simulation results and comparison with other methods show that the proposed control scheme is effective, and results in a good and robust performance in the presence of variation of solar radiation, wind speed, load disturbance, and time-varying dynamics of the other units of MG. Moreover, the effectiveness of the proposed fuzzy system and the learning algorithm are examined by using white noise as the control input, and it is shown that the proposed identification scheme results in good performance even in the noisy environment.																	0952-1976	1873-6769				APR	2020	90								103483	10.1016/j.engappai.2020.103483													
J								A type-2 fuzzy community detection model in large-scale social networks considering two-layer graphs	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Community detection; Overlapping communities; Structural/attribute similarities; Two-layer graph; Type-2 fuzzy clustering	LOGIC APPLICATIONS; SETS; CLASSIFICATION	This paper mainly aims to identify communities with different interactions between nodes in complex networks. Community detection algorithms partition vertices into densely-connected components in a complex network. In recent researches, a node is related to multiple aspects of relationships resulting in new challenges in social networks. The two aspects of relationships could be shown as a two-layer graph which comprises two graphs dependent on each other; and each graph shows a specific aspect of the interaction. In this research, a new community detection model is proposed based on the possibilistic c-means clustering model considering two-layer graphs (PCMTL) in order to detect overlapping communities based on the two-layer graphs using both structural and attribute similarities in large-scale social networks. The nodes are assigned to communities by upper and lower membership values that are indicative of the degree of belonging to the communities through type-2 fuzzy membership values, and the suggested values of interval type-2 fuzzy membership determine how a node belongs to a community with regard to two different aspects of interactions in a two-layer graph. Moreover, according to the proposed model, a validity index is introduced to assess the suggested model in comparison to the approach existing in the literature. Ultimately, two artificial and two real large-scale social networks are used to validate the performance of the suggested model.																	0952-1976	1873-6769				APR	2020	90								103206	10.1016/j.engappai.2019.07.021													
J								Fast and scalable algorithms for mining subgraphs in a single large graph	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Data mining; Decision support systems; Parallel; Sorting strategy; Subgraph mining	FREQUENT SUBGRAPH; PATTERNS	Mining frequent subgraphs is an important issue in graph mining. It is defined as finding all subgraphs whose occurrences in the dataset are greater than or equal to a given frequency threshold. In recent applications, such as social networks, the underlying graphs are very large. Algorithms for mining frequent subgraphs from a single large graph have been developing rapidly lately. Among all such algorithms, GraMi is considered the state-of-the-art. However, GraMi still consumes a lot of time and memory in the mining of a large graph. In this paper, we propose two effective strategies to optimize the GraMi algorithm, which help to increase performance as well as reduce memory consumption during execution. Firstly, GraMi only lists all frequent subgraphs, without computing the support of each mined subgraph. This is disadvantageous in decision support systems, which require information about the support of all subgraphs. Therefore, we optimize GraMi to compute the support values during the mining process. Secondly, we apply the strategy of sorting all edges in graphs by their frequencies, which means that edges with low frequencies will be mined first, and vice versa. This sorting strategy can reduce the number of possibly infrequent subgraph candidates, especially on large subgraphs that are usually derived from those edges with high frequency. Thirdly, we apply a parallel processing technique, in which each frequent edge is executed simultaneously in a separate thread, and improve our parallel strategy by combination with the sorting strategy. Our experiments were performed on three real datasets and the results showed that the performance, as well as memory requirements, are better than those of the original GraMi algorithm.																	0952-1976	1873-6769				APR	2020	90								103539	10.1016/j.engappai.2020.103539													
J								A comprehensive overview of smart wearables: The state of the art literature, recent advances, and future challenges	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Smart wearables; Information systems; Potential factors; Weight analysis technique	INFORMATION-TECHNOLOGY; ACCEPTANCE MODEL; PRIVACY CALCULUS; HEALTH; ADOPTION; INTENTION; INTERNET; DEVICES; PREDICTORS; WEIGHT	Smart wearables have gained considerable attention from Information Systems (IS) academics, business managers, and health practitioners. In spite of the availability of authentic research studies of smart wearables, there is still a lack of systematic review on the different aspects of smart wearables concept to find the current state of research, particularly from the perspective of IS field. Therefore, the predominant aim of this research is to review smart wearables literature, recent advances, and future challenges. Accordingly, a systematic literature review was conducted to explore smart wearables by reviewing previous studies from 2010 to 2019. For covering all related papers during these years, an integrated review protocol consisting of automatic and manual stages was pursued. 244 papers were identified to address smart wearables issues and challenges. According to the findings, it is observed that smart wearable studies have increased dramatically during the last years. Moreover, the results show that current studies covered different research themes which are related to smart wearables area, particularly user behavior, technology-focused, security and privacy, design, and social acceptability. Furthermore, based on the results of the weight analysis technique, perceived usefulness, attitude toward technology, social influence, and privacy concerns are identified as the best predictors of smart wearables adoption. Additionally, the results show that the Technology Acceptance Model (TAM) is the most commonly adopted theory in the smart wearable studies. The findings of this review would assist academics to realize the existing limitations and gaps as well as the future works for smart wearables research studies.																	0952-1976	1873-6769				APR	2020	90								103529	10.1016/j.engappai.2020.103529													
J								Learning the representation of raw acoustic emission signals by direct generative modelling and its use in chronology-based clusters identification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Acoustic emission; Raw waveform; Model-based clustering; Representation learning; Novelty detection	TIME-SERIES DATA; HIDDEN MARKOV; BEHAVIOR; TENSILE; TESTS; ONSET	Acoustic emission (AE) is a passive monitoring technique used for learning about the behaviour of an engineered system. The streaming obtained by continuously recording AE transient signals is treated by a four steps procedure: 1) The detection of salient AE signals by distinguishing noise against non-noise signals using wavelet denoising, 2) the statistical representation of randomly selected AE signals using Autoregressive Weakly Hidden Markov Models, 3) an inference phase by applying those models to unknown AE signals and generating a set of novelty scores reflecting differences between signals, 4) the clustering of novelty scores using constraint-based consensus clustering. Compared to the standard way relying on the transformation of all AE signals by manual feature engineering (MFE) before clustering, the main breaktrough proposed in this paper holds in the use of the raw AE signals, with different lengths and various scales, to build high level information and organise the low level streaming data. Validated first on simulated data, we show the potential of this methodology for interpreting acoustic emission streaming originating from composite materials.																	0952-1976	1873-6769				APR	2020	90								103478	10.1016/j.engappai.2020.103478													
J								Advising reinforcement learning toward scaling agents in continuous control environments with sparse rewards	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Reinforcement learning; Advising framework; Continuous control; Sparse reward; Multi-agent		This paper adapts the success of the teacher-student framework for reinforcement learning to a continuous control environment with sparse rewards. Furthermore, the proposed advising framework is designed for the scaling agents problem, wherein the student policy is trained to control multiple agents while the teacher policy is well trained for a single agent. Existing research on teacher-student frameworks have been focused on discrete control domain. Moreover, they rely on similar target and source environments and as such they do not allow for scaling the agents. On the other hand, in this work the agents face a scaling agents problem where the value functions of the source and target task converge at different rates. Existing concepts from the teacher-student framework are adapted to meet new challenges including early advising, importance of advising, and mistake correction, but a modified heuristic was used to decide on when to teach. The performance of the proposed algorithm was evaluated using the case study of pushing, and picking and placing objects with a dual arm manipulation system. The teacher policy was trained using a simulated scenario consisting of a single arm. The student policy was trained to handle the dual arm manipulation system in simulation under the advice of the teacher agent. The trained student policy was then validated using two Quanser Mico arms for experimental demonstration. The effects of varying parameters on the student performance in the advising framework was also analyzed and discussed. The results showed that the proposed advising framework expedited the training process and achieved the desired scaling within a limited advising budget.																	0952-1976	1873-6769				APR	2020	90								103515	10.1016/j.engappai.2020.103515													
J								The Fast Maximum Distance to Average Vector (F-MDAV): An algorithm for k-anonymous microaggregation in big data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										MDAV; k-anonymous microaggregation; Speedup; Data privacy; Big data	DATA-ORIENTED MICROAGGREGATION; T-CLOSENESS; PRIVACY; ANONYMIZATION; SELECT	The massive exploitation of tons of data is currently guiding critical decisions in domains such as economics or health. But serious privacy risks arise since personal data is commonly involved. k-Anonymous microaggregation is a well-known method that guarantees individuals' privacy while preserving much of data utility. Unfortunately, methods like this are computationally expensive in big data settings, whereas the application domain of data might require an immediate response to make "life or death'' decisions. Accordingly, this paper proposes five strategies to simplify the internal operations (such as distance calculations and element sorting) of the maximum distance to average vector algorithm, the de facto microaggregation standard. For the sake of its usability in large-scale databases, they, e.g., reduce the number of operations necessary to compute distances from 3m to 2m, where.. is the number of attributes of the data set. Also, the complexity of sorting operations gets reduced from O(n log n) to O(n) where n is the number of records. Through extensive experimentation over multiple data sets, we show that the new algorithm gets significantly faster. Interestingly, the speedup factor by each technique is not greater than 2, but the multiplicative effect of combining them all turns the algorithm four times faster than the original microaggregation mechanism. This remarkable speedup factor is achieved, literally, with no additional cost in terms of data utility, i.e., it does not incur greater information loss.																	0952-1976	1873-6769				APR	2020	90								103531	10.1016/j.engappai.2020.103531													
J								Design of multi-view graph embedding using multiple kernel learning	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-view learning; Graph embeddings; Graph kernels; Multiple kernel learning(MKL); R-convolution kernel	VECTOR-SPACE; REGULARIZATION	The graph embedding is the process of representing the graph in a vector space using properties of the graphs and this technique has now being widely used for analyzing the graph data using machine learning algorithms. The existing graph embeddings rely mostly on a single property of graphs for data representation which is found to be inappropriate to capture all the characteristics of the data. Hence we designed graph embedding using multi-view approach, where each view is an embedding of the graph using a graph property. The input space of multi-view learning is then taken as the direct sum of the subspaces in which the graph embedding lie. We did analysis on real world data by incorporating the proposed model on support vector machines (SVM). The reproducing kernel used in SVM is represented as the linear combination of the kernels defined on the individual embeddings. The optimization technique used in simple multiple kernel learning (simpleMKL) is used to find the parameters of the optimal kernel. To analyze the individual representation capability of the embeddings, an R-convolution graph kernel is designed over each of the views. In our experimental analysis, the multi-view graph embedding showed a superior performance in comparison with that of the state-of-the-art graph embeddings as well as graph kernels.																	0952-1976	1873-6769				APR	2020	90								103534	10.1016/j.engappai.2020.103534													
J								Teaching machines to write like humans using L-attributed grammar	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Automatic writing; Handwritten character recognition; L-attributed grammar; Top-down derivation	CHINESE CHARACTERS; RECOGNITION; ONLINE	Reading and writing are easy for humans. The automatic reading of handwritten characters has been studied for several decades. Machine learning algorithms for reading tasks often require a huge amount of data to perform with similar accuracy to humans, yet it is also difficult to gain sufficient meaningful data. Automatic writing tasks have not been studied as extensively. In this paper, we teach machines to write like teaching a child by telling the machine the method for writing each character using L-attributed grammar. With the aid of the proposed TMTW (Teaching Machines To Write) interacting system, a human as a teacher only needs to provide the writing sequence of parts and control lines. The proposed system automatically perceives the relationships between control lines and parts, and constructs the grammars. Top-down derivation and the stroke generation method are applied to generate varying characters based on the learned grammars. For as long as a machine can write, it can be applied in robot control or training sample generation for automatic reading tasks. The MNIST and CASIA datasets are used to demonstrate the effectiveness of the proposed system on different languages. The machine written samples are used to train a network, which is evaluated on the MNIST test set. A test error rate of 1.23% is achieved using only approximately 20 grammars on average for each digit. Using the generated and handwritten samples together as a training set can reduce the test error rate to 0.61%. Similar experiments are conducted using the CASIA data set, and the results demonstrated that the proposed method is effective in generating characters with a complex structure.																	0952-1976	1873-6769				APR	2020	90								103489	10.1016/j.engappai.2020.103489													
J								A hybrid meta-heuristic algorithm for scientific workflow scheduling in heterogeneous distributed computing systems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Cloud computing; Directed acyclic graph (DAG); Discrete particle swarm optimization	GENETIC ALGORITHM; TASK; DUPLICATION; PERFORMANCE; EXECUTION; PRIORITY; GRAPHS	Cloud computing has attracted great attentions in research community because of its ubiquitous, unlimited computing resources, low cost, and flexibility owing to virtualization technology. This paper presents a hybrid meta-heuristic algorithm to solve parallelizable scientific workflows on elastic cloud platforms since applying a single approach cannot yield optimal solution in such complicated problems. Scientific workflows are modeled in the form of directed acyclic graph (DAG) in which there exists data dependency between sub-tasks. In the cloud marketplace, each provider delivers variable virtual machine (VM) configurations which lead different performance. Generally, parallelizable task scheduling on parallel computing machines to obtain minimum total execution time, makespan, belongs to NP-Hard problem. To deal with the combinatorial problem, the hybrid discrete particle swarm optimization (HDPSO) algorithm is presented that has three main phases. At the first phase a random algorithm following by novel theorems is applied to produce swarm members; it is as input of presented new discrete particle swarm optimization (DPSO) algorithm in the second phase. To avoid getting stuck in sub-optimal trap and to balance between exploration and exploitation, local search improvement is randomly combined in DPSO by calling Hill Climbing technique at the third phase to enhance overall performance. Second and third phases are iterated till the termination criteria is met. The average results reported from different executions of intensive settings on 12 scientific datasets proved our hybrid meta-heuristic has the amount of 10.67, 14.48, and 3 percentage dominance in terms of SLR, SpeedUp, and efficiency respectively against other existing meta-heuristics.																	0952-1976	1873-6769				APR	2020	90								103501	10.1016/j.engappai.2020.103501													
J								Safe energy savings through context-aware hot water demand prediction	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Artificial intelligence; Autoregressive networks; Predictive models; Energy systems; Utilities; Behavior modeling	INTERNET; SYSTEM; LEGIONELLA	Tank-style water heaters provide a critical utility but often waste energy, as low temperatures encourage the formation of harmful bacteria. We develop low-cost hardware to capture training data from a single home's hot water consumption and present a system capable of proactively anticipating demand while remaining sensitive to health concerns by combining a flow-predictive autoregressive convolutional neural network with a Cognitive Supervisor for minimizing Legionella formation. The predictive performance is evaluated for different receptive fields, and the best-performing model successfully tracks water demand one week into the future, providing robust input for the Supervisor to reform Legionella growth with minimal energy expenditure. First-order estimates find U.S. homeowner savings of $10B in heating costs and 80MMT of CO2 without increased health risks. This system uniquely combines predictive and protective elements to augment new and incumbent water heater installations including those used in low to middle-high income countries, enhancing global impact.																	0952-1976	1873-6769				APR	2020	90								103481	10.1016/j.engappai.2020.103481													
J								A machine learning approach to modelling escalator demand response	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Escalators; Vertical transportation; Demand response; Modelling; Random forest; Neural networks	ENERGY-CONSUMPTION; IMPACT	This article relates to the topic of the escalator demand response potential. Previous studies mapped escalators as an unrealized potential for additional demand response. The decrease of the nominal speed is the proposed method of reducing the power consumption of an escalator that comes at the cost of passenger travel time and queuing. This work proposes a solution to a problem of selecting appropriate escalators from a large pool to accommodate the target of power curtailment at a minimum cost and highlights the escalator features that constitute the best demand response candidates. The paper compares four methods which differ in calculation speed and accuracy. The primal solution is the earlier developed and enhanced simulation-based model. The random forest and the neural network models provide a solution trained on the output of the simulationbased model aiming to enhance the calculation speed. Finally, all of the developed solutions are compared to the random selection of escalators. The comparison of the proposed statistical approaches shows that the random forest outperforms the neural networks with a maximum error in the prediction of the overall costs in the range of 10.5% of the simulation-based model solution, while the neural network solution lies within 10%-58%, depending on the targeted value of the power reduction. Statistical approaches enable performing predictions for different times of the day and for new escalator populations without the need for timedemanding simulations. Comparison to the random selection of escalators demonstrates that the proposed models generally outperform the random selection at least seven-fold.																	0952-1976	1873-6769				APR	2020	90								103521	10.1016/j.engappai.2020.103521													
J								Understanding of wheelchair ramp scenes for disabled people with visual impairments	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Wheelchair ramp scene; Non-manhattan structure; Monocular vision; Disabled people; Visual impairments	3D RECONSTRUCTION; APPEARANCE; PROJECTION	Helping disabled people with visual impairments understand wheelchair ramp scenes has considerable value in computer vision. However, due to the diversity of wheelchair ramp scenes, understanding them remains a big challenge. Wheelchair ramp planes can be considered as a composition of rectangles that do not satisfy manhattan assumption. These non-manhattan rectangles are projected into two dimensional projections, shaping into special geometric configurations, which may enable us to estimate their original orientation and position in 3D scenes. In this paper, we presented a method for disabled people with visual impairments to understand wheelchair ramp scenes from a single image without any prior training. Firstly angle projections can be assigned to different clusters. Secondly ramp vanishing points (RVPs) can be estimated. Then it is possible to determine ramp planes consisting of angle projections that belong to the estimated RVPs. Finally, the algorithm can understand wheelchair ramp scenes including not only manhattan structures but also ramp planes belonging to non-manhattan structures. The proposed approach requires no prior training or any knowledge of the camera's internal parameters. Besides, it is robust to the errors in calibration and image noise. We compared the estimated wheelchair ramp scene layout against the ground truth, measuring the percentage of pixels that were incorrectly classified. The experimental results showed that the method can understand wheelchair ramp scenes including not only manhattan structures but also non-manhattan structures of ramp planes, making it practical and efficient for disabled people with visual impairments.																	0952-1976	1873-6769				APR	2020	90								103569	10.1016/j.engappai.2020.103569													
J								Multiple Random Empirical Kernel Learning with Margin Reinforcement for imbalance problems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Classification; Imbalanced problems; Empirical kernel mapping; Multiple kernel learning; Margin reinforcement	CLASSIFICATION; ALGORITHM; PERFORMANCE	Imbalance problems arise in real-world applications when the number of negative samples far exceeds the number of positive samples, such as medical data. When solving the classification of imbalance problems, the samples located near the decision hyperplane contribute more to the decision hyperplane, and the samples far from the decision hyperplane contribute less to the decision hyperplane. So we can consider giving higher weights to the samples near the decision hyperplane, but they are sensitive to noise, and too much emphasis on them may lead to unstable performance. This paper proposes a Margin Reinforcement (MR) method to overcome the above dilemma. Because the imbalance problem is a cost-sensitive problem, MR gives positive samples a uniform high weight to improve the misclassification cost of the positive sample. For negative samples, according to their entropy, samples away from the decision surface and noise samples mixed in the positive samples are given a smaller weight, in order to improve the efficiency and robustness of the algorithm. Therefore, MR can emphasize the importance of samples located in overlapping regions of positive and negative classes and ignore the effects of noise samples to produce superior performance. Multiple Random Empirical Kernel Learning (MREKL) has proven to be effective and efficient in dealing with balance problems. In order to improve the performance of MREKL on imbalanced datasets, MR is introduced into MREKL to propose a novel Multiple Random Empirical Kernel Learning with Margin Reinforcement (MREKL-MR). MREKL-MR efficiently map the samples into low-dimensional feature spaces, then utilizes the MR approach to reenforce the importance of margin samples and decrease the effects of noise samples. Experimental results on 28 imbalanced datasets indicate that MREKL-MR is superior to comparison algorithms. Finally, the effectiveness of MREKL-MR in dealing with imbalance problems is verified on the Heart Failure dataset.																	0952-1976	1873-6769				APR	2020	90								103535	10.1016/j.engappai.2020.103535													
J								Ensemble framework by using nature inspired algorithms for the early-stage forest fire rescue-A case study of dynamic optimization problems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Forest fire suppression; Rescue ensemble; Forest fire rescue; Rescue simulator; Dynamic optimization problem; Unmanned platforms	PARAMETER-ESTIMATION; SPREAD PREDICTION; WILDLAND FIRE; MODEL; SIMULATION; MANAGEMENT; VEHICLES; WILDFIRE; SYSTEM; RELIEF	In this paper, we propose rescue ensemble to simulate the dynamic rescue process between forest fire spread and forest fire rescue, while simultaneously formulating this process as a dynamic optimization problem. However, there is still little research about simulating this kind of the dynamic rescue process, even when many new unmanned monitoring systems and large-scale firefighting aircraft emerge in the forest-fire-rescue field. Our rescue ensemble that consists of rescue simulator and rescue algorithm is characterized by supporting the offline simulation of the dynamic rescue process between forest fire spread (like offensive forces) and forest fire rescue (like defensive forces). Based on modifying the cellular automaton model of forest fire spread, rescue simulator is able to simulate forest fire spread and aircraft firefighting, simultaneously. Besides, the main goal of rescue algorithm is to realize the aircraft task allocation. Firefighting particle swarm optimization is proposed by us as our rescue algorithm, which is characterized by considering fire edge suppression, the burning-cell continuity, and wind direction. We construct our test problems based on real forest maps and aircraft firefighting capability. Comparing with four compared rescue algorithms, we test the different capabilities of firefighting particle swarm optimization, such as searching dynamic optimal solution, shortening the rescue time, controlling the spread speed of fire edge, and minimizing the burned cost. Experimental results demonstrate that the framework of rescue ensemble is feasible. Meanwhile, the results of firefighting particle swarm optimization are satisfactory in most cases.																	0952-1976	1873-6769				APR	2020	90								103517	10.1016/j.engappai.2020.103517													
J								Collaborative weighted multi-view feature extraction	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-view; Feature extraction; Local collaborative representative; Jensen Shannon divergence	CANONICAL CORRELATION-ANALYSIS; REPRESENTATION; REDUCTION; SPARSE	Most of the current multi-view feature extraction methods mainly consider the consistency and complementary information between multi-view samples, therefore have some drawbacks. They ignore the manifold structure of the single-view itself, and also ignore the differences among the similarities between any two views when the number of views is greater than two, because of assigning the same weight to them. In this paper, we propose a novel multi-view feature extraction method termed as collaborative weighted multi-view feature extraction or CWMvFE. Here the local collaborative representative (LCR) method is utilized to preserve the local correlation in between-view and within-view respectively. Furthermore, it realizes that less similar view pairs should share more consistency and complementary information, where Jensen Shannon divergence is used to reflect the similarity between different view pairs. Therefore, the proposed CWMvFE not only preserves the local correlation in multi-view, including local correlation in both between-view and within-view, but also explores the differences in similarities between different view pairs. Experiments on four image datasets demonstrate that CWMvFE has better performance than other related methods.																	0952-1976	1873-6769				APR	2020	90								103527	10.1016/j.engappai.2020.103527													
J								Category-specific upright orientation estimation for 3D model classification and retrieval	IMAGE AND VISION COMPUTING										Model-based 3D reconstruction; Multi-view object co-segmentation; Convolutional neural networks; Upright orientation estimation; 3D model classification; 3D model classification retrieval	NETWORKS	In this paper, we address a problem of correcting upright orientation of a reconstructed object to search. We first reconstruct an input object appearing in an image sequence, and generate a query shape using multi-view object co-segmentation. In the next phase, we utilize the Convolutional Neural Network (CNN) architecture to determine category-specific upright orientation of the queried shape for 3D model classification and retrieval. As a practical application of our system, a shape style and a pose from an inferred category and up-vector are obtained by comparing 3D shape similarity with candidate 3D models and aligning its projections with a set of 2D co-segmentation masks. We quantitatively and qualitatively evaluate the presented system with more than 720 upfront-aligned 3D models and five sets of multi-view image sequences. (C) 2020 Published by Elsevier B.V.																	0262-8856	1872-8138				APR	2020	96								103900	10.1016/j.imavis.2020.103900													
J								Automatic recognition of image of abnormal situation in scenic spots based on Internet of things	IMAGE AND VISION COMPUTING										Internet of things; Scenic spots; Abnormal situations; Image recognition	ALGORITHM; TRACKING; MODEL	Internet of things is an emerging information-aware technology that combines computer vision and artificial intelligence technology to ensure the safety of personnel and facilities in tourist attractions by discovering real-time alarms of abnormal conditions in the monitoring of tourist attractions. Therefore, this paper proposes an automatic image algorithm for tourism scene anomaly based on Internet of things. The algorithm uses the Internet of things intelligent camera in the image acquisition preprocessing platform to collect the tourist scenic spot image. Based on the traditional image segmentation technology, according to the neighborhood-related characteristics of the Markov random field, the dynamic characteristics of continuous frames are added. Reconstruct the Gibbs energy function. The location of the abnormal situation is determined by coding, and a treatment opinion is given. The experimental results show that the proposed algorithm not only considers the spatial information of each pixel and neighboring points, but also adds the time information of successive frames, accumulates the energy values of all the pixels in the whole image and analyzes the data with the energy curve. It can accurately and efficiently identify the abnormal situation images of tourist attractions. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				APR	2020	96								103908	10.1016/j.imavis.2020.103908													
J								A high-efficiency energy and storage approach for IoT applications of facial recognition	IMAGE AND VISION COMPUTING										Data compression; Face recognition; IoT; Deep learning	FACE RECOGNITION; EXPRESSION RECOGNITION; COMPRESSION; IMAGES	This work introduces a high-efficiency approach for face recognition applications based on features using a recent algorithm called Floor of Log (FoL). The advantage of this method is the reduction of storage and energy, maintaining accuracy. K-Nearest Neighbors and Support Vector Machine algorithm was applied to learn the better parameter of the FoL algorithm using cross-validation. Accuracy and the size after the compression process were adopted to evaluate the proposed algorithm. The FoL was tested in CelebA, Extended YaleB, AR, and LFW face datasets obtaining the same or better results when compared with the approach using the same classifiers with uncompressed features, but with a reduction of 86 to 91% compared to the original data size. The proposed method of this work presents a robust and straightforward algorithm of compression of features for face recognition applications. The FoL is a new supervised compression algorithm that can be adapted to achieve great results and integrated with edge computing systems. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				APR	2020	96								103899	10.1016/j.imavis.2020.103899													
J								Recovering facial reflectance and geometry from multi-view images	IMAGE AND VISION COMPUTING										3D facial reconstruction; Specular estimation; Multi-view capture		While the problem of estimating shapes and diffuse reflectances of human faces from images has been extensively studied, there is relatively less work done on recovering the specular albedo. This paper presents a lightweight solution for inferring photorealistic facial reflectance and geometry. Our system processes video streams from two views of a subject, and outputs two reflectance maps for diffuse and specular albedos, as well as a vector map of surface normals. A model-based optimization approach is used, consisting of the three stages of multi-view face model fitting, facial reflectance inference and facial geometry refinement. Our approach is based on a novel formulation built upon the 3D morphable model (3DMM) for representing 3D textured faces in conjunction with the Blinn-Phong reflection model. It has the advantage of requiring only a simple setup with two video streams, and is able to exploit the interaction between the diffuse and specular reflections across multiple views as well as timeframes. As a result, the method is able to reliably recover high-fidelity facial reflectance and geometry, which facilitates various applications such as generating photorealistic facial images under new viewpoints or illumination conditions. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				APR	2020	96								103897	10.1016/j.imavis.2020.103897													
J								Robust Multiview Subspace Learning With Nonindependently and Nonidentically Distributed Complex Noise	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Laplace equations; Adaptation models; Distributed databases; Feature extraction; Correlation; Robustness; Dirichlet process (DP) mixture model; hierarchical Dirichlet process (HDP); multiview; subspace learning; variational Bayes	MATRIX FACTORIZATION; REGRESSION	Multiview Subspace Learning (MSL), which aims at obtaining a low-dimensional latent subspace from multiview data, has been widely used in practical applications. Most recent MSL approaches, however, only assume a simple independent identically distributed (i.i.d.) Gaussian or Laplacian noise for all views of data, which largely underestimates the noise complexity in practical multiview data. Actually, in real cases, noises among different views generally have three specific characteristics. First, in each view, the data noise always has a complex configuration beyond a simple Gaussian or Laplacian distribution. Second, the noise distributions of different views of data are generally nonidentical and with evident distinctiveness. Third, noises among all views are nonindependent but obviously correlated. Based on such understandings, we elaborately construct a new MSL model by more faithfully and comprehensively considering all these noise characteristics. First, the noise in each view is modeled as a Dirichlet process (DP) Gaussian mixture model (DPGMM), which can fit a wider range of complex noise types than conventional Gaussian or Laplacian. Second, the DPGMM parameters in each view are different from one another, which encodes the "nonidentical" noise property. Third, the DPGMMs on all views share the same high-level priors by using the technique of hierarchical DP, which encodes the "nonindependent" noise property. All the aforementioned ideas are incorporated into an integrated graphics model which can be appropriately solved by the variational Bayes algorithm. The superiority of the proposed method is verified by experiments on 3-D reconstruction simulations, multiview face modeling, and background subtraction, as compared with the current state-of-the-art MSL methods.																	2162-237X	2162-2388				APR	2020	31	4					1070	1083		10.1109/TNNLS.2019.2917328													
J								Where Computation and Dynamics Meet: Heteroclinic Network-Based Controllers in Evolutionary Robotics	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Cognition; Computer architecture; Artificial neural networks; Task analysis; Robot sensing systems; Stability analysis; Erbium; Artificial neural networks; evolutionary computing; heteroclinic networks; nonlinear dynamical systems	ADAPTIVE-BEHAVIOR; COMPETITION; CONVECTION; BRAIN; BODY	In the fields of artificial neural networks and robotics, complicated, often high-dimensional systems can be designed using evolutionary/other algorithms to successfully solve very complex tasks. However, dynamical analysis of the underlying controller can often be near impossible, due to the high dimension and nonlinearities in the system. In this paper, we propose a more restricted form of controller, such that the underlying dynamical systems are forced to contain a dynamical object called a heteroclinic network. Systems containing heteroclinic networks share some properties with finite-state machines (FSMs) but are not discrete: both space and time are still described with continuous variables. Thus, we suggest that the heteroclinic networks can provide a hybrid between continuous and discrete systems. We investigate this innovated architecture in a minimal categorical perception task. The similarity of the controller to an FSM allows us to describe some of the system's behaviors as transition between states. However, other, essential behavior involves subtle ongoing interaction between the controller and the environment that eludes description at this level.																	2162-237X	2162-2388				APR	2020	31	4					1084	1097		10.1109/TNNLS.2019.2917471													
J								Learning Low-Dimensional Latent Graph Structures: A Density Estimation Approach	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Manifolds; Data models; Dimensionality reduction; Kernel; Data visualization; Noise measurement; Density estimation; feature selection; structure learning; unsupervised dimensionality reduction	ALGORITHM; INSIGHTS	We aim to automatically learn a latent graph structure in a low-dimensional space from high-dimensional, unsupervised data based on a unified density estimation framework for both feature extraction and feature selection, where the latent structure is considered as a compact and informative representation of the high-dimensional data. Based on this framework, two novel methods are proposed with very different but intuitive learning criteria from existing methods. The proposed feature extraction method can learn a set of embedded points in a low-dimensional space by naturally integrating the discriminative information of the input data with structure learning so that multiple disconnected embedding structures of data can be uncovered. The proposed feature selection method preserves the pairwise distances only on the optimal set of features and selects these features simultaneously. It not only obtains the optimal set of features but also learns both the structure and embeddings for visualization. Extensive experiments demonstrate that our proposed methods can achieve competitive quantitative (often better) results in terms of discriminant evaluation performance and are able to obtain the embeddings of smooth skeleton structures and select optimal features to unveil the correct graph structures of high-dimensional data sets.																	2162-237X	2162-2388				APR	2020	31	4					1098	1112		10.1109/TNNLS.2019.2917696													
J								A Probabilistic Synapse With Strained MTJs for Spiking Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Magnetic tunneling; Synapses; Biological neural networks; Switches; Neurons; Generators; Resistance; Handwritten digit recognition; magnetic tunnel junction (MTJ); spiking neural network (SNN); stochastic synapse; straintronics		Spiking neural networks (SNNs) are of interest for applications for which conventional computing suffers from the nearly insurmountable memory-processor bottleneck. This paper presents a stochastic SNN architecture that is based on specialized logic-in-memory synaptic units to create a unique processing system that offers massively parallel processing power. Our proposed synaptic unit consists of strained magnetic tunnel junction (MTJ) devices and transistors. MTJs in our synapse are dual purpose, used as both random bit generators and as general-purpose memory. Our neurons are modeled as integrate-and-fire components with thresholding and refraction. Our circuit is implemented using CMOS 28-nm technology that is compatible with the MTJ technology. Our design shows that the required area for the proposed synapse is only 3.64 mu m(2)/unit.. When idle, the synapse consumes 675 pW. When firing, the energy required to propagate a spike is 8.87 fJ. We then demonstrate an SNN that learns (without supervision) and classifies handwritten digits of the MNIST database. Simulation results show that our network presents high classification efficiency even in the presence of fabrication variability.																	2162-237X	2162-2388				APR	2020	31	4					1113	1123		10.1109/TNNLS.2019.2917819													
J								Continuously Constructive Deep Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Constructive learning; deep learning; neural networks	PRUNING ALGORITHM; SELECTION	Traditionally, deep learning algorithms update the network weights, whereas the network architecture is chosen manually using a process of trial and error. In this paper, we propose two novel approaches that automatically update the network structure while also learning its weights. The novelty of our approach lies in our parameterization, where the depth, or additional complexity, is encapsulated continuously in the parameter space through control parameters that add additional complexity. We propose two methods. In tunnel networks, this selection is done at the level of a hidden unit, and in budding perceptrons, this is done at the level of a network layer; updating this control parameter introduces either another hidden unit or layer. We show the effectiveness of our methods on the synthetic two-spiral data and on three real data sets of MNIST, MIRFLICKR, and CIFAR, where we see that our proposed methods, with the same set of hyperparameters, can correctly adjust the network complexity to the task complexity.																	2162-237X	2162-2388				APR	2020	31	4					1124	1133		10.1109/TNNLS.2019.2918225													
J								Performance Evaluation of Probabilistic Methods Based on Bootstrap and Quantile Regression to Quantify PV Power Point Forecast Uncertainty	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Bootstrap; neural networks (NNs); particle swarm optimization (PSO); quantile regression (QR); solar power forecasting	OPTIMAL PREDICTION INTERVALS; SHORT-TERM LOAD; WIND POWER; NEURAL-NETWORK; WAVELETS; SYSTEM	This paper presents two probabilistic approaches based on bootstrap method and quantile regression (QR) method to estimate the uncertainty associated with solar photovoltaic (PV) power point forecasts. Solar PV output power forecasts are obtained using a hybrid intelligent model, which is composed of a data filtering technique based on wavelet transform (WT) and a soft computing model (SCM) based on radial basis function neural network (RBFNN) that is optimized by particle swarm optimization (PSO) algorithm. The point forecast capability of the proposed hybrid WT+RBFNN+PSO intelligent model is examined and compared with other hybrid models as well as individual SCM. The performance of the proposed bootstrap method in the form of probabilistic forecasts is compared with the QR method by generating different prediction intervals (PIs). Numerical tests using real data demonstrate that the point forecasts obtained from the proposed hybrid intelligent model can be effectively used to quantify PV power uncertainty. The performance of these two uncertainty quantification methods is assessed through reliability.																	2162-237X	2162-2388				APR	2020	31	4					1134	1144		10.1109/TNNLS.2019.2918795													
J								Task Assignment for Multivehicle Systems Based on Collaborative Neurodynamic Optimization	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Task analysis; Optimization; Neurodynamics; Collaboration; Linear programming; Neural networks; Lagrangian functions; Multivehicle systems; neurodynamic optimization; task assignment (TA)	PARTICLE SWARM OPTIMIZATION; RECURRENT NEURAL-NETWORK; ALLOCATION; ALGORITHM; ROBOTS	This paper addresses task assignment (TA) for multivehicle systems. Multivehicle TA problems are formulated as a combinatorial optimization problem and further as a global optimization problem. To fulfill heterogeneous tasks, cooperation among heterogeneous vehicles is incorporated in the problem formulations. A collaborative neurodynamic optimization approach is developed for solving the TA problems. Experimental results on four types of TA problems are discussed to substantiate the efficacy of the approach.																	2162-237X	2162-2388				APR	2020	31	4					1145	1154		10.1109/TNNLS.2019.2918984													
J								An Improved N-Step Value Gradient Learning Adaptive Dynamic Programming Algorithm for Online Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Adaptive dynamic programming (ADP); convergence analysis; eligibility traces; online learning; reinforcement learning; temporal difference (TD); value gradient learning (VGL)	STABILITY ANALYSIS; NONLINEAR-SYSTEMS; TRACKING CONTROL; CONTINUOUS-TIME; HJB SOLUTION; BACKPROPAGATION; REPRESENTATION; APPROXIMATION; ARCHITECTURE	In problems with complex dynamics and challenging state spaces, the dual heuristic programming (DHP) algorithm has been shown theoretically and experimentally to perform well. This was recently extended by an approach called value gradient learning (VGL). VGL was inspired by a version of temporal difference (TD) learning that uses eligibility traces. The eligibility traces create an exponential decay of older observations with a decay parameter (lambda). This approach is known as TD(lambda), and its DHP extension is known as VGL(lambda), where VGL(0) is identical to DHP. VGL has presented convergence and other desirable properties, but it is primarily useful for batch learning. Online learning requires an eligibility-trace-work-space matrix, which is not required for the batch learning version of VGL. Since online learning is desirable for many applications, it is important to remove this computational and memory impediment. This paper introduces a dual-critic version of VGL, called N-step VGL (NSVGL), that does not need the eligibility-trace-workspace matrix, thereby allowing online learning. Furthermore, this combination of critic networks allows an NSVGL algorithm to learn faster. The first critic is similar to DHP, which is adapted based on TD(0) learning, while the second critic is adapted based on a gradient of n-step TD(lambda) learning. Both networks are combined to train an actor network. The combination of feedback signals from both critic networks provides an optimal decision faster than traditional adaptive dynamic programming (ADP) via mixing current information and event history. Convergence proofs are provided. Gradients of one- and n-step value functions are monotonically nondecreasing and converge to the optimum. Two simulation case studies are presented for NSVGL to show their superior performance.																	2162-237X	2162-2388				APR	2020	31	4					1155	1169		10.1109/TNNLS.2019.2919338													
J								RBFNN-Based Data-Driven Predictive Iterative Learning Control for Nonaffine Nonlinear Systems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Predictive models; Convergence; Data models; Nonlinear systems; Estimation; Iterative algorithms; Load modeling; Data-driven control; dynamic linearization; iterative learning control (ILC); nonrepetitive disturbances; radial basis function neural network (RBFNN)	FREE ADAPTIVE-CONTROL; REPETITIVE CONTROL; ALGORITHM; DISTURBANCES; NETWORKS; DESIGN; ILC	In this paper, a novel data-driven predictive iterative learning control (DDPILC) scheme based on a radial basis function neural network (RBFNN) is proposed for a class of repeatable nonaffine nonlinear discrete-time systems subjected to nonrepetitive external disturbances. First, by utilizing the dynamic linearization technique (DLT) with a newly introduced and unknown system parameter pseudopartial derivative (PPD) and designing a new RBFNN estimation algorithm along the iterative learning axis for addressing the unknown PPD and the unknown nonrepetitive external disturbances, a data-driven prediction model is established. It is theoretically shown that by constructing a composite energy function (CEF) with respect to the modeling error for the first time, the convergence of the modeling error via the proposed DLT-based RBFNN modeling method can be guaranteed, and the convergence speed is tunable. Then, a DDPILC with a disturbance compensation term is designed, and the convergence of the tracking control error is analyzed. Finally, simulations of a train operation system reveal that even if the train suffers from randomly varying load disturbances and nonlinear running resistance, the proposed scheme can make both the modeling error and the tracking control error decrease successively with increasing operation number.																	2162-237X	2162-2388				APR	2020	31	4					1170	1182		10.1109/TNNLS.2019.2919441													
J								Error Bounds for Piecewise Smooth and Switching Regression	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Chaining; covering number; guaranteed risk; learning theory; Rademacher complexity; regression	LINEAR-REGRESSION; COMPLEXITY; IDENTIFICATION; MIXTURES; OPTIMIZATION; CONVERGENCE; ALGORITHM	This paper deals with regression problems, in which the nonsmooth target is assumed to switch between different operating modes. Specifically, piecewise smooth (PWS) regression considers target functions switching deterministically via a partition of the input space, while switching regression considers arbitrary switching laws. This paper derives generalization error bounds in these two settings by following the approach based on Rademacher complexities. For PWS regression, our derivation involves a chaining argument and a decomposition of the covering numbers of PWS classes in terms of the ones of their component function classes and the capacity of the classifier partitioning the input space. This yields error bounds with a radical dependence on the number of modes. For switching regression, the decomposition can be performed directly at the level of the Rademacher complexities, which yields bounds with a linear dependence on the number of modes. By using once more chaining and decomposition at the level of covering numbers, we show how to recover a radical dependence. Examples of applications are given in particular for PWS and swichting regression with linear and kernel-based component functions.																	2162-237X	2162-2388				APR	2020	31	4					1183	1195		10.1109/TNNLS.2019.2919444													
J								Performance Enhancement of Learning Tracking Systems Over Fading Channels With Multiplicative and Additive Randomness	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Decreasing gain sequence; fading channels; learning control; moving-average-operator; randomness	RESOURCE-ALLOCATION; DISTRIBUTED DELAYS; MULTIPLE-ACCESS; STABILIZATION; NONLINEARITIES	This paper applies learning control to repetitive systems over fading channels at both output and input sides to improve tracking performance without applying restrictive fading conditions. Both multiplicative and additive randomness of the fading channel are addressed, and the effects of fading communication on the data are carefully analyzed. A decreasing gain sequence and a moving-average operator are introduced to modify the generic learning control algorithm to reduce the fading effect and improve control system performance. Results reveal that the tracking error converges to zero in the mean-square sense as the iteration number increases. Illustrative simulations are presented to verify the theoretical results.																	2162-237X	2162-2388				APR	2020	31	4					1196	1210		10.1109/TNNLS.2019.2919510													
J								Projective Synchronization of Delayed Neural Networks With Mismatched Parameters and Impulsive Effects	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Synchronization; Delays; Biological neural networks; Neurons; Convergence; Learning systems; Impulsive control; mixed time-varying delays; neural networks; parameter mismatch; projective synchronization	TIME-VARYING DELAYS; DYNAMICAL NETWORKS; EXPONENTIAL SYNCHRONIZATION; STABILITY; PHASE; ARRAY	In this paper, the impulsive effects on projective synchronization between the parameter mismatched neural networks with mixed time-varying delays have been analyzed. Since complete synchronization is not possible due to the existence of parameter mismatch and projective factor, a drive has been taken to achieve the weak projective synchronization of different neural networks under impulsive control strategies. Through the use of matrix measure technique and the extended comparison principle based on the formula of variation of parameters for mixed time-varying delayed impulsive systems, sufficient criteria have been derived for exponential convergence of the networks under the effects of extensive range of impulse. Instead of upper or lower bound of the impulsive interval, the concept of the average impulsive interval is applied to estimate the number of impulsive points in an interval. The concept of calculus is applied for optimizing the synchronization error bounds which are obtained because of different ranges of impulse. Finally, the numerical simulations for various impulsive ranges for different cases are presented graphically to validate the efficiency of the theoretical results.																	2162-237X	2162-2388				APR	2020	31	4					1211	1221		10.1109/TNNLS.2019.2919560													
J								Direct Adaptive Preassigned Finite-Time Control With Time-Delay and Quantized Input Using Neural Network	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Direct adaptive control; neural networks (NNs); practically preassigned finite-time stability (PPFTS); quantized control; time delays	BARRIER LYAPUNOV FUNCTIONS; NONLINEAR-SYSTEMS; TRACKING CONTROL; PRESCRIBED PERFORMANCE; FEEDBACK-SYSTEMS; STABILIZATION	This paper investigates an adaptive finite-time control (FTC) problem for a class of strict-feedback nonlinear systems with both time-delays and quantized input from a new point of view. First, a new concept, called preassigned finite-time performance function (PFTF), is defined. Then, another novel notion, called practically preassigned finite-time stability (PPFTS), is introduced. With PFTF and PPFTS in hand, a novel sufficient condition of the FTC is given by using the neural network (NN) control and direct adaptive backstepping technique, which is different from the existing results. In addition, a modified barrier function is first introduced in this work. Moreover, this work is first to focus on the FTC for the situation that the time-delay and quantized input simultaneously exist in the nonlinear systems. Finally, simulation results are carried out to illustrate the effectiveness of the proposed scheme.																	2162-237X	2162-2388				APR	2020	31	4					1222	1231		10.1109/TNNLS.2019.2919577													
J								Event-Based Dissipative Analysis for Discrete Time-Delay Singular Jump Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Dissipativity; event-based communication technique; Markovian jump parameters; singular neural networks; time-varying delays	SLIDING MODE CONTROL; H-INFINITY; MARKOVIAN JUMP; SYSTEMS; STABILITY; SYNCHRONIZATION	This paper investigates the event-triggered dissipative filtering issue for discrete-time singular neural networks with time-varying delays and Markovian jump parameters. Via event-triggered communication technique, a singular jump neural network (SJNN) model of network-induced delays is first given, and sufficient criteria are then provided to guarantee that the resulting augmented SJNN is stochastically admissible and strictly stochastically dissipative (SASSD) with respect to (X-iota, Y-iota, Z(iota), delta) by using slack matrix scheme. Furthermore, employing filter equivalent technique, codesigned filter gains, and event-triggered matrices are derived to make sure that the augmented SJNN model is SASSD with respect to (X-iota, Y-iota, Z(iota), delta). An example is also given to illustrate the effectiveness of the proposed method.																	2162-237X	2162-2388				APR	2020	31	4					1232	1241		10.1109/TNNLS.2019.2919585													
J								Completely Automated CNN Architecture Design Based on Blocks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Automatic architecture design; convolutional neural networks (CNNs); evolutionary deep learning; genetic algorithms (GAs); neural networks	NEURAL-NETWORKS; OPTIMIZATION; ALGORITHM	The performance of convolutional neural networks (CNNs) highly relies on their architectures. In order to design a CNN with promising performance, extensive expertise in both CNNs and the investigated problem domain is required, which is not necessarily available to every interested user. To address this problem, we propose to automatically evolve CNN architectures by using a genetic algorithm (GA) based on ResNet and DenseNet blocks. The proposed algorithm is completely automatic in designing CNN architectures. In particular, neither preprocessing before it starts nor postprocessing in terms of CNNs is needed. Furthermore, the proposed algorithm does not require users with domain knowledge on CNNs, the investigated problem, or even GAs. The proposed algorithm is evaluated on the CIFAR10 and CIFAR100 benchmark data sets against 18 state-of-the-art peer competitors. Experimental results show that the proposed algorithm outperforms the state-of-the-art CNNs hand-crafted and the CNNs designed by automatic peer competitors in terms of the classification performance and achieves a competitive classification accuracy against semiautomatic peer competitors. In addition, the proposed algorithm consumes much less computational resource than most peer competitors in finding the best CNN architectures.																	2162-237X	2162-2388				APR	2020	31	4					1242	1254		10.1109/TNNLS.2019.2919608													
J								Online Model-Free n-Step HDP With Stability Analysis	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Mathematical model; Stability analysis; Dynamic programming; Programming; Training; Computer architecture; Learning systems; Adaptive dynamic programming (ADP); action-dependent (AD) heuristic dynamic programming (ADHDP); lambda-return; Lyapunov stability; uniformly ultimately bounded (UUB)	BACKPROPAGATION; REPRESENTATION	Because of a powerful temporal-difference (TD) with lambda [TD( lambda )] learning method, this paper presents a novel n-step adaptive dynamic programming (ADP) architecture that combines TD( lambda ) with regular TD learning for solving optimal control problems with reduced iterations. In contrast with a backward view learning of TD( lambda ) that is required an extra parameter named eligibility traces to update at the end of each episode (offline training), the new design in this paper has forward view learning, which is updated at each time step (online training) without needing the eligibility trace parameter in various applications without mathematical models. Therefore, the new design is called the online model-free n-step action-dependent (AD) heuristic dynamic programming [NSHDP( lambda )]. NSHDP( lambda ) has three neural networks: the critic network (CN) with regular one-step TD [TD(0)], the CN with n-step TD learning [or TD( lambda )], and the actor network (AN). Because the forward view learning does not require any extra eligibility traces associated with each state, the NSHDP( lambda ) architecture has low computational costs and is memory efficient. Furthermore, the stability is proven for NSHDP( lambda ) under certain conditions by using Lyapunov analysis to obtain the uniformly ultimately bounded (UUB) property. We compare the results with the performance of HDP and traditional action-dependent HDP( lambda ) [ADHDP( lambda )] with different lambda values. Moreover, a complex nonlinear system and 2-D maze problem are two simulation benchmarks in this paper, and the third one is an inverted pendulum simulation benchmark, which is presented in the supplemental material part of this paper. NSHDP( lambda ) performance is examined and compared with other ADP methods.																	2162-237X	2162-2388				APR	2020	31	4					1255	1269		10.1109/TNNLS.2019.2919614													
J								Event-Triggered Neural Control of Nonlinear Systems With Rate-Dependent Hysteresis Input Based on a New Filter	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Backstepping; event-triggered; networked control systems (NCSs); neural network (NN); rate-dependent nonlinearity	OUTPUT-FEEDBACK CONTROL; BACKSTEPPING FUZZY CONTROL; VARYING DELAY SYSTEMS; SLIDING-MODE CONTROL; LARGE-SCALE SYSTEMS; ADAPTIVE-CONTROL; TRACKING CONTROL; ACTUATOR FAILURES; STATE-FEEDBACK; COMPENSATION	In controlling nonlinear uncertain systems, compensating for rate-dependent hysteresis nonlinearity is an important, yet challenging problem in adaptive control. In fact, it can be illustrated through simulation examples that instability is observed when existing control methods in canceling hysteresis nonlinearities are applied to the networked control systems (NCSs). One control difficulty that obstructs these methods is the design conflict between the quantized networked control signal and the rate-dependent hysteresis characteristics. So far, there is still no solution to this problem. In this paper, we consider the event-triggered control for NCSs subject to actuator rate-dependent hysteresis and failures. A new second-order filter is proposed to overcome the design conflict and used for control design. With the incorporation of the filter, a novel adaptive control strategy is developed from a neural network technique and a modified backstepping recursive design. It is proved that all the control signals are semiglobally uniformly ultimately bounded and the tracking error will converge to a tunable residual around zero.																	2162-237X	2162-2388				APR	2020	31	4					1270	1284		10.1109/TNNLS.2019.2919641													
J								Training Spiking Neural Networks for Cognitive Tasks: A Versatile Framework Compatible With Various Temporal Codes	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neurons; Computational modeling; Biological neural networks; Task analysis; Biological system modeling; Training; Synapses; Neural dynamics; sparse coding; spiking neural network (SNN); supervised learning; temporal code	ERROR-BACKPROPAGATION; EFFICIENT; PROPAGATION; DYNAMICS; APPROXIMATION; COMMUNICATION; ALGORITHM; NEURONS; MODELS; CORTEX	Recent studies have demonstrated the effectiveness of supervised learning in spiking neural networks (SNNs). A trainable SNN provides a valuable tool not only for engineering applications but also for theoretical neuroscience studies. Here, we propose a modified SpikeProp learning algorithm, which ensures better learning stability for SNNs and provides more diverse network structures and coding schemes. Specifically, we designed a spike gradient threshold rule to solve the well-known gradient exploding problem in SNN training. In addition, regulation rules on firing rates and connection weights are proposed to control the network activity during training. Based on these rules, biologically realistic features such as lateral connections, complex synaptic dynamics, and sparse activities are included in the network to facilitate neural computation. We demonstrate the versatility of this framework by implementing three well-known temporal codes for different types of cognitive tasks, namely, handwritten digit recognition, spatial coordinate transformation, and motor sequence generation. Several important features observed in experimental studies, such as selective activity, excitatory-inhibitory balance, and weak pairwise correlation, emerged in the trained model. This agreement between experimental and computational results further confirmed the importance of these features in neural function. This work provides a new framework, in which various neural behaviors can be modeled and the underlying computational mechanisms can be studied.																	2162-237X	2162-2388				APR	2020	31	4					1285	1296		10.1109/TNNLS.2019.2919662													
J								Adaptive Global Sliding-Mode Control for Dynamic Systems Using Double Hidden Layer Recurrent Neural Network Structure	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Double hidden layer recurrent neural network (DHLRNN); global sliding-mode control (GSMC); single hidden layer neural network (SHLNN); single hidden layer recurrent neural network (SHLRNN)	TRACKING; ROBUST	In this paper, a full-regulated neural network (NN) with a double hidden layer recurrent neural network (DHLRNN) structure is designed, and an adaptive global sliding-mode controller based on the DHLRNN is proposed for a class of dynamic systems. Theoretical guidance and adaptive adjustment mechanism are established to set up the base width and central vector of the Gaussian function in the DHLRNN structure, where six sets of parameters can be adaptively stabilized to their best values according to different inputs. The new DHLRNN can improve the accuracy and generalization ability of the network, reduce the number of network weights, and accelerate the network training speed due to the strong fitting and presentation ability of two-layer activation functions compared with a general NN with a single hidden layer. Since the neurons of input layer can receive signals which come back from the neurons of output layer in the output feedback neural structure, it can possess associative memory and rapid system convergence, achieving better approximation and superior dynamic capability. Simulation and experiment on an active power filter are carried out to indicate the excellent static and dynamic performances of the proposed DHLRNN-based adaptive global sliding-mode controller, verifying its best approximation performance and the most stable internal state compared with other schemes.																	2162-237X	2162-2388				APR	2020	31	4					1297	1309		10.1109/TNNLS.2019.2919676													
J								Multi-Objective Evolutionary Federated Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Data models; Biological neural networks; Servers; Neurons; Distributed databases; Communication cost; deep neural networks; federated learning; multi-objective evolutionary optimization; neural architecture search	GENETIC ALGORITHM	Federated learning is an emerging technique used to prevent the leakage of private information. Unlike centralized learning that needs to collect data from users and store them collectively on a cloud server, federated learning makes it possible to learn a global model while the data are distributed on the users' devices. However, compared with the traditional centralized approach, the federated setting consumes considerable communication resources of the clients, which is indispensable for updating global models and prevents this technique from being widely used. In this paper, we aim to optimize the structure of the neural network models in federated learning using a multi-objective evolutionary algorithm to simultaneously minimize the communication costs and the global model test errors. A scalable method for encoding network connectivity is adapted to federated learning to enhance the efficiency in evolving deep neural networks. Experimental results on both multilayer perceptrons and convolutional neural networks indicate that the proposed optimization method is able to find optimized neural network models that can not only significantly reduce communication costs but also improve the learning performance of federated learning compared with the standard fully connected neural networks.																	2162-237X	2162-2388				APR	2020	31	4					1310	1322		10.1109/TNNLS.2019.2919699													
J								Redundancy and Attention in Convolutional LSTM for Gesture Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Logic gates; Spatiotemporal phenomena; Redundancy; Convolutional codes; Gesture recognition; Kernel; Computer architecture; Attention; convolutional LSTM (ConvLSTM); gesture recognition; redundancy		Convolutional long short-term memory (ConvLSTM) networks have been widely used for action/gesture recognition, and different attention mechanisms have also been embedded into ConvLSTM networks. This paper explores the redundancy of spatial convolutions and the effects of the attention mechanism in ConvLSTM, based on our previous gesture recognition architectures that combine the 3-D convolutional neural network (CNN) and ConvLSTM. Depthwise separable, group, and shuffle convolutions are used to replace the convolutional structures in ConvLSTM for the redundancy analysis. In addition, four ConvLSTM variants are derived for attention analysis: 1) by removing the convolutional structures of the three gates in ConvLSTM; 2) by applying the attention mechanism on the ConvLSTM input; and 3) by reconstructing the input and 4) output gates with the modified channelwise attention mechanism. Evaluation results demonstrate that the spatial convolutions in the three gates scarcely contribute to the spatiotemporal feature fusion and that the attention mechanisms embedded into the input and output gates cannot improve the feature fusion. In other words, ConvLSTM mainly contributes to the temporal fusion along with the recurrent steps to learn long-term spatiotemporal features when taking spatial or spatiotemporal features as input. A new LSTM variant is derived on this basis in which the convolutional structures are embedded only into the input-to-state transition of LSTM. The code of the LSTM variants is publicly available. (1) (1) https://github.com/GuangmingZhu/ConvLSTMForGR.																	2162-237X	2162-2388				APR	2020	31	4					1323	1335		10.1109/TNNLS.2019.2919764													
J								Multilinear Multitask Learning by Rank-Product Regularization	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Multilinear; multitask learning (MTL); rank product; tensor sparsity		Multilinear multitask learning (MLMTL) considers an MTL problem in which tasks are arranged by multiple indices. By exploiting the higher order correlations among the tasks, MLMTL is expected to improve the performance of traditional MTL, which only considers the first-order correlation across all tasks, e.g., low-rank structure of the coefficient matrix. The key to MLMTL is designing a rational regularization term to represent the latent correlation structure underlying the coefficient tensor instead of matrix. In this paper, we propose a new MLMTL model by employing the rank-product regularization term in the objective, which on one hand can automatically rectify the weights along all its tensor modes and on the other hand have an explicit physical meaning. By using this regularization, the intrinsic high-order correlations among tasks can be more precisely described, and thus, the overall performance of all tasks can be improved. To solve the resulted optimization model, we design an efficient algorithm by applying the alternating direction method of multipliers (ADMM). We also analyze the convergence and show that the proposed algorithm, with certain restriction, is asymptotically regular. Experiments on both synthetic and real data sets substantiate the superiority of the proposed method beyond the existing MLMTL methods in terms of accuracy and efficiency.																	2162-237X	2162-2388				APR	2020	31	4					1336	1350		10.1109/TNNLS.2019.2919774													
J								Multiple Kernel Clustering With Neighbor-Kernel Subspace Segmentation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Kernel method; multiple kernel learning; neighbor kernel; subspace segmentation	ALGORITHM	Multiple kernel clustering (MKC) has been intensively studied during the last few decades. Even though they demonstrate promising clustering performance in various applications, existing MKC algorithms do not sufficiently consider the intrinsic neighborhood structure among base kernels, which could adversely affect the clustering performance. In this paper, we propose a simple yet effective neighbor-kernel-based MKC algorithm to address this issue. Specifically, we first define a neighbor kernel, which can be utilized to preserve the block diagonal structure and strengthen the robustness against noise and outliers among base kernels. After that, we linearly combine these base neighbor kernels to extract a consensus affinity matrix through an exact-rank-constrained subspace segmentation. The naturally possessed block diagonal structure of neighbor kernels better serves the subsequent subspace segmentation, and in turn, the extracted shared structure is further refined through subspace segmentation based on the combined neighbor kernels. In this manner, the above two learning processes can be seamlessly coupled and negotiate with each other to achieve better clustering. Furthermore, we carefully design an efficient iterative optimization algorithm with proven convergence to address the resultant optimization problem. As a by-product, we reveal an interesting insight into the exact-rank constraint in ridge regression by careful theoretical analysis: it back-projects the solution of the unconstrained counterpart to its principal components. Comprehensive experiments have been conducted on several benchmark data sets, and the results demonstrate the effectiveness of the proposed algorithm.																	2162-237X	2162-2388				APR	2020	31	4					1351	1362		10.1109/TNNLS.2019.2919900													
J								Evolving Local Plasticity Rules for Synergistic Learning in Echo State Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Classification; covariance matrix adaptation evolution strategy (ES); echo state networks (ESNs); local synaptic plasticity; regression; synergistic learning	NEURAL PLASTICITY; OPTIMIZATION; DESIGN; CLASSIFICATION; ENERGY; MODEL	Existing synaptic plasticity rules for optimizing the connections between neurons within the reservoir of echo state networks (ESNs) remain to be global in that the same type of plasticity rule with the same parameters is applied to all neurons. However, this is biologically implausible and practically inflexible for learning the structures in the input signals, thereby limiting the learning performance of ESNs. In this paper, we propose to use local plasticity rules that allow different neurons to use different types of plasticity rules and different parameters, which are achieved by optimizing the parameters of the local plasticity rules using the evolution strategy (ES) with covariance matrix adaptation (CMA-ES). We show that evolving neural plasticity will result in a synergistic learning of different plasticity rules, which plays an important role in improving the learning performance. Meanwhile, we show that the local plasticity rules can effectively alleviate synaptic interferences in learning the structure in sensory inputs. The proposed local plasticity rules are compared with a number of the state-of-the-art ESN models and the canonical ESN using a global plasticity rule on a set of widely used prediction and classification benchmark problems to demonstrate its competitive learning performance.																	2162-237X	2162-2388				APR	2020	31	4					1363	1374		10.1109/TNNLS.2019.2919903													
J								Composite Neural Learning-Based Nonsingular Terminal Sliding Mode Control of MEMS Gyroscopes	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Micromechanical devices; Gyroscopes; Artificial neural networks; Estimation; Sliding mode control; Uncertainty; Damping; Composite learning; MEMS gyroscopes; neural network; nonsingular terminal sliding mode control; time-varying barrier Lyapunov function	UNCERTAIN NONLINEAR-SYSTEMS; DYNAMIC SURFACE CONTROL; FINITE-TIME CONTROL; ADAPTIVE-CONTROL; NETWORK	The efficient driving control of MEMS gyroscopes is an attractive way to improve the precision without hardware redesign. This paper investigates the sliding mode control (SMC) for the dynamics of MEMS gyroscopes using neural networks (NNs). Considering the existence of the dynamics uncertainty, the composite neural learning is constructed to obtain higher tracking precision using the serial-parallel estimation model (SPEM). Furthermore, the nonsingular terminal SMC (NTSMC) is proposed to achieve finite-time convergence. To obtain the prescribed performance, a time-varying barrier Lyapunov function (BLF) is introduced to the control scheme. Through simulation tests, it is observed that under the BLF-based NTSMC with composite learning design, the tracking precision of MEMS gyroscopes is highly improved.																	2162-237X	2162-2388				APR	2020	31	4					1375	1386		10.1109/TNNLS.2019.2919931													
J								Hybrid Classifier Ensemble for Imbalanced Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimization; Learning systems; Training; Sampling methods; Clustering algorithms; Probabilistic logic; Bagging; Cost-sensitive method; ensemble classifier; imbalanced learning; undersampling	ALGORITHM; OPTIMIZATION; INSIGHT; SMOTE	The class imbalance problem has become a leading challenge. Although conventional imbalance learning methods are proposed to tackle this problem, they have some limitations: 1) undersampling methods suffer from losing important information and 2) cost-sensitive methods are sensitive to outliers and noise. To address these issues, we propose a hybrid optimal ensemble classifier framework that combines density-based undersampling and cost-effective methods through exploring state-of-the-art solutions using multi-objective optimization algorithm. Specifically, we first develop a density-based undersampling method to select informative samples from the original training data with probability-based data transformation, which enables to obtain multiple subsets following a balanced distribution across classes. Second, we exploit the cost-sensitive classification method to address the incompleteness of information problem via modifying weights of misclassified minority samples rather than the majority ones. Finally, we introduce a multi-objective optimization procedure and utilize connections between samples to self-modify the classification result using an ensemble classifier framework. Extensive comparative experiments conducted on real-world data sets demonstrate that our method outperforms the majority of imbalance and ensemble classification approaches.																	2162-237X	2162-2388				APR	2020	31	4					1387	1400		10.1109/TNNLS.2019.2920246													
J								Adaptive Decentralized Neural Network Tracking Control for Uncertain Interconnected Nonlinear Systems With Input Quantization and Time Delay	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Quantization (signal); Adaptive systems; Neural networks; Interconnected systems; Nonlinear systems; Control systems; Stability analysis; Decentralized control; input quantization; interconnected system; neural network; sliding-mode differentiator; time-delay system; tracking control	LARGE-SCALE SYSTEMS; FEEDBACK-CONTROL; POWER-SYSTEM; STABILIZATION; OPTIMIZATION	This study investigates the problem of adaptive decentralized tracking control for a class of interconnected nonlinear systems with input quantization, unknown function, and time-delay, where the time-delay and interconnection terms are supposed to be bounded by some completely unknown functions. An adaptive decentralized tracking controller is constructed via the backstepping method and neural network technique, where a sliding-mode differentiator is presented to estimate the derivative of the virtual control law and reduce the complexity of the control scheme. On the basis of Lyapunov analysis scheme and graph theory, all the signals of the closed-loop system are uniformly ultimately bounded. Finally, an application example of an inverted pendulum system is given to demonstrate the effectiveness of the developed methods.																	2162-237X	2162-2388				APR	2020	31	4					1401	1409		10.1109/TNNLS.2019.2919697													
J								Riemannian Curvature of Deep Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neural networks; Manifolds; Measurement; Geometry; Mathematical model; Indexes; Curvature; deep learning; machine learning; neural networks; Riemannian geometry		We analyze deep neural networks using the theory of Riemannian geometry and curvature. The objective is to gain insight into how Riemannian geometry can characterize and predict the trained behavior of neural networks. We define a method for calculating Riemann and Ricci curvature tensors, and Ricci scalar curvature values for a trained neural net, in such a way that the output classifier softmax values are related to the input transformations, through the curvature equations. We also measure these curvature tensors experimentally for different networks which are pretrained with stochastic gradient descent and offer a way of visualizing and understanding the measurements to gain insight into the effect curvature has on behavior the neural networks locally, and possibly predict their behavior for different transformations of the test data. We also analyze the effect of variation in depth of the neural networks as well as how it behaves for different choices of data set.																	2162-237X	2162-2388				APR	2020	31	4					1410	1416		10.1109/TNNLS.2019.2919705													
J								Dual Adversarial Autoencoders for Clustering	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Mutual information; Training; Clustering methods; Clustering algorithms; Gallium nitride; Generative adversarial networks; Task analysis; AAE; clustering; deep generative models; latent variable; mutual information regularization		As a powerful approach for exploratory data analysis, unsupervised clustering is a fundamental task in computer vision and pattern recognition. Many clustering algorithms have been developed, but most of them perform unsatisfactorily on the data with complex structures. Recently, adversarial autoencoder (AE) (AAE) shows effectiveness on tackling such data by combining AE and adversarial training, but it cannot effectively extract classification information from the unlabeled data. In this brief, we propose dual AAE (Dual-AAE) which simultaneously maximizes the likelihood function and mutual information between observed examples and a subset of latent variables. By performing variational inference on the objective function of Dual-AAE, we derive a new reconstruction loss which can be optimized by training a pair of AEs. Moreover, to avoid mode collapse, we introduce the clustering regularization term for the category variable. Experiments on four benchmarks show that Dual-AAE achieves superior performance over state-of-the-art clustering methods. In addition, by adding a reject option, the clustering accuracy of Dual-AAE can reach that of supervised CNN algorithms. Dual-AAE can also be used for disentangling style and content of images without using supervised information.																	2162-237X	2162-2388				APR	2020	31	4					1417	1424		10.1109/TNNLS.2019.2919948													
J								A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Videos; Hidden Markov models; Task analysis; Training; Supervised learning; Training data; Computational modeling; Weakly supervised learning; temporal action segmentation; temporal action alignment; action recognition	SEMI-MARKOV MODEL; ACTION RECOGNITION; STATE DURATION	Action recognition has become a rapidly developing research field within the last decade. But with the increasing demand for large scale data, the need of hand annotated data for the training becomes more and more impractical. One way to avoid frame-based human annotation is the use of action order information to learn the respective action classes. In this context, we propose a hierarchical approach to address the problem of weakly supervised learning of human actions from ordered action labels by structuring recognition in a coarse-to-fine manner. Given a set of videos and an ordered list of the occurring actions, the task is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. We address this problem by combining a framewise RNN model with a coarse probabilistic inference. This combination allows for the temporal alignment of long sequences and thus, for an iterative training of both elements. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes as well as by the introduction of a regularizing length prior. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.																	0162-8828	1939-3539				APR 1	2020	42	4					765	779		10.1109/TPAMI.2018.2884469													
J								Automated Video Face Labelling for Films and TV Material	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Face; Target tracking; Labeling; TV; Visualization; Pattern analysis; Automatic face labelling; face tracking; deep learning		The objective of this work is automatic labelling of characters in TV video and movies, given weak supervisory information provided by an aligned transcript. We make five contributions: (i) a new strategy for obtaining stronger supervisory information from aligned transcripts; (ii) an explicit model for classifying background characters, based on their face-tracks; (iii) employing new ConvNet based face features, and (iv) a novel approach for labelling all face tracks jointly using linear programming. Each of these contributions delivers a boost in performance, and we demonstrate this on standard benchmarks using tracks provided by authors of prior work. As a fifth contribution, we also investigate the generalisation and strength of the features and classifiers by applying them "in the raw" on new video material where no supervisory information is used. In particular, to provide high quality tracks on those material, we propose efficient track classifiers to remove false positive tracks by the face tracker. Overall we achieve a dramatic improvement over the state of the art on both TV series and film datasets, and almost saturate performance on some benchmarks.																	0162-8828	1939-3539				APR 1	2020	42	4					780	792		10.1109/TPAMI.2018.2889831													
J								Baselines Extraction from Curved Document Images via Slope Fields Recovery	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Estimation; Image segmentation; Layout; Distortion; Strips; Image quality; Degradation; Document image processing; curved baselines extraction; slope fields recovery; geometric distortion rectification	TEXT; SEGMENTATION; RECTIFICATION; ROBUST; SET	Baselines estimation is a critical preprocessing step for many tasks of document image processing and analysis. The problem is very challenging due to arbitrarily complicated page layouts and various types of image quality degradations. This paper proposes a method based on slope fields recovery for curved baseline extraction from a distorted document image captured by a hand-held camera. Our method treats the curved baselines as the solution curves of an ordinary differential equation defined on a slope field. By assuming the page shape is a smooth and developable surface, we investigate a type of intrinsic geometric constraints of baselines to estimate the latent slope field. The curved baselines are finally obtained by solving an ordinary differential equation through the Euler method. Unlike the traditional text-lines based methods, our method is free from text-lines detection and segmentation. It can exploit multiple visual cues other than horizontal text-lines available in images for baselines extraction and is quite robust to document scripts, various types of image quality degradation (e.g., image distortion, blur and non-uniform illumination), large areas of non-textual objects and complex page layouts. Extensive experiments on synthetic and real-captured document images are implemented to evaluate the performance of the proposed method.																	0162-8828	1939-3539				APR 1	2020	42	4					793	808		10.1109/TPAMI.2018.2886900													
J								Deep Self-Evolution Clustering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Task analysis; Unsupervised learning; Training; Clustering methods; Pattern analysis; Clustering; deep self-evolution clustering; self-evolution clustering training; deep unsupervised learning	IMAGE RETRIEVAL; REPRESENTATIONS	Clustering is a crucial but challenging task in pattern analysis and machine learning. Existing methods often ignore the combination between representation learning and clustering. To tackle this problem, we reconsider the clustering task from its definition to develop Deep Self-Evolution Clustering (DSEC) to jointly learn representations and cluster data. For this purpose, the clustering task is recast as a binary pairwise-classification problem to estimate whether pairwise patterns are similar. Specifically, similarities between pairwise patterns are defined by the dot product between indicator features which are generated by a deep neural network (DNN). To learn informative representations for clustering, clustering constraints are imposed on the indicator features to represent specific concepts with specific representations. Since the ground-truth similarities are unavailable in clustering, an alternating iterative algorithm called Self-Evolution Clustering Training (SECT) is presented to select similar and dissimilar pairwise patterns and to train the DNN alternately. Consequently, the indicator features tend to be one-hot vectors and the patterns can be clustered by locating the largest response of the learned indicator features. Extensive experiments strongly evidence that DSEC outperforms current models on twelve popular image, text and audio datasets consistently.																	0162-8828	1939-3539				APR 1	2020	42	4					809	823		10.1109/TPAMI.2018.2889949													
J								Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Graph and tree search strategies; artificial intelligence; information search and retrieval; information storage and retrieval; information technology and systems; search process; graphs and networks; data structures; nearest neighbor search; big data; approximate search; similarity search	NETWORKS	We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.																	0162-8828	1939-3539				APR 1	2020	42	4					824	836		10.1109/TPAMI.2018.2889473													
J								Extracting Geometric Structures in Images with Delaunay Point Processes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Kernel; Perturbation methods; Markov processes; Task analysis; Image segmentation; Three-dimensional displays; Monte Carlo methods; Spatial point process; delaunay triangulation; geometric structures; line network extraction; object contouring; image compression	LINEAR SPLINES; SEGMENTATION	We introduce Delaunay Point Processes, a framework for the extraction of geometric structures from images. Our approach simultaneously locates and groups geometric primitives (line segments, triangles) to form extended structures (line networks, polygons) for a variety of image analysis tasks. Similarly to traditional point processes, our approach uses Markov Chain Monte Carlo to minimize an energy that balances fidelity to the input image data with geometric priors on the output structures. However, while existing point processes struggle to model structures composed of inter-connected components, we propose to embed the point process into a Delaunay triangulation, which provides high-quality connectivity by construction. We further leverage key properties of the Delaunay triangulation to devise a fast Markov Chain Monte Carlo sampler. We demonstrate the flexibility of our approach on a variety of applications, including line network extraction, object contouring, and mesh-based image compression.																	0162-8828	1939-3539				APR 1	2020	42	4					837	850		10.1109/TPAMI.2018.2890586													
J								Hierarchical Bayesian Inverse Lighting of Portraits with a Virtual Light Stage	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Lighting; Face; Three-dimensional displays; Geometry; Solid modeling; Light sources; Estimation; Inverse lighting; 3D morphable model; single face image; virtual light stage; hierarchical Bayesian optimization; hyperparameters; generative model	FACE RECOGNITION; REFLECTANCE; IMAGE; DECOMPOSITION; MODELS; SKIN	From a single RGB image of an unknown face, taken under unknown conditions, we estimate a physically plausible lighting model. First, the 3D geometry and texture of the face are estimated by fitting a 3D Morphable Model to the 2D input. With this estimated 3D model and a Virtual Light Stage (VLS), we generate a gallery of images of the face with all the same conditions, but different lighting. We consider non-lambertian reflectance and non-convex geometry to handle more realistic illumination effects in complex lighting conditions. Our hierarchical Bayesian approach automatically suppresses inconsistencies between the model and the input. It estimates the RGB values for the light sources of a VLS to reconstruct the input face with the estimated 3D face model. We discuss the relevance of the hierarchical approach to this minimally constrained inverse rendering problem and show how the hyperparameters can be controlled to improve the results of the algorithm for complex effects, such as cast shadows. Our algorithm is a contribution to single image face modeling and analysis, provides information about the imaging condition and facilitates realistic reconstruction of the input image, relighting, lighting transfer and lighting design.																	0162-8828	1939-3539				APR 1	2020	42	4					865	879		10.1109/TPAMI.2019.2891638													
J								On Detection of Faint Edges in Noisy Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image edge detection; Noise measurement; Matched filters; Smoothing methods; Detection algorithms; Microscopy; Signal to noise ratio; Edge detection; fiber enhancement; multiscale methods; low signal-to-noise ratio; multiple hypothesis tests; microscopy images	COLOR; SEGMENTATION; ENHANCEMENT; BOUNDARIES; TRANSFORM	A fundamental question for edge detection in noisy images is how faint can an edge be and still be detected. In this paper we offer a formalism to study this question and subsequently introduce computationally efficient multiscale edge detection algorithms designed to detect faint edges in noisy images. In our formalism we view edge detection as a search in a discrete, though potentially large, set of feasible curves. First, we derive approximate expressions for the detection threshold as a function of curve length and the complexity of the search space. We then present two edge detection algorithms, one for straight edges, and the second for curved ones. Both algorithms efficiently search for edges in a large set of candidates by hierarchically constructing difference filters that match the curves traced by the sought edges. We demonstrate the utility of our algorithms in both simulations and applications involving challenging real images. Finally, based on these principles, we develop an algorithm for fiber detection and enhancement. We exemplify its utility to reveal and enhance nerve axons in light microscopy images.																	0162-8828	1939-3539				APR 1	2020	42	4					894	908		10.1109/TPAMI.2019.2892134													
J								Perspective-Adaptive Convolutions for Scene Parsing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Shape; Standards; Strain; Proposals; Convolutional neural networks; Training; Task analysis; Scene parsing; convolutional neural networks; perspective-adaptive convolutions; context adaptive biases		Many existing scene parsing methods adopt Convolutional Neural Networks with receptive fields of fixed sizes and shapes, which frequently results in inconsistent predictions of large objects and invisibility of small objects. To tackle this issue, we propose perspective-adaptive convolutions to acquire receptive fields of flexible sizes and shapes during scene parsing. Through adding a new perspective regression layer, we can dynamically infer the position-adaptive perspective coefficient vectors utilized to reshape the convolutional patches. Consequently, the receptive fields can be adjusted automatically according to the various sizes and perspective deformations of the objects in scene images. Our proposed convolutions are differentiable to learn the convolutional parameters and perspective coefficients in an end-to-end way without any extra training supervision of object sizes. Furthermore, considering that the standard convolutions lack contextual information and spatial dependencies, we propose a context adaptive bias to capture both local and global contextual information through average pooling on the local feature patches and global feature maps, followed by flexible attentive summing to the convolutional results. The attentive weights are position-adaptive and context-aware, and can be learned through adding an additional context regression layer. Experiments on Cityscapes and ADE20K datasets well demonstrate the effectiveness of the proposed methods.																	0162-8828	1939-3539				APR 1	2020	42	4					909	924		10.1109/TPAMI.2018.2890637													
J								Tensor Robust Principal Component Analysis with a New Tensor Nuclear Norm	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Principal component analysis; Sparse matrices; Matrix decomposition; Numerical models; Noise measurement; Convex functions; Tensor robust PCA; convex optimization; tensor nuclear norm; tensor singular value decomposition	FACTORIZATION; MATRICES	In this paper, we consider the Tensor Robust Principal Component Analysis (TRPCA) problem, which aims to exactly recover the low-rank and sparse components from their sum. Our model is based on the recently proposed tensor-tensor product (or t-product) [14]. Induced by the t-product, we first rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor average rank, and show that the tensor nuclear norm is the convex envelope of the tensor average rank within the unit ball of the tensor spectral norm. These definitions, their relationships and properties are consistent with matrix cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA problem by solving a convex program and provide the theoretical guarantee for the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA as a special case. Numerical experiments verify our results, and the applications to image recovery and background modeling problems demonstrate the effectiveness of our method.																	0162-8828	1939-3539				APR 1	2020	42	4					925	938		10.1109/TPAMI.2019.2891760													
J								Tracking-by-Fusion via Gaussian Process Regression Extended to Transfer Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Task analysis; Correlation; Target tracking; Probability distribution; Visualization; Collaboration; Visual tracking; Gaussian processes; correlation filters; transfer learning; tracking-by-fusion	VISUAL TRACKING; OBJECT TRACKING; NETWORKS	This paper presents a new Gaussian Processes (GPs)-based particle filter tracking framework. The framework non-trivially extends Gaussian process regression (GPR) to transfer learning, and, following the tracking-by-fusion strategy, integrates closely two tracking components, namely a GPs component and a CFs one. First, the GPs component analyzes and models the probability distribution of the object appearance by exploiting GPs. It categorizes the labeled samples into auxiliary and target ones, and explores unlabeled samples in transfer learning. The GPs component thus captures rich appearance information over object samples across time. On the other hand, to sample an initial particle set in regions of high likelihood through the direct simulation method in particle filtering, the powerful yet efficient correlation filters (CFs) are integrated, leading to the CFs component. In fact, the CFs component not only boosts the sampling quality, but also benefits from the GPs component, which provides re-weighted knowledge as latent variables for determining the impact of each correlation filter template from the auxiliary samples. In this way, the transfer learning based fusion enables effective interactions between the two components. Superior performance on four object tracking benchmarks (OTB-2015, Temple-Color, and VOT2015/2016), and in comparison with baselines and recent state-of-the-art trackers, has demonstrated clearly the effectiveness of the proposed framework.																	0162-8828	1939-3539				APR 1	2020	42	4					939	955		10.1109/TPAMI.2018.2889070													
J								Unsupervised Person Re-Identification by Deep Asymmetric Metric Embedding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Measurement; Cameras; Pattern matching; Distortion; Image color analysis; Feature extraction; Dictionaries; Unsupervised person re-identification; unsupervised metric learning; unsupervised deep learning; cross-view clustering; deep clustering		Person re-identification (Re-ID) aims to match identities across non-overlapping camera views. Researchers have proposed many supervised Re-ID models which require quantities of cross-view pairwise labelled data. This limits their scalabilities to many applications where a large amount of data from multiple disjoint camera views is available but unlabelled. Although some unsupervised Re-ID models have been proposed to address the scalability problem, they often suffer from the view-specific bias problem which is caused by dramatic variances across different camera views, e.g., different illumination, viewpoints and occlusion. The dramatic variances induce specific feature distortions in different camera views, which can be very disturbing in finding cross-view discriminative information for Re-ID in the unsupervised scenarios, since no label information is available to help alleviate the bias. We propose to explicitly address this problem by learning an unsupervised asymmetric distance metric based on cross-view clustering. The asymmetric distance metric allows specific feature transformations for each camera view to tackle the specific feature distortions. We then design a novel unsupervised loss function to embed the asymmetric metric into a deep neural network, and therefore develop a novel unsupervised deep framework named the DEep Clustering-based Asymmetric MEtric Learning (DECAMEL). In such a way, DECAMEL jointly learns the feature representation and the unsupervised asymmetric metric. DECAMEL learns a compact cross-view cluster structure of Re-ID data, and thus help alleviate the view-specific bias and facilitate mining the potential cross-view discriminative information for unsupervised Re-ID. Extensive experiments on seven benchmark datasets whose sizes span several orders show the effectiveness of our framework.																	0162-8828	1939-3539				APR 1	2020	42	4					956	973		10.1109/TPAMI.2018.2886878													
J								Weighted Manifold Alignment using Wave Kernel Signatures for Aligning Medical Image Datasets	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Manifolds; Kernel; Biomedical imaging; Pipelines; Laplace equations; Two dimensional displays; Manifold alignment; graph descriptor; wave kernel signature; magnetic resonance imaging; slice stacking	DIMENSIONALITY REDUCTION	Manifold alignment (MA) is a technique to map many high-dimensional datasets to one shared low-dimensional space. Here we develop a pipeline for using MA to reconstruct high-resolution medical images. We present two key contributions. First, we develop a novel MA scheme in which each high-dimensional dataset can be differently weighted preventing noisier or less informative data from corrupting the aligned embedding. We find that this generalisation improves performance in our experiments in both supervised and unsupervised MA problems. Second, we use the wave kernel signature as a graph descriptor for the unsupervised MA case finding that it significantly outperforms the current state-of-the-art methods and provides higher quality reconstructed magnetic resonance volumes than existing methods.																	0162-8828	1939-3539				APR 1	2020	42	4					988	997		10.1109/TPAMI.2019.2891600													
J								Denoising Autoencoders for Overgeneralization in Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Overgeneralization; fooling; autoencoder; open set recognition; open world recognition; 1-class recognition; confidence score; neural networks	SUPPORT; LEARN	Despite recent developments that allowed neural networks to achieve impressive performance on a variety of applications, these models are intrinsically affected by the problem of overgeneralization, due to their partitioning of the full input space into the fixed set of target classes used during training. Thus it is possible for novel inputs belonging to categories unknown during training or even completely unrecognizable to humans to fool the system into classifying them as one of the known classes, even with a high degree of confidence. This problem can lead to security problems in critical applications, and is closely linked to open set recognition and 1-class recognition. This paper presents a novel way to compute a confidence score using the reconstruction error of denoising autoencoders and shows how it can correctly identify the regions of the input space close to the training distribution. The proposed solution is tested on benchmarks of 'fooling', open set recognition and 1-class recognition constructed from the MNIST and Fashion-MNIST datasets.																	0162-8828	1939-3539				APR 1	2020	42	4					998	1004		10.1109/TPAMI.2019.2909876													
J								Efficient Graph Cut Optimization for Full CRFs with Quantized Edges	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image edge detection; Labeling; Inference algorithms; Computational modeling; Optimization methods; Approximation algorithms; Discrete optimization; graph cuts; fully connected CRFs	RANDOM-FIELDS; INFERENCE	Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Previous work develops efficient approximate optimization based on mean field inference, which is a local optimization method and can be far from the optimum. We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. To quantize edge weights, we partition the image into superpixels and assume that the weight of an edge between any two pixels depends only on the superpixels these pixels belong to. Our quantized edge CRF is an approximation to the Gaussian edge CRF, and gets closer to it as superpixel size decreases. Being an approximation, our model offers an intuition about the regularization properties of the Guassian edge Full-CRF. For efficient inference, we first consider the two-label case and develop an approximate method based on transforming the original problem into a smaller domain. Then we handle multi-label CRF by showing how to implement expansion moves. In both binary and multi-label cases, our solutions have significantly lower energy compared to that of mean field inference. We also show the effectiveness of our approach on semantic segmentation task.																	0162-8828	1939-3539				APR 1	2020	42	4					1005	1012		10.1109/TPAMI.2019.2906204													
J								Learning Raw Image Reconstruction-Aware Deep Image Compressors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image reconstruction; Image coding; Table lookup; Compressors; Transform coding; Cameras; Calibration; Image compression; radiometric calibration; raw image reconstruction; deep learning-based image compression	VISION	Deep learning-based image compressors are actively being explored in an effort to supersede conventional image compression algorithms, such as JPEG. Conventional and deep learning-based compression algorithms focus on minimizing image fidelity errors in the nonlinear standard RGB (sRGB) color space. However, for many computer vision tasks, the sensor's linear raw-RGB image is desirable. Recent work has shown that the original raw-RGB image can be reconstructed using only small amounts of metadata embedded inside the JPEG image [1]. However, [1] relied on the conventional JPEG encoding that is unaware of the raw-RGB reconstruction task. In this paper, we examine the ability of deep image compressors to be "aware" of the additional objective of raw reconstruction. Towards this goal, we describe a general framework that enables deep networks targeting image compression to jointly consider both image fidelity errors and raw reconstruction errors. We describe this approach in two scenarios: (1) the network is trained from scratch using our proposed joint loss, and (2) a network originally trained only for sRGB fidelity loss is later fine-tuned to incorporate our raw reconstruction loss. When compared to sRGB fidelity-only compression, our combined loss leads to appreciable improvements in PSNR of the raw reconstruction with only minor impact on sRGB fidelity as measured by MS-SSIM.																	0162-8828	1939-3539				APR 1	2020	42	4					1013	1019		10.1109/TPAMI.2019.2903062													
J								Estimating and Controlling Overlap in Gaussian Mixtures for Clustering Methods Evaluation	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Artificial data; high-dimensional data; Gaussian models; clustering dataset	MODEL TESTS; FUZZY; SELECTION; SIZE; MULTIVARIATE; MEMBERSHIP; ALGORITHM; INDEX	The ad hoc nature of the clustering methods makes simulated data paramount in assessing the performance of clustering methods. Real datasets could be used in the evaluation of clustering methods with the major drawback of missing the assessment of many test scenarios. In this paper, we propose a formal quantification of component overlap. This quantification is derived from a set of theorems which allow us to derive an automatic method for artificial data generation. We also derive a method to estimate parameters of existing models and to evaluate the results of other approaches. Automatic estimation of the overlap rate can also be used as an unsupervised learning approach in data mining to determine the parameters of mixture models from actual observations.																	0218-4885	1793-6411				APR	2020	28	2					183	211		10.1142/S0218488520500087													
J								Improving Classification Accuracy Using Hybrid of Extreme Learning Machine and Artificial Algae Algorithm with Multi-Light Source	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Machine learning; classification; extreme learning machine; meta-heuristic optimization; artificial algae algorithm	NUMERICAL OPTIMIZATION; BEE COLONY	Among other machine learning techniques, the extreme learning machine has evidently proved its diagnostic accuracy on many cases in medical domain. Its accuracy mainly depends on the optimal parameters that are used in training. The proposed work is based on optimizing the extreme learning machine using the recently proposed meta-heuristic optimization technique named artificial algae algorithm with multi-light source. In this work, two experiments are conducted using four binary classification datasets related to medical domain. The feasible number of hidden neurons is found from the first experiment using relevant performance parameters. In the second experiment, the classifier with feasible number of hidden neurons is further evaluated with the ten-fold cross-validation method based on its computation time and classification accuracy. In both the experiments, the proposed classifier performance compared with that of other four similar hybrid approaches. It is also statistically compared using Friedman test and Wilcoxon signed rank test based on the area under curve and accuracy values respectively. It is found that the proposed classifier produces better results than the other classifiers.																	0218-4885	1793-6411				APR	2020	28	2					213	236		10.1142/S0218488520500099													
J								Kappa Regression: An Alternative to Logistic Regression	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Kappa function; logistic function; kappa regression; logistic regression; fuzzy theory; statistics		In this study, a new regression method called Kappa regression is introduced to model conditional probabilities. The regression function is based on Dombi's Kappa function, which is well known in fuzzy theory. Here, we discuss how the Kappa function relates to the Logistic function as well as how it can be used to approximate the Logistic function. We introduce the so-called Generalized Kappa Differential Equation and show that both the Kappa and the Logistic functions can be derived from it. Kappa regression, like binary Logistic regression, models the conditional probability of the event that a dichotomous random variable takes a particular value at a given value of an explanatory variable. This new regression method may be viewed as an alternative to binary Logistic regression, but while in binary Logistic regression the explanatory variable is defined over the entire Euclidean space, in the Kappa regression model the predictor variable is defined over a bounded subset of the Euclidean space. We will also show that asymptotic Kappa regression is Logistic regression. The advantages of this novel method are demonstrated by means of an example, and afterwards some implications are discussed.																	0218-4885	1793-6411				APR	2020	28	2					237	267		10.1142/S0218488520500105													
J								Fuzzy Regression Model Based on Geometric Centroid and Incentre Points and Application to Performance Evaluation	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Fuzzy regression model; centroid point; incentre point; least-squares method; performance evaluation	LINEAR-REGRESSION; OUTPUT DATA; INPUT; NUMBERS; DISTANCES	Fuzzy regression model is developed to construct the relationship between independent variable and dependent variable in a fuzzy environment. In order to increase the explanatory performance of fuzzy regression model, the least-squares method usually is applied to determine the numeric coefficients based on the concept of distance. In this paper, we consider the fuzzy linear regression model with fuzzy input, fuzzy output and crisp parameters and introduce a new distance based on the geometric centroid and incentre points (GCIP) of triangular fuzzy number, merge least-squares method with the new GCIP distance and propose least-squares GCIP distance method. Finally, an example of employee job performance is given to illustrate the effectiveness and feasibility of the method. Comparisons with existing methods show that total estimation error using the same distance criterion, the explanatory performance of the GCIP method is satisfactory, and the calculation is relatively simple.																	0218-4885	1793-6411				APR	2020	28	2					269	288		10.1142/S0218488520500117													
J								Performance Efficiency of Public Health Sector Using Intuitionistic Fuzzy DEA	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Intuitionistic fuzzy data envelopment analysis; intuitionistic fuzzy interval efficiencies; minimax ranking; hospitals efficiency	DATA ENVELOPMENT ANALYSIS; MATHEMATICAL-PROGRAMMING APPROACH; DECISION-MAKING; SUPPLIER SELECTION; MODEL; SETS; WEIGHTS	Out of several generalizations of fuzzy set theory for various objectives, the notions of intuitionistic fuzzy sets (IFSs) is very useful in modeling real life problems. In existing fuzzy data envelopment analysis (FDEA) models, the inputs and outputs are limited to fuzzy input and fuzzy output data. In real life problems, the input data and output data can be considered as linguistic/vague characterized by intuitionistic fuzzy numbers (IFNs). So, in the present study, we extend FDEA to intuitionistic FDEA (IFDEA) in which the input and output data are taken as IFNs, in particular triangular IFNs (TIFNs). In this study, we develop models to measure the efficiencies of each DMU in intuitionistic fuzzy environment using alpha and beta-cuts and we get IF interval efficiencies. The ranking of FNs has been studied by many authors and extended to IFNs because of its applicability in real life problems. The ranking of IF interval efficiency plays an important role in DEA where the interval analysis is essential. Further, in this paper a new method for ranking IF interval efficiencies has been proposed and compared with other existing methods. A new general minimax approach is proposed to compare and rank the IF efficiency intervals of DMUs. One numerical example is provided to show the applications of the proposed IFDEA model and the proposed ranking approach. Moreover, we present an application of the proposed approach to the public health sector.																	0218-4885	1793-6411				APR	2020	28	2					289	315		10.1142/S0218488520500129													
J								Fuzzy Binary Rough Set	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Fuzzy rough sets; rough sets; fuzzy sets; fuzzy relations; alpha-cut		In this paper, we provide a definition of alpha-fuzzified lower and upper approximations for fuzzy sets based on the alpha-cut of fuzzy binary relations. We show that the definition is a proper generalization of the previous one for approximations of crisp sets and compare it with an existing definition in the context of fuzzy tolerance relation.																	0218-4885	1793-6411				APR	2020	28	2					317	329		10.1142/S0218488520500130													
J								Uncertainty Measurement for a Tolerance Knowledge Base	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Rough set theory; granular computing; tolerance knowledge base; knowledge granule; knowledge structure; dependence; inclusion degree; uncertainty; measure	ROUGH SET APPROACH; GRANULATION; REDUCTION; ACQUISITION; ENTROPY	A knowledge base is an important notion of rough set theory. A tolerance knowledge base is its generalization. Measures of uncertainty as important evaluation tools in the fields of machine learning can measure the dependence and similarity between two targets. This paper investigates uncertainty measurement for a tolerance knowledge base by using its knowledge structure. The knowledge structure of a given tolerance knowledge base is first introduced by means of set vectors. Then, the dependence and independence between knowledge structures of tolerance knowledge bases are depicted. Next, the measurement uncertainty of tolerance knowledge bases is investigated. Finally, to obtain two tolerance knowledge bases with additional data, two information systems from the UCI Repository of machine learning databases are selected to construct two numerical experiments, and an effectiveness analysis is performed from the perspective of statistics to show the feasibility of the proposed measures.																	0218-4885	1793-6411				APR	2020	28	2					331	357		10.1142/S0218488520500142													
J								Reasoning with smart objects' affordance for personalized behavior monitoring in pervasive information systems	KNOWLEDGE AND INFORMATION SYSTEMS										Affordance; Smart object; Activity; Behavior; Sensor; Ontology	ACTIVITY RECOGNITION	The miniaturization of sensors and their integration in everyday appliances have opened the way for ecologically monitoring people's behavior based on their interaction with smart objects. Thanks to behavior monitoring, mobile, and ubiquitous information systems in the areas of e-health, home automation, and smart cities are becoming more and more "smart," being able to dynamically adapt themselves to the current users' context and situation. However, human behavior is characterized by large variability due to individual habits, physical disabilities or cognitive impairment. This aspect makes behavior monitoring a challenging task. On the one side, execution variability makes it hard to acquire sufficiently large activity datasets needed by supervised learning methods. On the other side, being based on a strict definition of activities in terms of constituting simpler actions, existing knowledge-based frameworks fall short in adapting to the specific characteristics of the subject. Hence, the variability of activity execution by different subjects calls for personalized methods to capture human activities and interaction in smart spaces at a fine-grained level. In this paper, we address this challenge by proposing a novel hybrid reasoning framework to capture fine-grained interaction with smart objects considering the specific features of individuals. Our model has its roots in the well-founded psychological theory of affordances, i.e., those features of an object that naturally explain its possible uses and how it should be used. The core of the framework is the ontological model of smart objects affordance, expressed through the OWL 2 Web Ontology Language. Through a use case in pervasive healthcare, we show how our framework can be applied to personalized recognition of abnormal behaviors. In particular, we tackle a particularly challenging issue: how to recognize early behavioral symptoms of mild cognitive impairment in subjects with physical disabilities. Moreover, an extensive experimental evaluation with real-world datasets acquired from 24 subjects shows the effectiveness of our framework in recognizing human activities and fine-grained manipulative gestures in different pervasive computing environments.																	0219-1377	0219-3116				APR	2020	62	4					1255	1278		10.1007/s10115-019-01357-y													
J								Trajectory splicing	KNOWLEDGE AND INFORMATION SYSTEMS										Trajectory computation; Trajectory fusion; Trajectory recovery; Trajectory linking		With continued development of location-based systems, large amounts of trajectories become available which record moving objects' locations across time. If the trajectories collected by different location-based systems come from the same moving object, they are spliceable trajectories, which contribute to representing holistic behaviors of the moving object. In this paper, we consider how to efficiently identify spliceable trajectories. More specifically, we first formalize a spliced model to capture spliceable trajectories where their times are disjoint, and the distances between them are close. Next, to efficiently implement the model, we design three components: a disjoint time index, a directed acyclic graph of sub-trajectory location connections, and two splice algorithms. The disjoint time index saves a disjoint time set of each trajectory for querying disjoint time trajectories efficiently. The directed acyclic graph contributes to discovering groups of spliceable trajectories. Based on the identified groups, the splice algorithm findmaxCTR finds maximal groups containing all spliceable trajectories. Although the splice algorithm is efficient in some practical applications, its running time is exponential. Therefore, an approximate algorithm findApproxMaxCTR is proposed to find trajectories which can be spliced with each other with a certain probability within polynomial run time. Finally, experiments on two datasets demonstrate that the model and its components are effective and efficient.																	0219-1377	0219-3116				APR	2020	62	4					1279	1312		10.1007/s10115-019-01382-x													
J								ProSecCo: progressive sequence mining with convergence guarantees	KNOWLEDGE AND INFORMATION SYSTEMS										Approximation algorithms; Interactive data analysis; Pattern mining; Sampling; VC-dimension	DATABASES	We present ProSecCo, an algorithm for the progressive mining of frequent sequences from large transactional datasets: It processes the dataset in blocks and it outputs, after having analyzed each block, a high-quality approximation of the collection of frequent sequences. ProSecCo can be used for interactive data exploration, as the intermediate results enable the user to make informed decisions as the computation proceeds. These intermediate results have strong probabilistic approximation guarantees and the final output is the exact collection of frequent sequences. Our correctness analysis uses the Vapnik-Chervonenkis (VC) dimension, a key concept from statistical learning theory. The results of our experimental evaluation of ProSecCo on real and artificial datasets show that it produces fast-converging high-quality results almost immediately. Its practical performance is even better than what is guaranteed by the theoretical analysis, and ProSecCo can even be faster than existing state-of-the-art non-progressive algorithms. Additionally, our experimental results show that ProSecCo uses a constant amount of memory, and orders of magnitude less than other standard, non-progressive, sequential pattern mining algorithms.																	0219-1377	0219-3116				APR	2020	62	4					1313	1340		10.1007/s10115-019-01393-8													
J								Multi-label crowd consensus via joint matrix factorization	KNOWLEDGE AND INFORMATION SYSTEMS										Crowdsourcing; Multi-label crowd consensus; Joint matrix factorization; Low-rank; Spammers	CLASSIFICATION; AGGREGATION	Crowdsourcing is a useful and economic approach to annotate data. Various computational solutions have been developed to pursue a consensus of high quality. However, available solutions mainly target single-label tasks, and they neglect correlations among labels. In this paper, we introduce a multi-label crowd consensus (MLCC) model based on a joint matrix factorization. Specifically, MLCC selectively and jointly factorizes the sample-label association matrices into products of individual and shared low-rank matrices. As such, it makes use of the robustness of low-rank matrix approximation to noisy annotations and diminishes the impact of unreliable annotators by assigning small weights to their annotation matrices. To obtain coherent low-rank matrices, MLCC additionally leverages the shared low-rank matrix to model correlations among labels, and the individual low-rank matrices to measure the similarity between annotators. MLCC then computes the low-rank matrices and weights via a unified objective function, and adopts an alternative optimization technique to iteratively optimize them. Finally, MLCC uses the optimized low-rank matrices and weights to compute the consensus labels. Our experimental results demonstrate that MLCC outperforms competitive methods in inferring consensus labels. Besides identifying spammers, MLCC achieves robustness against their incorrect annotations, by crediting them small, or zero, weights.																	0219-1377	0219-3116				APR	2020	62	4					1341	1369		10.1007/s10115-019-01386-7													
J								A new last aggregation method of multi-attributes group decision making based on concepts of TODIM, WASPAS and TOPSIS under interval-valued intuitionistic fuzzy uncertainty	KNOWLEDGE AND INFORMATION SYSTEMS										Interval-valued intuitionistic fuzzy sets (IVIFSs); Multi-attributes group decision-making (MAGDM) problems; TODIM; WASPAS; TOPSIS; Objective and subjective weights; Last aggregation	SUPPLIER SELECTION; MCDM APPROACH; MODEL; SETS; SYSTEMS; DEMATEL; COPRAS	Due to the complexity of decision making under uncertainty and the existence of various and often conflicting criteria, several methods have been proposed to facilitate decision making, and fuzzy logic has been used successfully to address this issue. This paper presents a new framework for solving multi-attributes group decision-making problems under fuzzy environments. The proposed algorithm has several features. First of all, the TODIM (an acronym in Portuguese for interactive multi-criteria decision making) method under interval-valued intuitionistic fuzzy uncertainty is employed. Moreover, objective and subjective weights for each decision maker are used to address this last aggregation approach. To consider weights of attributes, knowledge measure in addition to a new mathematical approach is introduced. A new aggregation and ranking method based on the WASPAS and TOPSIS methods, namely WT method, is presented and applied in this paper. Finally, the effectiveness of the proposed framework is shown by comparing the results with two different real-world applications in the literature.																	0219-1377	0219-3116				APR	2020	62	4					1371	1391		10.1007/s10115-019-01390-x													
J								Sentiment analysis on big sparse data streams with limited labels	KNOWLEDGE AND INFORMATION SYSTEMS										Sentiment analysis; Semi-supervised learning; Class imbalance; Data augmentation	CLASSIFICATION	Sentiment analysis is an important task in order to gain insights over the huge amounts of opinionated texts generated on a daily basis in social media like Twitter. Despite its huge amount, standard supervised learning methods won't work upon such sort of data due to lack of labels and the impracticality of (human) labeling at this scale. In this work, we leverage distant supervision and semi-supervised learning to annotate a big stream of tweets from 2015 which consists of 228 million tweets without retweets (and 275 million with retweets). We present the insights from our annotation process regarding the effect of different semi-supervised learning approaches, namely Self-Learning, Co-Training and Expectation-Maximization. Moreover, we propose two annotation modes, the batch mode where all labeled and unlabeled data are available to the algorithms from the beginning and a lightweight streaming mode that processes the data in batches based on their arrival time in the stream. Our experiments show that stream processing with a sliding window of three months achieves comparable results to batch processing while being more efficient. Finally, to tackle the class imbalance problem, as our dataset is imbalanced toward the positive sentiment class, and its aggravation by the semi-supervised learning methods, we employ data augmentation in the semi-supervised learning process in order to equalize the class distribution. Our results show that semi-supervised learning coupled with data augmentation outperforms significantly the default semi-supervised annotation process. We make the so-called TSentiment15 sentiment-annotated dataset available to the community to be used for evaluation purposes and for developing new methods.																	0219-1377	0219-3116				APR	2020	62	4					1393	1432		10.1007/s10115-019-01392-9													
J								Parallel co-location mining with MapReduce and NoSQL systems	KNOWLEDGE AND INFORMATION SYSTEMS										Spatial data mining; Parallel co-location mining; Cloud computing; MapReduce; NoSQL	COLOCATION PATTERNS; DATA SETS; FRAMEWORK	With the rapid growth of georeferenced data, large-scale data processing and analysis methods are needed for spatial big data. Spatial co-location pattern mining is an interesting and important issue in spatial data mining area which discovers the subsets of features whose objects are frequently located together in geographic proximity. There are several works for efficiently processing co-location pattern discovery; however, they may be insufficient for large dense spatial data because the mining task takes up a lot of processing time and memory. In this work, we leveraged the power of a modern distributed computing platform, Hadoop, and developed an algorithm (called ParColoc) for parallel co-location mining on the MapReduce framework. This study explored challenge issues in designing the parallel co-location mining algorithm and solved them with adopting a spatial declusteirng technique and a NoSQL system. We conducted an experimental evaluation with real-world data and synthetic data to examine the effectiveness of proposed methods. The experiment result shows that ParColoc is a promising method for parallel co-location mining in cloud computing environment.																	0219-1377	0219-3116				APR	2020	62	4					1433	1463		10.1007/s10115-019-01381-y													
J								Micro- and macro-level churn analysis of large-scale mobile games	KNOWLEDGE AND INFORMATION SYSTEMS										Churn prediction; Representation learning; Graph embedding; Inductive learning; Semi-supervised learning; Mobile games		As mobile devices become more and more popular, mobile gaming has emerged as a promising market with billion-dollar revenue. A variety of mobile game platforms and services have been developed around the world. A critical challenge for these platforms and services is to understand the churn behavior in mobile games, which usually involves churn at micro-level (between an app and a specific user) and macro-level (between an app and all its users). Accurate micro-level churn prediction and macro-level churn ranking will benefit many stakeholders such as game developers, advertisers, and platform operators. In this paper, we present the first large-scale churn analysis for mobile games that supports both micro-level churn prediction and macro-level churn ranking. For micro-level churn prediction, in view of the common limitations of the state-of-the-art methods built upon traditional machine learning models, we devise a novel semi-supervised and inductive embedding model that jointly learns the prediction function and the embedding function for user-app relationships. We model these two functions by deep neural networks with a unique edge embedding technique that is able to capture both contextual information and relationship dynamics. We also design a novel attributed random walk technique that takes into consideration both topological adjacency and attribute similarities. To address macro-level churn ranking, we propose to construct a relationship graph with estimated micro-level churn probabilities as edge weights and adapt link analysis algorithms on the graph. We devise a simple algorithm SimSum and adapt two more advanced algorithms PageRank and HITS. The performance of our solutions to the two-level churn analysis problem is evaluated on real-world data collected from the Samsung Game Launcher platform. The data includes tens of thousands of mobile games and hundreds of millions of user-app interactions. The experimental results with this data demonstrate the superiority of our proposed models against existing state-of-the-art methods.																	0219-1377	0219-3116				APR	2020	62	4					1465	1496		10.1007/s10115-019-01394-7													
J								Effective construction of classifiers with the k-NN method supported by a concept ontology	KNOWLEDGE AND INFORMATION SYSTEMS										k-nearest neighbour algorithm; Ontology similarity metrics; Holter measurement; Coronary disease	SIMILARITY	In analysing sensor data, it usually proves beneficial to use domain knowledge in the classification process in order to narrow down the search space of relevant features. However, it is often not effective when decision trees or the k-NN method is used. Therefore, the authors herein propose to build an appropriate concept ontology based on expert knowledge. The use of an ontology-based metric enables mutual similarity to be determined between objects covered by respective concept ontology, taking into consideration interrelations of features at various levels of abstraction. Using a set of medical data collected with the Holter method, it is shown that predicting coronary disease with the use of the approach proposed is much more accurate than in the case of not only the k-NN method using classical metrics, but also most other known classifiers. It is also proved in this paper that the expert determination of appropriate structure of ontology is of key importance, while subsequent selection of appropriate weights can be automated.																	0219-1377	0219-3116				APR	2020	62	4					1497	1510		10.1007/s10115-019-01391-w													
J								On the calculation of the strength of threats	KNOWLEDGE AND INFORMATION SYSTEMS										Rhetorical arguments; Threats; Arguments strength calculation; Persuasive arguments; Persuasive negotiation dialogue	ARGUMENTATION; NEGOTIATION; FRAMEWORK; REWARDS; TRUST	Threats are used in persuasive negotiation dialogues when a proponent agent tries to persuade an opponent of him to accept a proposal. Depending on the information the proponent has modeled about his opponent(s), he may generate more than one threat, in which case he has to evaluate them in order to select the most adequate to be sent. One way to evaluate the generated threats is by calculating their strengths, i.e., the persuasive force of each threat. Related work considers mainly two criteria to do such evaluation: the certainty level of the beliefs that compose the threat and the importance of the goal of the opponent. This article aims to study the components of threats and propose further criteria that lead to improve their evaluation and to select more effective threats during the dialogue. Thus, the contribution of this paper is a model for calculating the strength of threats that is mainly based on the status of the goal of the opponent and the credibility of the proponent. The model is empirically evaluated and the results demonstrate that the proposed model is more efficient than previous works in terms of the number of exchanged arguments, and the number of reached agreements.																	0219-1377	0219-3116				APR	2020	62	4					1511	1538		10.1007/s10115-019-01399-2													
J								Measuring similarity and relatedness using multiple semantic relations in WordNet	KNOWLEDGE AND INFORMATION SYSTEMS										Semantic similarity; Semantic relatedness; Nearest common descendant; Multiple semantic complement; WordNet	INFORMATION-CONTENT; MODEL; CONTEXT; EDGE	Semantic similarity and relatedness computation has attracted an increasing amount of attention among researchers. The majority of previous studies, including edge-based and information content-based methods, rely on a single semantic relationship in WordNet such as the "is-a" relation. However, a performance ceiling may have been created by semantic unicity and inadequate calculation in solely "is-a" relation-based measurements, i.e., the computed results for some word pairs are too small and significantly deviate from human judgments. For this problem, we propose the following solutions: (1) We introduce the notion of the nearest common descendant to provide a supplement for commonalities between concepts according to genetics theory. (2) We design various targeted methods for different incomplete semantic relations. Therefore, various semantic relations can participate in similarity and relatedness computations in their most appropriate manners. (3) We utilize the cross-use of incomplete semantic relations similar-to and antonymy to solve the challenge of adjective and adverb similarity/relatedness measurements in WordNet. (4) We propose a targeted independent computation and largest contribution aggregation method to break through the performance ceiling of similarity/relatedness measurements based on single "is-a" relations. We conduct evaluations of our proposed model using seven extensively employed datasets. These evaluations indicate that our method significantly improves the performance of the existing methods based on single "is-a" relations. Their best Pearson coefficient with human judgments on both the MC30 and RG65 is increased to 0.9. With the development and enrichment of semantic relations in WordNet, our proposed model can be expected to have a more prominent role.																	0219-1377	0219-3116				APR	2020	62	4					1539	1569		10.1007/s10115-019-01387-6													
J								Consistent updating of databases with marked nulls	KNOWLEDGE AND INFORMATION SYSTEMS										Updates; Null values; Constraints; TGD; Logical database; RDF	DATA EXCHANGE	This paper revisits the problem of consistency maintenance when insertions or deletions are performed on a valid database containing marked nulls. This problem comes back to light in real-world linked data or RDF databases when blank nodes are associated with null values. This paper proposes solutions for the main problems one has to face when dealing with updates and constraints, namely update determinism, minimal change and leanness of an RDF graph instance. The update semantics is formally introduced and the notion of core is used to ensure a database as small as possible (i.e. the RDF graph leanness). Our algorithms allow the use of constraints such as tuple-generating dependencies, offering a way for solving many practical problems.																	0219-1377	0219-3116				APR	2020	62	4					1571	1609		10.1007/s10115-019-01402-w													
J								Estimating inelastic seismic response of reinforced concrete frame structures using a wavelet support vector machine and an artificial neural network	NEURAL COMPUTING & APPLICATIONS										Inelastic seismic response; Reinforced concrete; Wavelet support vector machines; Artificial neural networks	OPTIMIZATION; EARTHQUAKE; PREDICTION; FUZZY; DESIGN; SYSTEM	Modern building codes increasingly enforce evaluating the inelastic response of structures to ensure their safety in major seismic events. Although an inelastic dynamic analysis provides the most realistic and accurate measure for the seismic response, its application for large-scale structures is hampered by the excessive computational burden involved. This is particularly the case for the optimization of inelastic structures subjected to dynamic loads using metaheuristic algorithms where numerous analyses are required before the design converges to the optimum. In this regard, developing predictive models with sufficient accuracy will significantly help to reduce the computational demand, thus making the seismic analysis and optimization of large structures more feasible and common practice. Motivated by this need, this paper reports a study on the capabilities of a wavelet weighted least squares support vector machine (WWLSSVM) and a feedforward, backpropagation artificial neural network (ANN) to accurately predict the inelastic seismic responses of structures. The force- and displacement-based seismic responses of an 18-story reinforced concrete frame subjected to different earthquake ground motion records scaled to the design basis earthquake and maximum considered earthquake levels are used to train the models and examine their accuracies. The first three natural periods of the frame and combinations thereof are considered as the inputs for the model. The results indicate that both models exhibit satisfactory prediction performances, with the ANN model having a slight edge on accuracy in most of the cases studied, especially when a smaller number of samples are used for training. A parametric sensitivity analysis shows that the seismic responses predicted by the ANN model generally exhibit less sensitivity to the inputs than do those predicted by the WWLSSVM model. The results also indicate that force- and displacement-based responses exhibit the highest sensitivity to the first and second natural periods, respectively.																	0941-0643	1433-3058				APR	2020	32	8					2975	2988		10.1007/s00521-019-04075-2													
J								Semi-permutation-based genetic algorithm for order acceptance and scheduling in two-stage assembly problem	NEURAL COMPUTING & APPLICATIONS										Two-stage assembly; Order acceptance and scheduling; Mixed-integer linear program; Genetic algorithm	MINIMIZING TOTAL TARDINESS; BOUND ALGORITHM; MACHINE; LEVEL; TIME; JOBS	The joint decision-making of order acceptance and scheduling has recently gained increasing attention. Besides, the two-stage assembly scheduling problem has various real-life applications. The current paper considers an integrated model for order acceptance and scheduling decisions in two-stage assembly problem. The objective is maximizing profit which is the sum of revenues minus total weighted tardiness of the accepted orders. A mixed-integer linear programming model is developed based on time-index variables. Also, a new concept of semi-permutation scheduling is introduced assuming that positions of each job on all of the machines have no significant difference in the optimal solution. This problem is NP-hard, and therefore, a genetic algorithm (GA)-based heuristic is proposed to apply semi-permutation concept, named semi-permutation GA (SPGA), to solve the problem efficiently. The solutions of SPGA are compared with those of CPLEX and non-semi permutation GA (N-SPGA). Computational experiments are conducted in a diverse range of problem instances indicating that the SPGA performs much better than CPLEX regarding the average percentage of improvement, ranging from 1.4 to 168.84%, and run time. The results revealed that an increasing number of machines and orders could lead to a dramatic decrease in the performance of CPLEX and N-SPGA than SPGA. Also, the effect of semi-permutation scheduling is investigated. According to the result, semi-permutation scheduling had a strong effect on the performance of the algorithm. As a result, the SPGA algorithm outperformed the non-semi permutation version of GA completely. Moreover, SPGA could represent the better performance of 36.27% in average in comparison with N-SPGA.																	0941-0643	1433-3058				APR	2020	32	8					2989	3003		10.1007/s00521-019-04027-w													
J								Dragonfly Algorithm for solving probabilistic Economic Load Dispatch problems	NEURAL COMPUTING & APPLICATIONS										Economic dispatch; Dragonfly Algorithm; Point estimate method; Solar power; Valve point loading; Wind power	CHEMICAL-REACTION OPTIMIZATION; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; FLOW	Economic Load Dispatch problem in power system is solved by different methods to run the generating station in an economic way for different loading conditions. A combination of thermal power plant and renewable plant of wind and solar photovoltaic is considered for optimal solution at minimal cost using Dragonfly Algorithm. Swarming behaviour of dragonfly is used for optimization of the present Economic Load Dispatch problem. The modelling of solar and wind power plant is done using 2-m point estimation technique to consider the uncertainties in power generation from such renewable sources. The solution of objective function is found using the proposed method (Dragonfly Algorithm), and the same is compared with that obtained using other well-known algorithms such as Crow Search Algorithm, Ant Lion Optimizer, Oppositional Real-Coded Chemical Reaction Optimization, Biogeography-Based Optimization, Particle Swarm Optimization and Genetic Algorithm. It is found that the proposed method gives better solution in terms of execution time and cost effectiveness. To validate the performance of Dragonfly Algorithm, four test systems have been considered. It is observed that the total generation cost obtained by Dragonfly Algorithm for each test system is less (10,049.1948 $/h for test system I, 2018.0762 $/h for test system II, 15,268.8325 $/h for test system III and 32,310.2922 $/h for test system IV) as compared to other well-known optimization methods. It is also found that the time required to reach minimum solution is lesser in case of Dragonfly Algorithm (8 s for test system I, 12 s for test system II, 15 s for test system III and 20 s for test system IV) as compared to other optimization techniques.																	0941-0643	1433-3058				APR	2020	32	8					3029	3045		10.1007/s00521-019-04268-9													
J								Remote detection of idling cars using infrared imaging and deep networks	NEURAL COMPUTING & APPLICATIONS										Infrared image; Car detection; Idle detection; Deep neural networks	FEATURES	Idling vehicles waste energy and pollute the environment through exhaust emission. In some countries, idling a vehicle for more than a predefined duration is prohibited and automatic idling vehicle detection is desirable for law enforcement. We propose the first automatic system to detect idling cars, using infrared (IR) imaging and deep networks. We rely on the differences in spatio-temporal heat signatures of idling and stopped cars and monitor the car temperature with a long-wavelength IR camera. We formulate the idling car detection problem as spatio-temporal event detection in IR image sequences and employ deep networks for spatio-temporal modeling. We collected the first IR image sequence dataset for idling car detection. First, we detect the cars in each IR image using a convolutional neural network, which is pre-trained on regular RGB images and fine-tuned on IR images for higher accuracy. Then, we track the detected cars over time to identify the cars that are parked. Finally, we use the 3D spatio-temporal IR image volume of each parked car as input to convolutional and recurrent networks to classify them as idling or not. We carried out an extensive empirical evaluation of temporal and spatio-temporal modeling approaches with various convolutional and recurrent architectures. We present promising experimental results on our IR image sequence dataset.																	0941-0643	1433-3058				APR	2020	32	8					3047	3057		10.1007/s00521-019-04077-0													
J								An image classification framework exploring the capabilities of extreme learning machines and artificial bee colony	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine; Discrete wavelet transform; Artificial bee colony; MRI image classification	NEURAL-NETWORK; FEATURE-EXTRACTION; RECOGNITION; PERFORMANCE; REPRESENTATION; TRANSFORM; ALGORITHM; SCHEME	A hybridized image classification strategy is proposed based on discrete wavelet transform, artificial bee colony (ABC) and extreme learning machine (ELM). The proposed methodology works in three phases: (a) in preprocessing phase, images are decomposed and features are extracted from images using bi-orthogonal wavelet functions; (b) secondly, modified ABC (MABC) optimization algorithm is proposed to determine the optimal parameters such as hidden layer weights and biases to be used by ELM for classification; (c) the ELM in the third phase has been trained and tested with three brain image datasets for different diseases along with normal brain images. The performance recognition of the proposed MABC-ELM in terms of accuracy, rate of per-image classification and speedup has been made with variants of ELM such as ELM, ABC-ELM and MABC-ELM and also with MLPNN, naive Bayesian, linear regression classifiers. Finally, the percentage of accuracy observed by the proposed MABC-ELM, for acute stroke-speech arrest, glioma and multiple sclerosis datasets, is 90%, 90% and 100% with eight hidden nodes in the ELM architecture, and it can be concluded that MABC-ELM gives better generalization performance, more compact network architecture and the hybridization of ELM with modified ABC is worth investigated.																	0941-0643	1433-3058				APR	2020	32	8					3079	3099		10.1007/s00521-019-04385-5													
J								MOGSABAT: a metaheuristic hybrid algorithm for solving multi-objective optimisation problems	NEURAL COMPUTING & APPLICATIONS										Multi-objective optimisation problem; Gravitational search algorithm; Bat algorithm; Swarm intelligence	BAT ALGORITHM; COLONY	This study proposes a novel strength of multi-objective gravitational search algorithm and bat algorithm MOGSABAT to solve multi-objective optimisation problem. The proposed MOGSABAT algorithm is divided into three stages. In the first stage (moving space), a switch in a solution from single function to multiple functions that contain more than one objective to use the gravitational search algorithm GSA is determined. We established a new equation to calculate the masses of individuals in the population using the theoretical work found in the strength Pareto evolutionary algorithm. In the second stage (moving in space), how to handle the bat algorithm BAT to solve multiple functions is established. We applied the theoretical work of multi-objective particle swarm optimisation into the BAT algorithm to solve multiple functions. In the third stage, multi-objective GSA and multi-objective BAT are integrated to obtain the hybrid MOGSABAT algorithm. MOGSABAT is tested by adopting a three-part evaluation methodology that (1) describes the benchmarking of the optimisation problem (bi-objective and tri-objective) to evaluate the performance of the algorithm; (2) compares the performance of the algorithm with that of other intelligent computation techniques and parameter settings; and (3) evaluates the algorithm based on mean, standard deviation and Wilcoxon signed-rank test statistic of the function values. The optimisation results and discussion confirm that the MOGSABAT algorithm competes well with advanced metaheuristic algorithms and conventional methods.																	0941-0643	1433-3058				APR	2020	32	8					3101	3115		10.1007/s00521-018-3808-3													
J								Optimization of protein folding using chemical reaction optimization in HP cubic lattice model	NEURAL COMPUTING & APPLICATIONS										Protein folding optimization; Chemical reaction optimization algorithm; Hydrophobic-polar model; Cubic lattice; H&P; Evolution mechanism; Repair mechanism	HYDROPHOBIC-POLAR MODEL; STRUCTURE PREDICTION; GENETIC ALGORITHM	Protein folding optimization is a very important and tough problem in computational biology. For solving this problem, a population-based metaheuristic algorithm named chemical reaction optimization (CRO) with HP cubic lattice model has been proposed in this paper. The proposed algorithm is combined with evolution and H&P compliance mechanisms which are responsible for increasing the performance of the algorithm. The evolution mechanism improves the performance of each individual solution. On the other hand, the H&P compliance mechanism tries to place the H monomer close to the center and place the P monomer as far as possible from the center of the related structure. The algorithm also applies four reactant operations of typical CRO algorithm decomposition, on-wall ineffective collision, synthesis and inter-molecular ineffective collision to solve the problem efficiently. The reactants or mechanisms may cause overlapping of the corresponding solutions. The algorithm also includes a repair mechanism which transforms invalid solutions into valid ones by removing overlapping in cubic lattice points. This algorithm has been tested over some sets of sequences and it shows very good performance.																	0941-0643	1433-3058				APR	2020	32	8					3117	3134		10.1007/s00521-019-04447-8													
J								Intrusion detection using deep sparse auto-encoder and self-taught learning	NEURAL COMPUTING & APPLICATIONS										Self-taught learning; Deep sparse auto-encoder; Intrusion detection system	PREDICTION; FRAMEWORK; FACE	With the enormous increase in the use of the Internet, secure transfer of data across networks has become a challenging task. Attackers are in continuous search of getting information from network traffic, and this is the main reason that efficient intrusion detection techniques are required to identify different kinds of network attacks. In past, various supervised and semi-supervised methods have been developed for intrusion detection. Most of these methods require a large amount of data to develop an efficient intrusion detection system. In the proposed deep neural network and adaptive self-taught-based transfer learning technique, we have exploited the concept of self-taught learning to train deep neural networks for reliable network intrusion detection. In the proposed method, a pre-trained network on regression-related task is used to extract features from NSL-KDD dataset. Original features along with extracted features from the pre-trained network are then provided as an input to the sparse auto-encoder. Self-taught learning-based extracted features, when concatenated with the original features of NSL-KDD dataset, enhance the performance of the sparse auto-encoder. Performance of self-taught learning-based approach is compared against several techniques using ten independent runs in terms of accuracy, false alarm and detection rate, area under the ROC, and PR curve. It is experimentally observed that the auto-encoder trained on the combined original and extracted features is stable and offers good generalization in comparison with the sparse auto-encoder trained on original features alone.																	0941-0643	1433-3058				APR	2020	32	8					3135	3147		10.1007/s00521-019-04152-6													
J								Adversarial neural networks for playing hide-and-search board game Scotland Yard	NEURAL COMPUTING & APPLICATIONS										Deep reinforcement learning; Multi-agent system; Intelligent game agents; Adversarial models; Expert systems	CARLO TREE-SEARCH; NEUROEVOLUTION; PREDICTION; GO	This paper investigates the design of game playing agents, which should automatically play an asymmetric hide-and-search-based board game with imperfect information, called Scotland Yard. Neural network approaches have been developed to make the agents behave human-like in the sense that they would assess the game environment in a way a human would assess it. Specifically, a thorough investigation has been conducted on the application of adversarial neural network combined with Q-learning for designing the game playing agents in the game. The searchers, called detectives and the hider, called Mister X (Mr. X) have been modeled as neural network agents, which play the game of Scotland Yard. Though it is a type of two-player (or, two-sided) game, all the five detectives must cooperate to capture the hider to win the game. A special kind of feature space has been designed for both detectives and Mr. X that would aid the process of cooperation among the detectives. Rigorous experiments have been conducted, and the performance in each experiment has been noted. The evidence from the obtained results demonstrates that the designed neural agents could show promising performance in terms of learning the game, cooperating, and making efforts to win the game.																	0941-0643	1433-3058				APR	2020	32	8					3149	3164		10.1007/s00521-018-3701-0													
J								MQSMER: a mixed quadratic shape model with optimal fuzzy membership functions for emotion recognition	NEURAL COMPUTING & APPLICATIONS										Emotion recognition; Fuzzy shape; Geometric features; Facial expression	FACIAL EXPRESSION RECOGNITION; FEATURES; EIGENFACES; FUSION; EYE	The traditional geometrical-based approaches used in facial emotion recognition fail to capture the uncertainty present in the quadrilateral shape of emotions under analysis, which reduces the recognition accuracy rate. Furthermore, these approaches require extensive computational time to extract the facial features and to train the models. This article proposes a novel geometrical fuzzy-based approach to accurately recognize facial emotions in images in less time. The four corner vertices of the mouth are the most important features to recognize facial emotions and can be extracted without the need of a reference face. These extracted features can then be used to define the quadrilateral shape, and the associated degree of impreciseness in the shape can be accessed using the proposed geometric fuzzy membership functions. Hence, four fuzzy features are derived from the membership functions and given to classifiers for emotion evaluations. In our tests, the fuzzy features achieved an accuracy rate of 96.17% in the Japanese Female Facial Expression database, and 98.32% in the Cohn-Kanade Facial Expression database, which are higher than the ones achieved by other common up-to-date methods. In terms of computational time, the proposed method required an average of 0.375 s to build the used model in a common PC.																	0941-0643	1433-3058				APR	2020	32	8					3165	3182		10.1007/s00521-018-3940-0													
J								Interface depth modelling of gravity data and altitude variations: a Bayesian neural network approach	NEURAL COMPUTING & APPLICATIONS										Bayesian neural networks; Variogram modelling; Gravity data; Interface depth; 2D radially averaged power spectrum; Eastern Indian Shield	CLASSIFICATION; POWER; VARIABILITY; PREDICTION; TECTONICS; EFFICIENT; SPECTRUM; BENEATH; INDIA; RIVER	Modelling of anomalous geological source from the gravity data is vital for understanding the crustal/sub-crustal interface depths and associated hazard assessment. Two-dimensional radial power spectra have usually been used to infer average depth of protuberant geological structures, which lack details of dimensional comprehension of lateral interfaces. Here in this study, we implement jointly scaled conjugate gradient-based Bayesian neural network (SCG-BNN) scheme with variogram modelling to carve out shallow and deeper interfaces of complex geological terrain of Eastern Indian Shield, India, using Bouguer gravity anomaly (BGA) and altitude variations data. Our "learner codes" uses the SCG-BNN optimization algorithm to build up a statistical model involving appropriate control parameters for the modelling of shallow and deeper interfaces. We have also compared the proposed SCG-BNN modelling results with the results of both conventional artificial neural networks (ANNs) schemes (e.g. conjugate gradient-based ANNs (CG-ANN) and SCG-ANN) and support vector regression (SVR) modelling to demonstrate the robustness of the underlying method. Comparative analysis suggests that the SCG-BNN model produced superior results than the results of CG-ANN, SCG-ANN and SVR models. The results based on SCG-BNN analysis and variogram modelling have identified the existence of three conspicuous fault structures, namely Malda-Kishanganj Fault, Munger-Saharsha Ridge Marginal Fault and Katihar Fault. The analyses also significantly minimize prediction error in three independent datasets (viz. training, validation and test), enhancing the precision of estimated shallow and deeper interface depths and thereby extenuating feasibility of "SCG-BNN" learner code. We, therefore, conclude that the underlying approach is robust to model various interface depths and generate precisely variation in the shallow and deeper interfaces with appropriate input data from altitude variation and BGA. The "SCG-BNN learner scheme" may potentially be used to exploit interface depths from several other complex tectonic regions.																	0941-0643	1433-3058				APR	2020	32	8					3183	3202		10.1007/s00521-019-04276-9													
J								Community detection in attributed networks considering both structural and attribute similarities: two mathematical programming approaches	NEURAL COMPUTING & APPLICATIONS										Community detection; Mathematical programming model; Attributed network; Node attributes; Modularity maximization	MODULARITY MAXIMIZATION; LABEL PROPAGATION; SOCIAL NETWORKS; ALGORITHM; IDENTIFICATION; FORMULATIONS; MODEL	Community detection is one of the most well-known and emerging research topics in the area of social network analysis. There are a wide variety of approaches to find communities in the literature, each with its own advantages and disadvantages. A majority of these approaches tend to detect communities by only using the network topology. However, the distribution of the node attributes is correlated with the community structure in many real networks. Therefore, the quality of the discovered partitions can be enhanced by considering node attributes. In this study, two novel mathematical programming approaches are proposed to integrate the topological structure and node similarities, in which first the primary attributed network is converted into a secondary non-attributed network. Then, a mathematical model will be developed to find communities in the secondary network. Thanks to the fact that the objective function and constraints of the proposed model are defined linear, the global optimality of the obtained solutions is guaranteed. In order to validate the proposed approaches, they are applied to both real-world and benchmark networks. Computational results of two well-known evaluation measures including Rand index and normalized mutual information demonstrate the efficiency of the proposed approaches in discovering better partitions.																	0941-0643	1433-3058				APR	2020	32	8					3203	3220		10.1007/s00521-019-04064-5													
J								Aspect-based sentiment analysis using deep networks and stochastic optimization	NEURAL COMPUTING & APPLICATIONS										Sentiment analysis; Convolutional neural networks; PSO; Multi-objective function	ONTOLOGY	Sentiment analysis, also known as opinion mining, is a computational study of unstructured textual information which is used to analyze a persons attitude from a piece of text. This paper proposes an efficient method for sentiment analysis by effectively combining three procedures: (a) creating the ontologies for extraction of semantic features (b) Word2vec for conversion of processed corpus (c) convolutional neural network (CNN) for opinion mining. For CNN parameter tuning, a multi-objective function is solved for nondominant Pareto front optimal values using particle swarm optimization. Experiments show that the proposed technique outperforms other state-of-the-art techniques while yielding 88.52%, 94.30%, 85.63% and 86.03% in accuracy, precision, recall and F-measure, respectively.																	0941-0643	1433-3058				APR	2020	32	8					3221	3235		10.1007/s00521-019-04105-z													
J								Exploring nested ensemble learners using overproduction and choose approach for churn prediction in telecom industry	NEURAL COMPUTING & APPLICATIONS										Data mining; Churn prediction; Classification; Ensembles; Telecommunication industry	TELECOMMUNICATION SECTOR; CUSTOMER; CLASSIFICATION; MODEL; SYSTEM	Combining multiple classifiers to create hybrid learners (ensembles) has gained popularity in recent years. Ensembles are gaining more interest in the field of data mining as they have reportedly performed best predictions as compared to individual classifiers. This has resulted in experimentation with new ways of ensemble creation. This paper presents a study on creation of novel hybrid ways of combining multiple ensemble models using 'over production and choose approach.' In contrast to the original concept of ensembles that combine various learners, the proposed ensemble models comprise of combinations of other ensembles. In particular, we have combined learners as in composition of other learners, thus producing nested learners. Two such models named as Boosted-Stacked learners and Bagged-Stacked learners are proposed and are shown to outperform the traditional ensembles. Experiments are performed in churn prediction domain where a benchmark customer churn dataset (available on UCI repository) and a newly created dataset from a South Asian wireless telecom operator (named as SATO) are used. SATO dataset is created as balanced dataset (having equal number of churners and non-churners). The novel Boosted-Stacked learner and Bagged-Stacked learner achieved accuracies of 98.4% and 97.2%, respectively, on the UCI Churn dataset outperforming the existing state-of-the-art techniques. Furthermore, a high accuracy on the SATO dataset validates the effectiveness of the proposed models on balanced as well as imbalanced datasets.																	0941-0643	1433-3058				APR	2020	32	8					3237	3251		10.1007/s00521-018-3678-8													
J								Heat and mass transport phenomena of nanoparticles on time-dependent flow of Williamson fluid towards heated surface	NEURAL COMPUTING & APPLICATIONS										Williamson nanofluids; Unsteady flow; Buongiorno's model; Heated stretching surface	BOUNDARY-LAYER-FLOW; STRETCHING SURFACE; NANOFLUID FLOW; POROUS-MEDIUM; SHEET	An enhancement in the thermal conductivity of conventional base fluids has been a topic of great concern in recent years. An effective way to improve the heat transfer rate of conventional base fluids is the suspension of solid nanoparticles. In this framework, a theoretical study is performed to analyse the heat and mass transfer performance in the time-dependent flow of non-Newtonian Williamson nanofluid towards a stretching surface. There exist several studies focusing on the flow of Williamson fluid by assuming zero infinite shear rate viscosity. Nonetheless, there is a lack of knowledge regarding mathematical formulation for two-dimensional flow of the Williamson fluid by taking into account the impacts of infinite shear rate viscosity. In the current review, the Buongiorno model for nanofluids associated with Brownian motion and thermophoretic diffusion is employed to describe the heat transfer performance of nanofluids. The thermal system is composed of flow velocity, temperature, and nanoparticles concentration fields, respectively. The governing dimensionless equations are solved numerically by Runge-Kutta Fehlberg integration method. The numerical results are compared with published results and are found to have an excellent agreement. Effects of numerous dimensionless parameters on velocity, temperature, and nanoparticle concentration field together with the skin friction coefficient and rates of heat and mass transfer are presented with the assistance of graphical and tabular illustrations. With this analysis, we reached that the thermal boundary layer thickness as well as the nanofluids temperature has higher values with increase in thermophoresis and Brownian motion. It is further observed that the rate of heat transfer is significantly raised with an increment in Prandtl number and unsteadiness parameter.																	0941-0643	1433-3058				APR	2020	32	8					3253	3263		10.1007/s00521-019-04100-4													
J								Exponential stability of periodic solution for a memristor-based inertial neural network with time delays	NEURAL COMPUTING & APPLICATIONS										Memristor-based inertial neural network; Leary-Schauder alternative theorem; Periodic solutions; Global exponential stability	SYNCHRONIZATION; MODEL	In this paper, exponential stability of periodic solution for an inertial neural network is studied. Different from most research on inertial neural networks, the model in this paper is based on memristors and is involved with periodic solutions. In order to simplify the difficulty in dealing with inertial terms and constructing Lyapunov functions, the inertial neural network in this paper is transformed into a suitable neural network with enhanced characteristics by using appropriate variable transformation method. Under the Lyapunov stability theory and Leary-Schauder alternative theorem, we prove the existence and global exponential stability of the periodic solution for the inertial neural network under mild conditions. At last, the feasibility of the theoretical conclusions is illustrated by some numerical examples.																	0941-0643	1433-3058				APR	2020	32	8					3265	3281		10.1007/s00521-018-3702-z													
J								New efficient initialization and updating mechanisms in PSO for feature selection and classification	NEURAL COMPUTING & APPLICATIONS										Particle swarm optimization; Feature selection; Classification; Initialization strategy; Updating mechanisms	PARTICLE SWARM OPTIMIZATION; BINARY PSO; ALGORITHM	Feature selection is one of the important and difficult issues in classification. Particle swarm optimization (PSO) is an efficient evolutionary computing technique that has been widely used to deal with feature selection problem. However, it has been observed that the traditional initialization and personal best and global best updating mechanisms in PSO often limit its performance for feature selection and has to be further explored to see the full potential of PSO for the same. This paper proposes two new efficient initialization and updating mechanisms in PSO with the goal of minimizing the number of features and maximizing the classification performance in less computational time. The proposed algorithms are compared with six existing feature selection methods, including two traditional PSO-based feature selection methods and four PSO with different initialization strategy and updating mechanism-based feature selection methods. Experiments on eight benchmark dataset show that the proposed algorithms can automatically evolve a feature subset with a smaller number of features with higher classification performance than using all features. The proposed algorithms also outperform the eight existing feature selection algorithms in terms of the classification accuracy, the number of features, and the computational cost.																	0941-0643	1433-3058				APR	2020	32	8					3283	3294		10.1007/s00521-019-04395-3													
J								Singing voice separation with pre-learned dictionary and reconstructed voice spectrogram	NEURAL COMPUTING & APPLICATIONS										Singing voice separation; Low rank; Group-sparse; Dictionary Learning	OBSTRUCTIVE SLEEP-APNEA; INVERSE GAUSSIAN PARAMETERS; FACTOR WAVELET TRANSFORM; DECISION-SUPPORT-SYSTEM; EEG SIGNALS; AUTOMATED IDENTIFICATION; DECOMPOSITION; RETRIEVAL; FEATURES	Recently the mixture spectrogram of a song is usually considered as a superposition of a sparse spectrogram and a low-rank spectrogram, which correspond to the vocal part and the accompaniment part of the song, respectively. Based on this observation, one can separate singing voice from the background music. However, the quality of such separation might be limited, since the vocal part may be not described very well by low rank, and moreover its more prior information, such as annotation, should be considered when designing separation algorithm. Based on these considerations, in this paper, we present two categories, time-frequency-based source separation algorithms. Specifically, one incorporates both the vocal and instrumental spectrograms as sparse matrix and low-rank matrix, meanwhile combines some side information of vocal part, i.e., the reconstructed voice spectrogram from the annotation. The others further consider both the vocal and instrumental spectrograms as sparse matrix and group-sparse matrix, respectively. Evaluations on the iKala dataset show that the proposed methods are effective and efficient for both the separated singing voice and music accompaniment.																	0941-0643	1433-3058				APR	2020	32	8					3311	3322		10.1007/s00521-018-3757-x													
J								Adaptively weighted learning for twin support vector machines via Bregman divergences	NEURAL COMPUTING & APPLICATIONS										Insensitive loss functions; Twin support vector machines; Fuzzy membership; Bregman divergences; Data classification	CLASSIFICATION; IMPROVEMENTS; CONVERGENCE	Some versions of weighted (twin) support vector machines have been developed to handle the contaminated data. However, the weights of samples are generally obtained from the prior knowledge of data in advance. This article develops an adaptively weighted twin support vector machine via Bregman divergences. To better handle the contaminated data, we employ an insensitive loss function to control the fitting error of the samples in one class and introduce the weight (fuzzy membership) of each sample into the proposed model. The alternating optimization technique is utilized to solve the proposed model due to the characteristics of the model. The accelerated version of first-order methods is used to solve a quadratic programming problem, and the fuzzy membership of each sample is achieved analytically in the case of Bregman divergences. Experiments on some data sets have been conducted to show that our method gains better classification performance than previous methods, especially for the open set experiment.																	0941-0643	1433-3058				APR	2020	32	8					3323	3336		10.1007/s00521-018-3843-0													
J								A novel possibility measure to interval-valued intuitionistic fuzzy set using connection number of set pair analysis and its applications	NEURAL COMPUTING & APPLICATIONS										Interval-valued intuitionistic fuzzy set; Set pair analysis; Connection number; Multi-attribute decision-making; Possibility measures	TOPSIS METHOD; SIMILARITY MEASURES; NORM	The aim of this paper is to give a multi-attribute decision-making (MADM) method for the interval-valued intuitionistic fuzzy (IVIF) set using the set pair analysis (SPA) theory. IVIF set can express the uncertain information in a more valuable manner, while the connection number (CN) based on the "identity," "discrepancy" and "contrary" degrees of the SPA theory handles the uncertainties and certainties systems. Based on these features, in this paper, we develop a new possibility degree measure based on the CNs to rank the different IVIF numbers. The properties and advantages of the measure are described in details. Later on, we present a novel MADM method and illustrate with several examples to validate it. A comparative as well as superiority analysis is performed to show the feasibility and validity of the approach.																	0941-0643	1433-3058				APR	2020	32	8					3337	3348		10.1007/s00521-019-04291-w													
J								Optimised building energy and indoor microclimatic predictions using knowledge-based system identification in a historical art gallery	NEURAL COMPUTING & APPLICATIONS										System identification; Artificial neural networks; Energy prediction model; Indoor microclimatic control; Optimisation; Historical art gallery building	ARTIFICIAL NEURAL-NETWORK; COOLING LOAD; HVAC SYSTEM; MODEL; DEMAND; PERFORMANCE; QUALITY; METHODOLOGY; STRATEGIES; CLIMATE	This paper presents a system identification (SID) model for an historical art gallery of great cultural significance. These buildings require tight indoor temperature and moisture controls that demand significant energy from air handling units. Complex dynamic building systems, stringent conservation restrictions, and lack of detailed monitoring make diagnosing and optimising their energy use difficult. Building simulation software programmes have proven to be effective, but have tended to rely on data generated by simulation models. This study shows how artificial neural network (ANN) models trained with historical real data can predict a building's energy use and the optimal indoor microclimate necessary for conservation. Four ANN target-data scenarios were designed for optimised model predictions, and 12 ANN training algorithms were tested with six architectural scenarios collecting daily and hourly data. The ANN models used a randomised 80% sample of the database, with the remainder (20%) validating the models. The model displayed a high coefficient of correlation (0.99), with the mean square error and mean absolute error less than 0.1% and 2%, respectively. This ANN-based SID tool efficiently represents a complex building system and could be an ideal method for investigating optimisation strategies prior to their implementation.																	0941-0643	1433-3058				APR	2020	32	8					3349	3366		10.1007/s00521-019-04224-7													
J								A novel interval type-2 fuzzy decision model based on two new versions of relative preference relation-based MABAC and WASPAS methods (with an application in aircraft maintenance planning)	NEURAL COMPUTING & APPLICATIONS										Production projects; Decision-making model; Aircraft maintenance planning; Relative preference relation; MABAC method; WASPAS method; Interval type-2 fuzzy sets	LAST AGGREGATION; MAKING PROBLEMS; LOGIC SYSTEMS; SELECTION; TOPSIS; SETS; OPTIMIZATION; INFORMATION; RANKING; WEIGHT	In this paper, concerning advantages of relative preference relation (RPR) and interval type-2 fuzzy sets (IT2FSs), a new IT2F-RPR based on multi-attributive border approximation area comparison (MABAC) method is introduced for determining the critical path of production projects under group decision-making process. IT2FSs are more capable than classic fuzzy sets in coping with uncertainty and providing more degree of freedom to model the uncertain conditions. Also, the RPR is better than defuzzification because it does not lose the fuzzy message, and the method avoids pairwise comparisons. In fact, the MABAC method is extended by the RPR for reducing the time complexity. Furthermore, an extended MABAC method is developed under IT2FSs to better address the uncertainty. Weights of decision makers (DMs) are computed by introducing a new IT2F-RPR-based MABAC concept in the proposed new decision model. Also, experts' opinions are aggregated by using weights of the DMs. Moreover, weights of criteria are determined based on a new extended weighted aggregated sum-product assessment (WASPAS) method by using the DMs or experts' opinions on the importance of criteria and weights of DMs. As a matter of fact, weights of DMs are determined by a new version of extended MABAC method, and weights of criteria are specified using a new version of WASPAS method for the first time in the literature. Finally, a case study about aircraft maintenance planning is solved to address the calculation process and applicability of the proposed method. Computational results of the presented model are accurate and suitable for real-life situations.																	0941-0643	1433-3058				APR	2020	32	8					3367	3385		10.1007/s00521-019-04184-y													
J								Optimal design of damage tolerant composite using ply angle dispersion and enhanced bat algorithm	NEURAL COMPUTING & APPLICATIONS										Laminated composite; Bat algorithm; Tsai-Wu failure curve; Constrained optimization; Stacking sequence; Damage tolerance	MAXIMUM FUNDAMENTAL-FREQUENCY; STACKING-SEQUENCE DESIGN; GENETIC ALGORITHMS; OPTIMIZATION; LAMINATE	In this work, minimum weight optimization of laminated composite is performed using a newly developed enhanced bat algorithm (EBA). Bat algorithm (BA) is a recently developed swarm-based optimization technique which is inspired by the echolocation behavior of bats. The standard BA shows premature convergence and reduced convergence speeds under some conditions. So, the EBA is used to perform the design optimization of laminated composites. The laminate analysis based on classical laminate theory is utilized for the stress calculations. Tsai-Wu failure curve is considered as the constraint in this constrained optimization problem. Number of plies at each orientation angle are considered as the design variables. The design optimization has been carried out for both conventional and unconventional (dispersed plies) stacking sequences considering different loading configurations: uniaxial tension, biaxial tension with and without shear loadings. Ply angles dispersed in the range of 5 circle\-85 circle 25 circle-65 circle and 0 circle\-90 circle} at intervals of 5 circle are considered for the unconventional stacking sequence to increase damage tolerance. In addition, a new mathematical function is proposed to measure the dispersion of ply angles in the laminate called the dispersion function. Also, the performance of EBA is compared with standard BA in the optimum weight design of composite laminates.																	0941-0643	1433-3058				APR	2020	32	8					3387	3406		10.1007/s00521-019-04455-8													
J								An adaptive PID control algorithm for the two-legged robot walking on a slope	NEURAL COMPUTING & APPLICATIONS										Adaptive control; Sloping surface; NN; Torque-based PID controller	BIPED ROBOT; TRAJECTORY GENERATION	The crux in designing the PID controller lies in determining its gain values, which play a major role in deciding its performance. The gains are fed as inputs to the controller and are to be decided before its run. On the other hand, the effectiveness of the biped walk purely depends on the performance of the PID controller. Initially, the upper and lower body gaits of the two-legged robot are determined using the concept of inverse kinematics. Further, the dynamics of the biped robot is derived by using Lagrange-Euler formulation. The main objective of the present research is to decide the gains of the torque-based PID controller with the help of a neural network trained by using nature-inspired optimization algorithms, namely MCIWO and PSO. The adaptiveness of the algorithm lies in modifying the gains of the controller based on the magnitude of the error in the angular displacement received at the input to the NN. Once the controller is developed, its effectiveness is tested in computer simulations. Finally, the optimum controlled gait angles obtained by the best approach are tested on a real biped robot.																	0941-0643	1433-3058				APR	2020	32	8					3407	3421		10.1007/s00521-019-04326-2													
J								Data-driven historical preservation: a case study in Shanghai	NEURAL COMPUTING & APPLICATIONS										Historical preservation; Urban planning; Machine learning; Artificial neural network		Historical preservation is becoming ever important in globalizing Shanghai city. However, traditional survey-based ways of policy making are not efficient. This work introduces the data-driven technique with machine learning algorithm to find the relationship between the features of the historical sites and the popularity, which relates to the economy such as tourism and the associated GDP contribution. The method is automatic, which relieves the work load from statistical surveys and other inefficient traditional approaches. Moreover, while the surveys can only reflect the current conditions, the machine learning approach has the ability of predicting the possible outcomes based on existing data, which is helpful when decisions on protection and development are to be made. We collect data from selected historical sites in Shanghai to illustrate the procedure of the proposed data-driven approach. The case study demonstrates the capability of prediction and shows its promising future in guiding policy making, resource allocation and scientific research.																	0941-0643	1433-3058				APR	2020	32	8					3423	3430		10.1007/s00521-018-3710-z													
J								Neural correlates of action video game experience in a visuospatial working memory task	NEURAL COMPUTING & APPLICATIONS										Action video game (AVG); Electroencephalogram (EEG); Working memory; Wavelet transform	SHORT-TERM-MEMORY; EEG SIGNALS; THETA OSCILLATIONS; ALPHA; CLASSIFICATION; INFORMATION; MODULATION; WAVELET; BAND	Studies have shown improved cognitive performance of action video game players (AVGPs); however, the corresponding neural correlates are still unclear. The present study aims to investigate whether the unsupervised training on action video games contributes to the visuomotor adaptation of working memory or not, considering neural measures. Twenty-seven-Channel electroencephalogram (EEG) signals from 35 participants are recorded while performing Corsi block-tapping task (CBTT), a working memory task incorporating recall of visual and sequential-spatial information, and are analyzed employing wavelet transform-based feature extraction techniques. For removing artifacts, we have used the wavelet denoising method. Different mother wavelet functions, with different thresholding algorithms, are compared. The dmeyer mother wavelet function along with Rigorous SURE soft thresholding emerged as the best combination for denoising. Comparison of wavelet energy features has revealed improved rhythmic activity in AVGPs. Also, two measures of engagement index beta/(alpha + theta) and beta/alpha are computed which has provided good classification accuracies with ANFIS classifier. Higher engagement indexes are found in the frontal, right parietal and occipital cortex of AVGPs. These results evidence the neurocortical modulation among the players that may attribute to the effective cognitive processes in AVGPs related to the task.																	0941-0643	1433-3058				APR	2020	32	8					3431	3440		10.1007/s00521-018-3713-9													
J								Decentralized robust optimal control for modular robot manipulators via critic-identifier structure-based adaptive dynamic programming	NEURAL COMPUTING & APPLICATIONS										Modular robot manipulator; Decentralized control; Adaptive dynamic programming; Optimal control; Neural network; Interconnected dynamic coupling	NONLINEAR INTERCONNECTED SYSTEMS; REINFORCEMENT LEARNING CONTROL; FAULT-TOLERANT CONTROL; POLICY ITERATION; TRACKING CONTROL; HJB SOLUTION; DESIGN	This paper presents a decentralized robust optimal control method for modular robot manipulators (MRMs) via a critic-identifier structure-based adaptive dynamic programming (ADP) scheme. The robust control problem of MRMs is transformed into an optimal compensation control issue, which consists of model-based compensation control, identifier-based learning control and ADP-based optimal control. The dynamic model of MRMs is deployed for each joint module where the local dynamic information is utilized to design the model compensation controller. A neural network (NN) identifier is established to approximate the interconnected dynamic coupling. Based on ADP and local online policy iteration algorithm, the Hamiltonian-Jacobi-Bellman equation is solved by constructing a critic NN, and then the approximate optimal control policy derivation is possible. The closed-loop robotic system is asymptotic stable by the implementation of a set of developed decentralized control policies. Simulations are presented to demonstrate the effectiveness of the proposed method.																	0941-0643	1433-3058				APR	2020	32	8					3441	3458		10.1007/s00521-018-3714-8													
J								Odds ratio function estimation using a generalized additive neural network	NEURAL COMPUTING & APPLICATIONS										Generalized additive neural network; Flexible link function; Mortality prediction; Odds ratio function	MODELS; LINK	In biomedical research, generalized artificial neural networks (GANNs) have been proposed as an alternative to a multi-layer perceptron owing to their greater ability to generate more interpretable results. GANNs were inspired by statistical generalized additive models (GAMs), and because of the parallelism that can be established between ANNs and GAMs, it is natural for advances in GAMs to be incorporated into the field of neural networks. A GANN with a flexible link function was recently proposed, with results similar to those of a GAM with the same type of link function. However, in the medical field, more improvements must be introduced to obtain even more interpretable, and consequently more useful, ANNs. In this study, an algorithm for estimating the odds ratio function for continuous covariates is proposed, which increases the interpretability of a GANN.																	0941-0643	1433-3058				APR	2020	32	8					3459	3474		10.1007/s00521-019-04189-7													
J								NSNAD: negative selection-based network anomaly detection approach with relevant feature subset	NEURAL COMPUTING & APPLICATIONS										Intrusion detection system (IDS); Anomaly detection; Feature selection; Artificial immune system (AIS); Negative selection; NSL-KDD dataset; Kyoto2006+dataset; UNSW-NB15 dataset	INTRUSION DETECTION; SYSTEMS	Intrusion detection systems are one of the security tools widely deployed in network architectures in order to monitor, detect and eventually respond to any suspicious activity in the network. However, the constantly growing complexity of networks and the virulence of new attacks require more adaptive approaches for optimal responses. In this work, we propose a semi-supervised approach for network anomaly detection inspired from the biological negative selection process. Based on a reduced dataset with a filter/ranking feature selection technique, our algorithm, namely negative selection for network anomaly detection (NSNAD), generates a set of detectors and uses them to classify events as anomaly. Otherwise, they are matched against an Artificial Human Leukocyte Antigen in order to be classified as normal. The accuracy and the computational time of NSNAD are tested under three intrusion detection datasets: NSL-KDD, Kyoto2006+ and UNSW-NB15. We compare the performance of NSNAD against a fully supervised algorithm (Naive Bayes), an unsupervised clustering algorithm (K-means) and a semi-supervised algorithm (One-class SVM) with respect to multiple accuracy metrics. We also compare the time incurred by each algorithm in training and classification stages.																	0941-0643	1433-3058				APR	2020	32	8					3475	3501		10.1007/s00521-019-04396-2													
J								Fault detection in distillation column using NARX neural network	NEURAL COMPUTING & APPLICATIONS										Aspen plus (R) simulation; Distillation column; Fault detection; NARX neural network; Nonlinear process; Process monitoring	PRINCIPAL COMPONENT ANALYSIS; DIAGNOSIS; IDENTIFICATION; CLASSIFICATION; ALGORITHM	Fault detection in the process industries is one of the most challenging tasks. It requires timely detection of anomalies which are present with noisy measurements of a large number of variable, highly correlated data with complex interactions and fault symptoms. This study proposes the robust fault detection method for the distillation column. Fault detection and diagnosis (FDD) for process monitoring and control has been an effective field of research for two decades. This area has been used widely in sophisticated engineering design applications to ensure the proper functionality and performance diagnosis of advanced and complex technologies. Robust fault detection of the realistic faults in distillation column in dynamic condition has been considered in this study. For early detection of faults, the model is based on nonlinear autoregressive with exogenous input (NARX) network. Tapped delays lines (TDLs) have been used for the input and output sequences. A case study was carried out with three different fault scenarios, i.e., valve sticking at reflux and reboiler, and tray upset. These faults would cause the product degradation. The normal data (no fault) is used for the training of neural network in all three cases. It is shown that the proposed algorithm can be used for the detection of both internal and external faults in the distillation column for dynamic system monitoring and to predict the probability of failure.																	0941-0643	1433-3058				APR	2020	32	8					3503	3519		10.1007/s00521-018-3658-z													
J								Measuring distance-based semantic similarity using meronymy and hyponymy relations	NEURAL COMPUTING & APPLICATIONS										Semantic similarity; Path distance; WordNet; Semantic relationship; Structure property	INFORMATION-CONTENT; PARALLEL FRAMEWORK; WORDNET; MODEL; CONTEXT; VECTOR; EDGE	The assessment of semantic similarity between lexical terms plays a critical part in semantic-oriented applications for natural language processing and cognitive science. The optimization of calculation models is still a challenging issue for improving the performance of similarity measurement. In this paper, we investigate WordNet-based measures including distance-based, information-based, feature-based and hybrid. Among them, the distance-based measures are considered to have the lowest computational complexity due to simple distance calculation. However, most of existing works ignore the meronymy relation between concepts and the non-uniformity of path distances caused by various semantic relations, in which path distances are simply determined by conceptual hyponymy relation. To solve this problem, we propose a novel model to calculate the path distance between concepts, and also propose a similarity measure which nonlinearly transforms the distance to semantic similarity. In the proposed model, we assign different weights in accordance with various relations to edges that link different concepts. On basis of the distance model, we use five structure properties of WordNet for similarity measurement, which consist of multiple meanings, multiple inheritance, link type, depth and local density. Our similarity measure is compared against state-of-the-art WordNet-based measures on M&C dataset, R&G dataset and WS-353 dataset. According to experiment results, the proposed measure in this work outperforms others in terms of both Pearson and Spearman correlation coefficients, which indicates the effectiveness of our distance model. Besides, we construct six additional benchmarks to prove that the proposed measure maintains stable performance.																	0941-0643	1433-3058				APR	2020	32	8					3521	3534		10.1007/s00521-018-3766-9													
J								Application of soft computing techniques for estimating emotional states expressed in Twitter (R) time series data	NEURAL COMPUTING & APPLICATIONS										Human emotional states; Adaptive neuro-fuzzy inference systems; Artificial neural networks; Fuzzy time series; Twitter (R)	FORECASTING ENROLLMENTS; FUZZY	Because the emotional states of selected social groups may constitute a complex phenomenon, a suitable methodology is needed to analyze Twitter(R) text data that can reflect social emotions. Understanding the nature of social barometer data in terms of its underlying dynamics is critical for predicting the future states or behaviors of large social groups. This study investigated the use of the supervised soft computing techniques (1) fuzzy time series (FTS), (2) artificial neural network (ANN)-based FTS, and (3) adaptive neuro-fuzzy inference systems (ANFIS) for predicting the emotional states expressed in Twitter(R) data. The examined dataset contained 25,952 data points reflecting more than 380,000 Twitter(R) messages recorded hourly. The model prediction accuracy was performed using the root-mean-square error. The ANFIS approach resulted in the most accurate prediction among the three examined soft computing approaches. The findings of the study showed that the FTS, ANN-based FTS, and ANFIS models could be used to predict the emotional states of a large social group based on historical data. Such a modeling approach can support the development of real-time social and emotional awareness for practical decision-making, as well as rapid socio-cultural assessment and training.																	0941-0643	1433-3058				APR	2020	32	8					3535	3548		10.1007/s00521-019-04048-5													
J								A novel online method for identifying motion artifact and photoplethysmography signal reconstruction using artificial neural networks and adaptive neuro-fuzzy inference system	NEURAL COMPUTING & APPLICATIONS										Photoplethysmography; Motion artifact; Artificial neural network; ANFIS; Pearson correlation; Intraclass correlation	HEART-RATE-VARIABILITY; INDEPENDENT PREDICTOR; PULSE OXIMETRY; SUDDEN-DEATH; REDUCTION; CARDIOMYOPATHY	Photoplethysmography (PPG) is a noninvasive technique to measure blood volume changes in blood vessels. Despite the wide usage of PPG signal in medical and non-medical applications, this signal can be affected by the motion artifacts leading to data loss. In this paper, we proposed an algorithm to detect motion artifacts and reconstruct the corrupted parts of the signal using real-time modeling based on multilayer perceptron (MLP), radial basis function (RBF) artificial neural networks (ANNs) and adaptive-neuro fuzzy inference system (ANFIS). The developed algorithm was applied to reconstruct the corrupted parts of PPG signals of 23 healthy 25- to 28-year-old volunteers. In the experimental phase, the left- and right-hand PPG signals of the volunteers were simultaneously obtained. While the left hand of the subjects were fixed, they were asked to shake their hands without any predetermined pattern, to simulate the real-life motion accelerations. To statistically and physiologically evaluate the performance of the proposed models, Pearson correlation coefficient (PCC), intraclass correlation coefficient (ICC), Bland-Altman plot (with the 95% limits of agreement), and time-domain feature analysis tests were adopted. The results indicated that the ANFIS with subtractive clustering algorithm shows the best performance in modeling the lost parts of the right-hand signals with an average PCC and ICC of 0.80 and 0.77, respectively, with the reference signal (left-hand signals) over all the tests. Also, the proposed ANFIS-based algorithm had an ability to retrieve the important time-domain PPG signal features, namely mean of inter-beat intervals (NN), standard deviations of NN (SDNN), root mean square of standard deviations (RMSSD) and standard deviation of standard deviations (SDSD) without any significant difference at p < 0.05 level to those of the reference signal (left-hand signal).																	0941-0643	1433-3058				APR	2020	32	8					3549	3566		10.1007/s00521-018-3767-8													
J								Face image synthesis with weight and age progression using conditional adversarial autoencoder	NEURAL COMPUTING & APPLICATIONS										Age progression; Face simulation; Face synthesis; Weight synthesis	APPEARANCE; PERCEPTION; MODEL; SHAPE	The appearance of a human face changes with the change in body weight and age. With varying lifestyle choices, it is hard to imagine the appearance of a given human face in years to come. Future self-perception is highly associated with one's emotional state, as well as health behavior. Negative future self-perception can cause negative lifestyle choice and negative health behavior, leading to depression and eating disorder. In this paper, a new methodology is introduced for future self-face image synthesis using age and weight, resulting in visualization of future face image derived from given weight category and age. A Constrained Local Model is first used for weight progressed future face image synthesized and then age-progressed future face image is generated using Conditional Adversarial Auto Encoder. In the final step, both weight progressed and age-progressed face images fed to face morphing module which synthesized future face image by keeping natural looks. Experimental results show the advantages of proposed method with promising results.																	0941-0643	1433-3058				APR	2020	32	8					3567	3579		10.1007/s00521-019-04217-6													
J								Comparison of artificial bee colony and flower pollination algorithms in vehicle delay models at signalized intersections	NEURAL COMPUTING & APPLICATIONS										Delay estimation; Artificial bee colony algorithm; Flower pollination algorithm; Signalized intersection	FUZZY-LOGIC; VISSIM; CORSIM	Delay is a significant research topic since it includes indicators such as travel quality, lost time and fuel consumption. Furthermore, the delay is used for optimization of traffic control systems and determination of the level of service at signalized intersections. Therefore, researchers have focused on accurate estimation of delay. The objective of this study is to simply and accurately estimate the delay and evaluate the performance of the proposed approaches which are artificial bee colony (ABC) and flower pollination algorithms (FPA). In this study, ABC and FPA have been used to develop different delay models which are linear, semi-quadratic, quadratic and power forms. Analysis period (T), the green ratio (g/C; effective green to cycle length) and the degree of saturation (x = v/c; volume to capacity) are used as input parameters while developing the models. The results of present models are compared to estimations obtained from analytical models which are Highway Capacity Manual and Australian (Akcelik) delay models. Semi-quadratic form yielded to best results in terms of coefficient of determination (R-2), mean square error and mean absolute error. Additionally, FPA approach showed better performance than ABC approach finding the optimal solution in the lower number of iterations.																	0941-0643	1433-3058				APR	2020	32	8					3581	3597		10.1007/s00521-018-3670-3													
J								Measuring performance with common weights: network DEA	NEURAL COMPUTING & APPLICATIONS										Network structures; Data envelopment analysis; Common set of weights; Decomposition	DATA ENVELOPMENT ANALYSIS; DECISION-MAKING; EFFICIENCY DECOMPOSITION; MODELS; FRONTIER; UNIT	In conventional data envelopment analysis (DEA), a production system has been seen as a black box for measuring the efficiency without any attention to what is happening inside the system. However, in practice, performance improvement often requires observing the internal structure of the producing system in order to find the sources of inefficiencies. In addition, weight flexibility as a key property of the multiplier DEA models allows a system to totally disregard an assessment factor, either input or output, from the evaluation process by assigning a value of zero or epsilon to its weight. This paper contributes to the existing literature by proposing a common-weights DEA model when the production system includes a number of interrelated processes. To this end, we propose an aggregate DEA model to calculate the most favourable common weights for determining the efficiency of all production systems and their processes at the same time. Our proposed aggregate model not only is linear for equitably evaluating the producing units on the same scale, but also enables us to deal with the mixed network structures. Furthermore, the network system is decomposed into a series system to build a relational network DEA model that emphasises separate relatedness. This greatly reduces the computational complexities for enormous volumes of data in many real applications and treat difficulties in network DEA models including the zero value and fluctuating weights and multiple solutions. Managerially speaking, this paper aims to provide the top management team of a production system with an integrated framework to shape a better strategic decision process about firm performance, which is to treat the sources of inefficiencies and ultimately take corrective actions over the long run. Put differently, the proposed framework helps top managers make proper decisions in complex situations with a view of improving a firm's efficiency in all production divisions, which can be identified as a core competency leading to competitive advantages of the organisation. In the context of performance management, our study is equipped with a simple numerical example and a case study of the non-life insurance companies to demonstrate the applicability of the proposed common-weights network model.																	0941-0643	1433-3058				APR	2020	32	8					3599	3617		10.1007/s00521-019-04219-4													
J								Investigation of different sources in order to optimize the nuclear metering system of gas-oil-water annular flows	NEURAL COMPUTING & APPLICATIONS										Annular regime; Three-phase flow; Volume fraction; Intelligent integrated system; Gray wolf optimization (GWO) algorithm	CONVECTION HEAT-TRANSFER; VOID FRACTION PREDICTION; LIQUID-PHASE DENSITY; PSO-ANFIS APPROACH; 2-PHASE FLOWS; 3-PHASE FLOW; FUZZY; REGIME; IDENTIFICATION	The used metering technique in this paper is based on the multienergy (at least dual) gamma-ray attenuation. The aim of the current study is investigation of different combinations of sources in order to find the best combination for precise metering gas, oil and water percentages in annular three-phase flows. The required data were generated numerically using Monte Carlo N Particle extended (MCNPX) code. As a matter of fact, the current investigation devotes to predict the volume fractions in the annular three-phase flow, on the basis of a multienergy metering system including different radiation sources and one sodium iodide detector, using the hybrid model. Since the summation of volume fractions is constant, a constraint modeling problem exists, meaning that the hybrid model must predict only two volume fractions. Six hybrid models associated with the number of applied radiation sources are employed. The models are applied to predict the oil and gas volume fractions. For the next step, the hybrid models are trained based on numerically obtained data from the MCNPX code. The results show that the best prediction results are obtained for the oil and gas volume fractions of a system with the (Am-241 & Cs-137) radiation sources.																	0941-0643	1433-3058				APR	2020	32	8					3619	3631		10.1007/s00521-018-3673-0													
J								Large-margin Distribution Machine-based regression	NEURAL COMPUTING & APPLICATIONS										Support vector machine; Regression; Large-margin Distribution Machine; <mml; math><mml; mi>epsilon</mml; mi></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$; epsilon $$; end{document}<inline-graphic xlink; href="521_2018_3921_Article_IEq2; gif"; >-insensitive loss; Quadratic loss; Successive over-relaxation	SUPPORT VECTOR MACHINE	This paper presents an efficient and robust Large-margin Distribution Machine formulation for regression. The proposed model is termed as 'Large-margin Distribution Machine-based Regression' (LDMR) model, and it is in the spirit of Large-margin Distribution Machine (LDM) (Zhang and Zhou, in: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2014) classification model. The LDM model optimizes the margin distribution instead of minimizing a single-point margin as is done in the traditional SVM. The optimization problem of the LDMR model has been mathematically derived from the optimization problem of the LDM model using an interesting result of Bi and Bennett (Neurocomputing 55(1):79-108, 2003). The resulting LDMR formulation attempts to minimize the epsilon-insensitive loss function and the quadratic loss function simultaneously. Further, the successive over-relaxation technique (Mangasarian and Musicant, IEEE Trans Neural Netw 10(5):1032-1037, 1999) has also been applied to speed up the training procedure of the proposed LDMR model. The experimental results on artificial datasets, UCI datasets and time-series financial datasets show that the proposed LDMR model owns better generalization ability than other existing models and is less sensitive to the presence of outliers.																	0941-0643	1433-3058				APR	2020	32	8					3633	3648		10.1007/s00521-018-3921-3													
J								Transportation scheduling optimization by a collaborative strategy in supply chain management with TPL using chemical reaction optimization	NEURAL COMPUTING & APPLICATIONS										Supply chain management; Third-party logistics; Collaborative transportation scheduling; Random matrix generation; Chemical reaction optimization; Meta-heuristics algorithms	ALGORITHM	Optimization of supply chain management is a way of ensuring the usability of resources and related technologies at the best possible way. Transportation scheduling of vehicle and transportation nodes in supply chain management is an important factor in order to create a stable chained network by ensuring the highest amount of product distribution and lowest logistics cost. In recent years, a number of programming models like linear programming, heuristics and meta-heuristics optimization approaches are proposed by the researchers to solve this combinatorial NP-hard problem. In this paper, we have studied and analyzed the nature of transportation vehicle scheduling problem in a supply chain network with the help of third-party logistics enterprise by using a meta-heuristic algorithm called chemical reaction optimization (CRO). At first, we have classified all the transportation nodes into three distinct classifications. Then, a collaborative transportation scheduling strategy is used which is based on two significant kinds of transportation nodes. For the first two kinds of nodes, we have randomly created a large number of combined transportation routes, and the vehicle scheduling for the last standalone nodes is created by a random matrix generation. Then, we have proposed a CRO algorithm using four reaction operators with an additional repair operator to find out the best transportation routes within shortest computing time. We named our proposed algorithm as chemical reaction optimization for supply chain management (CRO-SCM). The proposed CRO-SCM algorithm is analyzed with the standard dataset from the proposed model using modified ACO-NSO algorithm which is the state of the art. In addition, a random dataset of different scales of transportation nodes is considered to evaluate the efficiency of the algorithm. Moreover, six different scales of problem sets consisting different number of nodes are adopted to analyze the performance of the proposed CRO algorithm. The simulation results demonstrate that the proposed approach is practical and efficient than existing ACO-based solutions and the experimental results are more efficient and optimal.																	0941-0643	1433-3058				APR	2020	32	8					3649	3674		10.1007/s00521-019-04218-5													
J								Autoscaling Bloom filter: controlling trade-off between true and false positives	NEURAL COMPUTING & APPLICATIONS										Bloom filter; Counting Bloom filter; Autoscaling Bloom filter; True positive rate; False positive rate		A Bloom filter is a special case of an artificial neural network with two layers. Traditionally, it is seen as a simple data structure supporting membership queries on a set. The standard Bloom filter does not support the delete operation, and therefore, many applications use a counting Bloom filter to enable deletion. This paper proposes a generalization of the counting Bloom filter approach, called "autoscaling Bloom filters", which allows adjustment of its capacity with probabilistic bounds on false positives and true positives. Thus, by relaxing the requirement on perfect true positive rate, the proposed autoscaling Bloom filter addresses the major difficulty of Bloom filters with respect to their scalability. In essence, the autoscaling Bloom filter is a binarized counting Bloom filter with an adjustable binarization threshold. We present the mathematical analysis of its performance and provide a procedure for minimizing its false positive rate.																	0941-0643	1433-3058				APR	2020	32	8					3675	3684		10.1007/s00521-019-04397-1													
J								Imbalanced dataset-based echo state networks for anomaly detection	NEURAL COMPUTING & APPLICATIONS										Anomaly detection; Echo state network; Imbalanced dataset; Adaptive threshold	NOVELTY DETECTION; SYSTEM; CLASSIFICATION; PREDICTION	Anomaly detection is a very effective method to extract useful information from abundant data. Most existing anomaly detection methods are based on normal region or some specific algorithms, which ignore the fact that many actual datasets are mainly imbalanced, resulting in not function properly or effectively in practical, especially in the medical field. On the other hand, imbalanced dataset is also a frequently encountered problem in the learning of neural network because the lack of data in a minority class may lead to uneven classification accuracy. In this paper, inspired by these observations, a novel anomaly detection approach by using classical echo state network (ESN), a brain-inspired neural computing model, is presented. The entire dataset of the proposed method obeys an extremely imbalanced distribution, that is, anomalies are much rarer than normal data. And the training dataset has only the normal data. When the ESN is well trained, the parameters in ESN are the memory of normal data. If the normal data are added into the well-trained network, the error between the input data and the corresponding output is smaller compared with the error between abnormal input data and its corresponding output. Then anomaly behavior is detected if the error between the input data and the corresponding predictive value exceeds a certain threshold. Different from setting an invariable threshold arbitrarily for all of the data, the threshold value used in the proposed method is determined from the analysis of information theory and can be adjust adaptively according to different datasets. Experiments on abnormal heart rate detection are conducted to demonstrate and verify the effectiveness of the proposed detection algorithm and theory.																	0941-0643	1433-3058				APR	2020	32	8					3685	3694		10.1007/s00521-018-3747-z													
J								Multilayer perceptron for short-term load forecasting: from global to local approach	NEURAL COMPUTING & APPLICATIONS										Data representation; Forecasting problem decomposition; Neural networks; Short-term load forecasting	ARTIFICIAL NEURAL-NETWORKS; SIMILARITY-BASED METHODS; ELECTRIC-LOAD; MODEL; IMPLEMENTATION; ENGINE	Many forecasting models are built on neural networks. The key issues in these models, which strongly translate into the accuracy of forecasts, are data representation and the decomposition of the forecasting problem. In this work, we consider both of these problems using short-term electricity load demand forecasting as an example. A load time series expresses both the trend and multiple seasonal cycles. To deal with multi-seasonality, we consider four methods of the problem decomposition. Depending on the decomposition degree, the problem is split into local subproblems which are modeled using neural networks. We move from the global model, which is competent for all forecasting tasks, through the local models competent for the subproblems, to the models built individually for each forecasting task. Additionally, we consider different ways of the input data encoding and analyze the impact of the data representation on the results. The forecasting models are examined on the real power system data from four European countries. Results indicate that the local approaches can significantly improve the accuracy of load forecasting, compared to the global approach. A greater degree of decomposition leads to the greater reduction in forecast errors.																	0941-0643	1433-3058				APR	2020	32	8					3695	3707		10.1007/s00521-019-04130-y													
J								On the improvement in grey wolf optimization	NEURAL COMPUTING & APPLICATIONS										Grey wolf optimization; Numerical optimization; Antenna arrays; Linear antenna array synthesis	LINEAR-ARRAY SYNTHESIS; ANT-COLONY-OPTIMIZATION; ANTENNA-ARRAYS; DIFFERENTIAL EVOLUTION; POWER-SYSTEM; GLOBAL OPTIMIZATION; GENETIC ALGORITHM; SIDELOBE LEVEL; CUCKOO SEARCH; DESIGN	Grey wolf optimization (GWO) is a recently developed nature-inspired global optimization method which mimics the social behaviour and hunting mechanism of grey wolves. Though the algorithm is very competitive and has been applied to various fields of research, it has poor exploration capability and suffers from local optima stagnation. So, in order to improve the explorative abilities of GWO, an extended version of grey wolf optimization (GWO-E) algorithm is presented. This newly proposed algorithm consists of two modifications: Firstly, it is able to explore new areas in the search space because of diverse positions assigned to the leaders. This helps in increasing the exploration and avoids local optima stagnation problem. Secondly, an opposition-based learning method has been used in the initial half of iterations to provide diversity among the search agents. The proposed approach has been tested on standard benchmarking functions for different population and dimension sizes to prove its effectiveness over other state-of-the-art algorithms. Experimental results show that the GWO-E algorithm performs better than GWO, bat algorithm, bat flower pollinator, chicken swarm optimization, differential evolution, firefly algorithm, flower pollination algorithm (FPA) and grasshopper optimization algorithm. Statistical testing of GWO-E has been done to prove its significance over other popular algorithms. Further, as a real-world application, the GWO-E is used to design non-uniform linear antenna array (LAA) for minimum possible sidelobe level and null control. Performance of GWO-E for the synthesis of LAA is evaluated by considering the several different case studies of LAA that exists in the literature, and the results are compared with the results of other popular meta-heuristic algorithms like genetic algorithm, ant lion algorithm, FPA, cat swarm optimization, GWO and many more. Numerical results further show the superior performance of GWO-E over original GWO and other popular algorithms.																	0941-0643	1433-3058				APR	2020	32	8					3709	3748		10.1007/s00521-019-04456-7													
J								Similarity measure on incomplete imprecise interval information and its applications	NEURAL COMPUTING & APPLICATIONS										Interval-valued intuitionistic fuzzy number; Similarity measure; TOPSIS method; Pattern recognition	INTUITIONISTIC FUZZY-SETS; COMPLETE RANKING; VAGUE SETS; DISTANCE	The concept of fuzzy numbers has been generalized to intuitionistic fuzzy interval numbers (IFINs) to solve problems with imprecision in the information modeling. Similarity measure is an important tool to measure the degree of resemblance between any two objects in real-life situations and is applied in many areas such as decision making, image processing, pattern recognition, etc. In this paper, a new distance-based similarity measure between IFINs is proposed using which a similarity measure on incomplete imprecise interval information is attempted. Some properties of the proposed distance measure and similarity measure are studied using illustrative examples. The nominal decreasing and increasing properties based on the proposed distance measure and similarity measure are proved. Further, the superiority of the proposed similarity measure over familiar existing methods is shown by different numerical examples and the proposed measure is applied to technique for order preference by similarity to ideal solution method under interval-valued intuitionistic fuzzy environment. Finally, the applicability of the proposed method in pattern recognition problems is illustrated.																	0941-0643	1433-3058				APR	2020	32	8					3749	3761		10.1007/s00521-019-04277-8													
J								Fuzzy c-means and K-means clustering with genetic algorithm for identification of homogeneous regions of groundwater quality	NEURAL COMPUTING & APPLICATIONS										Wilcox; Piper; Clustering; FCM; K-means; Iran	MULTIVARIATE-STATISTICS; WATER-QUALITY; CHEMISTRY; MANAGEMENT; RESOURCES; FLUORIDE; SYSTEMS; PLAIN	In this study, two different clustering algorithms, fuzzy c-means (FCM) and K-means with genetic algorithm, were used to identify the homogeneous regions in terms of groundwater water quality. For this purpose, data of 14 hydrochemical parameters from 108 wells were sampled in 2016, Golestan province, northeast of Iran. The results showed that the optimal clusters of the K-means and FCM were 5 and 6, respectively. The evaluation of water quality by FCM for drinking uses showed that in terms of total dissolved solid (TDS) and chlorine (Cl) parameters, cluster 3 was in an unfavorable condition. Moreover, according to the K-means algorithm, cluster 1 was in inappropriate condition in terms of the TDS and Cl. Water quality assessment by FCM for agricultural use showed that in general, cluster 3 was not in a good condition, especially for the electrical conductivity (EC) parameter. Also, according to the K-means, in general, cluster 1 had an inappropriate state for the EC and sodium adsorption ratio parameters. Investigating the hydrochemical facies of clusters using the FCM and K-means showed that in the northern half of the Golestan province, most samples are Cl-Na and in the southern half, most of the samples are HCO3-Ca. In general, by comparing the results of clustering algorithms, it was found that the FCM algorithm has better results than the K-means clustering algorithm, mainly due to consideration of uncertainty conditions in determining the class boundary.																	0941-0643	1433-3058				APR	2020	32	8					3763	3775		10.1007/s00521-018-3768-7													
J								Classification of EEG signals using hybrid combination of features for lie detection	NEURAL COMPUTING & APPLICATIONS										Electroencephalogram; Empirical mode decomposition; Features extraction; Support vector machine	CONCEALED INFORMATION TEST; EMPIRICAL MODE DECOMPOSITION; FEATURE-EXTRACTION; WAVELET ANALYSIS; RECOGNITION; P300; EMD; AWARENESS; COMPONENT; SEIZURE	The present work demonstrates the effectiveness of the combination of time, frequency, time-frequency, and statistical features extracted from the electroencephalogram (EEG) data, with support vector machine (SVM) for lie detection. Predominantly, the features extracted from the empirical mode decomposition (EMD) of the EEG data significantly improve the classification accuracy. A specific number of narrow band oscillatory components, called intrinsic mode functions (IMFs), are obtained after EMD of the data. The first three IMFs are selected to extract three time and three frequency domain statistical features corresponding to each IMF. These features are chosen due to the strong data adaptation capability of EMD for the transient signals such as an EEG. Furthermore, the features are selected keeping in mind the differences in the distribution, average value, and regularity of the guilty and innocent subjects' brain signals. The proposed combination of extracted features with customized SVM demonstrates better accuracy than the other state-of-the-art feature extraction methods reported earlier. The proposed hybrid combination of features prominently distinguishes the guilty and innocent subjects with the classification accuracy of 99.44%.																	0941-0643	1433-3058				APR	2020	32	8					3777	3787		10.1007/s00521-019-04078-z													
J								A mechanical data analysis using kurtogram and extreme learning machine	NEURAL COMPUTING & APPLICATIONS										Extreme learning machine (ELM); Fault diagnosis; Kurtogram; Rolling element bearings (REBs)	BEARING FAULT-DIAGNOSIS; SPECTRAL KURTOSIS; SCHEME	Today's industry demands precise functioning and zero failure of rotating machinery (RM) to avoid disastrous accidents as well as financial losses. Rolling element bearings (REBs) are the heart of RM. Therefore, as early as possible to provide the significant time for maintenance planning, an intelligent diagnosis of REB fault is a critical and challenging task. Thus, this paper presents an efficient method for fault diagnosis. The proposed method mainly consists of two consecutive units: (1) generation of kurtogram of raw vibration signal and (2) training of extreme learning machine (ELM) classifier using kurtogram. Kurtogram has a distinct capability to represent the hidden non-stationary components of a raw signal. Therefore, it is considered as a unique feature vector for fault classification. ELM is a well-organized fast learning method proposed by Huang et al. and showed that it is better than traditional learning algorithms. However, one of the open issues of ELM is to design compact-size ELM architecture by preserving the accuracy of the solution. Thus, improved random increment ELM is proposed in this paper. Initially, it randomly adds the nodes to network architecture to rapidly reduce the residual error up to the predefined threshold and then sequentially adds the nodes to the network architecture for further reducing the residual error. Performance of the proposed routine is evaluated by REB vibration data: artificially generated vibration data and Case Western Reserve University bearing data. The experimental study reveals the classification accuracy of the proposed approach with both the datasets for various faults and also compared with existing methods.																	0941-0643	1433-3058				APR	2020	32	8					3789	3801		10.1007/s00521-019-04398-0													
J								A novel improved antlion optimizer algorithm and its comparative performance	NEURAL COMPUTING & APPLICATIONS										Tournament selection; Heuristic optimization; Improved antlion; ANFIS	ANFIS; IDENTIFICATION; NETWORKS; SEARCH	In this study, the improvement of the ant lion optimization which is inspired by ant lion's hunting strategy is dealt with. The most disadvantageous property of this algorithm is its having a long run time due to the random walking process. In order to overcome this drawback, we proposed the improved random walking model, tournament selection method instead of the roulette wheel selection method, and reproduction mechanism at the boundary values. The performance of improved ant lion optimization algorithm based on the tournament selection (IALOT) is evaluated in comparison with the commonly known and used heuristic algorithms for ten benchmark functions. Furthermore, we have tested the performance of IALOT on the training of ANFIS known as a difficult optimization problem. The benchmark and ANFIS test results show that IALOT algorithm exhibits better performance than that of the ALO algorithm.																	0941-0643	1433-3058				APR	2020	32	8					3803	3824		10.1007/s00521-018-3871-9													
J								Optimization of green RNP problem for LTE networks using possibility theory	NEURAL COMPUTING & APPLICATIONS										Green LTE network planning; BS switching; Energy consumption; Genetic algorithms; Possibility theory	FUZZY-LOGIC	At present, the demand for natural energy has been ever increasing, so energy has become a major concern for everyone. As Long Term Evolution Base Stations consume a large amount of the total energy expenditure in a cellular network, it is of keen interest to researchers to reduce the energy consumed by BSs when considering network planning. In this paper, we consider the green radio network planning problem for the LTE cellular networks. Our aim is to reduce energy consumption by reducing the number of active BSs, which will also reduce the production of carbon dioxide. Now BSs are currently operated and deployed for the worst traffic peak estimates. However, traffic fluctuates with time depending on the mobile stations behavior and their data needs. From our point of view, in order to investigate more realistic cases, we consider the situation where the traffic information is taken as imprecise and uncertain value. So, we introduce a model of problem where each traffic is a fuzzy variable, and then, we present a decision-making model based on possibility theory. To solve the problem, we propose a solution method using genetic algorithms and a dynamic Evolved Node B switching on/off strategy. The obtained results showed the efficiency of our approach and demonstrated considerable energy saving, through dynamic adaptation of the number of active BSs.																	0941-0643	1433-3058				APR	2020	32	8					3825	3838		10.1007/s00521-018-3943-x													
J								A novel hybrid algorithm for aiding prediction of prognosis in patients with hepatitis	NEURAL COMPUTING & APPLICATIONS										Multilayer perceptron; Decision support; Prognosis; Hepatitis	DIAGNOSIS SYSTEM; LIVER FIBROSIS; DISEASE; CLASSIFICATION; RISK	This study investigated the application of a novel hybrid artificial intelligence (AI)-based classifier for aiding prediction of the prognosis in patients with chronic hepatitis. Nineteen biomarkers on 155 patients with hepatitis from the University California Irvine Machine Learning repository were used as input data. Weights derived by applying the geometric margin maximisation criterion of a Lagrangian support vector machine (LSVM) were used for selecting the features associated with the highest relative importance towards the required classification, i.e. to predict whether a patient with hepatitis would have survived or died. Thus, the 19 initial features were reduced to the 16 most important prognostic factors and were fed into various AI-based classifiers. Results indicated an overall classification accuracy and area under the receiver operating characteristic curve of 100% for the proposed hybrid algorithm, the LSVM multilayer perceptron (MLP), thus demonstrating its potential for aiding prediction of prognosis in patients with hepatitis in a clinical setting.																	0941-0643	1433-3058				APR	2020	32	8					3839	3852		10.1007/s00521-019-04050-x													
J								Online continuous multi-stroke Persian/Arabic character recognition by novel spatio-temporal features for digitizer pen devices	NEURAL COMPUTING & APPLICATIONS										Continuous character recognition; Digitizer pen; Genetic programming-based decision tree (GPDT); Non-deterministic finite automata (NDFA)	COMPUTATIONAL INTELLIGENCE; FUSION; HMM	Nowadays, digitizer pens have become front end of many digital devices. The increasing use of this technology has necessitated the need for producing pen-based virtual keyboard systems. Despite attempts to create such systems in English, their absence for Persian/Arabic languages is an obvious defect. The goal of this paper is presenting an online continuous Persian/Arabic character recognition method. A character in Persian/Arabic language is made of two types of signs or strokes: main body and delayed strokes (which may be zero or more sign). In this paper, a set of novel and discriminative spatial features are defined for these strokes. These features then are used in a novel algorithm to create a genetic programming-based decision tree called GPDT. The GPDT and spatio-temporal features are utilized by non-deterministic finite automata (NDFA) to recognize group-related strokes and related characters. The reason for using spatio-temporal features is the sameness of the main body of some Persian/Arabic letters (e.g., "& x62d;& x60c; & x62e;& x60c; & x62c;& x60c; & x686;"). There are also two other issues related to recognizing Persian/Arabic letters: unknown number of delayed stroke segments and the sameness of delayed strokes placement, which are removed by using an NDFA. In fact, after identifying group of main body with the help of GPDT, each recognized stroke makes a move in NDFA to stop in a character state (final state on the end of a path in NDFA). The proposed algorithm recognizes continuous Persian/Arabic letters and digits with a 92.43% accuracy and isolated letters and digits with 97.52% accuracy.																	0941-0643	1433-3058				APR	2020	32	8					3853	3872		10.1007/s00521-019-04225-6													
J								Neural network modeling of in situ fluid-filled pore size distributions in subsurface shale reservoirs under data constraints	NEURAL COMPUTING & APPLICATIONS										Convolution; Recurrent; Neural network; Shale; Nuclear magnetic resonance; Deep learning	WELL LOG DATA; COMMITTEE MACHINE; PREDICTION; SHEAR	Subsurface nuclear magnetic resonance (NMR) logs acquired in the wellbore environment are sensitive to fluid-filled pore size distribution, fluid mobility, permeability, and porosity in the near-wellbore reservoir volume. NMR response of a formation layer is processed to extract the T2 distribution, which approximates the fluid-filled pore size distribution. NMR logs are acquired in limited number of wells due to financial and operational challenges, which adversely affects reservoir characterization. We developed two neural-network-based machine learning techniques, long short-term memory (LSTM) network and variational autoencoder with a convolutional layer (VAEc) network, to process the 'easy-to-acquire' formation mineral and fluid saturation logs to generate synthetic NMR T2 distributions in the absence of 'hard-to-acquire' NMR T2 distribution log. Both the predictive models are trained and tested on limited wireline log measurements randomly selected from a 300-ft depth interval of the Bakken shale formation. Synthesis performances of LSTM and VAEc models in terms of R-2 are 0.78 and 0.75, respectively. Noise is inevitable in logging data due to the complex wellbore and formation conditions. Notably, both the predictive models robustly synthesize the fluid-filled pore size distributions in the presence of 50% noise in input logs and 30% noise in training T2 data. The performance of the proposed methodology improves with access to larger volume of training data from other formation types. The proposed method is critical to the synthesis of in situ fluid-filled pore size distributions in shale formations under data constraints due to financial and operational challenges.																	0941-0643	1433-3058				APR	2020	32	8					3873	3885		10.1007/s00521-019-04124-w													
J								A projection-based recurrent neural network and its application in solving convex quadratic bilevel optimization problems	NEURAL COMPUTING & APPLICATIONS										Bilevel optimization problem; Artificial neural network; Dynamical systems; Globally convergent	PROGRAMMING PROBLEMS; MODEL	In this paper, a projection-based recurrent neural network is proposed to solve convex quadratic bilevel programming problems (CQBPP). The Karush-Kuhn-Tucker optimal conditions (KKT) of the lower level problem are used to obtain identical one-level optimization problem. A projected dynamical system which its equilibrium point coincides with the global optimal solution of the corresponding optimization problem is presented. Compared to existing models, the proposed model has the least number of variables and a simple structure with low complexity. Analytically, it is demonstrated that the state vector of the suggested neural network model is stable in the sense of Lyapunov and globally convergent to an optimal solution of CQBPP in finite time. Some numerical examples, a supply chain model and an application deals with an environmental problem are discussed in order to confirm the efficiency of the theoretical results and the performance of the model.																	0941-0643	1433-3058				APR	2020	32	8					3887	3900		10.1007/s00521-019-04391-7													
J								Intuitionistic fuzzy divergence measure-based ELECTRE method for performance of cellular mobile telephone service providers	NEURAL COMPUTING & APPLICATIONS										Divergence measure; Intuitionistic fuzzy sets; ELECTRE; Outranking relation; TSPs MCDM	MULTICRITERIA DECISION-MAKING; AVERAGING AGGREGATION OPERATORS; INFORMATION MEASURES; ENTROPY MEASURES; HESITANT FUZZY; SETS; SELECTION; MODEL; EFFICIENCY; CRITERIA	In India, Telecom sector has been enduring a massive growth for over two and half decades in the bandwidth, preference of services and depth of penetration in urban, semi-urban and rural areas. Exploring the service providers or selecting the best cellular mobile telephone service providers (CMTSPs/TSPs) according to operational performance parameter can help to achieve the goal of telecom service provider selection (TSPS). Here, this assessment generally comprises various TSPs alternatives and different operational performance criteria; therefore, TSPS could be considered as a multi-criteria decision-making (MCDM) problem. The analytical data have been taken from the Telecom Regulatory Authority of India (TRAI), Jan-Mar 2017. Most of the data vary from 1 month to another. Thus, these small ranges of variation in the data are incorporated with the intuitionistic fuzzy numbers (IFNs). In this paper, we develop an outranking method, intuitionistic fuzzy ELECTRE (IF-ELECTRE), for MCDM problems. Proposed method is based on divergence measure, operators of IFNs, some modifications of classical ELECTRE method. In the MCDM, the criterion weight determination is a vital issue for the accuracy of evaluation results. In the process of calculation of criterion weight, concordance and discordance values, we propose new Jensen-Shannon divergence (JS-divergence) for IFSs to overcome the shortcoming of the existing ones and demonstrate different elegant properties of the proposed one. Since the uncertainty is an inevitable feature of MCDM problems, the proposed method can be a valuable tool for decision-making in an uncertain atmosphere to determine the partial-preference outranking order. To express the strength and reasonableness of the proposed method in the real-world MCDM problems, a TSPS problem is implemented. Using the proposed framework, a decision expert can make a strategy to improve the performance by benchmarking operational parameters. Finally, a comparison is discussed between the results of the proposed method and the existing ones for validating the proposed method. This analysis shows that the proposed method is efficient and well consistent with the other methods.																	0941-0643	1433-3058				APR	2020	32	8					3901	3921		10.1007/s00521-018-3716-6													
J								Weighted differential evolution algorithm for numerical function optimization: a comparative study with cuckoo search, artificial bee colony, adaptive differential evolution, and backtracking search optimization algorithms	NEURAL COMPUTING & APPLICATIONS										Cuckoo search algorithm; Artificial bee colony algorithm; Differential evolution algorithm; Backtracking search optimization; Particle swarm optimization	PARTICLE SWARM; ROBUST ESTIMATION; DESIGN	In this paper, weighted differential evolution algorithm (WDE) has been proposed for solving real-valued numerical optimization problems. When all parameters of WDE are determined randomly, in practice, WDE has no control parameter but the pattern size. WDE can solve unimodal, multimodal, separable, scalable, and hybrid problems. WDE has a very fast and quite simple structure, in addition, it can be parallelized due to its non-recursive nature. WDE has a strong exploration and exploitation capability. In this paper, WDE's success in solving CEC' 2013 problems was compared to 4 different EAs (i.e., CS, ABC, JADE, and BSA) statistically. One 3D geometric optimization problem (i.e., GPS network adjustment problem) and 4 constrained engineering design problems were used to examine the WDE's ability to solve real-world problems. Results obtained from the performed tests showed that, in general, problem-solving success of WDE is statistically better than the comparison algorithms that have been used in this paper.																	0941-0643	1433-3058				APR	2020	32	8					3923	3937		10.1007/s00521-018-3822-5													
J								A comparative study of artificial neural networks in predicting blast-induced air-blast overpressure at Deo Nai open-pit coal mine, Vietnam	NEURAL COMPUTING & APPLICATIONS										Artificial neural network; Multilayer perceptron neural network; Bayesian regularized neural networks; Hybrid neural fuzzy inference system; Air-blast overpressure; Deo Nai; Vietnam	INDUCED GROUND VIBRATION; AIRBLAST-OVERPRESSURE; PARAMETERS; OPERATIONS; ALGORITHM; ERROR	Air-blast overpressure (AOp) is one of the undesirable effects caused by blasting operations in open-pit mines. This side effect of blasting can seriously undermine surrounding residential structures and living quality. To control and mitigate this situation, this study using artificial neural networks to predict AOp implemented at Deo Nai open-pit coal mine, Vietnam. A total of 146 events of blasting were recorded, of which 80% (118 observations) was used for training and 20% (28 observations) was used for testing. A resampling technique, namely tenfold cross-validation, was performed with three repeats to increase the accuracy of the predictive models. In this paper, three different types of neural networks were developed to predict AOp including multilayer perceptron neural network (MLP neural nets), Bayesian regularized neural networks (BRNN) and hybrid neural fuzzy inference system (HYFIS). Each type was tested with ten model configurations to discover the best performing ones based on comparing standard metrics, including root-mean-square error (RMSE), coefficient of determination (R-2), and a simple ranking method. Eight parameters were considered for these models, including charge per delay, burden, spacing, length of stemming, powder factor, air humidity, and monitoring distance. The results indicated that MLP neural nets model with RMSE = 2.319, R-2 = 0.961 on testing datasets and a total ranking of 12 yielded the most accurate prediction over BRNN and HYFIS models.																	0941-0643	1433-3058				APR	2020	32	8					3939	3955		10.1007/s00521-018-3717-5													
J								Multilevel split of high-dimensional water quality data using artificial neural networks for the prediction of dissolved oxygen in the Danube River	NEURAL COMPUTING & APPLICATIONS										Similarity metrics; PMIS; DO prediction; Ward neural network	VARIABLE SELECTION; INPUT; MODEL; ELM	In this study, a self-organizing network-based monitoring location similarity index (LSI) was coupled with Ward neural networks (WNNs) with the aim to create a more accurate, but less complex, multiple sites model for the prediction of dissolved oxygen (DO) content. This multilevel splitting approach comprises the LSI-based grouping of monitoring locations according to their similarity, and virtual splitting of processed data based on their features using WNN. The values of 18 water quality parameters monitored for 12 years at 17 sites on the Danube River flow thought Serbia were used. The optimal input combinations were selected using partial mutual information algorithm with termination based on the Akaike information criterion. LSI-based splitting has yielded two groups of monitoring sites that were modeled with separate WNN models. The number and types of selected inputs differed between those two groups of sites, which was in agreement with possible pollution sources. Multiple performance metrics have revealed that the WNN models perform similar or better than multisite DO prediction models published in the literature, while using two to four times less inputs and data patterns.																	0941-0643	1433-3058				APR	2020	32	8					3957	3966		10.1007/s00521-019-04079-y													
J								A closed-loop supply chain robust optimization for disposable appliances	NEURAL COMPUTING & APPLICATIONS										Closed-loop supply chain; Uncertainty; Robust optimization; Genetic algorithm	LOGISTICS NETWORK DESIGN; GENETIC ALGORITHM APPROACH; REVERSE LOGISTICS; STOCHASTIC-MODEL; MANAGEMENT; LOCATION; ENERGY; TIME	Supply chain design is one of the important and strategic decisions influencing competitive advantages and economic growth. The increasing importance of using waste products has led many companies to move toward the design of a closed-loop supply chain network. The current research considers a multilayered closed-loop supply chain for disposable appliances recycling network. Since many parameters, especially demand and costs, are uncertain, discrete random scenarios are used to describe the parameters. The network modeling aimed at maximizing the value of returning products in the reverse network and products manufactured by the forward network. Optimization of the disposable appliance supply chain network is handled by the combined genetic algorithm and robust optimization. Finally, the computational analysis shows that the proposed model obtains effective solutions for the closed-loop network of disposable appliances. Also, the analysis shows that the genetic algorithm has a good convergence. Comparison of different scenarios shows that the objective function is highly sensitive to uncertain parameters. Hence, network modeling based on different scenarios can be a good approach for deciding under uncertainty of parameters.																	0941-0643	1433-3058				APR	2020	32	8					3967	3985		10.1007/s00521-018-3847-9													
J								Optimization of time, cost and surface roughness in grinding process using a robust multi-objective dragonfly algorithm	NEURAL COMPUTING & APPLICATIONS										Dragonfly algorithm; Grinding process; Constrained optimization; Multi-objective optimization; Taguchi method; Robust design	MODEL	Grinding is one of the most commonly used operations in the industry. Optimization of the grinding process can significantly improve the product quality and minimize operational costs and production time. Due to nonlinearity and complexity, optimization of the grinding process is one of the most challenging tasks in the field of mechanical engineering. This paper aims to optimize grinding process considering a tri-objective mathematical model to simultaneous optimization of final surface quality, grinding cost and total process time. To obtain non-dominated Pareto optimal solutions, a novel meta-heuristic algorithm named multi-objective dragonfly algorithm is utilized. Besides, an efficient constraint handling technique is implemented to handle complex operational constraints. Furthermore, an experimental case study is solved using the proposed algorithm and the results are compared to NSGA-II in the literature. The results revealed that the proposed algorithm is able to find non-dominated Pareto optimal solutions and make a significant improvement over the existing approaches.																	0941-0643	1433-3058				APR	2020	32	8					3987	3998		10.1007/s00521-018-3872-8													
J								Probabilistic approaches for music similarity using restricted Boltzmann machines	NEURAL COMPUTING & APPLICATIONS										Music similarity; Restricted Boltzmann machines; Machine learning		In music informatics, there has been increasing attention to relative similarity as it plays a central role in music retrieval, recommendation, and musicology. Most approaches for relative similarity are based on distance metric learning, in which similarity relationship is modelled by a parameterised distance function. Normally, these parameters can be learned by solving a constrained optimisation problem using kernel-based methods. In this paper, we study the use of restricted Boltzmann machines (RBMs) in similarity modelling. We take advantage of RBM as a probabilistic neural network to assign a true hypothesis "x is more similar to y than to z" with a higher probability. Such model can be trained by maximising the true hypotheses while, at the same time, minimising the false hypotheses using a stochastic method. Alternatively, we show that learning similarity relations can be done deterministically by minimising the free energy function of a bipolar RBM or using a classification approach. In the experiments, we evaluate our proposed approaches on music scripts extracted from MagnaTagATune dataset. The results show that an energy-based optimisation approach with bipolar RBM can achieve better performance than other methods, including support vector machine and machine learning rank which are the state-of-the-art for this dataset.																	0941-0643	1433-3058				APR	2020	32	8					3999	4008		10.1007/s00521-019-04106-y													
J								Piezometric level prediction based on data mining techniques	NEURAL COMPUTING & APPLICATIONS										Dam monitoring; Concrete dams; Piezometric levels; Data mining techniques	SUPPORT VECTOR REGRESSION; NEURAL-NETWORKS; MODEL	The safety assessment of dams is a complex task that is made possible thanks to a constant monitoring of pertinent parameters. Once collected, the data are processed by statistical analysis models in order to describe the behaviour of the structure. The aim of those models is to detect early signs of abnormal behaviour so as to take corrective actions when required. Because of the uniqueness of each structure, the behavioural models need to adapt to each of these structures, and thus flexibility is required. Simultaneously, generalization capacities are sought, so a trade-off has to be found. This flexibility is even more important when the analysed phenomenon is characterized by nonlinear features. This is notably the case of the piezometric levels (PL) monitored at the rock-concrete interface of arch dams, when this interface opens. In that case, the linear models that are classically used by engineers show poor performances. Consequently, interest naturally grows for the advanced learning algorithms known as machine learning techniques. In this work, the aim was to compare the predictive performances and generalization capacities of six different data mining algorithms that are likely to be used for monitoring purposes in the particular case of the piezometry at the interface of arch dams: artificial neural networks (ANN), support vector machines (SVM), decision tree, k-nearest neighbour, random forest and multiple regression. All six are used to analyse the same time series. The interpretation of those PL permits to understand the phenomenon of the aperture of the interface, which is highly nonlinear, and of great concern in dam safety. The achieved results show that SVM and ANN stand out as the most efficient algorithms, when it comes to analysing nonlinear monitored phenomenon. Through a global sensitivity analysis, the influence of the models' attributes is measured, showing a high impact of Z (relative trough) in PL prediction.																	0941-0643	1433-3058				APR	2020	32	8					4009	4024		10.1007/s00521-019-04392-6													
J								A TOPSIS multi-criteria decision method-based intelligent recurrent wavelet CMAC control system design for MIMO uncertain nonlinear systems	NEURAL COMPUTING & APPLICATIONS										TOPSIS; Wavelet function; Cerebellar model articulation controller; Nonlinear system	FUZZY NEURAL-NETWORK; SLIDING-MODE CONTROL; FRAMEWORK; ORDER	The main goal of this paper is to design a more effective control algorithm for multiple-input multiple-output uncertain nonlinear systems. Major advantages of the proposed method include: (1) A dynamic deletion threshold developed to consider whether to retain or to delete the hypercubes automatically so as to achieve more efficient network structure; (2) automatically parameter adaptation to achieve favorable control performance; (3) combine various techniques such as sliding-mode control, adaptive control, cerebellar model articulation controller and TOPSIS multi-criteria decision method for getting the advantages of these techniques. Based on the advantages of above techniques, a new intelligent recurrent wavelet cerebellar model articulation control system is designed. The gradient descent method is utilized to online adjust the parameters of the controller, and a Lyapunov function is employed to guarantee the system stability. The effectiveness of the proposed control scheme is validated through the applications for an inverted double pendulum system and an unmanned aerial vehicle motion control.																	0941-0643	1433-3058				APR	2020	32	8					4025	4043		10.1007/s00521-018-3795-4													
J								Non-fragile robust finite-time stabilization and H infinity performance analysis for fractional-order delayed neural networks with discontinuous activations under the asynchronous switching	NEURAL COMPUTING & APPLICATIONS										Uncertain switched fractional-order neural networks; Non-fragile robust <mml; math><mml; msub><mml; mi>H</mml; mi><mml; mi>infinity</mml; mi></mml; msub></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$H_{; infty }$$; end{document}<inline-graphic xlink; href="521_2018_3682_Article_IEq6; gif"; > performance analysis; Discontinuous activation functions; Filippov solution; Asynchronous switching	GLOBAL EXPONENTIAL STABILITY; NONLINEAR-SYSTEMS; STATE ESTIMATION; VARYING DELAYS; NEUTRAL-TYPE; ALGORITHM; SYNCHRONIZATION; CONVERGENCE; CONTROLLER; LEAKAGE	In this paper, the global non-fragile robust finite-time stabilization and H infinity performance analysis are investigated for uncertain switched fractional-order neural networks with discontinuous activations, time delay and external disturbance under the asynchronous switching. Firstly, a new inequality, which is concerned with the fractional derivative of the variable upper limit integral for the nonsmooth integrable functional, is developed. Secondly, the non-fragile switched controller with two types of gain perturbations is designed. Under the Filippov fractional differential inclusion framework, the global non-fragile robust finite-time stabilization conditions are addressed in terms of linear matrix inequalities (LMIs) by applying nonsmooth analysis theory, inequality analysis technique, average dwell-time method and Lyapunov functional approach. In addition, the global non-fragile robust finite-time H infinity performance analysis is performed, and the global non-fragile robust finite-time stabilization conditions with H infinity disturbance attenuation level are also derived in the form of LMIs. Finally, two numerical examples are given to illustrate the feasibility of the proposed non-fragile switched controller and the validity of the theoretical results.																	0941-0643	1433-3058				APR	2020	32	8					4045	4071		10.1007/s00521-018-3682-z													
J								Joint motion boundary detection and CNN-based feature visualization for video object segmentation	NEURAL COMPUTING & APPLICATIONS										Video object segmentation; Class saliency map; Co-segmentation; Convolutional neural network; Feature visualization; Motion boundary	CO-SEGMENTATION; COSEGMENTATION; ALGORITHM; TRACKING	This paper presents a video object segmentation method which jointly uses motion boundary and convolutional neural network (CNN)-based class-level maps to carry out the co-segmentation of the frames. The key characteristic of the proposed approach is a combination of those two sources of information to create initial object and background regions. These regions are employed within the co-segmentation energy function. The motion boundary map detects the areas which contain the object movement, and the CNN-based class saliency map determines the regions with more impact on acquiring the correct network classification. The proposed approach can be implemented on unconstrained natural videos which include changes in an object's appearance, rapidly moving background, object deformation in non-rigid moving, rapid camera motion and even the existence of a static object. Experimental results on two challenging datasets (i.e., Davis and SegTrackv2 datasets) demonstrate the competitive performance of the proposed method compared with the state-of-the-art approaches.																	0941-0643	1433-3058				APR	2020	32	8					4073	4091		10.1007/s00521-019-04448-7													
J								Numerical simulation of fractional-order reaction-diffusion equations with the Riesz and Caputo derivatives	NEURAL COMPUTING & APPLICATIONS										Caputo derivative; Difference schemes; Fractional reaction-diffusion; Numerical simulations; Spatiotemporal oscillations	PARTIAL-DIFFERENTIAL-EQUATIONS; SCHEME	The present paper deals with the numerical solution of space-time-fractional reaction-diffusion problems used to model some complex phenomena that are governed by dynamic of anomalous diffusion. The time- and space-fractional reaction-diffusion equation is modeled by replacing the first-order derivative in time and the second-order derivative in space, respectively, with the Caputo and Riesz operators. We propose an adaptable numerical scheme for the approximation of the derivatives. Accuracy of the method is justified by reporting both the maximum error and relative error in 2D with analytical solutions given. The effectiveness and applicability of the proposed methods are tested on two of practical problems that are of current and recurring interests in one and two dimensions to cover pitfalls that may arise.																	0941-0643	1433-3058				APR	2020	32	8					4093	4104		10.1007/s00521-019-04350-2													
J								FPGA-based system for artificial neural network arrhythmia classification	NEURAL COMPUTING & APPLICATIONS										Electrocardiogram; Discrete wavelet transform; Artificial neural network; Multilayer perceptron; Xilinx system generator; FPGA	WAVELET TRANSFORM; ELECTROCARDIOGRAPHIC CHANGES; ECG; FEATURES; IMPLEMENTATION	The automatic detection and cardiac classification are essential tasks for real-time cardiac diseases diagnosis. In this context, this paper describes a field programmable gates array (FPGA) implementation of arrhythmia recognition system, based on artificial neural network. Firstly, we have developed an optimized software-based medical diagnostic approach, capable of defining the best electrocardiogram (ECG) signal classes. The main advantage of this approach is the significant features minimization, compared to the existing researches, which leads to minimize the FPGA prototype size and saving energy consumption. Secondly, to provide a continuous and mobile arrhythmia monitoring system for patients, we have performed a hardware implementation. The FPGA has been referred due to their easy testing and quick implementation. The optimized approach implementation has been designed on the Nexys4 Artix7 evaluation kit using the Xilinx System Generator for DSP. In order to evaluate the performance of our proposal system, the classification performances of proposed FPGA fixed point have been compared to those obtained from the MATLAB floating point. The proposed architecture is validated on FPGA to be a customized mobile ECG classifier for long-term real-time monitoring of patients.																	0941-0643	1433-3058				APR	2020	32	8					4105	4120		10.1007/s00521-019-04081-4													
J								Supplier selection based on normal process yield: the Bayesian inference	NEURAL COMPUTING & APPLICATIONS										Bayesian; Markov chain Monte Carlo; Process capability; Supplier selection	MONTE-CARLO; CAPABILITY INDEX; GIBBS SAMPLER; QUALITY; GAMMA	Due to the risk in outsourcing, supplier selection is a critical issue for companies. Considerable evidence shows that among the criteria for selecting a supplier, quality is the most critical factor and the process yield index is an efficient tool for assessing the process quality of the supplier. Although the frequentist approach has been adopted to discriminate the degrees of two yield indices to solve the supplier selection problem, unknown parameters must be estimated from samples, which potentially introduce uncertainty into the statistical testing process. Instead of the frequentist inference, this study proposes using the Bayesian inference to derive the posterior distribution of the ratio of two yield indices. Furthermore, a Markov chain Monte Carlo technique is applied to discern the empirical posterior distribution of the ratio with the aim of discriminating the degrees of two yield indices. The simulations show that the proposed method is not only of reasonable empirical size but tests well in terms of power.																	0941-0643	1433-3058				APR	2020	32	8					4121	4133		10.1007/s00521-018-3718-4													
J								Simultaneous Deep Stereo Matching and Dehazing with Feature Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION										Stereo matching; Dehazing; CNN; Multi-task learning; Knowledge distillation; Stereo confidence; Adverse weather condition		Unveiling the dense correspondence under the haze layer remains a challenging task, since the scattering effects result in less distinctive image features. Contrarily, dehazing is often confused by the airlight-albedo ambiguity which cannot be resolved independently at each pixel. In this paper, we introduce a deep convolutional neural network that simultaneously estimates a disparity and clear image from a hazy stereo image pair. Both tasks are synergistically formulated by fusing depth information from the matching cost and haze transmission. To learn the optimal fusion of depth-related features, we present a novel encoder-decoder architecture that extends the core idea of attention mechanism to the simultaneous stereo matching and dehazing. As a result, our method estimates high-quality disparity for the stereo images in scattering media, and produces appearance images with enhanced visibility. Finally, we further propose an effective strategy for adaptation to camera-captured images by distilling the cross-domain knowledge. Experiments on both synthetic and real-world scenarios including comparisons with state-of-the-art methods demonstrate the effectiveness and flexibility of our approach.																	0920-5691	1573-1405				APR	2020	128	4			SI		799	817		10.1007/s11263-020-01294-2													
J								Pixelated Semantic Colorization	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image colorization; Semantic segmentation; Pixelated semantics	COLOR; IMAGE	While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from limited semantic understanding. To address this shortcoming, we propose to exploit pixelated object semantics to guide image colorization. The rationale is that human beings perceive and distinguish colors based on the semantic categories of objects. Starting from an autoregressive model, we generate image color distributions, from which diverse colored results are sampled. We propose two ways to incorporate object semantics into the colorization model: through a pixelated semantic embedding and a pixelated semantic generator. Specifically, the proposed network includes two branches. One branch learns what the object is, while the other branch learns the object colors. The network jointly optimizes a color embedding loss, a semantic segmentation loss and a color generation loss, in an end-to-end fashion. Experiments on Pascal VOC2012 and COCO-stuff reveal that our network, when trained with semantic segmentation labels, produces more realistic and finer results compared to the colorization state-of-the-art.																	0920-5691	1573-1405				APR	2020	128	4			SI		818	834		10.1007/s11263-019-01271-4													
J								Modeling Human Motion with Quaternion-Based Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Human motion modeling; Quaternion; Deep learning; Neural networks; Motion generation	ANIMATION	Previous work on predicting or generating 3D human pose sequences regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angles or exponential maps as parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. QuaterNet represents rotations with quaternions and our loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors. We investigate both recurrent and convolutional architectures and evaluate on short-term prediction and long-term generation. For the latter, our approach is qualitatively judged as realistic as recent neural strategies from the graphics literature. Our experiments compare quaternions to Euler angles as well as exponential maps and show that only a very short context is required to make reliable future predictions. Finally, we show that the standard evaluation protocol for Human3.6M produces high variance results and we propose a simple solution.																	0920-5691	1573-1405				APR	2020	128	4			SI		855	872		10.1007/s11263-019-01245-6													
J								End-to-End Learning of Decision Trees and Forests	INTERNATIONAL JOURNAL OF COMPUTER VISION										Decision forests; End-to-end learning; Efficient inference; Interpretability	CLASSIFICATION	Conventional decision trees have a number of favorable properties, including a small computational footprint, interpretability, and the ability to learn from little training data. However, they lack a key quality that has helped fuel the deep learning revolution: that of being end-to-end trainable. Kontschieder et al. (ICCV, 2015) have addressed this deficit, but at the cost of losing a main attractive trait of decision trees: the fact that each sample is routed along a small subset of tree nodes only. We here present an end-to-end learning scheme for deterministic decision trees and decision forests. Thanks to a new model and expectation-maximization training scheme, the trees are fully probabilistic at train time, but after an annealing process become deterministic at test time. In experiments we explore the effect of annealing visually and quantitatively, and find that our method performs on par or superior to standard learning algorithms for oblique decision trees and forests. We further demonstrate on image datasets that our approach can learn more complex split functions than common oblique ones, and facilitates interpretability through spatial regularization.																	0920-5691	1573-1405				APR	2020	128	4			SI		997	1011		10.1007/s11263-019-01237-6													
J								Recent trends in deep learning based personality detection	ARTIFICIAL INTELLIGENCE REVIEW										Personality detection; Multimodal interaction; Deep learning	BIG 5; 1ST IMPRESSIONS; FACE-RECOGNITION; TRAITS; MODEL; REGRESSION; LANGUAGE; FEATURES; NETWORK; STYLES	Recently, the automatic prediction of personality traits has received a lot of attention. Specifically, personality trait prediction from multimodal data has emerged as a hot topic within the field of affective computing. In this paper, we review significant machine learning models which have been employed for personality detection, with an emphasis on deep learning-based methods. This review paper provides an overview of the most popular approaches to automated personality detection, various computational datasets, its industrial applications, and state-of-the-art machine learning models for personality detection with specific focus on multimodal approaches. Personality detection is a very broad and diverse topic: this survey only focuses on computational approaches and leaves out psychological studies on personality detection.																	0269-2821	1573-7462				APR	2020	53	4					2313	2339		10.1007/s10462-019-09770-z													
J								Reduction foundation with multigranulation rough sets using discernibility	ARTIFICIAL INTELLIGENCE REVIEW										Discernibility matrix; Discernibility function; Decision making; Multigranulation rough set; Knowledge reduction	ATTRIBUTE REDUCTION; INFORMATION FUSION; GRANULATION; SELECTION; CONSISTENT; MATRIX	When multiple granulated knowledge in multigranulation spaces are involved in decision making, protocol principles are adopted to arrive at the final consensus. Multigranulation rough set theory utilizes a voting principle to combine the decision options derived from individual granulated knowledge. Note that those knowledge may provide different degrees of support to the final results, some are key, some are of less importance and some are even of no use. Selecting valuable knowledge and reducing worthless one are thus necessary for data processing, which can alleviate the storage occupancy and facilitate the logical and statistical analysis. However, the basic reduction foundation of multigranulation spaces has been rarely touched by researchers, which brings in many difficulties in algorithmic and real applications. This work aims to disclose the principles of multiple knowledge reduction in multigranulation spaces from the viewpoint of discernibility. First, the notions of knowledge reduction of multigranulation spaces are defined based on multigranulation rough set theory. Second, a decision function mapping each object into the decision options of its neighborhood granule is introduced. Third, several pairs of discernibility matrices and discernibility functions are successively developed using the decision function. We claim that the valuable and worthless knowledge in multigranulation spaces can be explicitly chose and eliminated respectively by using the proposed discernibility matrices and discernibility functions. That is to say, these discernibility tools provide a precise criterion for the knowledge reduction of multigranulation spaces. As a theoretical extension, a multigranulation information entropy is proposed and an approximate algorithm is constructed to compute a suboptimal reduct of a multigranulation space based on this entropy. In the end, numerical experiments are performed on public data sets to verify the effectiveness of the proposed reduction methods. This study can get us a grasp of the foundational principle of knowledge reduction and may bring a new insight for the designation of substantial reduction algorithms of multigranulation knowledge.																	0269-2821	1573-7462				APR	2020	53	4					2425	2452		10.1007/s10462-019-09737-0													
J								Comparison of support vector machine, back propagation neural network and extreme learning machine for syndrome element differentiation	ARTIFICIAL INTELLIGENCE REVIEW										Traditional Chinese medicine; Machine learning; Back propagation neural network; Support vector machine; Extreme learning machine; Attribute partial order structure diagram	DIAGNOSIS; MODELS	Artificial intelligence is a discipline which focuses on the study of simulating and extending human intelligence, while machine learning (ML) is one of the most rapidly developing subfields of AI research. The research of ML in the field of traditional Chinese medicine (TCM), has a bright prospect, as well as a profound significance. To explore the feasibility of using ML methods to approximate the diagnosis of TCM, support vector machine (SVM) is introduced and investigated for syndrome element differentiation of TCM in this paper. Based on 670 medical records, SVM was used to approximate the mapping relations between inputs (set of clinical manifestations) and outputs (set of syndrome elements). The value orders of syndrome elements were adopted to evaluate the approximation results, while attribute partial order structure diagram was employed to discover and visualize the knowledge of the records. Back propagation neural network (BPNN) and extreme learning machine (ELM), as comparative methods, were also employed to deal with the medical data. The value order's matching results between real and predicted results shows that, for SVM, the matched degree of each record is no less than 65%, while there are at least 88% records whose matched degree is more than 80%; and for BPNN and ELM, the highest proportion of records whose matched degree is more than 80% is only 45%. Using methods of ML to approximate the diagnosis of TCM should be feasible, and more relevant research can be conducted in the future.																	0269-2821	1573-7462				APR	2020	53	4					2453	2481		10.1007/s10462-019-09738-z													
J								A survey on structured discriminative spoken keyword spotting	ARTIFICIAL INTELLIGENCE REVIEW										Deep learning; Discriminative model; Hidden Markov model; Spoken keyword spotting; Structured data	HIDDEN MARKOV-MODELS; SPEECH RECOGNITION; CONFIDENCE MEASURES; ALGORITHM; NORMALIZATION; COMPRESSION; CLASSIFIER; NETWORKS; PHONEME; SEARCH	Spoken keyword spotting refers to the detection of all occurrences of desired words in continuous speech utterances. This paper includes a comprehensive review on various spoken keyword spotting (especially discriminative spoken keyword spotting) approaches. The most common datasets and evaluation measures for training and evaluating the spoken keyword spotting systems are reviewed in this paper. Moreover, the main framework for structured discriminative spoken keyword spotting (SDKWS) is presented. Different parts of the SDKWS framework such as feature extraction, model training, search algorithm and thresholding are discussed in this paper. Finally, the paper is concluded in the conclusion section and the future works are presented in the last part of that section.																	0269-2821	1573-7462				APR	2020	53	4					2483	2520		10.1007/s10462-019-09739-y													
J								Effective lexicon-based approach for Urdu sentiment analysis	ARTIFICIAL INTELLIGENCE REVIEW										Sentiment analysis; Urdu sentiment lexicon; Urdu sentiment analyzer	CLASSIFIER	The lexicon-based approach is used for sentiment analysis of Urdu. In the lexicon, apart from the traditional approach of having adjectives, nouns and negations we have also included verbs, intensifiers and context-dependent words. An effective Urdu sentiment analyzer is developed that applies rules and make use of this new lexicon and perform Urdu sentiment analysis by classifying sentences as positive, negative or neutral. Evaluating this Urdu sentiment analyzer, by using sentences from Urdu blogs, yields the most promising results so far in Urdu language with 89.03% accuracy with 0.86 precision, 0.90 recall and 0.88 F-measure. Results are evaluated using kappa statistics as well. The comparison with the previous work in Urdu shows that the combination of this Urdu sentiment lexicon and Urdu sentiment analyzer is much more effective than the previous such combinations. The main reason for increased efficiency is the development of wide coverage lexicon and effective handling of negations, intensifiers and context-dependent words by the Urdu sentiment analyzer.																	0269-2821	1573-7462				APR	2020	53	4					2521	2548		10.1007/s10462-019-09740-5													
J								A new integrated model of the group method of data handling and the firefly algorithm (GMDH-FA): application to aeration modelling on spillways	ARTIFICIAL INTELLIGENCE REVIEW										Aeration; GMDH-HS; Firefly algorithm; Artificial intelligence; Soft computing	AIR-ENTRAINMENT; HARMONY SEARCH; OPTIMIZATION; DEMAND; PERFORMANCE; EFFICIENCY; SYSTEM; WEIRS	Due to the high flow velocity over dam spillways and outlets, severe cavitation damage might occur to the structures. Aeration (introducing air into the passing flow) is a useful remedy for preventing or decreasing cavitation, however, proper estimation of aerators air demand is a complex problem. On that account, the standard GMDH model, integrated GMDH-HS (with the harmony search algorithm) model and a novel integrated GMDH-FA model (with the firefly algorithm), were developed and applied to estimate air demand on spillway aerators in dams. Input parameters including flow rate (Q(w)), flow depth (d(0)), relative pressure under the jet (h(s)), ramp angle (alpha), step height (s), and spillway slope (theta) were applied as the effective factors for estimating the amount of air flow of the aerators (Q(a)). General results based on several statistical measures (NRMSE, PCC, NMAE, NSE) and the test of ANOVA for models' residuals, showed that the standard GMDH improved the accuracy of estimating air flow in comparison to empirical equations (an average enhanced efficiency of 59.86% in terms of NRMSE) and multiple linear regression method (an enhanced efficiency of 37.15% in terms of NRMSE). Moreover, findings of the research revealed that the FA and HS algorithms improved the performance of the standard GMDH equal to 17% and 13%, respectively.																	0269-2821	1573-7462				APR	2020	53	4					2549	2569		10.1007/s10462-019-09741-4													
J								A survey of feature extraction and fusion of deep learning for detection of abnormalities in video endoscopy of gastrointestinal-tract	ARTIFICIAL INTELLIGENCE REVIEW										Convolutional neural network (CNN); Deep learning; Feature extraction; Gastrointestinal tract; Gastric cancer; Video endoscopy; Classification	WIRELESS CAPSULE ENDOSCOPY; CONVOLUTIONAL NEURAL-NETWORKS; COMPUTER-AIDED CLASSIFICATION; CONFOCAL LASER ENDOMICROSCOPY; SMALL-BOWEL TUMORS; GASTRIC-CANCER; AUTOMATIC DETECTION; TEXTURE DESCRIPTORS; IMAGE SEGMENTATION; BLEEDING DETECTION	A standard screening procedure involves video endoscopy of the Gastrointestinal tract. It is a less invasive method which is practiced for early diagnosis of gastric diseases. Manual inspection of a large number of gastric frames is an exhaustive, time-consuming task, and requires expertise. Conversely, several computer-aided diagnosis systems have been proposed by researchers to cope with the dilemma of manual inspection of the massive volume of frames. This article gives an overview of different available alternatives for automated inspection, detection, and classification of various GI abnormalities. Also, this work elaborates techniques associated with content-based image retrieval and automated systems for summarizing endoscopic procedures. In this survey, we perform a comprehensive review of feature extraction techniques and deep learning methods which were specifically developed for automatic analysis of endoscopic videos. In addition, we categorize features extraction techniques according to image processing domains and further we classify them based on their visual descriptions. We also review hybrid feature extraction techniques which are developed by the fusion of different kind of basic descriptors. Moreover, this survey covers various endoscopy data-sets available for the bench-marking of vision based algorithms. On the basis of literature, we explain emerging trends in computerized analysis of endoscopy. We also survey important issues, challenges, and future research directions to the development of computer-assisted systems for detection of maladies and interactive surgery in the GI tract.																	0269-2821	1573-7462				APR	2020	53	4					2635	2707		10.1007/s10462-019-09743-2													
J								Parameter identification of engineering problems using a differential shuffled complex evolution	ARTIFICIAL INTELLIGENCE REVIEW										Parameter identification; Shuffled complex evolution; Shrinkage stage; Differential operator; Chaotic systems	SYSTEM-IDENTIFICATION; ENHANCED EXPLORATION; GLOBAL OPTIMIZATION; HYDROLOGICAL MODEL; PSO ALGORITHM; SWARM	An accurate mathematical model has a vital role in controlling and synchronization of different systems. But generally in real-world problems, parameters are mixed with mismatches and distortions. In this paper, an improved shuffled complex evolution (SCE) is proposed for parameter identification of engineering problems. The SCE by employing parallel search efficiently finds neighborhoods of the optimal point. So it carries out exploration in a proper way. But its drawback is due to exploitation stages. The SCE cannot converge accurately to an optimal point, in many cases. The current study focuses to overcome this drawback by inserting a shrinkage stage to an original version of SCE and presents a powerful global numerical optimization method, named the differential SCE. The efficacy of the proposed algorithm is first tested on some benchmark problems. After achieving satisfactory performance on the test problems, to demonstrate the applicability of the proposed algorithm, it is applied to ten identification problems includes parameter identification of ordinary differential equations and chaotic systems. Practical experiences show that the proposed algorithm is very effective and robust so that it produces similar and promising results over repeated runs. Also, a comparison against other evolutionary algorithms reported in the literature demonstrates a significantly better performance of our proposed algorithm.																	0269-2821	1573-7462				APR	2020	53	4					2749	2782		10.1007/s10462-019-09745-0													
J								Flexible slider crank mechanism synthesis using meta-heuristic optimization techniques: a new designer tool assistance for a compliant mechanism synthesis	ARTIFICIAL INTELLIGENCE REVIEW										Assistance Tool for the Designer; Mechanism synthesis; Artificial Bee Colony Optimization; Ant Colony Optimization; Flexible multibody system	4-BAR MECHANISMS; PATH SYNTHESIS; DYNAMIC-ANALYSIS; DIFFERENTIAL EVOLUTION; PARALLEL MANIPULATOR; GLOBAL OPTIMIZATION; GENETIC ALGORITHM; GENERATION; TRANSMISSION; METHODOLOGY	This work bespeaks an insight into a real imperfect multibody systems synthesis, wherein the flexible behaviour of its components is considered. Based on the end-effector mechanism velocity, acceleration and defined transversal deflection for a flexible component, the mechanism optimal design variables have been investigated. Subsequently, the three aforementioned responses have been involved simultaneously for a combined optimization process. To this end, an Assistance Tool Design, subsuming several meta-heuristic optimization techniques such as Genetic Algorithm (GA), Imperialist Competitive Algorithm (ICA), Artificial Bee Colony (ABC), Ant Colony (AC), Differential Evolution (DE) and Simulating Annealing (SA) techniques, has been developed under MATLAB software. The Assistance Tool Design enables designers to carry out the synthesis of mechanisms by means of one or many optimization techniques mentioned above. It has been proven that AC and DE outperform the other optimization techniques. Nevertheless, awkwardness has been bearded for the mechanism synthesis using only a single response, mainly the flexible connecting rod mid-point transversal deflection. The combined synthesis subsuming simultaneously the three responses discussed above settles a perfect trade-off for all the optimization techniques.																	0269-2821	1573-7462				APR	2020	53	4					2809	2840		10.1007/s10462-019-09747-y													
J								Intuitionistic fuzzy beta-covering-based rough sets	ARTIFICIAL INTELLIGENCE REVIEW										Intuitionistic fuzzy rough sets; Covering; Multi-granulation; Reducts	ATTRIBUTE REDUCTION; APPROXIMATION OPERATORS; HIERARCHICAL STRUCTURES; DISTANCE MEASURE; INFORMATION; MODEL; SYSTEMS; MATRIX	Covering-based rough set is an important extended type of classical rough set model. In this model, concepts are approximated through substitution of a partition in classical rough set theory with a covering in covering-based rough set theory. Various generalized covering-based rough sets have been investigated, however, little work has been done on extending four classical covering-based rough set to intuitionistic fuzzy (IF) settings. In this study, four novel IF covering-based rough set models are developed by combining an IF beta-covering with four classical covering-based rough set models. First, we present the concept of IF beta-minimal description, and then construct four order relations on IF beta approximation space. Second, we propose four IF beta-covering-based rough set models and derive that they are generalizations of four existing covering-based rough sets in IF settings. We also discuss the properties of these IF beta-covering-based rough sets and reveal their relationships. We use the existing distance between two IF sets to characterize the uncertainty of the presented IF beta-covering-based rough sets. Third, we define the reducts of IF beta-covering decision systems and examine their discernibility-function-based reduction methods for these IF beta-covering-based rough sets. Fourth, we present four optimistic and pessimistic multi-granulation IF beta-covering-based rough sets and analyze their properties and uncertainty measures from multi-granulation perspective. Fifth, we study the discernibility-function-based reduction methods for the presented multi-granulation IF beta-covering-based rough sets. Finally, we discuss another two neighborhood-based IF covering-based rough sets. This study can provide a covering-based rough set method for acquiring knowledge from IF decision systems.																	0269-2821	1573-7462				APR	2020	53	4					2841	2873		10.1007/s10462-019-09748-x													
J								A new graph-preserving unsupervised feature selection embedding LLE with low-rank constraint and feature-level representation	ARTIFICIAL INTELLIGENCE REVIEW										Feature selection; Graph matrix; Dimensionality reduction; Manifold learning	CLASSIFICATION; REGRESSION	Unsupervised feature selection is a powerful tool to process high-dimensional data, in which a subset of features is selected out for effective data representation. In this paper, we proposes a novel robust unsupervised features selection method based on graph-preserving feature selection embedding LLE. Specifically, we integrate the graph matrix learning and the low-dimensional space learning together to identify the correlation among both features and samples from the intrinsic low-dimensional space of original data. Also, the global and local correlation of features have been taken into consideration through the low-rank constraint and the feature-level representation property to find lower-dimensional representation which preserves not only the global and local correlation of features but also the global and local structure of training samples. Furthermore, we propose a new optimization algorithm to the resulting objective function, which iteratively updates the graph matrix and the intrinsic space in order to collaboratively improve each of them. Experimental analysis on 18 benchmark datasets verified that our proposed method outperformed the state-of-the-art feature selection methods in terms of classification and clustering performance.																	0269-2821	1573-7462				APR	2020	53	4					2875	2903		10.1007/s10462-019-09749-w													
J								Feature selection in image analysis: a survey	ARTIFICIAL INTELLIGENCE REVIEW										Feature selection; Image analysis; Pattern recognition; High dimensionality; Image datasets	FEATURE SUBSET-SELECTION; MUTUAL INFORMATION; GENETIC ALGORITHM; NEURAL-NETWORKS; CLASSIFICATION; SEGMENTATION; COLOR; ANNOTATION; RETRIEVAL; TEXTURE	Image analysis is a prolific field of research which has been broadly studied in the last decades, successfully applied to a great number of disciplines. Since the apparition of Big Data, the number of digital images is explosively growing, and a large amount of multimedia data is publicly available. Not only is it necessary to deal with this increasing number of images, but also to know which features extract from them, and feature selection can help in this scenario. The goal of this paper is to survey the most recent feature selection methods developed and/or applied to image analysis, covering the most popular fields such as image classification, image segmentation, etc. Finally, an experimental evaluation on several popular datasets using well-known feature selection methods is presented, bearing in mind that the aim is not to provide the best feature selection method, but to facilitate comparative studies for the research community.																	0269-2821	1573-7462				APR	2020	53	4					2905	2931		10.1007/s10462-019-09750-3													
J								A corporate shuffled complex evolution for parameter identification	ARTIFICIAL INTELLIGENCE REVIEW										Evolutionary process; Parameter identification; Shuffled complex evolution; Simplex search	GLOBAL OPTIMIZATION; SYSTEM-IDENTIFICATION; GENETIC ALGORITHM; CALIBRATION; SWARM; STRATEGY; DESIGN	This paper proposes a new version of the shuffled complex evolution (SCE) algorithm for solving parameter identification problems. The SCE divides a population into several parallel subsets called complex and then improves each sub-complex through an evolutionary process using a Nelder-Mead (NM) simplex search method. This algorithm applies its evolutionary process only on the worst member of each sub-complex whereas the role of other members is not operative. Therefore, the number and variety of search moves are limited in the evolutionary process of SCE. The current study focuses to overcome this drawback by proposing a corporate SCE (CSCE). This algorithm provides an evolutionary possibility for all members of a sub-complex. In the CSCE, each member is influenced by a simplex made from all other members of the current sub-complex. The CSCE barrows three actions of NM, i.e. reflection, contraction and expansion, and applied them on each member to find a better candidate than the current one. The efficacy of the proposed algorithm is first tested on six benchmark problems. After achieving satisfactory performance on the test problems, it is applied to parameter identification problems and the obtained results are compared with some other algorithms reported in the literature. Numerical results and non-parametric analysis show that the proposed algorithm is very effective and robust since it produces similar and promising results over repeated runs.																	0269-2821	1573-7462				APR	2020	53	4					2933	2956		10.1007/s10462-019-09751-2													
J								A survey of quaternion neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Hypercomplex numbers; Quaternion neural networks; Deep Learning	LEARNING ALGORITHMS; ROBOTICS	Quaternion neural networks have recently received an increasing interest due to noticeable improvements over real-valued neural networks on real world tasks such as image, speech and signal processing. The extension of quaternion numbers to neural architectures reached state-of-the-art performances with a reduction of the number of neural parameters. This survey provides a review of past and recent research on quaternion neural networks and their applications in different domains. The paper details methods, algorithms and applications for each quaternion-valued neural networks proposed.																	0269-2821	1573-7462				APR	2020	53	4					2957	2982		10.1007/s10462-019-09752-1													
J								Modeling empathy: building a link between affective and cognitive processes	ARTIFICIAL INTELLIGENCE REVIEW										Empathy; Affective computing; Cognitive modeling; Artificial agents	HIGH-FUNCTIONING AUTISM; COMPLEX EMOTION RECOGNITION; ASPERGER-SYNDROME; ADULTS; MIND; RELIABILITY; APPRAISAL; COMPANION; QUOTIENT; ROBOTS	Computational modeling of empathy has recently become an increasingly popular way of studying human relations. It provides a way to increase our understanding of the link between affective and cognitive processes and enhance our interaction with artificial agents. However, the variety of fields contributing to empathy research has resulted in isolated approaches to modeling empathy, and this has led to various definitions of empathy and an absence of common ground regarding underlying empathic processes. Although this diversity may be useful in that it allows for an in-depth examination of various processes linked to empathy, it also may not yet provide a coherent theoretical picture of empathy. We argue that a clear theoretical positioning is required for collective progress. The aim of this article is, therefore, to call for a holistic and multilayered view of a model of empathy, taken from the rich background research from various disciplines. To achieve this, we present a comprehensive background on the theoretical foundations, followed by the working definitions, components, and models of empathy that are proposed by various fields. Following this introduction, we provide a detailed review of the existing techniques used in AI research to model empathy in interactive agents, focusing on the strengths and weaknesses of each approach. We conclude with a discussion of future directions in this emerging field.																	0269-2821	1573-7462				APR	2020	53	4					2983	3006		10.1007/s10462-019-09753-0													
J								Identification of epileptic seizures in EEG signals using time-scale decomposition (ITD), discrete wavelet transform (DWT), phase space reconstruction (PSR) and neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Electroencephalogram (EEG); Epileptic seizure detection; Time-scale decomposition (ITD); Phase space reconstruction (PSR); Discrete wavelet transform (DWT); System dynamics; Neural networks	EMPIRICAL MODE DECOMPOSITION; FEATURE-EXTRACTION; FAULT-DIAGNOSIS; BINARY PATTERN; CLASSIFICATION; REPRESENTATION; PREDICTION; DIMENSION; ENTROPY; FOREST	Traditionally, detection of epileptic seizures based on the visual inspection of neurologists is tedious, laborious and subjective. To overcome such disadvantages, numerous seizure detection techniques involving signal processing and machine learning tools have been developed. However, there still remain the problems of automatic detection with high efficiency and accuracy in distinguishing normal, interictal and ictal electroencephalogram (EEG) signals. In this study we propose a novel method for automatic identification of epileptic seizures in singe-channel EEG signals based upon time-scale decomposition (ITD), discrete wavelet transform (DWT), phase space reconstruction (PSR) and neural networks. First, EEG signals are decomposed into a series of proper rotation components (PRCs) and a baseline signal by using the ITD method. The first two PRCs of the EEG signals are extracted, which contain most of the EEG signals' energy and are considered to be the predominant PRCs. Second, four levels DWT is employed to decompose the predominant PRCs into different frequency bands, in which third-order Daubechies (db3) wavelet function is selected for analysis. Third, phase space of the PRCs is reconstructed based on db3, in which the properties associated with the nonlinear EEG system dynamics are preserved. Three-dimensional (3D) PSR together with Euclidean distance (ED) has been utilized to derive features, which demonstrate significant difference in EEG system dynamics between normal, interictal and ictal EEG signals. Fourth, neural networks are then used to model, identify and classify EEG system dynamics between normal (healthy), interictal and ictal EEG signals. Finally, experiments are carried out on the University of Bonn's widely used and publicly available epilepsy dataset to assess the effectiveness of the proposed method. By using the 10-fold cross-validation style, the achieved average classification accuracy for eleven cases is reported to be 98.15%. Compared with many state-of-the-art methods, the results demonstrate superior performance and the proposed method can serve as a potential candidate for the automatic detection of seizure EEG signals in the clinical application.																	0269-2821	1573-7462				APR	2020	53	4					3059	3088		10.1007/s10462-019-09755-y													
J								New multiparametric similarity measure for neutrosophic set with big data industry evaluation	ARTIFICIAL INTELLIGENCE REVIEW										Single-valued neutrosophic set; Similarity measure; Combination weight; Decision making	DECISION-MAKING METHOD; AGGREGATION OPERATORS; CROSS-ENTROPY; ALGORITHMS; INTERNET; WEIGHTS; THINGS	In the age of big data, we often face the huge Volume, high Velocity, rich Variety, accuracy Veracity and high Value (5Vs) with complicated structures. Big data industrial decision making is crucial for a country or society to improve the effectiveness of leadership, which can remarkably promote scale development and industrialization. Under such circumstance of big data industrial decision assessment, the intrinsic issue involves serious indeterminacy. Single-valued neutrosophic set, depicted by truth membership, indeterminacy membership and falsity membership, is a highly effective way to grasp uncertainty. The dominating aim is to explore multiparametric similarity measure and distance measure with their properties. Then, the objective weight is computed by deviation-based method and the combination weight is presented. Moreover, we propose a novel neutrosophic decision making method based on multiparametric similarity measure with combination weight, which is stated by a big data industry decision making issue, along with the impact of various parameters on the final ranking. In the end, a comparison between the proposed algorithm and some existing neutrosophic algorithms has been built by the antilogarithm by zero problem, counter-intuitive phenomena and division by zero problem for showing their validity.																	0269-2821	1573-7462				APR	2020	53	4					3089	3125		10.1007/s10462-019-09756-x													
J								The Global Optimization Geometry of Shallow Linear Neural Networks	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Deep learning; Linear neural network; Optimization geometry; Strict saddle; Spurious local minima	DEEP NETWORKS	We examine the squared error loss landscape of shallow linear neural networks. We show-with significantly milder assumptions than previous works-that the corresponding optimization problems have benign geometric properties: There are no spurious local minima, and the Hessian at every saddle point has at least one negative eigenvalue. This means that at every saddle point there is a directional negative curvature which algorithms can utilize to further decrease the objective value. These geometric properties imply that many local search algorithms (such as the gradient descent which is widely utilized for training neural networks) can provably solve the training problem with global convergence.																	0924-9907	1573-7683				APR	2020	62	3			SI		279	292		10.1007/s10851-019-00889-w													
J								Adversarial Noise Attacks of Deep Learning Architectures: Stability Analysis via Sparse-Modeled Signals	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Theory for deep learning; Adversarial noise; Sparse coding		Despite their impressive performance, deep convolutional neural networks (CNN) have been shown to be sensitive to small adversarial perturbations. These nuisances, which one can barely notice, are powerful enough to fool sophisticated and well performing classifiers, leading to ridiculous misclassification results. In this paper, we analyze the stability of state-of-the-art deep learning classification machines to adversarial perturbations, where we assume that the signals belong to the (possibly multilayer) sparse representation model. We start with convolutional sparsity and then proceed to its multilayered version, which is tightly connected to CNN. Our analysis links between the stability of the classification to noise and the underlying structure of the signal, quantified by the sparsity of its representation under a fixed dictionary. In addition, we offer similar stability theorems for two practical pursuit algorithms, which are posed as two different deep learning architectures-the layered thresholding and the layered basis pursuit. Our analysis establishes the better robustness of the later to adversarial attacks. We corroborate these theoretical results by numerical experiments on three datasets: MNIST, CIFAR-10 and CIFAR-100.																	0924-9907	1573-7683				APR	2020	62	3			SI		313	327		10.1007/s10851-019-00913-z													
J								Deep Neural Networks Motivated by Partial Differential Equations	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Machine learning; Deep neural networks; Partial differential equations; PDE-constrained optimization; Image classification		Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite-dimensional setting provides powerful tools for their analysis and solution. For the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction. In this paper, we establish a new PDE interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.																	0924-9907	1573-7683				APR	2020	62	3			SI		352	364		10.1007/s10851-019-00903-1													
J								Residual Networks as Flows of Diffeomorphisms	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Residual network; Diffeomorphism; Dynamical systems	IMAGE REGISTRATION	This paper addresses the understanding and characterization of residual networks (ResNet), which are among the state-of-the-art deep learning architectures for a variety of supervised learning problems. We focus on the mapping component of ResNets, which map the embedding space toward a new unknown space where the prediction or classification can be stated according to linear criteria. We show that this mapping component can be regarded as the numerical implementation of continuous flows of diffeomorphisms governed by ordinary differential equations. In particular, ResNets with shared weights are fully characterized as numerical approximation of exponential diffeomorphic operators. We stress both theoretically and numerically the relevance of the enforcement of diffeomorphic properties and the importance of numerical issues to make consistent the continuous formulation and the discretized ResNet implementation. We further discuss the resulting theoretical and computational insights into ResNet architectures.																	0924-9907	1573-7683				APR	2020	62	3			SI		365	375		10.1007/s10851-019-00890-3													
J								Variational Networks: An Optimal Control Approach to Early Stopping Variational Methods for Image Restoration	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Variational problems; Gradient flow; Optimal control theory; Early stopping; Variational networks; Deep learning		We investigate a well-known phenomenon of variational approaches in image processing, where typically the best image quality is achieved when the gradient flow process is stopped before converging to a stationary point. This paradox originates from a tradeoff between optimization and modeling errors of the underlying variational model and holds true even if deep learning methods are used to learn highly expressive regularizers from data. In this paper, we take advantage of this paradox and introduce an optimal stopping time into the gradient flow process, which in turn is learned from data by means of an optimal control approach. After a time discretization, we obtain variational networks, which can be interpreted as a particular type of recurrent neural networks. The learned variational networks achieve competitive results for image denoising and image deblurring on a standard benchmark data set. One of the key theoretical results is the development of first- and second-order conditions to verify optimal stopping time. A nonlinear spectral analysis of the gradient of the learned regularizer gives enlightening insights into the different regularization properties.																	0924-9907	1573-7683				APR	2020	62	3			SI		396	416		10.1007/s10851-019-00926-8													
J								Networks for Nonlinear Diffusion Problems in Imaging	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Neural networks; Deep learning; Partial differential equations; Nonlinear diffusion; Image flow; Nonlinear inverse problems	CONVOLUTIONAL NEURAL-NETWORK; RECONSTRUCTION	A multitude of imaging and vision tasks have seen recently a major transformation by deep learning methods and in particular by the application of convolutional neural networks. These methods achieve impressive results, even for applications where it is not apparent that convolutions are suited to capture the underlying physics. In this work, we develop a network architecture based on nonlinear diffusion processes, named DiffNet. By design, we obtain a nonlinear network architecture that is well suited for diffusion-related problems in imaging. Furthermore, the performed updates are explicit, by which we obtain better interpretability and generalisability compared to classical convolutional neural network architectures. The performance of DiffNet is tested on the inverse problem of nonlinear diffusion with the Perona-Malik filter on the STL-10 image dataset. We obtain competitive results to the established U-Net architecture, with a fraction of parameters and necessary training data.																	0924-9907	1573-7683				APR	2020	62	3			SI		471	487		10.1007/s10851-019-00901-3													
J								A Survey of Automatic Parameter Tuning Methods for Metaheuristics	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Tuning; Task analysis; Heuristic algorithms; Optimization; Computer science; Measurement; Systematics; Automatic parameter tuning; metaheuristics; parameter setting; parameter tuning	CMA EVOLUTION STRATEGY; COMBINATORIAL OPTIMIZATION; GLOBAL OPTIMIZATION; ALGORITHM; SEARCH; CONFIGURATION; SELECTION	Parameter tuning, that is, to find appropriate parameter settings (or configurations) of algorithms so that their performance is optimized, is an important task in the development and application of metaheuristics. Automating this task, i.e., developing algorithmic procedure to address parameter tuning task, is highly desired and has attracted significant attention from the researchers and practitioners. During last two decades, many automatic parameter tuning approaches have been proposed. This paper presents a comprehensive survey of automatic parameter tuning methods for metaheuristics. A new classification (or taxonomy) of automatic parameter tuning methods is introduced according to the structure of tuning methods. The existing automatic parameter tuning approaches are consequently classified into three categories: 1) simple generate-evaluate methods; 2) iterative generate-evaluate methods; and 3) high-level generate-evaluate methods. Then, these three categories of tuning methods are reviewed in sequence. In addition to the description of each tuning method, its main strengths and weaknesses are discussed, which is helpful for new researchers or practitioners to select appropriate tuning methods to use. Furthermore, some challenges and directions of this field are pointed out for further research.																	1089-778X	1941-0026				APR	2020	24	2					201	216		10.1109/TEVC.2019.2921598													
J								Evolutionary Collaborative Human-UAV Search for Escaped Criminals	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Search problems; Collaboration; Unmanned aerial vehicles; Path planning; Optimization; Planning; Collaborative human-unmanned aerial vehicle (UAV) search; criminal search and capture; discrete-time optimization; evolutionary algorithms (EAs); hybrid algorithm	PARTICLE SWARM OPTIMIZATION; BIOGEOGRAPHY-BASED OPTIMIZATION; PIGEON-INSPIRED OPTIMIZATION; GAUSSIAN MIXTURE MODEL; GENETIC ALGORITHM; PLANNER; ASSIGNMENT; TARGET	The use of unmanned aerial vehicles (UAVs) for target searching in complex environments has increased considerably in recent years. The numerous studies on UAV search methods have been reported, but few have been conducted on collaborative human-UAV search which is common in many applications. In this paper, we present a problem of collaborative human-UAV search for escaped criminals, the aim of which is to minimize the expected time of capture rather than detection. We show that our problem is much more complex than the problem of pure UAV search. The difficulty of our problem is further increased by the fact that criminals will attempt to avoid detection and capture. To solve the problem, we propose a hybrid evolutionary algorithm (EA) that uses three evolutionary operators, namely, comprehensive learning, variable mutation, and local search, to efficiently explore the solution space. The experimental results demonstrate that the proposed method outperforms some well-known EAs and other popular UAV search methods on test instances. An application of our method to a real-world operation took 311 min to capture a criminal who had escaped for over three days, validating its practicability and performance advantage. This paper provides a good basis for promoting the application of EAs to a wider class of man-machine collaboration scheduling problems.																	1089-778X	1941-0026				APR	2020	24	2					217	231		10.1109/TEVC.2019.2925175													
J								Evolutionary Development of Growing Generic Sorting Networks by Means of Rewriting Systems	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Sorting; Grammar; Delays; Evolution (biology); Digital circuits; Biological system modeling; Scalability; Development; genetic algorithm (GA); rewriting system; scalability; sorting network	GRAMMATICAL EVOLUTION; OPTIMIZATION; CIRCUITS; DESIGN	This paper presents an evolutionary developmental method for the design of arbitrarily growing sorting networks. The developmental model is based on a parallel rewriting system (a grammar) that is specified by an alphabet, an initial string (an axiom), and a set of rewriting rules. The rewriting process iteratively expands the axiom in order to develop more complex strings during a series of development steps (i.e., derivations in the grammar). A mapping function is introduced that allows for converting the strings onto comparator structures-building blocks of sorting networks. The construction of the networks is performed in such a way that a given (initial) sorting network grows progressively by adding further building blocks within each development step. For a given (fixed) alphabet, the axiom together with the rewriting rules themselves are the subjects of the evolutionary search. It will be shown that suitable grammars can be evolved for the construction of arbitrarily large sorting networks that grow with various given sizes of development steps. Moreover, the resulting networks exhibit significantly better properties (the number of comparators and delay) in comparison with those obtained by means of similar existing methods.																	1089-778X	1941-0026				APR	2020	24	2					232	244		10.1109/TEVC.2019.2918212													
J								Enhancing Decomposition-Based Algorithms by Estimation of Distribution for Constrained Optimal Software Product Selection	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Optimization; Software; Software algorithms; Probabilistic logic; Estimation; Software engineering; Evolutionary computation; Constrained multiobjective optimization; decomposition-based multiobjective algorithm; estimation of distribution (EoD); optimal software product selection (OSPS); search-based software engineering (SBSE)	NONDOMINATED SORTING APPROACH; LOCAL SEARCH ALGORITHM; EVOLUTIONARY ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; MOEA/D; MODELS	This paper integrates an estimation of distribution (EoD)-based update operator into decomposition-based multiobjective evolutionary algorithms for binary optimization. The probabilistic model in the update operator is a probability vector, which is adaptively learned from historical information of each subproblem. We show that this update operator can significantly enhance decomposition-based algorithms on a number of benchmark problems. Moreover, we apply the enhanced algorithms to the constrained optimal software product selection (OSPS) problem in the field of search-based software engineering. For this real-world problem, we give its formal definition and then develop a new repair operator based on satisfiability solvers. It is demonstrated by the experimental results that the algorithms equipped with the EoD operator are effective in dealing with this practical problem, particularly for large-scale instances. The interdisciplinary studies in this paper provide a new real-world application scenario for constrained multiobjective binary optimizers and also offer valuable techniques for software engineers in handling the OSPS problem.																	1089-778X	1941-0026				APR	2020	24	2					245	259		10.1109/TEVC.2019.2922419													
J								Novel Prediction Strategies for Dynamic Multiobjective Optimization	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Heuristic algorithms; Sociology; Statistics; Prediction algorithms; Optimization; Optical fibers; Convergence; Dynamic multiobjective optimization; nondominated sorting; prediction-based reaction; probability distribution	ALGORITHM; ENVIRONMENTS; MODEL	This paper proposes a new prediction-based dynamic multiobjective optimization (PBDMO) method, which combines a new prediction-based reaction mechanism and a popular regularity model-based multiobjective estimation of distribution algorithm (RM-MEDA) for solving dynamic multiobjective optimization problems. Whenever a change is detected, PBDMO reacts effectively to it by generating three subpopulations based on different strategies. The first subpopulation is created by moving nondominated individuals using a simple linear prediction model with different step sizes. The second subpopulation consists of some individuals generated by a novel sampling strategy to improve population convergence as well as distribution. The third subpopulation comprises some individuals generated using a shrinking strategy based on the probability distribution of variables. These subpopulations are tailored to form a population for the new environment. The experimental results carried out on a variety of bi- and three-objective benchmark functions demonstrate that the proposed technique has competitive performance compared with some state-of-the-art algorithms.																	1089-778X	1941-0026				APR	2020	24	2					260	274		10.1109/TEVC.2019.2922834													
J								An Experimental Method to Estimate Running Time of Evolutionary Algorithms for Continuous Optimization	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Optimization; Sociology; Evolutionary computation; Switches; Statistical analysis; Analytical models; Average gain model; evolutionary algorithms (EAs) for continuous optimization; experimental method; running time analysis	1ST HITTING TIME; DRIFT ANALYSIS; COMPUTATIONAL TIME; LOWER BOUNDS; 1+1 ES; STRATEGIES; SURFACES	Running time analysis is a fundamental problem of critical importance in evolutionary computation. However, the analysis results have rarely been applied to advanced evolutionary algorithms (EAs) in practice, let alone their variants for continuous optimization. In this paper, an experimental method is proposed for analyzing the running time of EAs that are widely used for solving continuous optimization problems. Based on Glivenko-Cantelli theorem, the proposed method simulates the distribution of gain, which is introduced by average gain model to characterize progress during the optimization process. Data fitting techniques are subsequently adopted to obtain a desired function for further analyses. To verify the validity of the proposed method, experiments were conducted to estimate the upper bounds on expected first hitting time of various evolutionary strategies, such as (1, $\lambda $ ) evolution strategy, standard evolution strategy, covariance matrix adaptation evolution strategy, and its improved variants. The results suggest that all estimated upper bounds are correct. Backed up by the proposed method, state-of-the-art EAs for continuous optimization will have identical results about the running time as simplified schemes, which will bridge the gap between theoretical foundation and applications of evolutionary computation.																	1089-778X	1941-0026				APR	2020	24	2					275	289		10.1109/TEVC.2019.2921547													
J								A Multimodel Prediction Method for Dynamic Multiobjective Evolutionary Optimization	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Sociology; Statistics; Optimization; Predictive models; Maintenance engineering; Mathematical model; Convergence; Dynamic multiobjective optimization; evolutionary algorithm (EA); multimodel prediction; particle swarm optimizer; type of the Pareto set (PS) change	ANT COLONY OPTIMIZATION; ALGORITHM; DIVERSITY; SEARCH; MEMORY	A large number of prediction strategies are specific to a dynamic multiobjective optimization problem (DMOP) with only one type of the Pareto set (PS) change. However, a continuous DMOP with more than one type of the unknown PS change has been seldom investigated. We present a multimodel prediction approach (MMP) realized in the framework of evolutionary algorithms (EAs) to tackle the problem. In this paper, we first detect the type of the PS change, followed by the selection of an appropriate prediction model to provide an initial population for the subsequent evolution. To observe the influence of MMP on EAs, optimal solutions obtained by three classical dynamic multiobjective EAs with and without MMP are investigated. Furthermore, to investigate the performance of MMP, three state-of-the-art prediction strategies are compared on a large number of dynamic test instances under the same particle swarm optimizer. The experimental results demonstrate that the proposed approach outperforms its counterparts under comparison on most optimization problems.																	1089-778X	1941-0026				APR	2020	24	2					290	304		10.1109/TEVC.2019.2925358													
J								Evolutionary Dynamic Multiobjective Optimization Assisted by a Support Vector Regression Predictor	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Decomposition; dynamic multiobjective optimization; multiobjective evolutionary algorithm (MOEA); nonlinear mapping; predictor; support vector regression (SVR)	ALGORITHM; ENVIRONMENTS; SELECTION; MACHINES; STRATEGY	Dynamic multiobjective optimization problems (DMOPs) challenge multiobjective evolutionary algorithms (MOEAs) because those problems change rapidly over time. The class of DMOPs whose objective functions change over time steps, in ways that exhibit some hidden patterns has gained much attention. Their predictability indicates that the problem exhibits some correlations between solutions obtained in sequential time periods. Most of the current approaches use linear models or similar strategies to describe the correlations between historical solutions obtained, and predict the new solutions in the following time period as an initial population from which the MOEA can begin searching in order to improve its efficiency. However, nonlinear correlations between historical solutions and current solutions are more common in practice, and a linear model may not be suitable for the nonlinear case. In this paper, we present a support vector regression (SVR)-based predictor to generate the initial population for the MOEA in the new environment. The basic idea of this predictor is to map the historical solutions into a high-dimensional feature space via a nonlinear mapping, and to do linear regression in this space. SVR is used to implement this process. We incorporate this predictor into the MOEA based on decomposition (MOEA/D) to construct a novel algorithm for solving the aforementioned class of DMOPs. Comprehensive experiments have shown the effectiveness and competitiveness of our proposed predictor, comparing with the state-of-the-art methods.																	1089-778X	1941-0026				APR	2020	24	2					305	319		10.1109/TEVC.2019.2925722													
J								Decomposition-Based Interactive Evolutionary Algorithm for Multiple Objective Optimization	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Sociology; Optimization; Evolutionary computation; Analytical models; Additives; Monte Carlo methods; Decomposition; indirect preference information; interactive evolutionary hybrid; Monte Carlo (MC) simulation; multiple objective optimization (MOO)	MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; RANKING	We propose a decomposition-based interactive evolutionary algorithm (EA) for multiple objective optimization. During an evolutionary search, a decision maker (DM) is asked to compare pairwise solutions from the current population. Using the Monte Carlo simulation, the proposed algorithm generates from a uniform distribution a set of instances of the preference model compatible with such an indirect preference information. These instances are incorporated as the search directions with the aim of systematically converging a population toward the DMs most preferred region of the Pareto front. The experimental comparison proves that the proposed decomposition-based method outperforms the state-of-the-art interactive counterparts of the dominance-based EAs. We also show that the quality of constructed solutions is highly affected by the form of the incorporated preference model.																	1089-778X	1941-0026				APR	2020	24	2					320	334		10.1109/TEVC.2019.2915767													
J								Parameter-Free Voronoi Neighborhood for Evolutionary Multimodal Optimization	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Optimization; Sociology; Statistics; Topology; Genetics; Evolutionary computation; Convergence; Evolutionary multimodal optimization; neighborhood information; niching technique; Voronoi diagram	INFORMED PARTICLE SWARM; DIFFERENTIAL EVOLUTION; MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; MUTATION; MODEL	Neighborhood information plays an important role in improving the performance of evolutionary computation in various optimization scenarios, particularly in the context of multimodal optimization. Several neighborhood concepts, i.e., index-based neighborhood, nearest neighborhood, and fuzzy neighborhood, have been studied and engaged in the design of niching methods. However, the use of these neighborhood concepts requires the specification of some problem-related parameters, which is difficult to determine without a prior knowledge. In this paper, we introduce a new neighborhood concept based on a geometrical construction called Voronoi diagram. The new concept offers two advantages at the expense of increasing the computational complexity to a higher level. It eliminates the need of additional parameters and it is more informative than the existing ones. The information provided by the Voronoi neighbors of an individual can be exploited to estimate the evolutionary state. Based on the information, we divide the population into three groups and assign each group a different reproduction strategy to support the exploration and exploitation of the search space. We show the use of the concept in the design of an effective evolutionary algorithm for multimodal optimization. The experiments have been conducted to investigate the performance of the algorithm. The results reveal that the proposed algorithm compare favorably with the state-of-the-art algorithms designed based on other types of neighborhood concepts.																	1089-778X	1941-0026				APR	2020	24	2					335	349		10.1109/TEVC.2019.2921830													
J								Surrogate-Assisted Evolutionary Deep Learning Using an End-to-End Random Forest-Based Performance Predictor	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Computer architecture; Training; Prediction algorithms; Optimization; Sociology; Statistics; Convolutional neural network (CNN); evolutionary deep learning (EDL); performance predictor; random forest; surrogate model	OPTIMIZATION; ALGORITHM; COEFFICIENT	Convolutional neural networks (CNNs) have shown remarkable performance in various real-world applications. Unfortunately, the promising performance of CNNs can be achieved only when their architectures are optimally constructed. The architectures of state-of-the-art CNNs are typically handcrafted with extensive expertise in both CNNs and the investigated data, which consequently hampers the widespread adoption of CNNs for less experienced users. Evolutionary deep learning (EDL) is able to automatically design the best CNN architectures without much expertise. However, the existing EDL algorithms generally evaluate the fitness of a new architecture by training from scratch, resulting in the prohibitive computational cost even operated on high-performance computers. In this paper, an end-to-end offline performance predictor based on the random forest is proposed to accelerate the fitness evaluation in EDL. The proposed performance predictor shows the promising performance in term of the classification accuracy and the consumed computational resources when compared with 18 state-of-the-art peer competitors by integrating into an existing EDL algorithm as a case study. The proposed performance predictor is also compared with the other two representatives of existing performance predictors. The experimental results show the proposed performance predictor not only significantly speeds up the fitness evaluations but also achieves the best prediction among the peer performance predictors.																	1089-778X	1941-0026				APR	2020	24	2					350	364		10.1109/TEVC.2019.2924461													
J								Efficient Generalized Surrogate-Assisted Evolutionary Algorithm for High-Dimensional Expensive Problems	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Optimization; Genetic algorithms; Evolutionary computation; Computational modeling; Prediction algorithms; Partitioning algorithms; Search methods; High-dimensional expensive problems; multiple surrogates; prescreening strategy; simplified Kriging; surrogate-assisted evolutionary algorithm; surrogate-guided crossover operation; trust region method	PARTICLE SWARM OPTIMIZATION; RESPONSE-SURFACE METHOD; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; GENETIC ALGORITHM; MODEL; CONVERGENCE; ENSEMBLE	Engineering optimization problems usually involve computationally expensive simulations and many design variables. Solving such problems in an efficient manner is still a major challenge. In this paper, a generalized surrogate-assisted evolutionary algorithm is proposed to solve such high-dimensional expensive problems. The proposed algorithm is based on the optimization framework of the genetic algorithm (GA). This algorithm proposes to use a surrogate-based trust region local search method, a surrogate-guided GA (SGA) updating mechanism with a neighbor region partition strategy and a prescreening strategy based on the expected improvement infilling criterion of a simplified Kriging in the optimization process. The SGA updating mechanism is a special characteristic of the proposed algorithm. This mechanism makes a fusion between surrogates and the evolutionary algorithm. The neighbor region partition strategy effectively retains the diversity of the population. Moreover, multiple surrogates used in the SGA updating mechanism make the proposed algorithm optimize robustly. The proposed algorithm is validated by testing several high-dimensional numerical benchmark problems with dimensions varying from 30 to 100, and an overall comparison is made between the proposed algorithm and other optimization algorithms. The results show that the proposed algorithm is very efficient and promising for optimizing high-dimensional expensive problems.																	1089-778X	1941-0026				APR	2020	24	2					365	379		10.1109/TEVC.2019.2919762													
J								An Evolutionary Algorithm for Large-Scale Sparse Multiobjective Optimization Problems	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Pareto optimization; Neural networks; Feature extraction; Evolutionary computation; Training; Sociology; Evolutionary algorithm; evolutionary neural network; feature selection; large-scale multiobjective optimization (MOP); sparse Pareto optimal solutions	NEURAL-NETWORKS; FEATURE-SELECTION; DECOMPOSITION; MOEA/D; PERFORMANCE; FRAMEWORK; ENSEMBLE; VERSION; FASTER	In the last two decades, a variety of different types of multiobjective optimization problems (MOPs) have been extensively investigated in the evolutionary computation community. However, most existing evolutionary algorithms encounter difficulties in dealing with MOPs whose Pareto optimal solutions are sparse (i.e., most decision variables of the optimal solutions are zero), especially when the number of decision variables is large. Such large-scale sparse MOPs exist in a wide range of applications, for example, feature selection that aims to find a small subset of features from a large number of candidate features, or structure optimization of neural networks whose connections are sparse to alleviate overfitting. This paper proposes an evolutionary algorithm for solving large-scale sparse MOPs. The proposed algorithm suggests a new population initialization strategy and genetic operators by taking the sparse nature of the Pareto optimal solutions into consideration, to ensure the sparsity of the generated solutions. Moreover, this paper also designs a test suite to assess the performance of the proposed algorithm for large-scale sparse MOPs. The experimental results on the proposed test suite and four application examples demonstrate the superiority of the proposed algorithm over seven existing algorithms in solving large-scale sparse MOPs.																	1089-778X	1941-0026				APR	2020	24	2					380	393		10.1109/TEVC.2019.2918140													
J								Evolving Deep Convolutional Neural Networks for Image Classification	IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION										Computer architecture; Architecture; Optimization; Genetic algorithms; Encoding; Task analysis; Convolutional neural networks; Convolutional neural network (CNN); deep learning; genetic algorithms (GAs); image classification		Evolutionary paradigms have been successfully applied to neural network designs for two decades. Unfortunately, these methods cannot scale well to the modern deep neural networks due to the complicated architectures and large quantities of connection weights. In this paper, we propose a new method using genetic algorithms for evolving the architectures and connection weight initialization values of a deep convolutional neural network to address image classification problems. In the proposed algorithm, an efficient variable-length gene encoding strategy is designed to represent the different building blocks and the potentially optimal depth in convolutional neural networks. In addition, a new representation scheme is developed for effectively initializing connection weights of deep convolutional neural networks, which is expected to avoid networks getting stuck into local minimum that is typically a major issue in the backward gradient-based optimization. Furthermore, a novel fitness evaluation method is proposed to speed up the heuristic search with substantially less computational resource. The proposed algorithm is examined and compared with 22 existing algorithms on nine widely used image classification tasks, including the state-of-the-art methods. The experimental results demonstrate the remarkable superiority of the proposed algorithm over the state-of-the-art designs in terms of classification error rate and the number of parameters (weights).																	1089-778X	1941-0026				APR	2020	24	2					394	407		10.1109/TEVC.2019.2916183													
J								Combinatorial Iterative Algorithms for Computing the Centroid of an Interval Type-2 Fuzzy Set	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Centroid computation; computational complexity; decision analysis; interval type-2 fuzzy sets (IT2 FSs); Karnik-Mendel algorithms	KARNIK-MENDEL ALGORITHMS; PID CONTROLLER; SYSTEMS; LOGIC	Computing the centroid of an interval type-2 fuzzy set (IT2 FS) is an important type-reduction method. The aim of this paper is to develop a new method to calculate the centroid of an IT2 FS when the problems of centroid computation of an IT2 FS are continuous. For the continuous centroid computation problems, the structures of optimal solutions are strictly proven from mathematics for the first time in this paper. Furthermore, we also prove that the structures of the optimal solutions are unique in the sense of almost everywhere equal, i.e., if there are two optimal solutions, the Lebesgue measure of is equal to 0. Subsequently, a combinatorial iterative (CI) method is proposed to solve the roots of the sufficiently differentiable objective functions. It is proven that the convergence of the proposed iterative method is at least sixth order. Based on the proposed iterative method, two algorithms, called CI algorithms, are devised to compute the centroid of an IT2 FS. The efficiencies of CI algorithms are demonstrated by comparing the continuous Karnik-Mendel algorithms and the Hallye's methods with the CI algorithms through three numerical examples.																	1063-6706	1941-0034				APR	2020	28	4					607	617		10.1109/TFUZZ.2019.2911918													
J								Evidence Combination Based on Credal Belief Redistribution for Pattern Classification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Belief functions; classifier fusion; discounting; evidence theory; pattern classification	RULE; CONFLICT	Evidence theory, also called belief function theory, provides an efficient tool to represent and combine uncertain information for pattern classification. Evidence combination can be interpreted, in some applications, as classifier fusion. The sources of evidence corresponding to multiple classifiers usually exhibit different classification qualities, and they are often discounted using different weights before combination. In order to achieve the best possible fusion performance, a new credal belief redistribution (CBR) method is proposed to revise such evidence. The rationale of CBR consists of transferring belief from one class not just to other classes, but also to the associated disjunctions of classes (i.e., meta-classes). As classification accuracy for different objects in a given classifier can also vary, the evidence is revised according to prior knowledge mined from its training neighbors. If the selected neighbors are relatively close to the evidence, a large amount of belief will be discounted for redistribution. Otherwise, only a small fraction of belief will enter the redistribution procedure. An imprecision matrix estimated based on these neighbors is employed to specifically redistribute the discounted beliefs. This matrix expresses the likelihood of misclassification (i.e., the probability of a test pattern belonging to a class different from the one assigned to it by the classifier). In CBR, the discounted beliefs are divided into two parts. One part is transferred between singleton classes, whereas the other is cautiously committed to the associated meta-classes. By doing this, one can efficiently reduce the chance of misclassification by modeling partial imprecision. The multiple revised pieces of evidence are finally combined by the Dempster-Shafer rule to reduce uncertainty and further improve classification accuracy. The effectiveness of CBR is extensively validated on several real datasets from the UCI repository and critically compared with that of other related fusion methods.																	1063-6706	1941-0034				APR	2020	28	4					618	631		10.1109/TFUZZ.2019.2911915													
J								Fuzzy Approximation Based Asymptotic Tracking Control for a Class of Uncertain Switched Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Dynamic feedback compensator; fuzzy approximation; switched nonlinear systems; tracking control	GLOBAL STABILIZATION; LINEAR-SYSTEMS; STABILITY; DESIGN; FORM	The problem of asymptotic tracking control for a class of uncertain switched nonlinear systems under fuzzy approximation framework is solved in this paper. Superior to most existing results based on fuzzy adaptive control strategy that can only achieve bounded error tracking performance, our proposed control scheme can guarantee the local asymptotic tracking performance for the uncertain switched nonlinear systems under consideration. This is accomplished by constructing a nonsmooth Lyapunov function and introducing a novel discontinuous controller with dynamic feedback compensator in the design procedure. Meanwhile, some concepts, such as differential inclusion and set-valued map, are introduced to theoretically verify the local asymptotic tracking performance of the systems with our proposed controller. With the help of set-valued Lie derivative, the common virtual control functions, the desired controller, and the adaptive laws can be precisely constructed. Finally, simulation results are given to show the effectiveness of the proposed method.																	1063-6706	1941-0034				APR	2020	28	4					632	644		10.1109/TFUZZ.2019.2912138													
J								Full State Constrained Adaptive Fuzzy Control for Stochastic Nonlinear Switched Systems With Input Quantization	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Actuator faults; adaptive backstepping control; Barrier Lyapunov Functions (BLFs); fuzzy logic; quantized inputs	BARRIER LYAPUNOV FUNCTIONS; SLIDING-MODE CONTROL; PURE-FEEDBACK SYSTEMS; COMPENSATION CONTROL; UNCERTAIN SYSTEMS; TRACKING CONTROL; DELAY SYSTEMS; STABILIZATION; STABILITY; DESIGN	In this paper, a fuzzy adaptive full state constrained control approach is proposed for a class of stochastic switched systems subject to quantized input signals and actuator faults. The inherent discontinuous and hybrid characteristics of the concerned systems lead to a difficult task for designing a stable controller. Several fuzzy logic systems are utilized to approximate the unknown nonlinearities and the bound estimation approach is employed to deal with the stochastic switched disturbances. As a result, the negative effects caused by the discontinuous multiple uncertainties can be suppressed. Furthermore, several four-order Barrier Lyapunov Functions are introduced to guarantee that the constraints of the system states are not violated. It is proved that all the signals in the closed-loop system is semiglobally uniformly ultimately bounded. Numerical simulation results have been provided to illustrate the satisfactory performance of the proposed control algorithm.																	1063-6706	1941-0034				APR	2020	28	4					645	657		10.1109/TFUZZ.2019.2912150													
J								Convergence of Recurrent Neuro-Fuzzy Value-Gradient Learning With and Without an Actor	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptive systems; Computer architecture; Dynamic programming; Mobile robots; Adaptive dynamic programming (ADP); convergence analysis; eligibility traces; mobile robot; recurrent neuro-fuzzy (RNF); Takagi-Sugeno (T-S) neuro-fuzzy	SYSTEMS; BACKPROPAGATION; IDENTIFICATION	In recent years, a gradient of the n-step temporaldifference [TD(lambda)] learning has been developed to present an advanced adaptive dynamic programming (ADP) algorithm, called value-gradient learning [VGLl]. In this paper, we improve the VGLl architecture, which is called the "single adaptive actor network [SNVGLl]" because it has only a single approximator function network (critic) instead of dual networks (critic and actor) as in VGLl. Therefore, SNVGLl has lower computational requirements when compared to VGLl. Moreover, in this paper, a recurrent hybrid neuro-fuzzy (RNF) and a first-order Takagi-Sugeno RNF (TSRNF) are derived and implemented to build the critic and actor networks. Furthermore, we develop the novel study of the theoretical convergence proofs for both VGLl and SNVGLl under certain conditions. In this paper, mobile robot simulation model (model based) is used to solve the optimal control problem for affine nonlinear discrete-time systems. Mobile robot is exposed various noise levels to verify the performance and to validate the theoretical analysis.																	1063-6706	1941-0034				APR	2020	28	4					658	672		10.1109/TFUZZ.2019.2912349													
J								Takagi-Sugeno Model Based Event-Triggered Fuzzy Sliding-Mode Control of Networked Control Systems With Semi-Markovian Switchings	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Manganese; Switches; Stability analysis; Dynamics; Networked control systems; Stochastic processes; Event-triggering scheme; fuzzy sliding-mode control; linear matrix inequality; semi-Markovian switching	TIME-VARYING DELAY; JUMP SYSTEMS; DIFFERENTIAL-EQUATIONS; NONLINEAR-SYSTEMS; ROBUST STABILITY; CONTROL DESIGN; STABILIZATION	This paper is focused on the event-triggered fuzzy sliding-mode control of networked control systems regulated by semi-Markov process. First, through movement-decomposition method, the networked control system is transformed into two lower-order subsystems. Then, an event-triggered scheme based on a delay system model approach is proposed in designing the switching surface and obtaining the sliding mode dynamics. Furthermore, a fuzzy sliding-mode controller is developed to realize reachability of a predefined switching surface and desirable sliding motion. Moreover, in terms of linear matrix inequality method, sufficient conditions for stochastic stability of the obtained sliding mode dynamics is developed in the sense of generally uncertain transition rates. Finally, the applicability of the proposed results are verified numerically on the single-link robot arm system.																	1063-6706	1941-0034				APR	2020	28	4					673	683		10.1109/TFUZZ.2019.2914005													
J								An Adaptive Fuzzy Recurrent Neural Network for Solving the Nonrepetitive Motion Problem of Redundant Robot Manipulators	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Manipulators; Fuzzy control; Robustness; Adaptive systems; Recurrent neural networks; Fuzzy control system; quadratic programming; recurrent neural network; robotics; robustness	SYLVESTER EQUATION; LOGIC CONTROL; CONTROLLER	In order to effectively decrease the joint-angular drifts and end-effector position accumulation errors, a novel adaptive fuzzy recurrent neural network (AFRNN) is proposed and exploited to solve the nonrepetitive motion problem of redundant robot manipulators in this paper. First, a quadratic programming (QP)-based repetitive motion scheme is designed according to the kinematics constraint of redundant robot manipulators. Second, the QP-based repetitive motion scheme is converted to a matrix equation according to the Lagrangian multiplier method. Third, inspired by the neural-dynamic and fuzzy control theory, the AFRNN model is designed, which can effectively solve the matrix equation as well as the original nonrepetitive motion problem of redundant robot manipulators. Computer simulation results verify the effectiveness, high accuracy, and robustness to resist external disturbance of the proposed AFRNN scheme.																	1063-6706	1941-0034				APR	2020	28	4					684	691		10.1109/TFUZZ.2019.2914618													
J								Stability and Stabilization With Additive Freedom for Delayed Takagi-Sugeno Fuzzy Systems by Intermediary-Polynomial-Based Functions	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Stability criteria; Linear matrix inequalities; Fuzzy systems; Delays; Symmetric matrices; Numerical stability; Delayed Takagi-Sugeno (T-S) fuzzy systems; intermediary-polynomial-based functions (IPFs); stability; stabilization; variable parameter	TIME-VARYING DELAY; PRODUCT MODEL REPRESENTATION; MATRIX INEQUALITY; ROBUST STABILITY; LINEAR-SYSTEMS; QLPV MODELS; FEASIBILITY; CRITERIA	This paper is devoted to the stability and stabilization for Takagi-Sugeno fuzzy systems with time-varying delays. First, an improved matrix inequality is presented to bound both strictly and nonstrictly proper rational functions, which is more general than the existing versions of reciprocally convex lemmas. Second, by suitable operations on parameter-dependent polynomial multiplied by state rate, a couple of novel intermediary-polynomial-based functions (IPFs) are developed in delay-product types. Benefitting from slack matrices of IPFs, a certain degree of flexibility is furnished. More importantly than that, by feat of adjustment of the variable parameter, the resulting conditions will be further endowed with additive freedom, which relaxes the feasible space in a distinctive manner. Third, by utilizing IPFs along with triple integrals, the stability criteria and the controller design approach are derived by some advanced integral inequalities. Resorting to elaborate construction of IPFs, the strengths of bounding techniques are sufficiently exploited, and the information on delay derivative is adequately reflected. Consequently, more desirable performances are achieved, while without excessive computational complexity. Finally, the effectiveness of the proposed methods is verified by numerical examples.																	1063-6706	1941-0034				APR	2020	28	4					692	705		10.1109/TFUZZ.2019.2914615													
J								Knowledge Distance Measure for the Multigranularity Rough Approximations of a Fuzzy Concept	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Uncertainty; Fuzzy sets; Measurement uncertainty; Extraterrestrial measurements; Radio frequency; Rough sets; Entropy; Earth Mover's distance (EMD); fuzzy concept; fuzzy knowledge distance (FKD); granularity selection; multi-granulation spaces	MEASURING UNCERTAINTY; GRANULAR STRUCTURES; SET; ENTROPY; FUZZINESS; MODEL	Different rough approximation spaces could be induced for an information system by its different attribute subsets, thus the multigranularity rough approximations of a fuzzy concept could be developed. Research on the uncertainty in multi-granulation spaces becomes a basic issue of uncertainty measure. If the uncertainty measure is not accurate enough, two different rough approximation spaces of a fuzzy concept may have the same uncertainty, and the difference between them for describing a fuzzy concept cannot be reflected. In this case, attribute reduction, granularity selection, and multigranularity measure cannot be conducted effectively. Therefore, establishing an uncertainty measure model with strong distinguishing ability in multi-granulation spaces is a key issue in uncertainty knowledge processing. In this paper, this problem will be solved in the view of knowledge distance. First, a fuzzy knowledge distance measure (FKD) based on the Earth Mover's distance is introduced. Even if two rough approximation spaces possess the same uncertainty when describing a fuzzy concept, they can be discriminated by FKD. Then, by studying the change rules of the FKD in a hierarchical quotient space structure, it is found that the FKD between any two rough approximation spaces in an HQSS is equal to the difference between their granularity measure or information measure. Furthermore, in order to show the applicability of the FKD, the FKD is used in granularity selection, attribute reduct, and multigranularity measure. The experimental results show that the FKD-based attribute significance function has a more powerful ability to obtain shorter reduct and it is more robustness, which show the effectiveness of the FKD.																	1063-6706	1941-0034				APR	2020	28	4					706	717		10.1109/TFUZZ.2019.2914622													
J								Epistasis Analysis Using an Improved Fuzzy C-Means-Based Entropy Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Diseases; Testing; Data models; Training data; Genetics; Classification algorithms; Biomedical measurement; Classification system; epistasis; fuzzy c-means (FCM); multifactor dimensionality reduction (MDR)	MULTIFACTOR-DIMENSIONALITY REDUCTION; GENE-GENE INTERACTIONS; INFERENCE	Epistasis detection is vital to determining disease susceptibility in the human genome. With rapid advances in technology, multifactor dimensionality reduction (MDR) has become an effective algorithm for epistasis detection. Classification of high-risk (H) and low-risk (L) groups in MDR operations is a key topic, but it has not been thoroughly investigated. In this paper, we propose an improved fuzzy c-means-based entropy (FCME) approach to address the limitations of binary classification. For this approach, the degree of membership in MDR, referred to as FCMEMDR, was used. The FCME approach and MDR measure were integrated to enable more precise differentiation between similar frequencies of multifactor genotypes in the cases of possible epistasis. We used the MDR measures of correct classification rate and likelihood ratio. Numerous simulated datasets were applied, and the experimental results revealed two measures of FCMEMDR with higher detection rates than those of other MDR-based algorithms. Our analysis of binary and fuzzy classifications in MDR operations may offer insights into the problem of uncertainty in H/L classification. Two measures of FCMEMDR detected significant instances of epistasis associated with coronary artery disease in the Wellcome Trust Case Control Consortium dataset. FCMEMDR is freely available at https://gitlab.com/yudalinemail/fcmemdr.																	1063-6706	1941-0034				APR	2020	28	4					718	730		10.1109/TFUZZ.2019.2914629													
J								Chaotic Type-2 Transient-Fuzzy Deep Neuro-Oscillatory Network (CT2TFDNN) for Worldwide Financial Prediction	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy logic; Predictive models; Oscillators; Indexes; Linguistics; Cryptography; Uncertainty; Chaotic bifurcation transfer function; chaotic deep neuro-oscillatory network; chaotic type-2 transient-fuzzy logic (CT2TFL); financial prediction; Lee oscillator	TIME-SERIES PREDICTION; AUTOASSOCIATIVE NETWORK; COMPONENT ANALYSIS; SYSTEM; MODEL; OPTIMIZATION; FORECAST; SPIKING	Over the years, financial engineering ranging from the study of financial signals to the modeling of financial prediction is one of the most exciting topics for both academia and financial community. With the flourishing AI technology in the past 20 years, various hybrid intelligent financial prediction systems with the integration of neural networks, chaos theory, fuzzy logic, and genetic algorithms have been proposed. An interval type-2 fuzzy logic system (IT2FLS) with its remarkable capability for the modeling of highly uncertain events and attributes provides a perfect tool to interpret various financial phenomena and patterns. In this paper, the author proposes a chaotic type-2 transient-fuzzy deep neuro-oscillatory network with retrograde signaling (CT2TFDNN) for worldwide financial prediction. With the extension of author's original work on Lee oscillator-a chaotic discrete-time neural oscillator with profound transient-chaotic property-CT2TFDNN provides: effective modeling of an IT2FLS with a chaotic transient-fuzzy membership function; and effective time-series network training and prediction using a chaotic deep neuro-oscillatory network with retrograde signaling. CT2TFDNN not only provides a fast chaotic fuzzy-neuro deep learning and forecast solution, but also successfully resolves the massive data overtraining and deadlock problems, which are usually imposed by traditional recurrent neural networks using classical sigmoid-based activation functions. From the implementation perspective, CT2TFDNN is integrated with 2048 trading-day time-series financial data and top-10 major financial signals as fuzzy financial signals for the real-time prediction of 129 worldwide financial products that consists of: nine major cryptocurrencies, 84 worldwide forex, 19 major commodities, and 17 worldwide financial indices.																	1063-6706	1941-0034				APR	2020	28	4					731	745		10.1109/TFUZZ.2019.2914642													
J								Nonlinear Systems With Uncertain Periodically Disturbed Control Gain Functions: Adaptive Fuzzy Control With Invariance Properties	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Nonlinear systems; Adaptive systems; Stability analysis; Controllability; Standards; Set theory; Adaptive fuzzy control; dynamic surface control (DSC); invariant set theory; periodic disturbances	DYNAMIC SURFACE CONTROL; TRACKING CONTROL	This paper proposes a novel adaptive fuzzy dynamic surface control (DSC) method for an extended class of periodically disturbed strict-feedback nonlinear systems. The peculiarity of this extended class is that the control gain functions are not bounded a priori but simply taken to be continuous and with a known sign. In contrast with existing strategies, controllability must be guaranteed by constructing appropriate compact sets ensuring that all trajectories in the closed-loop system never leave these sets. We manage to do this by means of invariant set theory in combination with the Lyapunov theory. In other words, boundedness is achieved a posteriori as a result of stability analysis. The approximator composed of fuzzy logic systems and Fourier series expansion is constructed to deal with the unknown periodic disturbance terms.																	1063-6706	1941-0034				APR	2020	28	4					746	757		10.1109/TFUZZ.2019.2915192													
J								Incremental Fuzzy C-Regression Clustering From Streaming Data for Local-Model-Network Identification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Prototypes; Clustering algorithms; Adaptation models; Data models; Partitioning algorithms; Takagi-Sugeno model; Linear programming; Evolving fuzzy model identification; fuzzy C-regression clustering; incremental clustering; stream data	EVOLVING FUZZY; INFERENCE SYSTEM; NEURAL-NETWORKS; ALGORITHM	In this paper, a new approach to evolving fuzzy model identification from streaming data is given. The structure of the model is given as a local model network in Takagi-Sugeno form, and the partitioning of the input-output space is based on metrics in which these local models are defined as prototypes of the clusters. This means that the clusters and the local models share the same parameters; therefore, the number of parameters of the evolving system is much lower in comparison to similar systems of comparable complexity, and the problems of parameter identifiability are not a particular issue. The algorithm adds the local models in an incremental fashion and recursively adapts the local model parameters. The proposed algorithm is tested on three examples to demonstrate the main features. The first example is a simple simulated example with intersecting clusters; the second is a very well-known benchmark that treats the Mackey-Glass time series; the third is an example that shows the classification of the data from a laser rangefinder. These examples show the great potential of the proposed approach in certain applications.																	1063-6706	1941-0034				APR	2020	28	4					758	767		10.1109/TFUZZ.2019.2916036													
J								Sliding Mode Control for Fuzzy Singular Systems With Time Delay Based on Vector Integral Sliding Mode Surface	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Symmetric matrices; Sliding mode control; Robustness; Stability analysis; Mathematical model; Trajectory; Fuzzy systems; linear matrix inequalities (LMIs); singular systems; sliding mode control (SMC); vector integral sliding mode surface (VISMS)	MARKOVIAN JUMP SYSTEMS; H-INFINITY CONTROL; NONLINEAR-SYSTEMS; STABILITY ANALYSIS; TRACKING CONTROL; NEURAL-NETWORKS; DESIGN	This article studies the sliding mode control (SMC) of a class of Takagi-Sugeno (T-S) fuzzy singular systems with matched external disturbances. The key advantage of this article is that the input matrices of the considered fuzzy systems could be different, and a novel SMC provided in this article has strong robustness. In this article, systems with delayed state are considered. The time delay is a known constant, and the full state vector is available for feedback. At first, a new sliding mode surface, vector integral sliding mode surface, is proposed. By use of this surface, the stabilization problem for some T-S fuzzy singular systems with different input matrices can be settled. Second, a new SMC law is provided to avoid the state of the considered fuzzy system escaping from every subsliding mode surface. Finally, two examples are provided to verify the validity of the method proposed in this article.																	1063-6706	1941-0034				APR	2020	28	4					768	782		10.1109/TFUZZ.2019.2916049													
J								Comparing the Performance Potentials of Singleton and Non-singleton Type-1 and Interval Type-2 Fuzzy Systems in Terms of Sculpting the State Space	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Measurement uncertainty; Firing; Fuzzy sets; Aerospace electronics; Extraterrestrial phenomena; Uncertainty; Interval type-2 (IT2) fuzzy system; nonsingleton (NS) fuzzifier; rule partitions; sculpting the state space; type-1 (T1) fuzzy system	LOGIC SYSTEMS; ALGORITHMS	This paper provides a novel and better understanding of the performance potential of a nonsingleton (NS) fuzzy system over a singleton (S) fuzzy system. It is done by extending sculpting the state space works from S to NS fuzzification and demonstrating uncertainties about measurements, modeled by NS fuzzification: first, fire more rules more often, manifested by a reduction (increase) in the sizes of first-order rule partitions for those partitions associated with the firing of a smaller (larger) number of rules-the coarse sculpting of the state space; second, this may lead to an increase or decrease in the number of type-1 (T1) and interval type-2 (IT2) first-order rule partitions, which now contain rule pairs that can never occur for S fuzzification-a new rule crossover phenomenon-discovered using partition theory; and third, it may lead to a decrease, the same number, or an increase in the number of second-order rule partitions, all of which are system dependent-the fine sculpting of the state space. The authors' conjecture is that it is the additional control of the coarse sculpting of the state space, accomplished by prefiltering and the max-min (or max-product) composition, which provides an NS T1 or IT2 fuzzy system with the potential to outperform an S T1 or IT2 system when measurements are uncertain.																	1063-6706	1941-0034				APR	2020	28	4					783	794		10.1109/TFUZZ.2019.2916103													
J								Sugeno Integrals, H-alpha, and H-beta Indices: How to Compare Scientists From Different Academic Areas	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Bibliometric databases; compensative h-index; h-index; Sugeno integral	FIELDS; CITATIONS; NUMBER	The usage of the standard h-index to compare scientists from different research fields is often misleading. To avoid this possible failure, based on the Sugeno integrals, three new modified indices Ha, H ss, and H ss a are proposed, thus, allowing to compensate the lower number of citations or of papers characteristic for a considered field. Our approach has a transparent geometric interpretation, and thus, it offers good candidates to replace the standard h-index H in considered databases, such as Web of Science (WOS), Scopus, or Google Scholar.																	1063-6706	1941-0034				APR	2020	28	4					795	800		10.1109/TFUZZ.2019.2914625													
J								Fusing Self-Organized Neural Network and Keypoint Clustering for Localized Real-Time Background Subtraction	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Self-organized neural network; clustering; background modeling; foreground detection	MOVING OBJECT DETECTION; FOREGROUND DETECTION; CAMERA; SEGMENTATION; ALGORITHM; FLOW	Moving object detection in video streams plays a key role in many computer vision applications. In particular, separation between background and foreground items represents a main prerequisite to carry out more complex tasks, such as object classification, vehicle tracking, and person re-identification. Despite the progress made in recent years, a main challenge of moving object detection still regards the management of dynamic aspects, including bootstrapping and illumination changes. In addition, the recent widespread of Pan-Tilt-Zoom (PTZ) cameras has made the management of these aspects even more complex in terms of performance due to their mixed movements (i.e. pan, tilt, and zoom). In this paper, a combined keypoint clustering and neural background subtraction method, based on SelfOrganized Neural Network (SONN), for real-time moving object detection in video sequences acquired by PTZ cameras is proposed. Initially, the method performs a spatio-temporal tracking of the sets of moving keypoints to recognize the foreground areas and to establish the background. Then, it adopts a neural background subtraction, localized in these areas, to accomplish a foreground detection able to manage bootstrapping and gradual illumination changes. Experimental results on three well-known public datasets, and comparisons with different key works of the current literature, show the efficiency of the proposed method in terms of modeling and background subtraction.																	0129-0657	1793-6462				APR	2020	30	4							2050016	10.1142/S0129065720500161													
J								Automatic Seizure Detection using Fully Convolutional Nested LSTM	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Seizure detection; EEG; deep learning; fully convolutional network; NLSTM	EPILEPTIC SEIZURES; NEURAL-NETWORK; CLASSIFICATION; METHODOLOGY; PREDICTION; TRANSFORM; ENTROPY; EEGS	The automatic seizure detection system can effectively help doctors to monitor and diagnose epilepsy thus reducing their workload. Many outstanding studies have given good results in the two-class seizure detection problems, but most of them are based on hand-wrought feature extraction. This study proposes an end-to-end automatic seizure detection system based on deep learning, which does not require heavy preprocessing on the EEG data or feature engineering. The fully convolutional network with three convolution blocks is first used to learn the expressive seizure characteristics from EEG data. Then these robust EEG features pertinent to seizures are presented as an input to the Nested Long Short-Term Memory (NLSTM) model to explore the inherent temporal dependencies in EEG signals. Lastly, the high-level features obtained from the NLSTM model are fed into the softmax layer to output predicted labels. The proposed method yields an accuracy range of 98.44-100% in 10 different experiments based on the Bonn University database. A larger EEG database is then used to evaluate the performance of the proposed method in real-life situations. The average sensitivity of 97.47%, specificity of 96.17%, and false detection rate of 0.487 per hour are yielded. For CHB-MIT Scalp EEG database, the proposed model also achieves a segment-level sensitivity of 94.07% with a false detection rate of 0.66 per hour. The excellent results obtained on three different EEG databases demonstrate that the proposed method has good robustness and generalization power under ideal and real-life conditions.																	0129-0657	1793-6462				APR	2020	30	4							2050019	10.1142/S0129065720500197													
J								Automatic Seizure Detection Based on S-Transform and Deep Convolutional Neural Network	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										S-transform; deep learning; Convolutional neural networks (CNN); time-frequency representation; Seizure detection	WAVELET-CHAOS METHODOLOGY; EEG-BASED DIAGNOSIS; PREDICTION; EPILEPSY; LONG; CLASSIFICATION; LOCALIZATION; FRACTALITY; SPECTRUM; BRAIN	Automatic seizure detection is significant for the diagnosis of epilepsy and reducing the massive workload of reviewing continuous EEGs. In this work, a novel approach, combining Stockwell transform (S-transform) with deep Convolutional Neural Networks (CNN), is proposed to detect seizure onsets in long-term intracranial EEG recordings. Primarily, raw EEG data is filtered with wavelet decomposition. Then, S-transform is used to obtain a proper time-frequency representation of each EEG segment. After that, a 15-layer deep CNN using dropout and batch normalization serves as a robust feature extractor and classifier. Finally, smoothing and collar technique are applied to the outputs of CNN to improve the detection accuracy and reduce the false detection rate (FDR). The segment- based and event-based evaluation assessments and receiver operating characteristic (ROC) curves are employed for the performance evaluation on a public EEG database containing 21 patients. A segment-based sensitivity of 97.01% and a specificity of 98.12% are yielded. For the event-based assessment, this method achieves a sensitivity of 95.45% with an FDR of 0.36/h.																	0129-0657	1793-6462				APR	2020	30	4							1950024	10.1142/S0129065719500242													
J								The Impact of Repetitive Transcranial Magnetic Stimulation on Functional Connectivity in Major Depressive Disorder and Bipolar Disorder Evaluated by Directed Transfer Function and Indices Based on Graph Theory	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										EEG; repetitive transcranial magnetic stimulation; functional connectivity; directed transfer function; graph theory; depression	EEG-SIGNAL; PHASE SYNCHRONIZATION; THEORETICAL ANALYSIS; GRANGER CAUSALITY; BRAIN NETWORKS; TIME-SERIES; GAMMA; DIAGNOSIS; THETA; POTENTIATION	The objective of this work was to study the impact of repetitive Transcranial Magnetic Stimulation (rTMS) on the EEG connectivity evaluated by indices based on graph theory, derived from Directed Transfer Function (DTF), in patients with major depressive disorder (MDD) or with bipolar disorder (BD). The results showed the importance of beta and gamma rhythms. The indices density, degree and clustering coefficient increased in MDD responders in beta and gamma bands after rTMS. Interestingly, the density and the degree changed in theta band in both groups of nonresponders (decreased in MDD nonresponders but increased in BD nonresponders). Moreover, both indices of integration (the characteristic path length and the global efficiency) as well as the clustering coefficient increased in BD nonresponders for gamma band. In BD responders, the activity increased in the frontal lobe, mainly in the left hemisphere, while in MDD responders in the central posterior part of brain. The fronto-posterior asymmetry decreased in both groups of responders in delta and beta bands. Changes in inter-hemispheric asymmetry were found only in BD nonresponders in all bands, except gamma band. Comparison between groups showed that the degree increased in delta band independently on disease (BD, MDD). These preliminary results showed that the DTF may be a useful marker allowing for evaluation of effectiveness of the rTMS therapy as well for group differentiation between MDD and BD considering separately groups of responders and nonresponders. However, further investigation should be performed over larger groups of patients to confirmed our findings.																	0129-0657	1793-6462				APR	2020	30	4							2050015	10.1142/S012906572050015X													
J								Real-Time Multi-Modal Estimation of Dynamically Evoked Emotions Using EEG, Heart Rate and Galvanic Skin Response	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Real-time; EEG; BVP; GSR; artifact removal; emotion estimation; HRI	CONNECTIVITY; RECOGNITION; BRAIN; COMPLEXITY; MEMORY; SYSTEM	Emotion estimation systems based on brain and physiological signals such as electro encephalography (EEG), blood-volume pressure (BVP), and galvanic skin response (GSR) are gaining special attention in recent years due to the possibilities they offer. The field of human-robot interactions (HRIs) could benefit from a broadened understanding of the brain and physiological emotion encoding, together with the use of lightweight software and cheap wearable devices, and thus improve the capabilities of robots to fully engage with the users emotional reactions. In this paper, a previously developed methodology for real-time emotion estimation aimed for its use in the field of HRI is tested under realistic circumstances using a self-generated database created using dynamically evoked emotions. Other state-of-theart, real-time approaches address emotion estimation using constant stimuli to facilitate the analysis of the evoked responses, remaining far from real scenarios since emotions are dynamically evoked. The proposed approach studies the feasibility of the emotion estimation methodology previously developed, under an experimentation paradigm that imitates a more realistic scenario involving dynamically evoked emotions by using a dramatic film as the experimental paradigm. The emotion estimation methodology has proved to perform on real-time constraints while maintaining high accuracy on emotion estimation when using the self-produced dynamically evoked emotions multi-signal database.																	0129-0657	1793-6462				APR	2020	30	4							2050013	10.1142/S0129065720500136													
J								DWnet: Deep-wide network for 3D action recognition	ROBOTICS AND AUTONOMOUS SYSTEMS										Action recognition; Deep structure; 3D skeleton; Human-robot cooperation	APPROXIMATION; SYSTEM	Action recognition plays an important role in human-robot cooperation and interaction. By recognizing human actions, robots can imitate or reproduce human actions and obtain skills. Recently, convolutional neural networks (CNNs) have been widely used to recognize actions based on 3D skeleton. Good performance has been achieved due to the approximation capability gained from the depth of the model. Unfortunately, in the mainstream deep structures, dropout and fully connected layers are usually used to classify actions. That is to say, ensemble is used to guarantee the recognition performance, which decreases the computational efficiency. In order to improve the computational efficiency, we propose in this paper a deep-wide network (DWnet) to recognize human actions based on 3D skeleton. Specifically, we modify the decision-making mechanism of the deep CNN with a shallow structure, which improves the computational efficiency. The state-of-the-art deep CNN is used to extract spatial-temporal features from the skeletal sequence. Then features are transformed into a higher dimensional feature space to obtain global information and classified by the modified decision making mechanism. Experiments on two skeletal datasets demonstrate the advantage of the proposed model on testing efficiency and the effectiveness of the novel model to recognize the action. The code has been publicly available at https://github.com/YHDang/DWnet. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103441	10.1016/j.robot.2020.103441													
J								Object shape estimation and modeling, based on sparse Gaussian process implicit surfaces, combining visual data and tactile exploration	ROBOTICS AND AUTONOMOUS SYSTEMS										Tactile sensing; Shape modeling; Implicit surface; 3D reconstruction; Gaussian process; Regression	INFORMATION; ALGORITHM	Inferring and representing three-dimensional shapes is an important part of robotic perception. However, it is challenging to build accurate models of novel objects based on real sensory data, because observed data is typically incomplete and noisy. Furthermore, imperfect sensory data suggests that uncertainty about shapes should be explicitly modeled during shape estimation. Such uncertainty models can usefully enable exploratory action planning for maximum information gain and efficient use of data. This paper presents a probabilistic approach for acquiring object models, based on visual and tactile data. We study Gaussian Process Implicit Surface (GPIS) representation. GPIS enables a non-parametric probabilistic reconstruction of object surfaces from 3D data points, while also providing a principled approach to encode the uncertainty associated with each region of the reconstruction. We investigate different configurations for GPIS, and interpret an object surface as the level-set of an underlying sparse GP. Experiments are performed on both synthetic data, and also real data sets obtained from two different robots physically interacting with objects. We evaluate performance by assessing how close the reconstructed surfaces are to ground-truth object models. We also evaluate how well objects from different categories are clustered, based on the reconstructed surface shapes. Results show that sparse GPs enable a reliable approximation to the full GP solution, and the proposed method yields adequate surface representations to distinguish objects. Additionally the presented approach is shown to provide computational efficiency, and also efficient use of the robot's exploratory actions. (C) 2020 The Authors. Published by Elsevier B.V.																	0921-8890	1872-793X				APR	2020	126								103433	10.1016/j.robot.2020.103433													
J								Autonomous navigation for UAVs managing motion and sensing uncertainty	ROBOTICS AND AUTONOMOUS SYSTEMS										Autonomous navigation; Motion planning; Motion uncertainty; UAVs; Scene reconstruction	PATH; AVOIDANCE	We present a motion planner for the autonomous navigation of UAVs that manages motion and sensing uncertainty at planning time. By doing so, optimal paths in terms of probability of collision, traversal time and uncertainty are obtained. Moreover, our approach takes into account the real dimensions of the UAV in order to reliably estimate the probability of collision from the predicted uncertainty. The motion planner relies on a graduated fidelity state lattice and a novel multi-resolution heuristic which adapt to the obstacles in the map. This allows managing the uncertainty at planning time and yet obtaining solutions fast enough to control the UAV in real time. Experimental results show the reliability and the efficiency of our approach in different real environments and with different motion models. Finally, we also report planning results for the reconstruction of 3D scenarios, showing that with our approach the UAV can obtain a precise 3D model autonomously. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103455	10.1016/j.robot.2020.103455													
J								Understanding a public environment via continuous robot observations	ROBOTICS AND AUTONOMOUS SYSTEMS										Human tracking; Point cloud data; Data analysis		This paper presents a study on a point cloud analysis captured by a robot navigating in a shopping mall environment. It investigates the type and how much information the robot could extract from the environment. For this purpose, information regarding environmental changes and the number of people in shops was extracted and analyzed. First, the robot was manually controlled to collect data in a typical shopping mall having different types of shops and a food court. As the robot navigated thoroughly around the environment, seven data recordings of data obtained from various onboard sensors were recorded during afternoon hours over three consecutive days. We built a composite map by overlaying 3D point clouds for each recording sharing the same coordinate frame, which reveals the changes in the environment's static objects. The number of humans at each shop in each recording was computed using a human tracker. Then, we computed a fourteen-dimensional vector for each shop: seven dimensions for environmental changes and seven for human density. Experimental results show that the environmental changes and the human density at each shop are consistent with the visual changes that occurred in the shops and the number of people who visited the shops. Correlation analysis was done among shop changes, shop open space, and human density where results suggest that change in shop configurations are often done in smaller shops and shops with larger open space tend to attract larger number of customers. Finally, information extracted from shops was used to categorize the shops according to similarity. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103443	10.1016/j.robot.2020.103443													
J								Deploying MAVs for autonomous navigation in dark underground mine environments	ROBOTICS AND AUTONOMOUS SYSTEMS										MAVs navigation; Autonomous tunnel inspection; Mining aerial robotics	EXPLORATION; VISION; INDOOR	Operating Micro Aerial Vehicles (MAVs) in subterranean environments is becoming more and more relevant in the field of aerial robotics. Despite the large spectrum of technological advances in the field, flying in such challenging environments is still an ongoing quest that requires the combination of multiple sensor modalities like visual/thermal cameras as well as 3D and 2D lidars. Nevertheless, there exist cases in subterranean environments where the aim is to deploy fast and lightweight aerial robots for area reckoning purposes after an event (e.g. blasting in production areas). This work proposes a novel baseline approach for the navigation of resource constrained robots, introducing the aerial underground scout, with the main goal to rapidly explore unknown areas and provide a feedback to the operator. The main proposed framework focuses on the navigation, control and vision capabilities of the aerial platforms with low-cost sensor suites, contributing significantly towards real-life applications. The merit of the proposed control architecture is that it considers the flying platform as a floating object, composing a velocity controller on the x, y axes and altitude control to navigate along the tunnel. Two novel approaches make up the cornerstone of the proposed contributions for the task of navigation: (1) a vector geometry method based on 2D lidar, and (2) a Deep Learning (DL) method through a classification process based on an on-board image stream, where both methods correct the heading towards the center of the mine tunnel. Finally, the framework has been evaluated in multiple field trials in an underground mine in Sweden. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103472	10.1016/j.robot.2020.103472													
J								Loop closure detection using supervised and unsupervised deep neural networks for monocular SLAM systems	ROBOTICS AND AUTONOMOUS SYSTEMS										Visual SLAM; Loop closure detection; Bag of words; Super dictionary	LOCALIZATION; RECOGNITION; MAP	The detection of true loop closure in Visual Simultaneous Localization And Mapping (vSLAM) can help in many ways, it helps in re-localization, improves the accuracy of the map, and helps in registration algorithms to obtain more accurate and consistent results. The loop closure detection is affected by many parameters, including illumination conditions, seasons, different viewpoints and mobile objects. This paper proposes a novel approach based on super dictionary different from traditional BoW dictionary that uses more advanced and more abstract features of deep learning. The proposed approach does not need to generate vocabulary, which makes it memory efficient and instead it stores exact features, which are small in number and hold very less amount of memory as compared to traditional BoW approach in which each frame holds the same amount of memory as the number of words in the vocabulary. Two deep neural networks are used together to speed up the loop closure detection and to ignore the effect of mobile objects on loop closure detection. We have compared the results with most popular Bag of Words methods DBoW2 and DBoW3, and state-of-the-art iBoW-LCD using five publicly available datasets, and the results show that the proposed method robustly performs loop closure detection and is eight times faster than the state-of-the-art approaches of a similar kind. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103470	10.1016/j.robot.2020.103470													
J								Subject-specific compliance control of an upper-limb bilateral robotic system	ROBOTICS AND AUTONOMOUS SYSTEMS										Compliance control; Subject-specific workspace; Upper-limb robotics; Bilateral	ANKLE REHABILITATION ROBOT; EXOSKELETON; DRIVEN	This paper proposes a new compliance control strategy on a robot-assisted bilateral upper limb rehabilitation system. The robotic compliance regulation was achieved by modifying predefined training trajectories in real time, based on measured human-robot interaction force and human users' position within subject-specific workspace. Experimental data were collected from healthy subjects, and indicate that human users can follow predefined training trajectories well under real-time compliance variation, with the maximum normalized root mean square error (NRMSE) value no greater than 1.44%. Preliminary findings are encouraging, which demonstrates the availability of the proposed subject-specific compliance adaptation strategy, and the potential with enhanced training safety and efficacy. Future work will consider conducting a direct comparison between a bilateral upper limb rehabilitation device (BULReD)-assisted compliance training with or without subject-specific adaptation on a large sample of participants with upper limb disabilities. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103478	10.1016/j.robot.2020.103478													
J								Semantic approach to RIoT autonomous robots mission coordination	ROBOTICS AND AUTONOMOUS SYSTEMS										Coordination; Autonomous robots; Internet of Things; Ontology; Robot sensing systems; Semantic technology; Testbeds	FRAMEWORK; INTERNET; GENERATION; THINGS	Internet of Things (IoT) has recently become the key for innovation and progress in many industrial sectors and scientific areas. However, it brings many challenges and issues, such as growing number of connected devices, heterogeneity, amount of generated data, security and privacy issues, interoperability and many others. Since devices not only collect data, but also take actions that affect the environment, device coordination in the context of IoT systems is becoming more and more important, especially if the IoT convergence with robotics, known as "Internet of Robotic Things'' (RIoT), is taken into consideration. In novel cyber-physical systems coordination is very important for situations when many devices working parallel have higher potential to achieve the given task more effectively, than a single device operating independently. RIoT experimentation testbeds facilitate development of such cyber-physical systems where devices need to be aware of the environment while interacting with other devices in order to achieve a common goal. In this paper, we propose a semantic-driven framework for automated autonomous robots coordination in the context of RIoT-based experimentation testbeds. Framework for automatic coordinated mission generation within the robotics experimentation platform testbed is evaluated. Results of evaluation are presented and discussed. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103438	10.1016/j.robot.2020.103438													
J								Solving the area coverage problem with UAVs: A vehicle routing with time windows variation	ROBOTICS AND AUTONOMOUS SYSTEMS										Vehicle routing problem; Unmanned aerial vehicle; VRPTW; Transportation problem; Simplex algorithm	TABU SEARCH ALGORITHM	In real life, providing security for a set of large areas by covering the areas with Unmanned Aerial Vehicles (UAVs) is a difficult problem that consists of multiple objectives. These difficulties are even greater if the area coverage has to be sustained through a specific time window. We address this by considering a Vehicle Routing Problem with a Time Windows (VRPTW) variation in which the capacity of agents is counted as one and each customer (target area) is to be supplied with more than one vehicle simultaneously and without violating time windows. In this problem, our aim is to find a way to cover all areas with the necessary number of UAVs during the time windows, while minimizing the total distance traveled, and providing a fast solution by satisfying the additional constraint that each agent has limited fuel. We present a novel algorithm that relies on clustering the target areas according to their time windows, and then incrementally generating transportation problems with each cluster and the ready UAVs. We then solve the transportation problems with a simplex algorithm. The performance of the proposed algorithm and other algorithms implemented in order to compare the solution quality is evaluated through example scenarios with practical problem sizes. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103435	10.1016/j.robot.2020.103435													
J								Development of a virtual reality simulator for a strategy for coordinating cooperative manipulator robots using cloud computing	ROBOTICS AND AUTONOMOUS SYSTEMS										Cloud; Services; Collaborative; Cooperative; Coordination; Virtual reality	MOBILE ROBOTS; SYSTEM; DESIGN; ENVIRONMENT; ALGORITHM	This article deals with the development of a simulator that recreates, in virtual reality, a team of Selectively Compliance Assembly Robot Arms (SCARA). This team works cooperatively to fulfill the task of stacking rectangular objects coordinated through a strategy that includes a cloud server responsible for communication between the robots. The execution of the task is based on a leader/follower configuration. In this configuration, the leader performs a computed trajectory constantly reporting its position to the remote server. The remote server, in turn, sends this information back to the follower so this can follow the leader. The application combines MatLab (R) and Java. The latter is specifically used for communication routines, since its versatility makes it easy to be incorporated into any type of machine. This paper seeks to demonstrate the advantages of incorporating cloud resources into a multi-robot system and how its performance can be tested by means of the application developed. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103447	10.1016/j.robot.2020.103447													
J								Research on optimized time-synchronous online trajectory generation method for a robot arm	ROBOTICS AND AUTONOMOUS SYSTEMS										Online trajectory generation; Jerk constraint; Time synchronization; Minimum peak of velocity; Minimum acceleration peak		Based on sensor input, a robot arm should dynamically adjust its trajectory while maintaining stability to react to a sudden change in the target point in an unknown environment. To solve this problem, in this study, a time-optimized online trajectory generation (OTG) method is proposed using an S-curve velocity profile, which can generate trajectories in response to external sensor signals. The generated trajectory has characteristics that can guarantee the synchronization of multijoints according to an arbitrary initial state and a desired target state under velocity, acceleration, and jerk constraints. For multijoints time synchronization, two different characteristics are considered according to different application scenarios: minimum velocity or peak acceleration, which correspond to two sub-methods. The first one can be used to calculate with a minimum velocity peak, which can quickly adjust the trajectory according to the signal feedback. The second can be used to calculate the minimization of the acceleration peak, which can reduce the vibration of the robot arm due to a change in the motion. Compared with other OTG methods, the second proposed sub-method can effectively reduce the acceleration peak of the planned motion with the same synchronization time and parameters. Additionally, both sub-methods have the advantage of a rapid calculation and can generate time synchronization motion trajectories for 6 axes in 0.21 ms on a personal computer, fully satisfying the requirements of online motion planning. Finally, the effectiveness of the algorithm is verified by simulations and experiments with a lab-developed robot arm. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103453	10.1016/j.robot.2020.103453													
J								I-Support: A robotic platform of an assistive bathing robot for the elderly population	ROBOTICS AND AUTONOMOUS SYSTEMS										Human-robot communication; Assistive human-robot interaction (HRI); Bathing robot; Multimodal dataset; Audio-gestural command recognition; Online validation with elderly users	DISABILITY; ACCURACY; KINECT; STATE; HOME	In this paper we present a prototype integrated robotic system, the I-Support bathing robot, that aims at supporting new aspects of assisted daily-living activities on a real-life scenario. The paper focuses on describing and evaluating key novel technological features of the system, with the emphasis on cognitive human-robot interaction modules and their evaluation through a series of clinical validation studies. The I-Support project on its whole has envisioned the development of an innovative, modular, ICT-supported service robotic system that assists frail seniors to safely and independently complete an entire sequence of physically and cognitively demanding bathing tasks, such as properly washing their back and their lower limbs. A variety of innovative technologies have been researched and a set of advanced modules of sensing, cognition, actuation and control have been developed and seamlessly integrated to enable the system to adapt to the target population abilities. These technologies include: human activity monitoring and recognition, adaptation of a motorized chair for safe transfer of the elderly in and out the bathing cabin, a context awareness system that provides full environmental awareness, as well as a prototype soft robotic arm and a set of user-adaptive robot motion planning and control algorithms. This paper focuses in particular on the multimodal action recognition system, developed to monitor, analyze and predict user actions with a high level of accuracy and detail in realtime, which are then interpreted as robotic tasks. In the same framework, the analysis of human actions that have become available through the project's multimodal audio-gestural dataset, has led to the successful modeling of Human-Robot Communication, achieving an effective and natural interaction between users and the assistive robotic platform. In order to evaluate the I-Support system, two multinational validation studies were conducted under realistic operating conditions in two clinical pilot sites. Some of the findings of these studies are presented and analyzed in the paper, showing good results in terms of: (i) high acceptability regarding the system usability by this particularly challenging target group, the elderly end-users, and (ii) overall task effectiveness of the system in different operating modes. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				APR	2020	126								103451	10.1016/j.robot.2020.103451													
J								Film mood induction and emotion classification using physiological signals for health and wellness promotion in older adults living alone	EXPERT SYSTEMS										emotion classification; health promotion; mood induction; older adult; physiological signals; wearable; wellness promotion	AROUSAL LEVEL CLASSIFICATION; AGING ADULT; STRESS; MODEL; VALIDATION; REACTIVITY; TRANSFORM	This paper introduces a wearable hardware/software system specifically tailored to detect seven emotions (neutral, tenderness, amusement, anger, disgust, fear, and sadness) aimed at promoting health and wellness in older adults living alone at home. The complete software and hardware architectures acquiring and processing electrodermal activity and photoplethysmography signals are introduced. The wearable emotion detection system is trained by eliciting the desired emotions on 39 older adults through a film mood induction procedure. Seventeen features are calculated on skin conductance response and heart rate variability data, grouped into five statistical, four temporal, and eight morphological features. Then, these features are used to run emotion classification considering support vector machines, decision trees, and quadratic discriminant analysis. In line with psychological findings, the results offer a global accuracy of 82% in negative emotion (anger, disgust, fear, and sadness) classification. For positive emotions (tenderness and amusement), also in conformity with previous psychological outcomes, amusement shows the highest ratio of hits (92%) but tenderness the lowest one (66%). These results demonstrate that our wearable emotion detection system can be used by ageing adults, especially for detecting negative emotions that usually damage health and wellness and lead to social isolation.																	0266-4720	1468-0394				APR	2020	37	2							e12425	10.1111/exsy.12425													
J								Fuzzy-description logic for supporting the rehabilitation of the elderly	EXPERT SYSTEMS										elderly; fuzzy-description logic; gerontechnology; ontology; telerehabilitation	INFERENCE SYSTEM; UPPER-EXTREMITY; FRAMEWORK; REASONER	According to the latest statistics, the proportion of the elderly (+65) is increasing and is expected to double within the European Union in a period of 50 years. This ageing is due to the improvement of quality of life and advances in medicine in the last decades. Gerontechnology is receiving a great deal of attention as a way of providing the elderly with sustainable products, environments, and services combining gerontology and technology. One of the most important aspects to consider by gerontechnology is the mobility/rehabilitation technologies, because there is an important relationship between mobility and the elderly's quality of life. Telerehabilitation systems have emerged to allow the elderly to perform their rehabilitation exercises remotely. However, in many cases, the proposed systems assist neither the patients nor the experts about the progress of the rehabilitation. To address this problem, we propose in this paper, a fuzzy-semantic system for evaluating patient's physical state during the rehabilitation process based on well-known standard for patients' evaluation. Moreover, a tool called FINE has been developed that facilitates the evaluation be accomplished in a semi-automatic way first asking patients to carry out a set of standard tests and then inferencing their state by means of a fuzzy-semantic approach using the data captured during the rehabilitation tasks.																	0266-4720	1468-0394				APR	2020	37	2							e12464	10.1111/exsy.12464													
J								COMBINING CLASSIFIERS FOR FOREIGN PATTERN REJECTION	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										data mining; knowledge engineering	OUTLIER DETECTION; ESTIMATOR; SUPPORT	In this paper, we look closely at the issue of contaminated data sets, where apart from legitimate (proper) patterns we encounter erroneous patterns. In a typical scenario, the classification of a contaminated data set is always negatively influenced by garbage patterns (referred to as foreign patterns). Ideally, we would like to remove them from the data set entirely. The paper is devoted to comparison and analysis of three different models capable to perform classification of proper patterns with rejection of foreign patterns. It should be stressed that the studied models are constructed using proper patterns only, and no knowledge about thecharacteristics of foreign patterns is needed. The methods are illustrated with a case study of handwritten digits recognition, but the proposed approach itself is formulated in a general manner. Therefore, it can be applied to different problems. We have distinguished three structures: global, local, and embedded, all capable to eliminate foreign patterns while performing classification of proper patterns at the same time. A comparison of the proposed models shows that the embedded structure provides the best results but at the cost of a relatively high model complexity. The local architecture provides satisfying results and at the same time is relatively simple.																	2083-2567	2449-6499				APR	2020	10	2					75	94		10.2478/jaiscr-2020-0006													
J								A NEW AUTO ADAPTIVE FUZZY HYBRID PARTICLE SWARM OPTIMIZATION AND GENETIC ALGORITHM	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										hybrid methods; Particle Swarm Optimization; Genetic Algorithm; fuzzy systems; multimodal functions	PSO; GA	The social learning mechanism used in the Particle Swarm Optimization algorithm allows this method to converge quickly. However, it can lead to catching the swarm in the local optimum. The solution to this issue may be the use of genetic operators whose random nature allows them to leave this point. The degree of use of these operators can be controlled using a neuro-fuzzy system. Previous studies have shown that the form of fuzzy rules should be adapted to the fitness landscape of the problem. This may suggest that in the case of complex optimization problems, the use of different systems at different stages of the algorithm will allow to achieve better results. In this paper, we introduce an auto adaptation mechanism that allows to change the form of fuzzy rules when solving the optimization problem. The proposed mechanism has been tested on benchmark functions widely adapted in the literature. The results verify the effectiveness and efficiency of this solution.																	2083-2567	2449-6499				APR	2020	10	2					95	111		10.2478/jaiscr-2020-0007													
J								FAST IMAGE INDEX FOR DATABASE MANAGEMENT ENGINES	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										image descriptors; content-based image retrieval; image indexing	RETRIEVAL; FEATURES; CLASSIFICATION; DESCRIPTOR	Large-scale image repositories are challenging to perform queries based on the content of the images. The paper proposes a novel, nested-dictionary data structure for indexing image local features. The method transforms image local feature vectors into two-level hashes and builds an index of the content of the images in the database. The algorithm can be used in database management systems. We implemented it with an example image descriptor and deployed in a relational database. We performed the experiments on two image large benchmark datasets.																	2083-2567	2449-6499				APR	2020	10	2					113	123		10.2478/jaiscr-2020-0008													
J								A NEW APPROACH TO DETECTION OF CHANGES IN MULTIDIMENSIONAL PATTERNS	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										edge detection; regression; nonparametric estimation	EDGE DETECTOR; IDENTIFICATION; CONVERGENCE; REGRESSION	Nowadays, unprecedented amounts of heterogeneous data collections are stored, processed and transmitted via the Internet. In data analysis one of the most important problems is to verify whether data observed or/and collected in time are genuine and stationary, i.e. the information sources did not change their characteristics. There is a variety of data types: texts, images, audio or video files or streams, metadata descriptions, thereby ordinary numbers. All of them changes in many ways. If the change happens the next question is what is the essence of this change and when and where the change has occurred. The main focus of this paper is detection of change and classification of its type. Many algorithms have been proposed to detect abnormalities and deviations in the data. In this paper we propose a new approach for abrupt changes detection based on the Parzen kernel estimation of the partial derivatives of the multivariate regression functions in presence of probabilistic noise. The proposed change detection algorithm is applied to one- and two-dimensional patterns to detect the abrupt changes.																	2083-2567	2449-6499				APR	2020	10	2					125	136		10.2478/jaiscr-2020-0009													
J								A PRACTICAL STATISTICAL APPROACH TO THE RECONSTRUCTION PROBLEM USING A SINGLE SLICE REBINNING METHOD	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										reconstruction algorithm; statistical iterative method; computed tomography	IMAGE-RECONSTRUCTION; ALGORITHMS	The paper presented here describes a new practical approach to the reconstruction problem applied to 3D spiral x-ray tomography. The concept we propose is based on a continuous-to-continuous data model, and the reconstruction problem is formulated as a shift invariant system. This original reconstruction method is formulated taking into consideration the statistical properties of signals obtained by the 3D geometry of a CT scanner. It belongs to the class of nutating reconstruction methods and is based on the advanced single slice rebinning (ASSR) methodology. The concept shown here significantly improves the quality of the images obtained after reconstruction and decreases the complexity of the reconstruction problem in comparison with other approaches. Computer simulations have been performed, which prove that the reconstruction algorithm described here does indeed significantly outperforms conventional analytical methods in the quality of the images obtained.																	2083-2567	2449-6499				APR	2020	10	2					137	149		10.2478/jaiscr-2020-0010													
J								Real-time monitoring of high-power disk laser welding statuses based on deep learning framework	JOURNAL OF INTELLIGENT MANUFACTURING										Features fusion; Deep learning; Genetic algorithm; Stacked sparse autoencoder; Multiple-sensor signals	NEURAL-NETWORKS; GENETIC ALGORITHM; FEATURE-SELECTION; VAPOR PLUME; KEYHOLE; DYNAMICS; CLASSIFICATION; PREDICTION; DIAGNOSIS; STEEL	The laser welding quality is determined by its welding statuses, and online welding statuses are depicted by the real-time signals captured from the welding process. A multiple-sensor system is designed to obtain information as comprehensive as possible for welding statuses monitoring. The multiple-sensor system includes an auxiliary illumination visual sensor system, an ultraviolet and visible band visual sensor system, a spectrometer and two photodiodes. The signals captured by different sensors are analyzed via signal or digital image processing algorithms, and distinct features are extracted from these signals to depict the online welding statuses. A deep learning framework based on stacked sparse autoencoder (SSAE) is established to model the relationship between the multi-sensor features and their corresponding welding statuses, and Genetic algorithm (GA) is applied to optimize the parameters of the SSAE framework (SSAE-GA). The proposed framework achieves higher accuracy and stronger robustness in monitoring welding status by comparing with the backpropagation neural network, support vector machine and random forest. Three new experiments with different welding parameters are implemented to validate the effectiveness and generalization of our proposed method. This study provides a novel and accurate method for high-power disk laser welding status monitoring.																	0956-5515	1572-8145				APR	2020	31	4					799	814		10.1007/s10845-019-01477-w													
J								Artificial intelligence planners for multi-head path planning of SwarmItFIX agents	JOURNAL OF INTELLIGENT MANUFACTURING										SwarmItFIX; Robot fixtureless assembly; Multi-head path planning; Policy iteration; Value iteration; Monte Carlo control	SYSTEM; DESIGN	Sheet metal manufacturing is finding wide applications in automotive and aerospace industries. Handling of giant sheet materials in manufacturing industries is one of the key problems. Utilization of robots, viz SwarmItFIX, will address this problem and automate the fixturing process, which greatly reduces lead time and thus the production cost. Implementation of intelligence into the robots will further improve efficiency in handling and reduce manufacturing inaccuracies. In this work, two different novel planners are proposed which do path planning for the heads of the SwarmItFIX agents. The environment of the problem is modeled as a Markov Decision Problem. The first planner uses the Value Iteration and Policy Iteration (PI) algorithms individually and the second planner performs the Monte Carlo control reinforcement learning. Finally, when the simulation is done and parameters of the proposed three algorithms along with existing Constraint Satisfaction Problem algorithm are compared with each other. It is observed that the proposed PI algorithm returns the plan much faster than the other algorithms. In the near future, the efficient planning model will be tested and implemented into the SwarmItFIX setup at the PMAR laboratory, University of Genoa, Italy.																	0956-5515	1572-8145				APR	2020	31	4					815	832		10.1007/s10845-019-01479-8													
J								A data-driven decision-making optimization approach for inconsistent lithium-ion cell screening	JOURNAL OF INTELLIGENT MANUFACTURING										Multi-source data fusion; Imbalanced learning; Convolutional auto-encoder; Generative adversarial networks; Inconsistent lithium-ion cell screening	BATTERY PACK; SMOTE	Because the data generated in the complex industrial manufacturing processes is multi-sourced and heterogeneous, it brings a challenge for addressing decision-making optimization problems embedded in the whole manufacturing processes. Especially, for inconsistent lithium-ion cell screening as such a special problem, it is a tough issue to fuse data from multiple sources in a lithium-ion cell manufacturing process to screen cells for relieving the inconsistency among cells in a battery pack with multiple cells configured in series, parallel, and series-parallel. This paper proposes a data-driven decision-making optimization approach (DDDMO) for inconsistent lithium-ion cell screening, which takes into account three dynamic characteristic curves of cells, thus ensuring that the screened cells have consistent electrochemical characteristics. The DDDMO method uses the convolutional auto-encoder to extract features from different characteristics curves of lithium-ion cells through multi-channels and then the features in different channels are combined into fusion features to build a feature base. It also proposes an effective sample generation approach for imbalanced learning using the conditional generative adversarial networks to enhance the feature base, thereby efficiently training a classifier for inconsistent lithium-ion cell screening. Finally, industrial applications verify the effectiveness of the proposed approach. The results show that the missing rate of inconsistent lithium-ion cells drops by an average of 93.74% compared to the screening performance in the single dynamic characteristic of cells, and the DDDMO approach has greater accuracy for screening cells at lower time costs than the existing methods.																	0956-5515	1572-8145				APR	2020	31	4					833	845		10.1007/s10845-019-01480-1													
J								Machine learning technique for data-driven fault detection of nonlinear processes	JOURNAL OF INTELLIGENT MANUFACTURING										Machine learning; RKPLS; MW-RKPLS; Nonlinear dynamic process; Fault detection; Tabu search	PARTIAL LEAST-SQUARES; PRINCIPAL COMPONENT ANALYSIS; MOVING-WINDOW; REDUCED COMPLEXITY; LATENT STRUCTURES; KERNEL; ALGORITHM; DIAGNOSIS; PROJECTION; GRAPH	This paper proposes a new machine learning method for fault detection using a reduced kernel partial least squares (RKPLS), in static and online forms, for handling nonlinear dynamic systems. The choice of the fault detection method has a vital role to improve efficiency and safety as well as production. The kernel partial least squares is a nonlinear extension of partial least squares. The present method has been mostly used as a monitoring method for nonlinear processes. Thus, the standard method cannot perform properly and quickly when the training data set is large. The main contributions of the suggested approach are: the approximation of the components retained by the standard method and the reduction in the computation time as well as the false alarm rate. Using the reduced principal, the online suggested method is presented for fault detection of nonlinear dynamic processes. The online reduced method is developed to monitor the dynamic process online and update the reduced reference model. For this reason, the moving window RKPLS is proposed. The general principle is to check if the new useful observation satisfies, in the feature space, the condition of independencies between variables. Thereafter, the relevance of the suggested methods is used to monitor the chemical stirred tank reactor benchmark process, the air quality and the tennessee eastman process. The simulation results of the suggested methods are compared to the standard one.																	0956-5515	1572-8145				APR	2020	31	4					865	884		10.1007/s10845-019-01483-y													
J								Image-based defect detection in lithium-ion battery electrode using convolutional neural networks	JOURNAL OF INTELLIGENT MANUFACTURING										Computer vision; Microstructure; Deep learning; Convolutional neural networks; Lithium-ion battery; Electrode defects; Quality evaluation	INTENSIVE DRY; MICROSTRUCTURE; CLASSIFICATION	During the manufacturing of lithium-ion battery electrodes, it is difficult to prevent certain types of defects, which affect the overall battery performance and lifespan. Deep learning computer vision methods were used to evaluate the quality of lithium-ion battery electrode for automated detection of microstructural defects from light microscopy images of the sectioned cells. The results demonstrate that deep learning models are able to learn accurate representations of the microstructure images well enough to distinguish instances with defects from those without defect. Furthermore, the benefits of using pretrained networks for microstructure classification were also demonstrated, achieving the highest classification accuracies. This method provides an approach to analyse thousands of Li-ion battery micrographs for quality assessment in a very short time and it can also be combined with other common battery characterization methods for further technical analysis.																	0956-5515	1572-8145				APR	2020	31	4					885	897		10.1007/s10845-019-01484-x													
J								A zero-shot learning method for fault diagnosis under unknown working loads	JOURNAL OF INTELLIGENT MANUFACTURING										Fault diagnosis; Zero-shot learning; Autoencoder; Unknown working load	FEATURE-EXTRACTION; TRANSFORM	Data-based fault diagnosis is an important technology in modern manufacturing systems. However, most of these diagnosis methods assume that all the data should be identically distributed. In diagnosis tasks, this assumption means that these methods can only handle faults from the same working load. In real-world applications, the working load of the equipment varies for the different productions; if an unknown working load with no prior data available is given, then these traditional methods may be invalid. Zero-shot learning, using known data to diagnose the fault under unknown working loads, provides a transfer approach to solve this problem. In this paper, a zero-shot learning method based on contractive stacked autoencoders is proposed. The proposed method is only trained by the data from the known working load and can diagnose the fault from unknown but related working loads without prior data. The experimental results on the Case Western Reserve University dataset indicate that the proposed method performs better than the traditional methods under unknown working loads and has an accuracy of 97.82%. In addition, the analysis of the singular value and feature space also suggests that the proposed method is more robust and the feature representation is more contractive.																	0956-5515	1572-8145				APR	2020	31	4					899	909		10.1007/s10845-019-01485-w													
J								Non-dominated sorting modified teaching-learning-based optimization for multi-objective machining of polytetrafluoroethylene (PTFE)	JOURNAL OF INTELLIGENT MANUFACTURING										Design of experiments; Multi-response; Non-dominated sorting modified teaching-learning-based optimization; Response surface model; Surface roughness	EVOLUTIONARY ALGORITHM; DESIGN OPTIMIZATION; GENETIC ALGORITHM; RSM; PARAMETERS	A non-dominated sorting modified teaching-learning-based optimization (NSMTLBO) is proposed to obtain the optimum solution for a multi-objective problem related to machining Polytetrafluoroethylene. Firstly, an experimental design is done and the L27 orthogonal array with three-level of cutting speed mml:mfenced close=")" open="("Vc$$ \left( {V_{c} } \right) $$\end{document}, feed rate (f), depth of cut (ap) and nose radius mml:mfenced close=")" open="("Nr$$ \left( {N_{r} } \right) $$\end{document} is formulated. A CNC turning machine is used to perform experiments with cemented carbide tool at an insert angle of 80 degrees and the response variables known as surface finish and material removal rate are measured. A response surface model is rendered from the experimental results to derive the minimization function of surface roughness mml:mfenced close=")" open="("Ra$$ \left( {R_{a} } \right) $$\end{document} and maximization function of material removal rate (MRR). Both optimization functions are solved simultaneously using NSMTLBO. A fuzzy decision maker is also integrated with NSMTLBO to determine the preferred optimum machining parameters from Pareto-front based on the relative importance level of each objective function. The best responses R-a = 2.2347 mu m and MRR = 96.835 cm(3)/min are predicted at the optimum machining parameters of V-c = 160 mm/min, f = 0.5 mm/rev, ap = 0.98 mm and N-r = 0.8 mm. The proposed NSMTLBO is reported to outperform other six peer algorithms due to its excellent capability in generating the Pareto-fronts which are more uniformly distributed and resulted higher percentage of non-dominated solutions. Furthermore, the prediction results of NSMTLBO are validated experimentally and it is reported that the performance deviations between the predicted and actual results are lower than 3.7%, implying the applicability of proposed work in real-world machining applications.																	0956-5515	1572-8145				APR	2020	31	4					911	935		10.1007/s10845-019-01486-9													
J								Intelligent pulse analysis of high-speed electrical discharge machining using different RNNs	JOURNAL OF INTELLIGENT MANUFACTURING										High-speed EDM; Recurrent neural network; Pulse analysis; Intelligent classification method	ARTIFICIAL NEURAL-NETWORKS; DISCRIMINATING SYSTEM; SURFACE-ROUGHNESS; TITANIUM-ALLOY; OPTIMIZATION; PERFORMANCE; ENERGY; ARCS	High-speed electrical discharge machining (EDM) is a nontraditional machining method using high electric energy to efficiently remove materials. In this paper, a novel pulse classification method was proposed based on the recurrent neural network (RNN) for high-speed EDM pulse analysis. This study is the first time that an RNN has been applied in high-speed EDM pulse analysis. Different from traditional EDM, discharge pulses of high-speed EDM were classified into five types during the machining process: open, spark, arc, partially short and short. Models based on three different RNNs including the traditional RNN, LSTM (long short-term memory) and IndRNN (independently recurrent neural network) with different activation functions were built to analyze the discharge pulses in the research. A new input data structure based on the minimum signal change period was proposed in the classification method to simplify the model structure and improve accuracy at the same time. Without setting thresholds, the highest classification accuracy of the proposed model is up to 97.85%, which can simultaneously classify discharge pulses based on 10,000 orders of magnitude including various current values. The proposed method was effectively adapted to the complicated machining conditions and the compound power source of the high-speed EDM. The optimal model was used to analyze the distribution of discharge pulses during the machining process under different currents, fluxes and feeding speeds. The proportion of the discharge pulses was clearly predicted. Through analyzing the discharge pulses of long machining time, the regulation of discharge under different machining parameters was revealed more reliably, providing valuable information for the improvement of high-speed EDM servo systems.																	0956-5515	1572-8145				APR	2020	31	4					937	951		10.1007/s10845-019-01487-8													
J								Tool wear predicting based on multi-domain feature fusion by deep convolutional neural network in milling operations	JOURNAL OF INTELLIGENT MANUFACTURING										Tool wear predicting; Multi-domain; Feature fusion; Convolutional neural network; Milling	FAULT-DIAGNOSIS; CUTTING FORCE; MACHINE	Tool wear monitoring has been increasingly important in intelligent manufacturing to increase machining efficiency. Multi-domain features can effectively characterize tool wear condition, but manual feature fusion lowers monitoring efficiency and hinders the further improvement of predicting accuracy. In order to overcome these deficiencies, a new tool wear predicting method based on multi-domain feature fusion by deep convolutional neural network (DCNN) is proposed in this paper. In this method, multi-domain (including time-domain, frequency domain and time-frequency domain) features are respectively extracted from multisensory signals (e.g. three-dimensional cutting force and vibration) as health indictors of tool wear condition, then the relationship between these features and real-time tool wear is directly established based on the designed DCNN model to combine adaptive feature fusion with automatic continuous prediction. The performance of the proposed tool wear predicting method is experimentally validated by using three tool run-to-failure datasets measured from three-flute ball nose tungsten carbide cutter of high-speed CNC machine under dry milling operations. The experimental results show that the predicting accuracy of the proposed method is significantly higher than other advanced methods.																	0956-5515	1572-8145				APR	2020	31	4					953	966		10.1007/s10845-019-01488-7													
J								An effective and automatic approach for parameters optimization of complex end milling process based on virtual machining	JOURNAL OF INTELLIGENT MANUFACTURING										Parameters optimization; End milling; Virtual machining; System integration	PARTICLE SWARM OPTIMIZATION; SIMULATION; ALGORITHM; SELECTION; SYSTEM	The demand for optimization of manufacturing processes rises as a reflection of the highly competitive market environment that requires shorter lead time and lower production costs. Although some approaches to milling process optimization have been developed based on analytical model using average cutting parameters, they are not available for complex workpieces when cutting parameters are time-varying and instantaneous cutting conditions need to be considered. In order to automate the optimization process and avoid costly machining tests, in this paper, an effective approach for parameters optimization of complex end milling process based on virtual machining is proposed. A computer-aided design (CAD)/computer-aided manufacturing (CAM) application is integrated for actual tool path generation and feedrate scheduling based on material removal rate. Then, a machining simulator based on octree and instantaneous force model is developed to evaluate feasibility of the given numerical control (NC) program, and the correctness of this simulator is verified by machining tests. The optimization process is controlled by the efficient global optimization method to find global optimal solution with fewer simulations and less computation time. During each iteration of the optimization process, NC programs are generated and evaluated automatically by the CAD/CAM application and the simulator, respectively. The effectiveness and efficiency of the proposed approach are proved by comparing the generated optimal solution (has reduced machining time and production cost) with the recommended cutting parameters from machining experts when machining an impeller.																	0956-5515	1572-8145				APR	2020	31	4					967	984		10.1007/s10845-019-01489-6													
J								Mixed-layer adaptive slicing for robotic Additive Manufacturing (AM) based on decomposing and regrouping	JOURNAL OF INTELLIGENT MANUFACTURING										Mixed-layer adaptive slicing; Robotic AM; Planar slicing; Non-planar slicing; Decomposing and regrouping	PATH GENERATION; DEPOSITION; DESIGN	AM, generally known as 3D printing, is a promising technology. Robotic AM enables the direct fabrication of products possessing complex geometry and high performance without extra support structures. Process planning of slicing and tool path generation has been a challenging issue due to geometric complexity, material property, etc. Simple and robust planar slicing has been widely researched and applied. However, support structures usually result in time-consuming and cost-expensive. Notwithstanding multi-direction slicing and non-planar slicing (curved layer slicing) have been proposed respectively to decrease support structures, capture some minute but critical features and improve the surface quality and part strength. There is no slicing method aiming at features of part's sub-volumes. A comprehensive literature review is given first to illustrate the problems and features of available slicing methods better. Then, in order to combine the merits of planar and non-planar slicing to realize intelligent manufacturing further, this paper reports the concept and implementation of a mixed-layer adaptive slicing method for robotic AM. Different from applying planar slicing in any cases or adopting the decomposing and regrouping based multi-direction planar slicing for finding the optimal slicing directions, the proposed method mainly focuses on how to apply planar and non-planar slicing for each sub-volume according to the geometrical features. Additionally, the requirements for robotic AM equipment in possessing multi-mode of printing and slicing are investigated.																	0956-5515	1572-8145				APR	2020	31	4					985	1002		10.1007/s10845-019-01490-z													
J								Shop floor data-driven spatial-temporal verification for manual assembly planning	JOURNAL OF INTELLIGENT MANUFACTURING										Spatial-temporal verification; Manual assembly planning; Data-driven; Shop floor; Wearable system	MOTION CAPTURE; ERGONOMIC ANALYSIS; SIMULATION; OPERATION; SYSTEM; TIME	Motivated by the increasing demand and highly customized products, accurate and up-to-date information about the manufacturing process become essential to meet these requirements. In manual assembly activities, performing theoretical planning in simulation environments is a crucial procedure to detect and avoid unreasonable assembly operations. However, the deviations between theoretical and actual assembly actions would result in the failure of the manual assembly planning. Therefore, the verification for the manual assembly planning is significant to ensure the correctness of the actual assembly operations, performing a convergence between the cyber and physical world. The challenges involved in retrieving and utilizing the actual data about the manual activities on a shop floor. In this paper, a self-contained wearable tracking system is proposed and applied to collect the shop-floor data during the manual assembly operations. And then, an unsupervised classification method is applied to empower semantic knowledge to the shop-floor data derived from the workplace. Thus, an automatic spatial-temporal verification for manual assembly planning is carried out, providing indicators to optimize the current manual assembly planning. Experimental results illustrate that the proposed work can perform the spatial-temporal verification for manual assembly task and indicate evidence to improve the manual assembly planning objectively.																	0956-5515	1572-8145				APR	2020	31	4					1003	1018		10.1007/s10845-019-01491-y													
J								On mining frequent chronicles for machine failure prediction	JOURNAL OF INTELLIGENT MANUFACTURING										Chronicle mining; Predictive maintenance; Industry 4; 0	SEQUENTIAL PATTERNS; DISCOVERY	In industry 4.0, machines generate a lot of data about several kinds of events that occur in the production process. This huge quantity of information contains valuable patterns that allow prediction of important events in the appropriate instant. In this paper, we are interested in mining frequent chronicles in the context of industrial data. We introduce a general approach to preprocess, mine, and use frequent chronicles to predict a special event; the failure of a machine. Our approach aims not only to predict the failure, but also the time of its appearance. Our approach is validated through a set of experiments performed on the chronicle mining phase as well as the prediction phase. Experiments were achieved on synthetic data in addition to a real industrial data set.																	0956-5515	1572-8145				APR	2020	31	4					1019	1035		10.1007/s10845-019-01492-x													
J								Even faster retinal vessel segmentation via accelerated singular value decomposition	NEURAL COMPUTING & APPLICATIONS										Medical image processing; Machine learning; Segmentation	OPPORTUNISTIC SPECTRUM ACCESS; ATTRIBUTE-BASED ENCRYPTION; LOCATING BLOOD-VESSELS; MATHEMATICAL MORPHOLOGY; PERFORMANCE ANALYSIS; CROWD EVACUATION; NETWORK; IMAGES; ALGORITHM; TRACKING	Retinal blood vessel segmentation plays a vital role in medical image analysis since the appearance of vessels would contribute in the diagnosis, treatment, and evaluation for various diseases in ophthalmology and other fields, such as cardiology and neurosurgery. Among the state-of-the-art blood vessel segmentation techniques, the Hessian-based multi-scale filter has been widely used and shown its superior performance in the accuracy and visual effect. However, its execution time still remains a challenge due to the employment of eigenvalue decomposition in this approach. Bearing this in mind, we propose an accelerated matrix decomposition mechanism, which could be used to boost not only the original Hessian-based multi-scale approach but also the singular value decomposition-based algorithms. To evaluate the proposed method, we conducted comparison experiments between state-of-the-art techniques and our method. Experimental results show the superior performance of the proposed approach over state of the arts especially in execution time.																	0941-0643	1433-3058				APR	2020	32	7			SI		1893	1902		10.1007/s00521-019-04505-1													
J								Traffic identification and traffic analysis based on support vector machine	NEURAL COMPUTING & APPLICATIONS										Support vector machine; Traffic classification; Feature extraction; Kernel function	CLASSIFICATION; DPI; SVM	As the number of applications based on the Internet is increasing, the traffic becomes more and more complex. So how to improve the service quality and security of the network is becoming more and more important. This paper studies the application of support vector machine in traffic identification to classify the network traffic. Through data collection and feature generation methods and network traffic feature screening methods, support vector machine is used as a classifier by using the generalization capability of support vector machine, and the parameters and kernel functions of the support vector machine are adjusted and selected based on cross-comparison ideas and methods. Using the cross-validation method to make the most reasonable statistics for the classification and recognition accuracy of the adjusted support vector machine avoids the situation that the classification accuracy of the support vector machine is unstable or the statistics are inaccurate. Finally, a traffic classification and identification system based on support vector machine is realized. The final recognition rate of encrypted traffic is up to 99.31%, which overcomes the disadvantages of traditional traffic identification and achieves a fairly reliable accuracy.																	0941-0643	1433-3058				APR	2020	32	7			SI		1903	1911		10.1007/s00521-019-04493-2													
J								A joint deep neural networks-based method for single nighttime rainy image enhancement	NEURAL COMPUTING & APPLICATIONS										Low-illumination rainy image; Image decomposition; Image enhancement; Rain removal; Deep neural network	COLOR TRANSFER; RETINEX; REMOVAL; BRIGHTNESS	In rainy conditions, especially at night with low illumination, the visual of images obtained by outdoor computer vision systems is degraded significantly, leading to a significant negative effect on the work of the outdoor computer vision system. In this paper, we develop a new rainy image model to describe rain scenes at night with low illumination. From this model, we propose a joint deep neural network-based method for single nighttime rainy image enhancement. First, a decom-net based on Retinex theory is employed for image decomposition, and the purpose of this sub-net is to extract the reflection image and the illumination image from the input image. Then, an enhancement net is proposed for illumination adjustment. The goal of this sub-net is to remove the negative effect (low visual) caused by low illumination. Finally, a symmetric sub-net termed multi-stream network-based contextual autoencoder is developed, where rain features are directly learned from the enhanced nighttime rainy images in a recurrent way. The goal of this sub-net is to effectively remove rain streaks from the illumination-enhanced image. The experimental results show the advantage and effectiveness of the proposed method, and evident improvements over existing state-of-the-art methods are obtained with the proposed method.																	0941-0643	1433-3058				APR	2020	32	7			SI		1913	1926		10.1007/s00521-019-04501-5													
J								Classification of Alzheimer's disease based on brain MRI and machine learning	NEURAL COMPUTING & APPLICATIONS										Machine learning; Support vector machine; Brain MRI; Alzheimer's disease	DIAGNOSTIC-CRITERIA; RISK; DEMENTIA; MEMORY; AD	Alzheimer's disease (AD) is one of the most common diseases in the world. It is a neurodegenerative disease that can cause cognitive impairment and memory deterioration. In recent years, the number of the elderly population is increasing, and the incidence of elderly diseases has increased significantly. The most representative of these diseases is Alzheimer's disease. According to some data, the average survival time of Alzheimer's disease patients is only 5.5 years, which is the "fourth killer" that endangers the health of the elderly after cardiovascular diseases, cerebrovascular diseases and cancer. According to conservative estimates of the International Federation of Alzheimer's Diseases, the number of Alzheimer's disease patients worldwide will increase to 75.62 million by 2030; by 2050, the number of patients will reach 135.46 million. Therefore, it is urgent to classify the course of Alzheimer's disease. In this paper, support vector machine (SVM) model method is used to classify and predict different disease processes of Alzheimer's disease based on structural brain magnetic resonance imaging (MRI) imaging data, so as to help the auxiliary diagnosis of the disease. In this paper, the extracted MRI data and the SVM model are combined to obtain more accurate classification prediction results. The accuracy of classification and prediction is the best. According to the predicted results, the data characteristics related to diseases can be determined, which can provide a basis for clinical and basic research, etiology and pathological changes.																	0941-0643	1433-3058				APR	2020	32	7			SI		1927	1936		10.1007/s00521-019-04495-0													
J								Recognition and prediction of ground vibration signal based on machine learning algorithm	NEURAL COMPUTING & APPLICATIONS										Ground motion signal; Wavelet packet transform; Support vector machine; Genetic algorithm; Target recognition	ARRIVAL-TIME DETECTION; DISCRIMINATION; CLASSIFICATION; INTERPOLATION	Accurate recognition of the type of ground motion is a basic task in the field of seismic engineering. In this paper, the key technologies of detection and recognition of underground seismic signal are studied. The target recognition algorithm is designed to realize the target recognition through denoising the collected target signal and extracting the characteristic quantity. Considering that ground motion signals generated by moving targets on the ground are susceptible to environmental noise, this paper introduces the working principle of wavelet packet denoising and its support vector machine classification model. Wavelet packet was used to transform the signal to denoise first, then zero-crossing rate analysis of the denoised signal was carried out after wavelet packet denoising and extracts the parameters, and the energy of cross-correlation criteria was selected finally. Quantitative indices are combined to construct multi-feature vectors, which are used as input of multi-class support vector machine for training and prediction. In this model, the optimal parameters of support vector machine model are found by genetic algorithm parameter optimization. The experimental results show that the improved model can recognize and classify the ground motion signals caused by people and vehicles correctly and can improve the performance of the classifier.																	0941-0643	1433-3058				APR	2020	32	7			SI		1937	1947		10.1007/s00521-019-04496-z													
J								Image object detection and semantic segmentation based on convolutional neural network	NEURAL COMPUTING & APPLICATIONS										CNN; AdaBoost; Image object detection; Semantic segmentation	CNN	In this paper, an unsupervised co-segmentation algorithm is proposed, which can be applied to the image with multiple foreground objects simultaneously and the background changes dramatically. The color edge image in RGB space is extracted for semantic extraction. This method can effectively distinguish foreground and background by recursively modeling the appearance distribution of pixels and regions. The coherence of image foreground and background model is enhanced by using the correlation between different image regions and image interior. Experimental results show that deep convolutional neural network can effectively realize semantic classification of scene images by end-to-end feature learning and achieve accurate semantic segmentation of scene images.																	0941-0643	1433-3058				APR	2020	32	7			SI		1949	1958		10.1007/s00521-019-04491-4													
J								Research on radar signal recognition based on automatic machine learning	NEURAL COMPUTING & APPLICATIONS										Radar signal recognition; Automatic machine learning; AUTO-SKLEARN algorithm; Model integration	PERFORMANCE EVALUATION; PREDICTION; TREE	With the advancement of machine learning and radar technology, machine learning is becoming more and more widely used in the field of radar. Radar scanning, signal acquisition and processing, one-dimensional range image, radar SAR, ISAR image recognition, radar tracking and guidance are all integrated into machine learning technology, but machine learning technology relies heavily on human machine learning experts for radar signal recognition. In order to realize the automation of radar signal recognition by machine learning, this paper proposes an automatic machine learning AUTO-SKLEARN system and applies it to radar radiation source signals. Identification: Firstly, this paper briefly introduces the classification of traditional machine learning algorithms and the types of algorithms specifically included in each type of algorithm. On this basis, the machine learning Bayesian algorithm is introduced. Secondly, the automatic machine learning AUTO based on Bayesian algorithm is proposed. -SKLEARN system, elaborates the process of AUTO-SKLEARN system in solving automatic selection algorithm and hyperparameter optimization, including meta-learning and its program implementation and automatic model integration construction. Finally, this paper introduces the process of automatic machine learning applied to radar emitter signal recognition. Through data simulation and experiment, the effect of traditional machine learning k-means algorithm and automatic machine learning AUTO-SKLEARN system in radar signal recognition is compared, which shows that automatic machine learning is feasible for radar signal recognition. The automatic machine learning AUTO-SKLEARN system can significantly improve the accuracy of the radar emitter signal recognition process, and the scheme is more reliable in signal recognition stability.																	0941-0643	1433-3058				APR	2020	32	7			SI		1959	1969		10.1007/s00521-019-04494-1													
J								Research on orthopedic auxiliary classification and prediction model based on XGBoost algorithm	NEURAL COMPUTING & APPLICATIONS										Orthopedic diseases; Assisted diagnosis; Classification prediction; XGBoost algorithm		In the big data environment, hospital medical data are also becoming more complex and diversified. The traditional method of manually processing data has not been able to meet the management needs of massive medical data. With the further development of big data technology and machine learning, the smart medical aided diagnosis model came into being. However, there is almost no auxiliary diagnosis mode for orthopedic diseases. In order to make up for the gap in the auxiliary diagnosis of orthopedics and promote the wisdom process of orthopedic disease diagnosis, this paper proposes an orthopedic auxiliary classification prediction model based on XGBoost algorithm. The experimental data were obtained from the clinical case information of femoral neck patients from April 2016 to October 2018, Department of Bone and Soft Tissue Tumor Surgery, Cancer Hospital of China Medical University, Liaoning Cancer Hospital & Institute. In order to make the experimental results more convincing, while constructing the XGBoost model, the orthopedic auxiliary classification prediction model is constructed based on the random forest algorithm and the associated classification algorithm, respectively, and the three models are compared and analyzed. The results show that compared with the random forest model and the associated classification model, the XGBoost algorithm classification prediction model has higher accuracy, faster calculation speed, and more applicability in orthopedic clinical data. The XGBoost algorithm can cope with complex and diverse medical data, and can better meet the requirements of timeliness and accuracy of auxiliary diagnosis. The classification and prediction model of orthopedic auxiliary diagnosis proposed in this paper helps to reduce the workload of medical workers, help patients prevent and recover early, and realize real auxiliary medical services.																	0941-0643	1433-3058				APR	2020	32	7			SI		1971	1979		10.1007/s00521-019-04378-4													
J								Genetic algorithm combined with BP neural network in hospital drug inventory management system	NEURAL COMPUTING & APPLICATIONS										Genetic algorithm; BP neural network; Drug; Inventory management; Intelligent management	PREDICTION; MODEL	At present, there are problems such as low level of drug inventory management in major hospitals, which directly lead to the stagnation of the drug supply chain and the high cost of hospital management. In order to improve the efficiency of hospital drug inventory management, based on genetic algorithm and BP neural network, this study combined the actual situation of hospital drug inventory forecast to build a system model based on hospital drug management mode. At the same time, in order to verify the validity of the model, this paper collected experimental data through data collection, inputted data for model simulation analysis, and established a three-layer network simulation model. In addition, this paper analyzed the accuracy and sensitivity of model prediction based on actual conditions and compared the research results with actual data. Studies have shown that the model proposed in this study has certain effects and can be applied to hospital drug inventory management practices.																	0941-0643	1433-3058				APR	2020	32	7			SI		1981	1994		10.1007/s00521-019-04379-3													
J								A Q-learning-based approach for virtual network embedding in data center	NEURAL COMPUTING & APPLICATIONS										Virtual tenant network; Network virtualization; Virtual network embedding; Virtual data center; Q-learning algorithm	IDENTIFICATION; SCHEME	Virtual network embedding (VNE) refers to allocating reasonable substrate network resources for virtual network (VN) requests that include computing resources and network resources, so as to obtain optimal income from leasing virtual resources. Such a way of providing virtual resources is the key technology of cloud computing and can greatly save the operating cost of enterprises and provide flexibility of application deployment. However, the existing VNE algorithms are mostly oriented to traditional stochastic network topologies. Due to the high connectivity and server density of data centers and the complexity of the user's resource requirements, the traditional VNE algorithms suffer from low resource utilization rate and revenues in the VNE on the data centers. Different from the existing algorithms which are often based on heuristic algorithms, this paper proposes a VNE algorithm for data center topology based on the Q-learning algorithm which is a typical reinforcement learning method. The algorithm an agent for each VN designs a reward function related to the effect of virtual link embedding, which is used to update the Q-matrix through unsupervised learning process. Then, the agent can find the optimal embedding strategy based on the Q-table from each learning. Simulation results demonstrate that the proposed algorithm can improve the resource utilization ratio and obtain a better revenue/cost ratio of the substrate network compared with the traditional heuristic algorithms.																	0941-0643	1433-3058				APR	2020	32	7			SI		1995	2004		10.1007/s00521-019-04376-6													
J								A genetic algorithm for fuzzy random and low-carbon integrated forward/reverse logistics network design	NEURAL COMPUTING & APPLICATIONS										Integrated logistics network; Carbon cap-and-trade; Fuzzy random variable; Genetic algorithm	SUPPLY CHAIN NETWORK; REVERSE LOGISTICS; EMISSION REDUCTION; MODEL; GREEN; OPTIMIZATION; MANAGEMENT; DEMAND; SYSTEM; COORDINATION	Considering the influence of carbon emissions trading, the fuzzy stochastic programming model was established to cut back the total cost of carbon trading balance. Modeling this chain is carried out by accounting for carbon cap-and-trade considerations and total cost optimization. In this paper, we analyze the low-carbon integrated forward/reverse logistics network and made relevant simulation tests. The results show that the changes of the confidence level and carbon emission limits have obvious influences on logistics costs. If the emission limit is large, carbon trading mechanism has little effect on the total logistics cost in the same scenario. Therefore, the government needs to use the appropriate emission limits to guide enterprises to reduce carbon emissions, and enterprises can make coping strategies according to the different limit at the same time. Therefore, the fuzzy random programming model proposed in this paper is practical. Its decision making applying the proposed algorithm is reasonable and applicable and could provide decision basis for enterprise managers.																	0941-0643	1433-3058				APR	2020	32	7			SI		2005	2025		10.1007/s00521-019-04340-4													
J								Deep belief network-based support vector regression method for traffic flow forecasting	NEURAL COMPUTING & APPLICATIONS										Machine learning; Deep belief network-support vector regression; Traffic flow prediction	PREDICTION	Instability is a common problem in deep belief network-back propagation forecasting model, and the trend of traffic data will affect the forecasting results of the model. Therefore, this paper proposes a short-term traffic flow forecasting method based on deep belief network-support vector regression. Support vector regression classifier SVR is used at the top of the model. Data processing is from bottom to top. Firstly, at the bottom of the model, the input traffic flow data are processed differently; then, the DBN model is used to learn the traffic flow characteristics. Finally, SVR is used to predict the traffic flow at the top of the model. The average absolute error of the prediction is 9.57%, and the average relative error is 5.91%. The relationship between the predicted value and the actual traffic flow data is found through simulation experiments. The predicted value of the model proposed in this paper is in good agreement with the measured value, and the prediction accuracy is high. The model can effectively predict short-term traffic flow. Finally, compared with the traditional DBN prediction model and other common prediction models, the proposed prediction model has higher prediction accuracy.																	0941-0643	1433-3058				APR	2020	32	7			SI		2027	2036		10.1007/s00521-019-04339-x													
J								Applications of deep convolutional neural networks in prospecting prediction based on two-dimensional geological big data	NEURAL COMPUTING & APPLICATIONS										Geological big data; Prospecting prediction; Convolutional neural networks; Songtao-Huayuan Mn deposit	MODELS	There are many challenges in the task of predicting ore deposits from big data repositories. The data are inherently complex and of great significance to the intervenient spatial relevance of deposits. The characteristics of the data make it difficult to use machine learning algorithms for the quantitative prediction of mineral resources. There are considerable interest and value in extracting spatial distribution characteristics from two-dimensional (2-d) ore-controlling factor layers under different metallogenic conditions. In this paper we undertake such analysis using a deep convolutional neural network algorithm named AlexNet. Training on the 2-d mineral prediction and classification model is performed using data from the Songtao-Huayuan sedimentary manganese deposit. It mines the coupling correlation between the spatial distribution of chemical elements, sedimentary facies, the outcrop of Datangpo Formation, faults, water system, and the areas where manganese ore bodies are present, as well as the correlation among different ore-controlling factors by employing the AlexNet networks. By comparing the training loss, training accuracy, verification accuracy, and recall of models trained by different scales of grids and different combinations of ore-controlling factor layers, we further discuss the most appropriate scale division and the optimal combination of ore-controlling factors to make the model achieve its strongest robustness. It is found that the prediction performance of AlexNet networks reaches its peak when selecting a grid division of 200 pixels x 200 pixels (the actual distance is 10 km x 10 km) and inputting the distribution layers of 21 chemical elements maps, lithofacies-paleogeographic map, formation and tectonic map, outcrop map of Datangpo Formation, and water system map. The training loss, training accuracy, verification accuracy, and recall of the optimal model are 0.0000001, 100.00%, 86.21%, and 91.67%, respectively. The proposed method is successfully applied to the 2-d metallogenic prediction in Songtao-Huayuan study area. And five metallogenic prospective areas from A to E are delineated with large probability for potential ore bodies.																	0941-0643	1433-3058				APR	2020	32	7			SI		2037	2053		10.1007/s00521-019-04341-3													
J								Robust nonlinear fractional order fuzzy PD plus fuzzy I controller applied to robotic manipulator	NEURAL COMPUTING & APPLICATIONS										FOFPD plus I; FPD plus I; NSGA-II; Robotic manipulator	MULTIOBJECTIVE GENETIC ALGORITHM; PID CONTROLLER; TRACKING CONTROL; DESIGN; OPTIMIZATION; SYSTEM; ARM	The aim of this article is to utilize fractional calculus for performance enhancement of nonlinear fuzzy PD + I controller. A fractional order fuzzy PD + I controller (FOFPD + I) is designed and implemented to control complex, uncertain and nonlinear robotic manipulator. FOFPD + I controller is derived from fractional order PD and fractional order I controller. The proposed control strategy has an adaptive capability due to its nonlinear gains and preserves the linear structure of fractional order PD + I controller. Further, integer-order fuzzy PD + I controller (FPD + I) and conventional PID controllers are also designed for comparative analysis. The optimum parameter values of FOFPD + I, FPD + I and PID controllers are obtained using non-dominated sorting genetic algorithm-II. The effectiveness of proposed controller is examined for reference tracking and disturbance rejection problems of robotic manipulator. The designed controllers are also validated experimentally on DC servomotor. Simulation and experimental results prove the superiority of FOFPD + I controller as compared to its integer-order equivalent and conventional PID controllers for control of robotic manipulator.																	0941-0643	1433-3058				APR	2020	32	7			SI		2055	2079		10.1007/s00521-019-04074-3													
J								Forward and reverse modelling of flow forming of solution annealed H30 aluminium tubes	NEURAL COMPUTING & APPLICATIONS										Flow-forming; H30 aluminium alloy; Springback; Ovality; Neural network; Genetic algorithm; L-BFGS-B algorithm	OPTIMIZATION; PARAMETERS; ROUGHNESS; ALGORITHM	Modelling of flow forming of tube-shaped solution annealed H30 Aluminium alloy is considered in the present study. Initially, a total of 136 experiments have been conducted to realize the process and subsequently influences of three inputs (feed-speed ratio, roller infeed and axial stagger) on the three outputs, viz. internal diameter, springback and ovality have been studied. Three neural network-based approaches (back-propagation neural network, limited-memory BFGS network and genetic neural system) have been developed for forward as well as reverse modelling of the process. During forward modelling, the performances of the three neural network-based approaches have been compared with the regression model. It is seen that GANN has performed much better compared to the other methods. Percentage accuracy in predicting ovality using regression analysis is the worst, and it necessitates consideration of more input process parameters for better prediction accuracy. However, NN-based approaches adapted such cases well. Comparison of all the three NN-based approaches among themselves has been made during reverse modelling. During this process, prediction accuracy, using LBFGSNN, is found to be better than the other two methods. Thus, it is perceived that NN-based models might suit better for prediction of shape accuracy of flow-formed shell.																	0941-0643	1433-3058				APR	2020	32	7			SI		2081	2093		10.1007/s00521-018-3749-x													
J								Binary whale optimization algorithm and its application to unit commitment problem	NEURAL COMPUTING & APPLICATIONS										Whale optimization; Binary whale optimization; Metaheuristics; Benchmark functions; Economic load dispatch; Unit commitment	GENETIC ALGORITHM; FIREFLY ALGORITHM	Whale optimization algorithm is a novel metaheuristic algorithm that imitates the social behavior of humpback whales. In this algorithm, the bubble-net hunting strategy of humpback whales is exploited. However, this algorithm, in its present form, is appropriate for continuous problems. To make it applicable to discrete problems, a binary version of this algorithm is being proposed in this paper. In the proposed approach, the solutions are binarized and sigmoidal transfer function is utilized to update the position of whales. The performance of the proposed algorithm is evaluated on 29 benchmark functions. Furthermore, unpaired t test is carried out to illustrate its statistical significance. The experimental results depict that the proposed algorithm outperforms others in respect of benchmark test functions. The proposed approach is applied on electrical engineering problem, a real-life application, named as "unit commitment". The proposed approach uses the priority list to handle spinning reserve constraints and search mechanism to handle minimum up/down time constraints. It is tested on standard IEEE systems consisting of 4, 10, 20, 40, 80, and 100 units and on IEEE 118-bus system and Taiwan 38-bus system as well. Experimental results reveal that the proposed approach is superior to other algorithms in terms of lower production cost.																	0941-0643	1433-3058				APR	2020	32	7			SI		2095	2123		10.1007/s00521-018-3796-3													
J								Wide area monitoring and protection of microgrid with DGs using modular artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Microgrid; mu PMUs; Fault detection; Fault classification; Fault section identification; MANN	SCHEME; WAVELET	The prominence of incorporating the renewable energy resources in a power system via microgrids has increased in the recent years, which impose a caution on conventional protection schemes. Protection schemes proposed earlier use local measurements, but fault classification for selective phase tripping using wide area measurements for microgrid has not been reported so far. This paper presents a wide area monitoring and protection of microgrid with distributed generations (DGs) using modular artificial neural networks (MANNs) for the fault detection and classification without affecting the relays in non-faulty or healthy sections of the microgrid. The distinct characteristics of the microgrid sort the proposed methodology into two stages. In stage 1, ANN 1 is developed to identify the operating mode of microgrid, whether it is operating in grid-connected mode (GCM) or islanded mode (IM). In stage 2, there are two MANNs corresponding to GCM and IM. Each MANNs consists of three separate ANNs for fault detection, classification, and section identification. A standard IEC 61850-7-420 microgrid with DGs (wind and photovoltaic) penetration is modeled in MATLAB/Simulink. The three-phase voltages and currents are measured with time synchronization considering the microphasor measurement units located at each bus. The extensive study includes different simulation scenarios such as shunt faults, high impedance fault, and dynamic situations like connection/disconnection of DGs/distribution lines. The results confirm the efficacy of the proposed methodology.																	0941-0643	1433-3058				APR	2020	32	7			SI		2125	2139		10.1007/s00521-018-3750-4													
J								Enhancing recommendation stability of collaborative filtering recommender system through bio-inspired clustering ensemble method	NEURAL COMPUTING & APPLICATIONS										Recommender system; Collaborative filtering; Clustering; Fuzzy; Swarm intelligence; Recommendation stability	FUZZY C-MEANS; PARTICLE SWARM OPTIMIZATION; OF-THE-ART; NEURAL-NETWORK; ALGORITHM; CLASSIFICATION; PERFORMANCE; FRAMEWORK	In recent years, internet technologies and its rapid growth have created a paradigm of digital services. In this new digital world, users suffer due to the information overload problem and the recommender systems are widely used as a decision support tool to address this issue. Though recommender systems are proven personalization tool available, the need for the improvement of its recommendation ability and efficiency is high. Among various recommendation generation mechanisms available, collaborative filtering-based approaches are widely utilized to produce similarity-based recommendations. To improve the recommendation generation process of collaborative filtering approaches, clustering techniques are incorporated for grouping users. Though many traditional clustering mechanisms are employed for the users clustering in the existing works, utilization of bio-inspired clustering techniques needs to be explored for the generation of optimal recommendations. This article presents a new bio-inspired clustering ensemble through aggregating swarm intelligence and fuzzy clustering models for user-based collaborative filtering. The presented recommendation approaches have been evaluated on the real-world large-scale datasets of Yelp and TripAdvisor for recommendation accuracy and stability through standard evaluation metrics. The obtained results illustrate the advantageous performance of the proposed approach over its peer works of recent times.																	0941-0643	1433-3058				APR	2020	32	7			SI		2141	2164		10.1007/s00521-018-3891-5													
J								Integrating mutation scheme into monarch butterfly algorithm for global numerical optimization	NEURAL COMPUTING & APPLICATIONS										Optimization; Harmony search; Monarch butterfly optimization; Evolutionary computation	ARTIFICIAL BEE COLONY; KRILL HERD ALGORITHM; SEARCH ALGORITHM	Monarch butterfly optimization algorithm (MBO) has recently been proposed as a robust metaheuristic optimization algorithm for solving numerical global optimization problems. To enhance the performance of MBO algorithm, harmony search (HS) is introduced as a mutation operator during the adjusting operator of MBO. A novel hybrid metaheuristic optimization method, the so-called HMBO, is introduced to find the best solution for the global optimization problems. HMBO combines HS exploration with MBO exploitation, and therefore, it produces potential candidate solutions. The implementation process for enhancing MBO method is also presented. To evaluate the effectiveness of this improvement, fourteen standard benchmark functions are used. The mean and the best performance of these benchmark functions in 20, 50, and 100 dimensions demonstrated that HMBO often performs better than the original MBO and other population-based optimization algorithms such as ACO, BBO, DE, ES, GAPBIL, PSO and SGA. Moreover, the t-test result proved that the performance differences between the enhanced HMBO and the original MBO as well as the other optimization methods are statistically significant.																	0941-0643	1433-3058				APR	2020	32	7			SI		2165	2181		10.1007/s00521-018-3676-x													
J								A robust multi-objective humanitarian relief chain network design for earthquake response, with evacuation assumption under uncertainties	NEURAL COMPUTING & APPLICATIONS										Disaster response; Humanitarian supply chain network design; Multi-objective; Robust optimization	LOCATION-ROUTING MODEL; OPTIMIZATION; ALLOCATION; LOGISTICS	In this paper, we have proposed a multi-objective mathematical model for the humanitarian supply chain design problem that minimizes: (1) total number of the injured not transferred to hospitals and total number of the homeless not evacuated from the affected area, and (2) total unmet relief commodity needs. In this model, such parameters as the demand and travel time have been considered as uncertain and two discrete robust counterpart models (with "ellipsoidal" and "box and polyhedral" uncertainty sets) have been developed to model uncertainties. Results found from Tehran Case Study have revealed that the one with the "box and polyhedral" uncertainty set performs better than the "ellipsoidal" set.																	0941-0643	1433-3058				APR	2020	32	7			SI		2183	2203		10.1007/s00521-019-04193-x													
J								Computer-aided classification of the mitral regurgitation using multiresolution local binary pattern	NEURAL COMPUTING & APPLICATIONS										Mitral regurgitation; Texture analysis; Gaussian pyramid; Local binary patterns; Computer-aided classification system	INVARIANT TEXTURE CLASSIFICATION; ISOVELOCITY SURFACE-AREA; RANDOM FORESTS; FLOW-RATE; SEVERITY; DIAGNOSIS	This paper introduces a computer-aided classification (CAC) system for the severity analysis of mitral regurgitation (MR) utilizing multiresolution local binary pattern variants texture features. Initially, the Gaussian pyramid has been used as a multiresolution technique. Subsequently, seven variants of the local binary pattern (LBP) have been employed to extract the features. At last, support vector machine and random forest classifiers are used for classification. The performances of conventional LBP variants and proposed features have been evaluated on MR image database in three classes, i.e., mild, moderate, and severe, in three different views. The Gaussian pyramid-based center-symmetric local binary pattern performed well in all three views. The achieved classification accuracies are 95.66 +/- 0.98% in the apical 2 chamber, 94.47 +/- 1.91% in the apical 4 chamber and 94.21 +/- 1.31% in parasternal long axis views using SVM classifier with the tenfold cross-validation. The outcomes of paper confirm that the performance of the conventional LBP features is enhanced significantly and the proposed CAC system is useful in assisting cardiologists in the severity analysis of MR.																	0941-0643	1433-3058				APR	2020	32	7			SI		2205	2215		10.1007/s00521-018-3935-x													
J								DC-DC converters design using a type-2 wavelet fuzzy cerebellar model articulation controller	NEURAL COMPUTING & APPLICATIONS										Type-2 fuzzy system; Cerebellar model articulation controller; DC-DC converter; Buck-boost converter	CONTROL-SYSTEM DESIGN; INTERVAL TYPE-2; NEURAL-NETWORK; MPPT	Recently, boost and buck converters are widely applied in many applications, especially in recycled energy industry. The efficiency of DC-DC converter, which can increase or decrease the input voltage according to the driver output voltage, can effectively affect the total efficiency of the systems. In this paper, a sliding mode interval type-2 fuzzy wavelet cerebellar model articulation controller (T2WFCMAC)-based control system is designed for the DC-DC converters. The proposed control system contains a main controller and a robust compensation controller. The main controller is the T2WFCMAC which is used to mimic an ideal controller, and the robust compensation is designed to compensate for the approximation error between the main controller and the ideal controller. The sliding hyperplane is applied to improve the robustness of the control system. All the adaptive laws for adjusting the parameters of T2WFCMAC are obtained using the gradient descent method. The stability of control system is guaranteed in the sense of Lyapunov function. Finally, numerical experimental results of boost and buck converters are presented to illustrate the effectiveness of the proposed approach under the change in the input voltage and the load resistance variations.																	0941-0643	1433-3058				APR	2020	32	7			SI		2217	2229		10.1007/s00521-018-3755-z													
J								Classification of sonar echo signals in their reduced sparse forms using complex-valued wavelet neural network	NEURAL COMPUTING & APPLICATIONS										Sonar detection; Sonar measurements; Target recognition; Neural networks; Neurons; Compressed sensing	ALGORITHM; TARGETS	This study aims to identify a method for classifying signals using their reduced sparse forms with a higher degree of accuracy. Many signals, such as sonar, radar, or seismic signals, are either sparse or can be made sparse in the sense that they have sparse or compressible representations when expressed in the appropriate basis. They have a convenient transform domain in which a small number of sparse coefficients express them as linear sums of sinusoidals, wavelets, or other bases. Although real-valued artificial neural networks (ANNs) have been frequently used in the classification of sonar signals for a long time, complex-valued wavelet neural network (CVWANN) is used for these complex reduced sparse forms of sonar signals in this study. Before the classification, the number of inputs was reduced to 1/3 dimension. Complex-valued sparse coefficients (CVSCs) obtained from the reduced form were classified by CVWANN. The performance of the proposed method is presented and compared to other classification methods. Our method, CVSCs + CVWANN, is very successful as 94.23% by tenfold cross-validation data selection and 95.19% by 50-50% training-testing data selection.																	0941-0643	1433-3058				APR	2020	32	7			SI		2231	2241		10.1007/s00521-018-3920-4													
J								Analyzing multimodal transportation problem and its application to artificial intelligence	NEURAL COMPUTING & APPLICATIONS										Transportation problem; Multimodal system; Neural network; Artificial intelligence; Decision-making problem	NEURAL-NETWORK; OPTIMIZATION; ALGORITHM; COST	In recent decades, there has been increased interest among both transportation researchers and practitioners in exploring the application of artificial intelligence (AI) paradigms to address the real-life problems in order to improve the efficiency, safety and environmental compatibility of transportation systems. In this paper, our main interest is to solve transportation problem by considering the multimodal transport systems and then utilize it to solve neural network (NN) problem in AI. The multimodal transportation problem (MMTP) is nothing but a linear programming problem, and so it is easy to solve by any simplex algorithm. To analyze the proposed method, a numerical example is included and solving it we reveal a better impact for analyzing the real-life decision-making problems. Thereafter, we revoke our approach for solving NN problems, which enhances a connection between MMTP and NN problems. Finally, conclusion and future research directions are presented regarding our study.																	0941-0643	1433-3058				APR	2020	32	7			SI		2243	2256		10.1007/s00521-019-04393-5													
J								Design of active noise control system using hybrid functional link artificial neural network and finite impulse response filters	NEURAL COMPUTING & APPLICATIONS										Active noise control; Functional link artificial neural network; Finite impulse response filter; Filtered-s least mean square algorithm	LMS ALGORITHM; FIR	The active noise control is the best approach to limit the low-frequency noise present in any applications and also to estimate the signals, which are corrupted by interference or additive noise. In this paper, the design of an active noise control system is proposed with the hybrid combination of functional link artificial neural network and finite impulse response filter. The filter coefficients of both functional link artificial neural network and finite impulse response filters are optimized by firefly algorithm, and the ensemble mean square error value is computed through the filtered-s least mean square algorithm, in which the convergence of the filter is improved through the proposed approach. The signals like Chaotic noise signal and Gaussian noise signal are considered to assess the proposed method. Then, the noise reduction capability of the proposed active noise control with firefly algorithm is compared with that achieved by the same active noise control with another optimization algorithm known as BAT algorithm in place of firefly algorithm. The performance analysis is carried out under the MATLAB environment.																	0941-0643	1433-3058				APR	2020	32	7			SI		2257	2266		10.1007/s00521-018-3697-5													
J								Iris tissue recognition based on GLDM feature extraction and hybrid MLPNN-ICA classifier	NEURAL COMPUTING & APPLICATIONS										Iris tissue recognition; Feature extraction; GLDM; Imperialist competitive algorithm; Multi-layer perceptron neural network	SEGMENTATION; FRAMEWORK	The use of iris tissue for identification is an accurate and reliable system for identifying people. This method consists of four main processing stages, namely segmentation, normalization, feature extraction, and matching. In this study, a new method of feature extraction and classification based on gray-level difference method and hybrid MLPNN-ICA classifier is proposed. For experimental results, our study is implemented on CASIA-Iris V3 dataset and UCI machine learning repository datasets.																	0941-0643	1433-3058				APR	2020	32	7			SI		2267	2281		10.1007/s00521-018-3754-0													
J								A machine learning approach for prediction of pregnancy outcome following IVF treatment	NEURAL COMPUTING & APPLICATIONS										In vitro fertilization (IVF); Feature selection; Machine learning techniques; Classification	IN-VITRO FERTILIZATION; NEURAL-NETWORKS; CLASSIFICATION; EMBRYO; RECOGNITION; KNOWLEDGE; ALGORITHM	Infertility affects one out of seven couples around the world. Therefore, the best possible management of the in vitro fertilization (IVF) treatment and patient advice is crucial for both patients and medical practitioners. The ultimate concern of the patients is the success of an IVF procedure, which depends on a number of influencing attributes. Without any automated tool, it is hard for the practitioners to assess any influencing trend of the attributes and factors that might lead to a successful IVF pregnancy. This paper proposes a hill climbing feature (attribute) selection algorithm coupled with automated classification using machine learning techniques with the aim to analyze and predict IVF pregnancy in greater accuracy. Using 25 attributes, we assessed the prediction ability of IVF pregnancy success for five different machine learning models, namely multilayer perceptron (MLP), support vector machines (SVM), C4.5, classification and regression trees (CART) and random forest (RF). The prediction ability was measured in terms of widely used performance metrics, namely accuracy rate, F-measure and AUC. Feature selection algorithm reduced the number of most influential attributes to nineteen for MLP, sixteen for RF, seventeen for SVM, twelve for C4.5 and eight for CART. Overall, the most influential attributes identified are: 'age', 'indication' of fertility factor, 'Antral Follicle Counts (AFC)', 'NbreM2', 'method of sperm collection', 'Chamotte', 'Fertilization rate in vitro', 'Follicles on day 14' and 'Embryo transfer day.' The machine learning models trained with the selected set of features significantly improved the prediction accuracy of IVF pregnancy success to a level considerably higher than those reported in the current literature.																	0941-0643	1433-3058				APR	2020	32	7			SI		2283	2297		10.1007/s00521-018-3693-9													
J								Efficient parallel implementation of reservoir computing systems	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Recurrent neural networks; Reservoir computing; Echo sate networks; Hardware neural network; Field-programmable gate array; Time-series prediction	NONLINEAR CHANNEL EQUALIZATION; NEURAL-NETWORKS; HARDWARE; DESIGN; HETEROSKEDASTICITY; CANCELLATION	Reservoir computing (RC) is a powerful machine learning methodology well suited for time-series processing. The hardware implementation of RC systems (HRC) may extend the utility of this neural approach to solve real-life problems for which software solutions are not satisfactory. Nevertheless, the implementation of massive parallel-connected reservoir networks is costly in terms of circuit area and power, mainly due to the requirement of implementing synapse multipliers that increase gate count to prohibitive values. Most HRC systems present in the literature solve this area problem by sequencializing the processes, thus loosing the expected fault-tolerance and low latency of fully parallel-connected HRCs. Therefore, the development of new methodologies to implement fully parallel HRC systems is of high interest to many computational intelligence applications requiring quick responses. In this article, we propose a compact hardware implementation for Echo-State Networks (an specific type of reservoir) that reduces the area cost by simplifying the synapses and using linear piece-wise activation functions for neurons. The proposed design is synthesized in a Field-Programmable Gate Array and evaluated for different time-series prediction tasks. Without compromising the overall accuracy, the proposed approach achieves a significant saving in terms of power and hardware when compared with recently published implementations. This technique pave the way for the low-power implementation of fully parallel reservoir networks containing thousands of neurons in a single integrated circuit.																	0941-0643	1433-3058				APR	2020	32	7			SI		2299	2313		10.1007/s00521-018-3912-4													
J								An enhanced moth flame optimization	NEURAL COMPUTING & APPLICATIONS										Nature-inspired algorithms; Moth flame optimization; Cauchy function; Benchmark functions	ALGORITHM	Moth flame optimization (MFO) is a recent nature-inspired algorithm, motivated from the transverse orientation of moths in nature. The transverse orientation is a special kind of navigation method, which demonstrates the movement of moths toward moon in a straight path. This algorithm has been successfully applied on various optimization problems. But, MFO suffers from the problem of poor exploration. So, in order to enhance the performance of MFO, some modifications are proposed. A Cauchy distribution function is added to enhance the exploration capability, influence of best flame has been added to improve the exploitation and adaptive step size and division of iterations is followed to maintain a balance between the exploration and exploitation. The proposed algorithm has been named as enhanced moth flame optimization (E-MFO) and to validate the applicability of E-MFO, and it has been applied to twenty benchmark functions. Also, comprehensive comparison of E-MFO with other meta-heuristic algorithms like bat algorithm, bat flower pollination, differential evolution, firefly algorithm, genetic algorithm, particle swarm optimization and flower pollination algorithm has been done. Further, the effect of population and dimension size on the performance of MFO and E-MFO has been discussed. The experimental analysis shows the superior performance of E-MFO over other algorithms in terms of convergence rate and solution quality. Also, statistical testing of E-MFO has been done to prove its significance.																	0941-0643	1433-3058				APR	2020	32	7			SI		2315	2349		10.1007/s00521-018-3821-6													
J								Distance and signal quality aware next hop selection routing protocol for vehicular ad hoc networks	NEURAL COMPUTING & APPLICATIONS										Geographical; Link quality; Distance; Direction; Mobility; Topologies; Routing; VANET	DATA DISSEMINATION; PREDICTION; ALGORITHM; VANETS	Newly emerged and integrated data communication technologies have changed the traditional processes and improve the quality of services. Wireless networks also provide efficient, low cost, fast and reliable solutions for different fields of life. Vehicular ad hoc networks are intended with various new smart features and make the inter communication systems for transportation and provide safety and comfort on the roads and establish the communication platform. However, because of unpredictable dense and sparse traffic conditions in urban areas, data communication is a challenging task for effective and reliable data links. Various topology-, location- and cluster-based protocols have been adopted to deal with high mobility of nodes and dynamic nature of network topologies. This paper presents a distance and signal quality aware routing (DSQR) protocol to deal with vehicular networks. The propose protocol initiates its forwarding decision based on Mid-Area node selection by evaluating direction and distance of neighbour nodes and also taking link quality into account and selects the best next forwarder node towards the destination node. The performance of DSQR is evaluated carrying out with vehicle speed and traffic density in realistic environment. Performance comparisons evaluation indicated the higher performance of propose protocol compared with existing protocols and reduces packet dropping and link failure issues.																	0941-0643	1433-3058				APR	2020	32	7			SI		2351	2364		10.1007/s00521-019-04320-8													
J								A new technique for time series forecasting by using symbiotic organisms search	NEURAL COMPUTING & APPLICATIONS										Time series forecasting; Symbiotic organisms search; Artificial neural network	NEURAL-NETWORK; OPTIMIZATION	Symbiotic organisms search (SOS) is a new metaheuristic optimization algorithm proposed by Cheng and Prayogo (Comput Struct 139:98-112, 2014). In this paper, SOS has been applied to determine the functional forms of different time series which are used to predict the time series. There are some previous attempts by researchers where genetic algorithm has been used to find the functional form of a time series. Here, we explore this new algorithm in time series analysis. SOS mimics the symbiotic relationships among organisms in the ecosystem. Improvement in SOS in parasitism phase has been proposed here. Also, several types of time series have been tested to compare the performance of the original SOS with its improved version and with already well-established artificial neural network (ANN) in the field of time series forecasting.																	0941-0643	1433-3058				APR	2020	32	7			SI		2365	2381		10.1007/s00521-019-04134-8													
J								Automatic optimized support vector regression for financial data prediction	NEURAL COMPUTING & APPLICATIONS										Prediction methods; Support vector regression; Evolutionary computation; Financial forecasting; Genetic algorithms	MODEL	The aim of this article is to introduce a hybrid approach, namely optimal multiple kernel-support vector regression (OMK-SVR) for time series data prediction and to analyze and compare its performances against those of support vector regression with a single RBF kernel (RBF-SVR), gene expression programming (GEP) and extreme learning machine (ELM) on the financial series formed by the monthly and weekly values of Bursa Malaysia KLCI Index, monthly values of Dow Jones Industrial Average Index (DJIA) and New York Stock Exchange. Our method provides an optimal multiple kernel and optimal parameters in Support Vector Regression, improving the accuracy of prediction. The proposed approach is structured on two levels. The macro-level uses a breeder genetic algorithm for choosing the optimal multiple kernel and the SVR optimal parameters. The fitness function of each chromosome is computed in the micro-level using a SVR algorithm. The regression model based on the optimal multiple kernel and optimal parameters is then validated and used for forecasting. The experimental results prove that OMK-SVR performs better than GEP, RBF-SVR and ELM for predicting the future behavior of the study series. A sensitivity study with respect to the number of kernels from the multiple kernel used by OMK-SVR and with respect to the ratio between training and testing data sets was conducted.																	0941-0643	1433-3058				APR	2020	32	7			SI		2383	2396		10.1007/s00521-019-04216-7													
J								An entropy-based classification of breast cancerous genes using microarray data	NEURAL COMPUTING & APPLICATIONS										Support vector machine; k-Nearest neighbor; Random forest; Naive Bayes; Classification; Machine learning algorithm	SUPPORT VECTOR MACHINE; EXPRESSION; PREDICTION; REGRESSION	Gene expression levels obtained from microarray data provide a promising technique for doing classification on cancerous data. Due to the high dimensionality of the microarray datasets, the redundant genes need to be removed and only significant genes are required for building the classifier. In this work, an entropy-based method was used based on supervised learning to differentiate between normal tissue and breast tumor based on their gene expression profiles. This work employs four widely used machine learning techniques for breast cancer prediction, namely support vector machine (SVM), random forest, k-nearest neighbor (KNN) and naive Bayes. The performance of these techniques was evaluated on four different classification performance measurements which result in getting more accuracy in case of SVM as compared to other machine learning algorithms. Classification accuracy of 91.5% was achieved by support vector machine with 0.833 F1 measures. Furthermore, these techniques were evaluated on the basis of performance by ROC curve and calibration graph.																	0941-0643	1433-3058				APR	2020	32	7			SI		2397	2404		10.1007/s00521-018-3864-8													
J								Bipolar delta-equal complex fuzzy concept lattice with its application	NEURAL COMPUTING & APPLICATIONS										Bipolar complex fuzzy set; Bipolar complex fuzzy graph; Bipolar concept lattice; Bipolar fuzzy graph; Formal concept analysis (FCA); Granulation	GRAPH REPRESENTATION; SETS; REDUCTION; MATRIX; LOGIC	Recently, bipolar as well as vague concept lattice visualization is introduced for precise representation of inconsistency and incompleteness in data sets based on its acceptation and rejection part simultaneously. In this process, a problem is addressed while measuring the periodic fluctuation in bipolar information at the given phase of time. This changes in human cognition used coexist often in our daily life where the sentiments (i.e., love or hatred) for anyone may change several times from morning to evening office time. In this case precise representation of this type of bipolar information and measuring its pattern is a major issue for the researchers. To deal with this problem, the current paper proposes three methods for adequate representation of bipolar complex data set using the calculus of complex fuzzy matrix, delta\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\delta$$\end{document}-equality and the calculus of granular computing, respectively. Hence, the proposed method provides an umbrella way to navigate or decompose the bipolar complex data sets and their semantics using an illustrative example. The results obtained from the proposed methods are also compared to validate the results.																	0941-0643	1433-3058				APR	2020	32	7			SI		2405	2422		10.1007/s00521-018-3936-9													
J								Self-adaptive global mine blast algorithm for numerical optimization	NEURAL COMPUTING & APPLICATIONS										Mine blast algorithm; Global optimization; Constrained optimization; Data clustering	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; STRATEGY	In this article, a self-adaptive global mine blast algorithm (GMBA) is proposed for numerical optimization. This algorithm is designed in a novel way, and a new shrapnel equation is proposed for the exploitation phase of mine blast algorithm. A theoretical study is performed, which proves the convergence of any typical shrapnel piece; a new definition for parameters values is defined based on the performed theoretical studies. The promising nature of newly designed exploitation idea is verified with the help of multiple numerical experiments. A state-of-the-art set of benchmark problems are solved with the proposed GMBA, and the optimization results are compared with seven state-of-the-art optimization algorithms. The experimental results are statistically validated by using Wilcoxon signed-rank test, and time complexity of GMBA is also calculated. It has been justified that the proposed GMBA works as a global optimizer for constrained optimization problems. As an application to the newly developed GMBA, an important data clustering problem is solved on six data clusters and the clustering results are compared with the state-of-the-art optimization algorithms. The promising results claim the proposed GMBA as a strong optimizer for data clustering application.																	0941-0643	1433-3058				APR	2020	32	7			SI		2423	2444		10.1007/s00521-019-04009-y													
J								Convergence of a modified gradient-based learning algorithm with penalty for single-hidden-layer feed-forward networks	NEURAL COMPUTING & APPLICATIONS										Neural networks; Penalty; Gradient; Monotonicity; Convergence	NEURAL-NETWORK; EXTREME; MACHINE; REGRESSION	Based on a novel algorithm, known as the upper-layer-solution-aware (USA), a new algorithm, in which the penalty method is introduced into the empirical risk, is studied for training feed-forward neural networks in this paper, named as USA with penalty. Both theoretical analysis and numerical results show that it can control the magnitude of weights of the networks. Moreover, the deterministic theoretical analysis of the new algorithm is proved. The monotonicity of the empirical risk with penalty term is guaranteed in the training procedure. The weak and strong convergence results indicate that the gradient of the total error function with respect to weights tends to zero, and the weight sequence goes to a fixed point when the iterations approach positive infinity. Numerical experiment has been implemented and effectively verifies the proved theoretical results.																	0941-0643	1433-3058				APR	2020	32	7			SI		2445	2456		10.1007/s00521-018-3748-y													
J								River discharge simulation using variable parameter McCarthy-Muskingum and wavelet-support vector machine methods	NEURAL COMPUTING & APPLICATIONS										Flood forecasting; VPMM; SVM; Wavelet transform	ARTIFICIAL-INTELLIGENCE MODELS; NEURAL-NETWORKS; HYDRODYNAMIC DERIVATION; FLOW; PREDICTION; STREAMFLOW; TRANSFORM; HYDROLOGY; ENSEMBLE	In this study, an extended version of variable parameter McCarthy-Muskingum (VPMM) method originally proposed by Perumal and Price (J Hydrol 502:89-102, 2013) was compared with the widely used data-based model, namely support vector machine (SVM) and hybrid wavelet-support vector machine (WASVM) to simulate the hourly discharge in Neckar River wherein significant lateral flow contribution by intermediate catchment rainfall prevails during flood wave movement. The discharge data from the year 1999 to 2002 have been used in this study. The extended VPMM method has been used to simulate 9 flood events of the year 2002, and later the results were compared with SVM and WASVM models. The analysis of statistical and graphical results suggests that the extended VPMM method was able to predict the flood wave movement better than the SVM and WASVM models. A model complexity analysis was also conducted which suggests that the two parameter-based extended VPMM method has less complexity than the three parameter-based SVM and WASVM model. Further, the model selection criteria also give the highest values for VPMM in 7 out of 9 flood events. The simulation of flood events suggested that both the approaches were able to capture the underlying physics and reproduced the target value close to the observed hydrograph. However, the VPMM models are slightly more efficient and accurate, than the SVM and WASVM model which are based only on the antecedent discharge data. The study captures the current trend in the flood forecasting studies and showed the importance of both the approaches (physical and data-based modeling). The analysis of the study suggested that these approaches complement each other and can be used in accurate yet less computational intensive flood forecasting.																	0941-0643	1433-3058				APR	2020	32	7			SI		2457	2470		10.1007/s00521-018-3745-1													
J								A biomorphic neuron model and principles of designing a neural network with memristor synapses for a biomorphic neuroprocessor	NEURAL COMPUTING & APPLICATIONS										Biomorphic neuron model; Biomorphic neural network; Memristor; Memristor-based synapses	STORAGE; DEVICE	This paper presents an original biomorphic neuron model, which differs from common IT models by a more complex synapse structure and from biological models by replacement of differential equations that describe the change in potential over time with explicit recurrence expressions by approximation of experimental data in the cortical neuron, and therefore, by transition from the spiking information coding to the coding using the average frequency of action potentials per a simulation step. This approach ensures sufficiently simple and efficient calculation of an ultra-large neural network in the stand-alone hardware with limited computing resources. The model consists of three separate functional parts: dendrites, soma, and axon, which allows implementing any connections between functional parts of different neurons, thus making the neural network architecture more flexible. To perform functional testing of the neuron model, the test neural network performing simple association and constructed as a consequent stack of functional blocks with primary connections organized using experimental neurophysiological data was simulated. It is shown that encoding of the information transmitted by the impulses, similar to biological ones, allows using memristors for calculating recurrence expressions that describe the change in the quantity of neurotransmitter receptors of the dendrite membrane. The elaborated biomorphic neuron model, defined conceptual principles of a neural network construction based on it, as well as replacement of synapses in the neural network with memristors will allow building an ultra-large biomorphic neural network that simulates the functioning of a separate brain cortical column in the stand-alone hardware-a biomorphic neuroprocessor.																	0941-0643	1433-3058				APR	2020	32	7			SI		2471	2485		10.1007/s00521-019-04383-7													
J								Hybrid bio-inspired user clustering for the generation of diversified recommendations	NEURAL COMPUTING & APPLICATIONS										Recommender system; Personalization; Diversity; Accuracy; Clustering; Swarm intelligence	FUZZY C-MEANS; PARTICLE SWARM OPTIMIZATION; KRILL HERD ALGORITHM; SYSTEM; RELEVANCY; POINT	The research and development of recommender systems are traditionally focused on the enhancement and guaranteeing the recommendation accuracy to achieve user satisfaction. On the other hand, the alternative recommendation qualities such as diversity and novelty have received significant attention from researchers in recent times. In this paper, we present a detailed study of the diversity in recommender systems to help researchers in the development of recommendation approaches to generate efficient recommendations. We have also analyzed the existing works for assessment of impact and quality of diversified recommendations. Based on our detailed investigation of the diversity in recommendations, we shift the generic focus from accuracy objectives to explore beyond the accuracy of recommendations. The need for recommender systems producing diversified recommendations without compromising the accuracy is very high to meet the growing demands of users. To address the personalization problem in travel recommender systems, we present the hybrid swarm intelligence clustering ensemble-based recommendation framework to generate diverse and accurate Point of Interest recommendations. Our proposed recommendation approach employs multiple swarm optimization algorithms to frame a clustering ensemble for the generation of efficient user clustering. We have evaluated our proposed recommendation approach over a real-time large-scale dataset of TripAdvisor to estimate the quality of recommendations in terms of diversity and accuracy. The experimental results demonstrate the enhanced efficiency of the proposed recommendation approach over state-of-the-art techniques.																	0941-0643	1433-3058				APR	2020	32	7			SI		2487	2506		10.1007/s00521-019-04128-6													
J								Adaptive neurofuzzy H-infinity control of DC-DC voltage converters	NEURAL COMPUTING & APPLICATIONS										Nonlinear dynamics; Adaptive control; Neurofuzzy control; H-infinity control; Approximate linearization; Jacobian matrices; Riccati equation; Lyapunov function; Asymptotic stability	FUZZY CONTROL; NONLINEAR-SYSTEMS; BOOST CONVERTER; DESIGN; TRACKING	A novel adaptive neurofuzzy H-infinity control approach to feedback control of DC-DC converters, being used in applications of renewable energy generation, is developed. The form and the parameters of the differential equations that constitute the dynamic model of the controlled system are considered to be unknown, while there is only knowledge about the order of the system. The model of the controlled system undergoes approximate linearization around a temporary operating point which is re-computed at each iteration of the control algorithm. The linearization procedure makes use of Taylor series expansion and the computation of Jacobian matrices. For the approximately linearized model of the DC-DC converter, it is possible to design a stabilizing H-infinity feedback controller, provided that knowledge about the matrices of the linearized state-space description is available. In such a case, the computation of the feedback controller's gain comes from the solution of an algebraic Riccati equation taking also place at each iteration of the control method. However, since the elements of these matrices are unknown, it is proposed to estimate them with the use of neurofuzzy approximators. Actually, neurofuzzy networks are employed for learning the constituent functions of the system's dynamic model. Based on these function estimates, the system's Jacobian matrices are also obtained and this allows the implementation of the H-infinity feedback controller. To assure the stability of the control loop, the learning rate of the neurofuzzy approximators is chosen from the requirement that the first derivative of the system's Lyapunov function to be always a negative one. The global asymptotic stability and the robustness properties of the control method are proven through Lyapunov stability analysis.																	0941-0643	1433-3058				APR	2020	32	7			SI		2507	2520		10.1007/s00521-019-04394-4													
J								A hybrid unsupervised approach toward EEG epileptic spikes detection	NEURAL COMPUTING & APPLICATIONS										DWT; ANFIS; Epileptic spike detection; Eye blink remover	FUZZY INFERENCE SYSTEM; PRINCIPAL COMPONENT ANALYSIS; NEURAL-NETWORK; ARTIFACT CORRECTION; AUTOMATIC DETECTION; WAVELET TRANSFORM; CLASSIFICATION; DIAGNOSIS; REMOVAL; REAL	Epileptic spikes are complementary sources of information in EEG to diagnose and localize the origin of epilepsy. However, not only is visual inspection of EEG labor intensive, time consuming, and prone to human error, but it also needs long-term training to acquire the level of skill required for identifying epileptic discharges. Therefore, computer-aided approaches were employed for the purpose of saving time and increasing the detection and source localization accuracy. One of the most important artifacts that may be confused as an epileptic spike, due to morphological resemblance, is eye blink. Only a few studies consider removal of this artifact prior to detection, and most of them used either visual inspection or computer-aided approaches, which need expert supervision. Consequently, in this paper, an unsupervised and EEG-based system with embedded eye blink artifact remover is developed to detect epileptic spikes. The proposed system includes three stages: eye blink artifact removal, feature extraction, and classification. Wavelet transform was employed for both artifact removal and feature extraction steps, and adaptive neuro-fuzzy inference system for classification purpose. The proposed method is verified using a publicly available EEG dataset. The results show the efficiency of this algorithm in detecting epileptic spikes using low-resolution EEG with least computational complexity, highest sensitivity, and lesser human interaction compared to similar studies. Moreover, since epileptic spike detection is a vital component of epilepsy source localization, therefore this algorithm can be utilized for EEG-based pre-surgical evaluation of epilepsy.																	0941-0643	1433-3058				APR	2020	32	7			SI		2521	2532		10.1007/s00521-018-3797-2													
J								A GA based hierarchical feature selection approach for handwritten word recognition	NEURAL COMPUTING & APPLICATIONS										Hierarchical feature selection; Genetic Algorithm; Handwritten city name; Bangla script; Elliptical feature; Gradient-based feature	MULTIOBJECTIVE GENETIC ALGORITHMS; BOUND ALGORITHM; CLASSIFICATION; BANGLA; BRANCH	Feature selection plays a key role in reducing the dimensionality of a feature vector by discarding redundant and irrelevant ones. In this paper, a Genetic Algorithm-based hierarchical feature selection (HFS) model has been designed to optimize the local and global features extracted from each of the handwritten word images under consideration. In this context, two recently developed feature descriptors based on shape and texture of the word images have been taken into account. Experimentation is conducted on an in-house dataset of 12,000 handwritten word samples written in Bangla script. This database comprises names of 80 popular cities of West Bengal, a state of India. Proposed model not only reduces the feature dimension by nearly 28%, but also enhances the performance of the handwritten word recognition (HWR) technique by 1.28% over the recognition performance obtained with unreduced feature set. Moreover, the proposed HFS-based HWR system performs better in comparison with some recently developed methods on the present dataset.																	0941-0643	1433-3058				APR	2020	32	7			SI		2533	2552		10.1007/s00521-018-3937-8													
J								Classification of calcified regions in atherosclerotic lesions of the carotid artery in computed tomography angiography images	NEURAL COMPUTING & APPLICATIONS										Medical imaging; Pattern recognition; Classification; Atherosclerosis	PLAQUES; MRI; QUANTIFICATION; IDENTIFICATION	The identification of atherosclerotic plaque components, extraction and analysis of their morphology represent an important role towards the prediction of cardiovascular events. In this article, the classification of regions representing calcified components in computed tomography angiography (CTA) images of the carotid artery is tackled. The proposed classification model has two main steps: the classification per pixel and the classification per region. Features extracted from each pixel inside the carotid artery are submitted to four classifiers in order to determine the correct class, i.e. calcification or non-calcification. Then, geometrical and intensity features extracted from each candidate region resulting from the pixel classification step are submitted to the classification per region in order to determine the correct regions of calcified components. In order to evaluate the classification accuracy, the results of the proposed classification model were compared against ground truths of calcifications obtained from micro-computed tomography images of excised atherosclerotic plaques that were registered with in vivo CTA images. The average values of the Spearman correlation coefficient obtained by the linear discriminant classifier were higher than 0.80 for the relative volume of the calcified components. Moreover, the average values of the absolute error between the relative volumes of the classified calcium regions and the ones calculated from the corresponding ground truths were lower than 3%. The new classification model seems to be adequate as an auxiliary diagnostic tool for identifying calcifications and allowing their morphology assessment.																	0941-0643	1433-3058				APR	2020	32	7			SI		2553	2573		10.1007/s00521-019-04183-z													
J								Novel cascaded Gaussian mixture model-deep neural network classifier for speaker identification in emotional talking environments	NEURAL COMPUTING & APPLICATIONS										Deep neural network; Emotional talking environments; Gaussian mixture model; Speaker identification	RECOGNITION; SPEECH; FEATURES	This research is an effort to present an effective approach to enhance text-independent speaker identification performance in emotional talking environments based on novel classifier called cascaded Gaussian Mixture Model-Deep Neural Network (GMM-DNN). Our current work focuses on proposing, implementing and evaluating a new approach for speaker identification in emotional talking environments based on cascaded Gaussian mixture model-deep neural network as a classifier. The results point out that the cascaded GMM-DNN classifier improves speaker identification performance at various emotions using two distinct speech databases: Emirati speech database (Arabic United Arab Emirates dataset) and "speech under simulated and actual stress" English dataset. The proposed classifier outperforms classical classifiers such as multilayer perceptron and support vector machine in each dataset. Speaker identification performance that has been attained based on the cascaded GMM-DNN is similar to that acquired from subjective assessment by human listeners.																	0941-0643	1433-3058				APR	2020	32	7			SI		2575	2587		10.1007/s00521-018-3760-2													
J								Development of new agglomerative and performance evaluation models for classification	NEURAL COMPUTING & APPLICATIONS										Clustering analysis; Hierarchical clustering; Weighted clustering; Neighbourhood clustering; Structure strength	CLUSTERING-ALGORITHM	This study proposes two new hierarchical clustering methods, namely weighted and neighbourhood to overcome the issues such as getting less accuracy, inability to separate the clusters properly and the grouping of more number of clusters which exist in present hierarchical clustering methods. We have also proposed three new criteria to assess the performance of clustering methods: (1) overall effectiveness which means the product of overall efficiency and accuracy of the clusters which is used to evaluate the performance of the hierarchical clustering methods for the class label datasets, (2) modified structure strength S(c) to overcome the usage problem in hierarchical clustering methods to determine the number of clusters for non-class label datasets and (3) R-value which is the ratio of the determinant of the sum of square and cross product matrix of between-clusters to the determinant of the sum of square and cross product matrix of within-clusters. This will help us to validate the performance of hierarchical clustering methods for non-class label datasets. The evolved algorithms provided high accuracy, ability to separate the clusters properly and the grouping of less number of clusters. The performance of the new algorithms with existing algorithms is compared in terms of newly developed performance criteria. The new algorithms thus performed better than the existing algorithms. The whole exercise is done with the help of twelve class label and six non-class label datasets.																	0941-0643	1433-3058				APR	2020	32	7			SI		2589	2600		10.1007/s00521-019-04297-4													
J								Trajectory tracking using artificial neural network for stable human-like gait with upper body motion	NEURAL COMPUTING & APPLICATIONS										Biped robot; Inverse kinematics; Trajectory generation; Neural network (NN); Zero momentum point (ZMP); Upper body	INVERSE KINEMATICS; BIPED ROBOT; WALKING; GENERATION	This paper presents a trajectory generation algorithm for robots which can walk like human with movable foot and active toe. The proposed algorithm allows smooth transition between walking phases namely, single and double support phases. A neural network approach is used for solving inverse kinematics so that the biped robot follows the ankle and hip trajectories to walk. Zero moment point (ZMP) stability is ensured by taking into account the upper body movements along with the planned motion trajectories. Here, we analyze the effect of lateral upper body motion on ZMP stability. Different types of trajectories for upper body are generated, and the one which ensured the most stable locomotion is identified.																	0941-0643	1433-3058				APR	2020	32	7			SI		2601	2619		10.1007/s00521-018-3842-1													
J								Auto-MeDiSine: an auto-tunable medical decision support engine using an automated class outlier detection method and AutoMLP	NEURAL COMPUTING & APPLICATIONS										Classification; Disease prediction; Machine learning; Multilayer perceptron; Outlier detection	GENERALIZED DISCRIMINANT-ANALYSIS; NEURAL-NETWORK; DIABETES DISEASE; DIAGNOSIS; CLASSIFICATION; PREDICTION; REGRESSION	With advanced data analysis techniques, efforts for more accurate decision support systems for disease prediction are on the rise. According to the World Health Organization, diabetes-related illnesses and mortalities are on the rise. Hence, early diagnosis is particularly important. In this paper, we present a framework, Auto-MeDiSine, that comprises an automated version of enhanced class outlier detection using a distance-based algorithm (AutoECODB), combined with an ensemble of automatic multilayer perceptron (AutoMLP). AutoECODB is built upon ECODB by automating the tuning of parameters to optimize outlier detection process. AutoECODB cleanses the dataset by removing outliers. Preprocessed dataset is then used to train a prediction model using an ensemble of AutoMLPs. A set of experiments is performed on publicly available Pima Indian Diabetes Dataset as follows: (1) Auto-MeDiSine is compared with other state-of-the-art methods reported in the literature where Auto-MeDiSine realized an accuracy of 88.7%; (2) AutoMLP is compared with other learners including individual (focusing on neural network-based learners) and ensemble learners; and (3) AutoECODB is compared with other preprocessing methods. Furthermore, in order to validate the generality of the framework, Auto-MeDiSine is tested on another publicly available BioStat Diabetes Dataset where it outperforms the existing reported results, reaching an accuracy of 97.1%.																	0941-0643	1433-3058				APR	2020	32	7			SI		2621	2633		10.1007/s00521-019-04137-5													
J								Group decision-making and grey programming approaches to optimal product mix in manufacturing supply chains	NEURAL COMPUTING & APPLICATIONS										Manufacturing supply chains; Group decision-making; Grey theory; Grey programming	TOPSIS METHOD; SELECTION; RESILIENCE; MODEL; AGGREGATION; NEGOTIATION; CONSENSUS; BARRIERS; POWER; AHP	Group decision-making is a significant task that involves trade-offs, risks and the interplay of various factors. Optimization models developed from group decision-making are often built on uncertainties and negotiated parameters and may not yield factual solutions. Manufacturing decision-making environment is subject to several uncertainties, and grey theory is an effective tool to improve the exactitude of decision-making in such situations. An application of grey programming in manufacturing decision-making environment has been implemented in this research. Grey programming models can potentially avoid the loss of data while formulating optimization models from group decisions. A grey programming model for optimizing profits has been constructed and solved for an indeterminate product mix problem of a case electronics manufacturing industry. For the case, the optimization models were constructed and solved for the ideal model, critical model and the most typical model. And the results favor the decision on the introduction of a new product variant in the smartphone segment. In the most favorable and unfavorable conditions, it is seen that the introduction of new variant in the smartphone section is meritorious when compared to the tablet or laptop segments. The solution to the case endorses flexibility of grey programming in uncertain decision-making environments. As the decisions from the group can be signified in intervals using grey numbers, managers are recommended to comprise the benefits of grey programming models into their optimization problems for increasing flexibility and reliability of solutions.																	0941-0643	1433-3058				APR	2020	32	7			SI		2635	2649		10.1007/s00521-018-3675-y													
J								Robust exponential stabilization for uncertain neutral neural networks with interval time-varying delays by periodically intermittent control	NEURAL COMPUTING & APPLICATIONS										Robust exponential stabilization; Uncertain neutral neural networks; Mixed time-varying delays; Periodically intermittent control	STABILITY ANALYSIS; CONTROL-SYSTEMS	This paper studies the robust exponential stabilization for a class of uncertain neutral neural networks with mixed interval time-varying delays. The aim of the paper is to design periodically intermittent control such that the closed-loop system is exponentially stable. By constructing a suitable Lyapunov-Krasovskii functional and by using some useful lemmas and some new analysis techniques, the researchers generate novel exponential stabilization criteria to ensure the robust exponential stabilization of considered uncertain neutral neural networks in terms of linear matrix inequalities. Based on the proposed criteria, an intermittent state-feedback controller design approach is introduced. Some numerical examples are given to show the effectiveness and benefits of the theoretical results.																	0941-0643	1433-3058				APR	2020	32	7			SI		2651	2664		10.1007/s00521-018-3671-2													
J								Estimation of missing prices in real-estate market agent-based simulations with machine learning and dimensionality reduction methods	NEURAL COMPUTING & APPLICATIONS										Agent-based simulation; Machine learning; Real-estate market; Simulation setup	BIG DATA; MATRIX FACTORIZATION; DATA SCIENCE; MODEL; ALGORITHMS; PREDICTION; MANAGEMENT	The opacity of real-estate market involves some challenges in their agent-based simulation. While some real-estate Web sites provide the prices of a great amount of houses publicly, the prices of the rest are not available. The estimation of these prices is necessary for simulating their evolution from a complete initial set of houses. Additionally, this estimation could also be useful for other purposes such as appraising houses, letting buyers know which are the best offered prices (i.e., the lowest ones compared to the appraisals) and recommending the buyers to set an initial price. This work proposes combining dimensionality reduction methods with machine learning techniques to obtain the estimated prices. In particular, this work analyzes the use of nonnegative factorization, recursive feature elimination and feature selection with a variance threshold, as dimensionality reduction methods. It compares the application of linear regression, support vector regression, the k-nearest neighbors and a multilayer perceptron neural network, as machine learning techniques. This work has applied a tenfold cross-validation for comparing the estimations and errors and assessing the improvement over a basic estimator commonly used in the beginning of simulations. The developed software and the used dataset are freely available from a data research repository for the sake of reproducibility and the support to other researchers.																	0941-0643	1433-3058				APR	2020	32	7			SI		2665	2682		10.1007/s00521-018-3938-7													
J								Solution of the optimal power flow problem considering security constraints using an improved chaotic electromagnetic field optimization algorithm	NEURAL COMPUTING & APPLICATIONS										Electromagnetic field optimization; Metaheuristics; Chaos; Optimal power flow	DIFFERENTIAL EVOLUTION ALGORITHM; BIOGEOGRAPHY-BASED OPTIMIZATION; BEE COLONY ALGORITHM; NONSMOOTH; COST; EMISSION; STABILITY	The main objective of this paper is to solve different configurations of the optimal power flow (OPF) problem efficiently using an improved version of the newly proposed electromagnetic field optimization (EFO) algorithm. The developed and improved new version of EFO is based on chaotic maps and on a new mechanism. This improved version is called improved chaotic electromagnetic field optimization (ICEFO) algorithm. The performances of the ICEFO algorithm are evaluated on a large set of cases using: tow formulations, three objective functions (cost minimization, cost minimization and voltage profile improvement and cost minimization and voltage stability enhancement) and three test systems (the IEEE 30-bus, the IEEE 57-bus and the IEEE 118-bus test systems). The obtained results of the developed algorithm are compared with other well-known algorithms. These results demonstrate that the developed algorithm is able to solve efficiently different configurations of the OPF problem and for different test systems.																	0941-0643	1433-3058				APR	2020	32	7			SI		2683	2703		10.1007/s00521-019-04298-3													
J								Compressive sensing MRI reconstruction using empirical wavelet transform and grey wolf optimizer	NEURAL COMPUTING & APPLICATIONS										Compressive sensing (CS); Empirical wavelet transform (EWT); Grey wolf optimizer (GWO); Magnetic resonance imaging (MRI)	IMAGE; ALGORITHM; APPROXIMATION; TIME	Magnetic resonance imaging (MRI) has exhibited an outstanding performance in the track of medical imaging compared to several imaging modalities, such as X-ray, positron emission tomography and computed tomography. MRI modality suffers from protracted scanning time, which affects the psychological status of patients. This scanning time also increases the blurring levels in MR image due to local motion actions, such as breathing as in the case of cardiac imaging. An acquisition technique called compressed sensing has contributed to solve the drawbacks of MRI and decreased the acquisition time by reducing the quantity of the measured data that is needed to reconstruct an image without significant degradation in image quality. All recent works have used different types of conventional wavelets for sparsifying the image, which employ constant filter banks that are independent of the characteristics of the input image. This paper proposes to use the empirical wavelet transform (EWT) which tunes its filter banks to the characteristics of the analyzed images. In other words, we use EWT to produce a sparse representation of the MRI images which yields a more accurate sparsification transform. In addition, the grey wolf optimizer is used to optimize the parameters of the proposed method. To validate the proposed method, we use three MRI datasets of different organs: brain, cardiac and shoulder. The experimental results show that the proposed method outperforms the state-of-the-art methods in terms of signal-to-noise ratio and structure similarity metrics.																	0941-0643	1433-3058				APR	2020	32	7			SI		2705	2724		10.1007/s00521-018-3812-7													
J								Content-based image retrieval system using ORB and SIFT features	NEURAL COMPUTING & APPLICATIONS										CBIR; ORB; SIFT; K-means; LPP		Measures of components in digital images are expanded and to locate a specific image in the light of substance from a huge database is sometimes troublesome. In this paper, a content-based image retrieval (CBIR) system has been proposed to extract a feature vector from an image and to effectively retrieve content-based images. In this work, two types of image feature descriptor extraction methods, namely Oriented Fast and Rotated BRIEF (ORB) and scale-invariant feature transform (SIFT) are considered. ORB detector uses a fast key points and descriptor use a BRIEF descriptor. SIFT be used for analysis of images based on various orientation and scale. K-means clustering algorithm is used over both descriptors from which the mean of every cluster is obtained. Locality-preserving projection dimensionality reduction algorithm is used to reduce the dimensions of an image feature vector. At the time of retrieval, the image feature vectors are stored in the image database and matched with testing data feature vector for CBIR. The execution of the proposed work is assessed by utilizing a decision tree, random forest, and MLP classifiers. Two, public databases, namely Wang database and corel database, have been considered for the experimentation work. Combination of ORB and SIFT feature vectors are tested for images in Wang database and corel database which accomplishes a highest precision rate of 99.53% and 86.20% for coral database and Wang database, respectively.																	0941-0643	1433-3058				APR	2020	32	7			SI		2725	2733		10.1007/s00521-018-3677-9													
J								Time-optimal memetic whale optimization algorithm for hypersonic vehicle reentry trajectory optimization with no-fly zones	NEURAL COMPUTING & APPLICATIONS										Trajectory optimization; Hypersonic vehicle; Whale optimization algorithm; Gauss pseudo-spectral method	GENETIC ALGORITHM	A novel time-optimal memetic whale optimization algorithm (WOA) integrating the Gauss pseudo-spectral methods (GPM), is proposed in this paper for the hypersonic vehicle entry trajectory optimization problem with no-fly zones. The WOA is featured with the strong global search ability and non-sensitive to the initial values, but also shows poor searching convergence speed around the global optimum. Conversely, GPM may be sensitive to the initial solution and easily trapped in a local optimum, but it also possesses more rapid convergence speed around the optimum and higher searching accuracy. Thus, a memetic optimization algorithm which contains a two-stage approach mechanism is proposed for searching the global optimum. The first searching stage, which is driven by an improved WOA (IWOA), works as an initializer of the entire searching due to its strong global search ability and non-sensitive to the initial values. The local optimum reservation and adaptive amplitude factor updating strategy are established to improve the convergent speed and the global search ability of the WOA. Once the changing of fitness value satisfies the predefined criterion, the next searching stage driven by GPM will take the place of the IWOA to expedite the search process around optimum and to obtain a precise global optimal solution. By this hybrid way, the proposed optimization algorithm may find an optimum more quickly and accurately. Simulation results show the proposed algorithm possesses faster convergence speed, higher accuracy, and stronger robustness for the hypersonic vehicle entry trajectory optimization.																	0941-0643	1433-3058				APR	2020	32	7			SI		2735	2749		10.1007/s00521-018-3764-y													
J								Nuclear Fission-Nuclear Fusion algorithm for global optimization: a modified Big Bang-Big Crunch algorithm	NEURAL COMPUTING & APPLICATIONS										Big Bang-Big Crunch algorithm; Nuclear Fission and Nuclear Fusion; Engineering design optimization; Metaheuristics	HARMONY SEARCH ALGORITHM; ENGINEERING OPTIMIZATION; CONSTRAINED OPTIMIZATION; DESIGN OPTIMIZATION; RECONFIGURATION; INTEGER	This study introduces a derivative of the well-known optimization algorithm, Big Bang-Big Crunch (BB-BC), named Nuclear Fission-Nuclear Fusion-based BB-BC, simply referred to as N2F. Broadly preferred in the engineering optimization community, BB-BC provides accurate solutions with reasonably fast convergence rates for many engineering problems. Regardless, the algorithm often suffers from stagnation issues. More specifically, for some problems, BB-BC either converges prematurely or exploits the promising regions inefficiently, both of which prevent obtaining the optimal solution. To overcome such problems, N2F algorithm is proposed, inspired by two major phenomena of nuclear physics: fission and fusion reactions. In N2F, two concepts named "Nuclear Fission" and "Nuclear Fusion" are introduced, replacing the "Big Bang" and "Big Crunch" phases of BB-BC, respectively. With the "Nuclear Fission" phase represented through a parameter named amplification factor, premature convergence issues are eliminated to a great extent. Meanwhile, convergence rate and exploitation capability of the algorithm are enhanced largely through a precision control parameter named magnification factor, in the "Nuclear Fusion" phase. The performance of N2F algorithm is investigated through unconstrained test functions and compared with the conventional BB-BC and other metaheuristics including genetic algorithm, Particle Swarm Optimization (PSO), Artificial Bee Colony Optimization (ABC), Drone Squadron Optimization (DSO) and Salp Swarm Algorithm (SSA). Then, further analyses are performed with constrained design benchmarks, validating the applicability of N2F to engineering problems. With superior statistical performance compared to BB-BC, GA, PSO, ABC, DSO and SSA in unconstrained problems and improved results with respect to the literature studies, N2F is proven to be an efficient and robust optimization algorithm.																	0941-0643	1433-3058				APR	2020	32	7			SI		2751	2783		10.1007/s00521-018-3907-1													
J								An optimum forceful generation scheduling and unit commitment of thermal power system using sine cosine algorithm	NEURAL COMPUTING & APPLICATIONS										Unit commitment problem (UCP); Sine cosine algorithm (SCA); Generation scheduling (GS); Economic load dispatch (ELD)	PARTICLE SWARM OPTIMIZATION; LAGRANGIAN-RELAXATION; GENETIC ALGORITHM; SEARCH ALGORITHM; PRIORITY LIST	Conventional thermal power system-based units and its participation schedule known as unit commitment problem (UCP) is a significant and stimulating undertaking of allocating generated power among the dedicated units subject to numerous restrictions above a scheduled time prospect to obtain the slightest generation cost. This problem becomes further more complex by increasing the size of the power system. Since unit commitment problem is link optimization problem as it has both binary and continuous variable that is why it is most challenging problem to solve. In this paper, a recently invented optimizer sine-cosine is used to solve unit commitment problem. Sine cosine algorithm (SCA) is an innovative population centered optimization algorithm that has been used for solving the unit commitment optimization problems bounded by some constraints centered on the concept of a mathematical model of the sine and cosine functions. This paper offers the solution of unit commitment optimization problems of the electric power system by using the SCA, as UCP is linked optimization as it has both binary and continuous variables, the strategy adopted to tackle both variables is different. In this paper, proposed sine cosine algorithm searches allocation of generators (units that participate in generation to take upload) and once units are decided, allocation of generations (economic load dispatch) is done by mixed integer quadratic programming. The feasibility and efficacy of operation of SCA algorithm are verified for small- and medium-power systems, in which results for 4 unit, 5 unit, 6 unit, 7 unit, 10 units, 19 unit, 20 unit and 40 units are evaluated. The 10 generating units are evaluated with 5% and 10% spinning reserve. The results obviously show that the suggested method gives the superior type of solutions as compared to other algorithms.																	0941-0643	1433-3058				APR	2020	32	7			SI		2785	2814		10.1007/s00521-019-04598-8													
J								Robust spike-and-slab deep Boltzmann machines for face denoising	NEURAL COMPUTING & APPLICATIONS										Restricted Boltzmann machine; Deep Boltzmann machine; Unsupervised learning; Denoising	RECOGNITION	The robust Gaussian restricted Boltzmann machine can effectively learn the structure of noise to achieve better results in the face denoising task. The robust Gaussian restricted Boltzmann machine model contains two types of the restricted Boltzmann machine (RBM) model, where a general RBM is used to model the structure of the noise and a Gaussian RBM is used to model the clean data. The spike-and-slab RBM shows better learning abilities than the Gaussian RBM in real images modeling. In addition, the deep Boltzmann machine (DBM) shows powerful image reconstruction ability. To model the real images better, we first stack the spike-and-slab RBM and the RBM to create the spike-and-slab DBM. And then, we utilize the spike-and-slab DBM instead of the Gaussian RBM to model the density of the clean data in the Robust Gaussian RBM, and the proposed method is named as the robust spike-and-slab DBM which can obtain clearer denoising images. Finally, in order to obtain better denoising results, we make use of the learned spike-and-slab DBM model and the mean field method to multi-inference the denoising data learned from the robust spike-and-slab DBM. Experimental results show that the robust spike-and-slab DBM is an effective neural network denoising method.																	0941-0643	1433-3058				APR	2020	32	7			SI		2815	2827		10.1007/s00521-018-3866-6													
J								Improved word-level handwritten Indic script identification by integrating small convolutional neural networks	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Deep learning; Haar wavelet transform; Document analysis; Indic script recognition; More	CHARACTER-RECOGNITION; WAVELET TRANSFORM; DATABASES	Handwritten document recognition has been an active domain of research in the field of computer vision for several years since 1914 with the development of handheld scanner for reading printed texts called "optophone". In India, which has several different scripts in one document page, identifying them is a must to automate process: document understanding. We propose a novel technique in integrating convolutional neural networks (CNNs) for script identification. We combined small individually trainable small CNNs, and used several different levels of variation in the architectures of the individual CNNs. Such a collection of individually trainable modules vary with respect to the input image size, CNN's depth and wavelet transformation. In our test, we used publicly available dataset of size 11K words (1K per script) from 11 different Indic Scripts: Bangla, Devanagari, Gujarati, Gurumukhi, Kannada, Malayalam, Oriya, Roman, Tamil, Telugu and Urdu. Several ensemble strategies were implemented such as max-voting and probabilistic voting are used in addition to other conventional approaches like feature concatenation. We achieved a maximum accuracy of 95.04%, and it outperforms the accuracy of the state-of-the-art techniques like AlexNet by 2.9% and more importantly, benchmark techniques as (for script identification) on the dataset by more than 4%.																	0941-0643	1433-3058				APR	2020	32	7			SI		2829	2844		10.1007/s00521-019-04111-1													
J								Integrated intelligent computing for heat transfer and thermal radiation-based two-phase MHD nanofluid flow model	NEURAL COMPUTING & APPLICATIONS										Neural networks; Genetic algorithms; Sequential quadratic programming; Fluid mechanics; Nanofluids; Magnetohydrodynamic	ARTIFICIAL NEURAL-NETWORKS; INTERIOR-POINT ALGORITHM; COMPUTATIONAL INTELLIGENCE; NUMERICAL TREATMENT; DESIGN; DYNAMICS; OPTIMIZATION; HEURISTICS; SIMULATION; EQUATIONS	In this work, novel application of integrated computational heuristics is presented for computational fluid mechanics problem arising in the study of heat transfer and thermal radiation in two-phase magnetohydrodynamic (MHD) fluid flow model involving nanoparticles using the accurate approximation ability of neural networks hybrid with global exploration of genetic algorithm aided with local search exploitation of sequential quadratic programming. The networks are designed and arbitrarily combined to formulate mean squared error-based objective function for solving and governing nonlinear nanofluidic system. The designed methodology is evaluated to study the dynamics of the system by means of velocities, temperature and concentration profiles for prevailing factors based on variation in Reynolds and Schmidt numbers, as well as, rotation, radiation, magnetic, thermophoretic and Brownian parameters. The pragmatic worth of the scheme is established through statistical inferences in terms of accuracy, convergence and complexity metrics.																	0941-0643	1433-3058				APR	2020	32	7			SI		2845	2877		10.1007/s00521-019-04157-1													
J								Extended hesitant fuzzy linguistic term set with fuzzy confidence for solving group decision-making problems	NEURAL COMPUTING & APPLICATIONS										Hesitant fuzzy linguistic term set; Intuitionistic fuzzy confidence; Group decision making; Green supplier selection; Scientific decision framework	AGGREGATION OPERATORS; CONSISTENCY; 2-TUPLE	This paper presents a new extension of the hesitant fuzzy linguistic term set (HFLTS) called intuitionistic fuzzy confidence-based HFLTS that associates an intuitionistic fuzzy value (IFV) with each linguistic term. The resulting term set is termed as intuitionistic fuzzy confidence hesitant fuzzy linguistic term set (IFCHFLTS). The previous studies on the linguistic decision making have emphasized little upon the preference and non-preference for each of the linguistic terms. This information, however, is crucial in multi-criteria decision making under uncertainty. In this regard, we find IFV particularly useful for qualifying each of the linguistic terms with the agent's degree of preference, non-preference, and hesitation values. Besides, a new aggregation operator named intuitionistic fuzzy confidence linguistic simple weighted geometry (IFCLSWG) is also proposed to fuse decision makers' linguistic preferences. Further, the criteria weights are estimated using a new method called intuitionistic fuzzy confidence linguistic standard variance. An approach is also suggested for ranking the given alternatives by adapting VIKOR under the proposed IFCHFLTS context. Finally, the practicality and usefulness of the proposal are demonstrated through two real-world problems in green supplier selection for manufacturing industry, and medical diagnosis. The strengths and weaknesses of the proposal are also highlighted by drawing upon a comparison with similar methods.																	0941-0643	1433-3058				APR	2020	32	7			SI		2879	2896		10.1007/s00521-019-04275-w													
J								Single-column CNN for crowd counting with pixel-wise attention mechanism	NEURAL COMPUTING & APPLICATIONS										Crowd counting; CNN; Pixel-wise attention mechanism; FCN	ANOMALY DETECTION	This paper presents a novel method for accurate people counting in highly dense crowd images. The proposed method consists of three modules: extracting foreground regions (EF), pixel-wise attention mechanism (PAM) and single-column density map estimator (S-DME). EF can suppress the disturbance of complex background efficiently with a fully convolutional network, PAM performs pixel-wise classification of crowd images to generate high-quality local crowd density maps, and S-DME is a carefully designed single-column network that can learn more representative features with much fewer parameters. In addition, two new evaluation metrics are introduced to get a comprehensive understanding of the performance of different modules in our algorithm. Experiments demonstrate that our approach can get the state-of-the-art results on several challenging datasets including our dataset with highly cluttered environments and various camera perspectives.																	0941-0643	1433-3058				APR	2020	32	7			SI		2897	2908		10.1007/s00521-018-3810-9													
J								A deep learning analysis on question classification task using Word2vec representations	NEURAL COMPUTING & APPLICATIONS										Deep learning; Question classification; SVM; Word embedding; Word2vec	SENTIMENT CLASSIFICATION; NETS	Question classification is a primary essential study for automatic question answering implementations. Linguistic features take a significant role to develop an accurate question classifier. Recently, deep learning systems have achieved remarkable success in various text-mining problems such as sentiment analysis, document classification, spam filtering, document summarization, and web mining. In this study, we explain our study on investigating some deep learning architectures for a question classification task in a highly inflectional language Turkish that is an agglutinative language where word structure is produced by adding suffixes (morphemes) to root word. As a non-Indo-European language, languages like Turkish have some unique features, which make it challenging for natural language processing. For instance, Turkish has no grammatical gender and noun classes. In this study, user questions in Turkish are used to train and test the deep learning architectures. In addition to this, the details of the deep learning architectures are compared in terms of test and 10-cross fold validation accuracy. We use two major deep learning models in our paper: long short-term memory (LSTM), Convolutional Neural Networks (CNN), and we also implemented the combination of CNN-LSTM, CNN-SVM structures and a number of various those architectures by changing vector sizes and the embedding types. As well as this, we have built word embeddings using the Word2vec method with a CBOW and skip gram models with different vector sizes on a large corpus composed of user questions. Our another investigation is the effect of using different Word2vec pre-trained word embeddings on these deep learning architectures. Experiment results show that the use of different Word2vec models has a significant impact on the accuracy rate on different deep learning models. Additionally, there is no Turkish question dataset labeled and so another contribution in this study is that we introduce new Turkish question dataset which is translated from UIUC English question dataset. By using these techniques, we have reached an accuracy of 94% on the question dataset.																	0941-0643	1433-3058				APR	2020	32	7			SI		2909	2928		10.1007/s00521-020-04725-w													
J								An admission control algorithm based on matching game and differentiated service in wireless mesh networks	NEURAL COMPUTING & APPLICATIONS										Mesh network; Game theory; Admission control; Resource allocation	QOS	With the widespread application and popularity of wireless mesh networks, related resource allocation, load balancing and network security issues have attracted the attention of many researchers and scholars. The admission control strategy can effectively solve related problems such as network load balancing and network resource allocation. At present, the majority of admission control related research is mainly aimed at a single factor such as user or access points. Little researches consider both of them at the same time. However, admission control strategies often require decision making based on multiple factors. This paper takes the overall benefit of the game participants as the optimization goal, establishes a reasonable and effective admission control model and completes the new user access control and network resource allocation problems. According to the network's attribute and system resource allocation, this paper proposes an admission control model based on matching game and multi-attribute decision making. The algorithm proposed in this paper balances the interests of the network and user, reflects the superiority of the balanced decision of both parties, and guarantees the common interests of the network and the user.																	0941-0643	1433-3058				APR	2020	32	7			SI		2945	2962		10.1007/s00521-018-3751-3													
J								Fuzzy representation of finite-valued quantum gates	SOFT COMPUTING										Quantum gates; Continuous t-norms; Quantum logics		The theory of logical gates in quantum computation has inspired the development of new forms of quantum logic based on the following semantic idea: the meaning of a formula is identified with a quantum information quantity, represented by a density operator. At the same time, the logical connectives are interpreted as operations defined in terms of quantum gates. In this framework, some possible relations between fuzzy representations based on continuous t-norms for quantum gates and the probabilistic behavior of quantum computational finite-valued connectives are investigated. In particular, a fuzzy-type representation for quantum many-valued extensions of the gates introduced by Toffoli, Fredkin and Peres is described.																	1432-7643	1433-7479				JUL	2020	24	14					10305	10313		10.1007/s00500-020-04870-3		APR 2020											
J								Fuzzy optimal control for nonlinear systems with time-varying delay via sampled-data controller	SOFT COMPUTING										Fuzzy system; Time delay; Sampled-data control; Optimal control	H-INFINITY CONTROL; EXPONENTIAL STABILIZATION; FEEDBACK; DESIGN	This work is devoted to a fuzzy sampled-data optimal control problem for nonlinear systems with time-varying delay in Takagi-Sugeno fuzzy form. Based on the Lyapunov-Krasovskii functional theory, some novel stability criteria are established. A fuzzy sampled-data controller is developed to ensure that the fuzzy closed-loop sampled-data control system is asymptotically stable and the guaranteed cost performance is also minimized. It is shown that the obtained results are less conservative in the stability analysis. Two examples of the computer-simulated truck-trailer system and the continuous stirred tank reactor system are provided to show the effectiveness and the merits of the proposed design.																	1432-7643	1433-7479				OCT	2020	24	19					14743	14755		10.1007/s00500-020-04827-6		APR 2020											
J								Semantics and algorithms for trustworthy commitment achievement under model uncertainty	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Commitment semantics; Model uncertainty; Constrained planning; Sequential decision making	SOCIAL COMMITMENTS	We focus on how an agent can exercise autonomy while still dependably fulfilling commitments it has made to another, despite uncertainty about outcomes of its actions and how its own objectives might evolve. Our formal semantics treats a probabilistic commitment as constraints on the actions an autonomous agent can take, rather than as promises about states of the environment it will achieve. We have developed a family of commitment-constrained (iterative) lookahead algorithms that provably respect the semantics, and that support different tradeoffs between computation and plan quality. Our empirical results confirm that our algorithms' ability to balance (selfish) autonomy and (unselfish) dependability outperforms optimizing either alone, that our algorithms can effectively handle uncertainty about both what actions do and which states are rewarding, and that our algorithms can solve more computationally-demanding problems through judicious parameter choices for how far our algorithms should lookahead and how often they should iterate.																	1387-2532	1573-7454				APR	2020	34	1							19	10.1007/s10458-020-09443-0													
J								Spectral-spatial Classification of Hyperspectral Images Using Signal Subspace Identification and Edge-preserving Filter	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Hyperspectral image; remote sensing; the hyperspectral signal subspace identification (HYSIME); edge-preserving filter; classification; support vector machine		Hyperspectral images in remote sensing include hundreds of spectral bands that provide valuable information for accurately identify objects. In this paper, a new method of classifying hyperspectral images using spectral spatial information has been presented. Here, using the hyperspectral signal subspace identification (HYSIME) method which estimates the signal and noise correlation matrix and selects a subset of eigenvalues for the best representation of the signal subspace in order to minimize the mean square error, subsets from the main sample space have been extracted. After subspace extraction with the help of the HYSIME method, the edge-preserving filtering (EPF), and classification of the hyperspectral subspace using a support vector machine (SVM), results were then merged into the decision-making level using majority rule to create the spectral-spatial classifier. The simulation results showed that the spectral-spatial classifier presented leads to significant improvement in the accuracy and validity of the classification of Indiana, Pavia and Salinas hyperspectral images, such that it can classify these images with 98.79%, 98.88% and 97.31% accuracy, respectively.																	1476-8186	1751-8520				APR	2020	17	2					222	232		10.1007/s11633-019-1188-5													
J								Localization and Classification of Rice-grain Images Using Region Proposals-based Convolutional Neural Network	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Mask region-based convolutional neural networks (R-CNN); computer vision; deep learning; rice grain classification; transfer learning		This paper proposes a solution to localization and classification of rice grains in an image. All existing related works rely on conventional based machine learning approaches. However, those techniques do not do well for the problem designed in this paper, due to the high similarities between different types of rice grains. The deep learning based solution is developed in the proposed solution. It contains pre-processing steps of data annotation using the watershed algorithm, auto-alignment using the major axis orientation, and image enhancement using the contrast-limited adaptive histogram equalization (CLAHE) technique. Then, the mask region-based convolutional neural networks (R-CNN) is trained to localize and classify rice grains in an input image. The performance is enhanced by using the transfer learning and the dropout regularization for overfitting prevention. The proposed method is validated using many scenarios of experiments, reported in the forms of mean average precision (mAP) and a confusion matrix. It achieves above 80% mAP for main scenarios in the experiments. It is also shown to perform outstanding, when compared to human experts.																	1476-8186	1751-8520				APR	2020	17	2					233	246		10.1007/s11633-019-1207-6													
J								Tracking Registration Algorithm for Augmented Reality Based on Template Tracking	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Tracking registration; augmented reality; markerless; random ferns; Lucas-Kanade (LK) optical flow	FEATURES	Tracking registration is a key issue in augmented reality applications, particularly where there are no artificial identifier placed manually. In this paper, an efficient markerless tracking registration algorithm which combines the detector and the tracker is presented for the augmented reality system. We capture the target images in real scenes as template images, use the random ferns classi- fier for target detection and solve the problem of reinitialization after tracking registration failures due to changes in ambient lighting or occlusion of targets. Once the target has been successfully detected, the pyramid Lucas-Kanade (LK) optical flow tracker is used to track the detected target in real time to solve the problem of slow speed. The least median of squares (LMedS) method is used to adaptively calculate the homography matrix, and then the three-dimensional pose is estimated and the virtual object is rendered and registered. Experimental results demonstrate that the algorithm is more accurate, faster and more robust.																	1476-8186	1751-8520				APR	2020	17	2					257	266		10.1007/s11633-019-1198-3													
J								Modeling of a Smart Nano Force Sensor Using Finite Elements and Neural Networks	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Nano force; sensor; carbon nanotube (CNT); finite elements; neural network		The aim of this work is to model and analyze the behavior of a new smart nano force sensor. To do so, the carbon nanotube has been used as a suspended gate of a metal-oxide-semiconductor field-effect transistor (MOSFET). The variation of the applied force on the carbon nanotube (CNT) generates a variation of the capacity of the transistor oxide-gate and therefore the variation of the threshold voltage, which allows the MOSFET to become a capacitive nano force sensor. The sensitivity of the nano force sensor can reach 0.124 31 V/nN. This sensitivity is greater than results in the literature. We have found through this study that the response of the sensor depends strongly on the geometric and physical parameters of the CNT. From the results obtained in this study, it can be seen that the increase in the applied force increases the value of the MOSFET threshold voltage V-Th. In this paper, we first used artificial neural networks to faithfully reproduce the response of the nano force sensor model. This neural model is called direct model. Then, secondly, we designed an inverse model called an intelligent sensor which allows linearization of the response of our developed force sensor.																	1476-8186	1751-8520				APR	2020	17	2					279	291		10.1007/s11633-018-1155-6													
J								Image Encryption Algorithm Based on Compressive Sensing and Fractional DCT via Polynomial Interpolation	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Compressive sensing; fractional discrete cosine transform (DCT) via polynomial interpolation; image encryption; three-dimensional piecewise and nonlinear chaotic maps; real-valued output	TRANSFORM; SCHEME; PERMUTATION; DECOMPOSITION; ROBUST; MAP	Based on compressive sensing and fractional discrete cosine transform (DCT) via polynomial interpolation (PI-FrDCT), an image encryption algorithm is proposed, in which the compression and encryption of an image are accomplished simultaneously. It can keep information secret more effectively with low data transmission. Three-dimensional piecewise and nonlinear chaotic maps are employed to obtain a generating sequence and the exclusive OR (XOR) matrix, which greatly enlarge the key space of the encryption system. Unlike many other fractional transforms, the output of PI-FrDCT is real, which facilitates the storage, transmission and display of the encrypted image. Due to the introduction of a plain-image-dependent disturbance factor, the initial values and system parameters of the encryption scheme are determined by cipher keys and plain-image. Thus, the proposed encryption scheme is very sensitive to the plain-image, which makes the encryption system more secure. Experimental results demonstrate the validity and the reliability of the proposed encryption algorithm.																	1476-8186	1751-8520				APR	2020	17	2					292	304		10.1007/s11633-018-1159-2													
J								Optimal power flow incorporating renewable uncertainty related opportunity costs	COMPUTATIONAL INTELLIGENCE										Monte-Carlo simulation; opportunity cost; optimal power flow; particle swarm optimization; renewable uncertainty	DISPATCH; GENERATION; RISK	In this paper, an optimal power flow solution method incorporating a cost model that associates the uncertainty-related expense incurred with the use of renewable energy sources, viz., solar and wind, is demonstrated. Wind speed and solar radiation are assumed to follow Weibull and normal distributions and the uncertainty is simulated using Monte-Carlo approach. Wind turbine mathematical model is used to estimate the wind generator output, while the same for solar PV is estimated using PV-inverter models. The uncertainty-induced opportunity cost for both the renewable sources is composed of the costs due to both power excess and deficit. These cost components are indicative of the reserve requirement and loss of benefit, due to the unavailability of the corresponding generation. This research models and integrates the opportunity costs of renewable generation into a conventional OPF formulation, which is then solved using four variants of particle swarm optimization method. Among these, mutation-based PSO approach provided better results than others. The test system used is modified IEEE 39-bus network and the performance of the method as well as the effect of the uncertainty cost is evaluated under multiple renewable penetration levels. The results also indicate that solar generation is preferred over wind in terms of the uncertainty cost, while the use of stochastic natured renewable systems is economically justified and preferred over thermal generators.																	0824-7935	1467-8640															10.1111/coin.12316		APR 2020											
J								Designing a multi-agent system architecture for managing distributed operations within cloud manufacturing	EVOLUTIONARY INTELLIGENCE										Distributed manufacturing; Cyber-physical production system; Intelligent capability management		Cloud manufacturing (CM) is a challenging scenario in the fourth stage of industrial production (i.e. Industry 4.0). In this context, the fusion of physical and virtual worlds in cyber-physical production systems transforms manufacturing resources into homogeneous services that can be shared and distributed in collaborative environments. CM systems are characterized by intelligent capability management and manufacturing cloud service-management. An interesting research topic in these areas is the production planning with a decentralized pool of homogeneous resources. The distributed Task Scheduling Problem in CM has been partially tackled in the current literature, but some issues, such as the dynamic task arrival, the downtime of machines, the anomalous tasks identification, have not been addressed. Armed with such a vision, we discuss the design of a multi-agent system for managing and monitoring homogeneous manufacturing services in a CM system based on Additive Manufacturing Technologies.																	1864-5909	1864-5917															10.1007/s12065-020-00390-z		APR 2020											
J								Neural network-based fault-tolerant control approach considering a submarine system	EVOLVING SYSTEMS										Fault-tolerant control approach; Submarine system; Radial basis function neural network	NONLINEAR-SYSTEMS; DESIGN; IDENTIFICATION; MODELS	The stability and satisfactory performance of a fault-tolerant control approach is important for designers and therefore the corresponding closed-loop systems are essential to be dealt with. Considering one of the most important faults including the operator fault in robotic systems leads us to reach the better performance, usually. The radial basis function neural network-based fault-tolerant control method is designed in this research to estimate the model uncertainties, the noise affecting the system, and the operator fault, in order to compensate this one. The results of simulating the fault-tolerant control system propose that this technique successfully secured the tracking function of the control system besides ensuring closed-loop stability. Since the traditional control methods fail to effectively fix the aforementioned problem, a new method so-called neural-network-based fault-tolerant control is proposed. This study proposes the design of a nominal controller, its examinations under operator faults, and finally the expansion of the controller into a fault-tolerant controller. In a word, the contribution made in the research over the state-of-the-art materials focusing on fault-tolerant controls designed in the area of submarine systems is briefly taken into real consideration as its nonlinearity, its fixed structure, the lack of need for switching systems, resistance to noise, and the ability to compensate the effect of the performance drop fault on the collective fault of the operators. Subsequently, the simulation results verified the effectiveness of the proposed approach in coping with the operator's performance faults, tangibly.																	1868-6478	1868-6486															10.1007/s12530-020-09338-1		APR 2020											
J								An adaptive stacked hourglass network with Kalman filter for estimating 2D human pose in video	EXPERT SYSTEMS										human detection; human pose estimation; Kalman filter; outlier detection	RECOGNITION	One of the main challenges in computer science and image processing is 2D human pose estimation. Specifically, occlusion and in particular occlusion of human joints caused by camera angle are of paramount importance. In this paper, a new highly accurate network was proposed that can estimate 2D human poses in video images using deep learning. We employ the Single Shot MultiBox Detector network to detect the centre position of each human within a video frame and then use the stacked hourglass network to estimate the 2D human pose. We approximate the human motion as a linear motion between different frames in a certain period; and optimize the human centres based on the local outlier factor and Kalman filters. The same method is applied to optimize the human pose estimations in video, which can address the inaccurate prediction caused by human joints occlusion. The proposed adaptive network is tested using the two well-known benchmarks for human pose estimation (MPII and Joint-annotated Human Motion Data Base datasets), and we also generate some 2D human pose estimating qualitative results of single and multiple people in Internet videos. The experimental results show that the proposed network has strong practicability and can achieve high accuracy on adaptive estimating the 2D human pose in video.																	0266-4720	1468-0394														e12552	10.1111/exsy.12552		APR 2020											
J								Binocular Vision Object Positioning Method for Robots Based on Coarse-fine Stereo Matching	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Object positioning; stereo matching; random fern; normalized cross correlation; binocular vision model		In order to improve the low positioning accuracy and execution efficiency of the robot binocular vision, a binocular vision positioning method based on coarse-fine stereo matching is proposed to achieve object positioning. The random fern is used in the coarse matching to identify objects in the left and right images, and the pixel coordinates of the object center points in the two images are calculated to complete the center matching. In the fine matching, the right center point is viewed as an estimated value to set the search range of the right image, in which the region matching is implemented to find the best matched point of the left center point. Then, the similar triangle principle of the binocular vision model is used to calculate the 3D coordinates of the center point, achieving fast and accurate object positioning. Finally, the proposed method is applied to the object scene images and the robotic arm grasping platform. The experimental results show that the average absolute positioning error and average relative positioning error of the proposed method are 8.22 mm and 1.96% respectively when the object's depth distance is within 600 mm, the time consumption is less than 1.029 s. The method can meet the needs of the robot grasping system, and has better accuracy and robustness.																	1476-8186	1751-8520				AUG	2020	17	4					562	571		10.1007/s11633-020-1226-3		APR 2020											
J								Fuzzy Risk Assessment Based on Interval Numbers and Assessment Distributions	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Risk assessment; Distribution assessment; Symbolic proportion; Fuzzy numbers; Interval numbers	LINGUISTIC DISTRIBUTION ASSESSMENTS; REPRESENTATION MODEL; DECISION-MAKING; SAFETY; WEIGHTS	In risk assessment problems, multiple experts are often involved. On many occasions, assessment experts cannot give crisp scores even if there are scoring standards because of limitation and uncertainty. It is reasonable that the assessment data are expressed in the form of interval numbers with self-confidence. A fuzzy risk assessment model based on interval numbers with self-confidence is proposed in this paper. First, a multi-expert interval numbers with self-confidence fusion model is constructed. In the model, experts weights are determined based on subjective weights and objective weights, and the objective weights are calculated based on the length of the base of interval number and self-confidence simultaneously. Second, a novel method determining the symbolic proportion in an assessment distribution is proposed. This method integrates the concept of similarity measure between generalized fuzzy numbers and the length of the base of intersection of fuzzy numbers. Some properties of the proposed symbolic proportion measure are proved. Third, a fuzzy risk assessment model is proposed based on the fuzzy inference system. The output of the model is a distribution with the possible risk ranks and corresponding symbolic proportions. Finally, an illustrative example which shows the proposed fuzzy risk assessment model effective is demonstrated.																	1562-2479	2199-3211				JUN	2020	22	4					1142	1157		10.1007/s40815-020-00837-6		APR 2020											
J								Traffic Signal Control Using Genetic Decomposed Fuzzy Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Intersection signal; Fuzzy control; Decomposed Fuzzy Systems; Genetic Algorithm	INTERSECTION	In this paper, Decomposed Fuzzy Systems (DFS) structure is applied to design single intersection signal fuzzy controller. The DFS structure is to decompose each fuzzy variable into layers of fuzzy systems and each layer is to characterize one traditional fuzzy set. DFS adjusts the fuzzy membership function, the leading part, and enriches the fuzzy rule base through structural changes, thus provides the system with more adjustable parameters to facilitate possible adaptation in fuzzy rules, but without introducing a learning burden. It also can be found that the function approximation capability of the DFS is much better than that of the traditional fuzzy systems. At the same time, in order to solve possible defects brought by expert experience, Genetic Algorithm (GA) is applied to the optimization of DFS rule base in this paper. Taking the four-phase single intersection as a case study, an intersection signal control algorithm is obtained using the proposed DFS based on Genetic Algorithm (G-DFS). Simulation results show that the G-DFS controller reduces average vehicle delay, queuing length, average parking rate, and average vehicle travel time effectively, and the controller can smoothly adapt to different traffic flow changes.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1939	1947		10.1007/s40815-020-00840-x		APR 2020											
J								E health care data privacy preserving efficient file retrieval from the cloud service provider using attribute based file encryption	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computation complexity; Communication complexity; Attributes; Privacy; Cloud computing; Attacks; Key computation; Index building	PROXY RE-ENCRYPTION; ACCESS-CONTROL; SECURE	File storing and retrieving is performed in the robust as well as secure manner by using the cloud computing technology. Various researchers have developed numerous mechanisms via attribute based encryption for the health care applications. Although, more protocols developed among them only very few techniques were efficient and robust for the quick retrieval of reports from the cloud but many protocols suffer by reason of less security, confidentiality and integrity. Existing techniques was based on encrypting the file based on the keyword. But in our proposed protocol, we have developed an attribute based encryption which will overcome the issues faced by the previous research techniques. The group of patient records are encrypted with single common attribute. From the survey, it is clear that the existing protocols suffer due to high computation and communication complexity. So as to rectify the existing issue, we proposed the effective recovery of files by using attribute based file encryption mechanism from cloud (ERFC). When comparing to the existing protocols, our proposed ERFC mechanism takes minimum computation and communication complexity for four working mechanisms namely, patient key computation, doctor index building computation, cloud working mechanism and finally patient report decryption. All these four working mechanisms are developed for effective recovery of files to the end users. Our proposed protocol is secure against some attacks like Eavesdropping, masquerade, replay and man in the middle attack. Our performance analysis section describes that our ERFC mechanism is better with communication as well as computation complexity when related to the other existing protocols.																	1868-5137	1868-5145															10.1007/s12652-020-01911-5		APR 2020											
J								Load balanced clustering scheme using hybrid metaheuristic technique for mobile sink based wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Clustering; Mobile sink; Artificial bee colony; Differential evolution; WSN	NODE PLACEMENT; STATION; ALGORITHM; LIFETIME; PROTOCOL	Fundamental design goal of a typical wireless sensor network is to optimize energy consumption. Recent studies have confirmed that node clustering mechanism efficiently utilizes energy resource of the network by organizing nodes into a set of clusters and helps in extending the network lifetime. Most of the existing node clustering schemes suffers from non-uniform distribution of cluster heads, unbalanced load problem among clusters and left-out node issues. In order to solve these issues, we have focused on to design a load-balanced clustering scheme which also resolves the left-out nodes problem. This study proposes a hybrid meta-heuristic technique where best features of Artificial Bee Colony and Differential Evolution are combined to evaluate the best set of load-balanced cluster heads. For energy efficient and load-balanced clustering, a novel objective function is derived based on average energy, intra-cluster distance and delay parameters. Following this, Artificial Bee Colony based meta-heuristic algorithm is proposed for the dynamic re-localization of the mobile sink within a cluster-based network infrastructure. Performance comparison of the proposed scheme with the existing three well known schemes is evaluated under different network scenarios. Simulation results validate that the proposed scheme performs better in terms of average energy consumption, total energy consumption, residual energy, and network lifetime.																	1868-5137	1868-5145															10.1007/s12652-020-01909-z		APR 2020											
J								A Formalized General Theory of Syntax with Bindings: Extended Version	JOURNAL OF AUTOMATED REASONING										Syntax with bindings; Recursion and induction principles; Isabelle; HOL	RECURSION; SYSTEM	We present the formalization of a theory of syntax with bindings that has been developed and refined over the last decade to support several large formalization efforts. Terms are defined for an arbitrary number of constructors of varying numbers of inputs, quotiented to alpha-equivalence and sorted according to a binding signature. The theory contains a rich collection of properties of the standard operators on terms, including substitution, swapping and freshness-namely, there are lemmas showing how each of the operators interacts with all the others and with the syntactic constructors. The theory also features induction and recursion principles and support for semantic interpretation, all tailored for smooth interaction with the bindings and the standard operators.																	0168-7433	1573-0670				APR	2020	64	4					641	675		10.1007/s10817-019-09522-2													
J								Homogeneous Length Functions on Groups: Intertwined Computer and Human Proofs	JOURNAL OF AUTOMATED REASONING										Type theory; Homotopy type theory; Geometric group theory		We describe a case of an interplay between human and computer proving which played a role in the discovery of an interesting mathematical result (Fritz et al. in Algebra Number Theory 12:1773-1786, 2018). The unusual feature of the use of computers here was that a computer generated but human readable proof was read, understood, generalized and abstracted by mathematicians to obtain the key lemma in an interesting mathematical result.																	0168-7433	1573-0670				APR	2020	64	4					677	688		10.1007/s10817-019-09523-1													
J								Automated Reasoning with Power Maps	JOURNAL OF AUTOMATED REASONING										Torsion-free groups; First-order logic; Power maps; n-abelian semigroups; Cancellation laws		In this paper, we employ automated deduction techniques to prove and generalize some well-known theorems in group theory that involve power maps x(n). The difficulty lies in the fact that the term x(n) cannot be expressed in the syntax of first-order logic when n is an integer variable. Here we employ a new concept of "power-like functions" by extracting relevant equational properties valid for all power functions and implement these equational rules in Prover9, a first-order theorem prover. We recast the original theorems and prove them in this new context of power-like functions. Consequently these first-order proofs remain valid for all n but the length and complexity of the proofs remain constant independent of the value of n. To give an example, it is well-known (Baer in Proc Am Math Soc 4:15-26, 1953, Alperin in Can J Math 21:1238-1244 1969) that every torsion-free group in which the power map f ( x) = x(n) is an endomorphism is abelian. Here we show that every torsion-free group in which a power-like map is an endomorphism is, indeed, abelian. Also, we generalize similar theorems from groups to a class of cancellative semigroups, and once again, Prover9 happily proves all these new generalizations as well.																	0168-7433	1573-0670				APR	2020	64	4					689	697		10.1007/s10817-019-09524-0													
J								A Verified Implementation of the Berlekamp-Zassenhaus Factorization Algorithm	JOURNAL OF AUTOMATED REASONING										Factor bounds; Hensel lifting; Isabelle; HOL; Local type definitions; Polynomial factorization; Theorem proving	FACTORING POLYNOMIALS	We formally verify the Berlekamp-Zassenhaus algorithm for factoring square-free integer polynomials in Isabelle/HOL. We further adapt an existing formalization of Yun's squarefree factorization algorithm to integer polynomials, and thus provide an efficient and certified factorization algorithm for arbitrary univariate polynomials. The algorithm first performs factorization in the prime field GF( p) and then performs computations in the ring of integers modulo p(k), where both p and k are determined at runtime. Since a natural modeling of these structures via dependent types is not possible in Isabelle/HOL, we formalize the whole algorithm using locales and local type definitions. Through experiments we verify that our algorithm factors polynomials of degree up to 500 within seconds.																	0168-7433	1573-0670				APR	2020	64	4					699	735		10.1007/s10817-019-09526-y													
J								Formalizing the Cox-Ross-Rubinstein Pricing of European Derivatives in Isabelle/HOL	JOURNAL OF AUTOMATED REASONING										Proof assistants; Financial mathematics; Discrete pricing; Isabelle; HOL		We formalize in the proof assistant Isabelle essential basic notions and results in financial mathematics. We provide generic formal definitions of concepts such as markets, portfolios, derivative products, arbitrages or fair prices, and we show that, under the usual no-arbitrage condition, the existence of a replicating portfolio for a derivative implies that the latter admits a unique fair price. Then, we provide a formalization of the Cox-Rubinstein model and we show that the market is complete in this model, i.e., that every derivative product admits a replicating portfolio. This entails that in this model, every derivative product admits a unique fair price. In addition, we provide Isabelle functions to compute the fair price of some derivative products.																	0168-7433	1573-0670				APR	2020	64	4					737	765		10.1007/s10817-019-09528-w													
J								An Assertional Proof of Red-Black Trees Using Dafny	JOURNAL OF AUTOMATED REASONING										Data structures; Balanced trees; Verification platforms		Red-black trees are convenient data structures for inserting, searching, and deleting keys with logarithmic costs. However, keeping them balanced requires careful programming, and sometimes to deal with a high number of cases. In this paper, we present a functional version of a red-black tree variant called left-leaning, due to R. Sedgewick, which reduces the number of cases to be dealt with to a few ones. The code is rather concise, but reasoning about its correctness requires a rather large effort. We provide formal preconditions and postconditions for all the functions, prove their termination, and that the code satisfies its specifications. The proof is assertional, and consists of interspersing enough assertions among the code in order to help the verification tool to discharge the proof obligations. We have used the Dafny verification platform, which provides the programming language, the assertion language, and the verifier. To our knowledge, this is the first assertional proof of this data structure, and also one of the few ones including deletion.																	0168-7433	1573-0670				APR	2020	64	4					767	791		10.1007/s10817-019-09534-y													
J								Multi-level complexity reduction for HEVC multiview coding	JOURNAL OF REAL-TIME IMAGE PROCESSING										HEVC; H; 265; Multiview; Motion and density estimation; Quantization; Coding tree unit; GPU	GPU-ACCELERATED MOTION; VIDEO; EFFICIENCY; SEARCH; EXTENSIONS; ALGORITHM	Standardized in 2014, multiview extension of high efficiency video coding (MV-HEVC) offers significantly better compression performance of up to 50% for multiview and 3D videos compared to multiple independent single view HEVC coding. However, the extreme high computational complexity of MV-HEVC demands significant optimization of the encoder. In this work, we propose a series of optimization techniques at various levels of abstraction: non-aggregation massively parallel motion estimation (ME) and disparity estimation (DE) for prediction units, fractional and bidirectional ME/DE, quantization parameter-based early termination of coding tree unit (CTU), and optimized resource-scheduled wave front parallel processing for CTU. When evaluated over three views for all available official multiview video coding test sequences, proposed optimization outperforms the anchor encoder by average factor of 5.4 at the cost of 4.4% bitrate (DBR) increase at no loss in PSNR, or alternatively a PSNR degradation of 0.12 dB at no change to the DBR.																	1861-8200	1861-8219				APR	2020	17	2					197	213		10.1007/s11554-018-0757-0													
J								An FPGA accelerator for PatchMatch multi-view stereo using OpenCL	JOURNAL OF REAL-TIME IMAGE PROCESSING										PatchMatch; Multi-view stereo (MVS); 3D reconstruction; OpenCL for FPGA; Reconfigurable computing		PatchMatch multi-view stereo (MVS) is one method generating depth maps from multi-view images and is expected to be used for various applications such as robot vision, 3D measurement, and 3D reconstruction. The major drawback of PatchMatch MVS is its large computational amount, and its acceleration is strongly desired. However, this acceleration is prevented by two problems. First, though PatchMatch MVS estimates depth maps by propagating estimation results among neighbor pixels, it is not suitable for GPU-based acceleration. Second, since the shape of a matching window used for stereo matching is changed dynamically, reading its pixels is inefficient in memory access. This paper proposes an FPGA accelerator exploiting on-chip FIFOs efficiently to solve the propagation problem. Moreover, reading pixels of a matching window is improved by a cover window which has the fixed shape and covers the matching window. The FPGA accelerator is designed using a design tool based on Open Computing Language (OpenCL). Although parameters of PatchMatch MVS depend on object images, these parameters can be changed easily by the OpenCL-based design. The experimental results demonstrate that the FPGA implementation achieves 3.4 and 2.2 times faster processing speeds than the CPU and GPU ones, respectively, and the power-delay product of the FPGA implementation is 3.2 and 5.7% of the CPU and GPU ones, respectively.																	1861-8200	1861-8219				APR	2020	17	2					215	227		10.1007/s11554-017-0745-9													
J								Toward reliable experiments on the performance of Connected Components Labeling algorithms	JOURNAL OF REAL-TIME IMAGE PROCESSING										Connected Components Labeling; Benchmarking; Performance Evaluation		The problem of labeling the connected components of a binary image is well defined, and several proposals have been presented in the past. Since an exact solution to the problem exists, algorithms mainly differ on their execution speed. In this paper, we propose and describe YACCLAB, Yet Another Connected Components Labeling Benchmark. Together with a rich and varied dataset, YACCLAB contains an open source platform to test new proposals and to compare them with publicly available competitors. Textual and graphical outputs are automatically generated for many kinds of tests, which analyze the methods from different perspectives. An extensive set of experiments among state-of-the-art techniques is reported and discussed.																	1861-8200	1861-8219				APR	2020	17	2					229	244		10.1007/s11554-018-0756-1													
