PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								An E-Assessment Methodology Based on Artificial Intelligence Techniques to Determine Students' Language Quality and Programming Assignments' Plagiarism	INTELLIGENT AUTOMATION AND SOFT COMPUTING										Electronic-Assessment; Machine Learning; Artificial Intelligence; WordNet; Technology Enhanced Assessment; Semantic Similarity		This research aims to an electronic assessment (e-assessment) of students' replies in response to the standard answer of teacher's question to automate the assessment by WordNet semantic similarity. For this purpose, a new methodology for Semantic Similarity through WordNet Semantic Similarity Techniques (SS-WSST) has been proposed to calculate semantic similarity among teacher' query and student's reply. In the pilot study-1 42 words' pairs extracted from 8 students' replies, which marked by semantic similarity measures and compared with manually assigned teacher's marks. The teacher is provided with 4 bins of the mark while our designed methodology provided an exact mcamre of marks. Secondly, the source codes plagiarism in students' assignments provide smart e-assessment. The WordNet semantic similarity techniques are used to investigate source code plagiarism in binary search and stack data structures programmed in C++, Java, C# respectively.																	1079-8587	2326-005X				MAR	2020	26	1					169	180		10.31209/2019.100000138													
J								Emotion-Based Painting Image Display System	INTELLIGENT AUTOMATION AND SOFT COMPUTING										Aesthetic analysis; Arousal-Valence; Attributes of Paintings; Emotional care; Emotion of Paintings; Internet of Things	VALENCE	As mobile devices have tremendously developed, people can now get sensor data easily. These data are not only physical data such as temperature, humidity, gravity, acceleration, etc. but also human health data such as blood pressure, heart pulse rate, etc. With this information, Internet of Things (IoT) technology has provided many systems to support human health care. Systems for human health care support physical health care like checking blood pressure, pulse rate, etc. However, the demand for physical health care as well as mental health care is increasing. So, a system, which automatically recommends a painting to users based on their feeling, is proposed in this paper. Using a smartphone application, users take a self-portrait. Then, the application reads the user's facial expression, and obtains an Arousal-Valence (A.V.) emotion value. Also, the application has a database of paintings with A.V. value in advance. To create this database, we extracted many features from various paintings and estimated their A.V. value using regression analysis. When users reach home, the application detects it automatically using GPS information, and shows the painting that best suits the user's emotion, based on the extracted A.V. value. Thereby, users can get a feeling of relaxation by admiring the painting.																	1079-8587	2326-005X				MAR	2020	26	1					181	192		10.31209/2019.100000139													
J								Weighted or Non-Weighted Negative Tree Pattern Discovery from Sensor-Rich Environments	INTELLIGENT AUTOMATION AND SOFT COMPUTING										Negative association rules; pruning constraints; tree-structured items; weighted tree items	ASSOCIATION RULES; DATABASE	It seems to be sure that the IoT is one of promising potential topics today. Sensors are the one that lead the current IoT revolution. The advances of sensor-rich environments produce the massive volume of raw data that is enlarging faster than the rate at which it is being handled. JSON is a lightweight data-interchange format and preferred for IoT applications. Before JSON, XML was de factor standard format for interchanging data. The common point is that their structure scheme is the tree. Tree structure provides data exchangeability and heterogeneity, which encourages user-flexibilities. Therefore, JSON sensor format is an easy to use human readable format for storing and transmitting sensor values. However, it is more challenging than ever to discover valuable and hidden information from the continuously generated tree-structured data. In the paper, we define and suggest an original method to predict and evaluate from the tree-structured sensing data.																	1079-8587	2326-005X				MAR	2020	26	1					193	204		10.31209/2019.100000140													
J								Detecting Outlier Behavior of Game Player Players Using Multimodal Physiology Data	INTELLIGENT AUTOMATION AND SOFT COMPUTING										Physiology multimodal system; Game behavior analysis	BRAIN-COMPUTER INTERFACE	This paper describes an outlier detection system based on a multimodal physiology data clustering algorithm in a PC gaming environment. The goal of this system is to provide information on a game player's abnormal behavior with a bio-signal analysis. Using this information, the game platform can easily identify players with abnormal behavior in specific events. To do this, we propose a mouse device that measures the wearer's skin conductivity, temperature, and motion. We also suggest a Dynamic Time Warping (DTW) based clustering algorithm. The developed system examines the biometric information of 50 players in a bullet dodge game. This paper confirms that a mouse coupled with a physiology multimodal system is useful for detecting outlier behavior of game players in a non-intrusive way.																	1079-8587	2326-005X				MAR	2020	26	1					205	214		10.31209/2019.100000141													
J								Document analysis systems that improve with use	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Interactive document analysis; Adaptive classification; Style-constrained recognition; Camera-based OCR; Memex; Lifetime reader	RECOGNIZE PATTERNS; FONT RECOGNITION; CLASSIFICATION; CONTEXT; EQUALIZATION; MACHINE; MODELS; OCR	Document analysis tasks for which representative labeled training samples are available have been largely solved. The next frontier is coping with hitherto unseen formats, unusual typefaces, idiosyncratic handwriting and imperfect image acquisition. Adaptive and style-constrained classification methods can overcome some expected variability, but human intervention will remain necessary in many tasks. Interactive pattern recognition includes data exploration and active learning as well as access to stored documents. The principle of "green interaction" is to make use of every intervention to reduce the likelihood that the automated system will make the same mistake again and again. Some of these techniques may pop up in forthcoming personal camera-based memex-like applications that will have a far broader range of input documents and scene text than the current, successful but highly specialized, systems for patents, postal addresses, bank checks and books.																	1433-2833	1433-2825				MAR	2020	23	1					13	29		10.1007/s10032-019-00344-x													
J								Total-Text: toward orientation robustness in scene text detection	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Curved text; Scene text detection		At present, text orientation is not diverse enough in the existing scene text datasets. Specifically, curve-orientated text is largely out-numbered by horizontal and multi-oriented text, hence, it has received minimal attention from the community so far. Motivated by this phenomenon, we collected a new scene text dataset, Total-Text, which emphasized on text orientations diversity. It is the first relatively large scale scene text dataset that features three different text orientations: horizontal, multi-oriented, and curve-oriented. In addition, we also study several other important elements such as the practicality and quality of ground truth, evaluation protocol, and the annotation process. We believe that these elements are as important as the images and ground truth to facilitate a new research direction. Secondly, we propose a new scene text detection model as the baseline for Total-Text, namely Polygon-Faster-RCNN, and demonstrated its ability to detect text of all orientations. Images of Total-Text and its annotation are available at .																	1433-2833	1433-2825				MAR	2020	23	1					31	52		10.1007/s10032-019-00334-z													
J								A unified method for augmented incremental recognition of online handwritten Japanese and English text	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Online recognition; Handwriting recognition; Batch recognition; Incremental recognition	CLASSIFICATION; DIRECTION; DATABASE	We present a unified method to augmented incremental recognition for online handwritten Japanese and English text, which is used for busy or on-the-fly recognition while writing, and lazy or delayed recognition after writing, without incurring long waiting times. It extends the local context for segmentation and recognition to a range of recent strokes called "segmentation scope" and "recognition scope," respectively. The recognition scope is inside of the segmentation scope. The augmented incremental recognition triggers recognition at every several recent strokes, updates the segmentation and recognition candidate lattice, and searches over the lattice for the best result incrementally. It also incorporates three techniques. The first is to reuse the segmentation and recognition candidate lattice in the previous recognition scope for the current recognition scope. The second is to fix undecided segmentation points if they are stable between character/word patterns. The third is to skip recognition of partial candidate character/word patterns. The augmented incremental method includes the case of triggering recognition at every new stroke with the above-mentioned techniques. Experiments conducted on TUAT-Kondate and IAM online database show its superiority to batch recognition (recognizing text at one time) and pure incremental recognition (recognizing text at every input stroke) in processing time, waiting time, and recognition accuracy.																	1433-2833	1433-2825				MAR	2020	23	1					53	72		10.1007/s10032-019-00343-y													
J								Graph-matching-based correspondence search for nonrigid point cloud registration	COMPUTER VISION AND IMAGE UNDERSTANDING										Nonrigid registration; Graph matching; Point cloud; Mesh	TRANSFORMATION; OBJECTS	Nonrigid registration finds transformations to fit a source point cloud/mesh to a target point cloud/mesh. Most nonrigid registration algorithms consist of two steps; finding correspondence and optimization. Among these, finding correspondence plays an important role in registration performance. However, when two point clouds have large displacement, it is hard to know correct correspondences and an algorithm often fails to find correct transformations. In this paper, we propose a novel graph-matching-based correspondence search for nonrigid registration and a corresponding optimization method for finding transformation to complete nonrigid registration. Considering global connectivity as well as local similarity for the correspondence search, the proposed method finds good correspondences according to semantics and consequently finds correct transformations even when the motion is large. Our algorithm is experimentally validated on human body and animal datasets, which verifies that it is capable of finding correct transformations to fit a source to a target.																	1077-3142	1090-235X				MAR	2020	192								102899	10.1016/j.cviu.2019.102899													
J								Monocular human pose estimation: A survey of deep learning-based methods	COMPUTER VISION AND IMAGE UNDERSTANDING										Deep learning; Human pose estimation; Survey	HUMAN MOTION ANALYSIS; RECOGNITION; CAPTURE; MODEL; VIDEO	Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.																	1077-3142	1090-235X				MAR	2020	192								102897	10.1016/j.cviu.2019.102897													
J								Guess where? Actor-supervision for spatiotemporal action localization	COMPUTER VISION AND IMAGE UNDERSTANDING										Actor-supervision; Spatiotemporal action localization; Action understanding; Video analysis; Weakly-supervised		This paper addresses the problem of spatiotemporal localization of actions in videos. Compared to leading approaches, which all learn to localize based on carefully annotated boxes on training video frames, we adhere to a solution only requiring video class labels. We introduce an actor-supervised architecture that exploits the inherent compositionality of actions in terms of actor transformations, to localize actions. We make two contributions. First, we propose actor proposals derived from a detector for human and non-human actors intended for images, which are linked over time by Siamese similarity matching to account for actor deformations. Second, we propose an actor-based attention mechanism enabling localization from action class labels and actor proposals. It exploits a new actor pooling operation and is end-to-end trainable. Experiments on four action datasets show actor supervision is state-of-the-art for action localization from video class labels and is even competitive to some box-supervised alternatives.																	1077-3142	1090-235X				MAR	2020	192								102886	10.1016/j.cviu.2019.102886													
J								Automating RTI: Automatic light direction detection and correcting non-uniform lighting for more accurate surface normals	COMPUTER VISION AND IMAGE UNDERSTANDING										Reflectance transformation imaging; Photometric stereo; Image enhancement		Reflectance Transformation Imaging (RTI) (Malzbender et al., 2001) is a photometric stereo technique that enables the interactive relighting of the object of interest from novel lighting directions, and an estimation of surface topography through the calculation of surface normal vectors. We propose a novel, fully automated technique for correcting common lighting errors in RTI and markedly improve the accuracy of surface normal estimation, as well as increasing the legibility of low relief surface variations. This moves RTI from the qualitative domain (e.g. enabling the reading of weathered inscriptions) into the quantitative domain of computer vision. RTI assumes only light direction, and not received intensity, changes as the object is imaged. Like other authors we show that this assumption is false and propose a novel method to correct for it. However, we estimate the lighting directions automatically, unlike other proposed correction techniques. Our method also requires no calibration equipment, meaning it can be easily retrofitted to any existing stack of RTI photographs. We increase the simplicity of the standard highlight RTI method by automatically detecting lighting directions and maintain its appeal to non-imaging professionals.																	1077-3142	1090-235X				MAR	2020	192								102880	10.1016/j.cviu.2019.102880													
J								Photometric camera characterization from a single image with invariance to light intensity and vignetting	COMPUTER VISION AND IMAGE UNDERSTANDING											RADIOMETRIC CALIBRATION; COLOR	Photometric characterization of a camera entails describing how the camera transforms the light reaching its sensors into an image and how this image can be defined in a standard color space. Although the research in this area has been extensive, the current literature lacks practical methods designed for cameras operating under near light. There are two major application scenarios considered in this paper that would benefit from this type of approaches. Camera rigs for minimally-invasive-procedures cannot be calibrated in the operating room with the current methods. This comes from the fact that existing approaches need multiple images, assume uniform lighting, and/or use over-simplistic camera models, which does not allow for the calibration of near light setups in a fast and reliable way. The second scenario refers to the calibration of cellphone cameras, which currently cannot be calibrated at close range with a single image, specially if the flash is used, as there would be non-uniform lighting on the scene. In this work, we describe a method to characterize cameras from a single image of a known target. This enables both geometric and photometric calibrations to be performed on-the-fly without making assumptions on the vignetting nor on the spatial properties of the light. The presented method showed good repeatability and color accuracy even when compared to multiple-image approaches. Applications to laparoscopic cameras, generic cameras (such as cellphone cameras), and cameras other than trichromatic are shown to be viable.																	1077-3142	1090-235X				MAR	2020	192								102887	10.1016/j.cviu.2019.102887													
J								Learning feature aggregation in temporal domain for re-identification	COMPUTER VISION AND IMAGE UNDERSTANDING												Person re-identification is a standard and established problem in the computer vision community. In recent years, vehicle re-identification is also getting more attention. In this paper, we focus on both these tasks and propose a method for aggregation of features in temporal domain as it is common to have multiple observations of the same object. The aggregation is based on weighting different elements of the feature vectors by different weights and it is trained in an end-to-end manner by a Siamese network. The experimental results show that our method outperforms other existing methods for feature aggregation in temporal domain on both vehicle and person re-identification tasks. Furthermore, to push research in vehicle re-identification further, we introduce a novel dataset CarsReld74k. The dataset is not limited to frontal/rear viewpoints. It contains 17,681 unique vehicles, 73,976 observed tracks, and 277,236 positive pairs. The dataset was captured by 66 cameras from various angles.																	1077-3142	1090-235X				MAR	2020	192								102883	10.1016/j.cviu.2019.102883													
J								Cascade multi-head attention networks for action recognition	COMPUTER VISION AND IMAGE UNDERSTANDING										Action recognition; Cascade multi-head attention network; Feature aggregation; Visual analysis		Long-term temporal information yields crucial cues for video action understanding. Previous researches always rely on sequential models such as recurrent networks, memory units, segmental models, self-attention mechanism to integrate the local temporal features for long-term temporal modeling. Recurrent or memory networks record temporal patterns (or relations) by memory units, which are proved to be difficult to capture long-term information in machine translation. Self-attention mechanisms directly aggregate all local information with attention weights which is more straightforward and efficient than the former. However, the attention weights from self-attention ignore the relations between local information and global information which may lead to unreliable attention. To this end, we propose a new attention network architecture, termed as Cascade multi-head ATtention Network (CATNet), which constructs video representations with two-level attentions, namely multi-head local self-attentions and relation based global attentions. Starting from the segment features generated by backbone networks, CATNet first learns multiple attention weights for each segment to capture the importance of local features in a self-attention manner. With the local attention weights, CATNet integrates local features into several global representations, and then learns the second level attention for the global information by a relation manner. Extensive experiments on Kinetics, HMDB51, and UCF101 show that our CATNet boosts the baseline network with a large margin. With only RGB information, we respectively achieve 75.8%, 75.2%, and 96.0% on these three datasets, which are comparable or superior to the state of the arts.																	1077-3142	1090-235X				MAR	2020	192								102898	10.1016/j.cviu.2019.102898													
J								Soft Computing Modelling of Urban Evolution: Tehran Metropolis	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Artificial Neural Network; Geographic Information System; Urban Development Models; Soft Computing; Fuzzy Logic; Spatial Information Systems	C-MEANS; GROWTH; GIS	Exploring computational intelligence, geographic information systems and statistical information, a creative and innovative model for urban evolution is presented in this paper. The proposed model employs fuzzy logic and artificial neural network as forecasting tools for describing the urban growth. This dynamic urban evolution model considers the spatial data of population, as well as its time changes and the building usage patterns. For clustering the spatial features, fuzzy algorithms were implemented to represent different levels of urban growth and development. Then, these fuzzy clusters were modeled by the multi-layer neural networks to estimate the urban growth. Based on this novel intelligent model, the current state of development of Tehran's population and the future of this urban evolution were evaluated by empirical data and the achieved outcomes were detailed in qualitative charts. The input data-set includes four censuses with five-year intervals. Tehran's demographic evolution model forecasts the next five years with an overall accuracy of 81% and Cohen's kappa coefficient up to 74% beside the qualitative charts. These performance indicators are higher than the previous advanced models. The primary objective of this proposed model is to aid planners and decision makers to predict the development trend of urban population.																	1989-1660					MAR	2020	6	1					7	15		10.9781/ijimai.2019.03.001													
J								Automated ECG Analysis for Localizing Thrombus in Culprit Artery Using Rule Based Information Fuzzy Network	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Cardio-Vascular Diseases; ECG; Identification; Classification; Information Fuzzy Network	MYOCARDIAL-INFARCTION; MULTILEAD ECG; CLASSIFICATION; DISEASE; DIAGNOSIS; SIGNALS; ENERGY	Cardio-vascular diseases are one of the foremost causes of mortality in today's world. The prognosis for cardiovascular diseases is usually done by ECG signal, which is a simple 12-lead Electrocardiogram (ECG) that gives complete information about the function of the heart including the amplitude and time interval of P-QRS-T-U segment. This article recommends a novel approach to identify the location of thrombus in culprit artery using the Information Fuzzy Network (IFN). Information Fuzzy Network, being a supervised machine learning technique, takes known evidences based on rules to create a predicted classification model with thrombus location obtained from the vast input ECG data. These rules are well-defined procedures for selecting hypothesis that best fits a set of observations. Results illustrate that the recommended approach yields an accurateness of 92.30%. This novel approach is shown to be a viable ECG analysis approach for identifying the culprit artery and thus localizing the thrombus.																	1989-1660					MAR	2020	6	1					16	25		10.9781/ijimai.2019.02.001													
J								Fuzzy C-Means Clustering with Histogram based Cluster Selection for Skin Lesion Segmentation using Non-Dermoscopic Images	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Medical Image Segmentation; Melanoma; Histogram; Fuzzy C-means; Local Maxima; Non-Dermoscopic Image; Cluster Size	BORDER DETECTION; DIAGNOSIS; MELANOMA; SYSTEM	Purpose - Pre-screening of skin lesion for malignancy is highly demanded as melanoma being a life-threatening skin cancer due to unpaired DNA damage. In this paper, lesion segmentation based on Fuzzy C-Means clustering using non-dermoscopic images has been proposed. Design/methodology/approach - The proposed methodology consists of automatic cluster selection for FCM using the histogram property. The system used the local maxima along with Euclidean distance to detect the binomial distribution property of the image histogram, to segment the melanoma from normal skin. As the Value channel of HSV color image provides better and distinct histogram distribution based on the entropy, it has been used for segmentation purpose. Findings - The proposed system can effectively segment the lesion region from the normal skin. The system provides a segmentation accuracy of 95.69 % and the comparative analysis has been performed with various segmentation methods. From the analysis, it has been observed that the proposed system can effectively segment the lesion region from normal skin automatically. Originality/Value - This paper suggests a new approach for skin lesion segmentation based on FCM with automatic cluster selection. Here, different color channel has also been analyzed using entropy to select the better channel for segmentation. In future, the classification of melanoma from benign naevi can be performed.																	1989-1660					MAR	2020	6	1					26	31		10.9781/ijimai.2020.01.001													
J								Voltage Stability Assessment of Radial Distribution Systems Including Optimal Allocation of Distributed Generators	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Whale Optimization Algorithm; Fuzzy Logic Controller; Voltage Stability Index; Optimal DG Placement	LOAD FLOW SOLUTION; OPTIMAL PLACEMENT; DG; MAXIMIZATION; IMPROVE; MARGIN; SVC	Assessment of power systems voltage stability is considered an important assignment for the operation and planning of power system. In this paper, a voltage stability study using Continuous Power Flow (CPF) is introduced to evaluate the impact of Distribution Generator (DG) on radial distribution systems. On the way to allocate the DG, a hybrid between the Voltage Stability Index (VSI) and Whale Optimization Algorithm (WOA) is developed. The main purpose of using VSI is to find the most sensitive buses for allocating the DG in the system. Hence, Fuzzy logic control with the Normalized VSI (NVSI) and the voltage magnitude at each bus are used to determine the candidate buses. However, the best DG size is calculated using WOA. Four standard radial distribution systems are used in this paper; 12, 33, 69, and 85-bus. The developed hybrid optimization method is compared with other existing analytical and metaheuristic optimization techniques to prove its efficiency. The results prove the ability of the developed method in the allocation of DG. In addition, the influence of the DG integration on enhancing the voltage stability through injecting the proper active and reactive powers is studied.																	1989-1660					MAR	2020	6	1					32	40		10.9781/ijimai.2020.02.004													
J								Mamdani Fuzzy Expert System Based Directional Relaying Approach for Six-Phase Transmission Line	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Fault Protection; Fuzzy Systems; Simulation; Transmission Lines	FAULT CLASSIFICATION; ZONE IDENTIFICATION; INFERENCE SYSTEM; LOCATION; SCHEME	Traditional directional relaying methods for 6-phase transmission lines have complex effort, and so there is still a need for novel direction relaying estimation scheme. This study presents a Mamdani-fuzzy expert system (MFES) approach for discriminating faulty section/zone, classifying faults and locating faults in 6-phase transmission lines. The 6-phase fundamental component of currents, voltages and phase angles are captured at single bus and are used in the protection scheme. Simulation results substantiate that the protection scheme is very successful against many parameters such as different fault types, fault resistances, transmission line fault locations and inception angles. A large number of fault case studies have been carried out to evaluate reach setting and % error of proposed method. It provides primary protection to transmission line length and also offers backup protection for a reverse section of transmission line. The experimental results show that the scheme performs better than the other schemes.																	1989-1660					MAR	2020	6	1					41	50		10.9781/ijimai.2019.06.002													
J								An Intelligent Technique for Grape Fanleaf Virus Detection	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Artificial Neural Networks; SVM; Fuzzy C-mean Algorithm; Grape Fanleaf Virus Detection; Grape Diseases	DISEASE IDENTIFICATION; CLASSIFICATION	Grapevine Fanleaf Virus (GFLV) is one of the most important viral diseases of grapes, which can damage up to 85% of the crop, if not treated at the right time. The aim of this study is to identify infected leaves with GFLV using artificial intelligent methods using an accessible database. To do this, some pictures are taken from infected and healthy leaves of grapes and labeled by technical specialists using conventional laboratory methods. In order to provide an intelligent method for distinguishing infected leaves from healthy ones, the area of unhealthy parts of each leaf is highlighted using Fuzzy C-mean Algorithm (FCM), and then the percentages of the first two segments area are fed to a Support Vector Machines (SVM). To increase the diagnostic reliability of the system, K-fold cross validation method with k = 3 and k = 5 is applied. After applying the proposed method over all images using K-fold validation technique, average confusion matrix is extracted to show the True Positive, True Negative, False Positive and False Negative percentages of classification. The results show that specificity, as the ability of the algorithm to really detect healthy images, is 100%, and sensitivity, as the ability of the algorithm to correctly detect infected images is around 97.3%. The average accuracy of the system is around 98.6%. The results imply the ability of the proposed method compared to previous methods.																	1989-1660					MAR	2020	6	1					62	67		10.9781/ijimai.2020.02.001													
J								Classification-based Deep Neural Network Architecture for Collaborative Filtering Recommender Systems	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Deep Learning; Neural Classification; Neural Collaborative Filtering; Recommender Systems; Scalable Neural Architecture	MATRIX FACTORIZATION	This paper proposes a scalable and original classification-based deep neural architecture. Its collaborative filtering approach can be generalized to most of the existing recommender systems, since it just operates on the ratings dataset. The learning process is based on the binary relevant/non-relevant vote and the binary voted/non-voted item information. This data reduction provides a new level of abstraction and it makes possible to design the classification-based architecture. In addition to the original architecture, its prediction process has a novel approach: it does not need to make a large number of predictions to get recommendations. Instead to run forward the neural network for each prediction, our approach runs forward the neural network just once to get a set of probabilities in its categorical output layer. The proposed neural architecture has been tested by using the MovieLens and FilmTrust datasets. A state-of-the-art baseline that outperforms current competitive approaches has been used. Results show a competitive recommendation quality and an interesting quality improvement on large number of recommendations, consistent with the architecture design. The architecture originality makes it possible to address a broad range of future works.																	1989-1660					MAR	2020	6	1					68	77		10.9781/ijimai.2020.02.006													
J								A Convolution Neural Network Engine for Sclera Recognition	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Biometrics; Convolution Neural Network; Deep Learning; Sclera Recognition	TASOM; IRIS	The world is shifting to the digital era in an enormous pace. This rise in the digital technology has created plenty of applications in the digital space, which demands a secured environment for transacting and authenticating the genuineness of end users. Biometric systems and its applications has seen great potentials in its usability in the tech industries. Among various biometric traits, sclera trait is attracting researchers from experimenting and exploring its characteristics for recognition systems. This paper, which is first of its kind, explores the power of Convolution Neural Network (CNN) for sclera recognition by developing a neural model that trains its neural engine for a recognition system. To do so, the proposed work uses the standard benchmark dataset called Sclera Segmentation and Recognition Benchmarking Competition (SSRBC 2015) dataset, which comprises of 734 images which are captured at different viewing angles from 30 different classes. The proposed methodology results showcases the potential of neural learning towards sclera recognition system.																	1989-1660					MAR	2020	6	1					78	83		10.9781/ijimai.2019.03.006													
J								Deep Neural Networks for Speech Enhancement in Complex-Noisy Environments	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Deep Neural Networks; Intelligibility; Deep Learning; Speech Enhancement; Time-Frequency Masking; Ideal Binary Mask	INTELLIGIBILITY	In this paper, we considered the problem of the speech enhancement similar to the real-world environments where several complex noise sources simultaneously degrade the quality and intelligibility of a target speech. The existing literature on the speech enhancement principally focuses on the presence of one noise source in mixture signals. However, in real-world situations, we generally face and attempt to improve the quality and intelligibility of speech where various complex stationary and nonstationary noise sources are simultaneously mixed with the target speech. Here, we have used deep learning for speech enhancement in complex-noisy environments and used ideal binary mask (IBM) as a binary classification function by using deep neural networks (DNNs). IBM is used as a target function during training and the trained DNNs are used to estimate IBM during enhancement stage. The estimated target function is then applied to the complex-noisy mixtures to obtain the target speech. The mean square error (MSE) is used as an objective cost function at various epochs. The experimental results at different input signal-to-noise ratio (SNR) showed that DNN-based complex-noisy speech enhancement outperformed the competing methods in terms of speech quality by using perceptual evaluation of speech quality (PESQ), segmental signal-to-noise ratio (SNRSeg), log-likelihood ratio (LLR), weighted spectral slope (WSS). Moreover, short-time objective intelligibility (STOI) reinforced the better speech intelligibility.																	1989-1660					MAR	2020	6	1					84	90		10.9781/ijimai.2019.06.001													
J								Binary Multi-Verse Optimization (BMVO) Approaches for Feature Selection	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Machine Learning; Feature Selection; K-Nearest Neighbors; Binary Multi-Verse Optimization	PARTICLE SWARM OPTIMIZATION; GRAVITATIONAL SEARCH ALGORITHM; ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; ROUGH SETS; MODEL; CLASSIFICATION; SYSTEM; SVM; PSO	Multi-Verse Optimization (MVO) is one of the newest meta-heuristic optimization algorithms which imitates the theory of Multi-Verse in Physics and resembles the interaction among the various universes. In problem domains like feature selection, the solutions are often constrained to the binary values viz. 0 and 1. With regard to this, in this paper, binary versions of MVO algorithm have been proposed with two prime aims: firstly, to remove redundant and irrelevant features from the dataset and secondly, to achieve better classification accuracy. The proposed binary versions use the concept of transformation functions for the mapping of a continuous version of the MVO algorithm to its binary versions. For carrying out the experiments, 21 diverse datasets have been used to compare the Binary MVO (BMVO) with some binary versions of existing metaheuristic algorithms. It has been observed that the proposed BMVO approaches have outperformed in terms of a number of features selected and the accuracy of the classification process.																	1989-1660					MAR	2020	6	1					91	106		10.9781/ijimai.2019.07.004													
J								Comparative Study on Ant Colony Optimization (ACO) and K-Means Clustering Approaches for Jobs Scheduling and Energy Optimization Model in Internet of Things (IoT)	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Ant Colony Optimization (ACO); Energy Consumption; Internet of Things (IoT); K-means Algorithm; Response Time; Message Scheduling		The concept of Internet of Things (IoT) was proposed by Professor Kevin Ashton of the Massachusetts Institute of Technology (MIT) in 1999. IoT is an environment that people understand in many different ways depending on their requirement, point of view and purpose. When transmitting data in IoT environment, distribution of network traffic fluctuates frequently. If links of the network or nodes fail randomly, then automatically new nodes get added frequently. Heavy network traffic affects the response time of all system and it consumes more energy continuously. Minimization the network traffic/by finding the shortest path from source to destination minimizes the response time of all system and also reduces the energy consumption cost. The ant colony optimization (ACO) and K-Means clustering algorithms characteristics conform to the auto-activator and optimistic response mechanism of the shortest route searching from source to destination. In this article, ACO and K-Means clustering algorithms are studied to search the shortest route path from source to destination by optimizing the Quality of Service (QoS) constraints. Resources are assumed in the active and varied IoT network atmosphere for these two algorithms. This work includes the study and comparison between ant colony optimization (ACO) and K-Means algorithms to plan a response time aware scheduling model for IoT. It is proposed to divide the IoT environment into various areas and a various number of clusters depending on the types of networks. It is noticed that this model is more efficient for the suggested routing algorithm in terms of response time, point-to-point delay, throughput and overhead of control bits.																	1989-1660					MAR	2020	6	1					107	116		10.9781/ijimai.2020.01.003													
J								Multilayer Feedforward Neural Network for Internet Traffic Classification	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Classification; Feature Transformation; Feedforward Network; Internet Traffic		Recently, the efficient internet traffic classification has gained attention in order to improve service quality in IP networks. But the problem with the existing solutions is to handle the imbalanced dataset which has high uneven distribution of flows between the classes. In this paper, we propose a multilayer feedforward neural network architecture to handle the high imbalanced dataset. In the proposed model, we used a variation of multilayer perceptron with 4 hidden layers (called as mountain mirror networks) which does the feature transformation effectively. To check the efficacy of the proposed model, we used Cambridge dataset which consists of 248 features spread across 10 classes. Experimentation is carried out for two variants of the same dataset which is a standard one and a derived subset. The proposed model achieved an accuracy of 99.08% for highly imbalanced dataset (standard).																	1989-1660					MAR	2020	6	1					117	122		10.9781/ijimai.2019.11.002													
J								A Multicriteria Optimization for Flight Route Networks in Large-Scale Airlines Using Intelligent Spatial Information	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Multi-Objective Optimization; Genetic Algorithm; Airway Topology; GIS; Artificial Intelligence; Non-Dominated Sorting Genetic Algorithm II (NSGA-II)	HUB LOCATION PROBLEM; ALGORITHM; PROGRAM	Air route network optimization, one of the airspace planning challenges, effectively manages airspace resources toward increasing airspace capacity and reducing air traffic congestion. In this paper, the structure of the flight network in air transport is analyzed with a multi-objective genetic algorithm regarding Geographic Information System (GIS) which is used to optimize this Iran airlines topology to reduce the number of airways and the aggregation of passengers in aviation industries organization and also to reduce changes in airways and the travel time for travelers. The proposed model of this study is based on the combination of two topologies point-to-point and Hub-and-spoke - with multiple goals for causing a decrease in airways and travel length per passenger and also to reach the minimum number of air stops per passenger. The proposed Multi-objective Genetic Algorithm (MOGA) is tested and assessed in data of the Iran airlines industry in 2018, as an example to real-world applications, to design Iran airline topology. MOGA is proven to be effective in general to solve a network-wide flight trajectory planning. Using the combination of point-to-point and Hub-and-spoke topologies can improve the performance of the MOGA algorithm. Based on Iran airline traffic patterns in 2018, the proposed model successfully decreased 50.8% of air routes (184 air routes) compared to the current situations while the average travel length and the average changes in routes were increased up to 13.8% (about 100 kilometers) and up to 18%, respectively. The proposed algorithm also suggests that the current air routes of Iran can be decreased up to 24.7% (89 airways) if the travel length and the number of changes increase up to 4.5% (32 kilometers) and 5%, respectively. Two intermediate airports were supposed for these experiments. The computational results show the potential benefits of the proposed model and the advantage of the algorithm. The structure of the flight network in air transport can significantly reduce operational cost while ensuring the operation safety. According to the results, this intelligent multi-object optimization model would be able to be successfully used for a precise design and efficient optimization of existing and new airline topologies.																	1989-1660					MAR	2020	6	1					123	131		10.9781/ijimai.2019.11.001													
J								Finding an Accurate Early Forecasting Model from Small Dataset: A Case of 2019-nCoV Novel Coronavirus Outbreak	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Forecasting; Machine Learning; Method; Prediction; Data Mining; Epidemic	NEURAL-NETWORKS; SELECTION	Epidemic is a rapid and wide spread of infectious disease threatening many lives and economy damages. It is important to fore-tell the epidemic lifetime so to decide on timely and remedic actions. These measures include closing borders, schools, suspending community services and commuters. Resuming such curfews depends on the momentum of the outbreak and its rate of decay. Being able to accurately forecast the fate of an epidemic is an extremely important but difficult task. Due to limited knowledge of the novel disease, the high uncertainty involved and the complex societal-political factors that influence the widespread of the new virus, any forecast is anything but reliable. Another factor is the insufficient amount of available data. Data samples are often scarce when an epidemic just started. With only few training samples on hand, finding a forecasting model which offers forecast at the best efforts is a big challenge in machine learning. In the past, three popular methods have been proposed, they include 1) augmenting the existing little data, 2) using a panel selection to pick the best forecasting model from several models, and 3) fine-tuning the parameters of an individual forecasting model for the highest possible accuracy. In this paper, a methodology that embraces these three virtues of data mining from a small dataset is proposed. An experiment that is based on the recent coronavirus outbreak originated from Wuhan is conducted by applying this methodology. It is shown that an optimized forecasting model that is constructed from a new algorithm, namely polynomial neural network with corrective feedback (PNN+cf) is able to make a forecast that has relatively the lowest prediction error. The results showcase that the newly proposed methodology and PNN+cf are useful in generating acceptable forecast upon the critical time of disease outbreak when the samples are far from abundant.																	1989-1660					MAR	2020	6	1					132	140		10.9781/ijimai.2020.02.002													
J								Performance evaluations in optical and wireless networks for CONDOR project	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network performance; Mobile applications; P2P wireless network		Wireless and optical networks are widely used nowadays. These networks offer a high throughput thanks to their optical link and allow the development of multiuser applications. Because the network performance is an important issue to provide services to a great number of users while assuring user's quality of service requirements, CONDOR (CONtribution a la Diffusion de l'histOiRe du traitement de l'information a l'aide du reseau de demain) project aims to evaluate the wireless and optical networks' performances in terms of link quality, throughput, jitter and delay. Our results show that a high throughput in optical and wireless networks supports a big load through the launched mobile applications while P2P wireless network connections upset some video applications.																	1868-5137	1868-5145				MAR	2020	11	3			SI		919	928		10.1007/s12652-019-01208-2													
J								iPACS: a physical access control system as a service and mobile application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Fog computing; Internet of Things; Bluetooth beacon sensors; Physical access control system; PACS	MANAGEMENT	We present iPACS, an automated physical access control system as a cloud service for controlling users activity, navigation and access in large residential infrastructures (e.g. apartment buildings, shopping malls). The main idea is to install on buildings, beacon Bluetooth radio transmitters that broadcast their identifier to nearby devices (e.g. users' mobile phones) and from there, transmit this information safely, together with other application and user data, to a private cloud. The service tracks people, monitors the overall activity in buildings and public venues and offers prompt response in cases of critical events (i.e. overcrowding, health incidents, attacks etc.). iPACS provides also services aiming to inform the infrastructure manager for possible increase of users activity or access requests that require permission based on subscriptions or authorization criteria. These services are deployed over secure private clouds capable of dealing safely with sensitive information while ensuring users privacy. Collectively, anonymous history (log) data are sent to a public cloud for analysis.																	1868-5137	1868-5145				MAR	2020	11	3			SI		929	943		10.1007/s12652-019-01205-5													
J								Multi-hop LoRaWAN uplink extension: specification and prototype implementation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										LoRa; LoRaWAN; Multi-hop; Routing		The coverage of a LoRaWAN network in a city is greatly hampered by the harsh propagation environment. Sensors are sometimes placed under the ground or in places with strong electromagnetic attenuation. Also, for users who have a contract with a network operator, installing another gateway to improve coverage of blind spots is not an option. In other cases, there is no or very weak connectivity (e.g. basements). In the present work, we design and implement a multi-hop uplink solution compatible with the LoRaWAN specification, which can act as an extension to already deployed gateways. End nodes transmit data messages to intermediate nodes, which relay them to gateways by choosing routes based on a simplified version of destination-sequenced distance vector routing. The routing protocol was successfully implemented and was assessed using a linear and bottleneck topology, where the packet reception rate (PRR) and throughput were measured. A carrier activity detection mechanism is also proposed. This paper presents the protocol specification and detailed description of a prototype implementation, as well as experimental performance results. On the bottleneck topology, it was observed that the PRR of each node did not significantly vary. On the linear topology, we observed that the throughput and PRR did not decrease considerably with the increase of hops. A listen-before talk multiple access mechanism is also proposed, which significantly reduces the probability of collision.																	1868-5137	1868-5145				MAR	2020	11	3			SI		945	959		10.1007/s12652-019-01207-3													
J								A real-time service system in the cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Real-time service system; Salesforce platform; SaaS cloud	CONTEXT	Recently, we have witnessed unprecedented use of cloud computing and its services. It is influencing the way software is built, as well as company' resources such as servers, workstations or generally hardware are used. This paper aims to examine the benefits of cloud usage to support real-time service systems, using the Salesforce platform. First, we explore the meaning and the role of cloud computing for the real-time service systems efficient functioning. Then, we build a service management platform for the Polish Billiards and Snooker Association (PBSA), based on a real-time system located in the cloud. This way, PBSA managers are able to complete their tasks in this system on-demand. Moreover, it is set up as a private cloud to grant access only to the snooker organization employees.																	1868-5137	1868-5145				MAR	2020	11	3			SI		961	977		10.1007/s12652-019-01203-7													
J								Recognition of basketball referee signals from real-time videos	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Basketball referee signals; Gesture recognition; Sport analysis; Image processing	GESTURE RECOGNITION; LANGUAGE	Recognition of hand gestures (hand signals) is an active research area for human computer interaction with many possible applications. Automatic machine vision-based hand gesture interfaces for real-time applications require fast and extremely robust human, pose and hand detection, and gesture recognition. Attempting to recognize gestures performed by official referees in sports (such as basketball game) video places tough requirements on the image segmentation techniques. Here we propose an image segmentation technique based on the histogram of oriented gradients and local binary pattern (LBP) features, which allow recognizing the signals of basketball referee from recorded game videos and achieved an accuracy of 95.6% using LBP features and support vector machine for classification. Our results are relevant for real-time analysis of basketball game.																	1868-5137	1868-5145				MAR	2020	11	3			SI		979	991		10.1007/s12652-019-01209-1													
J								Microsimulating labour market job-worker matching	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Labour market; Microsimulation; Integrated urban model	INTEGRATED LAND-USE; MODEL; TRANSPORTATION; SALARY	This paper introduces a new agent-based microsimulation (ABM) model of urban labour markets, in which workers actively seeking employment in each time period are matched with vacant jobs. The model is designed to operate within the Integrated Land Use, Transportation, Environment (ILUTE) urban simulation model system. In the current model application, 1986 is taken as the base year, with 20-year simulations being run (1986-2006) to test the model's performance within a known historical time-period.																	1868-5137	1868-5145				MAR	2020	11	3			SI		993	1006		10.1007/s12652-019-01206-4													
J								A configurable identity matching algorithm for community care management	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Healthcare interoperability; Performance management; Record linkage; Community care; Cloud computing; Performance management	RECORD LINKAGE; TRIPLE AIM; HEALTH; IDENTIFICATION	Systematic performance management for complex patient care is challenging. Heterogeneous healthcare data silos and inconsistent patient identity, coupled with patient privacy regulations, limit our ability to correlate healthcare data for complex patients. Cloud computing is an emerging technology that could be leveraged to address the issue of heterogeneous healthcare data silos if a regional health authority provided data hosting with appropriate data sharing agreements and identity management, in order to correlate healthcare data for complex patients. This paper introduces a configurable identity matching algorithm for correlating shared data from multiple stakeholders into a common data model to support performance management of community healthcare. The authors illustrate its use in a case study of cloud-hosted performance management for community care of complex patients at a regional health authority in Canada.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1007	1020		10.1007/s12652-019-01252-y													
J								Internet of Things based multiple disease monitoring and health improvement system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of Things (IoT); Healthcare; Disease monitoring; Hypertension; Glaucoma; Chronic obstructive pulmonary disease (COPD)		Internet of Things (IoT), the state-of-the-art technology, has recently started to enhance different industries and services. One of the few that are impacted by this technology are medicine and healthcare industries. The application of IoT in healthcare, in particular, is a major field attracting many researchers nowadays. The aim of this paper is to propose an IoT based system for monitoring multiple diseases using Cisco packet tracer tool. The achieve this aim, the system is divided into two parts. The first part deals with data collection and processing from sensors and microcontrollers. The second part deals with offered services such as disease diagnosis, medicine administration, and emergency responses. To demonstrate the feasibility of the proposed system, three diseases are consider including hypertension, glaucoma, and chronic obstructive pulmonary disease. It is also possible to incorporate other diseases.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1021	1029		10.1007/s12652-019-01204-6													
J								Agent based routing approach to support structural health monitoring-informed, intelligent transportation system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Structural health monitoring; Intelligent transportation systems; Wireless sensor networks		In structural health monitoring (SHM)-informed intelligent transportation systems (ITS) system, SHM sensors can provide data on a bridge health condition for ITS applications. Where ITS uses this bridge health condition information for real-time traffic management and monitoring. Most of the SHM-informed ITS use wireless sensor network (WSN) to allow devices to coordinate and collaborate to effectively measure a structure, whether the goal is to improve spatial resolution, structural resilience, bridge traffic, or perform advanced in-situ analysis. The WSN suffers from high power consumption, yet they are generally equipped with a limited power supply and in most cases AAA batteries. Even under the most stringent power management, the sensor nodes have an unattended life of approximately a few months to a year. Towards this end, the paper proposes a multi-agent based system to augment the life time of WSN supporting SHM-informed ITS systems. The proposed agent based routing (ABR) approach consists of a mobile agent that is able to traverse the network, i.e., sensor nodes, mounted onto a bridge using multi-hop communication; collecting and aggregate the data, thus eliminating the two major causes of power consumption. These include (a) the direct transmission/broadcast from each sensor node to the sink and (b) redundant sensory data. In addition, ABR reduce much of the communication overhead and hence prolong the operational lifetime of SHM-informed ITS. Our experiment conducted on an open source simulator shows that the proposed ABR is 30% more energy efficient in comparison to the baseline, i.e., direct transmission to the sink.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1031	1043		10.1007/s12652-019-01202-8													
J								Design and implementation of distributed RSA algorithm based on Hadoop	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distributed platform; RSA encryption algorithm; Hadoop		With the advent of the era of big data, the deep integration of the Internet and healthcare makes medical data exponential growth. How to ensure data security is particularly important. However, the traditional encryption speed is no longer applicable to massive information processing. Based on Hadoop open source project, this paper designs and studies distributed RSA encryption algorithm based on distributed file system and programming model. In the beginning of this paper, a design scheme of distributed encryption algorithm based on platform is proposed, the distribution of distributed algorithm is determined, module partition and process control are carried out to the whole, and the distributed encryption function of the single computer encryption system is realized. Finally, through the large-scale distributed cluster built in the laboratory, the distributed encryption algorithm is tested by function test, extensibility test and efficiency test. It is proved that the distributed encryption algorithm can optimize the operation speed and can be applied to the processing of massive data.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1047	1053		10.1007/s12652-018-1021-y													
J								An analysis on the economic cooperation and the industrial synergy of the main river region: from the perspective of the Yangtze river economic zone	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Main trunk river; The Yangtze River Economic Zone; Regional economic cooperation; Industrial synergy		The Yangtze River, as the third longest river in the world, is the longest river which just runs through one country. This paper firstly analyses the current development of the Yangtze River Economic Zone and the prominent issues restricting the economic competitiveness and the industrial synergy of the region. And then the economic synergistic development model and the industrial synergistic development model of cities in the Yangtze River Economic Zone are established. The statistics of Anhui Province and Jiangsu Province are selected as examples. The analysis indicates that the main cities in the Yangtze River Economic Zone have a slight influence on each other. The central city lacks of the impetus function and industrial linkage, so that the feasible supporting industries have not been developed. According to the analysis, the development strategy of the main trunk river region, the Yangtze River Economic Zone, is presented, which recommends strengthening the reaction among cities, driving the overall economic expansion, optimizing the urban industrial layout, forming a new industries supporting pattern with upstream and downstream linkage and improving the rapid development of "manufacturing and services", "manufacturing and the Internet" and so on. The analysis on the economic cooperation and the industrial synergy of the Yangtze River Economic Zone could have a significant influence on improving the feasible division of China's regional economy and the industry linkage and support the implementing of China's "the Belt and Road initiative". Meanwhile, the analysis also has important significance for researching the main trunk river regions' coordinating development, industry linkage and resources in the world.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1055	1064		10.1007/s12652-018-1011-0													
J								An efficient certificateless public key cryptography with authorized equality test in IIoT	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Industrial Internet of Things (IIoT); Cloud; Certificatless public key cryptography with equality test; Authorization	IDENTITY-BASED ENCRYPTION; SECURITY; SCHEME	Current time organizations have moved from the conventional industries to smart industries by embracing the approach of Industrial Internet of Things (IIoT), which has provided an avenue for the integration of smart devices and communication technologies. IIoT offers reduction of costs on services and technologies since it counts on the flexibility, availability and real-time data processing capabilities of the cloud server. And even with the untrusted nature of the cloud, the privacy and confidentiality of the data outsourced to the cloud has been able to be guaranteed. This has been achieved through encryption techniques apply to the data before outsourcing. This has made it feasible to have the data in the cloud secured. However, the limitation of data recovery due to it having the innate "all-or-nothing" decryption characteristic, results in a challenge in the search functionality for the encrypted data. In order to deal with this challenge, this paper nominates a Certificateless Public Key Cryptography with Authorized Equivalent Test (CL-PKC-AET). Whereby, a cloud server is given authority to actuate if two encryptions consist of the equivalent messages. Furthermore, the extra providence of a fine-grained testing capability to the cloud server makes the CL-PKC-AET scheme adaptable. We define our scheme basing on bilinear pairing and further advocate within the random oracle model its security, based on the Bilinear Diffie-Hellman assumption. Basing on comparison with existing work, our scheme proves to be more effective.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1065	1083		10.1007/s12652-019-01365-4													
J								Blood supply chain management: robust optimization, disruption risk, and blood group compatibility (a real-life case)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Blood supply chain; Multi-objective programming; Robust optimization; Disruption risk; Blood group compatibility	NETWORK DESIGN; MODEL	Based on the uncertain conditions such as uncertainty in blood demand and facility disruptions, and also, due to the uncertain nature of blood products such as perishable lifetime, distinct blood groups, and ABO-Rh(D) compatibility and priority rules among these groups, this paper aims to contribute blood supply chains under uncertainty. In this respect, this paper develops a bi-objective two-stage stochastic programming model for managing a red blood cells supply chain that observes above-mentioned issues. This model determines the optimum location-allocation and inventory management decisions and aims to minimize the total cost of the supply chain includes fixed costs, operating costs, inventory holding costs, wastage costs, and transportation costs along with minimizing the substitution levels to provide safer blood transfusion services. To handle the uncertainty of the blood supply chain environment, a robust optimization approach is devised to tackle the uncertainty of parameters, and the TH method is utilized to make the bi-objective model solvable. Then, a real case study of Mashhad city, in Iran, is implemented to demonstrate the model practicality as well as its solution approaches, and finally, the computational results are presented and discussed. Further, the impacts of the different parameters on the results are analyzed which help the decision makers to select the value of the parameters more accurately.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1085	1104		10.1007/s12652-019-01315-0													
J								A novel hybrid wrapper-filter approach based on genetic algorithm, particle swarm optimization for feature subset selection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; Hybrid (wrapper-filter) approach; Multi-objective optimization; Genetic algorithm; Particle swarm optimization (PSO)	MUTUAL INFORMATION; DIFFERENTIAL EVOLUTION; CLASSIFICATION; PROTOCOL; RELEVANCE; ENSEMBLE; INTERNET; DESIGN; COLONY	The classification is one of the main technique of machine learning science. In many problems, the data sets have a high dimensionality that the existence of all features is not important to the purpose of the problem, and this will decrease the accuracy and performance of the algorithm. In this situation, the feature selection will play a significant role, and by eliminating unrelated features, the efficiency of the algorithm will be increased. A hybrid filter-wrapper method is proposed in the present study for feature subset selection established with integration of evolutionary based genetic algorithms (GA) and particle swarm optimization (PSO). The presented method mainly aims to reduce the complication of calculation and the search time expended to achieve an optimum solution to the high dimensional datasets feature selection problem. The proposed method, named smart HGP-FS, utilizes artificial neural network (ANN) in the fitness function. The filter and wrapper methods are integrated in order to take the benefit of filter technique acceleration and the wrapper technique vigor for selection of dataset efficacious characteristics. Some dataset characteristics are eliminated through the filter phase, which in turn reduces complex computations and search time in the wrapper phase. Comparisons have been made for the effectiveness of the proposed hybrid algorithm with the usability of three hybrid filter-wrapper methods, two pure wrapper algorithms, two pure filter procedures, and two traditional wrapper feature selection techniques. The findings obtained over real-world datasets show the efficiency of the presented algorithm. The outcomes of algorithm examination on five datasets reveal that the developed method is able to obtain a more accurate classification and to remove unsuitable and unessential characteristics more effectively relative to the other approaches.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1105	1127		10.1007/s12652-019-01364-5													
J								Interest-based trust management scheme for social internet of things	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Social networking; Trust management	MODEL; IOT	The integration of social ties between IoT's devices has facilitated the emergence of new paradigm named the Social Internet of Things SIoT. In this paradigm, social relationships can be established in autonomous way by objects creating virtual communities with shared interests. To facilitate the location of the desired interests and enhance the cooperation between SIoT's devices, the trust systems have been proven to be an effective solution in such interconnected environment. In this paper, we propose an interest-based trust management scheme for SIoT. This scheme assesses the trust of SIoT's devices according to the interest preferences of the trustor (i.e., assessor object). Moreover, it introduces a new recommendation system based on the similarity in terms of interest preferences between the trustor and the recommender (i.e., provides recommendation about the assessed object) to enhance the location of the desired services. Simulation results validate the convergence of the proposed trust mechanism, and show its gain in terms of transaction success rate.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1129	1140		10.1007/s12652-019-01256-8													
J								Online human activity recognition employing hierarchical hidden Markov models	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Online activity recognition; Streaming sensor data; Activity segmentation; Hierarchical hidden Markov models; Smart homes; Internet of things	SENSOR DATA; HOME; PATTERNS; SEGMENTATION	In the last few years there has been a growing interest in Human activity recognition (HAR) topic. Sensor-based HAR approaches, in particular, has been gaining more popularity owing to their privacy preserving nature. Furthermore, due to the widespread accessibility of the internet, a broad range of streaming-based applications such as online HAR, has emerged over the past decades. However, proposing sufficiently robust online activity recognition approach in smart environment setting is still considered as a remarkable challenge. This paper presents a novel online application of Hierarchical Hidden Markov Model in order to detect the current activity on the live streaming of sensor events. Our method consists of two phases. In the first phase, data stream is segmented based on the beginning and ending of the activity patterns. Also, on-going activity is reported with every receiving observation. This phase is implemented using Hierarchical Hidden Markov models. The second phase is devoted to the correction of the provided label for the segmented data stream based on statistical features. The proposed model can also discover the activities that happen during another activity - so-called interrupted activities. After detecting the activity pane, the predicted label will be corrected utilizing statistical features such as time of day at which the activity happened and the duration of the activity. We validated our proposed method by testing it against two different smart home datasets and demonstrated its effectiveness, which is competing with the state-of-the-art methods.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1141	1152		10.1007/s12652-019-01380-5													
J								Example based facial aging simulation via facial detail transfer	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Aging simulation; Facial detail transfer; Image synthesizing	FACE RECOGNITION	The key problem of the facial aging simulation is protecting the identity information during the texture synthesizing procedure. In this paper, we address the aging simulation problem by using the facial detail transfer method, which provides a fundamental view of the aging simulation problem and shows its superiority in the small dataset. The proposed method decomposes the facial image into several layers, namely, structure layer, detail layer and color layer. The structure layer protects the identity information of the test image, while the detail layer contains the age information from the selected template image. We transfer the detail layer from the example images to the test images to generate the aging effects on the synthesized images. The images near the target age are also considered in the proposed method to further refine the synthesized results in the aging sequence. Experimental results conducted on the FG-NET database, Morph database and the IRIP-Aging database show that the proposed method can not only protect the identity information and the age information, but also generate more smooth aging sequence, which demonstrates the effectiveness of the proposed method.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1153	1162		10.1007/s12652-019-01243-z													
J								Flow based efficient data gathering in wireless sensor network using path-constrained mobile sink	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data collection; Energy efficiency; Mobile sink; Fixed path; Wireless sensor networks	DATA-COLLECTION; ALGORITHM; LIFETIME	In wireless sensor networks (WSNs), sensors are equipped with limited power batteries. Data collection from sensors is one of the fundamental tasks in WSNs. Maximizing the data collection with minimum energy consumption is one of the major challenging issues in WSNs. In this article, we consider data collection using a mobile sink, where the mobile sink efficiently collects data from nearby sensors while moving along a pre-specified path with constant speed. We refer this problem as a Maximizing Data Gathering with Minimum Energy Consumption (MDGMEC) problem. So far, existing works have heuristic algorithms for MDGMEC problem. Their solutions, being heuristic, could not ensure the optimality. In this article, we propose two efficient algorithms to improve the data gathering process. Our algorithms use network flow approach for efficient data forwarding. In the first proposed algorithm, the mobile sink can receive data from multiple nearby sensors simultaneously, while in the second, the mobile sink can receive data from only one nearby sensor at a time. The proposed algorithms run in polynomial time and are also scalable for large networks. We evaluate the proposed algorithms by implementing them in MATLAB. The obtained results show that our proposed schemes outperform other existing schemes in terms of collecting total data and energy efficiency.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1163	1175		10.1007/s12652-019-01245-x													
J								A niching behaviour-based algorithm for multi-level manufacturing service composition optimal-selection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud manufacturing (CMfg); Multi-level modelling; Optimal selection; Niching behaviour; Fitness sharing; Gravitational search algorithm (GSA)	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; INFORMATION; STRATEGY; SEARCH; MODEL	To improve the accuracy of service modelling and optimal selection in cloud manufacturing (CMfg), a multi-level modelling methodology is proposed to describe manufacturing services. In this methodology, manufacturing services are divided into three levels: resource, functional and process services. Based on time, cost and reputation analysis of these three service levels, the corresponding objective functions and services composition constraints are established. Considering intelligent optimal selection, a niching behaviour-based gravitational search algorithm (NGSA) is designed to address manufacturing service composition and optimal selection (MSCOS) problems. In NGSA, the niche crowding factor and fitness sharing technology are introduced to the standard gravitational search algorithm (GSA) to improve its convergence speed and accuracy. The results of a simulation experiment demonstrate that the proposed algorithm can find better solutions in less time than previous algorithms, such as the adaptive genetic algorithm (AGA) and the modified particle swarm optimization (MPSO) algorithm.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1177	1189		10.1007/s12652-019-01250-0													
J								Design sprint in classroom: exploring new active learning tools for project-based learning approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Design sprint; Active learning; Systematic literature review; Project-based learning; User experience; Authentic software	EXPERIENCE; STUDENTS	The application of content only through lectures in an expository format may not be sufficient for the teaching of software engineering in this new era. Creating software products that take the user experience (UX) into account as an essential requirement in the software development process is a necessary activity for all information technology (IT) professionals looking to build quality products. Currently, there are few initiatives that address the insertion of innovative techniques in the curriculum of undergraduate IT courses during the training of students. Design sprint (DS) in conjunction with project-based learning (PBL) provides an effective method to achieve the quality of software products when using UX techniques and creativity. One of the key features of PBL is the ability to generate artifacts with your application to solve real and non-trivial problems. This paper presents a systematic literature review (SLR) to investigate the hypothesis that joining the DS concepts with PBL identifies the user experience as one of the main attributes of quality and authenticity to be achieved in the software development projects. The objective of the SLR is to analyze how the PBL units that produce authentic software are executed. Furthermore, two case studies are reported that explore how DS behaves in a reduced number of classes when it is desired to generate a functional prototype that will be developed in a PBL unit. As a result of the SLR, it has been identified that Scrum is the software development process most used in PBL units that generate authentic software. The identified works report that the students are usually divided into groups of 2-5 people during the activities and the monitors play an important role in the quality of the software produced. The result of the case study indicates that the limited time for PBL conduction was a complicating factor in this adaptation and that students feel that their own learning and participation are positive in DS-based classes, as well as that DS provides insights that may be useful in a PBL context.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1191	1212		10.1007/s12652-019-01285-3													
J								Interval fuzzy number-based approach for modeling an uncertain fuzzy yield learning process	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Yield; Learning; Interval fuzzy number; Mixed binary nonlinear programming	COLLABORATIVE INTELLIGENCE APPROACH; AGGREGATION OPERATORS; LINEAR-REGRESSION; SET APPROACH; EXTENSION; CONSENSUS; SELECTION; SYSTEMS	An interval fuzzy number-based approach was proposed in this study to model an uncertain yield learning process. The study aimed to overcome the limitations of present methods, wherein the lower and upper bounds of the yield are generally determined by few extreme cases, thus resulting in an unacceptable widening of the yield range. In the proposed interval fuzzy number-based approach, the range of yield was divided into two sections, namely inner and outer sections, which corresponded with the lower and upper membership functions of a fuzzy yield forecast based on interval fuzzy numbers, respectively. To fulfill different managerial objectives, in this approach, all actual values are included in the outer section, whereas most of these values fall within the inner section. To derive the values of parameters in a fuzzy yield learning model based on interval fuzzy numbers, a mixed binary nonlinear programming model was proposed and optimized. The interval fuzzy number-based approach was applied to two real-time cases for evaluating its effectiveness. According to experimental results, the performance of the proposed method was superior to that of several existing methods, particularly in terms of forecasting precision for the average range. Forecasting accuracy obtained using the interval fuzzy number-based approach was satisfactory.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1213	1223		10.1007/s12652-019-01302-5													
J								The application of an amended FCA method on knowledge acquisition and representation for interpreting meteorological services	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Formal concept analysis; Knowledge acquisition; Knowledge representation; Description logic; Meteorological services	FORMAL CONCEPT ANALYSIS; CONCEPT LATTICE; MANAGEMENT; CONTEXTS	In the process of building an ontology-based knowledge base, developers need to transform the knowledge within the domain into a system conceptual framework that can be processed. This process is usually divided into two parts: knowledge acquisition and knowledge representation. If the way of acquiring knowledge is arbitrary, it is easy to form a concept that does not have consensus. When generating a conceptual hierarchy, it also needs to face the transformation of level confirmation and expression. Based on formal concept analysis (for knowledge acquisition) and description logic (for knowledge representation), this paper explores the use, problems and inconsistencies of these two aspects, and proposes an amended method. In addition, a method of exploring how to develop an unknown concept is proposed. This paper provides an ontology case based on the application amended method for the meteorological service field to solve the problem that the hidden knowledge in the meteorological service field is difficult to find and the concept processing is inaccurate, and the Formal Concept Analysis and implementation process of the meteorological service field are expounded. The method after the correction is to collect abstract and objective concept formation factors, and finally name the specific and subjective concepts. The formation of the concept of ontology is more in line with the cognitive development process.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1225	1239		10.1007/s12652-019-01305-2													
J								Pyramid and multi kernel based local binary pattern for texture recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi kernel LBP; Pyramid structure; Local binary pattern; Texture analysis; Texture classification	FACIAL IMAGE RECOGNITION; QUADRUPLE PATTERN; NUMBER PATTERN; CLASSIFICATION; DESCRIPTOR	In this article, a novel pyramid and multi kernel based method is proposed to increased success of the local binary pattern (LBP). Signum, ternary and quaternary binary feature extraction functions are used together and these are utilized as mathematical kernel of the LBP. In order to extract features in depth, pyramid model is used. Texture images are resized in the 4 levels to create pyramid. Finally, 5120 features are extracted from each level. In the feature reduction phase, principle component analysis is considered and linear discriminant analysis is utilized as classifier. To obtain numerical results, UIUC, Outex and USPTex datasets were used. The proposed method was compared to the other state of art texture classification methods. The recognition rates were calculated as 96.10%, 89.90% and 97.30% for UIUC, Outex and USPTex respectively. The robustness tests were performed using the Gaussian and salt and pepper noises. The best accuracy rates of the noisy images were calculated as 79.5% and 94.3% respectively. The experimental results proved the success of the proposed method.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1241	1252		10.1007/s12652-019-01306-1													
J								Forecasting heating and cooling loads of buildings: a comparative performance analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Heating load; Cooling load; Building; Deep neural network; Gradient boosted machine; Gaussian process regression; Minimax probability machine regression	EXTREME LEARNING-MACHINE; ENERGY PERFORMANCE; RESIDENTIAL BUILDINGS; THERMAL COMFORT; HYBRID MODEL; PREDICTION; CONSUMPTION; CONSERVATION; EFFICIENCY; NETWORKS	Heating load and cooling load forecasting are crucial for estimating energy consumption and improvement of energy performance during the design phase of buildings. Since the capacity of cooling ventilation and air-conditioning system of the building contributes to the operation cost, it is ideal to develop accurate models for heating and cooling load forecasting of buildings. This paper proposes a machine-learning technique for prediction of heating load and cooling load of residential buildings. The proposed model is deep neural network (DNN), which presents a category of learning algorithms that adopt nonlinear extraction of information in several steps within a hierarchical framework, primarily applied for learning and pattern classification. The output of DNN has been compared with other proposed methods such as gradient boosted machine (GBM), Gaussian process regression (GPR) and minimax probability machine regression (MPMR). To develop DNN model, the energy data set has been divided into training (70%) and testing (30%) sets. The performance of proposed model was benchmarked by statistical performance metrics such as variance accounted for (VAF), relative average absolute error (RAAE), root means absolute error (RMAE), coefficient of determination (R-2), standard deviation ratio (RSR), mean absolute percentage error (MAPE), Nash-Sutcliffe coefficient (NS), root means squared error (RMSE), weighted mean absolute percent error (WMAPE) and mean absolute percentage Error (MAPE). DNN and GPR have produced best-predicted VAF for cooling load and heating load of 99.76% and 99.84% respectively.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1253	1264		10.1007/s12652-019-01317-y													
J								SD-IoV: SDN enabled routing for internet of vehicles in road-aware approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software defined networking (SDN); Internet of vehicles (IoV); Road-aware approach; Edge controller (EC)	SOFTWARE-DEFINED NETWORKING; PROTOCOL	Proposing an optimal routing protocol for internet of vehicles with reduced overhead has endured to be a challenge owing to the incompetence of the current architecture to manage flexibility and scalability. The proposed architecture, therefore, consolidates an evolving network standard named as software defined networking in internet of vehicles. Which enables it to handle highly dynamic networks in an abstract way by dividing the data plane from the control plane. Firstly, road-aware routing strategy is introduced: a performance-enhanced routing protocol designed specifically for infrastructure-assisted vehicular networks. In which roads are divided into road segments, with road side units for multi-hop communication. A unique property of the proposed protocol is that it explores the cellular network to relay control messages to and from the controller with low latency. The concept of edge controller is introduced as an operational backbone of the vehicle grid in internet of vehicles, to have a real-time vehicle topology. Last but not least, a novel mathematical model is estimated which assists primary controller in a way to find not only a shortest but a durable path. The results illustrate the significant performance of the proposed protocol in terms of availability with limited routing overhead. In addition, we also found that edge controller contributes mainly to minimizes the path failure in the network.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1265	1280		10.1007/s12652-019-01319-w													
J								Genetic algorithm based optimized leach protocol for energy efficient wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; LEACH; GA; Throughput; PDR and energy consumption		Wireless sensor network (WSN) comprises of huge amount of sensing nodes being used for collecting the data in different situations. WSN finds applications mostly to gather information from remote places such as in environment monitoring, military, in transportation security and so on. The main problem in WSN is the availability of limited energy resources. For enhancing energy efficiency with the lifespan of sensor nodes, in this research, an energy effective routing protocol, low energy adaptive clustering hierarchy (LEACH) along with optimization algorithm genetic algorithm (GA) has been presented. LEACH is a hierarchical protocol which converts the sensor nodes into cluster heads (CH), and CH gathers and compress the data and send it to the target node. Genetic algorithm helps to find the optimal route by using its fitness function. After simulating the code in MATLAB, the energy consumption rate up to 17.39% has been reduced when GA is used. At last, comparison between proposed work and existing work is performed to determine the efficiency of the proposed work.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1281	1288		10.1007/s12652-019-01382-3													
J								A chaotic colour image encryption scheme combining Walsh-Hadamard transform and Arnold-Tent maps	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Chaotic image encryption; Chaotic maps; Walsh-Hadamard transform; Confusion; Diffusion; Arnold map; Tent map	LEVEL PERMUTATION; ALGORITHM	The third party misuse and manipulation of digital images is a treat to security and privacy of human subjects. Image encryption in the internet of things era becomes more important with edge computing and growth in intelligent consumer electronic devices. In this paper, we report a chaos-based cryptographic algorithm using Walsh-Hadamard transform and chaotic maps for encrypting images. The images are processed channel-wise and two different chaotic maps called Arnold and Tent maps are used for enciphering. The experimental results show that the random chaotic ranges and complex behaviours of chaotic maps improved both the keyspace and security of image encryption-decryption system.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1289	1308		10.1007/s12652-019-01385-0													
J								An efficient image encryption using non-dominated sorting genetic algorithm-III based 4-D chaotic maps Image encryption	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image encryption; 4-D chaotic map; NSGA-III; Security evaluation		Chaotic maps are extensively utilized in the field of image encryption to generate secret keys. However, these maps suffer from hyper-parameters tuning issues. These parameters are generally selected on hit and trial basis. However, inappropriate selection of these parameters may reduce the performance of chaotic maps. Also, these hyper-parameters are not sensitive to input images. Therefore, in this paper, to handle these issues, a non-dominated sorting genetic algorithm-III (NSGA) based 4-D chaotic map is designed. Additionally, to improve the computational speed of the proposed approach, we have designed a novel master-slave model for image encryption. Initially, computationally expensive operations such as mutation and crossover of NSGA-III are identified. Thereafter, NSGA-III parameters are split among two jobs, i.e., master and slave jobs. For communication between master and slave nodes, the message passing interface is used. Extensive experimental results reveal that the proposed image encryption technique outperforms the existing techniques in terms of various performance measures.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1309	1324		10.1007/s12652-019-01493-x													
J								Energy efficient optimized rate based congestion control routing in wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sensor networks; Congestion; Clustering; Routing	TRAFFIC MANAGEMENT; ALGORITHM; PROTOCOL	A wireless sensor network is designed to facilitate various real time applications, constituting a wide range of sensor nodes. In order to provide energy efficient transmissions, a novel congestion control mechanism is proposed on optimized rate. Here, the rate-based congestion control algorithm based on cluster routing is introduced to reduce energy consumption throughout the network. Rate control process reduces the end to end delay to improve network life time for large simulation period. Initially, nodes are clustered by the hybrid K-means and Greedy best first search algorithms. After that, the rate control is performed using firefly optimization strategy which is suitable for high packet delivery ratio. Finally, packets are sent with maximum throughput using Ant Colony Optimization-based routing. The simulation is performed on the MATLAB simulation platform. Finally, performances are evaluated with respect to, average delay in end to end node, delivery ratio of packets, throughput, energy efficiency, energy consumption and reliability.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1325	1338		10.1007/s12652-019-01449-1													
J								Efficient service recommendation using ensemble learning in the internet of things (IoT)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Service recommendation; Ensemble Learning; Internet of things (IoT); Binary decision tree	ALGORITHMS; PREDICTION; SYSTEMS	Internet of things (IoT) which has become widespread in various fields leads to a great variety in the provision of services to users. This variety in IoT services in turn pose challenges for users regarding their selections of appropriate services. Appropriate and proper service selection results in a better user satisfaction. In this regard, recommender systems (RS) are developed which help users to choose among various IoT services. Each user has his own characteristics in terms of profile and previous service selection, thus, customized methods should be proposed based on each user's own characteristics and service interests. In this paper, for an IoT environment, which includes a large number of objects, services and users, some service RSs are proposed. In the proposed RSs, user profiles and previous users' activities (in terms of service usage) are used based on which a binary decision tree has been made. The created decision tree is used for user classification in terms of their profile and service interest. The output of each RS is then ranked and combined using ensemble learning techniques, which improve the accuracy and efficiency of the recommendations. The result of ensemble learning is thus customized according to each user profile and service interest. The proposed RS is evaluated on a collected dataset and results confirm the high efficiency of the method presented in this research.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1339	1350		10.1007/s12652-019-01451-7													
J								A location privacy protection scheme based on random encryption period for VSNs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										VSNs; Location privacy; Mix-zone		Vehicle social networks (VSNs) are VANETs characterized by social aspect. For VSNs, it's very crucial to ensure its location privacy. Pseudonym is considered as an effective way to resist adversary's tracking, which ensures that each pseudonym holder communicates each other depending on such a virtual identity, but can not link to holder's real identity. This paper proposes a novel location privacy protection scheme for VSNs. With the help of RSU, when vehicles need to change the pseudonyms, they will cooperate with the neighbors to create the encrypted area through group key encryption and the external adversary cannot crack the message in this area. During this period, some vehicles change the pseudonyms jointly so that the external adversary can not associate the pseudonyms before and after, which achieves the goal of location privacy protection. The security and performance indicate the proposed scheme is reasonable.																	1868-5137	1868-5145				MAR	2020	11	3			SI		1351	1359		10.1007/s12652-019-01227-z													
J								An improved fuzzy risk analysis by using a new similarity measure with center of gravity and area of trapezoidal fuzzy numbers	SOFT COMPUTING										Fuzzy risk analysis; Generalized trapezoidal fuzzy numbers; Center of gravity; Similarity measure; Linguistic terms	GROUP DECISION-MAKING; PREFERENCE RELATIONS; CONSENSUS MODELS; COMPATIBILITY; OPERATORS; RANKING; SETS	This paper is to develop a new similarity measure of generalized trapezoidal fuzzy numbers (GTFNs). Firstly, a new method to calculate the center of gravity (COG) of GTFNs is put forward. Then, based on the drawbacks of existing similarity measures, a new similarity measure is proposed by using the COGs, areas, heights and geometric distances of GTFNs. Some properties of the proposed similarity measure are investigated. Moreover, with 32 different sets of GTFNs, we make a comparison between the proposed similarity measure and the existing similarity measures. Furthermore, two fuzzy risk analysis problems are analyzed by utilizing the new similarity measure and the results indicate that it is effective to deal with fuzzy risk analysis problems.																	1432-7643	1433-7479				MAR	2020	24	6					3923	3936		10.1007/s00500-019-04160-7													
J								A note on "Picture 2-tuple linguistic aggregation operators in multiple attribute decision making"	SOFT COMPUTING										Picture 2-tuple linguistic set; Operational laws of picture 2-tuple linguistic numbers; Adjusted picture 2-tuple linguistic weighted averaging operator		By combining picture fuzzy set and 2-tuple linguistic representation model, Wei et al. (Soft Comput 22(3):989-1002, 2018) proposed the concept of picture 2-tuple linguistic set and presented some operational laws of picture 2-tuple linguistic numbers (P2TLNs). On the basis of operational laws of P2TLNs, some picture 2-tuple linguistic aggregation operators have been further developed. However, some operational laws and results derived by aggregation operators proposed by Wei et al. (Soft Comput 22(3):989-1002, 2018) are conflict with definition of picture 2-tuple linguistic number (P2TLN). In this short note, we propose some novel operational laws of P2TLNs and develop one adjusted picture 2-tuple linguistic aggregation operator, i.e., the adjusted picture 2-tuple linguistic weighted averaging operator.																	1432-7643	1433-7479				MAR	2020	24	6					3937	3941		10.1007/s00500-019-04162-5													
J								A unified algorithm based on HTS and self-adapting PSO for the construction of octagonal and rectilinear SMT	SOFT COMPUTING										Hybrid transformation strategy; Particle swarm optimization; Self-adapting strategy; Unified algorithm; Octagonal steiner minimal tree; Rectilinear steiner minimal tree	PARTICLE SWARM OPTIMIZATION; X-ARCHITECTURE; GLOBAL ROUTER; TREES	The Steiner minimal tree (SMT) problem is an NP-hard problem, which is the best connection model for a multi-terminal net in global routing problem. This paper presents a unified algorithm for octagonal and rectilinear SMT construction based on hybrid transformation strategy (HTS) and self-adapting particle swarm optimization. Firstly, an effective HTS is proposed to enlarge the search space and improve the convergence speed. Secondly, the proposed HTS in the evolutionary process may produce an ineffective solution, and consequently the crossover and mutation operators of genetic algorithm (GA) based on union-find sets is proposed. Thirdly, a self-adapting strategy that can adjust the acceleration coefficients is proposed to further improve the convergence and the quality of the proposed algorithm. Finally, the hybrid transformation can be applied to GA and the proposed algorithm can be applied to rectilinear architecture. To our best knowledge, the proposed algorithm is the first unified algorithm to solve the SMT construction under both octagonal and rectilinear architecture. The experimental results show that the proposed algorithm can efficiently provide a better solution for SMT problem both in octagonal and rectilinear architectures than others. Moreover, the algorithm can obtain several topologies of SMT, which is beneficial for optimizing congestion in VLSI global routing stage.																	1432-7643	1433-7479				MAR	2020	24	6					3943	3961		10.1007/s00500-019-04165-2													
J								Fuzzy minimum spanning tree with interval type 2 fuzzy arc length: formulation and a new genetic algorithm	SOFT COMPUTING										Fuzzy minimum spanning tree; Genetic algorithm; Interval type 2 fuzzy set; Fuzzy graph; Type 1 fuzzy set	DECISION-MAKING; SETS; SEGMENTATION; NETWORKS	Fuzzy minimum spanning tree (FMST) has emerged from various real-life applications in different areas by considering uncertainty that exists in arc lengths of a fuzzy graph. In most relevant studies regarding FMST, type 1 fuzzy set was used to represent edge weights. Nonetheless, its membership values are totally crisp which is hard to determine its exact value by human perception. Interval type 2 fuzzy set (IT2FS) increases the number of degrees of freedom to express uncertainty of the edge weight and has more capacity to describe fuzzy information in a logically correct manner. In this paper, we propose the minimum spanning tree problem with undirected connected weighted interval type 2 fuzzy graph (FMST-IT2FS). Herein, the interval type 2 fuzzy set is used to represent the arc lengths of a fuzzy graph. Then, a new genetic algorithm is proposed to solve the FMST-IT2FS problem with the addition, ranking and defuzzification of IT2FSs being used. Illustrative examples are included to demonstrate the effectiveness of the proposed algorithm.																	1432-7643	1433-7479				MAR	2020	24	6					3963	3974		10.1007/s00500-019-04166-1													
J								Rough set-based feature selection for credit risk prediction using weight-adjusted boosting ensemble method	SOFT COMPUTING										Credit risk prediction (CRP); Preprocessing; Feature selection (FS); Missing value; Regression; Ensemble method; Friedman Test	ART CLASSIFICATION ALGORITHMS; SUPPORT VECTOR MACHINES; BANKRUPTCY PREDICTION; FINANCIAL DISTRESS; NEURAL-NETWORKS; CLASSIFIERS; MODELS; REGRESSION; BEHAVIOR; FAILURE	With the tremendous development of financial institutions, credit risk prediction (CRP) plays an essential role in granting loans to customers and helps them to minimize their loss because credit approval sometimes results in massive financial loss. So extra attention is needed to identify risky customer. Researchers have designed complex CRP models using artificial intelligence (AI) and statistical techniques to support the financial institutions to take correct business decisions. Though there are various statistical and AI methods available, the recent literature shows that the ensemble-based CRP model provides improved prediction results than single classifier system. The small increase in the performance of CRP model could result in a significant improvement in the profit of financial institutions and banks. This work proposes a weight-adjusted boosting ensemble method (WABEM) using rough set (RS)-based feature selection (FS) technique with the balancing and regression-based preprocessing called RS_RFS-WABEM. Regression is used to fill missing value in the records to improve the performance of CRP. Three credit datasets (Australia, German and Japanese) are chosen to validate the feasibility and effectiveness of the proposed ensemble method. The trade-off between the uncertainty and imprecise probability of the proposed classifier model is evaluated using the performance measures such as accuracy and area under the curve. Experimental results show that the proposed ensemble method performs better than other base and ensemble classifier methods.																	1432-7643	1433-7479				MAR	2020	24	6					3975	3988		10.1007/s00500-019-04167-0													
J								Incorporating decision makers' preferences into DEA and common weight DEA models based on the best-worst method (BWM)	SOFT COMPUTING										DEA; Best-worst method (BWM); Decision makers' preferences; Common weights	DATA ENVELOPMENT ANALYSIS; ELECTRICITY DISTRIBUTION; MEASURING EFFICIENCY; VALUE JUDGMENTS; UNCERTAIN DATA; RESTRICTIONS; FLEXIBILITY; PERFORMANCE; OPTIMIZATION; UNITS	Incorporating decision makers' (DMs') judgments and preferences into DEA models is very important in some real-world problems. This paper presents an integrated data envelopment analysis (DEA)-best-worst method (BWM)-for considering DMs' preferences in DEA and reducing flexibility in weights of inputs and outputs. First, the preferences vectors are designed using BWM, and then, a multi-objective DEA-BWM model is introduced. The proposed DEA-BWM model simultaneously maximizes the efficiency scores of DMUs and considers DMs' preferences about weights of inputs and outputs. Finally, a goal programming model is suggested for extending the DEA-BWM model and finding common weights of inputs and outputs based on the DMs' judgments. The proposed common weight DEA-BWM (CWDEA-BWM) model maximizes the efficiencies of DMUs, considers DMs' preferences and uses a set of common weights. In order to illustrate the capability of proposed models, a numerical example is solved. Moreover, the proposed DEA-BWM and common weight DEA-BWM models are applied to evaluate 39 Iranian electricity distribution companies, and the results are analyzed and compared. The results indicate that the proposed DEA-BWM and CWDEA-BWM models are suitable for incorporating DMs' preferences into DEA and fully ranking of DMUs.																	1432-7643	1433-7479				MAR	2020	24	6					3989	4002		10.1007/s00500-019-04168-z													
J								Kernel intuitionistic fuzzy entropy clustering for MRI image segmentation	SOFT COMPUTING										Intuitionistic fuzzy sets; Fuzzy entropy clustering; Kernel distance measure; Image segmentation; Magnetic resonance imaging	MAGNETIC-RESONANCE IMAGES; C-MEANS; SPATIAL INFORMATION; ALGORITHM; FUZZINESS; NEGATION; FCM	Fuzzy entropy clustering (FEC) is a variant of hard c-means clustering which utilizes the concept of entropy. However, the performance of the FEC method is sensitive to the noise and the fuzzy entropy parameter as it gives incorrect clustering and coincident cluster sometimes. In this work, a variant of the FEC method is proposed which incorporates advantage of intuitionistic fuzzy set and kernel distance measure termed as kernel intuitionistic fuzzy entropy c-means (KIFECM). While intuitionistic fuzzy set allows to handle uncertainty and vagueness associated with data, kernel distance measure helps to reveal the inherent nonlinear structures present in data without increasing the computational complexity. In this work, two popular intuitionistic fuzzy sets generators, Sugeno and Yager's negation function, have been utilized for generating intuitionistic fuzzy sets corresponding to data. The performance of the proposed method has been evaluated over two synthetic datasets, Iris dataset, publicly available simulated human brain MRI dataset and IBSR real human brain MRI dataset. The experimental results show the superior performance of the proposed KIFECM over FEC, FCM, IFCM, UPCA, PTFECM and KFEC in terms of several performance measures such as partition coefficient, partition entropy, average segmentation accuracy, dice score, Jaccard score, false positive ratio and false negative ratio.																	1432-7643	1433-7479				MAR	2020	24	6					4003	4026		10.1007/s00500-019-04169-y													
J								A heuristic fuzzy algorithm for assessing and managing tourism sustainability	SOFT COMPUTING										Evolutionary algorithms; Threshold accepting; Fuzzy sets; Multi-objective optimization; Sustainability	INDICATORS; SYSTEMS; MANAGEMENT	"Smartness" and "sustainability" are gaining growing attention from both practitioners and policy makers. "Smartness" and "sustainability" assessments are of crucial importance for directing, in a systemic perspective, the decision-making process toward sustainability and smart growth objectives. Sustainability assessment is a major challenge due to the multidisciplinary aspects involved that make the evaluation process complex and hinder the effectiveness of available monitoring tools. To achieve the assessment objective, we introduce an enhanced fuzzy logic-based framework for handling the inherent uncertainty and vagueness of the involved variables: we apply our approach to Italy, and we compare it with two other sustainability methodologies. We also perform a correlation analysis to assess the relationship between our ranking results and the attained quality of life scores. Our approach ensures a high level of versatility that makes its use possible jointly with (or alternatively to) other existing methodologies such as the Global Sustainable Tourism Council Criteria and the United Nations World Tourism Organization destination-level indicators.																	1432-7643	1433-7479				MAR	2020	24	6					4027	4040		10.1007/s00500-019-04170-5													
J								Palmprint identification combining hierarchical multi-scale complete LBP and weighted SRC	SOFT COMPUTING										Palmprint recognition; Local binary pattern (LBP); Complete LBP (CLBP); Hierarchical multi-scale CLBP (HMS-CLBP); Sparse representation-based classification (SRC); Weighted SRC (WSRC)	SPARSE REPRESENTATION; RECOGNITION	Palmprint is one of the most reliable biometrics and has been widely used for human identification due to its high recognition accuracy and convenience for practical application. But the existing palmprint-based human identification system often suffers from image misalignment, pixel corruption and much computational time on the large database. An effective palmprint recognition method is proposed by combining hierarchical multi-scale complete local binary pattern (HMS-CLBP) and weighted sparse representation-based classification (WSRC). The hierarchical multi-scale local invariant texture features are extracted firstly from each palmprint by multi-scale local binary pattern (MS-LBP) and multi-scale complete local binary pattern (MS-CLBP) and are concatenated into one hierarchical multi-scale fusion feature vector. Then, WSRC is constructed by the Gaussian kernel distance, and use the Gaussian kernel distances between the fusion feature vectors of the training and testing samples. Finally, the sparse decomposition of testing samples is implemented by solving the optimization problem based on l(1) norm, and the palmprints are recognized by the minimum residuals. The proposed method inherits the advantages of CLBP and WSRC and has good rotation, illumination and occlusion invariance. The results on the PolyU and CASIA palmprint databases illustrate the good performance and rationale interpretation of the proposed method.																	1432-7643	1433-7479				MAR	2020	24	6					4041	4053		10.1007/s00500-019-04172-3													
J								Learning fuzzy cognitive maps with convergence using a multi-agent genetic algorithm	SOFT COMPUTING										Fuzzy cognitive maps; Convergence error; Multi-agent genetic algorithm		Fuzzy cognitive maps (FCMs) are generally applied to model and analyze complex dynamical systems. Recently, many evolutionary-based algorithms are proposed to learn FCMs from historical data by optimizing Data_Error, which is used to evaluate the difference between available response sequences and generated response sequences. However, when Data_Error is adopted as the fitness function for learning FCMs, two problems arise. One is that the optimization reaches the desired result slowly; the other is that the learned FCMs have high link density. To solve these problems, we propose another objective named as convergence error, which is inspired by the convergence of FCMs, to evaluate the difference between the convergent value of available response sequences and that of generated response sequences. In addition, a multi-agent genetic algorithm (MAGA), which is effective for large-scale global numerical optimization, is adopted to optimize convergence error for learning FCMs. To this end, a novel learning approach, a multi-agent genetic algorithm based on the convergence error (MAGA-Convergence), is proposed for learning FCMs. MAGA-Convergence needs less data, because the only initial value and convergent value of the available response sequences are needed for learning FCMs. In the experiments, MAGA-Convergence is applied to learn the FCMs of synthetic data and the benchmarks DREAM3 and DREAM4 for gene regulatory network reconstruction. The experimental results show that the learned FCMs are sparse and could be learned in much fewer generations than other learning algorithms.																	1432-7643	1433-7479				MAR	2020	24	6					4055	4066		10.1007/s00500-019-04173-2													
J								Convolutional neural networks for sleep stage scoring on a two-channel EEG signal	SOFT COMPUTING										Convolutional neural networks; Deep learning; Electroencephalography; Polysomnography; Signal processing	DECISION-SUPPORT-SYSTEM; AUTOMATED IDENTIFICATION; RESOURCE; FEATURES; ADULTS; MODEL	Sleeping problems have become one of the major diseases all over the world. To tackle this issue, the basic tool used by specialists is the polysomnogram, which is a collection of different signals recorded during sleep. After its recording, the specialists have to score the different signals according to one of the standard guidelines. This process is carried out manually, which can be a high-time-consuming task and very prone to annotation errors. Therefore, over the years, many approaches have been explored in an attempt to support the specialists in this task. In this paper, an approach based on convolutional neural networks is presented, where an in-depth comparison is made in order to determine the convenience of using more than one signal simultaneously as input. This approach is similar to the one made in other problems although, additionally to those models, they were also used as parts of an ensemble model to check whether any useful information can be extracted from processing a single signal at a time which the dual-signal model cannot identify. Tests have been performed by using a well-known dataset called sleep-EDF-expanded, which is the most commonly used dataset as benchmark for this problem. The tests were carried out with a leave-one-out cross-validation over the patients, which ensures that there is no possible contamination between training and testing. The resulting proposal is a network smaller than previously published ones, but it overcomes the results of any previous models on the same dataset. The best result shows an accuracy of 92.67% and a Cohen's kappa value over 0.84 compared to human experts.																	1432-7643	1433-7479				MAR	2020	24	6					4067	4079		10.1007/s00500-019-04174-1													
J								Solving multi-objective optimization problems using self-adaptive harmony search algorithms	SOFT COMPUTING										Harmony search algorithm; Optimization method; Pareto optimization; Multi-objective optimization; Self-adaptive		In recent years, there have been many multi-objective evolutionary algorithms proposed to solve multi-objective optimization problems. These evolutionary algorithms generate many solutions for iterations and move to the true Pareto optimal region gradually. As expected, since the harmony search algorithm can also iterate over a large number of solutions (in HM memory) and moves to the true Pareto optimal region, we use it to solve multi-objective optimization problems. In this paper, the proposed system architecture can be divided into two phases. In the first phase, we aim to search feasible solution regions as widely as possible in the entire process. In the second phase, we focus on searching optimized solutions stepwise in the feasible solution regions. Since the proposed algorithm uses many parameters, we adjust some of them in a self-adaptive way and call the algorithm self-adaptive. In the experiments, we use the eleven well-known multi-objective problems and three many-objective problems to examine the proposed algorithm and other existing algorithms, based on five performance indicators. As a result, our algorithm achieves better performances than the others in inverted generational distance, hypervolume, and spread indicators.																	1432-7643	1433-7479				MAR	2020	24	6					4081	4107		10.1007/s00500-019-04175-0													
J								Some new performance definitions of second-order fuzzy systems	SOFT COMPUTING										Fuzzy differential equations; Horizontal membership functions; Granular differentiability; Fuzzy eigenvalues		The mathematical models are usually considered for the assessment of a dynamical system performance. Using such models often is based on an assumption which takes all the system parameter values as crisp values into account. However, the parameter values may not be always determined exactly. Since fuzzy logic has been well known as an effective tool in modeling uncertainty, in this paper, fuzzy numbers are used in modeling of dynamical systems. In this paper, concentrate on second-order dynamical systems with fuzzy parameters as they play a major role in many aspects of control theory. New performance definitions such as fuzzy rise time, fuzzy settling time, and fuzzy maximum overshoot are presented. The new definitions for such systems provide a more comprehensive view of the system output which helps for designing a better control. Also, the solutions of an RLC circuit and intelligent transformer with fuzzy parameter based on these definitions are considered.																	1432-7643	1433-7479				MAR	2020	24	6					4109	4120		10.1007/s00500-019-04177-y													
J								Solution of a fuzzy global optimization problem by fixed point methodology using a weak coupled contraction	SOFT COMPUTING										Fuzzy metric space; Coupled contraction; Coupled proximity point; Fuzzy P-property; Global optimality	PROXIMITY POINT; NONLINEAR CONTRACTIONS; THEOREMS; PRINCIPLE; MAPPINGS	In the present work, we consider the global optimization problem of obtaining distance between two subsets of a fuzzy metric space and solve it by fixed point methodology through the determination of two different pairs of points each of which determines the fuzzy distance. We use fuzzy weak coupled contractions for that purpose. The problem is well studied in metric spaces where it is known as a proximity point problem. We use geometric notions in fuzzy metric spaces. Our result is valid for arbitrary continuous t-norms associated with the fuzzy metric space. The problem is solved by reducing it to that of finding optimal approximate solution of a fuzzy coupled fixed point equation. We also obtain a coupled fixed point result as a consequence of our main theorem. The main result is illustrated with an example.																	1432-7643	1433-7479				MAR	2020	24	6					4121	4129		10.1007/s00500-019-04179-w													
J								Minimizing the state of health degradation of Li-ion batteries onboard low earth orbit satellites	SOFT COMPUTING										Battery; Satellites; Scheduling; Li-Ion; Heuristic; SOH	CHARGE ESTIMATION	Satellites have a tangible impact on our daily lives; they provide us with many services like communication, global positioning, etc. Satellite batteries are expected to deliver the power demand at any time during the period of an eclipse, or when the power received from the onboard solar panels is not sufficient. The focus of this research is to develop an energy management mathematical model that reduces the state of health degradation of a battery in a low earth orbit (LEO) satellite. This improves the battery lifetime; thus, increasing the length of time a LEO satellite can stay in service. The developed model for a LEO satellite is solved separately for meeting three different objectives. In addition to the model, a heuristic approach is developed, and the results are compared to those obtained from the above-mentioned model. In this endeavor, data are collected for an existing LEO satellite, Nayif-1, in order to analyze the current battery behavior in space and to compare it with the developed model and heuristics. Sensitivity analysis is conducted to observe the effects of altering different parameters of the model. The results presented in this research show that minimizing the sum of products of the battery state switches and the battery current yields the best results by enhancing the lifetime of the battery by 8 days and providing 122 more cycles than that observed in the data from Nayif-1, assuming that the DOD of the battery remains constant throughout all orbits.																	1432-7643	1433-7479				MAR	2020	24	6					4131	4147		10.1007/s00500-019-04180-3													
J								Formalizing UML/OCL structural features with FoCaLiZe	SOFT COMPUTING										Software engineering; Model-Driven Engineering; Formal methods; Functional programming; UML modeling; Semantics	UML	Unified Modeling Language (UML) is the de facto standard for the development of software models, and Object Constraint Language (OCL) is used within UML models to specify model constraints. Several UML/OCL tools provide Model-Driven Engineering transformation into general object-oriented programming languages such as Java and C++. But the latter did not provide mechanisms for the specification and the verification of OCL constraints. In this context, formal methods are largely used for the specification of UML/OCL models and the verification of their OCL constraints. However, the divergence between UML (object-oriented modeling) and formal methods (mathematical- and logical-based tools) leads in general to ignore most UML/OCL architectural and conceptual features such as OCL constraints simple and multiple inheritance, late binding, template binding and dependencies. To address the formalization of these features, we have used FoCaLiZe, an object-oriented development environment using a proof-based formal approach. More precisely, we propose a formal transformation of the essential UML/OCL features into FoCaLiZe specifications. The derived formal model reflects perfectly the structural features of the original UML/OCL model. In addition, it is possible to check and prove model properties using Zenon, the automatic theorem prover of FoCaLiZe.																	1432-7643	1433-7479				MAR	2020	24	6					4149	4164		10.1007/s00500-019-04181-2													
J								Dynamic urban land-use change management using multi-objective evolutionary algorithms	SOFT COMPUTING										Dynamic urban land-use change; Multi-objective optimization; NSGA-II; Clustering; Decision support; Soft computing	USE SUITABILITY ANALYSIS; SPATIAL OPTIMIZATION; DECISION-MAKING; USE ALLOCATION; GIS; INTEGRATION; SEARCH; SYSTEM	Frequent land-use changes in urban areas require an efficient and dynamic approach to reform and update detailed plans by re-arrangement of surrounding land-uses in case of change in one or several urban land-uses. However, re-arrangement of land-uses is problematic, since a variety of conflicting criteria must be considered and satisfied. This paper proposes and examines a two-step approach to resolve the issue. The first step adopts a multi-objective optimization technique to obtain an optimal arrangement of surrounding land-uses in case of change in one or several urban land-uses, whereas the second step uses clustering analysis to produce appropriate solutions for decision makers from the outputs of the first step. To present and assess the approach, a case study was conducted in Tehran, the capital of Iran. To satisfy the first step, four conflicting objective functions including maximization of consistency, maximization of dependency, maximization of suitability and maximization of compactness were defined and optimized using non-dominated sorting genetic algorithm. Per-capita demand was also employed as a constraint in the optimization process. Clustering analysis based on ant colony optimization was used to satisfy the second step. The results of the optimization were satisfactory both from a convergence and from a repeatability point of view. Furthermore, the objective functions of optimized arrangements were better than existing land-use arrangement in the area, with the per-capita demand deficiency significantly compensated. The approach was also communicated to urban planners in order to assess its usefulness. In conclusion, the proposed approach can extensively support and facilitate decision making of urban planners and policy makers in reforming and updating existing detailed plans after land-use changes.																	1432-7643	1433-7479				MAR	2020	24	6					4165	4190		10.1007/s00500-019-04182-1													
J								A game-based resource pricing and allocation mechanism for profit maximization in cloud computing	SOFT COMPUTING										Revenue maximization; Dynamic resource pricing; Resource allocation; Stackelberg game; Cloud computing	GENERALIZED NASH EQUILIBRIA; REVENUE MAXIMIZATION; PROCUREMENT	In cloud computing environment, Software as a Service (SaaS) providers offer diverse software services to customers and commonly host their applications and data on the infrastructures supplied by Infrastructure as a Service (IaaS) providers. From the perspective of economics, the basic challenges for both SaaS and IaaS providers are to design resource pricing and allocation policies to maximize their own final revenue. However, IaaS providers seek an optimal price policy of virtual machines to generate more revenue, while SaaS providers want to minimize the cost of using infrastructure resources, and comply with service-level agreement contracts with users at the same time. In this situation, there exists conflict in maximizing revenue of both IaaS and SaaS providers simultaneously. In this paper, we model this revenue maximization problem as the Stackelberg game and analyze the existence and uniqueness of the game equilibrium. Moreover, considering the impact of resource price on users' willing to access service, we propose a dynamic pricing mechanism to maximize the revenue of both SaaS and IaaS providers. The simulation results demonstrate that, compared to fixed pricing and auction-based pricing mechanisms, the proposed mechanism is superior in the revenue maximization and resource utilization.																	1432-7643	1433-7479				MAR	2020	24	6					4191	4203		10.1007/s00500-019-04183-0													
J								Reconstructing gene regulatory networks via memetic algorithm and LASSO based on recurrent neural networks	SOFT COMPUTING										Gene regulatory networks; Memetic algorithm; LASSO; Recurrent neural network	MODELS; DECOMPOSITION; INFERENCE	Reconstructing gene regulatory networks (GRNs) from gene expression data is an important and challenging problem in system biology. In general, the problem of reconstructing GRNs can be modeled as an optimization problem. Recurrent neural network (RNN) has been widely used for GRNs. However, in a real GRN, the number of genes is very large and the relationships between genes are usually very sparse. In this paper, we design a memetic algorithm to learn partial parameters of RNN, and develop a framework based on the least absolute shrinkage and selection operator (LASSO) to reconstruct GRNs based on RNN, which is termed as MALASSO(RNN)-GRN. In the LASSO, the task of reconstructing GRNs is decomposed into a sparse signal reconstructing problem. In the experiments, MALASSO(RNN)-GRN is applied on synthetic data and well-known benchmark datasets DREAM3 and DREAM4. The effect of parameters on MALASSO(RNN)-GRN is discussed, and MALASSO(RNN)-GRN is compared with three other algorithms which are all state-of-the-art RNN learning algorithms. The results show that MALASSO(RNN)-GRN performs best and is capable of reconstructing large-scale GRNs. Graphic abstract [GRAPHICS] .																	1432-7643	1433-7479				MAR	2020	24	6					4205	4221		10.1007/s00500-019-04185-y													
J								Hesitant fuzzy soft multisets and their applications in decision-making problems	SOFT COMPUTING										Soft multiset; Hesitant fuzzy soft multiset; Comparison table		In this paper, we introduce some important and basic issues of hesitant fuzzy soft multisets and present some results for hesitant fuzzy sets. The main results of the current branch are studied, and some of its structural properties are established such as the neighborhood hesitant fuzzy soft multisets, interior hesitant fuzzy soft multisets, hesitant fuzzy soft multi-topological spaces and hesitant fuzzy soft multi-basis. Therefore, we show that how to apply the concept of hesitant fuzzy soft multisets in decision-making problems.																	1432-7643	1433-7479				MAR	2020	24	6					4223	4232		10.1007/s00500-019-04187-w													
J								An efficient neural network for solving convex optimization problems with a nonlinear complementarity problem function	SOFT COMPUTING										One-layer neural networks; Convex programming; Nonlinear complementarity problem	NCP-FUNCTIONS; PROGRAMMING PROBLEMS; FAMILY; MODEL	In this paper, we present a one-layer recurrent neural network (NN) for solving convex optimization problems by using the Mangasarian and Solodov (MS) implicit Lagrangian function. In this paper by using Krush-Kuhn-Tucker conditions and MS function the NN model was derived from an unconstrained minimization problem. The proposed NN model is one layer and compared to the available NNs for solving convex optimization problems, which has a better performance in convergence time. The proposed NN model is stable in the sense of Lyapunov and globally convergent to optimal solution of the original problem. Finally, simulation results on several numerical examples are presented and the validity of the proposed NN model is demonstrated.																	1432-7643	1433-7479				MAR	2020	24	6					4233	4242		10.1007/s00500-019-04189-8													
J								An integrated decision making model for supplier and carrier selection with emphasis on the environmental factors	SOFT COMPUTING										Green supply chain; Supplier selection; Carrier selection; Emission of greenhouse gases; Multi-objective meta-heuristic imperialist competitive algorithm; Crowding distance	IMPERIALIST COMPETITIVE ALGORITHM; OPTIMIZATION; CHAIN	A suitable green supply chain network can significantly affect both the supply chain and the environment. Such network should guide the supply chain toward an efficient and effective management to increase the profit, desirable impacts on the environments and responsiveness to the customers' requests. In this research, a green supply chain with limited greenhouse gas emission was designed which could reduce the chain costs by simultaneous selection of the supplier and carriers with various capacities. Thus, in this research a bi-objective nonlinear programming model was proposed which is aimed to select the carriers between the chain levels and select the supplier based on the quality of the consumed material. Moreover, the delivery time to customers and emission from transportation and production were the other constraints of the problem. The first goal was to minimize the total chain costs while reduction in the rejection of the consumed material was in the second rank. In order to validate the proposed model, several numerical problems were randomly generated and solved using GAMS optimization software. Since the problem is NP-hard and its solution time increased exponentially by increase in the problem dimension, a multi-objective meta-heuristic imperialist competitive algorithm was proposed to solve the problem in large scales. Crowding distance was also used to rank the solutions of one front. The computational results and comparisons by indices like mean distance from the ideal were employed to describe the algorithm efficiency. The results showed that features such as carrier selection and environmental factors can enable the decision process of supplier selection to be well approximated with the real-world situation, showing the potential usefulness of these concepts.																	1432-7643	1433-7479				MAR	2020	24	6					4243	4258		10.1007/s00500-019-04190-1													
J								A multivariate grey prediction model with grey relational analysis for bankruptcy prediction problems	SOFT COMPUTING										Grey prediction; Time series; Multi-criteria decision making; Feature selection; Bankruptcy prediction	ALGORITHM; EMISSIONS; OUTPUT	Regarding bankruptcy prediction as a kind of grey system problem, this study aims to develop multivariate grey prediction models based on the most representative GM(1, N) for bankruptcy prediction. There are several distinctive features of the proposed grey prediction model. First, to improve the prediction performance of the GM(1, N), grey relational analysis is used to sift relevant features that have the strongest relationship with the class feature. Next, the proposed model effectively extends the multivariate grey prediction model for time series to bankruptcy prediction irrespective of time series. It turns out that the proposed model uses the genetic algorithms to avoid indexing by time and using the ordinary least squares with statistical assumptions for the traditional GM(1, N). The empirical results obtained from the financial data of Taiwanese firms in the information and technology industry demonstrated that the proposed prediction model performs well compared with other GM(1, N) variants considered.																	1432-7643	1433-7479				MAR	2020	24	6					4259	4268		10.1007/s00500-019-04191-0													
J								The effects of consumer confusion on hotel brand loyalty: an application of linguistic nonlinear regression model in the hospitality sector	SOFT COMPUTING										Interaction effect; Nonlinear regression; Fuzzy Likert scale; Consumer confusion; Hotel brand loyalty	WORD-OF-MOUTH; SCALE DEVELOPMENT; SERVICE; CHOICE; PERCEPTIONS; PRONENESS; INTENTION; TOURISTS; QUALITY; MARKET	The aim of the study is to estimate the interaction and quadratic relationships between dimensions by estimating a model for the confusion dimensions that affect hotel brand loyalty, thus providing the interested parties with a perspective and direction regarding consumer confusion. This study also aimed to strengthen the use of FLS in the field of social sciences and will use this method to transform the discrete ordinal variable into a continuous variable while preserving the semantic meaning. Four hundred and six individuals participated in the study. Hypotheses demonstrating the interaction and quadratic effects between the continuous variables have been analysed using nonlinear multiple regression analysis. This study proposes a survey-based method to estimate a model for the confusion dimensions that affect hotel brand loyalty. The results demonstrated that ambiguity confusion, overload confusion, similarity confusion, quadratic effect of similarity confusion and interaction of ambiguity, overload and similarity confusion decrease the hotel brand loyalty. Also, quadratic effect of ambiguity confusion, interaction of ambiguity and overload confusion, interaction of overload and similarity confusion, interaction of ambiguity and similarity confusion increase the hotel brand loyalty. Despite its importance for marketing and consumer behaviour, the definition, measurement, dimensions and existing results of consumer confusion have begun to be discussed and examined recently in a limited scope. Studies have demonstrated that consumer confusion about tourism products is a non-functional and under-evaluated area but also is utmost prominent for tourism product. This study aimed to obtain a stronger model in which all the interactions between variables and their (quadratic) increasing effects are considered using a nonlinear regression model.																	1432-7643	1433-7479				MAR	2020	24	6					4269	4281		10.1007/s00500-019-04192-z													
J								MCDM based on new membership and non-membership accuracy functions on trapezoidal-valued intuitionistic fuzzy numbers	SOFT COMPUTING										Interval-valued intuitionistic fuzzy number; Trapezoidal-valued intuitionistic fuzzy numbers; Multi-criteria decision making; Index matrix; Accuracy function	COMPLETE RANKING	Ranking of trapezoidal-valued intuitionistic fuzzy numbers (TVIFNs) plays an important role in multi-criteria decision making (MCDM) based on the TVIFNs. The main objective of this paper is to introduce new membership and non-membership accuracy functions on the classes of interval-valued intuitionistic fuzzy numbers (IVIFNs) and TVIFNs by which the orderings on IVIFNs and TVIFNs are done. This paper reveals the better part of the proposed accuracy functions than the existing or previous functions. Further, some operations on IVIFNs and TVIFNs are defined. Finally, a new method is proposed to solve the MCDM problem based on the multi-criteria trapezoidal-valued intuitionistic fuzzy index matrix and illustrated through numerical examples.																	1432-7643	1433-7479				MAR	2020	24	6					4283	4293		10.1007/s00500-019-04193-y													
J								A hybrid method for evaluating the effectiveness of giant systems with indicator correlations: an application for naval formation decision making in multiple scenarios	SOFT COMPUTING										Giant system; Effectiveness evaluation; Indicator correlation; Naval formation; Evidential reasoning approach; Prospect theory	PROSPECT-THEORY; MANAGEMENT EFFECTIVENESS; AGGREGATION	This article discusses the giant system effectiveness evaluation (GSEE) problem with inevitable correlation in indicator systems due to their high specificity and complexity and proposes a hybrid method that is then applied to the naval formation decision-making process in multiple scenarios. The indicator correlation in a large-scale system will generate bias in its evaluation of effectiveness; the proposal that the lower the correlation is, the better the performance of the evidential reasoning approach (ERA) has been proven mathematically. In light of this proposition, a corollary was put forward: Fewer indicators would improve the precision of the result of the ERA application when considering the correlation. Considering that the giant system can be split into respective subsystems, which can then be analyzed by experts in their own fields, a hybrid method was developed for the GSEE problem based on the ERA and prospect theory. The core of the method is the construction of a nonlinear optimization model (NOM) aimed at minimizing the correlation and maximizing the evaluation ability of the prospect value of the indicator system. By constraint, the NOM also includes the optimized weight value of each indicator. For demonstration purposes, a naval formation operation effectiveness evaluation (NFOEE) was performed to assess the feasibility of the proposed method and the NOM. The results show that the proposed method can solve the NFOEE effectively and allow the decision maker to obtain useful information for naval formation-type decisions in multiple scenarios. Furthermore, the evaluation method is a general tool that can be applied to other GSEE problems.																	1432-7643	1433-7479				MAR	2020	24	6					4295	4306		10.1007/s00500-019-04194-x													
J								Improving predictive uncertainty estimation using Dropout-Hamiltonian Monte Carlo	SOFT COMPUTING										Bayesian learning; Hamiltonian Monte Carlo; Dropout; Transfer learning; Classification	LANGEVIN	Estimating predictive uncertainty is crucial for many computer vision tasks, from image classification to autonomous driving systems. Hamiltonian Monte Carlo (HMC) is an sampling method for performing Bayesian inference. On the other hand, Dropout regularization has been proposed as an approximate model averaging technique that tends to improve generalization in large-scale models such as deep neural networks. Although HMC provides convergence guarantees for most standard Bayesian models, it do not handle discrete parameters arising from Dropout regularization. In this paper, we present a robust methodology for improving predictive uncertainty in classification problems, based on Dropout and HMC. Even though Dropout induces a non-smooth energy function with no such convergence guarantees, the resulting discretization of the Hamiltonian proves empirical success. The proposed method allows to effectively estimate the predictive accuracy and to provide better generalization for difficult test examples.																	1432-7643	1433-7479				MAR	2020	24	6					4307	4322		10.1007/s00500-019-04195-w													
J								Cloud service selection based on QoS-aware logistics	SOFT COMPUTING										Cloud computing; Logistics cloud services; QoS; Service selection		With the development of technologies such as cloud computing, Big Data, the Internet of Things, etc., Internet + logistics models are being sought by all parties, leading to the current rise of cloud service platforms for logistics. As such platforms combine many logistics services with similar functions, identification of methods to choose from among a large number of similar services to meet the personalized needs of customers has become especially important. In the work presented herein, QoS data are quantified and filtered through the establishment of quality of service (QoS) decision information systems. Meanwhile, using the variable precision rough set method, the weight of each QoS attribute index is calculated, then the similarity of services, to obtain a comprehensive sequence in terms of service quality that provides a basis for selection of the optimal service. The calculation and analysis results show that this method can effectively choose the best logistics service according to specific business needs.																	1432-7643	1433-7479				MAR	2020	24	6					4323	4332		10.1007/s00500-019-04196-9													
J								Hopf bifurcation of forced Chen system and its stability via adaptive control with arbitrary parameters	SOFT COMPUTING										Dynamical system; Forced Chen system; Hopf bifurcation; The first Lyapunov coefficient; Stability	CHAOS; DESIGN	In this article, forced Chen system is analyzed for nonlinear dynamical behavior. The chaotic behavior of forced Chen system is verified by phase portraits and sensitivity dependence of system upon initial condition. Hopf bifurcation for the complex system is derived and theorem of first Lyapunov coefficient is used to investigate the type of Hopf bifurcation. It is further shown that Hopf bifurcation exists only on two equilibrium points for the proposed chaotic model. In addition, an adaptive control technique is used to control unpredictable behavior for the forced Chen system. Global stability is achieved by constructing an energy type function through Lyapunov theory, whereas its error dynamics is used to synchronize two identical forced Chen systems. Numerical simulation results are used to validate analytical results given in this article and also to demonstrate effectiveness of the considered chaotic system.																	1432-7643	1433-7479				MAR	2020	24	6					4333	4341		10.1007/s00500-019-04197-8													
J								A holistic optimization approach for inverted cart-pendulum control tuning	SOFT COMPUTING										Inverted cart-pendulum system; Nonlinear control; Optimization; Holistic approach; Swing-up; Stabilization	KRILL HERD ALGORITHM; SWING-UP; LQR CONTROLLER; FUZZY-LOGIC; SYSTEM; STABILIZATION; DESIGN; TRACKING	The inverted cart-pendulum (ICP) is a nonlinear underactuated system, which dynamics are representative of many applications. Therefore, the development of ICP control laws is important since these laws are suitable to other systems. Indeed, many nonlinear control strategies have emerged from the control of the ICP. For these reasons, the ICP remains a canonical and fundamental benchmark problem in control theory and robotics that is of interest to the scientific community. Till now, the trial-and-error method is still widely applied for ICP controller tuning as well as the sequential tuning referring to tune the swing-up controller and thereafter, the stabilization controller. Therefore, the aim of this paper is to automate and facilitate the ICP control in one step. Thus, this paper proposes to holistically optimize ICP controllers. The holistic optimization is performed by a simplified Ant Colony Optimization method with a constrained Nelder-Mead algorithm (ACO-NM). Holistic optimization refers to a simultaneous tuning of the swing-up, stabilization and switching mode parameters. A new cost function is designed to minimize swing-up time, achieve high stabilization performance and consider system constraints. The holistic approach optimizes four controller structures, which include controllers that have never been tuned by a specific method besides by the trial-and-error method. Simulation results on a ICP nonlinear model show that ACO-NM in the holistic approach is effective compared to other algorithms. In addition, contrary to the majority of work on the subject, all the optimized controllers are validated experimentally. The simulation and experimental results obtained confirm that the holistic approach is an efficient optimization tool and specifically responds to the need of optimization technique for the potential-well controller structure and for the Q [diagonal of the matrix and the full matrix] in the linear-quadratic regulator (LQR) technique. Moreover, ICP experimental response analysis demonstrates that using the full Q provides greater experimental stabilization performance than using its diagonal terms in the LQR technique.																	1432-7643	1433-7479				MAR	2020	24	6					4343	4359		10.1007/s00500-019-04198-7													
J								Missing value imputation using unsupervised machine learning techniques	SOFT COMPUTING										K-means; Fuzzy C-means; Rough K-means; Machine learning; Missing values; Imputation	ALGORITHMS; SET	In data mining, preprocessing is one of the essential processes which involves data normalization, noise removal, handling missing values, etc. This paper focuses on handling missing values using unsupervised machine learning techniques. Soft computation approaches are combined with the clustering techniques to form a novel method to handle the missing values, which help us to overcome the problems of inconsistency. Rough K-means centroid-based imputation method is proposed and compared with K-means centroid-based imputation method, fuzzy C-means centroid-based imputation method, K-means parameter-based imputation method, fuzzy C-means parameter-based imputation method, and rough K-means parameter-based imputation methods. The experimental analysis is carried out on four benchmark datasets, viz. Dermatology, Pima, Wisconsin, and Yeast datasets, which have taken from UCI data repository. The proposed method proves the efficacy of different datasets, and the results are also promising one.																	1432-7643	1433-7479				MAR	2020	24	6					4361	4392		10.1007/s00500-019-04199-6													
J								Supervised machine learning techniques and genetic optimization for occupational diseases risk prediction	SOFT COMPUTING										Predictive medicine; Occupational diseases risk prediction; Support Vector Machine; Cluster analysis; Pattern recognition; Computational intelligence	PARTICLE SWARM OPTIMIZATION; DATA MINING TECHNIQUES; PATTERN-RECOGNITION; FEATURE-SELECTION; CLASSIFICATION; INJURIES	Workers healthcare gained a lot of attention recently as many countries are increasingly concerning about welfare. This paper faces the problem of predicting occupational disease risks by means of computational intelligence and pattern recognition techniques. Specifically, three different machine learning approaches are compared: the first one is based on the k-means algorithm, in charge to determine a set of meaningful labelled clusters as the final model. The latter two are based on fully supervised techniques, namely Support Vector Machines and K-Nearest Neighbours. Real data regarding both the worker and the workplace by mixing numerical and categorical attributes have been used for testing. The three approaches are automatically tuned by means of genetic algorithms in order to simultaneously find the optimal hyperparameters for the classification systems and the optimal ad-hoc dissimilarity measure weights in order to maximize the classification performances. Computational results show that the three approaches are rather comparable in terms of performances, but a clustering-based approach allows a deeper knowledge discovery phase, helpful for further risk assessment and forecasting.																	1432-7643	1433-7479				MAR	2020	24	6					4393	4406		10.1007/s00500-019-04200-2													
J								Feature selection by recursive binary gravitational search algorithm optimization for cancer classification	SOFT COMPUTING										Gene selection; Cancer classification; Gravitational search algorithm	PARTICLE SWARM OPTIMIZATION; GENE-EXPRESSION DATA; FEATURE SUBSET-SELECTION; MICROARRAY DATA; WRAPPER APPROACH; FILTER; DESIGN	DNA microarray technology has become a prospective tool for cancer classification. However, DNA microarray datasets typically have very large number of genes (usually more than tens of thousands) and less number of samples (often less than one hundred). This raises the issue of getting the most relevant genes prior to cancer classification. In this paper, we have proposed a two-phase feature selection method for cancer classification. This method selects a low-dimensional set of genes to classify biological samples of binary and multi-class cancers by integrating ReliefF with recursive binary gravitational search algorithm (RBGSA). The proposed RBGSA refines the gene space from a very coarse level to a fine-grained one at each recursive step of the algorithm without degrading the accuracy. We evaluate our method by comparing it with state-of-the-art methods on 11 benchmark microarray datasets of different cancer types. Comparison results show that our method selects only a small number of genes while yielding substantial improvements in accuracy over other methods. In particular, it achieved up to 100% classification accuracy for 7 out of 11 datasets with a very small size of gene subset (up to < 1.5%) for all 11 datasets.																	1432-7643	1433-7479				MAR	2020	24	6					4407	4425		10.1007/s00500-019-04203-z													
J								Learning path combination recommendation based on the learning networks	SOFT COMPUTING										Learning path combination recommendation; Learning network; MOOC; Scenarios	ANT COLONY SYSTEM; OPTIMIZATION; MODEL	Discovering useful hidden learning behavior pattern from learning data for online learning platform is valuable in education technology. Studies on learning path recommendation to recommend an appropriate resource for different users are particularly important for the development of advanced online education. However, it may suffer from low recommendation quality for beginners or learner with low participation. In order to improve the recommendation quality, a learning path combination recommendation method based on the learning network (LPCRLN) is proposed. In LPCRLN, it introduces complex network technology. Based on the characteristics of courses and learners, the course network and learner network, respectively, are constructed, and then learners are divided into three types. Finally, the recommendation is made in different scenarios according to the learner's learning records. In this study, a series of experiments have been carried out. By comparisons, experimental results indicate that our proposed method is able to make sound recommendations on appropriate courses for different types of learners with significant improvement in terms of accuracy and efficiency.																	1432-7643	1433-7479				MAR	2020	24	6					4427	4439		10.1007/s00500-019-04205-x													
J								Tuning of reinforcement learning parameters applied to SOP using the Scott-Knott method	SOFT COMPUTING										Reinforcement learning; Sequential Ordering Problem; Factorial design; Scott-Knott method; Tuning parameters	GENETIC ALGORITHM; LOCAL SEARCH; SOLVE; OPTIMIZATION; PERFORMANCE; SYSTEMS	In this paper, we present a technique to tune the reinforcement learning (RL) parameters applied to the sequential ordering problem (SOP) using the Scott-Knott method. The RL has been widely recognized as a powerful tool for combinatorial optimization problems, such as travelling salesman and multidimensional knapsack problems. It seems, however, that less attention has been paid to solve the SOP. Here, we have developed a RL structure to solve the SOP that can partially fill that gap. Two traditional RL algorithms, Q-learning and SARSA, have been employed. Three learning specifications have been adopted to analyze the performance of the RL: algorithm type, reinforcement learning function, and epsilon parameter. A complete factorial experiment and the Scott-Knott method are used to find the best combination of factor levels, when the source of variation is statistically different in analysis of variance. The performance of the proposed RL has been tested using benchmarks from the TSPLIB library. In general, the selected parameters indicate that SARSA overwhelms the performance of Q-learning.																	1432-7643	1433-7479				MAR	2020	24	6					4441	4453		10.1007/s00500-019-04206-w													
J								Design of fuzzy logic system framework using evolutionary techniques	SOFT COMPUTING										Fuzzy logic system; Time series forecasting; Genetic algorithm; Hybrid particle swarm optimization; Firefly algorithm; Artificial bee colony optimization; Whale optimization algorithm	CHAOTIC TIME-SERIES; DIFFERENTIAL EVOLUTION; OPTIMIZATION; ALGORITHM; EXPLORATION/EXPLOITATION; INTELLIGENCE; PREDICTION	Designing fuzzy logic system is one of the most popular and research-demanding NP-hard problems. It involves numerous parameters like shape and location of fuzzy sets, antecedents and consequents of fuzzy rule base and other strategic parameters like aggregation, implication and defuzzification methods. Time series forecasting has also become increasingly popular for the applications like share market prediction, weather forecasting. Many researchers have investigated the use of fuzzy logic system for forecasting of time series. In this paper, the authors have investigated the design framework of fuzzy logic systems for forecasting benchmark Mackey-Glass time series. Designing fuzzy logic systems is a class of NP-hard problems which is evolved using most popular and recent evolutionary algorithms. Authors have evolved fuzzy logic system using genetic algorithm, particle swarm optimization, artificial bee colony optimization, firefly algorithm and whale optimization algorithm. Finally, from simulations, it is found that whale optimization algorithm requires less time and shows fuzzy system predictions are more precise than others.																	1432-7643	1433-7479				MAR	2020	24	6					4455	4468		10.1007/s00500-019-04207-9													
J								A locally based feature descriptor for abnormalities detection	SOFT COMPUTING										Ulcer; Inflammatory; Polyp; Feature extraction; Contrast	WIRELESS CAPSULE ENDOSCOPY; ULCER DETECTION; TEXTURE; CLASSIFICATION; BOWEL	Wireless capsule endoscopy (WCE) is a novel imaging technique that can view the entire small bowel in human body. Therefore, it has been gradually adopted compared with traditional endoscopies for gastrointestinal diseases. However, the task of reviewing the vast amount of images produced by a WCE test is exhaustive for the physicians. This paper presents a new feature extraction scheme for pathological inflammation and ulcer regions discrimination in WCE images. In addition, the novel approach is adopted for polyp recognition in colonoscopy videos. A novel idea based on extracting certain local features from the image is proposed. Then, the occurrence histogram of these features is used as descriptor of the image. The new feature descriptor scheme is grayscale rotation invariant and computationally simple as the operator can be realized with a few operations in a small neighborhood. The proposed operator does not discard the contrast information. Besides, we propose to test the quality of the model using logarithmic loss metric and show how calibration can be useful in reducing the aforementioned measure. Extensive classification experiments have been applied on different datasets, which prove that the occurrence histogram of the extracted features is powerful. The proposed method achieved 99.1%, 99.7% and 99.2% in terms of the precision in the first, second and third datasets, respectively, and surpassed some known local descriptors on a texture dataset.																	1432-7643	1433-7479				MAR	2020	24	6					4469	4481		10.1007/s00500-019-04208-8													
J								A new preference disaggregation method for clustering problem: DISclustering	SOFT COMPUTING										Clustering; Particle swarm optimization (PSO); Simulated annealing (SA); Feed-forward neural network (FFNN); Multi-attribute preference disaggregation	KRILL HERD ALGORITHM; TEXT FEATURE-SELECTION; PARTICLE SWARM; DIMENSION REDUCTION; CLASSIFICATION; STRATEGY; IMPROVE	Clustering, a famous technique in data analysis and data mining, attempts to find valuable patterns in datasets. In this technique, a set of alternatives is partitioned into logical groups which are called clusters. The partitioning is based on some predefined attributes to find clusters in which their alternatives are similar to each other comparing to other clusters. In conventional methods, the similarity is usually defined by a distance-based measurement, whereas in this study, we have proposed a new multi-attribute preference disaggregation method called DISclustering in which a new measurement named global utility is introduced for cluster similarity. In DISclustering, the global utility of each alternative is calculated through a feed-forward neural network in which its parameters are determined using SA algorithm. Each alternative is assigned to a cluster based on comparing the obtained global utility with cluster boundaries, called utility thresholds; aim to minimize the intra-cluster distances (ICD). For this purpose, all utility thresholds are estimated using PSO algorithm. The performance of the proposed method is compared with 18 clustering algorithms on 14 real datasets based on F-measure and object function values (ICD values using intra-cluster or Gower distances). The experimental results and hypothesis statistical test indicate that DISclustering algorithm significantly improved clustering results on F-measure criteria in which outperforms in almost 13 compared algorithms out of 18. Note that, DISclustering calculates cluster centroid in a different way comparing to other algorithms. Hence, its ICD values are less eligible to perform a fair comparison.																	1432-7643	1433-7479				MAR	2020	24	6					4483	4503		10.1007/s00500-019-04210-0													
J								Inverse analysis and multi-objective optimization of single-point incremental forming of AA5083 aluminum alloy sheet	SOFT COMPUTING										Single-point incremental forming; Aluminum alloy sheet; Response surface methodology; Inverse modeling; Multi-objective optimization	PROCESS PARAMETERS; FORMABILITY; SELECTION; ALGORITHM; NETWORK; FUZZY	This paper presents soft computing-based modeling and multi-objective optimization of process parameters in single-point incremental forming (SPIF) of aluminum alloy sheet in order to obtain desired deformed shape with optimal formability satisfying multiple objectives. Response surface methodology and adaptive neuro-fuzzy inference system (ANFIS)-based models were developed to predict the responses based on the experimental data collected according to central composite design of experiments considering tool diameter, feed rate and step height as inputs, and outputs, namely forming wall angle, deformed sheet thickness and surface roughness. Inverse analyses were also performed to determine the set of input parameters to achieve desired outputs. Two different algorithms, namely back-propagation and hybrid, were employed to train the ANFIS in batch mode with the help of experimental data. The performances of the developed models were tested through real experimental data and also cross-validation methods. ANFIS trained by hybrid algorithm was found to be slightly better than that trained by the back-propagation algorithm in terms of prediction accuracy. Desirability function and a non-dominated sorting genetic algorithm were utilized for performing multi-objective optimization in SPIF, and the obtained optimal results were found satisfactory compared to the experimental data. The proposed approach could provide a reliable guidance for selection of suitable parameters in SPIF to achieve desired formed parts.																	1432-7643	1433-7479				MAR	2020	24	6					4505	4521		10.1007/s00500-019-04211-z													
J								The lambda-additive measure in a new light: the Q(nu) measure and its connections with belief, probability, plausibility, rough sets, multi-attribute utility functions and fuzzy operators	SOFT COMPUTING										Belief; Probability; Plausibility; lambda-Additive measure; Rough sets; Multi-attribute utility functions	GENERAL-CLASS	The aim of this paper is twofold. On the one hand, the lambda-additive measure (Sugeno lambda-measure) is revisited, and a stateof-the-art summary of its most important properties is provided. On the other hand, the so-called nu-additive measure as an alternatively parameterized lambda-additive measure is introduced. Here, the advantages of the nu-additive measure are discussed, and it is demonstrated that these two measures are closely related to various areas of science. The motivation for introducing the.-additive measure lies in the fact that its parameter.. (0, 1) has an important semantic meaning as it is the fix point of the complement operation. Here, by utilizing the nu-additive measure, some well-known results concerning the lambda-additive measure are put into a new light and rephrased in more advantageous forms. It is discussed here how the nu-additive measure is connected with the belief-, probability- and plausibility measures. Next, it is also shown that two.-additive measures, with the parameters.1 and.2, are a dual pair of belief- and plausibility measures if and only if nu(1) + nu(2) = 1. Furthermore, it is demonstrated how a nu-additive measure (or a lambda-additive measure) can be transformed to a probability measure and vice versa. Lastly, it is discussed here how the nu-additive measures are connected with rough sets, multi-attribute utility functions and certain operators of fuzzy logic.																	1432-7643	1433-7479				MAR	2020	24	6					4523	4543		10.1007/s00500-019-04212-y													
J								Color quantization with Particle swarm optimization and artificial ants	SOFT COMPUTING										Color quantization; Artificial ants; Ant-tree algorithm; Particle swarm optimization algorithm; Clustering	MEANS CLUSTERING-ALGORITHM; IMAGE QUANTIZATION; REDUCTION	This article describes a color quantization algorithm that combines two swarm-based methods: Particle swarm optimization and artificial ants. The proposed method is based on a previous method that solves the quantization problem by combining the Particle swarm optimization algorithm with the K-means algorithm. K-means is a popular clustering method that has been applied to solve a variety of problems, including the color quantization problem. Nevertheless, it is a time-consuming method, which makes combining the Particle swarm optimization algorithm and K-means less suitable than other color quantization techniques. The proposed method, however, discards the K-means algorithm and applies the Ant-tree for color quantization algorithm in order to reduce execution time. This article shows that the new method outperforms the original one, since it requires less time to obtain higher quality images. In addition, the images produced are also of better quality than those produced by other well-known color quantization methods, such as Neuquant, Octree, Median-cut, Variance-based, Binary splitting and Wu's methods.																	1432-7643	1433-7479				MAR	2020	24	6					4545	4573		10.1007/s00500-019-04216-8													
J								Efficient feature selection using one-pass generalized classifier neural network and binary bat algorithm with a novel fitness function	SOFT COMPUTING										Feature selection; Wrapper approach; Bio-inspired algorithms; One-pass neural network	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; STATISTICAL-ANALYSIS; MODEL; RECOGNITION; SYSTEM; COLONY	In high-dimensional data, many of the features are either irrelevant to the machine learning task or are redundant. These situations lead to two problems, firstly overfitting and secondly high computational overhead. The paper proposes a feature selection method to identify the relevant subset of features for the machine-learning task using wrapper approach. The wrapper approach uses the Binary Bat algorithm to select the set of features and One-pass Generalized Classifier Neural Network (OGCNN) to evaluate the selected set of features using a novel fitness function. The proposed fitness function accounts for the entropy of sensitivity and specificity along with accuracy of classifier and fraction of selected features. The fitness function is compared using four classifiers (Radial Basis Function Neural Network, Probabilistic Neural Network, Extreme Learning Machine and OGCNN) on six publicly available datasets. One-pass classifiers are chosen as these are computationally faster. The results suggest that OGCNN along with the novel fitness function performs well in the majority of cases.																	1432-7643	1433-7479				MAR	2020	24	6					4575	4587		10.1007/s00500-019-04218-6													
J								Adaptively secure efficient broadcast encryption with constant-size secret key and ciphertext	SOFT COMPUTING										Broadcast encryption; Public key broadcast encryption; Data access control; Cloud storage service; Adaptive security; Standard model	ATTRIBUTE-BASED ENCRYPTION; DUAL SYSTEM ENCRYPTION; ENCAPSULATION; ALGORITHM; HIBE	In those broadcast application scenarios with a great quantity of receivers, e.g., the data access control system in cloud storage service, the single sender is apt to become the efficiency bottleneck of the system, because the computation and storage overhead of the sender will grow rapidly with the amount of qualified receivers. In order to overcome this problem, we first introduce the novel conception of complete binary identity tree which is adopted to manage the qualified receivers. Then we design the prune-merge algorithm to further optimize the structure of the tree and cut down the amount of receivers. The algorithm effectively reduces the computation and storage cost of the trusted authority in the system. Subsequently, in virtue of composite-order bilinear groups, we bring forward an efficient public key broadcast encryption scheme combined its application to the system of data access control in cloud storage service. Compared with the existing schemes, the lengths of system public parameters, secret key and ciphertext in our scheme are all constant. In addition, the number of secret keys in our scheme increases logarithmically with the maximum amount of receivers, while the numbers of secret keys in the existing schemes increase linearly with the maximum amount of receivers. Furthermore, the proposed scheme is proved to guarantee adaptive security under general subgroup decision assumption in the standard model. The performance analysis manifests that our scheme is feasible for those broadcast applications with fixed senders.																	1432-7643	1433-7479				MAR	2020	24	6					4589	4606		10.1007/s00500-019-04219-5													
J								A novel spherical fuzzy analytic hierarchy process and its renewable energy application	SOFT COMPUTING										Spherical fuzzy sets; Multi-criteria decision making; AHP; Neutrosophic AHP	AHP; SELECTION; EXTENSION; SYSTEMS	The extensions of ordinary fuzzy sets such as intuitionistic fuzzy sets, Pythagorean fuzzy sets, and neutrosophic sets, whose membership functions are based on three dimensions, aim at collecting experts' judgments more informatively and explicitly. In the literature, generalized three-dimensional spherical fuzzy sets have been introduced by Kutlu Gundogdu and Kahraman (J Intell Fuzzy Syst 36(1):337-352, 2019a), including their arithmetic operations, aggregation operators, and defuzzification operations. In this paper, our aim is to extend classical analytic hierarchy process (AHP) to spherical fuzzy AHP (SF-AHP) method and to show its applicability and validity through a renewable energy location selection example and a comparative analysis between neutrosophic AHP and SF-AHP.																	1432-7643	1433-7479				MAR	2020	24	6					4607	4621		10.1007/s00500-019-04222-w													
J								An adaptive sliding mode controller based on online support vector regression for nonlinear systems	SOFT COMPUTING										Sliding mode control; Stability analysis; Support vector regression; SVR-based parameter estimator; SVR-based SMC	VARIABLE-STRUCTURE SYSTEMS; DESIGN; FEEDBACK	In this paper, a novel adaptive sliding mode controller (SMC) based on support vector regression (SVR) is introduced for nonlinear systems. The closed-loop margin notion introduced for self-tuning regulators is rearranged in order to optimize the parameters of SMC. The proposed adjustment mechanism consists of an online SVR to identify the forward dynamics of the controlled system and SMC parameter estimators realized by separate online SVRs to approximate each tunable controller parameter. The performance of the proposed control architecture has been evaluated by simulations performed on a nonlinear continuously stirred tank reactor system, and the obtained results indicate that the SMC based on SVR provides robust and stable closed-loop performance.																	1432-7643	1433-7479				MAR	2020	24	6					4623	4643		10.1007/s00500-019-04223-9													
J								A novel hybrid GA-PSO framework for mining quantitative association rules	SOFT COMPUTING										Quantitative association rule mining; Multi-objective optimization; Hybridization; Genetic algorithm; Particle swarm optimization	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; MODEL	Discovering association rules is a useful and common technique for data mining in which dependencies among datasets are shown. Discovering the rules from continuous numeric datasets is one of the common challenges in data mining. Furthermore, another restriction imposed by algorithms in this area is the need to determine the minimum threshold for the criteria of support and confidence. By drawing on two heuristic optimization techniques, to wit, the genetic algorithm (GA) and particle swarm optimization (PSO) algorithm, a hybrid algorithm for extracting quantitative association rules was developed in this research. Accurate and interpretable rules result from the integration of the multiple objectives GA with the multiple objective PSO algorithms, which redresses the balance in the exploitation and exploration tasks. The useful and appropriate rules and the most suitable numerical intervals are discovered by proposing a multi-criteria method in which there is no need to discretize numerical values and to determine threshold values of minimum support and confidence. Different criteria are used to determine appropriate rules. In this algorithm, the selected rules are extracted based on confidence, interestingness and comprehensibility. The results gained over five real-world datasets evidence the effectiveness of the proposed method. By hybridization of the GA and the PSO algorithm, the proposed approach has achieved considerable improvements compared with the basic algorithms in the criteria of the number of extracted rules from dataset, high confidence measure and support percentage.																	1432-7643	1433-7479				MAR	2020	24	6					4645	4666		10.1007/s00500-019-04226-6													
J								Parameterized utility functions on interval-valued intuitionistic fuzzy numbers with two kinds of entropy and their application in multi-criteria decision making	SOFT COMPUTING										Interval-valued intuitionistic fuzzy set; Utility function; Fuzzy multi-criteria decision making; Score function; Accuracy function	ACCURACY FUNCTION	In this study, the entropy of interval-valued intuitionistic fuzzy numbers (IVIFNs) is analyzed, and two kinds of entropy factors are proposed. By using the normalized score function, normalized Type-1 entropy factor, and normalized Type-2 entropy factor, a series of utility functions on IVIFNs are proposed. In particular, one of the proposed utility functions is structured based on integral. By using the proposed utility functions, IVIFNs can be compared and ranked. The characteristic of these proposed utility functions is that they are objective on comparing IVIFNs from the point of probability. Thereafter, two kinds of fuzzy multi-criteria decision-making methods in interval-valued intuitionistic fuzzy setting are introduced by using the proposed entropy functions. Finally, an example is given to demonstrate the effectiveness of the proposed utility functions and the fuzzy multi-criteria decision-making methods.																	1432-7643	1433-7479				MAR	2020	24	6					4667	4674		10.1007/s00500-019-04227-5													
J								A fuzzy similarity-based rough set approach for attribute selection in set-valued information systems	SOFT COMPUTING										Set-valued data; Rough set; Fuzzy tolerance relation; Degree of dependency; Attribute selection	DECISION SYSTEMS; REDUCTION; APPROXIMATIONS; ALGORITHMS	Databases obtained from different search engines, market data, patients' symptoms and behaviours, etc., are some common examples of set-valued data, in which a set of values are correlated with a single entity. In real-world data deluge, various irrelevant attributes lower the ability of experts both in speed and in predictive accuracy due to high dimension and insignificant information, respectively. Attribute selection is the concept of selecting those attributes that ideally are necessary as well as sufficient to better describe the target knowledge. Rough set-based approaches can handle uncertainty available in the real-valued information systems after the discretization process. In this paper, we introduce a novel approach for attribute selection in set-valued information system based on tolerance rough set theory. The fuzzy tolerance relation between two objects using a similarity threshold is defined. We find reducts based on the degree of dependency method for selecting best subsets of attributes in order to obtain higher knowledge from the information system. Analogous results of rough set theory are established in case of the proposed method for validation. Moreover, we present a greedy algorithm along with some illustrative examples to clearly demonstrate our approach without checking for each pair of attributes in set-valued decision systems. Examples for calculating reduct of an incomplete information system are also given by using the proposed approach. Comparisons are performed between the proposed approach and fuzzy rough-assisted attribute selection on a real benchmark dataset as well as with three existing approaches for attribute selection on six real benchmark datasets to show the supremacy of proposed work.																	1432-7643	1433-7479				MAR	2020	24	6					4675	4691		10.1007/s00500-019-04228-4													
J								A dynamic expert contribution-based consensus model for hesitant fuzzy group decision making with an application to water resources allocation selection	SOFT COMPUTING										Hesitant fuzzy preference relation; Consensus; Weight changing; Water resources allocation	PREFERENCE RELATIONS; SUPPORT MODEL; ADDITIVE CONSISTENCY; SELF-CONFIDENCE; SOCIAL NETWORKS; AGGREGATION; INFORMATION	This research focuses on group decision making (GDM) under hesitant fuzzy condition. All experts (decision makers) are allowed to use hesitant fuzzy preference relations (HFPRs) to express their opinions. Subsequently, a dynamically expert contribution-based consensus model is developed for GDM with HFPRs. In the proposed method, a combination of a weight update model and a preference adjustment model is applied to consensus reaching processes (CRPs). In the weight update model, we propose to dynamically update experts' weights according to their contributions in the CRPs. In the preference adjustment model, only the preferences which are far away from the expected values are modified, aiming to retain the experts' original information as much as possible. Finally, the proposed model is applied to water resources allocation selection to show how it works in practice. And some comparisons and discussions are given to show the advantages of the proposed method.																	1432-7643	1433-7479				MAR	2020	24	6					4693	4708		10.1007/s00500-019-04229-3													
J								Discernibility matrix based incremental feature selection on fused decision tables	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Incremental learning; Feature selection; Fuzzy rough sets; Discernibility matrix; Fused decision table	ATTRIBUTE REDUCTION APPROACH; ROUGH SET APPROACH; DYNAMIC DATA; KNOWLEDGE GRANULARITY; HYBRID DATA; APPROXIMATIONS; ALGORITHM; UNCERTAINTY; SYSTEMS; MAINTENANCE	In rough set philosophy, each set of data can be seen as a fuzzy decision table. Since a decision table dynamically increases with time and space, these decision tables are integrated into a new one called fused decision table. In this paper, we focus on designing an incremental feature selection method on fused decision table by optimizing the space constraint of storing discernibility matrix. Here discernibility matrix is a known way of discernibility information measure in rough set theory. This paper applies the quasi/pseudo value of discernibility matrix rather than the true value of discernibility matrix to design an incremental mechanism. Unlike those discernibility matrix based non-incremental algorithms, the improved algorithm needs not save the whole discernibility matrix in main memory, which is desirable for the large data sets. More importantly, with the increment of decision tables, the discernibility matrix-based feature selection algorithm could constrain the computational cost by applying efficient information updating techniques-quasi/pseudo approximation operators. Finally, our experiments reveal that the proposed algorithm needs less computational cost, especially less occupied space, on the condition that the accuracy is limitedly lost. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						1	26		10.1016/j.ijar.2019.11.010													
J								Constructing copulas from shock models with imprecise distributions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Marshall's copula; Maxmin copula; p-box; Imprecise probability; Shock model	DECISION-MAKING; DEPENDENCE STRUCTURE; ORDER-STATISTICS; SKLARS THEOREM; UNCERTAINTY; PROBABILITIES	The omnipotence of copulas when modeling dependence given marginal distributions in a multivariate stochastic situation is assured by the Sklar's theorem. Monies et al. (2015) suggest the notion of what they call an imprecise copula that brings some of its power in bivariate case to the imprecise setting. When there is imprecision about the marginals, one can model the available information by means of p-boxes, that are pairs of ordered distribution functions. By analogy they introduce pairs of bivariate functions satisfying certain conditions. In this paper we introduce the imprecise versions of some classes of copulas emerging from shock models that are important in applications. The so obtained pairs of functions are not only imprecise copulas but satisfy an even stronger condition. The fact that this condition really is stronger is shown in Omladic and Stopar (2019) thus raising the importance of our results. The main technical difficulty in developing our imprecise copulas lies in introducing an appropriate stochastic order on these bivariate objects. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						27	46		10.1016/j.ijar.2019.11.009													
J								Variance based three-way clustering approaches for handling overlapping clustering	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way clustering; Clustering; Three-way decisions; Overlapping clusters	COMPLEX NETWORKS; FUZZY	The conventional clustering approaches are not very effective in dealing with clusters having overlapping regions. The three-way clustering (3WC) is an effective and promising approach in this regards. A key issue in 3WC is the determination of thresholds which plays a crucial and important role in accurate estimation of the overlapping region. In this article, we propose different variance based criteria for determining the thresholds. In particular, we examine the variance or spread in evaluation function values of objects contained in the three regions obtained with 3WC of objects. An algorithm called 3WC-OR is introduced that considers the optimization of the proposed criteria for determining effective thresholds by incorporating approaches such as genetic algorithms and game-theoretic rough sets. Experimental results on five UCI datasets indicate that the proposed algorithm significantly improves results on datasets with overlapping clusters and provide comparable results on datasets with non-overlapping clusters. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						47	63		10.1016/j.ijar.2019.11.011													
J								Finding strongly connected components of simple digraphs based on granulation strategy	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Strongly connected components; Rough set theory; Graph theory; Granulation strategy	ROUGH SET; DEPTH-1ST SEARCH; GRAPH; ALGORITHMS; CLASSIFICATION; APPROXIMATIONS	Strongly connected components (SCCs) are an important kind of subgraphs in digraphs. It can be viewed as a kind of knowledge in the viewpoint of knowledge discovery. In our previous work, a knowledge discovery algorithm called RSCC was proposed for finding SCCs of simple digraphs based on two operators, k-step R-related set and k-step upper approximation, of rough set theory (RST). RSCC algorithm can find SCCs more efficiently than Tarjan algorithm of linear complexity. However, on the one hand, as the theoretical basis of RST applied to SCCs discovery of digraphs, the theoretical relationships between RST and graph theory investigated in previous work only include four equivalences between fundamental RST and graph concepts relating to SCCs. The reasonability of using the two RST operators to find SCCs still need to be investigated. On the other hand, it is found that there are three SCCs correlations between vertices after we use three RST concepts, R-related set, lower and upper approximation sets, to analyze SCCs. RSCC algorithm ignores these SCCs correlations so that the efficiency of RSCC is affected negatively. For the above two issues, firstly, we explore the equivalence between the two RST operators and Breadth-First Search (BFS) which is one of the most basic graph search algorithms and the most direct way to find SCCs. These equivalences explain the reasonability of using the two RST operators to find SCCs, and enrich the content of the theoretical relationships between RST and graph theory. Secondly, we design a granulation strategy according to these three SCCs correlations. Then an algorithm called GRSCC for finding SCCs of simple digraphs based on granulation strategy is proposed. Experimental results show that GRSCC provides better performance to RSCC. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						64	78		10.1016/j.ijar.2019.12.001													
J								New transformations of aggregation functions based on monotone systems of functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Aggregation function; Convex sum; Copula; *-product; GCS-transform; Weighted arithmetic mean	COPULAS	The paper introduces a Generalized-Convex-Sum-Transformation of aggregation functions. It has the form of a transformation of aggregation functions by monotone systems of functions. A special case of the proposed Generalized-Convex-Sum-Transformation is the well-known *-product, also called the Darsow product of copulas. Similarly, our approach covers Choquet integrals with respect to capacities induced by the considered aggregation function. The paper offers basic definitions and some properties of the mentioned transformation. Various examples illustrating the transformation are presented. The paper also gives two alternative transformations of aggregation functions under which the dimension of the transformed aggregation functions is higher than that of the original one. Interestingly, if a copula is transformed, under some conditions put on the monotone systems of functions, the transformed aggregation function is again a copula. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						79	95		10.1016/j.ijar.2019.12.004													
J								On the construction of uninorms by paving	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Associative operations; Uninorms; T-norms; T-conorms; Paving		Inspired by the paving construction method, we construct a new operation on the unit interval from a given operation defined on the unit interval and a discrete operation defined on an index set. In particular, we construct a new t-norm from a t-norm on the unit interval and a discrete t-norm; a t-conorm from a t-norm and a discrete t-superconorm; a proper uninorm from a t-norm and a discrete t-conorm and a uninorm from a t-norm and a discrete uninorm. Dually, we also construct some uninorms (including t-norms and t-conorms) from a t-conorm and a discrete t-conorm, t-subnorm, t-norm or uninorm. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						96	111		10.1016/j.ijar.2019.12.007													
J								Effectiveness assessment of Cyber-Physical Systems	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Cyber Physical Systems; Degree of effectiveness; Transferable Belief Model; Input/Output Hidden Markov Model; Zone of viability	TRANSFERABLE BELIEF MODEL; HIDDEN MARKOV-MODELS; COMBINATION; TAXONOMY; HMMS	By achieving their purposes through interactions with the physical world, Cyber-Physical Systems (CPS) pose new challenges in terms of dependability. Indeed, the evolution of the physical systems they control with transducers can be affected by surrounding physical processes over which they have no control and which may potentially hamper the achievement of their purposes. While it is illusory to hope for a comprehensive model of the physical environment at design time to anticipate and remove faults that may occur once these systems are deployed, it becomes necessary to evaluate their degree of effectiveness in vivo. In this paper, the degree of effectiveness is formally defined and generalized in the context of the measure theory. The measure is developed in the context of the Transferable Belief Model (TBM), an elaboration on the Dempster-Shafer Theory (DST) of evidence so as to handle epistemic and aleatory uncertainties respectively pertaining the users' expectations and the natural variability of the physical environment. The TBM is used in conjunction with the Input/Output Hidden Markov Modeling framework we denote by Ev-IOHMM to specify the expected evolution of the physical system controlled by the CPS and the tolerances towards uncertainties. The measure of effectiveness is then obtained from the forward algorithm, leveraging the conflict entailed by the successive combinations of the beliefs obtained from observations of the physical system and the beliefs corresponding to its expected evolution. The proposed approach is applied to autonomous vehicles and shows how the degree of effectiveness can be used for bench-marking their controller relative to the highway code speed limitations and passengers' well-being constraints, both modeled through an Ev-IOHMM. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						112	132		10.1016/j.ijar.2019.12.002													
J								Complexity results for probabilistic answer set programming	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Probabilistic logic programming; Answer set programming; Computational complexity	LOGIC PROGRAMS; INFERENCE; SEMANTICS	We analyze the computational complexity of probabilistic logic programming with constraints, disjunctive heads, and aggregates such as sum and max. We consider propositional programs and relational programs with bounded-arity predicates, and look at cautious reasoning (i.e., computing the smallest probability of an atom over all probability models), cautious explanation (i.e., finding an interpretation that maximizes the lower probability of evidence) and cautious maximum-a-posteriori (i.e., finding a partial interpretation for a set of atoms that maximizes their lower probability conditional on evidence) under Lukasiewicz's credal semantics. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						133	154		10.1016/j.ijar.2019.12.003													
J								Indices for rough set approximation and the application to confusion matrices	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Rough set approximation; Confusion matrix; Precision indices; Odds ratios; Error estimation	CLASSIFICATION	Confusion matrices and their associated statistics are a well established tool in machine learning to evaluate the accuracy of a classifier. In the present study, we define a rough confusion matrix based on a very general classifier, and derive various statistics from it which are related to common rough set estimators. In other words, we perform a rough set-like analysis on a confusion matrix, which is the converse of the usual procedure; in particular, we consider upper approximations. A suitable index for measuring the tightness of the upper bound uses a ratio of odds. Odds ratios offer a symmetric interpretation of lower and upper precision, and remove the bias in the upper approximation. We investigate rough odds ratios of the parameters obtained from the confusion matrix; to guard against undue random influences, we also approximate their standard errors. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						155	172		10.1016/j.ijar.2019.12.008													
J								Incremental concept-cognitive learning based on attribute topology	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Concept-cognitive learning; Concept tree; Formal concept analysis; Incremental algorithm; Attribute topology	ROUGH SET; DECISION	Incremental learning is an alternative approach for maintaining knowledge by utilizing previous computational results of dynamic data contexts. As a new and important part of incremental learning, incremental Concept-cognitive learning (CCL) is an emerging field of concerning evolution of object or attributes sets and dynamic knowledge processing in the dynamic big data. However, existing incremental CCL algorithms still face some challenges that improve the generalization ability of new concepts, and the previously acquired knowledge should be efficiently utilized to reduce the computational complexity of the algorithm. At the same time, formal concept analysis has become a potential direction of cognitive computing, which can describe the processes of CCL. Attribute topology (AT) as a new representation of formal concepts can clearly display the relationship between new data and original data for reducing the complexity of the CCL process; therefore, we present an incremental concept-cognitive algorithm based on AT for incremental concept calculation, which is expressed by a concept tree. First, a relationship between the new object and some of the original objects is established. Then, on the basis of this finding, we propose an algorithm for updating the concept and presenting them through a concept tree. The algorithm determines the position and subtree of the new object by the relationship between the object and the original objects. Finally, an example is presented to demonstrate that the concept update algorithm based on AT is feasible and effective, and different orders of increments will result in different concept tree structures. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						173	189		10.1016/j.ijar.2019.12.010													
J								The linear algebra of pairwise comparisons	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Pairwise comparisons; Multiplicative preference relations; Additive preference relations; Consistency; Analytic hierarchy process; Vector spaces	GENERAL UNIFIED FRAMEWORK; COMPARISON MATRICES; CONSISTENT APPROXIMATION; WEIGHTS	In this paper, we start from the premise that pairwise comparisons between alternatives can be modeled by means of the additive representation of preferences. In this setting we study some algebraic properties of three sets: the set of pairwise comparison matrices, its subset of consistent ones and the orthogonal complement of the latter. The three sets are all vector spaces and we propose and interpret simple bases for each one. We prove that a convenient inner product can be found in the three cases such that the corresponding basis is orthonormal with respect to the considered inner product. In addition (i) we prove that the well-known method of the logarithmic least squares used to estimate the weight vector can be reinterpreted by referring to a basis for the set of consistent preferences and (ii) we interpret a transformation recently proposed by Csato. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAR	2020	118						190	207		10.1016/j.ijar.2019.12.009													
J								Clustering data with the presence of attribute noise: a study of noise completely at random and ensemble of multiple k-means clusterings	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Data clustering; Attribute noise; NCAR; Cluster ensemble; Robustness	CONSENSUS; CLASSIFICATION; ALGORITHM; MODELS; SCHEME	In general practice, the perception of noise has been inevitably negative. Specific to data analytic, most of the existing techniques developed thus far comply with a noise-free assumption. Without an assistance of data pre-processing, it is hard for those models to discover reliable patterns. This is also true for k-means, one of the most well known algorithms for cluster analysis. Based on several works in the literature, they suggest that the ensemble approach can deliver accurate results from multiple clusterings of data with noise completely at random. Provided this motivation, the paper presents the study of using different consensus clustering techniques to analyze noisy data, with k-means being exploited as base clusterings. The empirical investigation reveals that the ensemble approach can be robust to low level of noise, while some exhibit improvement over the noise-free cases. This finding is in line with the recent published work that underlines the benefit of small noise to centroid-based clustering methods. In addition, the outcome of this research provides a guideline to analyzing a new data collection of uncertain quality level.																	1868-8071	1868-808X				MAR	2020	11	3					491	509		10.1007/s13042-019-00989-4													
J								An active multi-class classification using privileged information and belief function	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Classification; Support vector machine; Privileged information; Active learning	SUPPORT VECTOR MACHINE; SVM; CLASSIFIERS; PLUS	Many classification models, based on support vector machine, have been designed so far to improve classification performance in both supervised and semi-supervised learning. One of the studies which is done in this case is about the use of privileged information that is hidden in training data. However, the challenge is how to find the privileged information. In most researches, experts have defined privileged information, but in this paper, it has been tried to automatically select a feature as privileged information and classify training data into several groups. This grouping has been used to correct the decision function of classifier. Moreover, the proposed classifier has been used in one-against-all (OAA) approach for semi-supervised datasets. To overcome uncertain areas in OAA, belief function and active learning techniques are applied to extract the most informative samples. The experimental results indicate the superiority of the proposed method among the other state-of-the-art methods in terms of classification accuracy.																	1868-8071	1868-808X				MAR	2020	11	3					511	524		10.1007/s13042-019-00991-w													
J								Multi-view local linear KNN classification: theoretical and experimental studies on image classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-view scenarios; Prediction model; Clustering property; FISTA; KNN	FACE RECOGNITION; DISCRIMINATION; DICTIONARY; REGRESSION; ALGORITHM; SPARSITY	When handling special multi-view scenarios where data from each view keep the same features, we may perhaps encounter two serious challenges: (1) samples from different views of the same class are less similar than those from the same view but different class, which sometimes happen in local way in both training and/or testing phases; (2) training an explicit prediction model becomes unreliable and even infeasible for test samples in multi-view scenarios. In this study, we prefer the philosophy of the k nearest neighbor method (KNN) to circumvent the second challenge. Without an explicit prediction model trained directly from the above multi-view data, a new multi-view local linear k nearest neighbor method (MV-LLKNN) is then developed to circumvent the two challenges so as to predict the label of each test sample. MV-LLKNN has its two reliable assumptions. One is the theoretically and experimentally provable assumption that any test sample can be well approximated by a linear combination of its neighbors in the multi-view training dataset. The other assumes that these neighbors should demonstrate their clustering property according to certain commonality-based similarity measure between the multi-view test sample and these multi-view neighbors so as to avoid the first challenge. MV-LLKNN can realize its effective prediction for a test multi-view sample by cheaply using both on-hand fast iterative shrinkage thresholding algorithm (FISTA) and KNN. Our theoretical analysis and experimental results about real multi-view face datasets indicate the effectiveness of MV-LLKNN.																	1868-8071	1868-808X				MAR	2020	11	3					525	543		10.1007/s13042-019-00992-9													
J								Improved fuzzy C-means algorithm based on density peak	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Fuzzy C-means algorithm; Density peak; Clustering	CLUSTERING-ALGORITHM; FAST SEARCH; COMPLEXITY; FIND	Fuzzy C-means (FCM) algorithm is a fuzzy clustering algorithm based on objective function compared with typical "hard clustering" such as k-means algorithm. FCM algorithm calculates the membership degree of each sample to all classes and obtain more reliable and accurate classification results. However, in the process of clustering, FCM algorithm needs to determine the number of clusters manually, and is sensitive to the initial clustering center. It is easy to generate problems such as multiple clustering iterations, slow convergence speed and local optimal solution. To address those problems, we propose to combine the FCM algorithm and DPC (Clustering by fast search and find of density peaks) algorithm. First, DPC algorithm is used to automatically select the center and number of clusters, and then FCM algorithm is used to realize clustering. The comparison experiments show that the improved FCM algorithm has a faster convergence speed and higher accuracy.																	1868-8071	1868-808X				MAR	2020	11	3					545	552		10.1007/s13042-019-00993-8													
J								A survey on analysis of human faces and facial expressions datasets	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Emotion; Face; Facial expressions; Image datasets; Video datasets	RECOGNITION; EMOTIONS	Facial expressions are the basic input for visual emotion detection. They are of great importance in computer vision society. In last decade, substantial amount of work has been done in the field of facial expressions datasets. This survey covers all of the publically available databases in detail and provides necessary information about these sets. This review delivers comprehensive support to researchers in selection of their desired dataset. The datasets are organized in decreasing order of their importance with respect to diversity in expressions, poses, number of images and resolution. This survey also provides comprehensive tabular comparison of different face based databases.																	1868-8071	1868-808X				MAR	2020	11	3					553	571		10.1007/s13042-019-00995-6													
J								Feature selection based on rough set approach, wrapper approach, and binary whale optimization algorithm	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Feature selection; Classification; Whale optimization algorithm; Rough set theory; Wrapper approach; Logistic regression	ATTRIBUTE REDUCTION; DIFFERENTIAL EVOLUTION; SEARCH; CLASSIFICATION; CANCER; SVM	The principle of any approach for solving feature selection problem is to find a subset of the original features. Since finding a minimal subset of the features is an NP-hard problem, it is necessary to develop and propose practical and efficient heuristic algorithms. The whale optimization algorithm is a recently developed nature-inspired meta-heuristic optimization algorithm that imitates the hunting behavior of humpback whales to solve continuous optimization problems. In this paper, we propose a novel binary whale optimization algorithm (BWOA) to solve feature selection problem. BWOA is especially desirable and appealing for feature selection problem whenever there is no heuristic information that can lead the search to the optimal minimal subset. Nonetheless, whales can find the best features as they hunt the prey. Rough set theory (RST) is one of the effective algorithms for feature selection. We use RST with BWOA as the first experiment, and in the second experiment, we use a wrapper approach with BWOA on three different classifiers for feature selection. Also, we verify the performance and the effectiveness of the proposed algorithm by performing our experiments using 32 datasets from the UCI machine learning repository and comparing the proposed algorithm with some powerful existing algorithms in the literature. Furthermore, we employ two nonparametric statistical tests, Wilcoxon Signed-Rank test, and Friedman test, at 5% significance level. Our results show that the proposed algorithm can provide an efficient tool to find a minimal subset of the features.																	1868-8071	1868-808X				MAR	2020	11	3					573	602		10.1007/s13042-019-00996-5													
J								Flexible robust principal component analysis	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Error correction; Robust principal component analysis (RPCA); Subspace learning	DISPERSION MATRICES; SYSTEMS	The error correction problem is a very important topic in machine learning. However, existing methods only focus on data recovery and ignore data compact representation. In this paper, we propose a flexible robust principal component analysis (FRPCA) method in which two different matrices are used to perform error correction and the data compact representation can be obtained by using one of matrices. Moreover, FRPCA selects the most relevant features to guarantee that the recovered data can faithfully preserve the original data semantics. The learning is done by solving a nuclear-norm regularized minimization problem, which is convex and can be solved in polynomial time. Experiments were conducted on image sequences containing targets of interest in a variety of environments, e.g., offices, campuses. We also compare our method with existing method in recovering the face images from corruptions. Experimental results show that the proposed method achieves better performances and it is more practical than the existing approaches.																	1868-8071	1868-808X				MAR	2020	11	3					603	613		10.1007/s13042-019-00999-2													
J								Unsupervised learning of monocular depth and ego-motion with space-temporal-centroid loss	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Deep learning; Depth estimation; Visual odometry (VO); 3D feature transformation loss; Centroid transformation loss		We propose DPCNN (Depth and Pose Convolutional Network), a novel framework for monocular depth with absolute scale and camera motion estimation from videos. DPCNN uses our proposed stereo training examples, in which the spatial and temporal images can be combined more closely, thus providing more priori constraint relationships. In addition, there are two significant features existing in DPCNN: One is that the entire space-temporal-centroid model is established to independently constrain the rotation matrix and the translation vector, so that the spatial and temporal images are collectively limited in a common, real-world scale. The other is to use the triangulation principle to establish a two-channel depth consistency loss, which penalizes inconsistency of the depths estimated from the spatial images and inconsecutive temporal images, respectively. Experiments on the KITTI datasets show that DPCNN achieves the most advanced results in both tasks and outperforms the current monocular methods.																	1868-8071	1868-808X				MAR	2020	11	3					615	627		10.1007/s13042-019-01020-6													
J								Granular matrix-based knowledge reductions of formal fuzzy contexts	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Formal fuzzy contexts; Fuzzy relation matrix; Granular computing; Join-irreducible elements; Knowledge reduction	CONCEPT LATTICES; ATTRIBUTE REDUCTION; RULE ACQUISITION; GRAPH; ALGORITHMS	Knowledge reduction is an important issue in formal fuzzy contexts, which can simplify the structure of concept lattices. In this paper, a novel granular matrix-based for knowledge reduction of crisp-fuzzy concept is investigated. Firstly, matrix representations of extents and intents of concepts are defined, respectively, which are used to characterize the join-irreducible elements and propose the corresponding algorithm. In this framework, granular consistent set and granular reduct are developed. Then the judgement theorem of reduction and its corresponding algorithm in formal fuzzy context are proposed. Furthermore, we generalize the matrix approach to formal fuzzy decision contexts. Finally, numerical experiments are conducted to evaluate the effectiveness of the proposed approaches. Our methods present a new framework for knowledge reduction in formal fuzzy contexts.																	1868-8071	1868-808X				MAR	2020	11	3					643	656		10.1007/s13042-019-01022-4													
J								An efficient clustering algorithm based on the k-nearest neighbors with an indexing ratio	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Clustering; Data mining; Unsupervised learning; Cluster analysis; Nearest point; Indexing ratio; Nearest neighbor search technique; Nearest point with indexing ratio algorithm; NPIR	COLOR IMAGE SEGMENTATION; GENETIC ALGORITHM; FEATURE-SELECTION; PARAMETERS	Clustering is a challenging problem that is commonly used for many applications. It aims at finding the similarity between data points and grouping similar ones into the same cluster. In this paper, we introduce a new clustering algorithm named Nearest Point with Indexing Ratio (NPIR). The algorithm tries to solve the clustering problem based on the nearest neighbor search technique by finding the nearest neighbors for the points that are already clustered based on the distance between them and cluster them accordingly. The algorithm does not consider all the nearest points at once to cluster a single point but iteratively considers only one nearest point based on an election operation using a distance vector. NPIR tries to solve some limitations of other clustering algorithms. It tries to cluster arbitrary shapes which have non-spherical clusters, clusters with unusual shapes, or clusters with different densities. NPIR is evaluated using 20 real and artificial data sets of different levels of complexity with different number of clusters and points. Results are compared with those obtained for other well-known and common clustering algorithms. The comparative study demonstrates that NPIR outperforms the other algorithms for the majority of the data sets in terms of different evaluation measures including Homogeneity Score, Completeness Score, V-measure, Adjusted Mutual Information, and Adjusted Rand Index. Furthermore, NPIR is experimented on a real-life application for segmenting mall customers for effective decision making. The source code of NPIR is available at http://evo-ml.com/2019/10/28/npir/.																	1868-8071	1868-808X				MAR	2020	11	3					675	714		10.1007/s13042-019-01027-z													
J								Large-scale evolutionary optimization: a survey and experimental comparative study	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Differential evolution; Particle swarm optimization; Large-scale global optimization; Large-scale evolutionary optimization algorithms	DIFFERENTIAL EVOLUTION; COOPERATIVE COEVOLUTION; ALGORITHM; COLONY	In the last decades, global optimization problems are very common in many research fields of science and engineering and lots of evolutionary computation algorithms have been used to deal with such problems, such as differential evolution (DE) and particle swarm optimization (PSO). However, the algorithms performance rapidly decreases as the increasement of the problem dimension. In order to solve large-scale global optimization problems more efficiently, a lot of improved evolutionary computation algorithms, especially the improved DE or improved PSO algorithms have been proposed. In this paper, we want to analyze the differences and characteristics of various large-scale evolutionary optimization (LSEO) algorithms on some benchmark functions. We adopt the CEC2010 and the CEC2013 large-scale optimization benchmark functions to compare the performance of seven well-known LSEO algorithms. Then, we try to figure out which algorithms perform better on different types of benchmark functions based on simulation results. Finally, we give some potential future research directions of LSEO algorithms and make a conclusion.																	1868-8071	1868-808X				MAR	2020	11	3					729	745		10.1007/s13042-019-01030-4													
J								Hierarchical third-order tensor decomposition through inverse difference pyramid based on the three-dimensional Walsh-Hadamard transform with applications in data mining	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										3D inverse difference pyramid; data mining; hierarchical tensor decomposition; tensor representation; Walsh-Hadamard transform		A new approach is presented for hierarchical decomposition of third-order tensors through their transformation into the generalized three-dimensional (3D) spectrum space based on the inverse difference pyramid (IDP). For this, we choose the 3D Walsh-Hadamard transform (3D-WHT). As result, each tensor is represented as a spectral tensor of m hierarchical levels which contains selected low-frequency 3D-WHT coefficients. Calculating sequentially the inverse 3D-WHT for the coefficients from each pyramid level starting from its top, the tensor is approximated with increasing accuracy until its full restoration is achieved. To illustrate the new approach, given is the algorithm for hierarchical three-level tensor decomposition based on the reduced IDP. The proposed approach permits simultaneous decorrelation of tensor elements in three mutually orthogonal directions. The energy of the tensor elements is concentrated in a small number of spectral coefficients which build the top of the inverse pyramid. The use of the 3D-WHT permits to achieve minimum computational complexity, compared to deterministic 3D orthogonal transforms. The main applications of the new method for data mining in the contemporary intelligent systems are in the processing and analysis of large sets of different kinds of data/images/videos in the following areas: Compression of correlated image sequences, computer tomography, thermo vision, ultrasound and multichannel medical signals; search of 3D objects in image databases; extraction of features for recognition of 3D objects; multidimensional data denoising; multilayer watermarking of video sequences; and so on. This article is categorized under: Fundamental Concepts of Data and Knowledge > Big Data Mining Algorithmic Development > Spatial and Temporal Data Mining																	1942-4787	1942-4795				MAR	2020	10	2							e1314	10.1002/widm.1314													
J								Matching cognitively sympathetic individual styles to develop collective intelligence in digital communities	AI & SOCIETY										Argumentation research; Cyber humanistic; Cognition; Collaboration; Knowledge building; Knowledge management; Teamwork; Virtual groups	MEDIA RICHNESS; PERFORMANCE; DESIGN; OFFICE; TRUST; WORK	Creation, collection and retention of knowledge in digital communities is an activity that currently requires being explicitly targeted as a secure method of keeping intellectual capital growing in the digital era. In particular, we consider it relevant to analyze and evaluate the empathetic cognitive personalities and behaviors that individuals now have with the change from face-to-face communication (F2F) to computer-mediated communication (CMC) online. This document proposes a cyber-humanistic approach to enhance the traditional SECI knowledge management model. A cognitive perception is added to its cyclical process following design thinking interaction, exemplary for improvement of the method in which knowledge is continuously created, converted and shared. In building a cognitive-centered model, we specifically focus on the effective identification and response to cognitive stimulation of individuals, as they are the intellectual generators and multiplicators of knowledge in the online environment. Our target is to identify how geographically distributed-digital-organizations should align the individual's cognitive abilities to promote iteration and improve interaction as a reliable stimulant of collective intelligence. The new model focuses on analyzing the four different stages of knowledge processing, where individuals with sympathetic cognitive personalities can significantly boost knowledge creation in a virtual social system. For organizations, this means that multidisciplinary individuals can maximize their extensive potential, by externalizing their knowledge in the correct stage of the knowledge creation process, and by collaborating with their appropriate sympathetically cognitive remote peers.																	0951-5666	1435-5655				MAR	2020	35	1					5	15		10.1007/s00146-017-0780-x													
J								Potential of full human-machine symbiosis through truly intelligent cognitive systems	AI & SOCIETY										Cognitive architecture; Emotion; Motivation; Personality; Symbiosis	IMPLICIT; EXPLICIT; MODEL	It is highly likely that, to achieve full human-machine symbiosis, truly intelligent cognitive systems-human-like (or even beyond)-may have to be developed first. Such systems should not only be capable of performing human-like thinking, reasoning, and problem solving, but also be capable of displaying human-like motivation, emotion, and personality. In this opinion article, I will argue that such systems are indeed possible and needed to achieve true and full symbiosis with humans. A computational cognitive architecture (named Clarion) is used in this article to illustrate, in a preliminary way, what can be achieved in this regard. It is shown that Clarion involves complex structures, representations, and mechanisms, and is capable of capturing human cognitive performance (including skills, reasoning, memory, and so on) as well as human motivation, emotion, personality, and other relevant aspects. It is further argued that the cognitive architecture can enable and facilitate true human-machine symbiosis.																	0951-5666	1435-5655				MAR	2020	35	1					17	28		10.1007/s00146-017-0775-7													
J								Machine learning, inductive reasoning, and reliability of generalisations	AI & SOCIETY										Representation; Object naturalism; Subject naturalism; Machine learning; Statistical learning theory; Deep learning	NETWORKS	The present paper shows how statistical learning theory and machine learning models can be used to enhance understanding of AI-related epistemological issues regarding inductive reasoning and reliability of generalisations. Towards this aim, the paper proceeds as follows. First, it expounds Price's dual image of representation in terms of the notions of e-representations and i-representations that constitute subject naturalism. For Price, this is not a strictly anti-representationalist position but rather a dualist one (e- and i-representations). Second, the paper links this debate with machine learning in terms of statistical learning theory becoming more viable epistemological tool when it abandons the perspective of object naturalism. The paper then argues that machine learning grounds a form of knowing that can be understood in terms of e- and i-representation learning. Third, this synthesis shows a way of analysing inductive reasoning in terms of reliability of generalisations stemming from a structure of e- and i-representations. In the age of Artificial Intelligence, connecting Price's dual view of representation with Deep Learning provides an epistemological way forward and even perhaps an approach to how knowing is possible.																	0951-5666	1435-5655				MAR	2020	35	1					29	37		10.1007/s00146-018-0860-6													
J								The Borg-eye and the We-I. The production of a collective living body through wearable computers	AI & SOCIETY										Phenomenology; Postphenomenology; Mediation theory; Embodiment relations; Wearable computers; Collective subject	PHENOMENOLOGICAL ANALYSIS; TECHNOLOGY; CYBORG	The aim of this work is to analyze the constitution of a new collective subject thanks to wearable computers. Wearable computers are emerging technologies which are supposed to become pervasively used in the near future. They are devices designed to be on us every single moment of our life and to capture every experience we have. Therefore, we need to be prepared to such intrusive devices and to analyze potential effect they will have on us and our society. Thanks to a phenomenological and postphenomenological analysis, we will show how these technologies are able to generate a new collective subject with its different collective needs and appetites by merging the living body of many subjects into one. The world becomes dwelled by these new collective subjects, and we will feed their own peculiar needs.																	0951-5666	1435-5655				MAR	2020	35	1					39	49		10.1007/s00146-018-0840-x													
J								Organic and dynamic tool for use with knowledge base of AI ethics for promoting engineers' practice of ethical AI design	AI & SOCIETY										AI ethics; Design theory; Creativity support; Hierarchical representation of artifacts; Knowledge base	CONFIGURATION-DESIGN	In recent years, ethical questions related to the development of artificial intelligence (AI) are being increasingly discussed. However, there has not been enough corresponding increase in the research and development associated with AI technology that incorporates with ethical discussion. We therefore implemented an organic and dynamic tool for use with knowledge base of AI ethics for engineers to promote engineers' practice of ethical AI design to realize further social values. Here, "organic" means that the tool deals with complex relationships among different AI ethics. "Dynamic" means that the tool dynamically adopts new issues and helps engineers think in their own contexts. Data in the knowledge base of the tool is standardized based on the ethical design theory that consists of an extension of the hierarchical representation of artifacts to understand ethical considerations from the perspective of engineering, and a description method to express the design ideas. In addition, we apply the dynamic knowledge management model called knowledge liquidization and crystallization. To discuss the effects, we introduce three cases: a case for the clarification of differences in the structures among AI ethics and design ideas, a case for the presentation of semantic distance among them, and a case for the recommendation of the scenario paths that allow engineers to seamlessly use AI ethics in their own contexts. We discuss the effectiveness of the tool. We also show the probability that engineers can reconstruct AI ethics as a more practical one with professional ethicists.																	0951-5666	1435-5655				MAR	2020	35	1					51	71		10.1007/s00146-018-0867-z													
J								Digital hermeneutics: from interpreting with machines to interpretational machines	AI & SOCIETY										Hermeneutics; Digital hermeneutics; Political opinion; Data; Digital traces; Methods; Information; Information technologies; Interpretational machines	INFORMATION; NETWORKS	Today, there is an emerging interest for the potential role of hermeneutics in reflecting on the practices related to digital technologies and their consequences. Nonetheless, such an interest has neither given rise to a unitary approach nor to a shared debate. The primary goal of this paper is to map and synthetize the different existing perspectives to pave the way for an open discussion on the topic. The article is developed in two steps. In the first section, the authors analyze digital hermeneutics "in theory" by confronting and systematizing the existing literature. In particular, they stress three main distinctions among the approaches: (1) between "methodological" and "ontological" digital hermeneutics; (2) between data- and text-oriented digital hermeneutics; and (3) between "quantitative" and "qualitative" credos in digital hermeneutics. In the second section, they consider digital hermeneutics "in action", by critically analyzing the uses of digital data (notably tweets) for studying a classical object such as the political opinion. In the conclusion, we will pave the way to an ontological turn in digital hermeneutics. Most of this article is devoted to the methodological issue of interpreting with digital machines. The main task of an ontological digital hermeneutics would consist instead in wondering if it is legitimate, and eventually to which extent, to speak of digital technologies, or at least of some of them, as interpretational machines.																	0951-5666	1435-5655				MAR	2020	35	1					73	86		10.1007/s00146-018-0856-2													
J								Bird Song Diamond in Deep Space 8k	AI & SOCIETY										Birdsong; Collaboration; Iterative design; Language; Interspecies communication; Site; habitat specificity	SOUND; LOUDSPEAKER	The Bird Song Diamond (BSD) project is a series of multifaceted and multidisciplinary installations with the aim of bringing contemporary research on bird communication to a large public audience. Using art and technology to create immersive experiences, BSD allows large audiences to embody bird communication rather than passively observe. In particular, BSD Mimic, a system for mimicking bird song, asks participants to grapple with both audition and vocalization of birdsong. The use of interactive installations for public outreach provides unique experiences to a diverse audience, while providing direct feedback for artists and researchers interested in the success of such outreach. By following an iterative design process, both artists and researchers have been able to evaluate the effectiveness of each installation for promoting audience engagement with the subject matter. The execution and evaluation of each iteration of BSD is described throughout the paper. In addition, the process of interdisciplinary collaboration in our project has led to a more defined role of the artist as a facilitator of specialists. BSD Mimic has also led to further questions about the nature of audience collaboration for an engaged experience.																	0951-5666	1435-5655				MAR	2020	35	1					87	101		10.1007/s00146-018-0862-4													
J								The problem of machine ethics in artificial intelligence	AI & SOCIETY										Artificial intelligence; Artificial moral agent; Moral agency; Mind; Subjectivity		The advent of the intelligent robot has occupied a significant position in society over the past decades and has given rise to new issues in society. As we know, the primary aim of artificial intelligence or robotic research is not only to develop advanced programs to solve our problems but also to reproduce mental qualities in machines. The critical claim of artificial intelligence (AI) advocates is that there is no distinction between mind and machines and thus they argue that there are possibilities for machine ethics, just as human ethics. Unlike computer ethics, which has traditionally focused on ethical issues surrounding human use of machines, AI or machine ethics is concerned with the behaviour of machines towards human users and perhaps other machines as well, and the ethicality of these interactions. The ultimate goal of machine ethics, according to the AI scientists, is to create a machine that itself follows an ideal ethical principle or a set of principles; that is to say, it is guided by this principle or these principles in decisions it makes about possible courses of action it could take(a). Thus, machine ethics task of ensuring ethical behaviour of an artificial agent. Although, there are many philosophical issues related to artificial intelligence, but our attempt in this paper is to discuss, first, whether ethics is the sort of thing that can be computed. Second, if we are ascribing mind to machines, it gives rise to ethical issues regarding machines. And if we are not drawing the difference between mind and machines, we are not only redefining specifically human mind but also the society as a whole. Having a mind is, among other things, having the capacity to make voluntary decisions and actions. The notion of mind is central to our ethical thinking, and this is because the human mind is self-conscious, and this is a property that machines lack, as yet.																	0951-5666	1435-5655				MAR	2020	35	1					103	111		10.1007/s00146-017-0768-6													
J								Legal framework for small autonomous agricultural robots	AI & SOCIETY										Agriculture; Robotics; Legal; Safety agribot		Legal structures may form barriers to, or enablers of, adoption of precision agriculture management with small autonomous agricultural robots. This article develops a conceptual regulatory framework for small autonomous agricultural robots, from a practical, self-contained engineering guide perspective, sufficient to get working research and commercial agricultural roboticists quickly and easily up and running within the law. The article examines the liability framework, or rather lack of it, for agricultural robotics in EU, and their transpositions to UK law, as a case study illustrating general international legal concepts and issues. It examines how the law may provide mitigating effects on the liability regime, and how contracts can be developed between agents within it to enable smooth operation. It covers other legal aspects of operation such as the use of shared communications resources and privacy in the reuse of robot-collected data. Where there are some grey areas in current law, it argues that new proposals could be developed to reform these to promote further innovation and investment in agricultural robots.																	0951-5666	1435-5655				MAR	2020	35	1					113	134		10.1007/s00146-018-0846-4													
J								AI recognition of differences among book-length texts	AI & SOCIETY										Artificial Intelligence; Latent semantic analysis; Natural language processing; Textual analysis	INFORMATION; RETRIEVAL	Can an Artificial Intelligence make distinctions among major works of politics, philosophy, and fiction without human assistance? In this paper, latent semantic analysis (LSA) is used to find patterns in a relatively small sample of notable works archived by Project Gutenberg. It is shown that an LSA-equipped AI can distinguish quite sharply between fiction and non-fiction works, and can detect some differences between political philosophy and history, and between conventional fiction and fantasy/science fiction. It is conjectured that this capability is a step in the direction of "M-comprehension" (or "machine comprehension") by AIs.																	0951-5666	1435-5655				MAR	2020	35	1					135	146		10.1007/s00146-018-0851-7													
J								Classification of global catastrophic risks connected with artificial intelligence	AI & SOCIETY										Artificial intelligence; Global risks; Military drones; Superintelligence; Existential risk	COMPUTER	A classification of the global catastrophic risks of AI is presented, along with a comprehensive list of previously identified risks. This classification allows the identification of several new risks. We show that at each level of AI's intelligence power, separate types of possible catastrophes dominate. Our classification demonstrates that the field of AI risks is diverse, and includes many scenarios beyond the commonly discussed cases of a paperclip maximizer or robot-caused unemployment. Global catastrophic failure could happen at various levels of AI development, namely, (1) before it starts self-improvement, (2) during its takeoff, when it uses various instruments to escape its initial confinement, or (3) after it successfully takes over the world and starts to implement its goal system, which could be plainly unaligned, or feature-flawed friendliness. AI could also halt at later stages of its development either due to technical glitches or ontological problems. Overall, we identified around several dozen scenarios of AI-driven global catastrophe. The extent of this list illustrates that there is no one simple solution to the problem of AI safety, and that AI safety theory is complex and must be customized for each AI development level.																	0951-5666	1435-5655				MAR	2020	35	1					147	163		10.1007/s00146-018-0845-5													
J								Social choice ethics in artificial intelligence	AI & SOCIETY										Artificial intelligence; Ethics; Social choice; Standing; Measurement; Aggregation	PREFERENCE; BEHAVIOR	A major approach to the ethics of artificial intelligence (AI) is to use social choice, in which the AI is designed to act according to the aggregate views of society. This is found in the AI ethics of "coherent extrapolated volition" and "bottom-up ethics". This paper shows that the normative basis of AI social choice ethics is weak due to the fact that there is no one single aggregate ethical view of society. Instead, the design of social choice AI faces three sets of decisions: standing, concerning whose ethics views are included; measurement, concerning how their views are identified; and aggregation, concerning how individual views are combined to a single view that will guide AI behavior. These decisions must be made up front in the initial AI design-designers cannot "let the AI figure it out". Each set of decisions poses difficult ethical dilemmas with major consequences for AI behavior, with some decision options yielding pathological or even catastrophic results. Furthermore, non-social choice ethics face similar issues, such as whether to count future generations or the AI itself. These issues can be more important than the question of whether or not to use social choice ethics. Attention should focus on these issues, not on social choice.																	0951-5666	1435-5655				MAR	2020	35	1					165	176		10.1007/s00146-017-0760-1													
J								The role of robotics and AI in technologically mediated human evolution: a constructive proposal	AI & SOCIETY										Cognitive social science; Computational model; Social simulation; Free energy principle; Directed evolution; AI arms race	FREE-ENERGY PRINCIPLE; INTENTION RECOGNITION; MODEL; COOPERATION; COMMITMENT	This paper proposes that existing computational modeling research programs may be combined into platforms for the information of public policy. The main idea is that computational models at select levels of organization may be integrated in natural terms describing biological cognition, thereby normalizing a platform for predictive simulations able to account for both human and environmental costs associated with different action plans and institutional arrangements over short and long time spans while minimizing computational requirements. Building from established research programs, the proposal aims to take advantage of current momentum in the direction of the integration of the cognitive with social and natural sciences, reduce start-up costs and increase speed of development. These are all important upshots given rising unease over the potential for AI and related technologies to shape the world going forward.																	0951-5666	1435-5655				MAR	2020	35	1					177	185		10.1007/s00146-019-00877-z													
J								An invitation to critical social science of big data: from critical theory and critical research to omniresistance	AI & SOCIETY										Philosophy of big data; Cognitive science of big data; Economics of big data; Sociology of big data; Psychology of big data; Politics of big data	MEDIA; SOCIOLOGY; INTERNET	How a social science of big data would look like? In this article, we exemplify such a social science through a number of cases. We start our discussion with the epistemic qualities of big data. We point out to the fact that contrary to the big data champions, big data is neither new nor a miracle without any error nor reliable and rigorous as assumed by its cheer leaders. Secondly, we identify three types of big data: natural big data, artificial big data and human big data. We present and discuss in what ways they are similar and in what other ways they differ. The assumption of a homogenous big data in fact misleads the relevant discussions. Thirdly, we extended 3 Vs of the big data and add veracity with reference to other researchers and violability which is the current author's proposal. We explain why the trinity of Vs is insufficient to characterize big data. Instead, a quintinity is proposed. Fourthly, we develop an economic analogy to discuss the notions of data production, data consumption, data colonialism, data activism, data revolution, etc. In this context, undertaking a Marxist approach, we explain what we mean by data fetishism. Fifthly, we reflect on the implications of growing up with big data, offering a new research area which is called as developmental psychology of big data. Finally, we sketch data resistance and the newly proposed notion of omniresistance, i.e. resisting anywhere at any occasion against the big brother watching us anywhere and everywhere.																	0951-5666	1435-5655				MAR	2020	35	1					187	195		10.1007/s00146-018-0868-y													
J								The contradictions of digital modernity	AI & SOCIETY										Digital modernity; Modernisation; Disruption; Innovation; Data	SMART CITIES; INTELLIGENCE; INNOVATION; RISE	This paper explores the concept of digital modernity, the extension of narratives of modernity with the special affordances of digital networked technology. Digital modernity produces a new narrative which can be taken in many ways: to be descriptive of reality; a teleological account of an inexorable process; or a normative account of an ideal sociotechnical state. However, it is understood that narratives of digital modernity help shape reality via commercial and political decision-makers, and examples are given from the politics and society of the United Kingdom. The paper argues that digital modernity has two dimensions, of progression through time and progression through space, and these two dimensions can be in contradiction. Contradictions can also be found between ideas of digital modernity and modernity itself, and also between digital modernity and some of the basic pre-modern concepts that underlie the whole technology industry. Therefore, digital modernity may not be a sustainable goal for technology development.																	0951-5666	1435-5655				MAR	2020	35	1					197	208		10.1007/s00146-018-0843-7													
J								What do we owe to intelligent robots?	AI & SOCIETY										Artificially intelligent robots; Moral status; Moral rights; Moral agency; Full ethical agents; Machine rights	DIGNITY	Great technological advances in such areas as computer science, artificial intelligence, and robotics have brought the advent of artificially intelligent robots within our reach within the next century. Against this background, the interdisciplinary field of machine ethics is concerned with the vital issue of making robots "ethical" and examining the moral status of autonomous robots that are capable of moral reasoning and decision-making. The existence of such robots will deeply reshape our socio-political life. This paper focuses on whether such highly advanced yet artificially intelligent beings will deserve moral protection (in the form of being granted moral rights) once they become capable of moral reasoning and decision-making. I argue that we are obligated to grant them moral rights once they have become full ethical agents, i.e., subjects of morality. I present four related arguments in support of this claim and thereafter examine four main objections to the idea of ascribing moral rights to artificial intelligent robots.																	0951-5666	1435-5655				MAR	2020	35	1					209	223		10.1007/s00146-018-0844-6													
J								Artificial intelligence: consciousness and conscience	AI & SOCIETY										Artificial intelligence; Deep-learning neural networks; Social robots; Consciousness; Conscience		Our society is in the middle of the AI revolution. We discuss several applications of AI, in particular medical causality, where deep-learning neural networks screen through big data bases, extracting associations between a patient's condition and possible causes. While beneficial in medicine, several questionable AI trading strategies have emerged in finance. Though advantages in many aspects of our lives, serious threats of AI exist. We suggest several regulatory measures to reduce these threats. We further discuss whether 'full AI robots' should be programmed with a virtual consciousness and conscience. While this would reduce AI threats via motivational control, other threats such as the desire for AI-human socioeconomic equality could prove detrimental.																	0951-5666	1435-5655				MAR	2020	35	1					225	235		10.1007/s00146-019-00880-4													
J								Digital akrasia: a qualitative study of phubbing	AI & SOCIETY										Attention; Distraction; Habits; Phubbing; Smartphones	DISTRACTION; TECHNOFERENCE; TECHNOLOGY; LIFE; MOTHERS; PARENT; PHONE	The present article focuses on the issue of ignoring conversational partners in favor of one's phone, or what has also become known as phubbing. Prior research has shown that this behavior is associated with a host of negative interpersonal consequences. Since phubbing by definition entails adverse effects, however, it is interesting to explore why people continue to engage in this hurtful behavior: Are they unaware that phubbing is hurtful to others? Or do they simply not care? Building on interviews with students in a Danish business college, the article reveals a pronounced discrepancy in young people's relationship to phubbing: While they emphatically denounce phubbing as both annoying and disrespectful, they readily admit to phubbing others. In other words, they often act against their own moral convictions. Importantly, participants describe this discrepancy as a result of an unintentional inclination to divert attentional engagement. On the basis of these results, the article develops the notion of digital akrasia, which can be defined as a tendency to become swept up by ones digital devices in spite of better intentions. It is proposed that this phenomenon may be the result of bad technohabits. Further implications are discussed.																	0951-5666	1435-5655				MAR	2020	35	1					237	244		10.1007/s00146-019-00876-0													
J								Is it possible to cure Internet addiction with the Internet?	AI & SOCIETY										Internet addiction; Intervention; Recovery framework; Artificial intelligence	PSYCHOMETRIC PROPERTIES; PARENTAL MEDIATION; DISORDER; PERSONALITY; ADOLESCENTS	Significant technological advancements over the last two decades have led to enhanced accessibility to computing devices and the Internet. Our society is experiencing an ever-growing integration of the Internet into everyday lives, and this has transformed the way we obtain and exchange information, communicate and interact with one another as well as conduct business. However, the term 'Internet addiction' (IA) has emerged from problematic and excessive Internet usage which leads to the development of addictive cyber-behaviours, causing health and social problems. The most commonly used intervention treatments such as motivational interviewing, cognitive-behavioural therapy, and retreat or inpatient care mix a variety of psychotherapy theories to treat such addictive behaviour and try to address underlying psychosocial issues that are often coexistent with IA, but the efficacy of these approaches is not yet proved. The aim of this paper is to address the question of whether it is possible to cure IA with the Internet. After detailing the current state-of-the-art including various IA definitions, risk factors, assessment methods and IA treatments, we outline the main research challenges that need to be solved. Moreover, we propose an Internet-based IA Recovery Framework (IARF) which uses AI to closely observe, visualize and analyse patient's Internet usage behaviour for possible staged intervention. The proposal to use smart Internet-based systems to control IA can be expected to be controversial. This paper is intended to stimulate further discussion and research in IA recovery through Internet-based frameworks.																	0951-5666	1435-5655				MAR	2020	35	1					245	255		10.1007/s00146-018-0858-0													
J								The posthuman: AI, dronology, and "becoming alien"	AI & SOCIETY										Posthumanism; Technological singularity; Human extinction event; Robotic evolution; Ideology; Biotechnology; Commodification; Neganthropocene; End of history		This paper is addressed to recent theoretical discussions of the Anthropocene, in particular Bernard Stiegler's Neganthropocene (Open Universities Press, 2018), which argues: "As we drift past tipping points that put future biota at risk, while a posttruth regime institutes the denial of 'climate change' (as fake news), and as Silicon Valley assistants snatch decision and memory, and as gene-editing and a financially engineered bifurcation advances over the rising hum of extinction events and the innumerable toxins and conceptual opiates that Anthropocene Talk fascinated itself with-in short, as 'the Anthropocene' discloses itself as a dead-end trap horizontal ellipsis ". The objective of this paper is, therefore, twofold: (1) to discuss how the Anthropocene is appropriated to certain ideological discourses (paradoxically) to maintain the hegemony of precisely those systems of production that have most accelerated climate change etc. and (2) to consider how the factography of the Anthropocene is exploited in this process to mask the ideological character of industry-aligned "technocratic" environmental management. The paper is not concerned with specific case studies in terms of government and industry policy, or climate science, but rather with the ways in which the discourse of the Anthropocene has been inflected within the humanities and the broader cultural field-that is to say, ideologically, as a system or logic of meaning. How the Anthropocene "means" is, in this respect, a question of some importance. This paper does not attempt to address all the facets of this question, but focuses upon a central "apocalyptic" strain in the discourse of the Anthropocene drawn particularly from Francis Fukuyama's millennial posthumanism and centred in the question of "sustainability" as catastrophe management-with the risk that real environmental degradation will become an alibi for a revived neoliberalism. In other words, the critical Earth system transformations that characterise the Anthropocene are themselves commodities, and that the project of their amelioration is in process of defining a future (opportunistic) "crisis" rhetoric with a global political franchise. The ideological import of the Anthropocene stems precisely from the fact that it is planetary and, while catalysed by human agency, independent in its specific behaviour from it. The Anthropocene objectively presents as the contemporary counterpart of the Cold War doctrine of mutually assured destruction and the most compelling argument for a new kind of technological "arms race". However, it also presents as the condition of an emerging ideological discourse which will determine how this race is run. From the discourse on "energy security" to the widespread "security crackdown" on environmental activists across the socalled developed and developing world, the Anthropocene has come to represent the co-option of a scientific factography for the thinly disguised resurgence of "ideological science" of the Fukuyamaesque variety (posthistory, posthuman). For Fukuyama, the true meaning of "posthuman" is thus the accomplishment of humanity's historical mission. As the "End of History" designates an end of ideological struggle, so too, the denouement of the Anthropocene and the "ends of man" represents the accomplished purpose of species warfare: dominion, not simply over the world, but over all possible worlds. According to this narrative, science-like technology-must be uniquely at the service of the maintenance of the global order, organised around a universal appeal to "crisis management". It is precisely for this reason that what calls itself posthuman masks the return of an ever-more-apocalyptic Humanism.																	0951-5666	1435-5655				MAR	2020	35	1					257	262		10.1007/s00146-018-0872-2													
J								Virtuous vs. utilitarian artificial moral agents	AI & SOCIETY										Machine ethics; Artificial moral agent; Machine learning; Virtue theory; Two-level utilitarianism		Given that artificial moral agents-such as autonomous vehicles, lethal autonomous weapons, and automated trading systems-are now part of the socio-ethical equation, we should morally evaluate their behavior. How should artificial moral agents make decisions? Is one moral theory better suited than others for machine ethics? After briefly overviewing the dominant ethical approaches for building morality into machines, this paper discusses a recent proposal, put forward by Don Howard and Ioan Muntean (2016, 2017), for an artificial moral agent based on virtue theory. While the virtuous artificial moral agent has various strengths, this paper argues that a rule-based utilitarian approach (in contrast to a strict act utilitarian approach) is superior, because it can capture the most important features of the virtue-theoretic approach while realizing additional significant benefits. Specifically, a two-level utilitarian artificial moral agent incorporating both established moral rules and a utility calculator is especially well suited for machine ethics.																	0951-5666	1435-5655				MAR	2020	35	1					263	271		10.1007/s00146-018-0871-3													
J								The future of war: could lethal autonomous weapons make conflict more ethical?	AI & SOCIETY										Lethal autonomous weapons; Artificial intelligence; Military robots; Ethics; Laws of war		Lethal Autonomous Weapons (LAWs) are robotic weapon systems, primarily of value to the military, that could engage in offensive or defensive actions without human intervention. This paper assesses and engages the current arguments for and against the use of LAWs through the lens of achieving more ethical warfare. Specific interest is given particularly to ethical LAWs, which are artificially intelligent weapon systems that make decisions within the bounds of their ethics-based code. To ensure that a wide, but not exhaustive, survey of the implications of employing such ethical devices to replace humans in warfare is taken into account, this paper will engage on matters related to current scholarship on the rejection or acceptance of LAWs-including contemporary technological shortcomings of LAWs to differentiate between targets and the behavioral and psychological volatility of humans-and current and proposed regulatory infrastructures for developing and using such devices. After careful consideration of these factors, this paper will conclude that only ethical LAWs should be used to replace human involvement in war, and, by extension of their consistent abilities, should remove humans from war until a more formidable discovery is made in conducting ethical warfare.																	0951-5666	1435-5655				MAR	2020	35	1					273	282		10.1007/s00146-019-00879-x													
J								Imagination machines, Dartmouth-based Turing tests, & a potted history of responses	AI & SOCIETY										Imagination machine; Creative imagination; Turing Test; Definitional problem; Attributional problem; Creativity		Mahadevan (2018, AAAI Conference. ) proposes that we are at the cusp of imagination science, one of whose primary concerns will be the design of imagination machines. Programs have been written that are capable of generating jokes (Kim Binsted's JAPE), producing line-drawings that have been exhibited at such galleries as the Tate (Harold Cohen's AARON), composing music in several styles reminiscent of such greats as Vivaldi and Mozart (David Cope's Emmy), proving geometry theorems (Herb Gelernter's IBM program), and inducing quantitative laws from empirical data (Pat Langley, Gary Bradshaw, Jan Zytkow, and Herbert Simon's BACON). In recent years, Dartmouth has been hosting Turing Tests in creativity in three categories: short stories, sonnets, and dance music DJ sets. In this post, I will provide a brief and non-exhaustive survey of some plausible responses to these imagination machines and the related prospects for our understanding of the imagination.																	0951-5666	1435-5655				MAR	2020	35	1					283	287		10.1007/s00146-018-0855-3													
J								Dynamic search trajectory methods for global optimization	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Dynamic search trajectories; Trajectory methods; Autonomous initial value problems; Globally convergent algorithms; Nonmonotone convergent strategies; Global optimization; Neural networks training	LAMINATED COMPOSITE PLATES; OPTIMAL-DESIGN; UNCONSTRAINED MINIMIZATION; CONVERGENCE CONDITIONS; STEEPEST DESCENT; ALGORITHM; OPERATORS; VERSION	A detailed review of the dynamic search trajectory methods for global optimization is given. In addition, a family of dynamic search trajectories methods that are created using numerical methods for solving autonomous ordinary differential equations is presented. Furthermore, a strategy for developing globally convergent methods that is applicable to the proposed family of methods is given and the corresponding theorem is proved. Finally, theoretical results for obtaining nonmonotone convergent methods that exploit the accumulated information with regard to the most recent values of the objective function are given.																	1012-2443	1573-7470				MAR	2020	88	1-3					3	37		10.1007/s10472-019-09661-7													
J								Kernel classification using a linear programming approach	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Kernel methods; Classification; Linear programming		A support vector machine (SVM) classifier corresponds in its most basic form to a quadratic programming problem. Various linear variations of support vector classification have been investigated such as minimizing the L-1-norm of the weight-vector instead of the L-2-norm. In this paper we introduce a classifier where we minimize the boundary (lower envelope) of the epigraph that is generated over a set of functions, which can be interpreted as a measure of distance or slack from the origin. The resulting classifier appears to provide a generalization performance similar to SVMs while displaying a more advantageous computational complexity. The discussed formulation can also be extended to allow for cases with imbalanced data.																	1012-2443	1573-7470				MAR	2020	88	1-3					39	51		10.1007/s10472-019-09642-w													
J								Complexity and approximability of the Euclidean generalized traveling salesman problem in grid clusters	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Generalized traveling salesman problem; Polynomial time approximation scheme; Polynomial time solvable subclass	APPROXIMATION ALGORITHMS; TSP; SCHEMES; MST	We consider the geometric version of the well-known Generalized Traveling Salesman Problem introduced in 2015 by Bhattacharya et al. that is called the Euclidean Generalized Traveling Salesman Problem in Grid Clusters (EGTSP-GC). They proved the intractability of the problem and proposed first polynomial time algorithms with fixed approximation factors. The extension of these results in the field of constructing the polynomial time approximation schemes (PTAS) and the description of non-trivial polynomial time solvable subclasses for the EGTSP-GC appear to be relevant in the light of the classic C. Papadimitriou result on the intractability of the Euclidean TSP and recent inapproximability results for the Traveling Salesman Problem with Neighborhoods (TSPN) in the case of discrete neighborhoods. In this paper, we propose Efficient Polynomial Time Approximation Schemes (EPTAS) for two special cases of the EGTSP-GC, when the number of clusters k=O(logn)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$k=O(\log n)$\end{document} and k=n-O(logn)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$k=n-O(\log n)$\end{document}. Also, we show that any time, when one of the grid dimensions (height or width) is fixed, the EGTSP-GC can be solved to optimality in polynomial time. As a consequence, we specify a novel non-trivial polynomially solvable subclass of the Euclidean TSP in the plane.																	1012-2443	1573-7470				MAR	2020	88	1-3					53	69		10.1007/s10472-019-09626-w													
J								Combining intelligent heuristics with simulators in hotel revenue management	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Machine learning and intelligent optimization; Hotel revenue management; Simulation-based optimization; Optimization heuristics	VIRTUAL NESTING CONTROLS; MINIMIZATION; BOOKING	Revenue Management uses data-driven modelling and optimization methods to decide what to sell, when to sell, to whom to sell, and for which price, in order to increase revenue and profit. Hotel Revenue Management is a very complex context characterized by nonlinearities, many parameters and constraints, and stochasticity, in particular in the demand by customers. It suffers from the curse of dimensionality (Bellman 2015): when the number of variables increases (number of rooms, number possible prices and capacities, number of reservation rules and constraints) exact solutions by dynamic programming or by alternative global optimization techniques cannot be used and one has to resort to intelligent heuristics, i.e., methods which can improve current solutions but without formal guarantees of optimality. Effective heuristics can incorporate "learning" ("reactive" schemes) that update strategies based on the past history of the process, the past reservations received up to a certain time and the previous steps in the iterative optimization process. Different approaches can be classified according to the specific model considered (stochastic demand and hotel rules), the control mechanism (the pricing policy) and the optimization technique used to determine improving or optimal solutions. In some cases, model definitions, control mechanism and solution techniques are strongly interrelated: this is the case of dynamic programming, which demands suitably simplified problem formulations. We design a flexible discrete-event simulator for the hotel reservation process and experiment different approaches though measurements of the expected effect on profit (obtained by carefully separating a "training" phase from the final "validation" phase obtained from different simulations). The experimental results show the effectiveness of intelligent heuristics with respect to exact optimization methods like dynamic programming, in particular for more constrained situations (cases when demand tends to saturate hotel room availability), when the simplifying assumptions needed to make the problem analytically treatable do not hold.																	1012-2443	1573-7470				MAR	2020	88	1-3					71	90		10.1007/s10472-019-09651-9													
J								Model simplification for supervised classification of metabolic networks	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Supervised classification; Network data; Metabolic networks; Network model simplification	GRAPH; VALIDATION; GEO	Many real applications require the representation of complex entities and their relations. Frequently, networks are the chosen data structures, due to their ability to highlight topological and qualitative characteristics. In this work, we are interested in supervised classification models for data in the form of networks. Given two or more classes whose members are networks, we build mathematical models to classify them, based on various graph distances. Due to the complexity of the models, made of tens of thousands of nodes and edges, we focus on model simplification solutions to reduce execution times, still maintaining high accuracy. Experimental results on three datasets of biological interest show the achieved performance improvements.																	1012-2443	1573-7470				MAR	2020	88	1-3					91	104		10.1007/s10472-019-09640-y													
J								Soft computing methods for multiobjective location of garbage accumulation points in smart cities	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Computational intelligence; Waste management; Smart cities	URBAN WASTE MANAGEMENT; COLLECTION SITES; AREA	This article describes the application of soft computing methods for solving the problem of locating garbage accumulation points in urban scenarios. This is a relevant problem in modern smart cities, in order to reduce negative environmental and social impacts in the waste management process, and also to optimize the available budget from the city administration to install waste bins. A specific problem model is presented, which accounts for reducing the investment costs, enhance the number of citizens served by the installed bins, and the accessibility to the system. A family of single- and multi-objective heuristics based on the PageRank method and two mutiobjective evolutionary algorithms are proposed. Experimental evaluation performed on real scenarios on the cities of Montevideo (Uruguay) and Bahia Blanca (Argentina) demonstrates the effectiveness of the proposed approaches. The methods allow computing plannings with different trade-off between the problem objectives. The computed results improve over the current planning in Montevideo and provide a reasonable budget cost and quality of service for Bahia Blanca.																	1012-2443	1573-7470				MAR	2020	88	1-3					105	131		10.1007/s10472-019-09647-5													
J								Exact algorithms for two integer-valued problems of searching for the largest subset and longest subsequence	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Euclidean space; Largest subset; Longest subsequence; Quadratic variation; Exact algorithm; Pseudopolynomial time	TIME-SERIES DATA; CLUSTER-ANALYSIS	The following two strongly NP-hard problems are considered. In the first problem, we need to find in the given finite set of points in Euclidean space the subset of largest size. The sum of squared distances between the elements of this subset and its unknown centroid (geometrical center) must not exceed a given value. This value is defined as percentage of the sum of squared distances between the elements of the input set and its centroid. In the second problem, the input is a sequence (not a set) and we have some additional constraints on the indices of the elements of the chosen subsequence. The restriction on the sum of squared distances is the same as in the first problem. Both problems can be treated as data editing problems aimed to find similar elements and removal of extraneous (dissimilar) elements. We propose exact algorithms for the cases of both problems in which the input points have integer-valued coordinates. If the space dimension is bounded by some constant, our algorithms run in a pseudopolynomial time. Some results of numerical experiments illustrating the performance of the algorithms are presented.																	1012-2443	1573-7470				MAR	2020	88	1-3					157	168		10.1007/s10472-019-09623-z													
J								Effective IG heuristics for a single-machine scheduling problem with family setups and resource constraints	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Single machine scheduling; Family setup-times; Resource constraints; Total tardiness; Meta-heuristics	ITERATED GREEDY ALGORITHM; MINIMIZING TOTAL TARDINESS; TIMES	In this paper we investigate the problem of scheduling a set of jobs on a single-machine. The jobs are classified in families and setup times are required between the processing of two jobs of different families. Each job requires a certain amount of a common resource that is supplied through upstream processes. The total resource consumed must not exceed the resource supply up. Therefore, jobs may have to wait and the machine has to be idle due to an insufficient availability of the resource. To minimize the total tardiness, simple and effective iterated greedy (IG) heuristics are proposed. Different neighborhood operators are used in the local search phase. To choose the right neighborhood operators, we propose an adaptive selecting strategy. The heuristics are tested over an extensive computational experience on benchmark of instances from the literature and instances randomly generated in this work. Experimental results and statistical tests show that the proposed heuristics are able to obtain high-quality solutions within reasonable computational effort, and they outperform the state-of-the-art heuristic.																	1012-2443	1573-7470				MAR	2020	88	1-3					169	185		10.1007/s10472-019-09646-6													
J								Targeting solutions in Bayesian multi-objective optimization: sequential and batch versions	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Gaussian processes; Bayesian optimization; Computer experiments; Preference-based optimization; Parallel optimization; 65Kxx	IMPROVEMENT CRITERIA; GLOBAL OPTIMIZATION; ALGORITHM	Multi-objective optimization aims at finding trade-off solutions to conflicting objectives. These constitute the Pareto optimal set. In the context of expensive-to-evaluate functions, it is impossible and often non-informative to look for the entire set. As an end-user would typically prefer a certain part of the objective space, we modify the Bayesian multi-objective optimization algorithm which uses Gaussian Processes and works by maximizing the Expected Hypervolume Improvement, to focus the search in the preferred region. The cumulated effects of the Gaussian Processes and the targeting strategy lead to a particularly efficient convergence to the desired part of the Pareto set. To take advantage of parallel computing, a multi-point extension of the targeting criterion is proposed and analyzed.																	1012-2443	1573-7470				MAR	2020	88	1-3					187	212		10.1007/s10472-019-09644-8													
J								Constructing orthogonal designs in powers of two via symbolic computation and rewriting techniques	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Orthogonal designs; Unification theory; Algorithms; Grobner bases	HADAMARD-MATRICES	In the past few decades, design theory has grown to encompass a wide variety of research directions. It comes as no surprise that applications in coding theory and communications continue to arise, and also that designs have found applications in new areas. Computer science has provided a new source of applications of designs, and simultaneously a field of new and challenging problems in design theory. In this paper, we revisit a construction for orthogonal designs using the multiplication tables of Cayley-Dixon algebras of dimension 2(n). The desired orthogonal designs can be described by a system of equations with the aid of a Grobner basis computation. For orders greater than 16 the combinatorial explosion of the problem gives rise to equations that are unfeasible to be handled by traditional search algorithms. However, the structural properties of the designs make this problem possible to be tackled in terms of rewriting techniques, by equational unification. We establish connections between central concepts of design theory and equational unification where equivalence operations of designs point to the computation of a minimal complete set of unifiers. These connections make viable the computation of some types of orthogonal designs that have not been found before with the aforementioned algebraic modeling.																	1012-2443	1573-7470				MAR	2020	88	1-3					213	236		10.1007/s10472-018-9607-9													
J								A BRKGA-DE algorithm for parallel-batching scheduling with deterioration and learning effects on parallel machines under preventive maintenance consideration	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Scheduling; Deterioration; Learning; Parallel-batching; BRKGA-DE	PROCESSING-TIMES; SINGLE; JOBS; MAKESPAN; MINIMIZE	This paper introduces a parallel-batching scheduling problem with deterioration and learning effects on parallel machines, where the actual processing time of a job is subject to the phenomena of deterioration and learning. All jobs are first divided into different parallel batches, and the processing time of the batches is equal to the largest processing time of their belonged jobs. Then, the generated batches are assigned to parallel machines to be processed. Motivated by the characteristics of machine maintenance activities in a semiconductor manufacturing process, we take the machine preventive maintenance into account, i.e., the machine should be maintained after a fixed number of batches have been completed. In order to solve the problem, we analyze several structural properties with respect to the batch formation and sequencing. Based on these properties, a hybrid BRKGA-DE algorithm combining biased random-key genetic algorithm (BRKGA) and Differential Evolution (DE) is proposed to solve the parallel-batching scheduling problem. A series of computational experiments is conducted to demonstrate the effectiveness and efficiency of the proposed algorithm.																	1012-2443	1573-7470				MAR	2020	88	1-3					237	267		10.1007/s10472-018-9602-1													
J								Feature uncertainty bounds for explicit feature maps and large robust nonlinear SVM classifiers	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Robust classification; Random Fourier features; Nystrom method; Robust optimization; Support vector machines; Machine learning	NYSTROM; CLASSIFICATION; OPTIMIZATION	We consider the binary classification problem when data are large and subject to unknown but bounded uncertainties. We address the problem by formulating the nonlinear support vector machine training problem with robust optimization. To do so, we analyze and propose two bounding schemes for uncertainties associated to random approximate features in low dimensional spaces. The proposed bound calculations are based on Random Fourier Features and the Nystrom methods. Numerical experiments are conducted to illustrate the benefit of the technique. We also emphasize the decomposable structure of the proposed robust nonlinear formulation that allows the use of efficient stochastic approximation techniques when datasets are large.																	1012-2443	1573-7470				MAR	2020	88	1-3					269	289		10.1007/s10472-019-09676-0													
J								Outcome prediction for salivary gland cancer using multivariate adaptative regression splines (MARS) and self-organizing maps (SOM)	NEURAL COMPUTING & APPLICATIONS										Salivary gland cancer; Prognosis; Data mining; Multivariate adaptative regression splines (MARS)	ADENOID CYSTIC CARCINOMA; INCREASED EXPRESSION; PROGNOSTIC-FACTORS; NEURAL-NETWORKS; SURVIVAL; MUC1	Over the last decades, advances in diagnosis and tissue microsurgical reconstruction of soft tissues have modified the therapeutic approach to salivary gland cancers, but long-term survival rates have increased only marginally. Due to the relatively low frequency of these tumors together with their diverse histopathological types, it is not easy to perform a prognosis assessment. Multivariate adaptative regression splines (MARS) is a data mining technique with a well-known ability to describe a response starting from a large number of predictors. In this work, MARS was used for determining the prognosis of cancers of salivary glands using clinical and histological variables, as well as molecular markers. Here, we have generated four different models combining different sets of variables, with sensitivities and specificities ranging from 95.45 to 100%. Specifically, one of these models which combined five clinical variables (Tumor size -T-, neck node metastasis -N-, distant metastasis -M-, age and number of tumor recurrences) plus one molecular factor (gelatinase B -MMP-9-) showed a sensitivity and a specificity of 100%. Therefore, the MARS model was applied to the modeling of the influence of several clinical and molecular variables on the prognosis of salivary gland cancers with success. A self-organizing map (SOM) is a type of neural network what was used here to determine a prognostic model composed for four variables: N, M, number of recurrences and tumor type. The sensitivity of this model was that of 97%, and its specificity was that of 94.7%.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1219	1229		10.1007/s00521-018-3473-6													
J								A methodology for detecting relevant single nucleotide polymorphism in prostate cancer with multivariate adaptive regression splines and backpropagation artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Multivariate adaptive regression splines; Prostate cancer; Single nucleotide polymorphism	EXPERIMENTAL CYANOBACTERIA CONCENTRATIONS; MULTILAYER PERCEPTRON NETWORKS; CYANOTOXINS PRESENCE; RISK; SUPPORT; MARS; PREVENTION; EVALUATE; OBESITY; MODEL	The objective of the present paper is to model the genetic influence on prostate cancer with multivariate adaptive regression splines (MARS) and artificial neural networks (ANNs) techniques for classification. These models will be able to classify subjects that have cancer according to the values of the selected proteins from the genes selected with the models as most relevant. Subjects are selected as cases and controls from the MCC-Spain database and represent a heterogeneous group. Multivariate adaptive regression splines models allow to select a set of the most relevant proteins from the database. These models were trained in nine different degrees and chosen regarding its performance and complexity. Artificial neural networks models were trained on with data restricted to the most significant variables. The performance of both types of models was analyzed in terms of the area under the curve of the receiver operating characteristics curve. The ANN technique resulted in a model with AUC of 0.62006, while for MARS technique, the value was 0.569312 in the best situation. Then, the artificial neural network model obtained can determine whether a patient suffers prostate cancer significantly better than MARS models and with high rate of success. The best model presented was based on support vector machines, reaching values of AUC of 0.65212.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1231	1238		10.1007/s00521-018-3503-4													
J								Genetic algorithm based on support vector machines for computer vision syndrome classification in health personnel	NEURAL COMPUTING & APPLICATIONS										Support vector machines; Genetic algorithms; Computer vision syndrome; Health personnel	ARTIFICIAL NEURAL-NETWORKS; SURFACE DISEASE INDEX; OCULAR-SURFACE; RISK-FACTORS; SHIFT WORK; MODEL; DISCOMFORT; MARKERS; DESIGN; USERS	The inclusion in workplaces of video display terminals has brought multiple benefits for the organization of work. Nevertheless, it also implies a series of risks for the health of the workers, since it can cause ocular and visual disorders, among others. In this research, a group of eye and vision-related problems associated with prolonged computer use (known as computer vision syndrome) are studied. The aim is to select the characteristics of the subject that are most relevant for the occurrence of this syndrome, and then, to develop a classification model for its prediction. The estimate of this problem is made by means of support vector machines for classification. This machine learning technique will be trained with the support of a genetic algorithm. This provides the training of the support vector machine with different patterns of parameters, improving its performance. The model performance is verified in terms of the area under the ROC curve, which leads to a model with high accuracy in the classification of the syndrome.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1239	1248		10.1007/s00521-018-3581-3													
J								Hybrid model for the ANI index prediction using Remifentanil drug and EMG signal	NEURAL COMPUTING & APPLICATIONS										ElectroMyoGram signal (EMG); Analgesia Nociception Index (ANI); Multi-layer perceptron (MLP); Support vector regression (SVR)	CONTROLLER; ANESTHESIA; PROPOFOL; PHARMACOKINETICS; SYSTEMS	With the aim to control and reduce the pain of patients during a surgery with general anesthesia, one of the main challenges is the proposal of safe an optimal and efficient methods of drugs administering. First step to achieve this goal is the proposal and development of right indexes that correlate satisfactory with analgesia. One of this index gives the most hopeful results is the Analgesia Nociception Index (ANI). The present research work deals the ANI response of patients during surgeries with general anesthesia with intravenous drug infusion. The main aim is to predict the ANI signal behavior regarding of the analgesic infusion rate. To do that, a hybrid intelligent model is developed, using clustering and regression techniques based on artificial neural networks and support vector regression. The proposal was validated with a dataset of surgeries real cases of patients undergoing general anesthesia. The achieved results attest for the potential of the proposed technique.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1249	1258		10.1007/s00521-018-3605-z													
J								An artificial neural network model for the prediction of bruxism by means of occlusal variables	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Bruxism; Clenching patient; Grinding patient	SYSTEM; AGREEMENT; DIAGNOSIS; SYMPTOMS; SPLINTS; BRUXERS; PAIN	Patients suspected of bruxism represent a very heterogeneous group. Some require immediate treatment while others, with only minor disorders, may not need treatment at all. In this work, artificial neural network ensembles models were trained on with data from 325 bruxist patients examined at the Department of Prosthodontics and Occlusion (Craniomandibular Dysfunction Unit) of Oviedo University. The information retrieved from each patient included some occlusal variables and other information such as their gender and age. The aim of the model is to classify individuals suffering from bruxism in clenching and grinding patients. The models were analyzed using receiver operating characteristics curve analysis, calibration assessments, inter- and intra-method variations. Effective odds ratios for the artificial neural network ensembles were compared. The model resulted in an area under the receiver operation characteristics curve of 86%. At 95% sensitivity, the specificity was 84.1%, for the existence of 43.5% of bruxists clenching patients in the population of this study. This population corresponds to a grinding patients' best predictive value of 97.2% and a clenching patients' best predictive value of 89.5% both using the bagging method. The artificial neural network model obtained can distinguish between clenching and grinding patients requiring the analysis of a few variables and with a high rate of success.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1259	1267		10.1007/s00521-018-3715-7													
J								Sparse representation and overcomplete dictionary learning for anomaly detection in electrocardiograms	NEURAL COMPUTING & APPLICATIONS										Electrocardiogram; Sparse representation; Dictionary learning; Electrocardiographic signal analysis; Anomaly detection; MIT-BIH Arrhythmia Database	ECG; CLASSIFICATION; ALGORITHM; DECOMPOSITION	In the hereby work, we present the use of sparse representation and overcomplete dictionary learning method for examining the case of anomaly detection in an electrocardiographic record. The above mentioned signal was introduced in a form of correct electrocardiographic morphological structures and outliers which describe different sorts of disorders. In the course of study, two sorts of dictionaries were used. The first consists of atoms created with the use of differently parameterized analytic Gabor functions. The second sort of dictionaries uses the modified Method of Optimal Directions to find a dictionary reflecting proper structures of an electrocardiographic signal. In addition, in this approach, the condition of decorrelation of dictionary atoms was introduced for the sake of gaining more precise and optimal representation. The dictionaries obtained in these two ways became a basis for the analyzed sparse representation of electrocardiographic record. During the anomaly detection process, which was based on decomposition of the analyzed signal into correct values and outliers, a modified alternating minimization algorithm was used. A commonly accessible base of data of electrocardiograms, that is MIT-BIH Arrhythmia Database, was utilized to examine the conduct of the recommended method. The effectiveness of the solution, which validated itself in searching of anomalies in the analyzed electrocardiographic record, was confirmed by experiment results.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1269	1285		10.1007/s00521-018-3814-5													
J								Design issues in Time Series dataset balancing algorithms	NEURAL COMPUTING & APPLICATIONS										Imbalanced Time Series; Correlation measures; Human activity recognition; Epilepsy onset recognition; Fall detection	IMBALANCED DATA; DATA-SETS; CLASSIFICATION; IDENTIFICATION; ACCELEROMETER	Nowadays, the Internet of Things and the e-Health are producing huge collections of Time Series that are analyzed in order to classify current status or to detect certain events, among others. In two-class problems, when the positive events to detect are infrequent, the gathered data lack balance. Even in unsupervised learning, this imbalance causes models to decrease their generalization capability. In order to solve such problem, Time Series balancing algorithms have been proposed. Time Series balancing algorithms have barely been studied; the different approaches make use of either a single bag of Time Series extracting some of them in order to generate a synthetic new one or ghost points in the distance space. These solutions are suitable when there is one only data source and they are univariate datasets. However, in the context of the Internet of Things, where multiple data sources are available, these approaches may not perform coherently. Besides, up to our knowledge there is not multiple datasources and multivariate TS balancing algorithms in the literature. In this research, we study two main concerns that should be considered when designing balancing Time Series algorithms: on the one hand, the TS balancing algorithms should deal with multiple multivariate data sources; on the other hand, the balancing algorithms should be shape preserving. A new algorithm is proposed for balancing multivariate Time Series datasets, as part of our work. A complete evaluation of the algorithm is performed dealing with two real-world multivariate Time Series datasets coming from the e-Health domain: one about epilepsy crisis identification and the other on fall detection. A thorough analysis of the performance is discussed, showing the advantages of considering the Time Series issues within the balancing algorithm.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1287	1304		10.1007/s00521-019-04011-4													
J								Estimation of hydrogen flow rate in atmospheric Ar:H-2 plasma by using artificial neural network	NEURAL COMPUTING & APPLICATIONS										Artificial neural network; Ar; H-2 plasma; Hydrogen flow rate; Emission lines; Estimation	SMELTING REDUCTION; GENETIC ALGORITHM; PREDICTION; IRON; TEMPERATURE; ORES	Atmospheric Ar:H-2 plasma is an eco-friendly option for the reduction of metal oxides. For better reduction performance and safety concern, the hydrogen gas injected into the reactor should be monitored. A hydrogen flow rate estimation system is presented in this paper by using an artificial neural network (ANN) model fed with features of optical emission spectra of the plasma. ANN models are studied with two different sets of input, i.e. for the first case the inputs to the model are the three features of H-alpha line such as the peak intensity count, full-width half maximum and area under H-alpha line, while for the second case, the peak intensity count of a group of emission lines like H-alpha, Ar I, O I, K I, Na D lines are considered as the inputs. ANN model is developed for estimating four different sets of hydrogen flow rates 5, 8, 10 and 12 litres per minute (lpm) when the argon flow rate is constant at 10 lpm. For both the input features, the model performances are compared, and it is shown that improved estimation accuracy is observed from the second case, i.e. from peak intensity count of a group of emission lines instead of only hydrogen emission line.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1357	1365		10.1007/s00521-018-3674-z													
J								A classifier of matrix modular neural network to simplify complex classification tasks	NEURAL COMPUTING & APPLICATIONS										Modular neural networks; Task decomposition; Computational complexity; Generalization capability	MODEL	This paper proposes the matrix modular neural network (MMNN), which is a modular neural network and adopts a novel task decomposition technique to solve complex problems, such as the large training sets and the category asymmetric training sets. A complex problem can be decomposed into many easier problems, each of which is dealt in two subspaces and can be solved by a single neural network module. All of these modules form a neural network matrix, which produces an output matrix that leads to an integration machine so that finally a classification decision result can be efficiently made. This paper's theoretic analyses and experiments show that the MMNN can reduce the learning time and improve the generalization capability and the classification accuracy of neural networks.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1367	1377		10.1007/s00521-018-3631-x													
J								SVM-based robust image watermarking technique in LWT domain using different sub-bands	NEURAL COMPUTING & APPLICATIONS										Image watermarking; Lifting wavelet transform (LWT); Support vector machine (SVM); Watermarking attacks	DISCRETE WAVELET TRANSFORM; SCHEME; ALGORITHM; SELECTION	In this paper, a robust image watermarking system in lifting wavelet transform domain using different sub-bands has been proposed. SVM classifier is used during watermark extraction to obtain improved robustness under diverse attack conditions. In this work, a detailed analysis of imperceptibility and robustness performance with the use of different sub-bands has been presented. The performance on different sub-band has been analyzed so as to maximize the robustness against different attacks keeping imperceptibility at adequate level. Robustness is observed against various attacks such as noising attacks, denoising attacks, image processing attacks, lossy compression attacks and geometric attacks. It is seen that high-frequency sub-band provides better invisibility, whereas variation of robustness performance on different sub-bands depend on the type of attacks. It is observed from the performance analysis that all the attacks do not have exactly same effect on the frequency content of the image. For instance, noising attack affects every frequency component of the image almost equally, whereas the embedding in high-frequency band makes the system fragile to lossy compression attack. The algorithm is tested on a large image database to observe the variation in the performance of the system. Comparative analysis suggests that the proposed sub-band provides improved performance over some benchmark methods in most of the cases.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1379	1403		10.1007/s00521-018-3647-2													
J								Open-set single-sample face recognition in video surveillance using fuzzy ARTMAP	NEURAL COMPUTING & APPLICATIONS										Open-set single-sample face recognition; Video surveillance; Fuzzy ARTMAP; Identity of interest (IoI)	IMAGE	Single-sample face recognition has been investigated by a few researches over the past few decades. However, due to the demand of identity of interest searching from video surveillance in recent years, this system has been expanded to open-set face recognition (OSFR) scheme. The OSFR system provides the identity of registered subjects and rejects the unregistered ones using only single-sample reference. This is important in video surveillance applications in which both database members and non-members are expected to appear in the scene. In this paper, we propose to use fuzzy ARTMAP neural network to solve the problem of open-set single-sample face recognition in real-world video surveillance scenario. Our proposed approach can recognize faces in near-frontal views under various illumination and facial expression conditions. Facial features are extracted using histograms of oriented gradients and Gabor wavelets and then fused using canonical correlation analysis to yield feature vectors that are robust against the aforementioned conditions. The fuzzy ARTMAP classifier has been trained using only single sample per person. We have conducted experiments on three challenging benchmark datasets, namely AR, FRGC, and ChokePoint. The experimental results have shown that the proposed approach has a superior performance than the state-of-the-art approaches.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1405	1412		10.1007/s00521-018-3649-0													
J								An improved evolutionary approach-based hybrid algorithm for Bayesian network structure learning in dynamic constrained search space	NEURAL COMPUTING & APPLICATIONS										Bayesian networks; Structure learning; Mutual information; Genetic algorithm	GENETIC ALGORITHMS	Learning Bayesian network (BN) structures from data is a NP-hard problem due to the vastness of the solution space. To address this issue, hybrid approaches that integrate the constraint-based (CB) method and the score-and-search (SS) method have been developed in the literature, but when the constrained search space is fixed and inaccurate, it is very likely to lose the optimal solution, leading to low learning accuracy. Besides, due to the randomness and uncertainty of the search, it is difficult to preserve the superiority of the structures, resulting in low learning efficiency. Therefore, we propose a novel hybrid algorithm based on an improved evolutionary approach to explore BN structure with highest matching degree of data set in dynamic constrained search space. The proposed algorithm involves two phases, namely the CB phase and the SS phase. In the CB phase, the mutual information is utilized as the restriction to limit the search space, and a binding parameter is introduced to the novel encoding scheme so that the search space can be dynamically changed in the evolutionary process. In the SS phase, a new operator is developed to pass on the excellent genes from generation to generation, and an update principle for the binding parameter is exploited for the dynamic selection of the search space. We conduct the comparative experiments on the benchmark network data sets and provide performance and applicability analysis of our proposed method. The experimental results show that the new algorithm is effective in learning the BN structures.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1413	1434		10.1007/s00521-018-3650-7													
J								(mu,nu)\documentclass[12pt]-Pseudo-almost automorphic solutions for high-order Hopfield bidirectional associative memory neural networks	NEURAL COMPUTING & APPLICATIONS										<mml; math><mml; mrow><mml; mo stretchy="false">(</mml; mo><mml; mi>mu</mml; mi><mml; mo>; </mml; mo><mml; mi>nu</mml; mi><mml; mo stretchy="false">)</mml; mo></mml; mrow></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$(; mu; nu )$$; end{document}<inline-graphic xlink; href="521_2018_3651_Article_IEq4; gif"; >-Pseudo-almost automorphic function; High-order BAM neural networks; Global exponential stability	ALMOST-PERIODIC-SOLUTION; EXPONENTIAL STABILITY; DIFFERENTIAL-EQUATIONS; BAM NETWORKS; MU-PSEUDO; EXISTENCE; COEFFICIENTS; DELAY	This article is concerned with a high-order Hopfield bidirectional associative memory neural networks with time-varying coefficients and mixed delays. Sufficient conditions are derived for the existence, the uniqueness and the exponential stability of (mu,nu)\documentclass[12pt]pseudo-almost automorphic solutions of the considered model. Banach fixed-point theorem is applied for the existence and the uniqueness results. Global exponential stability is derived via differential inequalities. Finally, two examples are provided to support the feasibility of the theoretical results.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1435	1456		10.1007/s00521-018-3651-6													
J								Short-term electricity price forecasting and classification in smart grids using optimized multikernel extreme learning machine	NEURAL COMPUTING & APPLICATIONS										Electricity price forecasting and classification; Extreme learning machine; Kernel extreme learning machine; Kernel functions; Price thresholds; Mutated water cycle algorithm	WATER CYCLE ALGORITHM; NEURAL-NETWORK; WAVELET TRANSFORM; MODEL; REGRESSION; VECTOR	Short-term electricity price forecasting in deregulated electricity markets has been studied extensively in recent years but without significant reduction in price forecasting errors. Also demand-side management and short-term scheduling operations in smart grids do not require strictly very accurate forecast and can be executed with certain practical price thresholds. This paper, therefore, presents a multikernel extreme learning machine (MKELM) for both short-term electricity price forecasting and classification according to some prespecified price thresholds. The kernel ELM does not require the hidden layer mapping function to be known and produces robust prediction and classification in comparison with the conventional ELM using random weights between the input and hidden layers. Further in the MKELM formulation, the linear combination of the weighted kernels is optimized using vaporization precipitation-based water cycle algorithm (WCA) to produce significantly accurate electricity price prediction and classification. The combination of MKELM and WCA is named as WCA-MKELM in this work. To validate the effectiveness of the proposed approach, three electricity markets, namely PJM, Ontario and New South Wales, are considered for electricity price forecasting and classification producing fairly accurate results.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1457	1480		10.1007/s00521-018-3652-5													
J								A database-driven neural computing framework for classification of vertical jump patterns of healthy female netballers using 3D kinematics-EMG features	NEURAL COMPUTING & APPLICATIONS										Vertical Jump tests; Female netballers; Database-driven; 3D kinematics; Electromyography; Pattern classification; ANN; SVM	NETWORK; SYSTEM; MODEL	Classification of athletes' performance in vertical jump (VJ) tests is a recommended practice at various stages of athlete fitness development and performance enhancement. The practice is, however, currently conducted subjectively and mainly based on measures of the vertical jump height (VJH) attained in standardized VJ tests. The current study presents an intelligent integrated classification framework (IICF) for classification of athlete performance in single-leg (SL) and double-leg (DL) standing VJ tests based on lower extremity (LE) biomechanical data. Biomechanical data consisted of three-dimensional (3D) kinematics and electromyography (EMG) features generated from ankle-knee joints and eight LE muscles of 13 healthy female national netball players (subjects) obtained during six trials of SL-left leg (SLLL), SL-right leg (SLRL), and DL VJ tests. Each participating subject had prior VJ classification by the trainer as either excellent or very good in each of the three VJ tests. IICF introduced in this work utilizes an integration of the scalable and interoperable relational database management system (MySQLDB) and artificial neural networks (ANNs). Integrated pattern sets containing the extracted features (EF) obtained from first three VJ trials data were randomly partitioned into design and test data sets and ANN implemented using fully connected multilayer perceptron feedforward neural networks (MLP-FFNN) with three different training algorithms. Subjects' prior VJ classifications were used as MLP-FFNN target outputs. A second classifier is trained using support vector machine (SVM) using three different kernel mapping functions and performances between MPL-FFNN and SVM classifiers compared on both training and independent test pattern sets. The test pattern sets comprise of EF generated from the latter three VJ trials data stored in MySQLDB. The average classification accuracy (F-measure) achieved by the optimally trained MLP-FFNN classifier on independent test pattern sets across all the three VJ activities was 93.33% (86.67-96.77%), whereas SVM classifier's was 82.5% (73.33-87.5%). Through a custom-made web-based interface having backend integration of MySQLDB and the optimally trained MLP-FFNN, classification of individual subjects' pattern sets from either VJ activities is also enabled. Individual subjects' classification results can then be compared with prior classifications by the trainer, and differences were highlighted. IICF introduced in this work has demonstrated its feasibility as an objective complementary assessment tool for trainers during conducting of SL and DL vertical jump tests for netball players and athletes in general. Descriptive statistics for the entire experimental data sets' features used in this study are also presented.																	0941-0643	1433-3058				MAR	2020	32	5			SI		1481	1500		10.1007/s00521-018-3653-4													
J								The role of respiration audio in multimodal analysis of movement qualities	JOURNAL ON MULTIMODAL USER INTERFACES										Movement expressive qualities; Respiration; Intrapersonal synchronization	SOUNDS; RECOGNITION; EMOTIONS; TRACKING; PEOPLE	In this paper, we explore how the audio respiration signal can contribute to multimodal analysis of movement qualities. Within this aim, we propose two novel techniques which use the audio respiration signal captured by a standard microphone placed near to mouth and supervised machine learning algorithms. The first approach consists of the classification of a set of acoustic features extracted from exhalations of a person performing fluid or fragmented movements. In the second approach, the intrapersonal synchronization between the respiration and kinetic energy of body movements is used to distinguish the same qualities. First, the value of synchronization between modalities is computed using the Event Synchronization algorithm. Next, a set of features, computed from the value of synchronization, is used as an input to machine learning algorithms. Both approaches were applied to the multimodal corpus composed of short performances by three professionals performing fluid and fragmented movements. The total duration of the corpus is about 17 min. The highest F-score (0.87) for the first approach was obtained for the binary classification task using Support Vector Machines (SVM-LP). The best result for the same task using the second approach was obtained using Naive Bayes algorithm (F-score of 0.72). The results confirm that it is possible to infer information about the movement qualities from respiration audio.																	1783-7677	1783-8738				MAR	2020	14	1					1	15		10.1007/s12193-019-00302-1													
J								Multi-modal facial expression feature based on deep-neural networks	JOURNAL ON MULTIMODAL USER INTERFACES										Emotion recognition; Multi-modal feature; Convolutional neural networks; Support vector machine	RECOGNITION; MODEL	Emotion recognition based on facial expression is a challenging research topic and has attracted a great deal of attention in the past few years. This paper presents a novel method, utilizing multi-modal strategy to extract emotion features from facial expression images. The basic idea is to combine the low-level empirical feature and the high-level self-learning feature into a multi-modal feature. The 2-dimensional coordinate of facial key points are extracted as low-level empirical feature and the high-level self-learning feature are extracted by the Convolutional Neural Networks (CNNs). To reduce the number of free parameters of CNNs, small filters are utilized for all convolutional layers. Owing to multiple small filters are equivalent of a large filter, which can reduce the number of parameters to learn effectively. And label-preserving transformation is used to enlarge the dataset artificially, in order to address the over-fitting and data imbalance of deep neural networks. Then, two kinds of modal features are fused linearly to form the facial expression feature. Extensive experiments are evaluated on the extended Cohn-Kanade (CK+) Dataset. For comparison, three kinds of feature vectors are adopted: low-level facial key point feature vector, high-level self-learning feature vector and multi-modal feature vector. The experiment results show that the multi-modal strategy can achieve encouraging recognition results compared to the single modal strategy.																	1783-7677	1783-8738				MAR	2020	14	1					17	23		10.1007/s12193-019-00308-9													
J								Gaze-based interactions in the cockpit of the future: a survey	JOURNAL ON MULTIMODAL USER INTERFACES										Eye tracking; Gaze-based interactions; Aviation; Survey	EYE-TRACKING; FLIGHT DECKS; PERFORMANCE; STRATEGIES; EXPERTISE	Flying an aircraft is a mentally demanding task where pilots must process a vast amount of visual, auditory and vestibular information. They have to control the aircraft by pulling, pushing and turning different knobs and levers, while knowing that mistakes in doing so can have fatal outcomes. Therefore, attempts to improve and optimize these interactions should not increase pilots' mental workload. By utilizing pilots' visual attention, gaze-based interactions provide an unobtrusive solution to this. This research is the first to actively involve pilots in the exploration of gaze-based interactions in the cockpit. By distributing a survey among 20 active commercial aviation pilots working for an internationally operating airline, the paper investigates pilots' perception and needs concerning gaze-based interactions. The results build the foundation for future research, because they not only reflect pilots' attitudes towards this novel technology, but also provide an overview of situations in which pilots need gaze-based interactions.																	1783-7677	1783-8738				MAR	2020	14	1					25	48		10.1007/s12193-019-00309-8													
J								Elderly users' acceptance of mHealth user interface (UI) design-based culture: the moderator role of age	JOURNAL ON MULTIMODAL USER INTERFACES										Technology acceptance; Elderly users; Mobile health applications; User interface; TAM; Culture; UI; UX	BEHAVIORAL INTENTION; OLDER-ADULTS; INTERNET USE; TECHNOLOGY; MODEL; INFORMATION; DETERMINANTS; EXTENSION; GENDER; GOVERNMENT	In the Arab world, mobile health (mHealth) applications are an effective way to provide health benefits to medically needy in the absence of health services. However, end users around the world use technology to perform tasks in a way that appears more natural, and closer to their cultural and personal preferences. Evidence from prior studies shows that culture is a vital factor in the success of a system or product. In view of this fact, this study investigated elderly Arab users' acceptance of mHealth User Interface (UI) design-based culture. The TAM model was used to shape the theoretical foundation for this study with a questionnaire as data gathering tool from 81 participants. The findings showed that perceived ease of use and attitude towards use had a significant positive influence on users' behavioral intention to use mHealth UI design-based culture. The impact of age on the relationship between ease of use, usefulness, and intention was significant. Overall, the findings showed that elderly Arab users found the UI design of mHealth acceptable due to its cultural significance. To enhance the design of mobile UI targeting elderly users, it is important to consider the cultural rules and their behavioral applications.																	1783-7677	1783-8738				MAR	2020	14	1					49	59		10.1007/s12193-019-00307-w													
J								Are older people any different from younger people in the way they want to interact with robots? Scenario based survey	JOURNAL ON MULTIMODAL USER INTERFACES										Older people; Human-robot interaction; Communication preferences; User-centred design	LANGUAGE; HEALTH; FOCUS	Numerous projects, normally run by younger people, are exploring robot use by older people. But are older any different from younger people in the way they want to interact with robots? Understanding older compared to younger people's preferences will give researchers more insight into good design. We compared views on multi-modal human-robot interfaces, of older people living independently, with students and university staff. We showed 96 participants aged under 65 and 18 aged 65 + , six videos presenting different scenarios, including interfaces both working properly and failing, for an older man interacting with a robot by speech and touch screen tablet. Participants were asked about the interfaces they might use and why, using self-completed questionnaires with mainly open-ended questions. People over 65 were more like people under 21 than those aged 22-64 (78%, 67%, 47% respectively) in preferring speech over tablet for robot-human interaction. But reasons for doing so may differ, for example, hearing and eyesight impairment versus speaking while hands full. Older participants were more likely (83% vs. 55%) to want a robot in the house than those under 65. Older people were as familiar with tablets and smart speakers as younger people, but less likely to use smart phones. Some younger people suggested interacting with robot via their smart phone, and while not at home. Answers to similar questions about preferences for robot interaction varied according to position in the questionnaire. User-centred design of human-robot interfaces should include open questions to understand people's preferences, should account for question wording and order in interpreting user preferences, and should include people of all age ranges to better understand interface use. Older people's technology needs have differences and similarities to the younger people who are likely carrying out the research. Our sample of older people were more like people under 21 than those aged in between for preference of robot-human interaction, and more willing to have a robot in the home than younger people. Differences may come from a more home based lifestyle and difficulties with vision, hearing, or dexterity rather than lack of interest in technology.																	1783-7677	1783-8738				MAR	2020	14	1					61	72		10.1007/s12193-019-00306-x													
J								Comparison of spatial and temporal interaction techniques for 3D audio trajectory authoring	JOURNAL ON MULTIMODAL USER INTERFACES										3DUI; 3D Audio; Human-computer interaction	CONFIDENCE-INTERVALS	With the popularity of immersive media, developing usable tools for content development is important for the production process. In the context of 3D audio production, user interfaces for authoring and editing 3D audio trajectories enable content developers, composers, practitioners, and recording and mixing engineers to define how audio sources travel in time. However, common interaction techniques in 3D audio production tools can make the workflow of this task tedious and difficult to accomplish. This study investigates this problem by classifying the atomic tasks (spatially and temporally) of a general composite task of authoring 3D audio trajectories and then evaluating different interaction techniques across these tasks. Common graphical user interfaces were compared with input devices having varying degrees-of-freedom for spatial atomic tasks in order to investigate the effect of direct manipulation and integrality of interaction techniques. Continuous and discrete interaction techniques were compared for temporal tasks in order to investigate the effect of direct manipulation. Results suggest that interaction techniques with high degrees of integrality and direct manipulation reduce task completion time compared to standard GUI techniques. The design of temporal tasks can create a visual bias, and discrete-time controls can be a suitable method for traversing a small number of control points. These results and further observations provide directions on the study of interaction technique design for 3D audio tools, which in turn should improve workflows of 3D audio content creation.																	1783-7677	1783-8738				MAR	2020	14	1					83	100		10.1007/s12193-019-00314-x													
J								A comparative assessment of Wi-Fi and acoustic signal-based HCI methods on the practicality	JOURNAL ON MULTIMODAL USER INTERFACES										Motion recognition; Wi-Fi signal; Channel state information; Acoustic signal; Practicality assessment; Human-computer interaction (HCI)		Wi-Fi and acoustic signal-based human-computer interaction (HCI) methods have received growing attention in academia. However, there still are issues to be addressed despite their flourishing. In this work, we evaluate the practicality of the state-of-the-art signal-based HCI research in terms of the following six aspects-granularity, robustness, usability, efficiency, stability, and deployability. The paper presents our analysis results, observations and prospective research directions. We believe that this work will serve as a standard for future signal-based HCI research for assessing the practicality of newly developed methods.																	1783-7677	1783-8738				MAR	2020	14	1					123	137		10.1007/s12193-019-00315-w													
J								Finite-time and fixed-time anti-synchronization of Markovian neural networks with stochastic disturbances via switching control	NEURAL NETWORKS										Fixed-time synchronization; Finite-time synchronization; Markovian neural networks; Switching control; Stochastic disturbance	LAG SYNCHRONIZATION; DYNAMICAL NETWORKS; COMPLEX NETWORKS; STABILIZATION; PERTURBATION	This paper proposes a unified theoretical framework to study the problem of finite/fixed-time drive-response anti-synchronization for a class of Markovian stochastic neural networks. State feedback switching controllers without the sign function are designed to achieve the finite/fixed-time anti-synchronization of the addressed systems. Compared with the existing synchronization criteria, our results indicate that the controllers via the switching control without the sign function are given with less conservativeness, and the controllers without any sign function can deal with the chattering problem. By employing Lyapunov functional method and properties of the Weiner process, several finite/fixed-time synchronization criteria are presented and the corresponding settling times are calculated as well. Finally, three numerical examples are provided to illustrate the effectiveness of the theoretical results. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						1	11		10.1016/j.neunet.2019.11.012													
J								Neonatal seizure detection from raw multi-channel EEG using a fully convolutional architecture	NEURAL NETWORKS										Convolutional neural networks; EEG; Neonatal seizure detection; Weak labels	NETWORKS; MODEL	A deep learning classifier for detecting seizures in neonates is proposed. This architecture is designed to detect seizure events from raw electroencephalogram (EEG) signals as opposed to the state-of-the-art hand engineered feature-based representation employed in traditional machine learning based solutions. The seizure detection system utilises only convolutional layers in order to process the multichannel time domain signal and is designed to exploit the large amount of weakly labelled data in the training stage. The system performance is assessed on a large database of continuous EEG recordings of 834h in duration; this is further validated on a held-out publicly available dataset and compared with two baseline SVM based systems. The developed system achieves a 56% relative improvement with respect to a feature-based state-of-the art baseline, reaching an AUC of 98.5%; this also compares favourably both in terms of performance and run-time. The effect of varying architectural parameters is thoroughly studied. The performance improvement is achieved through novel architecture design which allows more efficient usage of available training data and end-to-end optimisation from the front-end feature extraction to the back-end classification. The proposed architecture opens new avenues for the application of deep learning to neonatal EEG, where the performance becomes a function of the amount of training data with less dependency on the availability of precise clinical labels. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						12	25		10.1016/j.neunet.2019.11.023													
J								Multiple Partial Empirical Kernel Learning with Instance Weighting and Boundary Fitting	NEURAL NETWORKS										Empirical Kernel Mapping; Multiple Empirical Kernel Learning; Instance weighting; Boundary fitting; Pattern recognition	REGULARIZATION; PREDICTION; REDUCTION; ALGORITHM	By dividing the original data set into several sub-sets, Multiple Partial Empirical Kernel Learning (MPEKL) constructs multiple kernel matrixes corresponding to the sub-sets, and these kernel matrixes are decomposed to provide the explicit kernel functions. Then, the instances in the original data set are mapped into multiple kernel spaces, which provide better performance than single kernel space. It is known that the instances in different locations and distributions behave differently. Therefore, this paper defines the weight of instance in accordance with the location and distribution of the instances. According to the location, the instances can be categorized into intrinsic instances, boundary instances and noise instances. Generally, the boundary instances, as well as the minority instances in the imbalanced data set, are assigned high weight. Meanwhile, a regularization term, which regulates the classification hyperplane to fit the distribution trend of the class boundary, is constructed by the boundary instances. Then, the weight of instance and the regularization term are introduced into MPEKL to form an algorithm named Multiple Partial Empirical Kernel Learning with Instance Weighting and Boundary Fitting (IBMPEKL). Experiments demonstrate the good performance of IBMPEKL and validate the effectiveness of the instance weighting and boundary fitting. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						26	37		10.1016/j.neunet.2019.11.019													
J								Simplified calcium signaling cascade for synaptic plasticity	NEURAL NETWORKS										Synaptic plasticity; Calcium signaling cascade; Back-propagating action potential boost; Synaptic competition	DEPENDENT PLASTICITY; MODEL; CALMODULIN; STDP; CALCINEURIN; PROTEIN; CORTEX; COINCIDENCE; SPECIFICITY; MECHANISM	We propose a model for synaptic plasticity based on a calcium signaling cascade. The model simplifies the full signaling pathways from a calcium influx to the phosphorylation (potentiation) and dephosphorylation (depression) of glutamate receptors that are gated by fictive Cl and C2 catalysts, respectively. This model is based on tangible chemical reactions, including fictive catalysts, for long-term plasticity rather than the conceptual theories commonplace in various models, such as preset thresholds of calcium concentration. Our simplified model successfully reproduced the experimental synaptic plasticity induced by different protocols such as (i) a synchronous pairing protocol and (ii) correlated presynaptic and postsynaptic action potentials (APs). Further, the ocular dominance plasticity (or the experimental verification of the celebrated Bienenstock-Cooper-Munro theory) was reproduced by two model synapses that compete by means of back-propagating APs (bAPs). The key to this competition is synapse-specific bAPs with reference to bAP-boosting on the physiological grounds. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						38	51		10.1016/j.neunet.2019.11.022													
J								Modeling functional resting-state brain networks through neural message passing on the human connectome	NEURAL NETWORKS										Neural message passing; Belief Propagation; Susceptibility Propagation; Brain criticality; Resting State Networks; Brain connectivity	DEFAULT NETWORK; SMALL-WORLD; CONNECTIVITY; ORGANIZATION; EEG; ANATOMY; CORTEX	In this work, we propose a natural model for information flow in the brain through a neural message-passing dynamics on a structural network of macroscopic regions, such as the human connectome (HC). In our model, each brain region is assumed to have a binary behavior (active or not), the strengths of interactions among them are encoded in the anatomical connectivity matrix defined by the HC, and the dynamics of the system is defined by the Belief Propagation (BP) algorithm, working near the critical point of the network. We show that in the absence of direct external stimuli the BP algorithm converges to a spatial map of activations that is similar to the Default Mode Network (DMN) of the brain, which has been defined from the analysis of functional MRI data. Moreover, we use Susceptibility Propagation (SP) to compute the matrix of long-range correlations between the different regions and show that the modules defined by a clustering of this matrix resemble several Resting State Networks (RSN) determined experimentally. Both results suggest that the functional DMN and RSNs can be seen as simple consequences of the anatomical structure of the brain and a neural message-passing dynamics between macroscopic regions. With the new model, we explore predictions on how functional maps change when the anatomical brain network suffers structural alterations, like in Alzheimer's disease and in lesions of the Corpus Callosum. The implications and novel interpretations suggested by the model, as well as the role of criticality, are discussed. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						52	69		10.1016/j.neunet.2019.11.014													
J								Global exponential synchronization of delayed memristive neural networks with reaction-diffusion terms	NEURAL NETWORKS										Global exponential synchronization; Delayed memristive neural networks; Reaction-diffusion terms; Pinning control technique	TIME-VARYING DELAYS; LAG SYNCHRONIZATION; ADAPTIVE SYNCHRONIZATION; STABILITY ANALYSIS; PASSIVITY ANALYSIS; PINNING CONTROL; MISMATCH; SYSTEMS; DEVICE	This paper investigates the global exponential synchronization problem of delayed memristive neural networks (MNNs) with reaction-diffusion terms. First, by utilizing the pinning control technique, two novel kinds of control methods are introduced to achieve synchronization of delayed MNNs with reaction-diffusion terms. Then, with the help of inequality techniques, pinning control technique, the drive-response concept and Lyapunov functional method, two sufficient conditions are obtained in the form of algebraic inequalities, which can be used for ensuring the exponential synchronization of the proposed delayed MNNs with reaction-diffusion terms. Moreover, the obtained results based on algebraic inequality complement and improve the previously known results. Finally, two illustrative examples are given to support the effectiveness and validity of the obtained theoretical results. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						70	81		10.1016/j.neunet.2019.11.008													
J								Learning physical properties in complex visual scenes: An intelligent machine for perceiving blood flow dynamics from static CT angiography imaging	NEURAL NETWORKS										Learning physical properties; Tree-structured RNN; LSTM; Fractional flow reserve; CT angiography	CORONARY-ARTERY-DISEASE; CAROTID-ARTERY; MYOCARDIAL-INFARCTION; RESERVE; LUMEN; HEMODYNAMICS; SIMULATION; FRAMEWORK; STENOSIS; MODEL	Humans perceive physical properties such as motion and elastic force by observing objects in visual scenes. Recent research has proven that computers are capable of inferring physical properties from camera images like humans. However, few studies perceive the physical properties in more complex environment, i.e. humans have difficulty estimating physical quantities directly from the visual observation, or encounter difficulty visualizing the physical process in mind according to their daily experiences. As an appropriate example, fractional flow reserve (FFR), which measures the blood pressure difference across the vessel stenosis, becomes an important physical quantitative value determining the likelihood of myocardial ischemia in clinical coronary intervention procedure. In this study, we propose a novel deep neural network solution (TreeVes-Net) that allows machines to perceive FFR values directly from static coronary CT angiography images. Our framework fully utilizes a tree-structured recurrent neural network (RNN) with a coronary representation encoder. The encoder captures coronary geometric information providing the blood fluid-related representation. The tree-structured RNN builds a long-distance spatial dependency of blood flow information inside the coronary tree. The experiments performed on 13000 synthetic coronary trees and 180 real coronary trees from clinical patients show that the values of the area under ROC curve (AUC) are 0.92 and 0.93 under two clinical criterions. These results can demonstrate the effectiveness of our framework and its superiority to seven FFR computation methods based on machine learning. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						82	93		10.1016/j.neunet.2019.11.017													
J								Discriminative margin-sensitive autoencoder for collective multi-view disease analysis	NEURAL NETWORKS										Multi-view learning; Latent representation learning; Bioimage classification; Semantic autoencoder; Disease analysis	PROTEIN FOLD RECOGNITION; MILD COGNITIVE IMPAIRMENT; JOINT REGRESSION; PREDICTION; CLASSIFICATION; CONVERSION; MATRIX; CORTEX; PET	Medical prediction is always collectively determined based on bioimages collected from different sources or various clinical characterizations described from multiple physiological features. Notably, learning intrinsic structures from multiple heterogeneous features is significant but challenging in multi-view disease understanding. Different from existing methods that separately deal with each single view, this paper proposes a discriminative Margin-Sensitive Autoencoder (MSAE) framework for automated Alzheimer's disease (AD) diagnosis and accurate protein fold recognition. Generally, our MSAE aims to collaboratively explore the complementary properties of multi-view bioimage features in a semantic-sensitive encoder-decoder paradigm, where the discriminative semantic space is explicitly constructed in a margin-scalable regression model. Specifically, we develop a semantic-sensitive autoencoder, where an encoder projects multi-view visual features into the common semantic-aware latent space, and a decoder is exerted as an additional constraint to reconstruct the respective visual features. In particular, the importance of different views is adaptively weighted by self-adjusting learning scheme, such that their underlying correlations and complementary characteristics across multiple views are simultaneously preserved into the latent common representations. Moreover, a flexible semantic space is formulated by a margin-scalable support vector machine to improve the discriminability of the learning model. Importantly, correntropy induced metric is exploited as a robust regularization measurement to better control outliers for effective classification. A half-quadratic minimization and alternating learning strategy are devised to optimize the resulting framework such that each subproblem exists a closed-form solution in each iterative minimization phase. Extensive experimental results performed on the Alzheimer's Disease Neuroimaging Initiative (ADNI) datasets show that our MSAE can achieve superior performances for both binary and multi-class classification in AD diagnosis, and evaluations on protein folds demonstrate that our method can achieve very encouraging performance on protein structure recognition, outperforming the state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						94	107		10.1016/j.neunet.2019.11.013													
J								A scalable multi-signal approach for the parallelization of self-organizing neural networks	NEURAL NETWORKS										Self-organizing neural networks; Surface reconstruction; GPU parallelization; Self-organizing adaptive map	WIRELESS SENSOR NETWORKS	Self-Organizing Neural Networks (SONNs) have a wide range of applications with massive computational requirements that often need to be satisfied with optimized parallel algorithms and implementations. In literature, SONN have been generally parallelized with GPU computing according to a single-signal paradigm: each GPU thread manages one or more nodes of the network and works concurrently on one input signal at the time. This paper presents two contributions. The first one is the experimental proof that the single-signal approach for SONNs is not optimal for the task, as it is intrinsically sequential at its core and thus inherently limited in its performance. The non-optimality of the single-signal paradigm is illustrated via a specific and simplified benchmark. The second contribution is the introduction of a new multi-signal paradigm for the parallelization of SONNs, whereby multiple signals are processed at once in each iteration hence allowing different GPU threads to work on different signals. The advantages of the multi-signal approach are shown through several benchmarks involving the Self-Organizing Adaptive Map (SOAM) algorithm as a basis for evaluation. Having a graph-based termination condition that depends on the features of the network being grown, the SOAM algorithm allows assessing both functional equivalence and performances of the paradigm proposed without relying on arbitrary thresholds. Nonetheless, the evaluation proposed has a broader scope since it refers to a unified framework for the GPU parallelization of a generic SONN. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						108	117		10.1016/j.neunet.2019.11.016													
J								Exploiting the stimuli encoding scheme of evolving Spiking Neural Networks for stream learning	NEURAL NETWORKS										Stream learning; Gaussian receptive fields; Population encoding; Evolving Spiking Neural Networks	ENVIRONMENTS; ERROR	Stream data processing has lately gained momentum with the arrival of new Big Data scenarios and applications dealing with continuously produced information flows. Unfortunately, traditional machine learning algorithms are not prepared to tackle the specific challenges imposed by data stream processing, such as the need for learning incrementally, limited memory and processing time requirements, and adaptation to non-stationary data, among others. To face these paradigms, Spiking Neural Networks have emerged as one of the most promising stream learning techniques, with variants such as Evolving Spiking Neural Networks capable of efficiently addressing many of these challenges. Interestingly, these networks resort to a particular population encoding scheme - Gaussian Receptive Fields - to transform the incoming stimuli into temporal spikes. The study presented in this manuscript sheds light on the predictive potential of this encoding scheme, focusing on how it can be applied as a computationally lightweight, model-agnostic preprocessing step for data stream learning. We provide informed intuition to unveil under which circumstances the aforementioned population encoding method yields effective prediction gains in data stream classification with respect to the case where no preprocessing is performed. Results obtained for a variety of stream learning models and both synthetic and real stream datasets are discussed to empirically buttress the capability of Gaussian Receptive Fields to boost the predictive performance of stream learning methods, spanning further research towards extrapolating our findings to other machine learning problems. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						118	133		10.1016/j.neunet.2019.11.021													
J								Structured pruning of recurrent neural networks through neuron selection	NEURAL NETWORKS										Feature selection; Recurrent neural networks; Learning sparse models; Model compression		Recurrent neural networks (RNNs) have recently achieved remarkable successes in a number of applications. However, the huge sizes and computational burden of these models make it difficult for their deployment on edge devices. A practically effective approach is to reduce the overall storage and computation costs of RNNs by network pruning techniques. Despite their successful applications, those pruning methods based on Lasso either produce irregular sparse patterns in weight matrices, which is not helpful in practical speedup. To address these issues, we propose a structured pruning method through neuron selection which can remove the independent neuron of RNNs. More specifically, we introduce two sets of binary random variables, which can be interpreted as gates or switches to the input neurons and the hidden neurons, respectively. We demonstrate that the corresponding optimization problem can be addressed by minimizing the L-0 norm of the weight matrix. Finally, experimental results on language modeling and machine reading comprehension tasks have indicated the advantages of the proposed method in comparison with state-of-the-art pruning competitors. In particular, nearly 20x practical speedup during inference was achieved without losing performance for the language model on the Penn TreeBank dataset, indicating the promising performance of the proposed method. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						134	141		10.1016/j.neunet.2019.11.018													
J								Dimension independent bounds for general shallow networks	NEURAL NETWORKS										Shallow and deep networks; Dimension independent bounds; Out-of-sample extension; Tractability of integration	APPROXIMATION	This paper proves an abstract theorem addressing in a unified manner two important problems in function approximation: avoiding curse of dimensionality and estimating the degree of approximation for out-of-sample extension in manifold learning. We consider an abstract (shallow) network that includes, for example, neural networks, radial basis function networks, and kernels on data defined manifolds used for function approximation in various settings. A deep network is obtained by a composition of the shallow networks according to a directed acyclic graph, representing the architecture of the deep network. In this paper, we prove dimension independent bounds for approximation by shallow networks in the very general setting of what we have called G-networks on a compact metric measure space, where the notion of dimension is defined in terms of the cardinality of maximal distinguishable sets, generalizing the notion of dimension of a cube or a manifold. Our techniques give bounds that improve without saturation with the smoothness of the kernel involved in an integral representation of the target function. In the context of manifold learning, our bounds provide estimates on the degree of approximation for an out-of-sample extension of the target function to the ambient space. One consequence of our theorem is that without the requirement of robust parameter selection, deep networks using a non-smooth activation function such as the ReLU, do not provide any significant advantage over shallow networks in terms of the degree of approximation alone. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						142	152		10.1016/j.neunet.2019.11.006													
J								Evolving artificial neural networks with feedback	NEURAL NETWORKS										Deep learning; Feedback; Transfer entropy; Convolutional neural network	DEEP; REPRESENTATIONS; REVEAL	Neural networks in the brain are dominated by sometimes more than 60% feedback connections, which most often have small synaptic weights. Different from this, little is known how to introduce feedback into artificial neural networks. Here we use transfer entropy in the feed-forward paths of deep networks to identify feedback candidates between the convolutional layers and determine their final synaptic weights using genetic programming. This adds about 70% more connections to these layers all with very small weights. Nonetheless performance improves substantially on different standard benchmark tasks and in different networks. To verify that this effect is generic we use 36000 configurations of small (2-10 hidden layer) conventional neural networks in a non-linear classification task and select the best performing feed-forward nets. Then we show that feedback reduces total entropy in these networks always leading to performance increase. This method may, thus, supplement standard techniques (e.g. error backprop) adding a new quality to network learning. (C) 2019 The Author(s). Published by Elsevier Ltd.																	0893-6080	1879-2782				MAR	2020	123						153	162		10.1016/j.neunet.2019.12.004													
J								Discriminative structure learning of sum-product networks for data stream classification	NEURAL NETWORKS										Sum-product network; Discriminative structure learning; Data stream classification		Sum-product network (SPN) is a deep probabilistic representation that allows for exact and tractable inference. There has been a trend of online SPN structure learning from massive and continuous data streams. However, online structure learning of SPNs has been introduced only for the generative settings so far. In this paper, we present an online discriminative approach for SPNs for learning both the structure and parameters. The basic idea is to keep track of informative and representative examples to capture the trend of time-changing class distributions. Specifically, by estimating the goodness of model fitting of data points and dynamically maintaining a certain amount of informative examples over time, we generate new sub-SPNs in a recursive and top-down manner. Meanwhile, an outlier-robust margin-based log-likelihood loss is applied locally to each data point and the parameters of SPN are updated continuously using most probable explanation (MPE) inference. This leads to a fast yet powerful optimization procedure and improved discrimination capability between the genuine class and rival classes. Empirical results show that the proposed approach achieves better prediction performance than the state-of-the-art online structure learner for SPNs, while promising order-ofmagnitude speedup. Comparison with state-of-the-art stream classifiers further proves the superiority of our approach. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						163	175		10.1016/j.neunet.2019.12.002													
J								A novel multi-modal machine learning based approach for automatic classification of EEG recordings in dementia	NEURAL NETWORKS										Machine learning; Continuous wavelet transform; Bispectrum; Alzheimer's disease; Mild cognitive impairment; Data fusion	MILD COGNITIVE IMPAIRMENT; ALZHEIMERS-DISEASE; DIAGNOSIS; DYNAMICS	Electroencephalographic (EEG) recordings generate an electrical map of the human brain that are useful for clinical inspection of patients and in biomedical smart Internet-of-Things (IoT) and BrainComputer Interface (BCI) applications. From a signal processing perspective, EEGs yield a nonlinear and nonstationary, multivariate representation of the underlying neural circuitry interactions. In this paper, a novel multi-modal Machine Learning (ML) based approach is proposed to integrate EEG engineered features for automatic classification of brain states. EEGs are acquired from neurological patients with Mild Cognitive Impairment (MCI) or Alzheimer's disease (AD) and the aim is to discriminate Healthy Control (HC) subjects from patients. Specifically, in order to effectively cope with nonstationarities, 19-channels EEG signals are projected into the time-frequency (TF) domain by means of the Continuous Wavelet Transform (CWT) and a set of appropriate features (denoted as CWT features) are extracted from delta, theta, alpha(1), alpha(2), beta EEG sub-bands. Furthermore, to exploit nonlinear phase-coupling information of EEG signals, higher order statistics (HOS) are extracted from the bispectrum (BiS) representation. BiS generates a second set of features (denoted as BiS features) which are also evaluated in the five EEG sub-bands. The CWT and BiS features are fed into a number of ML classifiers to perform both 2-way (AD vs. HC, AD vs. MCI, MCI vs. HC) and 3-way (AD vs. MCI vs. HC) classifications. As an experimental benchmark, a balanced EEG dataset that includes 63 AD, 63 MCI and 63 HC is analyzed. Comparative results show that when the concatenation of CWT and BiS features (denoted as multimodal (CWT+BiS) features) is used as input, the Multi-Layer Perceptron (MLP) classifier outperforms all other models, specifically, the Autoencoder (AE), Logistic Regression (LR) and Support Vector Machine (SVM). Consequently, our proposed multi-modal ML scheme can be considered a viable alternative to state-of-the-art computationally intensive deep learning approaches. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						176	190		10.1016/j.neunet.2019.12.006													
J								Minimum variance-embedded deep kernel regularized least squares method for one-class classification and its applications to biomedical data	NEURAL NETWORKS										One-class classification; Kernel learning; Outlier detection; Alzheimer's disease; Magnetic resonance imaging; Breast cancer	NOVELTY DETECTION; SUPPORT; OUTLIERS	Deep kernel learning has been well explored for multi-class classification tasks; however, relatively less work is done for one-class classification (OCC). OCC needs samples from only one class to train the model. Most recently, kernel regularized least squares (KRL) method-based deep architecture is developed for the OCC task. This paper introduces a novel extension of this method by embedding minimum variance information within this architecture. This embedding improves the generalization capability of the classifier by reducing the intra-class variance. In contrast to traditional deep learning methods, this method can effectively work with small-size datasets. We conduct a comprehensive set of experiments on 18 benchmark datasets (13 biomedical and 5 other datasets) to demonstrate the performance of the proposed classifier. We compare the results with 16 state-of-the-art one-class classifiers. Further, we also test our method for 2 real-world biomedical datasets viz.; detection of Alzheimer's disease from structural magnetic resonance imaging data and detection of breast cancer from histopathological images. Proposed method exhibits more than 5% F-1 score compared to existing state-of-the-art methods for various biomedical benchmark datasets. This makes it viable for application in biomedical fields where relatively less amount of data is available. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						191	216		10.1016/j.neunet.2019.12.001													
J								CS-MRI reconstruction based on analysis dictionary learning and manifold structure regularization	NEURAL NETWORKS										CS-MRI; Analysis dictionary learning; Correlation of patches; Manifold structure regularization	RESONANCE IMAGE-RECONSTRUCTION; THRESHOLDING ALGORITHM; SPARSE REPRESENTATION; K-SVD; DOMAIN; OPTIMIZATION	Compressed sensing (CS) significantly accelerates magnetic resonance imaging (MRI) by allowing the exact reconstruction of image from highly undersampling 1<-space data. In this process, the high sparsity obtained by the learned dictionary and exploitation of correlation among patches are essential to the reconstructed image quality. In this paper, by a use of these two aspects, we propose a novel CS-MRI model based on analysis dictionary learning and manifold structure regularization (ADMS). Furthermore, a proper tight frame constraint is used to obtain an effective overcomplete analysis dictionary with a high sparsifying capacity. The constructed manifold structure regularization nonuniformly enforces the correlation of each group formed by similar patches, which is more consistent with the diverse nonlocal similarity in realistic images. The proposed model is efficiently solved by the alternating direction method of multipliers (ADMM), in which the fast algorithm for each sub-problem is separately developed. The experimental results demonstrate that main components in the proposed method contribute to the final reconstruction performance and the effectiveness of the proposed model. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						217	233		10.1016/j.neunet.2019.12.010													
J								A counterexample regarding "New study on neural networks: The essential order of approximation"	NEURAL NETWORKS										Neural networks; Sharpness of error bounds; Rates of convergence		The paper "New study on neural networks: the essential order of approximation" by Jianjun Wang and Zongben Xu, which appeared in Neural Networks 23 (2010), deals with upper and lower estimates for the error of best approximation with sums of nearly exponential type activation functions in terms of moduli of smoothness. In particular, the presented lower bound is astonishingly good. However, the proof is incorrect and the bound is wrong. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						234	235		10.1016/j.neunet.2019.12.007													
J								New H-infinity state estimation criteria of delayed static neural networks via the Lyapunov-Krasovskii functional with negative definite terms	NEURAL NETWORKS										Static neural networks; H-infinity state estimation; Time-varying delay; Lyapunov-Krasovskii functional	TIME-VARYING DELAY; STABILITY ANALYSIS; SYSTEMS; INEQUALITY	In the estimation problem for delayed static neural networks (SNNs), constructing a proper Lyapunov-Krasovskii functional (LKF) is crucial for deriving less conservative estimation criteria. In this paper, a delay-product-type LKF with negative definite terms is proposed. Based on the third-order Bessel-Legendre (B-L) integral inequality and mixed convex combination approaches, a less conservative estimator design criterion is derived. Furthermore, the desired estimator gain matrices and the H-infinity performance index are obtained by solving a set of linear matrix inequalities (LMIs). Finally, a numerical example is given to demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						236	247		10.1016/j.neunet.2019.12.008													
J								Existence and finite-time stability of discrete fractional-order complex-valued neural networks with time delays	NEURAL NETWORKS										Existence; Discrete time; Fractional-order complex-valued neural networks; Finite-time stability; Delays	MITTAG-LEFFLER STABILITY; DYNAMICAL ANALYSIS; GLOBAL STABILITY; SYNCHRONIZATION	Without decomposing complex-valued systems into real-valued systems, the existence and finite-time stability for discrete fractional-order complex-valued neural networks with time delays are discussed in this paper. First of all, in order to obtain the main results, a new discrete Caputo fractional difference equation is proposed in complex field based on the theory of discrete fractional calculus, which generalizes the fractional-order neural networks in the real domain. Additionally, by utilizing Arzela-Ascoli's theorem, inequality scaling skills and fixed point theorem, some sufficient criteria of delay-dependent are deduced to ensure the existence and finite-time stability of solutions for proposed networks. Finally, the validity and feasibility of the derived theoretical results are indicated by two numerical examples with simulations. Furthermore, we have drawn the following facts: with the lower order, the discrete fractional-order complex-valued neural networks will achieve the finite-time stability more easily. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						248	260		10.1016/j.neunet.2019.12.012													
J								Robust face alignment by cascaded regression and de-occlusion	NEURAL NETWORKS										Partial occlusion; Deep regression; Face de-occlusion; Heatmap; Generative adversarial network	MODELS	Face alignment is a typical facial behavior analysis task in computer vision. However, the performance of face alignment is degraded greatly when the face image is partially occluded. In order to achieve better mapping between facial appearance features and shape increments, we propose a robust and occlusion-free face alignment algorithm in which a face de-occlusion module and a deep regression module are integrated into a cascaded deep generative regression model. The face de-occlusion module is a disentangled representation learning Generative Adversarial Networks (GANs) which aims to locate occlusions and recover the genuine appearance from partially occluded face image. The deep regression module can enhance facial appearance representation by utilizing the recovered faces to obtain more accurate regressors. Then, by the cascaded deep generative regression model, we recover the partially occluded face image and achieve accurate locating of landmarks gradually. It is interesting to show that the cascaded deep generative regression model can effectively locate occlusions and recover more genuine faces, which can be further used to improve the performance of face alignment. Experimental results conducted on four challenging occluded face datasets demonstrate that our method outperforms state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						261	272		10.1016/j.neunet.2019.12.009													
J								Cuneate spiking neural network learning to classify naturalistic texture stimuli under varying sensing conditions	NEURAL NETWORKS										Spiking neural network; Neurorobotics; Cuneate neurons; Primary afferents; Tactile sensing; Synaptic weight learning	TACTILE SIGNALS; ORIENTATION; NEURONS; SYSTEM; MODEL	We implemented a functional neuronal network that was able to learn and discriminate haptic features from biomimetic tactile sensor inputs using a two-layer spiking neuron model and homeostatic synaptic learning mechanism. The first order neuron model was used to emulate biological tactile afferents and the second order neuron model was used to emulate biological cuneate neurons. We have evaluated 10 naturalistic textures using a passive touch protocol, under varying sensing conditions. Tactile sensor data acquired with five textures under five sensing conditions were used for a synaptic learning process, to tune the synaptic weights between tactile afferents and cuneate neurons. Using post-learning synaptic weights, we evaluated the individual and population cuneate neuron responses by decoding across 10 stimuli, under varying sensing conditions. This resulted in a high decoding performance. We further validated the decoding performance across stimuli, irrespective of sensing velocities using a set of 25 cuneate neuron responses. This resulted in a median decoding performance of 96% across the set of cuneate neurons. Being able to learn and perform generalized discrimination across tactile stimuli, makes this functional spiking tactile system effective and suitable for further robotic applications. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						273	287		10.1016/j.neunet.2019.11.020													
J								Extreme learning machine for a new hybrid morphological/linear perceptron	NEURAL NETWORKS										Mathematical morphology; Lattice computing; Morphological neural networks; Hybrid morphological/linear perceptron; Extreme learning machine; Classification	NEURAL-NETWORKS; MATHEMATICAL MORPHOLOGY; REGRESSION; MAPPINGS; NEURONS	Morphological neural networks (MNNs) can be characterized as a class of artificial neural networks that perform an operation of mathematical morphology at every node, possibly followed by the application of an activation function. Morphological perceptrons (MPs) and (gray-scale) morphological associative memories are among the most widely known MNN models. Since their neuronal aggregation functions are not differentiable, classical methods of non-linear optimization can in principle not be directly applied in order to train these networks. The same observation holds true for hybrid morphological/linear perceptrons and other related models. Circumventing these problems of non-differentiability, this paper introduces an extreme learning machine approach for training a hybrid morphological/linear perceptron, whose morphological components were drawn from previous MP models. We apply the resulting model to a number of well-known classification problems from the literature and compare the performance of our model with the ones of several related models, including some recent MNNs and hybrid morphological/linear neural networks. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						288	298		10.1016/j.neunet.2019.12.003													
J								Liver disease screening based on densely connected deep neural networks	NEURAL NETWORKS										Dense connected; DNN; Liver disease; Liver function tests		Liver disease is an important public health problem. Liver Function Tests (LFT) is the most achievable test for liver disease diagnosis. Most liver diseases are manifested as abnormal LFT. Liver disease screening by LFT data is helpful for computer aided diagnosis. In this paper, we propose a densely connected deep neural network (DenseDNN), on 13 most commonly used LFT indicators and demographic information of subjects for liver disease screening. The algorithm was tested on a dataset of 76,914 samples (more than 100 times of data than the previous datasets). The Area Under Curve (AUC) of DenseDNN is 0.8919, that of DNN is 0.8867, that of random forest is 0.8790, and that of logistic regression is 0.7974. The performance of deep learning models are significantly better than conventional methods. As for the deep learning methods, DenseDNN shows better performance than DNN. (C) 2019 Published by Elsevier Ltd.																	0893-6080	1879-2782				MAR	2020	123						299	304		10.1016/j.neunet.2019.11.005													
J								Efficient network architecture search via multiobjective particle swarm optimization based on decomposition	NEURAL NETWORKS										Convolutional neural network; Neural architecture search; Multiobjective particle swarm optimization; Decomposition	NEURAL-NETWORKS; MOEA/D	The efforts devoted to manually increasing the width and depth of convolutional neural network (CNN) usually require a large amount of time and expertise. It has stimulated a rising demand of neural architecture search (NAS) over these years. However, most popular NAS approaches solely optimize for low prediction error without penalizing high structure complexity. To this end, this paper proposes MOPSO/D-Net, a CNN architecture search method with multiobjective particle swarm optimization based on decomposition (MOPSO/D). The main goal is to reformulate NAS as a multiobjective evolutionary optimization problem, where the optimal architecture is learned by minimizing two conflicting objectives, namely the error rate of classification and number of parameters of the network. Along with the hybrid binary encoding and adaptive penalty-based boundary intersection, an improved MOPSO/D is further proposed to solve the formulated multiobjective NAS and provide diverse tradeoff solutions. Experimental studies verify the effectiveness of MOPSO/D-Net compared with current manual and automated CNN generation methods. The proposed algorithm achieves impressive classification performance with a small number of parameters on each of two benchmark datasets, particularly, 0.4% error rate with 0.16 M params on MNIST and 5.88% error rate with 8.1 M params on CIFAR-10, respectively. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						305	316		10.1016/j.neunet.2019.12.005													
J								Finite-time nonfragile time-varying proportional retarded synchronization for Markovian Inertial Memristive NNs with reaction-diffusion items	NEURAL NETWORKS										Inertial memristive neural networks; Reaction-diffusion items; Markovian jump parameters; Nonfragile time-varying proportional retarded control; Finite-time interval	GLOBAL EXPONENTIAL STABILITY; NEURAL-NETWORKS; DISSIPATIVITY ANALYSIS; DELAY; SYSTEMS; TERMS	The issue of synchronization for a class of inertial memristive neural networks over a finite-time interval is investigated in this paper. Specifically, reaction-diffusion items and Markovian jump parameters are both considered in the system model, meanwhile, a novel nonfragile time-varying proportional retarded control strategy is proposed. First, a befitting variable substitution is invoked to transform the original second-order differential system into a first-order one so that the corresponding synchronization error system that is represented by a first-order differential form is established. Second, by utilizing the integral inequality technique, reciprocally convex combination approach and free-weighting matrix method, a less conservative synchronization criterion in terms of linear matrix inequalities is obtained. Finally, three simulations are exploited to illustrate the feasibility, practicability and superiority of the designed controller so that the acquired theoretical results are supported. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						317	330		10.1016/j.neunet.2019.12.011													
J								ELM embedded discriminative dictionary learning for image classification	NEURAL NETWORKS										Discriminative dictionary learning; Extreme learning machine; Sparse representation; Maximum margin criterion	FACE RECOGNITION; SPARSE REPRESENTATION; LOW-RANK; K-SVD; EXTREME; PROJECTION; REDUCTION; EFFICIENT; MACHINE	Dictionary learning is a widely adopted approach for image classification. Existing methods focus either on finding a dictionary that produces discriminative sparse representation, or on enforcing priors that best describe the dataset distribution. In many cases, the dataset size is often small with large intra-class variability and nondiscriminative feature space. In this work we propose a simple and effective framework called ELM-DDL to address these issues. Specifically, we represent input features with Extreme Learning Machine (ELM) with orthogonal output projection, which enables diverse representation on nonlinear hidden space and task specific feature learning on output space. The embeddings are further regularized via a maximum margin criterion (MMC) to maximize the interclass variance and minimize intra-class variance. For dictionary learning, we design a novel weighted class specific l(1,2) norm to regularize the sparse coding vectors, which promotes uniformity of the sparse patterns of samples belonging to the same class and suppresses support overlaps of different classes. We show that such regularization is robust, discriminative and easy to optimize. The proposed method is combined with a sparse representation classifier (SRC) to evaluate on benchmark datasets. Results show that our approach achieves state-of-the-art performance compared to other dictionary learning methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						331	342		10.1016/j.neunet.2019.11.015													
J								On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces	NEURAL NETWORKS										Neural network; Deep learning; Linear estimator; Nonparametric regression; Minimax optimality	REGRESSION; INEQUALITIES; RATES	Deep learning has been applied to various tasks in the field of machine learning and has shown superiority to other common procedures such as kernel methods. To provide a better theoretical understanding of the reasons for its success, we discuss the performance of deep learning and other methods on a nonparametric regression problem with a Gaussian noise. Whereas existing theoretical studies of deep learning have been based mainly on mathematical theories of well-known function classes such as Holder and Besov classes, we focus on function classes with discontinuity and sparsity, which are those naturally assumed in practice. To highlight the effectiveness of deep learning, we compare deep learning with a class of linear estimators representative of a class of shallow estimators. It is shown that the minimax risk of a linear estimator on the convex hull of a target function class does not differ from that of the original target function class. This results in the suboptimality of linear methods over a simple but non-convex function class, on which deep learning can attain nearly the minimax-optimal rate. In addition to this extreme case, we consider function classes with sparse wavelet coefficients. On these function classes, deep learning also attains the minimax rate up to log factors of the sample size, and linear methods are still suboptimal if the assumed sparsity is strong. We also point out that the parameter sharing of deep neural networks can remarkably reduce the complexity of the model in our setting. (C) 2019 The Author(s). Published by Elsevier Ltd.																	0893-6080	1879-2782				MAR	2020	123						343	361		10.1016/j.neunet.2019.12.014													
J								Global synchronization of coupled delayed memristive reaction-diffusion neural networks	NEURAL NETWORKS										Neural network; Memristor; Reaction-diffusion; Exponential synchronization; Coupling topology	TIME-VARYING DELAYS; EXPONENTIAL SYNCHRONIZATION; ADAPTIVE SYNCHRONIZATION; STATE ESTIMATION; STABILITY; PASSIVITY; TERMS; CRITERIA	This paper focuses on the global exponential synchronization of multiple memristive reaction-diffusion neural networks (MRDNNs) with time delay. Due to introducing the influences of space as well as time on state variables and replacing resistors with memristors in circuit realization, the state-dependent partial differential mathematical model of MRDNN is more general and realistic than traditional neural network model. Based on Lyapunov functional theory, Divergence theorem and inequality techniques, global exponential synchronization criteria of coupled delayed MRDNNs are derived via directed and undirected nonlinear coupling. Finally, three numerical simulation examples are presented to verify the feasibility of our main results. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						362	371		10.1016/j.neunet.2019.12.016													
J								Synchronization of Hindmarsh Rose Neurons	NEURAL NETWORKS										Computational neuroscience; Hindmarsh Rose neuron (HR); Digital; Spiking Neural Networks (SNNs); Field programmable gate arrays (FPGAs); Nengo	NEURAL-NETWORKS; VISUAL-CORTEX; MODEL; RESPONSES	Modeling and implementation of biological neurons are key to the fundamental understanding of neural network architectures in the brain and its cognitive behavior. Synchronization of neuronal models play a significant role in neural signal processing as it is very difficult to identify the actual interaction between neurons in living brain. Therefore, the synchronization study of these neuronal architectures has received extensive attention from researchers. Higher biological accuracy of these neuronal units demands more computational overhead and requires more hardware resources for implementation. This paper presents a two coupled hardware implementation of Hindmarsh Rose neuron model which is mathematically simpler model and yet mimics several behaviors of a real biological neuron. These neurons are synchronized using an exponential function. The coupled system shows several behaviors depending upon the parameters of HR model and coupling function. An approximation of coupling function is also provided to reduce the hardware cost. Both simulations and a low cost hardware implementations of exponential synaptic coupling function and its approximation are carried out for comparison. Hardware implementation on field programmable gate array (FPGA) of approximated coupling function shows that the coupled network produces different dynamical behaviors with acceptable error. Hardware implementation shows that the approximated coupling function has significantly lower implementation cost. A spiking neural network based on HR neuron is also shown as a practical application of this coupled HR neural networks. The spiking network successfully encodes and decodes a time varying input. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						372	380		10.1016/j.neunet.2019.11.024													
J								The role of coupling connections in a model of the cortico-basal ganglia-thalamocortical neural loop for the generation of beta oscillations	NEURAL NETWORKS										Cortico-basal ganglia-thalamocortical circuits; Double-oscillator system; Beta oscillations; Neural mass model; Coupling strength	DEEP BRAIN-STIMULATION; SUBTHALAMIC NUCLEUS; PARKINSONS-DISEASE; PATHOLOGICAL SYNCHRONIZATION; MATHEMATICAL-MODEL; ACTIVITY PATTERNS; DOPAMINE; NETWORK; STATE; TRANSMISSION	Excessive neural synchronization in the cortico-basal ganglia-thalamocortical circuits in the beta (beta) frequency range (12-35 Hz) is closely associated with dopamine depletion in Parkinson's disease (PD) and correlated with movement impairments, but the neural basis remains unclear. In this work, we establish a double-oscillator neural mass model for the cortico-basal ganglia-thalamocortical closed-loop system and explore the impacts of dopamine depletion induced changes in coupling connections within or between the two oscillators on neural activities within the loop. Spectral analysis of the neural mass activities revealed that the power and frequency of their principal components are greatly dependent on the coupling strengths between nuclei. We found that the increased intra-coupling in the basal ganglia-thalamic (BG-Th) oscillator contributes to increased oscillations in the lower beta frequency band (12-25 Hz), while increased intra-coupling in the cortical oscillator mainly contributes to increased oscillations in the upper beta frequency band (26-35 Hz). Interestingly, pathological upper beta oscillations in the cortical oscillator may be another origin of the lower beta oscillations in the BGTh oscillator, in addition to increased intra-coupling strength within the BG-Th network. Lower beta oscillations in the BG-Th oscillator can also change the dominant oscillation frequency of a cortical nucleus from the upper to the lower beta band. Thus, this work may pave the way towards revealing a possible neural basis underlying the Parkinsonian state. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						381	392		10.1016/j.neunet.2019.12.021													
J								Global collaboration through local interaction in competitive learning	NEURAL NETWORKS										Locally interacting SOM; Competitive & collaborative learning; Point cloud estimation; Topologically preserving maps	ORGANIZATION	Feature maps, that preserve the global topology of arbitrary datasets, can be formed by self-organizing competing agents. So far, it has been presumed that global interaction of agents is necessary for this process. We establish that this is not the case, and that global topology can be uncovered through strictly local interactions. Enforcing uniformity of map quality across all agents results in an algorithm that is able to consistently uncover the global topology of diversely challenging datasets. The applicability and scalability of this approach is further tested on a large point cloud dataset, revealing a linear relation between map training time and size. The presented work not only reduces algorithmic complexity but also constitutes first step towards a distributed self organizing map. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						393	400		10.1016/j.neunet.2019.12.018													
J								Multi-task learning for the prediction of wind power ramp events with deep neural networks	NEURAL NETWORKS										Wind power ramp events; Multi-task learning; Multi-output; Deep neural networks; Renewable energies	GENERATION	In Machine Learning, the most common way to address a given problem is to optimize an error measure by training a single model to solve the desired task. However, sometimes it is possible to exploit latent information from other related tasks to improve the performance of the main one, resulting in a learning paradigm known as Multi-Task Learning (MTL). In this context, the high computational capacity of deep neural networks (DNN) can be combined with the improved generalization performance of MTL, by designing independent output layers for every task and including a shared representation for them. In this paper we exploit this theoretical framework on a problem related to Wind Power Ramps Events (WPREs) prediction in wind farms. Wind energy is one of the fastest growing industries in the world, with potential global spreading and deep penetration in developed and developing countries. One of the main issues with the majority of renewable energy resources is their intrinsic intermittency, which makes it difficult to increase the penetration of these technologies into the energetic mix. In this case, we focus on the specific problem of WPREs prediction, which deeply affect the wind speed and power prediction, and they are also related to different turbines damages. Specifically, we exploit the fact that WPREs are spatially-related events, in such a way that predicting the occurrence of WPREs in different wind farms can be taken as related tasks, even when the wind farms are far away from each other. We propose a DNN-MTL architecture, receiving inputs from all the wind farms at the same time to predict WPREs simultaneously in each of the farms locations. The architecture includes some shared layers to learn a common representation for the information from all the wind farms, and it also includes some specification layers, which refine the representation to match the specific characteristics of each location. Finally we modified the Adam optimization algorithm for dealing with imbalanced data, adding costs which are updated dynamically depending on the worst classified class. We compare the proposal against a baseline approach based on building three different independent models (one for each wind farm considered), and against a state-of-the-art reservoir computing approach. The DNN-MTL proposal achieves very good performance in WPREs prediction, obtaining a good balance for all the classes included in the problem (negative ramp, no ramp and positive ramp). (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						401	411		10.1016/j.neunet.2019.12.017													
J								A new fixed-time stability theorem and its application to the fixed-time synchronization of neural networks	NEURAL NETWORKS										Fixed-time stability; Fixed-time synchronization; Neural networks	FINITE-TIME; EXPONENTIAL STABILITY; LAG SYNCHRONIZATION; SYSTEMS; STABILIZATION; FEEDBACK; DESIGN	In this paper, we derive a new fixed-time stability theorem based on definite integral, variable substitution and some inequality techniques. The fixed-time stability criterion and the upper bound estimate formula for the settling time are different from those in the existing fixed-time stability theorems. Based on the new fixed-time stability theorem, the fixed-time synchronization of neural networks is investigated by designing feedback controller, and sufficient conditions are derived to guarantee the fixed-time synchronization of neural networks. To show the usability and superiority of the obtained theoretical results, we propose a secure communication scheme based on the fixed-time synchronization of neural networks. Numerical simulations illustrate that the new upper bound estimate formula for the settling time is much tighter than those in the existing fixed-time stability theorems. Moreover, the plaintext signals can be recovered according to the new fixed-time stability theorem, while the plaintext signals cannot be recovered according to the existing fixed-time stability theorems. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						412	419		10.1016/j.neunet.2019.12.028													
J								Bayesian deep matrix factorization network for multiple images denoising	NEURAL NETWORKS										Matrix factorization; Bayesian neural networks; Variational Bayes		This paper aims at proposing a robust and fast low rank matrix factorization model for multiple images denoising. To this end, a novel model, Bayesian deep matrix factorization network (BDMF), is presented, where a deep neural network (DNN) is designed to model the low rank components and the model is optimized via stochastic gradient variational Bayes. By the virtue of deep learning and Bayesian modeling, BDMF makes significant improvement on synthetic experiments and real-world tasks (including shadow removal and hyperspectral image denoising), compared with existing state-of-the-art models. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						420	428		10.1016/j.neunet.2019.12.023													
J								Spacial sampled-data control for H-infinity output synchronization of directed coupled reaction-diffusion neural networks with mixed delays	NEURAL NETWORKS										Reaction-diffusion neural network; Directed coupling; Mixed time delay; H-infinity output synchronization; Spacial sampled-data control	TIME-VARYING DELAYS; COMPLEX DYNAMICAL NETWORKS; PINNING CONTROL; TERMS; SYSTEMS; ARRAY	This work investigates the H-infinity output synchronization (HOS) of the directed coupled reaction-diffusion (R-D) neural networks (NNs) with mixed delays. Firstly, a model of the directed state coupled RD NNs is introduced, which not only contains some discrete and distributed time delays, but also obeys a mixed Dirichlet-Neumann boundary condition. Secondly, a spacial sampled-data controller is proposed to achieve the HOS of the considered networks. This type of controller can reduce the update rate in the process of control by measuring the state of networks at some fixed sampling points in the space region. Moreover, some criteria for the HOS are established by designing an appropriate Lyapunov functional, and some quantitative relations between diffusion coefficients, mixed delays, coupling strength and control parameters are given accurately by these criteria. Thirdly, the case of directed spatial diffusion coupled networks is also studied and, the following finding is obtained: the spatial diffusion coupling can suppress the HOS while the state coupling can promote it. Finally, one example is simulated as the verification of the theoretical results. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAR	2020	123						429	440		10.1016/j.neunet.2019.12.026													
J								Adaptive Fuzzy Quantized Control for Nonlinear Systems With Hysteretic Actuator Using a New Filter-Connected Quantizer	IEEE TRANSACTIONS ON CYBERNETICS										Hysteresis; Actuators; Nonlinear systems; Fuzzy logic; Adaptive systems; Adaptation models; Actuator hysteresis; adaptive quantized control; backstepping technique; fuzzy logic systems (FLSs); nonlinear systems	OUTPUT-FEEDBACK CONTROL; TRACKING CONTROL; NEURAL-CONTROL; STABILIZATION; CONSTRAINTS; DESIGN	This paper aims at the issue of adaptive fuzzy quantized control for a class of uncertain nonlinear systems preceded by unknown actuator hysteresis. One challenging problem that obstructs the development of the control scheme is that the direct application of the quantized signal containing high-frequency components to the hysteretic actuator will lead to system performance deterioration. To resolve this challenge, we propose a filter-connected quantizer in which a hysteretic quantizer is employed to reduce the communication rate and an adaptive high-cut filter is designed to smooth the hysteresis input. Furthermore, based on fuzzy logic systems' online approximation capability, a novel adaptive fuzzy control scheme involves a new control strategy and an adaptive strategy is constructed via a backstepping technique. It is proved that the proposed control scheme guarantees the tracking error is asymptotically convergent to an adjustable neighborhood of zero and all of the closed-loop signals are uniform ultimate bounded. Lastly, simulations are conducted to further verify our theoretical results.																	2168-2267	2168-2275				MAR	2020	50	3					876	889		10.1109/TCYB.2018.2864166													
J								Event-Triggered Adaptive Tracking Control for Multiagent Systems With Unknown Disturbances	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Disturbance observers; Control systems; Backstepping; Adaptive backstepping control; cooperative control; disturbance observer; event-triggered control	LEADER-FOLLOWING CONSENSUS; NONLINEAR-SYSTEMS; INPUT DELAYS; DESIGN; COMMUNICATION	This paper considers the event-triggered tracking control problem of nonlinear multiagent systems with unknown disturbances. The event-triggering mechanism is considered in the controller update, which decreases the amount of communication and reduces the frequency of the controller update in practice. By designing a disturbance observer, the unknown external disturbances are estimated. Moreover, a part of adaptive parameters are only dependent on the number of followers, which weakens the computational burden. It is shown that all the signals are bounded, and the consensus tracking errors are located in a small neighborhood of the origin based on the Lyapunov stability theory and backstepping approach. Finally, the effectiveness of the approach proposed in this paper is proved by simulation results.																	2168-2267	2168-2275				MAR	2020	50	3					890	901		10.1109/TCYB.2018.2869084													
J								Cooperative Control of Multiple Nonlinear Benchmark Systems Perturbed by Second-Order Moment Processes	IEEE TRANSACTIONS ON CYBERNETICS										Benchmark testing; Stochastic processes; Process control; Automobiles; White noise; Cybernetics; Graph theory; Cooperative control; directed topology; multiple nonlinear benchmark systems; second-order moment processes	OUTPUT-FEEDBACK STABILIZATION; LEADER-FOLLOWING CONSENSUS; MULTIAGENT SYSTEMS; TRACKING CONTROL; ACTIVE LEADER; NETWORKS; DESIGN; AGENTS; COORDINATION; ALGORITHMS	This paper studies the cooperative control problem of multiple nonlinear benchmark systems perturbed by second-order moment processes. The nonlinear benchmark system consists of a moving car and a rolling ball in oscillating surroundings. When the leader is only accessible to a small part of the followers in a directed graph, a new vectorial backstepping method is proposed for the design of distributed cooperative control laws. By using stochastic analysis techniques and algebra graph theory, it is shown that the cooperative control problem under consideration is solvable. Specifically, the errors between the followers' outputs and the leader's output can be made arbitrarily small while keeping all states of the closed-loop system bounded in probability. Finally, the effectiveness of the proposed control scheme is demonstrated through a simulation example.																	2168-2267	2168-2275				MAR	2020	50	3					902	910		10.1109/TCYB.2018.2869385													
J								Adaptive Synchronization of Reaction-Diffusion Neural Networks and Its Application to Secure Communication	IEEE TRANSACTIONS ON CYBERNETICS										Synchronization; Artificial neural networks; Adaptation models; Brain modeling; Delay effects; Delays; Adaptive systems; Adaptive control; image encryption; linear matrix inequality (LMI); neural networks (NNs); reaction-diffusion; synchronization	EXPONENTIAL SYNCHRONIZATION; IMPULSIVE SYNCHRONIZATION; FEEDBACK-CONTROL; SYSTEMS; STABILITY; DELAYS; DISCRETE; CRITERIA	This paper is mainly concerned with the synchronization problem of reaction-diffusion neural networks (RDNNs) with delays and its direct application in image secure communications. An adaptive control is designed without a sign function in which the controller gain matrix is a function of time. The synchronization criteria are established for an error model derived from master-slave models through solving the set of linear matrix inequalities derived by constructing the suitable novel Lyapunov-Krasovskii functional candidate, Green's formula, and Wirtinger's inequality. If the proposed sufficient conditions are satisfied, then the global asymptotic synchronization of the error model is guaranteed. The numerical illustrations are provided to demonstrate the validity of the derived synchronization criteria. In addition, the role of system parameters is picturized through the chaotic nature of RDNNs and those unprecedented solutions is utilized to promote better security of image transactions. As is evident, the enhancement of image encryption algorithm is designed with two levels, namely, image watermarking and diffusion process. The contributions of this paper are discussed as concluding remarks.																	2168-2267	2168-2275				MAR	2020	50	3					911	922		10.1109/TCYB.2018.2877410													
J								Objective-Domain Dual Decomposition: An Effective Approach to Optimizing Partially Differentiable Objective Functions	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Linear programming; Search problems; Convergence; Heuristic algorithms; Cybernetics; Computer science; Domain decomposition; hybrid process; objective decomposition; partial differentiable objective function; simulated water-stream algorithm (SWA	EVOLUTIONARY ALGORITHM; OPTIMIZATION; SEARCH; REPRESENTATION	This paper addresses a class of optimization problems in which either part of the objective function is differentiable while the rest is nondifferentiable or the objective function is differentiable in only part of the domain. Accordingly, we propose a dual-decomposition-based approach that includes both objective decomposition and domain decomposition. In the former, the original objective function is decomposed into several relatively simple subobjectives to isolate the nondifferentiable part of the objective function, and the problem is consequently formulated as a multiobjective optimization problem (MOP). In the latter decomposition, we decompose the domain into two subdomains, that is, the differentiable and nondifferentiable domains, to isolate the nondifferentiable domain of the nondifferentiable subobjective. Subsequently, the problem can be optimized with different schemes in the different subdomains. We propose a population-based optimization algorithm, called the simulated water-stream algorithm (SWA), for solving this MOP. The SWA is inspired by the natural phenomenon of water streams moving toward a basin, which is analogous to the process of searching for the minimal solutions of an optimization problem. The proposed SWA combines the deterministic search and heuristic search in a single framework. Experiments show that the SWA yields promising results compared with its existing counterparts.																	2168-2267	2168-2275				MAR	2020	50	3					923	934		10.1109/TCYB.2018.2870487													
J								Decentralized Adaptive Output Feedback Fault Detection and Control for Uncertain Nonlinear Interconnected Systems	IEEE TRANSACTIONS ON CYBERNETICS										Fault detection; Noise measurement; Nonlinear systems; Output feedback; Interconnected systems; Adaptive systems; Decentralized control; fault detection; measurement noise; neural networks (NNs)	TOLERANT CONTROL; ACCOMMODATION	This paper studies the problem of decentralized adaptive output feedback fault detection and control for a class of uncertain nonlinear interconnected systems. The K-filters are designed to estimate the unmeasured state variables of the system. Moreover, the built-in noise dampening filters are introduced to attenuate the influence caused by the measurement noises. Then the fault detection scheme is proposed by designing the residual and threshold signals. Subsequently, by using the backstepping design method, the decentralized switched control strategies are proposed with the help of the neural network approximation technique. Based on the Lyapunov stability theory, it is proved strictly that all signals of the resulting closed-loop system are bounded. Finally, a simulation example is presented to verify the effectiveness of the theoretical result.																	2168-2267	2168-2275				MAR	2020	50	3					935	945		10.1109/TCYB.2018.2872802													
J								Passivity Analysis of Delayed Neural Networks Based on Lyapunov-Krasovskii Functionals With Delay-Dependent Matrices	IEEE TRANSACTIONS ON CYBERNETICS										Lyapunov-Krasovsknii functional (LKF); neural networks; passivity; time-varying delay	TIME-VARYING DELAY; GLOBAL ASYMPTOTIC STABILITY; TRIGGERED H-INFINITY; LINEAR-SYSTEMS; INEQUALITY; DISCRETE; CRITERIA; STATE	This paper is concerned with passivity of a class of delayed neural networks. In order to derive less conservative passivity criteria, two Lyapunov-Krasovskii functionals (LKFs) with delay-dependent matrices are introduced by taking into consideration a second-order Bessel-Legendre inequality. In one LKF, the system state vector is coupled with those vectors inherited from the second-order Bessel-Legendre inequality through delay-dependent matrices, while no such coupling of them exists in the other LKF. These two LKFs are referred to as the coupled LKF and the noncoupled LKF, respectively. A number of delay-dependent passivity criteria are derived by employing a convex approach and a nonconvex approach to deal with the square of the time-varying delay appearing in the derivative of the LKF. Through numerical simulation, it is found that: 1) the coupled LKF is more beneficial than the noncoupled LKF for reducing the conservatism of the obtained passivity criteria and 2) the passivity criteria using the convex approach can deliver larger delay upper bounds than those using the nonconvex approach.																	2168-2267	2168-2275				MAR	2020	50	3					946	956		10.1109/TCYB.2018.2874273													
J								Cooperative Set Aggregation of Second-Order Multiagent Systems: Approximate Projection and Prescribed Performance	IEEE TRANSACTIONS ON CYBERNETICS										Convergence; Projection algorithms; Transient analysis; Multi-agent systems; Decentralized control; Optimization; Approximation algorithms; Approximate projection; directed graph; guaranteed performance; second-order multiagent systems (MASs); set aggregation	CONVEX-OPTIMIZATION; DYNAMICAL-SYSTEMS; CONSENSUS; TRACKING; UAVS	This paper studies the cooperative set aggregation problem for second-order multiagent systems by utilizing the approximate projection algorithm. First, an aggregation law that uses the approximate projection is proposed, where the feasible set of the approximate projection points is established based on an Euclidean distance with respect to the targeted set and a deviated angle with respect to the exact projection point. Under the proposed law, the position vectors of all the agents are shown to reach an agreement in the intersection of their individual targeted sets and the velocity vector of each agent is shown to converge to zero. Then, a time-dependent performance bound is set for the norm of the weighted error of the relative positions among neighboring agents and the absolute velocity of the agent, and a performance-guaranteed aggregation controller is designed to guarantee the prescribed transient performance. During the process of aggregation, the norm of the weighted error is shown to not exceed the performance bound. The convergence conditions of the proposed algorithms with respect to the control strength and the terms induced by the approximate projection are obtained by using the techniques of convex analysis and Lyapunov stability. Simulations are provided to validate the theoretical results.																	2168-2267	2168-2275				MAR	2020	50	3					957	970		10.1109/TCYB.2018.2875131													
J								Identification of Cellular Automata Based on Incomplete Observations With Bounded Time Gaps	IEEE TRANSACTIONS ON CYBERNETICS										Table lookup; Genetic algorithms; Automata; Visualization; Cybernetics; Task analysis; Machine learning algorithms; Cellular automata (CAs); genetic algorithms (GAs); nonlinear dynamical systems; system identification	LYAPUNOV EXPONENTS; RULES	In this paper, the problem of identifying the cellular automata (CAs) is considered. We frame and solve this problem in the context of incomplete observations, i.e., prerecorded, incomplete configurations of the system at certain, and unknown time stamps. We consider 1-D, deterministic, two-state CAs only. An identification method based on a genetic algorithm with individuals of variable length is proposed. The experimental results show that the proposed method is highly effective. In addition, connections between the dynamical properties of CAs (Lyapunov exponents and behavioral classes) and the performance of the identification algorithm are established and analyzed.																	2168-2267	2168-2275				MAR	2020	50	3					971	984		10.1109/TCYB.2018.2875266													
J								Dual Encoding for Abstractive Text Summarization	IEEE TRANSACTIONS ON CYBERNETICS										Decoding; Encoding; Task analysis; Semantics; Recurrent neural networks; Computational modeling; Abstractive text summarization; dual encoding; primary encoder; recurrent neural network (RNN); secondary encoder		Recurrent neural network-based sequence-to-sequence attentional models have proven effective in abstractive text summarization. In this paper, we model abstractive text summarization using a dual encoding model. Different from the previous works only using a single encoder, the proposed method employs a dual encoder including the primary and the secondary encoders. Specifically, the primary encoder conducts coarse encoding in a regular way, while the secondary encoder models the importance of words and generates more fine encoding based on the input raw text and the previously generated output text summarization. The two level encodings are combined and fed into the decoder to generate more diverse summary that can decrease repetition phenomenon for long sequence generation. The experimental results on two challenging datasets (i.e., CNN/DailyMail and DUC 2004) demonstrate that our dual encoding model performs against existing methods.																	2168-2267	2168-2275				MAR	2020	50	3					985	996		10.1109/TCYB.2018.2876317													
J								Text Image Deblurring Using Kernel Sparsity Prior	IEEE TRANSACTIONS ON CYBERNETICS										L-0-norm; motion deblurring; text image	REGULARIZATION; VIDEO	Previous methods on text image motion deblurring seldom consider the sparse characteristics of the blur kernel. This paper proposes a new text image motion deblurring method by exploiting the sparse properties of both text image itself and kernel. It incorporates the L-0-norm for regularizing the blur kernel in the deblurring model, besides the L-0 sparse priors for the text image and its gradient. Such a L-0-norm-based model is efficiently optimized by half-quadratic splitting coupled with the fast conjugate descent method. To further improve the quality of the recovered kernel, a structure-preserving kernel denoising method is also developed to filter out the noisy pixels, yielding a clean kernel curve. Experimental results show the superiority of the proposed method. The source code and results are available at: https://github.com/shenjianbing/text-image-deblur.																	2168-2267	2168-2275				MAR	2020	50	3					997	1008		10.1109/TCYB.2018.2876511													
J								Intraspectrum Discrimination and Interspectrum Correlation Analysis Deep Network for Multispectral Face Recognition	IEEE TRANSACTIONS ON CYBERNETICS										Face recognition; Feature extraction; Face; Correlation; Image recognition; Cameras; Focusing; Deep convolutional neural networks (DCNNs); intraspectrum discriminant information exploration; multispectral face recognition; spectra selection; useful interspectrum correlation information exploration	PERSON REIDENTIFICATION; IMAGES; FUSION	Multispectral images contain rich recognition information since the multispectral camera can reveal information that is not visible to the human eye or to the conventional RGB camera. Due to this characteristic of multispectral images, multispectral face recognition has attracted lots of research interest. Although some multispectral face recognition methods have been presented in the last decade, how to fully and effectively explore the intraspectrum discriminant information and the useful interspectrum correlation information in multispectral face images for recognition has not been well studied. To boost the performance of multispectral face recognition, we propose an intraspectrum discrimination and interspectrum correlation analysis deep network (IDICN) approach. Multiple spectra are divided into several spectrum-sets, with each containing a group of spectra within a small spectral range. The IDICN network contains a set of spectrum-set-specific deep convolutional neural networks attempting to extract spectrum-set-specific features, followed by a spectrum pooling layer, whose target is to select a group of spectra with favorable discriminative abilities adaptively. IDICN jointly learns the nonlinear representations of the selected spectra, such that the intraspectrum Fisher loss and the interspectrum discriminant correlation are minimized. Experiments on the well-known Hong Kong Polytechnic University, Carnegie Mellon University, and the University of Western Australia multispectral face datasets demonstrate the superior performance of the proposed approach over several state-of-the-art methods.																	2168-2267	2168-2275				MAR	2020	50	3					1009	1022		10.1109/TCYB.2018.2876591													
J								Nonparametric Estimation of Probabilistic Membership for Subspace Clustering	IEEE TRANSACTIONS ON CYBERNETICS										Probabilistic logic; Optimization; Clustering methods; Clustering algorithms; Sparse matrices; Estimation; Principal component analysis; Nonparametric estimation; probabilistic membership; subspace clustering	SEGMENTATION; ROBUST; RECOGNITION; ALGORITHM	Recent advances of subspace clustering have provided a new way of constructing affinity matrices for clustering. Unlike the kernel-based subspace clustering, which needs tedious tuning among infinitely many kernel candidates, the self-expressive models derived from linear subspace assumptions in modern subspace clustering methods are rigorously combined with sparse or low-rank optimization theory to yield an affinity matrix as a solution of an optimization problem. Despite this nice theoretical aspect, the affinity matrices of modern subspace clustering have quite different meanings from the traditional ones, and even though the affinity matrices are expected to have a rough block-diagonal structure, it is unclear whether these are good enough to apply spectral clustering. In fact, most of the subspace clustering methods perform some sort of affinity value rearrangement to apply spectral clustering, but its adequacy for the spectral clustering is not clear; even though the spectral clustering step can also have a critical impact on the overall performance. To resolve this issue, in this paper, we provide a nonparametric method to estimate the probabilistic cluster membership from these affinity matrices, which we can directly find the clusters from. The likelihood for an affinity matrix is defined nonparametrically based on histograms given the probabilistic membership, which is defined as a combination of probability simplices, and an additional prior probability is defined to regularize the membership as a Bernoulli distribution. Solving this maximum a posteriori problem replaces the spectral clustering procedure, and the final discrete cluster membership can be calculated by selecting the clusters with maximum probabilities. The proposed method provides state-of-the-art performance for well-known subspace clustering methods on popular benchmark databases.																	2168-2267	2168-2275				MAR	2020	50	3					1023	1036		10.1109/TCYB.2018.2878069													
J								A Novel Sliding Mode Control for a Class of Stochastic Polynomial Fuzzy Systems Based on SOS Method	IEEE TRANSACTIONS ON CYBERNETICS										Polynomial fuzzy system (PFS); sliding mode control (SMC); sum of squares (SOS); vector integral sliding mode surface (VISMS)	TIME-VARYING DELAY; H-INFINITY CONTROL; STABILITY ANALYSIS; DESIGN; STABILIZATION	In this paper, a novel robust controller for continuous stochastic polynomial fuzzy systems is investigated. The aim of the proposed method is to eliminate the restrictive assumptions that the local input matrix Bi must be uniform and the sliding mode surface proposed did not consider the stochastic perturbations, which are required in most existing results. At the same time, the proposed method could handle the system with matched external disturbances. First, a novel vector integral sliding mode surface (VISMS) is constructed according to the basis matrix B(x). The sliding mode surface parameter matrix K(x) can be obtained through the provided sum of squares conditions. Second, by using an improved Lyapunov method and a new proposed lemma, a novel sliding mode control law is designed to keep the state of the closed-loop systems on the VISMS approximately since the initial time. Third, a practical example and a numerical one are provided to illustrate the validity of the proposed approach.																	2168-2267	2168-2275				MAR	2020	50	3					1037	1046		10.1109/TCYB.2018.2879839													
J								MHTN: Modal-Adversarial Hybrid Transfer Network for Cross-Modal Retrieval	IEEE TRANSACTIONS ON CYBERNETICS										Semantics; Training data; Knowledge transfer; Training; Correlation; Task analysis; Solid modeling; Adversarial training; cross-modal retrieval; hybrid transfer network; knowledge transfer; modal-adversarial	DOMAIN ADAPTATION; REPRESENTATION	Cross-modal retrieval has drawn wide interest for retrieval across different modalities (such as text, image, video, audio, and 3-D model). However, existing methods based on a deep neural network often face the challenge of insufficient cross-modal training data, which limits the training effectiveness and easily leads to overfitting. Transfer learning is usually adopted for relieving the problem of insufficient training data, but it mainly focuses on knowledge transfer only from large-scale datasets as a single-modal source domain (such as ImageNet) to a single-modal target domain. In fact, such large-scale single-modal datasets also contain rich modal-independent semantic knowledge that can be shared across different modalities. Besides, large-scale cross-modal datasets are very labor-consuming to collect and label, so it is significant to fully exploit the knowledge in single-modal datasets for boosting cross-modal retrieval. To achieve the above goal, this paper proposes a modal-adversarial hybrid transfer network (MHTN), which aims to realize knowledge transfer from a single-modal source domain to a cross-modal target domain and learn cross-modal common representation. It is an end-to-end architecture with two subnetworks. First, a modal-sharing knowledge transfer subnetwork is proposed to jointly transfer knowledge from a single modality in the source domain to all modalities in the target domain with a star network structure, which distills modal-independent supplementary knowledge for promoting cross-modal common representation learning. Second, a modal-adversarial semantic learning subnetwork is proposed to construct an adversarial training mechanism between the common representation generator and modality discriminator, making the common representation discriminative for semantics but indiscriminative for modalities to enhance cross-modal semantic consistency during the transfer process. Comprehensive experiments on four widely used datasets show the effectiveness of MHTN.																	2168-2267	2168-2275				MAR	2020	50	3					1047	1059		10.1109/TCYB.2018.2879846													
J								PPLS/D: Parallel Pareto Local Search Based on Decomposition	IEEE TRANSACTIONS ON CYBERNETICS										Multiobjective combinatorial optimization; parallel metaheuristic; Pareto local search; unconstrained binary quadratic programming; traveling salesman problem	ALGORITHM	Pareto local search (PLS) is a basic building block in many metaheuristics for a multiobjective combinatorial optimization problem. In this paper, an enhanced PLS variant called parallel PLS based on decomposition (PPLS/D) is proposed. PPLS/D improves the efficiency of PLS using the techniques of parallel computation and problem decomposition. It decomposes the original search space into L subregions and executes L parallel processes searching in these subregions simultaneously. Inside each subregion, the PPLS/D process is guided by a unique scalar objective function. PPLS/D differs from the well-known two phase PLS in that it uses the scalar objective function to guide every move of the PLS procedure in a fine-grained manner. In the experimental studies, PPLS/D is compared against the basic PLS and a recently proposed PLS variant on the multiobjective unconstrained binary quadratic programming problems and the multiobjective traveling salesman problems with, at most, four objectives. The experimental results show that regardless of whether the initial solutions are randomly generated or generated by heuristic methods, PPLS/D always performs significantly better than the other two PLS variants.																	2168-2267	2168-2275				MAR	2020	50	3					1060	1071		10.1109/TCYB.2018.2880256													
J								Scalable Large-Margin Distance Metric Learning Using Stochastic Gradient Descent	IEEE TRANSACTIONS ON CYBERNETICS										Large-margin nearest neighbor; metric learning; positive semidefinite (PSD) matrix; stochastic gradient descent (SGD)	EIGENVALUE	The key to success of many machine learning and pattern recognition algorithms is the way of computing distances between the input data. In this paper, we propose a large-margin-based approach, called the large-margin distance metric learning (LMDML), for learning a Mahalanobis distance metric. LMDML employs the principle of margin maximization to learn the distance metric with the goal of improving ${k}$ -nearest-neighbor classification. The main challenge of distance metric learning is the positive semidefiniteness constraint on the Mahalanobis matrix. Semidefinite programming is commonly used to enforce this constraint, but it becomes computationally intractable on large-scale data sets. To overcome this limitation, we develop an efficient algorithm based on a stochastic gradient descent. Our algorithm can avoid the computations of the full gradient and ensure that the learned matrix remains within the positive semidefinite cone after each iteration. Extensive experiments show that the proposed algorithm is scalable to large data sets and outperforms other state-of-the-art distance metric learning approaches regarding classification accuracy and training time.																	2168-2267	2168-2275				MAR	2020	50	3					1072	1083		10.1109/TCYB.2018.2881417													
J								Distributed Repetitive Learning Control for Cooperative Cadence Tracking in Functional Electrical Stimulation Cycling	IEEE TRANSACTIONS ON CYBERNETICS										Muscles; Iron; Switches; Electric motors; Task analysis; Torque; Distributed control; functional electrical stimulation (FES); FES-cycling; repetitive learning control (RLC)	SPINAL-CORD-INJURY; STROKE; ROBOTICS; ASSIST; FES; STRATEGIES; LIMB	Closed-loop control of functional electrical stimulation coupled with motorized assistance to induce cycling is a rehabilitative strategy that can improve the mobility of people with neurological conditions (NCs). However, robust control methods, which are currently pervasive in the cycling literature, have limited effectiveness due to the use of high stimulation intensity leading to accelerated fatigue during cycling protocols. This paper examines the design of a distributed repetitive learning controller (RLC) that commands an independent learning feedforward term to each of the six stimulated lower-limb muscle groups and an electric motor during the tracking of a periodic cadence trajectory. The switched controller activates lower limb muscles during kinematic efficient regions of the crank cycle and provides motorized assistance only when most needed (i.e., during the portions of the crank cycle where muscles evoke a low torque output). The controller exploits the periodicity of the desired cadence trajectory to learn from previous control inputs for each muscle group and electric motor. A Lyapunov-based stability analysis guarantees asymptotic tracking via an invariance-like corollary for nonsmooth systems. The switched distributed RLC was evaluated in experiments with seven able-bodied individuals and five participants with NCs. A mean root-mean-squared cadence error of 3.58 +/- 0.43 revolutions per minute (RPM) (0.07 +/- 7.35% average error) and 4.26 +/- 0.84 RPM (0.1 +/- 8.99% average error) was obtained for the healthy and neurologically impaired populations, respectively.																	2168-2267	2168-2275				MAR	2020	50	3					1084	1095		10.1109/TCYB.2018.2882755													
J								Unsupervised Classification of Multivariate Time Series Using VPCA and Fuzzy Clustering With Spatial Weighted Matrix Distance	IEEE TRANSACTIONS ON CYBERNETICS										Dimensionality reduction; fuzzy clustering; multivariate time series (MTS)	PATTERN-RECOGNITION	Due to high dimensionality and multiple variables, unsupervised classification of multivariate time series (MTS) involves more challenging problems than those of univariate ones. Unlike the vectorization of a feature matrix in traditional clustering algorithms, an unsupervised pattern recognition scheme based on matrix data is proposed for MTS samples in this paper. To reduce the computational load and time consumption, a novel variable-based principal component analysis (VPCA) is first devised for the dimensionality reduction of MTS samples. Afterward, a spatial weighted matrix distance-based fuzzy clustering (SWMDFC) algorithm is proposed to directly group MTS samples into clusters as well as preserve the structure of the data matrix. The spatial weighted matrix distance (SWMD) integrates the spatial dimensionality difference of elements of data into the distance of MST pairs. In terms of the SWMD, the MTS samples are clustered without vectorization in the dimensionality-reduced feature matrix space. Finally, three open-access datasets are utilized for the validation of the proposed unsupervised classification scheme. The results show that the VPCA can capture more features of MTS data than principal component analysis (PCA) and 2-D PCA. Furthermore, the clustering performance of SWMDFC is superior to that of fuzzy ${c}$ -means clustering algorithms based on the Euclidean distance or image Euclidean distance.																	2168-2267	2168-2275				MAR	2020	50	3					1096	1105		10.1109/TCYB.2018.2883388													
J								Guiding Evolutionary Multiobjective Optimization With Generic Front Modeling	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Shape; Training; Computer science; Sociology; Statistics; Evolutionary computation; Evolutionary algorithm; fitness function; front modeling; multiobjective and many-objective optimization	NONDOMINATED SORTING APPROACH; REFERENCE-POINT; PARETO FRONT; ALGORITHM; MOEA/D; SELECTION; PERFORMANCE	In evolutionary multiobjective optimization, the Pareto front (PF) is approximated by using a set of representative candidate solutions with good convergence and diversity. However, most existing multiobjective evolutionary algorithms (MOEAs) have general difficulty in the approximation of PFs with complicated geometries. To address this issue, we propose a generic front modeling method for evolutionary multiobjective optimization, where the shape of the nondominated front is estimated by training a generalized simplex model. On the basis of the estimated front, we further develop an MOEA, where both the mating selection and environmental selection are driven by the approximate nondominated fronts modeled during the optimization process. For performance assessment, the proposed algorithm is compared with several state-of-the-art evolutionary algorithms on a wide range of benchmark problems with various types of PFs and different numbers of objectives. Experimental results demonstrate that the proposed algorithm performs consistently on a variety of multiobjective optimization problems.																	2168-2267	2168-2275				MAR	2020	50	3					1106	1119		10.1109/TCYB.2018.2883914													
J								SCN: Switchable Context Network for Semantic Segmentation of RGB-D Images	IEEE TRANSACTIONS ON CYBERNETICS										Image segmentation; Semantics; Switches; Image color analysis; Image resolution; Computer architecture; Cybernetics; Context representation; convolutional neural network (CNN); RGB-D images; semantic segmentation		Context representations have been widely used to profit semantic image segmentation. The emergence of depth data provides additional information to construct more discriminating context representations. Depth data preserves the geometric relationship of objects in a scene, which is generally hard to be inferred from RGB images. While deep convolutional neural networks (CNNs) have been successful in solving semantic segmentation, we encounter the problem of optimizing CNN training for the informative context using depth data to enhance the segmentation accuracy. In this paper, we present a novel switchable context network (SCN) to facilitate semantic segmentation of RGB-D images. Depth data is used to identify objects existing in multiple image regions. The network analyzes the information in the image regions to identify different characteristics, which are then used selectively through switching network branches. With the content extracted from the inherent image structure, we are able to generate effective context representations that are aware of both image structures and object relationships, leading to a more coherent learning of semantic segmentation network. We demonstrate that our SCN outperforms state-of-the-art methods on two public datasets.																	2168-2267	2168-2275				MAR	2020	50	3					1120	1131		10.1109/TCYB.2018.2885062													
J								Deep Learning Meets Game Theory: Bregman-Based Algorithms for Interactive Deep Generative Adversarial Networks	IEEE TRANSACTIONS ON CYBERNETICS										Deep learning; Games; Game theory; Biological neural networks; Gallium nitride; Neurons; Heuristic algorithms; Big data; deep learning; distributional robustness; game theory; mean-field		This paper presents an interplay between deep learning and game theory. It models basic deep learning tasks as strategic games. Then, distributionally robust games and their relationship with deep generative adversarial networks (GANs) are presented. To achieve a higher order convergence rate without using a second derivative of the objective function, a Bregman discrepancy is used to construct a speed-up deep learning. Each player has a continuous action space which corresponds to weight space and aims to learn his/her optimal strategy. The convergence rate of the proposed deep learning algorithm is derived using a mean estimate. Experiments are carried out on a real dataset in both shallow and deep GANs. Both qualitative and quantitative evaluation results show that the generative model trained by the Bregman deep learning algorithm can speed up the state-of-the-art performance.																	2168-2267	2168-2275				MAR	2020	50	3					1132	1145		10.1109/TCYB.2018.2886238													
J								Blind Noisy Image Quality Assessment Using Sub-Band Kurtosis	IEEE TRANSACTIONS ON CYBERNETICS										Noise measurement; Image quality; Discrete wavelet transforms; Discrete cosine transforms; Distortion; AWGN; Blind noisy image quality assessment (IQA); discrete wavelet transform (DWT); extreme learning machine (ELM); kurtosis; sub-band	EXTREME LEARNING-MACHINE; STATISTICS; OPTIMIZATION; TUTORIAL; DOMAIN	Noise that afflicts natural images, regardless of the source, generally disturbs the perception of image quality by introducing a high-frequency random element that, when severe, can mask image content. Except at very low levels, where it may play a purpose, it is annoying. There exist significant statistical differences between distortion-free natural images and noisy images that become evident upon comparing the empirical probability distribution histograms of their discrete wavelet transform (DWT) coefficients. The DWT coefficients of low- or no-noise natural images have leptokurtic, peaky distributions with heavy tails; while noisy images tend to be platykurtic with less peaky distributions and shallower tails. The sample kurtosis is a natural measure of the peakedness and tail weight of the distributions of random variables. Here, we study the efficacy of the sample kurtosis of image wavelet coefficients as a feature driving, an extreme learning machine which learns to map kurtosis values into perceptual quality scores. The model is trained and tested on five types of noisy images, including additive white Gaussian noise, additive Gaussian color noise, impulse noise, masked noise, and high-frequency noise from the LIVE, CSIQ, TID2008, and TID2013 image quality databases. The experimental results show that the trained model has better quality evaluation performance on noisy images than existing blind noise assessment models, while also outperforming general-purpose blind and full-reference image quality assessment methods.																	2168-2267	2168-2275				MAR	2020	50	3					1146	1156		10.1109/TCYB.2018.2889376													
J								Reviewing and Benchmarking Parameter Control Methods in Differential Evolution	IEEE TRANSACTIONS ON CYBERNETICS										Phase change materials; Benchmark testing; Optimization; Sociology; Statistics; Pulse modulation; Cybernetics; Benchmarking study; continuous optimization; differential evolution (DE); parameter control methods (PCMs); review	GLOBAL OPTIMIZATION; ALGORITHM; MUTATION; ADAPTATION; CROSSOVER; ENSEMBLE	Many differential evolution (DE) algorithms with various parameter control methods (PCMs) have been proposed. However, previous studies usually considered PCMs to be an integral component of a complex DE algorithm. Thus, the characteristics and performance of each method are poorly understood. We present an in-depth review of 24 PCMs for the scale factor and crossover rate in DE and a large-scale benchmarking study. We carefully extract the 24 PCMs from their original, complex algorithms and describe them according to a systematic manner. Our review facilitates the understanding of similarities and differences between existing, representative PCMs. The performance of DEs with the 24 PCMs and 16 variation operators is investigated on 24 black-box benchmark functions. Our benchmarking results reveal which methods exhibit high performance when embedded in a standardized framework under 16 different conditions, independent from their original, complex algorithms. We also investigate how much room there is for further improvement of PCMs by comparing the 24 methods with an oracle-based model, which can be considered to be a conservative lower bound on the performance of an optimal method.																	2168-2267	2168-2275				MAR	2020	50	3					1170	1184		10.1109/TCYB.2019.2892735													
J								Optimization Tools Based on Metaheuristics for Performance Enhancement in a Gaussian Adaptive PID Controller	IEEE TRANSACTIONS ON CYBERNETICS										Genetic algorithms; Optimization; PI control; Particle swarm optimization; Steady-state; PD control; Voltage control; Gaussian adaptive proportional-integral-derivative (GAPID) control; genetic algorithm (GA); nonlinear control; particle swarm optimization (PSO) algorithm	PARTICLE SWARM OPTIMIZATION; DESIGN	This paper presents the proposal of using two bio-inspired metaheuristics-genetic algorithms (GAs) and particle swarm optimization (PSO)-to adjust the free coefficients of a Gaussian adaptive proportional-integral-derivative (GAPID) controller. When a specific adaptation rule is imposed to a conventional proportional-integral-derivative (PID) controller, either by means of a hyperbolic tangent function or a Gaussian function, the solution is left exposed to the function restrictions/impositions. Finding the correct proportionality between the parameters is an arduous task, which often does not have an algebraic solution. Each Gaussian function of each control action has three parameters, resulting in a total of nine parameters to be defined. This paper proposes making the parameters linked to the linear PID gains, in order to keep the GAPID the same design requirements as for the PID. Then, two metaheuristics (GA and PSO) were employed in order to find the best parameters for the GAPID. A comparison between these two strategies is presented. In this investigation, a well-known plant of a step-down dc-dc converter is used, which represents a typical second-order system, where the absence of significant nonlinearities helps focus the study on the control behavior. Simulation and experimentation were performed, and both have been successful, but PSO stood out due to its simplicity and low-computational effort.																	2168-2267	2168-2275				MAR	2020	50	3					1185	1194		10.1109/TCYB.2019.2895319													
J								HUOPM: High-Utility Occupancy Pattern Mining	IEEE TRANSACTIONS ON CYBERNETICS										Data mining; Frequency measurement; Sea measurements; Gallium nitride; Transaction databases; Computer science; Interesting pattern; utility mining; utility occupancy; utility theory	EFFICIENT ALGORITHMS; ITEMSET UTILITIES; FREQUENT ITEMSETS	Mining useful patterns from varied types of databases is an important research topic, which has many real-life applications. Most studies have considered the frequency as sole interestingness measure to identify high-quality patterns. However, each object is different in nature. The relative importance of objects is not equal, in terms of criteria, such as the utility, risk, or interest. Besides, another limitation of frequent patterns is that they generally have a low occupancy, that is, they often represent small sets of items in transactions containing many items and, thus, may not be truly representative of these transactions. To extract high-quality patterns in real-life applications, this paper extends the occupancy measure to also assess the utility of patterns in transaction databases. We propose an efficient algorithm named high-utility occupancy pattern mining (HUOPM). It considers user preferences in terms of frequency, utility, and occupancy. A novel frequency-utility tree and two compact data structures, called the utility-occupancy list and frequency-utility table, are designed to provide global and partial downward closure properties for pruning the search space. The proposed method can efficiently discover the complete set of high-quality patterns without candidate generation. Extensive experiments have been conducted on several datasets to evaluate the effectiveness and efficiency of the proposed algorithm. Results show that the derived patterns are intelligible, reasonable, and acceptable, and that HUOPM with its pruning strategies outperforms the state-of-the-art algorithm, in terms of runtime and search space, respectively.																	2168-2267	2168-2275				MAR	2020	50	3					1195	1208		10.1109/TCYB.2019.2896267													
J								Mission Aware Motion Planning (MAP) Framework With Physical and Geographical Constraints for a Swarm of Mobile Stations	IEEE TRANSACTIONS ON CYBERNETICS										Planning; Resource management; Cascading style sheets; Unmanned aerial vehicles; Collision avoidance; Trajectory; Land vehicles; Adaptive density estimation (ADE); mission awareness; motion planning; obstacle avoidance; unmanned aerial vehicle (UAV); unmanned ground vehicle (UGV)	WIRELESS SENSOR NETWORKS; STRATEGIES; PLACEMENT	In this paper, we propose a mission aware motion planning (MAP) framework for a swarm of autonomous unmanned ground vehicles (UGVs) or mobile stations in an uncertain environment for efficient supply of resources/services to unmanned aerial vehicles (UAVs) performing a specific mission. The MAP framework consists of two levels, namely, centralized mission planning and decentralized motion planning. On the first level, the centralized mission planning algorithm estimates the density of UAV in a given environment for determining the number of UGVs and their initial operating location. In the subsequent level, a decentralized motion planning algorithm which provides a closed-form expression for velocity command using adaptive density estimation has been proposed. Further, the physical and geographical constraints are integrated into motion planning. A Monte-Carlo simulation is performed to evaluate the advantages of the MAP over distributed stationary stations (DSSs) often used in the literature. The obtained results clearly indicate that in comparison with DSS, MAP reduces the average distance traveled by UAVs about 20%, reduces the loss of mission time by 90 s per interruption and power loss by 3 dB.																	2168-2267	2168-2275				MAR	2020	50	3					1209	1219		10.1109/TCYB.2019.2897027													
J								Secure Distributed Finite-Time Filtering for Positive Systems Over Sensor Networks Under Deception Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Deception attacks; distributed finite-time filtering; linear programming; network-induced constraints; positive systems; sensor networks	JUMP LINEAR-SYSTEMS; PERFORMANCE; CONSENSUS; DESIGN; DELAYS	This paper is concerned with secure ${\ell _{1}}$ -gain performance analysis and distributed finite-time filter design for a positive discrete-time linear system over a sensor network in the presence of deception attacks. A group of intercommunicating sensors is densely deployed to measure, gather, and process the output of the positive system. Each sensor is capable of sharing its measurement with its neighboring sensors in accordance with a prescribed network topology while suffering from random communication link failure. Meanwhile, the aggregated measurement on each sensor during network transmission is corrupted by stochastic deception attacks which compromise the sensor's measurement integrity. First, a unified sensor measurement transmission model is put forward to account for the simultaneous presence of deception attacks and various network-induced constraints. Second, delicate secure distributed filters are constructed by admitting the corrupted sensor measurement. Third, theoretical analysis on finite-time ${\ell _{1}}$ -gain boundedness of the filtering error system and design of desired positive filters are carried out. The solution to the filter gain parameters is characterized by a set of linear programming inequalities. Finally, the effectiveness of the obtained results is verified through the secure monitoring of power distribution in the smart grid.																	2168-2267	2168-2275				MAR	2020	50	3					1220	1229		10.1109/TCYB.2019.2900478													
J								A Generic Human-Machine Annotation Framework Based on Dynamic Cooperative Learning	IEEE TRANSACTIONS ON CYBERNETICS										Uncertainty; Task analysis; Reliability; Heuristic algorithms; Training; Labeling; Computational modeling; Human-machine systems; active learning; semi-supervised learning; confidence measures; inter-rater agreement	INTERRATER RELIABILITY	The task of obtaining meaningful annotations is a tedious work, incurring considerable costs and time consumption. Dynamic active learning and cooperative learning are recently proposed approaches to reduce human effort of annotating data with subjective phenomena. In this paper, we introduce a novel generic annotation framework, with the aim to achieve the optimal tradeoff between label reliability and cost reduction by making efficient use of human and machine work force. To this end, we use dropout to assess model uncertainty and thereby to decide which instances can be automatically labeled by the machine and which ones require human inspection. In addition, we propose an early stopping criterion based on inter-rater agreement in order to focus human resources on those ambiguous instances that are difficult to label. In contrast to the existing algorithms, the new confidence measures are not only applicable to binary classification tasks but also regression problems. The proposed method is evaluated on the benchmark datasets for non-native English prosody estimation, provided in the Interspeech computational paralinguistics challenge. In the result, the novel dynamic cooperative learning algorithm yields 0.424 Spearman's correlation coefficient compared to 0.413 with passive learning, while reducing the amount of human annotations by 74%.																	2168-2267	2168-2275				MAR	2020	50	3					1230	1239		10.1109/TCYB.2019.2901499													
J								Resilient and Robust Synchronization of Multiagent Systems Under Attacks on Sensors and Actuators	IEEE TRANSACTIONS ON CYBERNETICS										Distributed adaptive compensator; faulty agents; H-infinity control; malicious agents; resilient synchronization	CYBER-PHYSICAL SYSTEMS; H-INFINITY CONSENSUS; NONLINEAR-SYSTEMS; DISTRIBUTED CONTROL; STATE-FEEDBACK; PERFORMANCE; DESIGN; NETWORK; AGENTS	Resilient and robust distributed control protocols for multiagent systems under attacks on sensors and actuators are designed. A distributed H-infinity control protocol is designed to attenuate the disturbance or attack effects. However, the H-infinity controller is too conservative in the presence of attacks. Therefore, it is augmented with a distributed adaptive compensator to mitigate the adverse effects of attacks. The proposed controller can make the synchronization error arbitrarily small in the presence of faulty attacks, and satisfy global L-2-gain performance in the presence of malicious attacks or disturbances. A significant advantage of the proposed method is that it requires no restriction on the number of agents or agents' neighbors under attacks on sensors and/or actuators, and it recovers even compromised agents under attacks on actuators. Simulation examples verify the effectiveness of the proposed method.																	2168-2267	2168-2275				MAR	2020	50	3					1240	1250		10.1109/TCYB.2019.2903411													
J								Preview-Based Discrete-Time Dynamic Formation Control Over Directed Networks via Matrix-Valued Laplacian	IEEE TRANSACTIONS ON CYBERNETICS										Shape; Laplace equations; Multi-agent systems; Task analysis; Position measurement; Information exchange; Dynamic environment; formation control; graph Laplacian; multiagent system; preview action	MULTIAGENT SYSTEMS; CONSENSUS; AGENTS; TRACKING	This paper studies the dynamic formation control problem for cooperative agents with discrete-time dynamics over directed graphs. Unlike using absolute coordinate, relative coordinate, interagent distance, or interagent bearing to specify the target formation and coordinate agents to achieve the formation, we study a coordination problem where the desired formation varies with time and only its geometric shape is predefined. Matrix-valued Laplacian approach has been adopted to address this problem in the continuous-time setting. However, the discrete-time counterpart is more challenging due to the constraint in information exchange. On the other hand, observe that in many real operations, at a given time instant, the agents will be able to plan their target formation configurations for a period of time ahead. We propose preview-based P-like and PD-like controllers for the formation control. The controllers with proper parameter setting are proved to be effective to address the dynamic formation control problem. Numerical simulations are given to validate the effectiveness of the proposed controllers.																	2168-2267	2168-2275				MAR	2020	50	3					1251	1263		10.1109/TCYB.2019.2910534													
J								A Quantum-Inspired Similarity Measure for the Analysis of Complete Weighted Graphs	IEEE TRANSACTIONS ON CYBERNETICS										Kernel; Quantum computing; Weight measurement; Image edge detection; Time series analysis; Entropy; Laplace equations; Financial networks; graph kernels; graph similarity; Jensen-Shannon divergence; quantum walks	TIME-SERIES; KERNELS; WALKS; COSPECTRALITY; PREDICTION; NETWORK	We develop a novel method for measuring the similarity between complete weighted graphs, which are probed by means of the discrete-time quantum walks. Directly probing complete graphs using discrete-time quantum walks is intractable due to the cost of simulating the quantum walk. We overcome this problem by extracting a commute time minimum spanning tree from the complete weighted graph. The spanning tree is probed by a discrete-time quantum walk which is initialized using a weighted version of the Perron-Frobenius operator. This naturally encapsulates the edge weight information for the spanning tree extracted from the original graph. For each pair of complete weighted graphs to be compared, we simulate a discrete-time quantum walk on each of the corresponding commute time minimum spanning trees and, then, compute the associated density matrices for the quantum walks. The probability of the walk visiting each edge of the spanning tree is given by the diagonal elements of the density matrices. The similarity between each pair of graphs is then computed using either: 1) the inner product or 2) the negative exponential of the Jensen-Shannon divergence between the probability distributions. We show that in both cases the resulting similarity measure is positive definite and, therefore, corresponds to a kernel on the graphs. We perform a series of experiments on publicly available graph datasets from a variety of different domains, together with time-varying financial networks extracted from data for the New York Stock Exchange. Our experiments demonstrate the effectiveness of the proposed similarity measures.																	2168-2267	2168-2275				MAR	2020	50	3					1264	1277		10.1109/TCYB.2019.2913038													
J								Hierarchical Distributed Control for Global Network Integrity Preservation in Multirobot Systems	IEEE TRANSACTIONS ON CYBERNETICS										Robot sensing systems; Decentralized control; Maintenance engineering; Mobile robots; Estimation; Behavioral control; distributed connectivity control; distributed node control; global network integrity preservation; hierarchical distributed control (HDC); local connectivity minimization strategy; multirobot systems (MRSs)	CONNECTIVITY MAINTENANCE; COOPERATIVE CONTROL; SENSOR NETWORKS; COORDINATION	In this paper, we address a novel hierarchical distributed control (HDC) strategy for networked multirobot systems (MRSs). This strategy is developed on a geometric approach without requiring estimation of algebraic connectivity. It is originally based upon behavioral control, but upgraded by distributed node control with a mobility constraint for global network integrity preservation and distributed connectivity control with a local connectivity minimization strategy for network coverage expansion. Thanks to properties of HDC, a networked MRS is capable of achieving high performance with cooperative tasks. We have examined and evaluated our proposed method in both simulations with up to 100 simulated robots and real-world experiments with up to 14 real robots.																	2168-2267	2168-2275				MAR	2020	50	3					1278	1291		10.1109/TCYB.2019.2913326													
J								Task-Oriented Feature-Fused Network With Multivariate Dataset for Joint Face Analysis	IEEE TRANSACTIONS ON CYBERNETICS										Face; Task analysis; Training; Face recognition; Facial features; Pipelines; Attribute analysis; face analysis; face detection; landmark localization; multitask learning		Deep multitask learning for face analysis has received increasing attentions. From literature, most existing methods focus on optimizing a main task by jointly learning several auxiliary tasks. It is challenging to consider the performance of each task in a multitask framework due to the following reasons: 1) different face tasks usually rely on different levels of semantic features; 2) each task has different learning convergence rate, which could affect the whole performance when joint training; and 3) multitask model needs rich label information for efficient training, but existing facial datasets provide limited annotations. To address these issues, we propose a task-oriented feature-fused network (TFN) for simultaneously solving face detection, landmark localization, and attribute analysis. In this network, a task-oriented feature-fused block is designed to learn task-specific feature combinations; then, an alternative multitask training scheme is presented to optimize each task with considering of their different learning capacities. We also present a large-scale face dataset called JFA in support of proposed method, which provides multivariate labels, including face bounding box, 68 facial landmarks, and 3 attribute labels (i.e., apparent age, gender, and ethnicity). The experimental results suggest that the TFN outperforms several multitask models on the JFA dataset. Furthermore, our approach achieves competitive performances on WIDER FACE and 300W dataset, and obtains state-of-the-art results for gender recognition on the MORPH II dataset.																	2168-2267	2168-2275				MAR	2020	50	3					1292	1305		10.1109/TCYB.2019.2917049													
J								Distributed Event-Triggered Estimation Over Sensor Networks: A Survey	IEEE TRANSACTIONS ON CYBERNETICS										Estimation; Monitoring; Sensors; Reliability; Smart grids; Information processing; Germanium; Attack detection; communication constraint; distributed estimation; event-triggered mechanism; islanding detection; secure estimation; sensor networks; set-membership filtering; target tracking	ISLANDING DETECTION METHOD; KALMAN-CONSENSUS FILTER; JUMP LINEAR-SYSTEMS; H-INFINITY; STATE ESTIMATION; STOCHASTIC-SYSTEMS; PREDICTIVE CONTROL; PERFORMANCE ANALYSIS; COOPERATIVE CONTROL; MULTIAGENT SYSTEMS	An event-triggered mechanism is of great efficiency in reducing unnecessary sensor samplings/transmissions and, thus, resource consumption such as sensor power and network bandwidth, which makes distributed event-triggered estimation a promising resource-aware solution for sensor network-based monitoring systems. This paper provides a survey of recent advances in distributed event-triggered estimation for dynamical systems operating over resource-constrained sensor networks. Local estimates of an unavailable state signal are calculated in a distributed and collaborative fashion based on only invoked sensor data. First, several fundamental issues associated with the design of distributed estimators are discussed in detail, such as estimator structures, communication constraints, and design methods. Second, an emphasis is laid on recent developments of distributed event-triggered estimation that has received considerable attention in the past few years. Then, the principle of an event-triggered mechanism is outlined and recent results in this subject are sorted out in accordance with different event-triggering conditions. Third, applications of distributed event-triggered estimation in practical sensor network-based monitoring systems including distributed grid-connected generation systems and target tracking systems are provided. Finally, several challenging issues worthy of further research are envisioned.																	2168-2267	2168-2275				MAR	2020	50	3					1306	1320		10.1109/TCYB.2019.2917179													
J								Estimation and Observability Analysis of Human Motion on Lie Groups	IEEE TRANSACTIONS ON CYBERNETICS										Position measurement; Observability; Kalman filters; Kinematics; Estimation; Acceleration; Geometry; Human body kinematics; inertial measurement units (IMUs); marker measurements; motion estimation on Lie groups; observability analysis	KALMAN FILTER; LOCALIZATION; TRACKING; UNCERTAINTY; ALGORITHM; DESIGN	This article proposes a framework for human-pose estimation from the wearable sensors that rely on a Lie group representation to model the geometry of the human movement. Human body joints are modeled by matrix Lie groups, using special orthogonal groups SO(2) and SO(3) for joint pose and special Euclidean group SE(3) for base-link pose representation. To estimate the human joint pose, velocity, and acceleration, we develop the equations for employing the extended Kalman filter on Lie groups (LG-EKF) to explicitly account for the non-Euclidean geometry of the state space. We present the observability analysis of an arbitrarily long kinematic chain of SO(3) elements based on a differential geometric approach, representing a generalization of kinematic chains of a human body. The observability is investigated for the system using marker position measurements. The proposed algorithm is compared with two competing approaches: 1) the extended Kalman filter (EKF) and 2) unscented KF (UKF) based on the Euler angle parametrization, in both simulations and extensive real-world experiments. The results show that the proposed approach achieves significant improvements over the Euler angle-based filters. It provides more accurate pose estimates, is not sensitive to gimbal lock, and more consistently estimates the covariances.																	2168-2267	2168-2275				MAR	2020	50	3					1321	1332		10.1109/TCYB.2019.2933390													
J								Learning Optimized Structure of Neural Networks by Hidden Node Pruning With L-1 Regularization	IEEE TRANSACTIONS ON CYBERNETICS										Convergence; neural network; pruning; regularization; smoothing approximation	GROUP LASSO; MULTILAYER PERCEPTRON; CONVERGENCE ANALYSIS; WEIGHT NOISE; ALGORITHM; CLASSIFICATION; REGRESSION; SELECTION; MACHINE	We propose three different methods to determine the optimal number of hidden nodes based on L-1 regularization for a multilayer perceptron network. The first two methods, respectively, use a set of multiplier functions and multipliers for the hidden-layer nodes and implement the L-1 regularization on those, while the third method equipped with the same multipliers uses a smoothing approximation of the L-1 regularization. Each of these methods begins with a given number of hidden nodes, then the network is trained to obtain an optimal architecture discarding redundant hidden nodes using the multiplier functions or multipliers. A simple and generic method, namely, the matrixbased convergence proving method (MCPM), is introduced to prove the weak and strong convergence of the presented smoothing algorithms. The performance of the three pruning methods has been tested on 11 different classification datasets. The results demonstrate the efficient pruning abilities and competitive generalization by the proposed methods. The theoretical results are also validated by the results.																	2168-2267	2168-2275				MAR	2020	50	3					1333	1346		10.1109/TCYB.2019.2950105													
J								Emergent Inference of Hidden Markov Models in Spiking Neural Networks Through Winner-Take-All	IEEE TRANSACTIONS ON CYBERNETICS										Hidden Markov models; Biological neural networks; Mathematical model; Neurons; Cybernetics; Markov processes; Brain modeling; Hidden Markov models (HMMs); neural implementation; posterior inference; spiking neural network; winner-take-all (WTA) circuits	NEURONS; UNCERTAINTY; BRAIN; COMPUTATION; ADAPTATION; PLASTICITY; CIRCUITS	Hidden Markov models (HMMs) underpin the solution to many problems in computational neuroscience. However, it is still unclear how to implement inference of HMMs with a network of neurons in the brain. The existing methods suffer from the problem of being nonspiking and inaccurate. Here, we build a precise equivalence between the inference equation of HMMs with time-invariant hidden variables and the dynamics of spiking winner-take-all (WTA) neural networks. We show that the membrane potential of each spiking neuron in the WTA circuit encodes the logarithm of the posterior probability of the hidden variable in each state, and the firing rate of each neuron is proportional to the posterior probability of the HMMs. We prove that the time course of the neural firing rate can implement posterior inference of HMMs. Theoretical analysis and experimental results show that the proposed WTA circuit can get accurate inference results of HMMs.																	2168-2267	2168-2275				MAR	2020	50	3					1347	1354		10.1109/TCYB.2018.2871144													
J								Optimal State Estimation of Boolean Control Networks With Stochastic Disturbances	IEEE TRANSACTIONS ON CYBERNETICS										State estimation; Stochastic processes; Probability distribution; Euclidean distance; Iterative methods; Cybernetics; Boolean control network; maximum posterior estimation; minimum mismatching estimation; semitensor product (STP) of matrices; stochastic disturbance	COMPLETE SYNCHRONIZATION; FEEDBACK STABILIZATION; CONTROLLABILITY; OBSERVABILITY; STABILITY	This paper presents an investigation of the optimal estimation of state for Boolean control networks subject to stochastic disturbances. The disturbances are modeled as independently and identically distributed processes that are assumed to be both mutually independent and independent of the current and the historical states. An iterative algorithm is proposed to calculate the conditional probability distribution of the state given the output measurements. This algorithm is applied to the problems of minimum mismatching estimation and maximum posterior estimation of the state. An example is provided to illustrate the proposed results.																	2168-2267	2168-2275				MAR	2020	50	3					1355	1359		10.1109/TCYB.2018.2885124													
J								Sophisticated collective foraging with minimalist agents: a swarm robotics test	SWARM INTELLIGENCE										Foraging; Swarm robotics; Stigmergy; Kilobot; Augmented reality; Traffic congestion	DECISION-MAKING; PHEROMONE COMMUNICATION; ANTS; FOOD; TRAIL; ORGANIZATION; FLEXIBILITY; RECRUITMENT; COLONY; MODEL	How groups of cooperative foragers can achieve efficient and robust collective foraging is of interest both to biologists studying social insects and engineers designing swarm robotics systems. Of particular interest are distance-quality trade-offs and swarm-size-dependent foraging strategies. Here, we present a collective foraging system based on virtual pheromones, tested in simulation and in swarms of up to 200 physical robots. Our individual agent controllers are highly simplified, as they are based on binary pheromone sensors. Despite being simple, our individual controllers are able to reproduce classical foraging experiments conducted with more capable real ants that sense pheromone concentration and follow its gradient. One key feature of our controllers is a control parameter which balances the trade-off between distance selectivity and quality selectivity of individual foragers. We construct an optimal foraging theory model that accounts for distance and quality of resources, as well as overcrowding, and predicts a swarm-size-dependent strategy. We test swarms implementing our controllers against our optimality model and find that, for moderate swarm sizes, they can be parameterised to approximate the optimal foraging strategy. This study demonstrates the sufficiency of simple individual agent rules to generate sophisticated collective foraging behaviour.																	1935-3812	1935-3820				MAR	2020	14	1			SI		25	56		10.1007/s11721-019-00176-9													
J								Tooth morphometry using quasi-conformal theory	PATTERN RECOGNITION										Tooth morphometry; Quasi-conformal theory; Shape analysis; Teichmuller map; Ancestry; Sexual dimorphism; Classification	MILD COGNITIVE IMPAIRMENT; SEXUAL-DIMORPHISM; SHAPE-ANALYSIS; PARAMETERIZATION; SEGMENTATION; REGISTRATION; DISEASE	Shape analysis is important in anthropology, bioarchaeology and forensic science for interpreting useful information from human remains. In particular, teeth are morphologically stable and hence well-suited for shape analysis. In this work, we propose a framework for tooth morphometry using quasi-conformal theory. Landmark-matching Teichmuller maps are used for establishing a 1-1 correspondence between tooth surfaces with prescribed anatomical landmarks. Then, a quasi-conformal statistical shape analysis model based on the Teichmuller mapping results is proposed for building a tooth classification scheme. We deploy our framework on a dataset of human premolars to analyze the tooth shape variation among genders and ancestries. Experimental results show that our method achieves much higher classification accuracy with respect to both gender and ancestry when compared to the existing methods. Furthermore, our model reveals the underlying tooth shape difference between different genders and ancestries in terms of the local geometric distortion and curvatures. In particular, our experiment suggests that the shape difference between genders is mostly captured by the conformal distortion but not the curvatures, while that between ancestries is captured by both of them. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107064	10.1016/j.patcog.2019.107064													
J								Efficient nearest neighbor search in high dimensional hamming space	PATTERN RECOGNITION										Binary feature; Feature matching; Approximate nearest neighbor search; Scalable image matching	ALGORITHM	Fast approximate nearest neighbor search has been well studied for real-valued vectors, however, the methods for binary descriptors are less developed. The paper addresses this problem by resorting to the well established techniques in Euclidean space. To this end, the binary descriptors are firstly mapped into low dimensional float vectors under the condition that the neighborhood information in the original Hamming space could be preserved in the mapped Euclidean space as much as possible. Then, KD-Tree is used to partitioning the mapped Euclidean space in order to quickly find approximate nearest neighbors for a given query point. This is identical to filter out a subset of nearest neighbor candidates in the original Hamming space due to the property of neighborhood preserving. Finally, Hamming ranking is applied to the small number of candidates to find out the approximate nearest neighbor in the original Hamming space, with only a fraction of running time compared to the bruteforce linear scan. Our experiments demonstrate that the proposed method significantly outperforms the state of the arts, obtaining improved search accuracy at various speed up factors, e.g., at least 16% improvement of search accuracy over previous methods (from 67.7% to 83.7%) when the search speed is 200 times faster than the linear scan for a one million database. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107082	10.1016/j.patcog.2019.107082													
J								Estimation of ergodicity limits of bag-of-words modeling for guaranteed stochastic convergence	PATTERN RECOGNITION										Bag-of-words; Ergodicity; Statistical modeling; Stochastic process	IMAGE CLASSIFICATION; FEATURES; RECOGNITION	This paper suggests an efficient dual ergodicity limits-based bag-of-words (DEL-BoW) modeling technique. The suggested DEL-BoW technique estimates two limits of ergodicity of a discrete random variable (drv) that is formed from the BoW classification performance of multiple runs. The first limit of ergodicity is estimated with a relatively larger ball of convergence to keep the drv shorter. Hence both robustness against random initialization and estimation of the optimal model-order are realized with a reduced number of iterations. Once the optimal model-order is estimated, the radius of ball of convergence is reduced and a second limit of ergodicity is estimated. Reducing the ball of convergence enlarges the size of the considered performance drv that enhances the classification performance. Experiments conducted on Caltech-101, Caltech-256, 15-Scenes, and Flower-102 datasets resulted in classification accuracy of 86.91%, 72.57%, 90.57%, and 90.86%, respectively. Comparison with state-of-the-art techniques shows the excellent performance of the DEL-BoW modeling process. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107094	10.1016/j.patcog.2019.107094													
J								Dynamic auto-weighted multi-view co-clustering	PATTERN RECOGNITION										Multi-view co-clustering; Information bottleneck; Weighting	SPARSE	To exploit the complementary information of multi-view data, many weighted multi-view clustering methods have been proposed and have demonstrated impressive performance. However, most of these methods learn the view weights by introducing additional parameters, which can not be easily obtained in practice. Moreover, they all simply apply the learned weights on the original feature representation of each view, which may deteriorate the clustering performance in the case of high-dimensional data with redundancy and noise. In this paper, we extend information bottleneck co-clustering into a multi-view framework and propose a novel dynamic auto-weighted multi-view co-clustering algorithm to learn a group of weights for views with no need for extra weight parameters. By defining the new concept of the discrimination-compression rate, we quantify the importance of each view by evaluating the discriminativeness of the compact features (i.e., feature-wise clusters) of the views. Unlike existing weighted methods that impose weights on the original feature representations of multiple views, we apply the learned weights on the discriminative ones, which can reduce the negative impact of noisy features in high-dimensional data. To solve the optimization problem, a new two-step sequential method is designed. Experimental results on several datasets show the advantages of the proposed algorithm. To our knowledge, this is the first work incorporating weighting scheme into multi-view co-clustering framework. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107101	10.1016/j.patcog.2019.107101													
J								Metric learning-based kernel transformer with triplets and label constraints for feature fusion	PATTERN RECOGNITION										Feature fusion; Kernel transformer; Kernel metric learning; LogDet divergence	IMAGE; MATRIX	Feature fusion is an important skill to improve the performance in computer vision, the difficult problem of feature fusion is how to learn the complementary properties of different features. We recognize that feature fusion can benefit from kernel metric learning. Thus, a metric learning-based kernel transformer method for feature fusion is proposed in this paper. First, we propose a kernel transformer to convert data from data space to kernel space, which makes feature fusion and metric learning can be performed in the transformed kernel space. Second, in order to realize supervised learning, both triplets and label constraints are embedded into our model. Third, in order to solve the unknown kernel matrices, LogDet divergence is also introduced into our model. Finally, a complete optimization objective function is formed. Based on an alternating direction method of multipliers (ADMM) solver and the Karush-Kuhn-Tucker (KKT) theorem, the proposed optimization problem is solved with the rigorous theoretical analysis. Experimental results on image retrieval demonstrate the effectiveness of the proposed methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107086	10.1016/j.patcog.2019.107086													
J								Heterogeneous oblique random forest	PATTERN RECOGNITION										Benchmarking; Classifiers; Oblique random forest; Heterogeneous; One-vs-all; Ensemble learning	DECISION FORESTS; CLASSIFICATION; CLASSIFIERS; REGRESSION; ENSEMBLE	Decision trees in random forests use a single feature in non-leaf nodes to split the data. Such splitting results in axis-parallel decision boundaries which may fail to exploit the geometric structure in the data. In oblique decision trees, an oblique hyperplane is employed instead of an axis-parallel hyperplane. Trees with such hyperplanes can better exploit the geometric structure to increase the accuracy of the trees and reduce the depth. The present realizations of oblique decision trees do not evaluate many promising oblique splits to select the best. In this paper, we propose a random forest of heterogeneous oblique decision trees that employ several linear classifiers at each non-leaf node on some top ranked partitions which are obtained via one-vs-all and two-hyperclasses based approaches and ranked based on ideal Gini scores and cluster separability. The oblique hyperplane that optimizes the impurity criterion is then selected as the splitting hyperplane for that node. We benchmark 190 classifiers on 121 UCI datasets. The results show that the oblique random forests proposed in this paper are the top 3 ranked classifiers with the heterogeneous oblique random forest being statistically better than all 189 classifiers in the literature. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107078	10.1016/j.patcog.2019.107078													
J								Learning disentangling and fusing networks for face completion under structured occlusions	PATTERN RECOGNITION											ADVERSARIAL NETWORK	Face completion aims to generate semantically new pixels for missing facial components. It is a challenging generative task due to large variations of face appearance. This paper studies generative face completion under structured occlusions. We treat the face completion and corruption as disentangling and fusing processes of clean faces and occlusions, and propose a jointly disentangling and fusing Generative Adversarial Network (DF-GAN). First, three domains are constructed, corresponding to the distributions of occluded faces, clean faces and structured occlusions. The disentangling and fusing processes are formulated as the transformations between the three domains. Then the disentangling and fusing networks are built to learn the transformations from unpaired data, where the encoder-decoder structure is adopted and allows DF-GAN to simulate structure occlusions by modifying the latent representations. Finally, the disentangling and fusing processes are unified into a dual learning framework along with an adversarial strategy. The proposed method is evaluated on Meshface verification problem. Experimental results on four Meshface databases demonstrate the effectiveness of our proposed method for the face completion under structured occlusions. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107073	10.1016/j.patcog.2019.107073													
J								Depth-map completion for large indoor scene reconstruction	PATTERN RECOGNITION										Depth completion; MVS; 3D Reconstruction; Point cloud	MULTIVIEW STEREO; DEEP	Traditional Multi View Stereo (MVS) algorithms are often difficult to deal with large-scale indoor scene reconstruction, due to the photo-consistency measurement errors in weak textured regions, which are commonly exist in indoor scenes. To solve this limitation, in this paper we proposed a point cloud completion strategy that combines learning-based depth-map completion and geometry-based consistency filtering to fill large-area missing in depth-maps. The proposed method takes nonuniform and noisy MVS depth-map as input, and completes each depth-map individually. In the completion process, we first complete depth-maps using learning based method, and then filter each depth-map using depth consistency validation with its neighboring depth-maps. This depth-map completion and geometric filtering steps are performed iteratively until the number of depth points is converged. Experiments on large-scale indoor scenes and benchmark MVS datasets demonstrate the effectiveness of the proposed methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107112	10.1016/j.patcog.2019.107112													
J								Video classification and retrieval through spatio-temporal Radon features	PATTERN RECOGNITION										Bagged trees classification model; Linear discriminant analysis; Radon Projections; Spatio temporal feature; Video classification and retrieval	HUMAN ACTION RECOGNITION	The rise in the availability of video content for access via the Internet and the medium of television has resulted in the development of automatic search procedures to retrieve the desired video. Searches can be simplified and hastened by employing automatic classification of videos. This paper proposes a descriptor called the Spatio-Temporal Histogram of Radon Projections (STHRP) for representing the temporal pattern of the contents of a video and demonstrates its application to video classification and retrieval. The first step in STHRP pattern computation is to represent any video as Three Orthogonal Planes (TOPs), i.e., XY, XT and YT, signifying the spatial and temporal contents. Frames corresponding to each plane are partitioned into overlapping blocks. Radon projections are obtained over these blocks at different orientations, resulting in weighted transform coefficients that are normalized and grouped into bins. Linear Discriminant Analysis (LDA) is performed over these coefficients of the TOPs to arrive at a compact description of STHRP pattern. Compared to existing classification and retrieval approaches, the proposed descriptor is highly robust to translation, rotation and illumination variations in videos. To evaluate the capabilities of the invariant STHRP pattern, we analyse the performance by conducting experiments on the UCF-101, HMDB51, 10contexts and TRECVID data sets for classification and retrieval using a bagged tree model. Experimental evaluation of video classification reveals that STHRP pattern can achieve classification rates of 96.15%, 71.7%, 93.24% and 97.3% for the UCF-101, HMDB51,10contexts and TRECVID 2005 data sets respectively. We conducted retrieval experiments on the TRECVID 2005, JHMDB and 10contexts data sets and the results revealed that STHRP pattern is able to provide the videos relevant to the user's query in minimal time (0.05s) with a good precision rate. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107099	10.1016/j.patcog.2019.107099													
J								Collaborative and geometric multi-kernel learning for multi-class classification	PATTERN RECOGNITION										Multi-class classification; Empirical kernel mapping; Multiple empirical kernel learning; Regularized learning	KERNEL; SELECTION	The multi-class classification is the problem of classifying the sample into one of three or more classes. In this paper, we propose an algorithm named collaborative and geometric multi-kernel learning (CGMKL) to classify multi-class data into corresponding class directly. The CGMKL uses the Multiple Empirical Kernel Learning (MEKL) to map the sample into multiple kernel spaces, and then trains the softmax function in each kernel space. To realize the collaborative learning, one regularization term, which controls the consistent outputs of samples in different kernel spaces, provides the complementary information. Moreover, another regularization term exhibits the classification result with a geometric feature by reducing the within-class distance of the outputs of samples. Extensive Experiments on the multi-class data sets validate the effectiveness of the CGMKL. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107050	10.1016/j.patcog.2019.107050													
J								Fault recognition using an ensemble classifier based on Dempster-Shafer Theory	PATTERN RECOGNITION										Fault recognition; Ensemble classifier; Dempster-Shafer Theory; Correlation entropy; Evidence weight	COMBINATION; SELECTION; FRAMEWORK	Aiming at the poor performance of individual classifier in the field of fault recognition, in this paper, a new ensemble classifier is constructed to improve the classification accuracy by combining multiple classifiers based on Dempster-Shafer Theory (DST). However, in some specific cases, especially when dealing with the combination of conflicting evidences, the DST may produce counter-intuitive results and loss its advantages in combining classifiers. To solve this problem, a new improved combination method is proposed to alleviate the conflicts between evidences and a new ensemble technique is developed for the combination of individual classifiers, which can be well used in the design of accurate classifier ensembles. The main advantage of the proposed combination method is that of making the combination process more efficient and accurate by defining the objective weights and subjective weights of member classifiers outputs. To verify the effectiveness of the proposed combination method, four individual classifiers are selected for constructing ensemble classifier and tested on Tennessee-Eastman Process (TEP) datasets and UCI machine learning datasets. The experimental results show that the ensemble classifier can significantly improve the classification accuracy and outperforms all the selected individual classifiers. By comparison with other combination methods based on DST and some state-of-the-art ensemble methods, the proposed combination method shows better abilities in dealing with the combination of individual classifiers and outperforms the others in multiple performance measurements. Finally, the proposed ensemble classifier is applied to the fault recognition in real chemical plant. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107079	10.1016/j.patcog.2019.107079													
J								Multi-model ensemble with rich spatial information for object detection	PATTERN RECOGNITION										Ensemble learning; Object detection; Dilated convolution; Feature fusion		Due to the development of deep learning networks and big data dimensionality, research on ensemble deep learning is receiving an increasing amount of attention. This paper takes the object detection task as the research domain and proposes an object detection framework based on ensemble deep learning. To guarantee the accuracy as well as real-time detection, the detector uses a Single Shot MultiBox Detector (SSD) as the backbone and combines ensemble learning with context modeling and multi-scale feature representation. Two modes were designed in order to achieve ensemble learning: NMS Ensembling and Feature Ensembling. In addition, to obtain contextual information, we used dilated convolution to expand the receptive field of the network. Compared with state-of-the-art detectors, our detector achieves superior performance on the PASCAL VOC set and the MS COCO set. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107098	10.1016/j.patcog.2019.107098													
J								Large-scale multi-label classification using unknown streaming images	PATTERN RECOGNITION										Multi-label image classification; Recurrent novel-class detector; Streaming images		In this paper, we investigate the large-scale multi-label image classification problem when images with unknown novel classes come in stream during the training stage. It coincides with the practical requirement that usually novel classes are detected and used to update an existing image recognition system. Most existing multi-label image classification methods cannot be directly applied in this scenario, where the training and testing stages must have the same label set. In this paper, we proposed to learn a multi-label classifier and a novel-class detector alternately to solve this problem. The multi-label classifier is learned using a convolutional neural network (CNN) from the images in the known classes. We proposed a recurrent novel-class detector which is learned in the supervised manner to detect the novel class by encoding image features with the multi-label information. In the experiment, our method is evaluated on several large-scale multi-label benchmarks including MS COCO. The results show the proposed method is comparable to most existing multi-label image classification methods, which validate its efficacy when encountering streaming images with unknown classes. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAR	2020	99								107100	10.1016/j.patcog.2019.107100													
J								Towards sophisticated decision models: Nonadditive robust ordinal regression for preference modeling	KNOWLEDGE-BASED SYSTEMS										Fuzzy sets; Multicriteria decision making; Fuzzy measure; Ordinal regression; Aggregation functions	INTERACTING CRITERIA; FUZZY MEASURES; AGGREGATION; CONSTRUCTION; CAPACITIES; FAMILIES; CONTEXT; INDEX; SUM; SET	This paper addresses a methodology for decision support under multiple and correlated decision criteria. Nonadditive robust ordinal regression (NAROR) aims to build capacities that fit the decision makers explicit preferences and pairwise rankings of some alternatives. The capacities provide great flexibility to model decision problems accounting for interactions among the decision criteria. The feasible set of capacities helps identifying all the necessary and possible dominance relations among all the decision alternatives. In this paper we enhance the NAROR method by identifying optimal capacities through entropy maximisation. We formulate suitable optimisation problems and provide avenues for capacity simplification based on k-interactivity. We also consider the situation of large number of sparse constraints, for which we formulate a linear program based on Renyi entropy. We deal with preferences inconsistency by using multiple goal linear programming technique. The results show that the k-interactivity is an efficient way to reduce the complexity of capacities while preserving their expressiveness and representation ability, and that optimal capacities can be found by standard mathematical programming techniques. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105351	10.1016/j.knosys.2019.105351													
J								A context-aware embeddings supported method to extract a fuzzy sentiment polarity dictionary	KNOWLEDGE-BASED SYSTEMS										Sentiment analysis; Polarity extraction; Word embeddings; Information retrieval; Contextual bias; Fuzzy polarity	MODEL; NEWS; RETURN	The latest development in cognitive technologies are helping us understand emotions and sentiments with unprecedented precision. Polarity detection is the key enabler to sentiment analysis and typically relies on experimental dictionaries, where terms are assigned polarity scores, yet lacking contextual information and based on human inputs and conventions. In this article, we present a novel approach to automatically extract a polarity dictionary from a particular domain, the stock market, without human intervention and addressing the scaling and thresholding problem. Our approach tracks the price changes of particular stocks over time, using it as a guiding polarity value. The magnitude of the price variation for a particular stock is then attributed to the financial news about this stock in corresponding period of time and that is what we use as our working corpus. On top of that, we derive the so-called binned corpus and apply the well-known TF-IDF information retrieval techniques to compute the TF-IDF value for each term. These values are then disseminated within the neighbourhood of each term based on the embeddings-enabled cosine distance. After introducing the problem and providing the background information, we thoroughly describe our method and all the components required to implement the system. Last but not least, we assign the terms to fuzzy linguistic labels and provide a volatility metric indicating how reliable our scores are depending on their distribution of occurrences in the corpus. To show how our approach works, we implement it for the Euro Stoxx 50 from January 2018 to March 2019 and discuss the results compared with typical approaches, pointing out potential improvements for further research work. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105236	10.1016/j.knosys.2019.105236													
J								Machine learning based decision making for time varying systems: Parameter estimation and performance optimization	KNOWLEDGE-BASED SYSTEMS										Machine learning; Model predictive control; Time varying system	MODEL-PREDICTIVE CONTROL; SUPPLIER SELECTION; REGRESSION; OPERATION	The class of decision making problems focuses on the optimization of single or multiple design objectives, and the classical decision making procedures require the full scope of the system information. However, the system dynamics consist of unknown time varying parameters within a specific range of dynamic decision making problems, which cannot be handled by the classical procedures. To solve these problems, this paper proposes a machine learning based decision making algorithm. It uses the technique of machine learning to estimate the real-time unknown parameters using the recorded system data, and makes appropriate decisions using model predictive control (MPC) method to optimize some desired key performance indicators (KPIs). The effective performance of the proposed algorithm is further evaluated using a simulation based case study. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105479	10.1016/j.knosys.2020.105479													
J								Generative adversarial networks based on Wasserstein distance for knowledge graph embeddings	KNOWLEDGE-BASED SYSTEMS										Knowledge graph embedding; Generative adversarial networks; Wasserstein distance; Weak supervision information		Knowledge graph embedding aims to project entities and relations into low-dimensional and continuous semantic feature spaces, which has captured more attention in recent years. Most of the existing models roughly construct negative samples via a uniformly random mode, by which these corrupted samples are practically trivial for training the embedding model. Inspired by generative adversarial networks (GANs), the generator can be employed to sample more plausible negative triplets, that boosts the discriminator to improve its embedding performance further. However, vanishing gradient on discrete data is an inherent problem in traditional GANs. In this paper, we propose a generative adversarial network based knowledge graph representation learning model by introducing the Wasserstein distance to replace traditional divergence for settling this issue. Moreover, the additional weak supervision information is also absorbed to refine the performance of embedding model since these textual information contains detailed semantic description and offers abundant semantic relevance. In the experiments, we evaluate our method on the tasks of link prediction and triplet classification. The experimental results indicate that the Wasserstein distance is capable of solving the problem of vanishing gradient on discrete data and accelerating the convergence, additional weak supervision information also can significantly improve the performance of the model. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				FEB 29	2020	190								105165	10.1016/j.knosys.2019.105165													
J								Automatic construction of multi-faceted user profiles using text clustering and its application to expert recommendation and filtering problems	KNOWLEDGE-BASED SYSTEMS										Clustering; Content-based recommendation; Expert finding; Filtering; User profiling	INFORMATION-RETRIEVAL; ALGORITHMS	In the information age we are living in today, not only are we interested in accessing multimedia objects such as documents, videos, etc. but also in searching for professional experts, people or celebrities, possibly for professional needs or just for fun. Information access systems need to be able to extract and exploit various sources of information (usually in text format) about such individuals, and to represent them in a suitable way usually in the form of a profile. In this article, we tackle the problems of profile-based expert recommendation and document filtering from a machine learning perspective by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested. The experts will then be represented by means of multi-faceted profiles. Our experiments show that this is a valid technique to improve the performance of expert finding and document filtering. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105337	10.1016/j.knosys.2019.105337													
J								Senpy: A framework for semantic sentiment and emotion analysis services	KNOWLEDGE-BASED SYSTEMS										Sentiment analysis; Emotion analysis; Linked data; Classification; Evaluation		Senpy is a framework to develop, evaluate and publish web services for sentiment and emotion analysis in text. The framework is aimed towards both developers and users. For developers, it is a means to evaluate their classifiers and easily publish them as web services. For users, it is a way to consume sentiment analysis from different providers through the same interface. This is achieved through a combination of an API aligned with the NLP Interchange Format (NIF) service specification, the use of semantic formats and a series of well established vocabularies. The framework is Open Source, and has been used extensively in several projects. As a result, several Senpy Open Source services are available for use and download. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105193	10.1016/j.knosys.2019.105193													
J								Unsupervised emotional state classification through physiological parameters for social robotics applications	KNOWLEDGE-BASED SYSTEMS										Emotional state detection; Cluster analysis; Pleasure; Physiological signals; Social robotics	DESIGN	Future social robots should have personalized behaviors based on user emotional state to fit more in ordinary users activities and to improve the human-robot interaction. Several, literature works use cameras to record emotions. However, these approaches may not be effective in everyday life, due to camera obstructions and different types of stimulation, which can be related also with the interaction with other human beings. Therefore, in this work, it is investigated the electrocardiogram, the electrodermal activity, and the electric brain activity physiological signals as main informative channels. The aforementioned signals have been acquired through the use of a wireless wearable sensor network. An experimental methodology was proposed to induce three different emotional states by means of social interaction. Two different combinations of sensors were analyzed using three different time-window frames (180s, 150s, and 120s) and classified with three unsupervised machine learning approaches (K-Means, K-medoids and Self-organizing maps). Finally, their classification performances were compared to the ones obtained by four commonly used supervised techniques (i.e. Support Vector Machine, Decision Tree and k-nearest neighbor) to discuss the optimal combination of sensors, time-window length, and unsupervised classifier. Fifteen healthy young participants were recruited in the study and more than 100 instances were analyzed. The proposed approaches achieve an accuracy of 77% in the best-unsupervised case and 85% with the best-supervised ones. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105217	10.1016/j.knosys.2019.105217													
J								Predicting ICD-9 code groups with fuzzy similarity based supervised multi-label classification of unstructured clinical nursing notes	KNOWLEDGE-BASED SYSTEMS										Clinical decision support systems; Disease prediction; Healthcare analytics; ICD-9 code group prediction; Machine learning; Natural language processing	INTENSIVE-CARE-UNIT; CHRONIC HEALTH EVALUATION; NEURAL-NETWORKS; MORTALITY PREDICTION; ACUTE PHYSIOLOGY; HOSPITAL MORTALITY; LEARNING APPROACH; STAY; MODELS; APACHE	In hospitals, caregivers are trained to chronicle the subtle changes in the clinical conditions of a patient at regular intervals, for enabling decision-making. Caregivers text-based clinical notes are a significant source of rich patient-specific data, that can facilitate effective clinical decision support, despite which, this treasure-trove of data remains largely unexplored for supporting the prediction of clinical outcomes. The application of sophisticated data modeling and prediction algorithms with greater computational capacity have made disease prediction from raw clinical notes a relevant problem. In this paper, we propose an approach based on vector space and topic modeling, to structure the raw clinical data by capturing the semantic information in the nursing notes. Fuzzy similarity based data cleansing approach was used to merge anomalous and redundant patient data. Furthermore, we utilize eight supervised multi-label classification models to facilitate disease (ICD-9 code group) prediction. We present an exhaustive comparative study to evaluate the performance of the proposed approaches using standard evaluation metrics. Experimental validation on MIMIC-III, an open database, underscored the superior performance of the proposed Term weighting of unstructured notes AGgregated using fuzzy Similarity (TAGS) model, which consistently outperformed the state-of-the-art structured data based approach by 7.79% in AUPRC and 1.24% in AUROC. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105321	10.1016/j.knosys.2019.105321													
J								A BPSO-based method for high-utility itemset mining without minimum utility threshold	KNOWLEDGE-BASED SYSTEMS										High-utility itemset mining; Binary particle swarm optimization; Minimum utility threshold; Computational intelligence; Data mining	EFFICIENT ALGORITHMS; VERSION	High-utility itemset mining is used to obtain high utility itemsets by taking into account both the quantity as well as the utility of each item, which have not been considered in frequent itemset mining. Many algorithms compute high utility itemsets by setting a minimum utility threshold in advance. However, determining the minimum utility threshold is not easy. Too high or too low a threshold may result in incorrect high utility itemsets. In this paper, we propose a method based on binary particle swarm optimization to optimize the search for high utility itemsets without setting the minimum utility threshold beforehand. Instead, the application of the minimum utility threshold is performed as a post-processing step. Experiments on five datasets indicate that the proposed method is better than existing methods in finding high utility itemsets, and the time to obtain those itemsets is faster than that with setting the minimum utility threshold first. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105164	10.1016/j.knosys.2019.105164													
J								Anomaly detection method using center offset measurement based on leverage principle	KNOWLEDGE-BASED SYSTEMS										Unsupervised anomaly detection; Leverage principle; Center offset; Golden ratio; Adaptive threshold setting	NEAREST; ALGORITHM; SELECTION	Anomaly detection is an important branch of data mining and has been researched within diverse research areas and application domains. Many existing unsupervised anomaly detection methods have high computational complexity and more adjustable parameters. In addition, the proportion of anomalies needs to be estimated by long-term experience in most methods to set the threshold that distinguishes normal instances and anomalies, which makes the algorithms more subjective. This paper presents an anomaly detection algorithm based on the leverage principle. When detecting a testing instance, we copy it in large quantities and take these replicated data into the training dataset. The anomaly degree of the testing instance can be assessed by measuring the offset of the dataset center. Meanwhile, an adaptive threshold setting method using the golden ratio is proposed to solve the subjectivity in distinguishing normal instances and anomalies. In the experiments, we compare the anomaly detection algorithm proposed in this paper with other eight detection methods and report the experimental results in terms of AUC, the F1 of the anomaly classes and the running time. The results show that our algorithm can achieve high detection performance with high efficiency, and the proposed threshold setting method also has strong practicality in unsupervised anomaly detection. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105191	10.1016/j.knosys.2019.105191													
J								Fitness-distance balance (FDB): A new selection method for meta-heuristic search algorithms	KNOWLEDGE-BASED SYSTEMS										FDB selection method; FDB-based symbiotic organism search; Meta-heuristic search algorithms; Benchmark problems; Optimization	PARTICLE SWARM OPTIMIZATION; SINE COSINE ALGORITHM; DIFFERENTIAL EVOLUTION; MECHANISM; EXPLORATION; PSO	Selection methods have an important role in the meta-heuristic search (MHS) process. However, apart from a few successful methods developed in the past, new and effective studies have not been found in recent years. It is known that solution candidates selecting from the population during the search process directly affects the direction and success of the search. In this study, a new selection method based on fitness-distance balance (FDB) is developed in order to solve the premature convergence problem in the MHS process. Thanks to the developed method, solution candidates with the highest potential to improve the search process can be determined effectively and consistently from the population. Experimental studies have been conducted to test and verify the developed FDB selection method. For this purpose, 90 benchmark functions with different types and complexity levels have been used. In order to test the developed FDB method, numerous variants have been formed. These variants have been compared to each other to determine the most effective FDB variant. In the validation study, the FDB-SOS (FDB-based symbiotic organism search) algorithm is compared with thirteen well-known and up-to-date MHS techniques. The search performance of the algorithms has been analyzed by the Wilcoxon Rank Sum Test. The results show that the developed selection method makes a significant contribution to the meta-heuristic search process. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105169	10.1016/j.knosys.2019.105169													
J								Online transfer learning with multiple source domains for multi-class classification	KNOWLEDGE-BASED SYSTEMS										Online transfer learning; Transfer learning; Multi-class classification; Multiple source domains; Online learning	FRAMEWORK	The major objective of transfer learning is to handle the learning tasks on a target domain by utilizing the knowledge extracted from the source domain(s), when the labeled data in the target domain are not sufficient. Transfer learning can be classified into offline transfer learning (OffTL) and online transfer learning (OnTL), and OnTL has attracted much attention and research due to its more realistic scenario assumed in practice. There can be multiple source domains, therefore, OnTL with Multiple Source Domains has been studied in recent years and algorithms have been proposed. Nevertheless, it can be noted that existing research on OnTL with Multiple Source Domains only deals with binary classification tasks. In this paper, we make the first attempt to study OnTL with Multiple Source Domains for multi-class classification (MC), and propose an algorithm, referred to as Online Multi-source Transfer Learning for Multi-class classification (OMTL-MC) algorithm. OMTL-MC algorithm is built on two-stage ensemble strategy, in this way, the knowledge extracted from different source domains can be simultaneously online transferred to improve the performance of the classifier in the target domain. In order to deeper explore the underlying structure among multiple classes, an Extended Hinge Loss (EHL) function is adopted in OMTL-MC. We theoretically analyze the mistake bound of OMTL-MC algorithm. In addition, experiments on several popular datasets expound that the proposed OMTL-MC algorithm outperforms the other compared algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105149	10.1016/j.knosys.2019.105149													
J								Bringing the organization back in: Flexing structural responses to competing logics in budgeting	KNOWLEDGE-BASED SYSTEMS										Institutional complexity; Organizations; Organizational studies in accounting; Structured flexibility; Bounded rationality; Interactive budgeting	INSTITUTIONAL COMPLEXITY; BOUNDED RATIONALITY; MANAGEMENT CONTROL; HIGHER-EDUCATION; SYSTEMS; TRANSFORMATION; PERSPECTIVE; RESOURCE; STRATEGY; CREATION	This paper aims to understand the mechanisms through which organizations, over time, manage competing logics within budgeting practices. We draw insights from new institutional studies in accountingto highlight the importance of practice-level negotiations for managing institutional conflicts, but we complement them with a focus on the organizational embeddedness of hybrid practices from organizational studies in accounting. More specifically, we build on the notion of 'structured flexibility,' according to which organizational structures frame and enable negotiations on hybrid practices, as critical to manage complexity in face of changing environmental conditions. We thus show how 'structured flexibility' is made possible by a number of characteristics of decision-making that have been widely studied by the behavioral theory of the firm, i.e. the decomposition of decision-making processes, the framing of local negotiations and the search for satisficing solutions. Our findings from a case study of a public university facing the conflict between a professional and a managerial logic shed light on how organizations may actively manage institutional complexity over time and suggest specific interventions to transform complexity into a source of strategic advantage. (C) 2019 Elsevier Ltd. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								101075	10.1016/j.aos.2019.101075													
J								Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization	KNOWLEDGE-BASED SYSTEMS										Graph-regularization; Semi-supervised learning; Multi-view clustering; Non-negative matrix factorization		Non-negative matrix factorization is widely used in multi-view clustering due to its ability of learning a common dimension-reduced factor. Recently, it is combined with the label information to improve the clustering, but the affection of the dimension-reduction to the classes of the labeled data is seldom considered. Motivated by that the graph constraint can keep the geometric structure of the data, it is employed to restrict the class variation of the data caused by the dimension reduction, and a semi-supervised method called Graph-regularized Partially Shared Non-negative Matrix Factorization (GPSNMF) is proposed for multi-view clustering in this paper. In our method, the affinity graph of each view is constructed to encode the geometric information, and the corresponding multiplication update algorithm based on alternative iteration rule is derived. In the experiments, two clustering approaches are tested based on the results of the proposed GPSNMF, and four real-world databases with different label proportions are performed to demonstrate the advantages of our method over the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105185	10.1016/j.knosys.2019.105185													
J								Adaptive early classification of temporal sequences using deep reinforcement learning	KNOWLEDGE-BASED SYSTEMS										Early classification; Adaptive prediction time; Deep reinforcement learning; Temporal sequences; Double DQN; Trade-off between accuracy vs. speed		In this article, we address the problem of early classification (EC) of temporal sequences with adaptive prediction times. We frame EC as a sequential decision making problem and we define a partially observable Markov decision process (POMDP) fitting the competitive objectives of classification earliness and accuracy. We solve the POMDP by training an agent for EC with deep reinforcement learning (DRL). The agent learns to make adaptive decisions between classifying incomplete sequences now or delaying its prediction to gather more measurements. We adapt an existing DRL algorithm for batch and online learning of the agent's action value function with a deep neural network. We propose strategies of prioritized sampling, prioritized storing and random episode initialization to address the fact that the agent's memory is unbalanced due to (1): all but one of its actions terminate the process and thus (2): actions of classification are less frequent than the action of delay. In experiments, we show improvements in accuracy induced by our specific adaptation of the algorithm used for online learning of the agents action value function. Moreover, we compare two definitions of the POMDP based on delay reward shaping against reward discounting. Finally, we demonstrate that a static naive deep neural network, i.e. trained to classify at static times, is less efficient in terms of accuracy against speed than the equivalent network trained with adaptive decision making capabilities. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105290	10.1016/j.knosys.2019.105290													
J								The marketization of a social movement: Activists, shareholders and CSR disclosure	KNOWLEDGE-BASED SYSTEMS										Social movement; CSR disclosure; Shareholder activism; Transparency; Marketization; Social justice	ENVIRONMENTAL DISCLOSURES; PROPENSITY SCORE; CORPORATE DISCLOSURE; OIL-SPILL; SUSTAINABILITY; RESPONSIBILITY; PERFORMANCE; MARKET; ACCOUNTABILITY; CONSEQUENCES	In this paper, we conceptualize shareholder activism demanding CSR transparency as an outcome of the marketization of a social movement. We argue that the infusion of profit-oriented motivations into the social justice ideals on which the original shareholder activism movement was founded has contributed to create a conceptualization of CSR as a risk to be managed. As marketized solutions to risk management privilege the provision of information, they contribute to explaining the emphasis placed by shareholders on transparency in their proposals. Drawing on a sample of U.S. firms over 2006-2012, our evidence suggests a marked increase in CSR disclosure for the sample firms targeted by transparency proposals. However, our analysis reveals that concerns over the CSR practices of the same firms worsen, suggesting that shareholder activism demanding CSR transparency does not inspire change in corporate activities beyond disclosure, at least in the short term. Our contribution to the accounting literature lies in conceptualizing how the emphasis placed on CSR disclosure contributes to ensconcing the social movement into a corporation-centric, market-driven approach, moving away from the initial ideals of social justice aiming to push corporations to act on societal concerns. Altogether, we expose how the accounting practice of CSR disclosure is complicit in the attrition of the initial ideals of shareholder activism on CSR. (C) 2019 Elsevier Ltd. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								101074	10.1016/j.aos.2019.101074													
J								The role of diagnostic and interactive control uses in innovation	KNOWLEDGE-BASED SYSTEMS										Diagnostic controls; Interactive controls; Control combination; Coordination routines; Dynamic capabilities; New product development; Innovation	MANAGEMENT CONTROL-SYSTEMS; PRODUCT DEVELOPMENT PRACTICES; TESTING CONTINGENCY HYPOTHESES; SUCCESS FACTORS; DYNAMIC CAPABILITIES; NPD PRACTICES; FIT; BENCHMARKING; PERFORMANCE; STRATEGY	The purpose of this paper is to shed insights into the current inconsistencies of the control-innovation relationship. We draw on the dynamic capabilities literature to hypothesize that diagnostic and interactive control uses are indirectly related to innovativeness through their influence on the perceived formalization of coordination routines. That is, diagnostic and interactive control uses provide an input for an effective coordination process; the information provided allows for the coordination of activities to flourish. Further, we draw on contingency literature to hypothesize that the controls' direct effect on innovativeness depends on the degree of technological turbulence. Using data from a survey of 695 R&D professionals from North America and Europe, this study examines the relationships between the interactive and diagnostic uses of control systems, the perceived coordination routines, and innovativeness in terms of product newness and innovation rate in contexts of low and high technological turbulence. This study makes two primary contributions to the accounting literature. First, it provides a starting point for solving the contradictory evidence on the controlinnovation relationship by integrating complexity; it includes a previously un-researched mediating variable (coordination routines), a moderator (technological turbulence), and two types of innovativeness (rate and newness). Second, the study demonstrates why and when diagnostic use is beneficial for innovativeness. (C) 2019 Elsevier Ltd. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								101078	10.1016/j.aos.2019.101078													
J								An OWA-based hierarchical clustering approach to understanding users' lifestyles	KNOWLEDGE-BASED SYSTEMS										OWA; Clustering; Online reviews	SENTIMENT ANALYSIS PROBLEM; RESTAURANT RECOMMENDATION; SYSTEMS	Based on users' interactions with social networks, a method to understand users' life-styles is developed. Descriptions of their lifestyles are obtained from previously reported experiences on these sites. Contextual information and contributed reviews lend insight into which elements are important for different lifestyles. In this paper, an ordered weighted averaging operator (OWA) is integrated with hierarchical clustering in order to find the similarity between users and clusters. Specifically, a two step measure is defined to compare and aggregate two clusters. To illustrate the efficiency of the methodology, a real case is implemented for 499 Yelp reviewers associated with 134,102 reviews across 11 variables and 373 Airbnb reviewers associated with 1,826 reviews across 14 variables. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105308	10.1016/j.knosys.2019.105308													
J								Discriminative Streaming Network Embedding	KNOWLEDGE-BASED SYSTEMS										Network embedding; Streaming algorithm; Discriminative model		Many real-world networks (e.g., friendship network among Facebook users) generate data (e.g., friend requests) in a stream fashion. Recently, several network embedding methods are proposed to learn embeddings on such networks incrementally. However, these methods perform incremental updates in a heuristic manner and thus fail to quantitatively restrict the differences between incremental learning and direct learning on the entire network (i.e., the batch learning). Moreover, they ignore the node labels (e.g., interests) when learning node embeddings, which undermines the performance of network embeddings for applications such as node classification. To solve this problem, in this paper we propose a novel network embedding framework, Discriminative Streaming Network Embedding (DimSim). When an edge insertion/deletion occurs, DimSim fast learns node embeddings incrementally, which is desired for many online applications such as anomaly detection. With the incremental learning method, at any time, the objective function well approximates to that of batch learning on the current snapshot. More importantly, the average amount of updating operations of DimSim for processing each newly coming edge is aboutO(n(1-lambda)) , where lambda is constant between 1 and 2 and n is the number of nodes. As a result, DimSim can be increasingly fast as the network grows, which adheres to the intuition that an edge insertion/deletion has a smaller impact on larger networks. In addition, we also utilize node label information to learn discriminative node embeddings and we deliberately design a new model to automatically trade-off the node-specific importance of topology and label information. Extensive experiments on real-world streaming networks show that our method DimSim is up to 50 times faster and 40% more accurate than the state-of-the-arts. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105138	10.1016/j.knosys.2019.105138													
J								Intelligent decision making for service providers selection in maintenance service network: An adaptive fuzzy-neuro approach	KNOWLEDGE-BASED SYSTEMS										Maintenance service network; Service providers selection; Evaluation criteria; Fuzzy-neuro approach	SUPPLIER SELECTION; LOGISTICS PROVIDER; REVERSE LOGISTICS; MODEL; SYSTEM; MANAGEMENT; INTEGRATION; CRITERIA; INDUSTRY; ART	Establishment of a maintenance service network has become increasingly urgent to ensure the safety and reliability of agricultural operation in busy farming seasons. This paper aims to design the maintenance service network in agriculture and focuses on the joint selection of service providers in network nodes. A novel intelligent decision-making approach is developed based on expert knowledge and machine learning techniques. First, a set of evaluation criteria for the group of service providers is defined from both qualitative and quantitative aspects. The adaptive fuzzy-neuro approach is proposed for selection of the group of service providers. Fuzzy rules designed by experts are applied for scheme classification and adaptive neural network is developed for the modification of memberships functions. Lastly, the proposed methodology is demonstrated by designing a real maintenance service network in Hunan, China. Experimental results in real application scenarios manifest that the proposed approach provides optimal solutions for decision making and providers selection in a maintenance service network. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105263	10.1016/j.knosys.2019.105263													
J								On extracting data from tables that are encoded using HTML	KNOWLEDGE-BASED SYSTEMS										HTML documents; Web tables; Table mining; Data extraction	INFORMATION EXTRACTION; WEB; TRANSDUCERS; SEMANTICS	Tables are a common means to display data in human-friendly formats. Many authors have worked on proposals to extract those data back since this has many interesting applications. In this article, we summarise and compare many of the proposals to extract data from tables that are encoded using HTML and have been published between 2000 and 2018. We first present a vocabulary that homogenises the terminology used in this field; next, we use it to summarise the proposals; finally, we compare them side by side. Our analysis highlights several challenges to which no proposal provides a conclusive solution and a few more that have not been addressed sufficiently; simply put, no proposal provides a complete solution to the problem, which seems to suggest that this research field shall keep active in the near future. We have also realised that there is no consensus regarding the datasets and the methods used to evaluate the proposals, which hampers comparing the experimental results. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105157	10.1016/j.knosys.2019.105157													
J								Reinforcement learning approach for optimal control of multiple electric locomotives in a heavy-haul freight train:A Double-Switch-Q-network architecture	KNOWLEDGE-BASED SYSTEMS										Reinforcement learning; Double-Switch Q-network; Optimal control; Electric locomotive; Heavy-haul freight train	HIGH-SPEED TRAINS; FAULT-TOLERANT CONTROL; CRUISE CONTROL; NEURAL-NETWORKS; OPERATION; APPROXIMATION; OPTIMIZATION; STRATEGIES; GAME; GO	Electric locomotives provide high tractive power for fast acceleration of heavy-haul freight trains, and significantly reduce the energy consumption with regenerative braking. This paper proposes a reinforcement learning (RL) approach for the optimal control of multiple electric locomotives in a heavy-haul freight train, without using the prior knowledge of train dynamics and the pre-designed velocity profile. The optimization takes the velocity, energy consumption and coupler force as objectives, considering the constraints on locomotive notches and their change rates, speed restrictions, traction and regenerative braking. Besides, since the problem in this paper has continuous state space and large action space, and the adjacent actions influences on states share similarities, we propose a Double-Switch Q-network (DSQ-network) architecture to achieve fast approximation of the action-value function, which enhances the parameter sharing of states and actions, and denoises the action-value function. In the numerical experiments, we test DSQ-network in 28 cases using the data of China Railways HXD3B electric locomotive. The results indicate that compared with table-lookup Q-learning, DSQ-network converges much faster and uses less storage space in the optimal control of electric locomotives. Besides, we analyze 1)the influences of ramps and speed restrictions on the optimal policy, and 2)the inter-dependent and inter-conditioned relationships between multiple optimization objectives. Finally, the factors that influence the convergence rate and solution accuracy of DSQ-network are discussed based on the visualization of the high-dimensional value functions. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105173	10.1016/j.knosys.2019.105173													
J								Detection of SQL injection based on artificial neural network	KNOWLEDGE-BASED SYSTEMS										SQL injection; Neural network; MLP; LSTM		The SQL injection, a common web attack, has been a challenging network security issue which causes annually millions of dollars of financial loss worldwide as well as a large amount of users privacy data leakage. This work presents a high accuracy SQL injection detection method based on neural network. We first acquire authentic user URL access log data from the Internet Service Provider(ISP), ensuring that our approach is real, effective and practical. We then conduct statistical research on normal data and SQL injection data. Based on the statistical results, we design eight types of features and train an MLP model. The accuracy of the model maintains over 99%. Meanwhile, we compare and evaluate the training effect of other machine learning algorithms(LSTM, for example), the results reveal that the accuracy of our method is superior to the relevant machine learning algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105528	10.1016/j.knosys.2020.105528													
J								Sources of dissension: The making and breaking of the individual in Swedish aged care	KNOWLEDGE-BASED SYSTEMS										Management control; Governmentality; Dissension; Individualisation; Aged care	PUBLIC MANAGEMENT; NEXT-GENERATION; OPPORTUNITIES; CONSTRUCTION; GOVERNMENT; INVENTION; SUBJECT; SECTOR; SELF	It is well known that both management and professional work in areas such as health and aged care rely upon division of individuals into categories of, for example, diagnoses or costs and revenues. The present paper turns this around and asks: what happens if individuality, the indivisible wholeness of the person, is taken seriously in such practices? If every human being is interpreted as unique and special, and their wholeness is recognised in the relationship between professionals and an individual receiving care? The paper analyses two rival programmes - those of efficiency and individuality - and their operationalisation in Swedish aged care, and show how these programmes and corresponding technologies are sources of dissension that can be used to problematise how care should be conducted. However, such dissension also opens up spaces of freedom, which allow care practitioners to conduct care differently. (C) 2019 Elsevier Ltd. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								101077	10.1016/j.aos.2019.101077													
J								adVAE: A self-adversarial variational autoencoder with Gaussian anomaly prior knowledge for anomaly detection	KNOWLEDGE-BASED SYSTEMS										Anomaly detection; Outlier detection; Novelty detection; Deep generative model; Variational autoencoder	PRINCIPAL COMPONENT ANALYSIS	Recently, deep generative models have become increasingly popular in unsupervised anomaly detection. However, deep generative models aim at recovering the data distribution rather than detecting anomalies. Moreover, deep generative models have the risk of overfitting training samples, which has disastrous effects on anomaly detection performance. To solve the above two problems, we propose a self-adversarial variational autoencoder (adVAE) with a Gaussian anomaly prior assumption. We assume that both the anomalous and the normal prior distribution are Gaussian and have overlaps in the latent space. Therefore, a Gaussian transformer net T is trained to synthesize anomalous but near-normal latent variables. Keeping the original training objective of a variational autoencoder, a generator G tries to distinguish between the normal latent variables encoded by E and the anomalous latent variables synthesized by T, and the encoder E is trained to discriminate whether the output of G is real. These new objectives we added not only give both G and E the ability to discriminate, but also become an additional regularization mechanism to prevent overfitting. Compared with other competitive methods, the proposed model achieves significant improvements in extensive experiments. The employed datasets and our model are available in a Github repository. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105187	10.1016/j.knosys.2019.105187													
J								Word Sense Disambiguation: A comprehensive knowledge exploitation framework	KNOWLEDGE-BASED SYSTEMS										Word sense disambiguation; Background knowledge; Information retrieval; Relation exploitation; Semantic path	REPRESENTATION; WEB	Word Sense Disambiguation (WSD) has been a basic and on-going issue since its introduction in natural language processing (NLP) community. Its application lies in many different areas including sentiment analysis, Information Retrieval (IR), machine translation and knowledge graph construction. Solutions to WSD are mostly categorized into supervised and knowledge-based approaches. In this paper, a knowledge-based method is proposed, modeling the problem with semantic space and semantic path hidden behind a given sentence. The approach relies on the well-known Knowledge Base (KB) named WordNet and models the semantic space and semantic path by Latent Semantic Analysis (LSA) and PageRank respectively. Experiments has proven the method's effectiveness, achieving state-of-the-art performance in several WSD datasets. (C) 2019 The Author(s). Published by Elsevier B.V.																	0950-7051	1872-7409				FEB 29	2020	190								105030	10.1016/j.knosys.2019.105030													
J								An efficient active learning method for multi-task learning	KNOWLEDGE-BASED SYSTEMS										Multi-task classification; Active learning; Support vector machine	OBJECT TRACKING; CLASSIFICATION; OPTIMIZATION; UNCERTAINTY; DIVERSITY; ALGORITHM; DENSITY	In multi-task learning, the sharing of information between related tasks affects and promotes the learning of each task. However, the traditional multi-task learning techniques always require sufficient labeled data to improve the learning of each task, and labeling samples is always expensive in practice. In this paper, we propose two variants of active learning methods for multi-task classification. In the uncertainty step, we propose the support vector preservation criterion that evaluates uncertainty at the level of classifier, which is called classifier-level uncertainty (CLU). In the diversity step, we propose two diversity criteria that evaluate diversity by the clustering method and the partition method respectively, which are called clustering-based diversity (CBD) and partition-based diversity (PBD) respectively. Each diversity criterion together with the uncertainty criterion is to form an active learning method for multi-task learning. In addition, the proposed support vector preservation criterion selects local informative samples which determine the hyperplane for each task. Furthermore, in order to maintain the distribution structure of the samples, we put forward the micro-kernel k-means clustering method and partition-based method to select global informative samples from the non-support vectors. By incorporating the local and global informative samples into active learning, we propose the two active learning methods for multi-task problems. We evaluate the effectiveness of the proposed methods by conducting experiments with other active learning methods. The experimental results show that the proposed two methods perform better than other active learning methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105137	10.1016/j.knosys.2019.105137													
J								Machine learning-based wear fault diagnosis for marine diesel engine by fusing multiple data-driven models	KNOWLEDGE-BASED SYSTEMS										Wear fault diagnosis; Marine diesel engine; Machine learning-based diagnostic model; Fusion system; ER rule	CLASSIFIER FUSION; EXPERT-SYSTEMS	Wear fault is one of the dominant causes for marine diesel engine damage which significantly influences ship safety. By taking full advantage of the data generated in engine operation, machine learning-based wear fault diagnostic model can help engineers to determine fault modes correctly and take quick action to avoid severe accidents. To identify wear faults more accurately, a multi-model fusion system based on evidential reasoning (ER) rule is proposed in this paper. The outputs of three data-driven models including an artificial neural network (ANN) model, a belief rule-based inference (BRB) model, and an ER rule model are used as pieces of evidence to be fused in decision level. In this paper, the fusion system defines reliability and importance weight of every single model respectively. A novel method is presented to determine the reliability of evidence by considering the accuracy and stability of every single model. The importance weight is optimized by genetic algorithm to improve the performance of the fusion system. The proposed machine learning-based diagnostic system is validated by a set of real samples acquired from marine diesel engines in operation. The test results show that the system is more accurate and robust, and the fault tolerant ability is improved remarkably compared with every single data-driven diagnostic model. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105324	10.1016/j.knosys.2019.105324													
J								Consensus-based non-cooperative behaviors management in large-group emergency decision-making considering experts' trust relations and preference risks	KNOWLEDGE-BASED SYSTEMS										Non-cooperative behaviors; Social network analysis; Risk; Large group; Emergency decision-making	MODEL; MECHANISM	Consensus-based large-group emergency decision-making (LGEDM) is a dynamic and iterative process, in which some experts may show non-cooperative behaviors because they have different knowledge backgrounds and represent different stakeholders. Non-cooperative behaviors greatly affect the efficiency and results of decision-making. Time is of the essence in an emergency situation, and thus rational treatment of non-cooperative behaviors is required. In addition, in traditional group decision-making (GDM), the decision model is based on the premise that decision makers are independent. However, with the development of online social networks, the objective trust relationships between experts should be considered. Hence, this paper proposes a consensus model that considers the experts trust relations based on social network analysis and preference risks based on interval-valued intuitionistic fuzzy numbers. The trust risk, preference risk, and an approach for analyzing and managing non-cooperative behaviors are proposed. An illustrative example and a comparison are provided to verify the feasibility and effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105108	10.1016/j.knosys.2019.105108													
J								Semi-supervised representation learning via dual autoencoders for domain adaptation	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Dual autoencoders; Representation learning; Semi-supervised	NEURAL-NETWORKS	Domain adaptation aims to exploit the knowledge in source domain to promote the learning tasks in target domain, which plays a critical role in real-world applications. Recently, lots of deep learning approaches based on autoencoders have achieved a significance performance in domain adaptation. However, most existing methods focus on minimizing the distribution divergence by putting the source and target data together to learn global feature representations, while they do not consider the local relationship between instances in the same category from different domains. To address this problem, we propose a novel Semi-Supervised Representation Learning framework via Dual Autoencoders for domain adaptation, named SSRLDA. More specifically, we extract richer feature representations by learning the global and local feature representations simultaneously using two novel autoencoders, which are referred to as marginalized denoising autoencoder with adaptation distribution (MDA(ad)) and multi-class marginalized denoising autoencoder (MMDA) respectively. Meanwhile, we make full use of label information to optimize feature representations. Experimental results show that our proposed approach outperforms several state-of-the-art baseline methods. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				FEB 29	2020	190								105161	10.1016/j.knosys.2019.105161													
J								Effective Bayesian-network-based missing value imputation enhanced by crowdsourcing	KNOWLEDGE-BASED SYSTEMS										Missing values; Bayesian network; Crowdsourcing	SELECTION	During the process of data collection, incompleteness is one of the most serious data quality problems to deal with. Traditional imputation methods mostly rely on statistics and machine learning techniques. However, both types of methods are limited in their accuracy due to lacking enough information about the missing data. To obtain more information, recent methods resort to external sources such as knowledge bases or the worldwide web. Unfortunately, such methods may still be less helpful, since there may exist little information about the missing values in the knowledge bases, or too much noise on the web. To tackle these issues, this paper adopts crowdsourcing as the external source, where hundreds of thousands of ordinary workers on the platform can provide high-quality information based on contextual knowledge and human cognitive ability. To reduce the cost, a joint model is proposed for imputation, which integrates crowdsourcing into the process of Bayesian inference. We first construct a Bayesian network for the attributes in the dataset, then the missing attribute values are inferred by Bayesian inference. To improve the accuracy of the Bayesian inference, we outsource a small number of informative tasks to the crowd workers, where the informative tasks are selected based on uncertainty and influence. The proposed approach is evaluated with extensive experiments using real-world datasets with a simulated crowd and two real crowdsourcing platforms. The experimental results show that our approach achieves a better performance compared to other imputation approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105199	10.1016/j.knosys.2019.105199													
J								Fuzzy entropy clustering by searching local border points for the analysis of gene expression data	KNOWLEDGE-BASED SYSTEMS										Fuzzy clustering; Fuzzy entropy; Border points; Local density peeks; Gene expression data	CLASSIFICATION; PREDICTION; DISCOVERY; ALGORITHM	Clustering data by identifying a subset of representative border points is essential for detecting dataset structures and processing complicated messages in data. Such border points can be found by exactly choosing an initial subset of data points, and then iteratively refining it with fuzzy membership values; however, this only works well if the fuzzy rule choice is close to a good solution. We propose a clustering algorithm, known as affine fuzzy neuron (AFN), which measures the relations of linear memberships between border points and the cluster centers of points within a subset. Fuzzy memberships of border points are calculated until their corresponding clusters gradually emerge. AFN exploits a typical feature of real-life biological gene clusters, like gene expression measurements, from microarrays or single-cell RNA-Seq. That is, genes sharing the same expression profile belong to different clusters with diverse functions. We hypothesize that these genes only appear at cluster boundaries and a subset of genes comprises points with similar expression profiles but entirely different functions. We use AFN to cluster several test cases and analyze various gene expression datasets. The proposed clustering algorithm is a flexible modeling framework that increases accuracy and combines border regions across multiple clusters while remaining computationally feasible. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105309	10.1016/j.knosys.2019.105309													
J								Deep time-frequency representation and progressive decision fusion for ECG classification	KNOWLEDGE-BASED SYSTEMS										Decision-making; Electrocardiography; Fourier transforms; Neural networks	NEURAL-NETWORK; EXPERT-SYSTEM; NOISE; TRANSFORM; FILTER; PCA	Early recognition of abnormal rhythms in ECG signals is crucial for monitoring and diagnosing patients cardiac conditions, increasing the success rate of the treatment. Classifying abnormal rhythms into exact categories is very challenging due to the broad taxonomy of rhythms, noises and lack of large-scale real-world annotated data. Different from previous methods that utilize hand-crafted features or learn features from the original signal domain, we propose a novel ECG classification method by learning deep time-frequency representation and progressive decision fusion at different temporal scales in an end-to-end manner. First, the ECG wave signal is transformed into the time-frequency domain by using the Short-Time Fourier Transform. Next, several scale-specific deep convolutional neural networks are trained on ECG samples of a specific length. Finally, a progressive online decision fusion method is proposed to fuse decisions from the scale-specific models into a more accurate and stable one. Extensive experiments on both synthetic and real-world ECG datasets demonstrate the effectiveness and efficiency of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105402	10.1016/j.knosys.2019.105402													
J								NearCount: Selecting critical instances based on the cited counts of nearest neighbors	KNOWLEDGE-BASED SYSTEMS										Critical instance; Nearest neighbor; Cited counts; Imbalanced problem; Instance selection	DATA REDUCTION; SMOTE	Traditional instance selection algorithms are not good at addressing imbalanced problems. Moreover, most of them are sensitive to noise instances and suffer from complex selection rules. To solve these problems, in this paper, we propose a concise learning framework named NearCount to determine the importance of the instance without editing noise. In NearCount, the importance of an instance corresponds to the cited counts. The count is determined by the number of times that one instance is selected as a nearest neighbor of instances in different classes. For the instances with nonzero cited counts, the importance of the instance is inversely proportional to the cited count. To handle classification problems with different data distributions, two detailed NearCount-based algorithms - NearCount-IM and NearCount-IS - are introduced. For imbalanced problems, NearCount-IM selects the important majority instances with an equal number of minority instances, thus balancing the data distribution. For balanced scenarios, NearCount-IS selects the instances whose cited counts are greater than zero and equal or less than the number of nearest neighbors as critical instances in every class. The proposed NearCount-IM and NearCount-IS algorithms are evaluated by comparing them with classical instance selection algorithms on the benchmark data sets. Experiments validate the effectiveness of the proposed algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 29	2020	190								105196	10.1016/j.knosys.2019.105196													
J								Online Learning Using Multiple Times Weight Updating	APPLIED ARTIFICIAL INTELLIGENCE											PERCEPTRON	Online learning makes sequence of decisions with partial data arrival where next movement of data is unknown. In this paper, we have presented a new technique as multiple times weight updating that update the weight iteratively for same instance. The proposed technique analyzed with popular state-of-art algorithms from literature and experimented using established tool. The results indicate that mistake rate reduces to zero or close to zero for various datasets and algorithms. The overhead running cost is not too expensive and achieving mistake rate close to zero further strengthens the proposed technique. The present work includes bound nature of weight updating for single instance and achieve optimal weight value. This proposed work could be extended to big datasets problems to reduce mistake rate in online learning environment. Also, the proposed technique could be helpful to meet real life challenges.																	0883-9514	1087-6545				MAY 11	2020	34	6					515	536		10.1080/08839514.2020.1730623		FEB 2020											
J								Optimal robust disturbance observer based sliding mode controller using multi-objective grasshopper optimization algorithm to enhance power system stability	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PSS; Optimal RPO-SMC; MOGOA; Power system stability; Multi-objective problem	DUAL-INPUT PSS; COORDINATED DESIGN; ADAPTIVE-CONTROL; RCGA; EXCITATION	Power system stabilizer (PSS) has operated as an auxiliary controller for the synchronous generator excitation system to suppress the power system low frequency oscillations. This paper aims to design an optimal robust disturbance observer based sliding mode controller (RDO-SMC) acting as PSS to enhance the power system dynamic stability through the nonlinear control of excitation system. Indeed, the sliding disturbance observer has expeditiously estimated the accumulated consequence of unknown nonlinearities, parameter uncertainties, non-ideal dynamic models and exterior time-varying interferences in support of sliding mode controller. The sliding surface trajectory has been mathematically derived using Lyapunov theory. RDO-SMC has investigated the low frequency suppression of both the terminal voltage and angular speed. Due to multi-objective nature of the designed control problem, multi-objective grasshopper optimization algorithm (MOGOA) has been implemented to optimally tune the parameters of RDO-SMC and other understudy controllers. To verify and validate the performance of proposed controller, the analyses have been performed in single-machine infinite-bus and multi-machine power systems under different disturbance conditions. At last, the comprehensive simulation results have certainly confirmed the dynamic capability of the optimal RPO-SMC.																	1868-5137	1868-5145															10.1007/s12652-020-01811-8		FEB 2020											
J								A zone routing protocol incorporated with sleep scheduling for MANETs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile ad hoc networks (MANETs); Zone routing protocol (ZRP); Load; Sleep scheduling		Mobile ad hoc networks (MANETs) are dynamic in nature, and this poses inefficient routing challenges. In the traditional zone routing protocol (ZRP), since there is inter-zone routing, the border nodes will be overloaded. Further, there is also high energy consumption due to idle listening, retransmission, overhearing and control packets generation. The paper proposes a ZRP incorporated with sleep scheduling for MANETs. Since this ZRP is energy-efficient and load-balanced, it is referred to as energy-efficient and load-balanced ZRP. In this approach, the nodes are grouped into zones, and each zone elects a zone leader (ZL) on the basis of its residual energy, degree of connectivity, link quality, and distance to the border nodes. To ensure load balancing, the ZL chooses multiple paths from border nodes to the destination, and distributes the traffic along these paths. An adaptive sleep duty cycling mechanism is also implemented. This is to save the power on border nodes. Simulation results show that the proposed protocol reduces energy consumption, besides enhancing the throughput and packet delivery ratio.																	1868-5137	1868-5145															10.1007/s12652-020-01798-2		FEB 2020											
J								The Effect of Large Deflections of Joints on Foldable Miniature Robot Dynamics	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Foldable robots; Miniature robots; Legged robots; Robot dynamics	RIGID-BODY MODEL; COMPLIANT MECHANISMS; BEAMS; ELASTICA	In miniature robotics applications, compliant mechanisms are widely used because of their scalability. In addition, compliant mechanism architecture is compatible with the manufacturing methods used to fabricate small scale robots, such as "foldable robotics", where the size and the materials used allow much larger deflections. In this paper, the kinematics of compliant mechanisms used in miniature foldable robots are investigated with the assumption of nonlinear large deflections that occur at the flexure joints. The solution of the large beam deflection is acquired using elliptic integrals and is verified with finite element analysis and experiments on a simple small foldable leg linkage. The large deflection model takes joint strain energies into account and yields accurate estimations for load capacity of the mechanism as well as the necessary input torque for actuation of the leg. Therefore, the model presented can be used to estimate the load capacity of a miniature robot, as well as to select appropriate actuators. The work is also extended to estimate the compliant leg kinematics and rigid body dynamics of a foldable robot. The robot's large deflection simulation results are compared with experiments and a simplified rigid-link pin-joint kinematic model. Our results demonstrate the modeling accuracy of the two approaches and can be used by foldable robotics community when deciding on the strategy to choose for modeling their robots.																	0921-0296	1573-0409				OCT	2020	100	1					15	28		10.1007/s10846-020-01169-1		FEB 2020											
J								Extending Maps with Semantic and Contextual Object Information for Robot Navigation: a Learning-Based Framework Using Visual and Depth Cues	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Semantic object mapping; RGB-D localization and mapping; Object detection; Visual robot navigation		This paper addresses the problem of building augmented metric representations of scenes with semantic information from RGB-D images. We propose a complete framework to create an enhanced map representation of the environment with object-level information to be used in several applications such as human-robot interaction, assistive robotics, visual navigation, or in manipulation tasks. Our formulation leverages a CNN-based object detector (Yolo) with a 3D model-based segmentation technique to perform instance semantic segmentation, and to localize, identify, and track different classes of objects in the scene. The tracking and positioning of semantic classes is done with a dictionary of Kalman filters in order to combine sensor measurements over time and then providing more accurate maps. The formulation is designed to identify and to disregard dynamic objects in order to obtain a medium-term invariant map representation. The proposed method was evaluated with collected and publicly available RGB-D data sequences acquired in different indoor scenes. Experimental results show the potential of the technique to produce augmented semantic maps containing several objects (notably doors). We also provide to the community a dataset composed of annotated object classes (doors, fire extinguishers, benches, water fountains) and their positioning, as well as the source code as ROS packages.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		555	569		10.1007/s10846-019-01136-5		FEB 2020											
J								Actuator Fault-Tolerant Control Architecture for Multirotor Vehicles in Presence of Disturbances	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Fault tolerant control; Unmanned aerial vehicles; Multirotors; Dynamic surface control	SLIDING MODE CONTROL; DYNAMIC SURFACE CONTROL; SUBJECT; FTC	In this article we present an actuator fault-tolerant control architecture for the attitude and altitude tracking problem of multirotor aircrafts, under the effects of unknown drag coefficients and external wind. The tracking problem is faced by splitting it into two sub-problems, namely control law and control allocation. The control law is designed in terms of desired forces and moments which should act on the system, it does permit to exploit possible estimations of the disturbances acting on the vehicle and does not depend on the multirotor configuration. The control allocation, instead, optimally solves the redistribution of the control efforts among the motors according to the specific multirotor configuration, moreover it can actively cope with actuator faults whenever their estimations are available. Numerical simulations based on realistic scenarios confirm that the control architecture permits to solve the attitude and altitude tracking problem, despite the effects of faults and disturbances on the system.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		859	874		10.1007/s10846-020-01150-y		FEB 2020											
J								Video segmentation scheme based on AMC	IET IMAGE PROCESSING										Markov processes; directed graphs; video signal processing; image segmentation; Gaussian processes; optimisation; mixture models; video segmentation scheme; multimedia applications; spatiotemporal coherence; deformation; graph segmentation; feature scores; directed AMC graph; edge weights; trained weight models; proposal selection; modified absorbed time; motion object; graph-cuts based optimisation algorithm; pixel segmentation; background models; candidate proposals; Gaussian mixture models; Markov chain model	TRACKING; EXTRACTION	Video segmentation has become a fundamental of various multimedia applications. Spatiotemporal coherence is important for video segmentation. In this study, to balance the spatiotemporal coherence in scenes with deformation or large motion, the authors propose a novel segmentation scheme based on the absorbing Markov chain (AMC) model named directed graph segmentation based on AMC. In their study, they first generate primary proposals per frame. Then, they train weight models by using a part of primary proposals with their features and feature scores. Next, they construct a directed AMC graph, in which states are the generated primary proposals and edge weights are decided by trained weight models. They subsequently perform the first proposal selection per frame by thresholding the modified absorbed time. Afterwards, they design a reselection algorithm to filter the selected proposals and ensure the proposals, which are the most likely to be the motion object in each frame, to be selected as candidates. Finally, they employ the graph-cuts based optimisation algorithm to generate refined per pixel segmentation by using object and background models built by candidate proposals under the concept of Gaussian mixture models. Experimental results demonstrate that the proposed scheme shows competitive performance compared with advanced algorithms.																	1751-9659	1751-9667				FEB 28	2020	14	3					407	416		10.1049/iet-ipr.2018.6659													
J								CAD: concatenated action descriptor for one and two person(s), using silhouette and silhouette's skeleton	IET IMAGE PROCESSING										support vector machines; image sequences; image motion analysis; object recognition; image classification; object detection; CAD; concatenated action descriptor; human action recognition; signature-based corner points; silhouette skeleton; keypoint detection; signature-based optical flow; binary robust invariant scalable keypoints	HUMAN ACTION RECOGNITION; OPTICAL-FLOW; FEATURES; 3D; CONTEXT	This study introduces an action descriptor that has the ability to perform human action recognition efficiently for one and two person(s). The authors' proposed descriptor computes information like motion, spatial-temporal, diversion with respect to the centroid, critical point and keypoint detection, whereas the existing approaches lack to address this information efficiently. Action descriptors are developed from signature-based optical flow, signature-based corner points and binary robust invariant scalable keypoints. These action descriptors are applied to silhouette and silhouette's skeleton frames. These aforementioned action descriptors lead to developing the concatenated action descriptor (CAD). In order to develop action descriptors, the reference video frame plays an important role. Weizmann (one person) and both clean and noise versions of SBU Kinect Interaction (two persons) datasets are used for the evaluation of their proposed descriptors. On the other hand, classifications are performed by using support vector machine. Experimental results demonstrate that CAD not only outperforms among the entire proposed descriptors, but also provides better performance as compared to state-of-the-art approaches.																	1751-9659	1751-9667				FEB 28	2020	14	3					417	422		10.1049/iet-ipr.2018.6437													
J								Design of vision-based indoor positioning based on embedded system	IET IMAGE PROCESSING										learning (artificial intelligence); object detection; indoor radio; cameras; video signal processing; image motion analysis; video surveillance; computer vision; Gaussian processes; surveillance; foreground object; embedded system; indoor positioning techniques; surveillance cameras; security persons; monitors all the time; vision-based indoor positioning system; frame processing technique; Gaussian mixture learning; video background model		Indoor positioning techniques have become very important in recent years. Due to the wide deployment of surveillance cameras, it has become feasible to use the videos for indoor positioning. The success of using this approach can also reduce the load of security persons of watching the monitors all the time. In this study, the authors propose a vision-based indoor positioning system. The proposed method uses a frame processing technique and applies the Gaussian mixture learning for video background model. The foreground object can be extracted by using the background subtraction. Based on the foreground object, the objects can be tracked and used in the direct linear transform, and generate a bird's-eye map with camera information. A real-time demonstration has been also provided. It shows the tracing of the moving objects and the bird's-eye view.																	1751-9659	1751-9667				FEB 28	2020	14	3					423	430		10.1049/iet-ipr.2018.6285													
J								BM3D-GT&AD: an improved BM3D denoising algorithm based on Gaussian threshold and angular distance	IET IMAGE PROCESSING										image denoising; Gaussian noise; image matching; image filtering; BM3D-GT&amp; AD; improved BM3D denoising algorithm; Gaussian threshold; three-dimensional filtering; image denoising; hard thresholding; transform domain; frequency domain; image detail information; low amplitude; adaptable threshold; Gaussian function; high-frequency information; normalised angular distance; higher peak signal-to-noise ratio; Gaussian noise; denoised images; block-matching and three-dimensional filtering		Block-matching and three-dimensional filtering (BM3D) is generally considered as a milestone for its outstanding performance in the area of image denoising. However, it still suffers from the loss of image detail due to the utilisation of hard thresholding on transform domain during the phase of the basic estimate. In the frequency domain, a large amount of image detail information is in high frequency, which tends to be mixed with noise. Since its low amplitude is below the threshold, some image detail is filtered out with the noise. To retain more details, this study proposes an improved BM3D. It adopts an adaptable threshold with the core of Gaussian function during hard thresholding, which can filter out more noise while retaining more high-frequency information. When grouping, the normalised angular distance is taken as a measure of similarity to relieve the interference of noise further and achieve a higher peak signal-to-noise ratio (PSNR). The experimental results show that under the background of Gaussian noise with standard deviation of 20-60, the PSNR of denoised images (with a large amount of detail), applied with the authors' improved algorithm, can be improved by $0.1 - 0.4 \, {\rm dB}$0.1-0.4dB compared with original BM3D.																	1751-9659	1751-9667				FEB 28	2020	14	3					431	441		10.1049/iet-ipr.2019.0469													
J								Integrating Taylor-Krill herd-based SVM to fuzzy-based adaptive filter for medical image denoising	IET IMAGE PROCESSING										support vector machines; adaptive filters; optimisation; medical image processing; image denoising; image filtering; fuzzy set theory; decision making; search problems; stochastic processes; Taylor-Krill herd-based support vector machine; Taylor-KH-based SVM; Taylor series; KH optimisation algorithm; SVM classifier; Taylor-Krill herd-based SVM; medical image denoising; medical imaging systems; effective decision-making; medical devices; fuzzy-based adaptive filter; optimal weights; local optimisers; adaptive operators; stochastic search; hyperplane parameters; noisy pixels; optimal identification; peak signal-to-noise ratio; PSNR; structural similarity; SSIM	NONLOCAL MEANS; CLASSIFICATION; REMOVAL; SPARSE; NOISE	Medical imaging systems contribute much towards effective decision-making by the physicians, which is highly essential in the day-to-day life of humans. In this study, Taylor-Krill herd (KH)-based support vector machine (SVM) is proposed for medical image denoising. The Taylor-KH-based SVM is the integration of Taylor series in KH optimisation algorithm, which is used for tuning the optimal weights of the SVM classifier. The efficiency of KH is due to two global and two local optimisers, and the adaptive operators ensure the adaptive nature of KH. Above all, KH never uses the derivative information as it employs the stochastic search and thereby, reduces the complexity of the algorithm. The proposed method tunes the hyperplane parameters of SVM optimally so that the optimal identification of the noisy pixels in the image is ensured and replaced with adaptive weights. The proposed method is analysed based on the metrics, such as peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and the comparative analysis is done with existing methods for showing the effectiveness of the proposed method. The simulation result shows that the proposed method acquired a PSNR of 30.36 dB and SSIM of 0.89, respectively.																	1751-9659	1751-9667				FEB 28	2020	14	3					442	450		10.1049/iet-ipr.2018.6434													
J								Soft biometrics-based face image retrieval using improved grey wolf optimisation	IET IMAGE PROCESSING										biometrics (access control); image matching; feature extraction; optimisation; neural nets; face recognition; image retrieval; soft biometrics-based face image retrieval; face recognition; evidence image; unconstrained images; input face image; scale face dataset; similar faces; convolutional neural network approach; improved grey wolf optimisation approach; soft biometric-based face matcher; exact face image; deep feature retrieval; soft biometric face matcher	RECOGNITION	Humans often use faces to recognise and identify individuals. Face recognition is one of the important tasks carried out by forensic examiners manually during their investigation, when there is an evidence image/video available from a crime scene. There is a growing demand for face recognition from unconstrained images, which is valuable for criminal investigators in identifying the victims. When an input face image is given to the proposed system, it filters from large scale face dataset to find the top-k similar faces. Deep convolutional neural network approach is employed to extract important features present in the input face image and improved grey wolf optimisation approach is proposed to select optimal features from the extracted features. It is then preceded by a soft biometric-based face matcher that helps in retrieving exact face image from the top-k similar faces matched using approximate nearest neighbour. The performance of the proposed system is evaluated using LFW, CASIA, Multi-pie and Color-Feret datasets. The proposed system addresses the challenge of searching face images from a large collection of unconstrained images by incorporating feature retrieval using DCNN and IGWO with soft biometric face matcher in cascaded framework which improves accuracy and reduces computation and retrieval time.																	1751-9659	1751-9667				FEB 28	2020	14	3					451	461		10.1049/iet-ipr.2019.0271													
J								Enhanced copy-paste forgery detection in digital images using scale-invariant feature transform	IET IMAGE PROCESSING										copy protection; pattern clustering; feature extraction; transforms; image forensics; object detection; enhanced copy-paste forgery detection; digital images; digital forensics; image region; clustering techniques; false pixel detections; SIFT features; copy-pasted pixels; public image datasets MICC-F; copied regions; scale-invariant feature transform; image forgery detection; image forgery localisation; density-based spatial clustering; hierarchical agglomerative clustering; MICC-F8 multidataset; MICC-F2000 dataset; multiple copy-pasted region detection; sensitivity-based clustering		Image forgery detection and localisation is one of the principal problems in digital forensics. Copy-paste forgery in digital images is a type of forgery in which an image region is copied and pasted at another location within the same image. In this work, the authors propose a methodology to detect and localise copy-pasted regions in images based on scale-invariant feature transform (SIFT). Existing copy-paste forgery detection in images using SIFT and clustering techniques such as hierarchical agglomerative and density-based spatial clustering of applications with noise resulted many false pixel detections. They have introduced sensitivity-based clustering along with SIFT features to identify copy-pasted pixels and disregard the false pixels. Experimental evaluation on public image datasets MICC-F220, MICC-F2000 and MICC-F8 multi shows that the proposed method is showing improved performance in detecting and localising copy-paste forgeries in images than the existing works. Also the proposed work detects multiple copy-pasted regions in the images and is robust to attacks such as geometrical transformation of copied regions such as scaling and rotation.																	1751-9659	1751-9667				FEB 28	2020	14	3					462	471		10.1049/iet-ipr.2019.0842													
J								Edge-aware image filtering using a structure-guided CNN	IET IMAGE PROCESSING										image segmentation; image restoration; computer vision; filtering theory; neural nets; edge detection; feature extraction; image enhancement; image denoising; restoration problems; edge-preserving smoothing; image denoising; edge-aware image filtering; structure-guided; fundamental preprocessing step; accurate computer vision applications; robust computer vision applications; image segmentation; convolutional neural network-based methods; significant edge information; feature extraction layers; deep CNN model; network model; convolution artefact removal; structure extraction network; end-to-end trainable architecture; significant edges; state-of-the-art denoising filters; image enhancement		Image filtering is a fundamental preprocessing step for accurate, robust computer vision applications such as image segmentation, object classification, and reconstruction. However, many convolutional neural network (CNN)-based methods tend to lose significant edge information in the output layer, and generate undesired artefacts in the feature extraction layers. This study presents a deep CNN model for edge-aware image filtering. The proposed network model consists of three sub-networks: (i) feature extraction, (ii) convolution artefact removal, and (iii) structure extraction networks. The proposed network model has an end-to-end trainable architecture that does not need any post-processing steps. Especially, the structure extraction network can successfully preserve significant edges. The proposed filter outperforms state-of-the-art denoising filters in terms of both objective and subjective measures, and can be used for various image enhancement and restoration problems such as edge-preserving smoothing, image denoising, deblurring, and deblocking.																	1751-9659	1751-9667				FEB 28	2020	14	3					472	479		10.1049/iet-ipr.2018.6691													
J								Effect of pooling strategy on convolutional neural network for classification of hyperspectral remote sensing images	IET IMAGE PROCESSING										remote sensing; neural nets; image representation; geophysical image processing; image classification; convolution; stochastic pooling; rank-based average pooling; rank-based weighted pooling; hyperspectral remote sensing datasets; max pooling; pooling strategy; hyperspectral remote sensing images; deep convolutional neural network; convolution layer; pooling layer; deep CNN	DIMENSIONALITY; IDENTIFICATION	The deep convolutional neural network (CNN) has recently attracted the researchers for classification of hyperspectral remote sensing images. The CNN mainly consists of convolution layer, pooling layer and fully connected layer. The pooling is a regularisation technique and improves the performance of CNN while reducing the computation time. Various pooling strategies have been developed in literature. This study shows the effect of pooling strategy on the performance of deep CNN for classification of hyperspectral remote sensing images. The authors have compared the performance of various pooling strategies such as max pooling, average pooling, stochastic pooling, rank-based average pooling and rank-based weighted pooling. The experiments were performed on three well-known hyperspectral remote sensing datasets: Indian Pines, University of Pavia and Kennedy Space Center. The proposed experimental results show that max pooling has produced better results for all the three considered datasets.																	1751-9659	1751-9667				FEB 28	2020	14	3					480	486		10.1049/iet-ipr.2019.0561													
J								Image classification using SLIC superpixel and FAAGKFCM image segmentation	IET IMAGE PROCESSING										image representation; computer vision; learning (artificial intelligence); image segmentation; pattern clustering; iterative methods; image classification; feature extraction; radial basis function networks; image classification; Caltech 101 dataset; Caltech 256 dataset; SLIC superpixel; FAAGKFCM image segmentation; simple linear iterative clustering superpixel; speeded up robust feature; deep features; MIT 67 image datasets; automatically adjustable Gaussian radial basis function kernel-based fuzzy C-means	OBJECT CATEGORIZATION; SCALE; CODEBOOKS; FEATURES; SPARSE; SCENE; SHIFT	Image classification is one of the popular fields for researchers in computer vision. This study highlights the use of simple linear iterative clustering (SLIC) superpixel in combination with fast and automatically adjustable Gaussian radial basis function kernel-based fuzzy C-means (FAAGKFCM) for image segmentation along with the deep learning techniques. Bag-of-feature with speeded up robust feature along with deep features are used for classification of 101 classes of the image and 256 classes of the image from Caltech 101, Caltech 256 and MIT 67 image datasets. The combination of SLIC superpixel with FAAGKFCM image segmentation acts as the pre-processing step for image classification, which in turn provides a better result in the classification of images. This method has achieved an accuracy of 94% in Caltech 101 dataset, 85% in Caltech 256 dataset and 84% in MIT 67 dataset.																	1751-9659	1751-9667				FEB 28	2020	14	3					487	494		10.1049/iet-ipr.2019.0255													
J								Fully automated scheme for computer-aided detection and breast cancer diagnosis using digitised mammograms	IET IMAGE PROCESSING										medical image processing; image texture; support vector machines; learning (artificial intelligence); wavelet transforms; mammography; image classification; cancer; feature extraction; object detection; expectation-maximisation algorithm; image resolution; statistical analysis; feature selection; correlation methods; computer-aided detection; breast cancer diagnosis; public health problem; benign-malignant patterns; digital mammograms; fully automated computer-aided diagnosis system; low-computational requirements; textural features; multiorientation features; feature extraction technique; hybrid feature selection approach; support vector machine recursive feature elimination; CAD system; breast cance early detection; expectation-maximisation algorithm; region of interest extraction; ROIs; multiresolution features; wavelet-based contourlet transform; correlation bias reduction algorithm; Q similarity-based learning algorithm; benign-malignant classification; real-time systems; real clinical mammograms; time 2; 2 s	FEATURE-SELECTION; CLASSIFICATION; SEGMENTATION; STATISTICS	Breast cancer becomes a significant public health problem in the world. During the early detection of breast cancer, it is a very challenging task to classify accurately the benign-malignant patterns in digital mammograms. This study proposes a new fully automated computer-aided diagnosis (CAD) system for breast cancer diagnosis with high-accuracy and low-computational requirements. The expectation-maximisation algorithm is investigated to extract automatically the region of interests (ROIs) within mammograms. The standard shape, statistical, and textural features of ROIs are extracted and combined with multi-resolution and multi-orientation features derived from a new feature extraction technique based on wavelet-based contourlet transform. A hybrid feature selection approach based on combining the support vector machine recursive feature elimination with correlation bias reduction algorithm is proposed. Also, the authors investigate a new similarity-based learning algorithm, called Q, for benign-malignant classification. The proposed CAD system is applied to real clinical mammograms, and the experimental results demonstrate the superior performance of the proposed CAD system over other existing CAD systems in terms of accuracy 98.16%, sensitivity 98.63%, specificity 97.80%, and computational time 2.2 s. This reveals the effectiveness of the proposed CAD system in improving the accuracy of breast cancer diagnosis in real-time systems.																	1751-9659	1751-9667				FEB 28	2020	14	3					495	505		10.1049/iet-ipr.2018.5953													
J								Left ventricular segmentation based on a parallel watershed transformation towards an accurate heart function evaluation	IET IMAGE PROCESSING										image segmentation; biomedical MRI; cardiology; medical image processing; cardiovascular system; cardiac magnetic resonance image; cardiac cavities; semiautomated segmentation methods; cardiac cine-MR short-axis images; left ventricular functions; left ventricle parameters; parallel watershed-based approach; end-systolic volume; cardiac output; left ventricular mass; 20 patients; faithful segmentation; left ventricular segmentation; parallel watershed transformation; accurate heart function evaluation; magnetic resonance imaging; golden reference; cardiac examination; human cardiovascular morphology	SHORT-AXIS; MR-IMAGES; AUTOMATIC SEGMENTATION; LEVEL SET; EJECTION FRACTION; MODEL; REGISTRATION; COMPUTATION; INTENSITY; TRACKING	Magnetic resonance imaging (MRI) has emerged as the golden reference for cardiac examination. This modality allows the assessment of human cardiovascular morphology, functioning, and perfusion. Although a couple of challenging issues, such as the cardiac magnetic resonance (MR) image's features and the large variability of images among several patients, still influences the cardiac cavities' segmentation and needs to be carried out. In this study, the authors have profoundly reviewed and fully compared semi-automated segmentation methods performed on cardiac cine-MR short-axis images for the evaluation of the left ventricular functions. However, the number of parameters handled by the synthesised works is limited if not null. For the sake of ensuring the highest coverage of the left ventricle parameters computing, they have introduced a parallel watershed-based approach to segment the left ventricular allowing hence the computation of six parameters (end-diastolic volume, end-systolic volume, ejection fraction, cardiac output, stroke volume, and left ventricular mass). An algorithm is associated with the main considered measurements. The experimental results that were obtained through studying 20 patients' MRI data base demonstrate their approach's accuracy in estimating real values of the parameters' set thanks to a faithful segmentation of the myocardium.																	1751-9659	1751-9667				FEB 28	2020	14	3					506	517		10.1049/iet-ipr.2018.6379													
J								Novel multiple images encryption algorithm using CML system and DNA encoding	IET IMAGE PROCESSING										DNA; cryptography; matrix algebra; image coding; statistical analysis; CML system; DNA encoding; coupled map lattices system; permutation-diffusion structure; multiple images encryption; SHA hash key; DNA-level multiplication; dynamic DNA coding-decoding	PERMUTATION-DIFFUSION; WAVELET TRANSFORM; HYPER-CHAOS; CRYPTANALYSIS; SCHEME; MAP; ENHANCEMENTS	In this study, the authors advance a novel multiple images encryption algorithm using the coupled map lattices (CML) system and DNA encoding. The algorithm adopts a permutation-diffusion structure. First, the initial values and parameters of the CML system are determined by given values and SHA hash key of original images. Secondly, several plain images are grouped and permuted among groups. Next, different groups are combined into eight blocks of the same size, each of which is independently coded and scrambled. Thirdly, in the diffusion process, a DNA-level multiplication operation is redefined, and required key matrices are resulted from a small key connected with original images. Finally, dynamic DNA coding/decoding is adopted, and the coding and decoding rules are determined by original images. The evaluation of the simulation experiment shows that the proposed algorithm is safe and feasible, and has good encryption effect.																	1751-9659	1751-9667				FEB 28	2020	14	3					518	529		10.1049/iet-ipr.2019.0771													
J								Application of random elements in ISS	IET IMAGE PROCESSING										data encapsulation; cryptography; polynomials; image processing; random elements; ISS; secret image; n shares; k shares; visual secret sharing; comprehensible share	VISUAL CRYPTOGRAPHY; SHARING SCHEME; RECOVERY	The (k,n)-threshold image secret sharing (ISS) encodes a secret image into n shares. When k or more shares are obtained, the secret image can be decoded; however, less than k shares could decode none of the secret image. ISS primarily includes polynomial-based ISS and visual secret sharing (VSS). In this study, the authors find that the random elements in ISS can be used not only to hide information but also to obtain more features such as multiple decryptions and comprehensible share. They have established an application model of random elements that is suitable for both polynomial-based ISS and VSS. On the basis of the model, they have extended three algorithms to achieve information hiding, multiple decryptions and comprehensible share. Experiments indicate the effectiveness of these algorithms.																	1751-9659	1751-9667				FEB 28	2020	14	3					530	535		10.1049/iet-ipr.2018.5648													
J								Automatic thresholding using a modified valley emphasis	IET IMAGE PROCESSING										medical image processing; image segmentation; automatic thresholding; modified valley emphasis; Otsu's method; important algorithm category; intra-class variances; improved Otsu's methods; valley emphasis method; nonoptimal segmentation performance; modified valley metric using second-order derivative; Otsu's algorithm; typical test images whose histograms; 22 cell images; existing improved algorithms; image subtypes; image segmentation field	IMAGE SEGMENTATION; OTSU	Otsu's method is one of the most well-known methods for automatic thresholding, which serves as an important algorithm category for image segmentation. However, it fails if the histogram is close to unimodal or has large intra-class variances. To alleviate this limitation, improved Otsu's methods such as the valley emphasis method and weighted object variances method have been proposed, which still yield non-optimal segmentation performance in some cases. In this study, a modified valley metric using second-order derivative is proposed to improve the Otsu's algorithm. Experiments are firstly conducted on five typical test images whose histograms are unimodal, multimodal or have large intra-class variances, and then expanded to a larger data set consisting of 22 cell images. The proposed algorithm is compared with original Otsu's method and existing improved algorithms. Four evaluation metrics including misclassification error, foreground recall, Dice similarity coefficient and Jaccard index are adopted to quantitatively measure the segmentation performance. Results show that the proposed algorithm achieves best segmentation results on both data sets quantitatively and qualitatively. The proposed algorithm adapts the Otsu's method to more image subtypes, indicating a wider application in automatic thresholding and image segmentation field.																	1751-9659	1751-9667				FEB 28	2020	14	3					536	544		10.1049/iet-ipr.2019.0176													
J								Quality assessment framework for video contextualisation of personal videos	IET IMAGE PROCESSING										motion estimation; meta data; image sequences; visual databases; video signal processing; data compression; image restoration; video coding; image colour analysis; quality assessment framework; video contextualisation; personal videos; rapid transformations; processing power; handheld device; personal video capture; damaged video frames; uni-coloured frames; video quality metadata; contextualisation process; motion estimation		The world is witnessing rapid transformations in hardware technology. This will keep on improving day by day. The processing power of every handheld device is significantly improved as well as the storage capacity is increased. With all these advancements, the personal video capture and usage of videos have tremendously increased across many applications. The quality assessment of personal videos has become very important. It is a vital task to design a model for assessing the video quality. A novel methodology for detecting damaged video frames is proposed here. The primary objective of the research is to detect uni-coloured frames and frames with ice effect. The novel histograms bin comparison technique is proposed for inter- and intraframe analysis. The shakiness of the video is calculated using motion estimation. The video quality is also assessed using blur detection as well as contrast calculation to spot useful portion in the video. The proposed framework generates the video quality metadata and supports the contextualisation process.																	1751-9659	1751-9667				FEB 28	2020	14	3					545	551		10.1049/iet-ipr.2018.6022													
J								Image denoising using 2D orthogonal locality preserving discriminant projection	IET IMAGE PROCESSING										image denoising; transforms; pattern clustering; learning (artificial intelligence); 2D orthogonal locality; discriminant projection; domain techniques; noisy image; structural similarity; transformed domain; orthogonal locality preserving projection; locality preserving nature; image patches; nonlocal self-similarity based image denoising approaches; two-dimensional orthogonal locality; denoising denoising performance; comparable denoising performance		Image denoising using a combination of non-local self-similarity and transformed domain techniques has become popular in past few years. Instead of working on independent pixels, patches extracted from the noisy image are grouped together based on structural similarity and noise elimination is performed in transformed domain. Orthogonal locality preserving projection and its variant that processes the images directly in matrix format have been used for image denoising recently. Locality preserving nature of these techniques takes care of similarity within image patches while learning the basis, hence reducing the task of grouping patches explicitly. Non-local self-similarity based image denoising approaches perform patch grouping based on structural similarity. Discriminant information, if considered can play pivotal role in achieving superior clustering of data and thereby is expected to enhance the quality of denoising. With this aim in mind, two-dimensional (2D) orthogonal locality preserving discriminant projection is formulated in this study. While learning the basis, along with the similarity, proposed approach also takes into account dissimilarity between patches. A global basis thus learnt from the noisy image is used for denoising and comparable denoising performance is shown relative to the state-of-the-art methods.																	1751-9659	1751-9667				FEB 28	2020	14	3					552	560		10.1049/iet-ipr.2019.0436													
J								Design of fuzzy inference system for apple ripeness estimation using gradient method	IET IMAGE PROCESSING										agricultural products; learning (artificial intelligence); image colour analysis; gradient methods; image classification; food products; fuzzy neural nets; fuzzy set theory; nearest neighbour methods; fuzzy reasoning; fuzzy inference system; apple ripeness estimation; gradient method; fuzzy classification approach; colour features; apple fruits; maturity stages; k nearest neighbour algorithm; fruit image; green area; yellow area; red area; image database; testing phases; classifier parameters; classification evaluation metrics; trained fuzzy classifier	MACHINE VISION; RIPENING STAGES; MATURITY	In this study, a fuzzy classification approach based on colour features has been investigated to estimate the ripeness of apple fruits according to three maturity stages; unripe, turning-ripe and ripe. The K nearest neighbour algorithm was applied in order to segment the fruit image into four regions namely background, green area, yellow area and red area. The last three regions represent the colour features and were subsequently given as inputs to the fuzzy classifier. Gradient method has been used for tuning the fuzzy classifier in order to obtain the best performance. Image database used for simulation has been collected and exploited for the training and testing phases using cross-validation. Simulation results indicate that the best classifier parameters can be obtained. The efficiency of the proposed system compared with the non-use of the gradient method has been proved by the confusion matrix and the most known classification evaluation metrics. Moreover, the trained fuzzy classifier demonstrates its outperformance in terms of accuracy and execution time compared with other existing methods.																	1751-9659	1751-9667				FEB 28	2020	14	3					561	569		10.1049/iet-ipr.2018.6524													
J								Fast search real-time face recognition based on DCT coefficients distribution	IET IMAGE PROCESSING										discrete cosine transforms; feature extraction; face recognition; visual databases; real-time systems; fast search real-time face recognition; DCT coefficients distribution; adaptive face recognition algorithm; face images; nonface information; image transformation; key coefficients; training database; fast search mode; face features; recognition rate; ORL face database testing; real-time recognition; camera imaging		The authors propose an adaptive face recognition algorithm based on the discrete cosine transform (DCT) coefficients approach. For the database's establishment, the face images are pre-processed with colour transform, hair cutting, and background removing to eliminate non-face information. The recognised kernel applied the weights of DCT coefficient distribution with the entire image transformation, to avoid position mismatch and reduce the light effect. The key coefficients of DCT are chosen from the training database by maximum variance. The fast search mode can reject 90% weak candidates with few coefficients to fasten the processing speed. The significant coefficients weighting methods are used to enhance face features. Only using 50 coefficients per picture, the recognition rate can achieve 95% for ORL face database testing. For real-time recognition, camera imaging is processed with algorithms using C-programming based on Windows system. The recognition rate can achieve 95% and the speed is about nine frames per second for real-time recognition in practice.																	1751-9659	1751-9667				FEB 28	2020	14	3					570	575		10.1049/iet-ipr.2018.6175													
J								Image segmentation algorithm based on neutrosophic fuzzy clustering with non-local information	IET IMAGE PROCESSING										image segmentation; fuzzy set theory; pattern clustering; nonlocal information; antinoise performance; image segmentation algorithm; data distribution; deterministic subset; clustering centre; fuzzy subset; nonlocal pixel correlation; neutrosophic fuzzy mean clustering algorithm; synthetic images; medical images; natural images		To improve the boundary processing ability and anti-noise performance of image segmentation algorithm?a neutrosophic fuzzy clustering algorithm based on non-local information is proposed here. Initially, the proposed approach uses the data distribution of deterministic subset to determine the clustering centre of the fuzzy subset. Besides, the fuzzy non-local pixel correlation is introduced into the neutrosophic fuzzy mean clustering algorithm. The experimental results on synthetic images, medical images and natural images show that the proposed method is more robust and more accurate than the existing clustering segmentation methods.																	1751-9659	1751-9667				FEB 28	2020	14	3					576	584		10.1049/iet-ipr.2018.5949													
J								Goal programming technique for solving fully interval-valued intuitionistic fuzzy multiple objective transportation problems	SOFT COMPUTING										Multi-objective programming; Interval-valued triangular intuitionistic fuzzy numbers; Fuzzy goal programming; Expected value; Membership functions	EFFICIENT SOLUTIONS; OPTIMIZATION	In transportation problems, the cost depends on various irresistible factors like climatic conditions, fuel expenses, etc. Consequently, the transportation problems with crisp parameters fail to handle such situations. However, the construction of the problems under an imprecise environment can significantly tackle these circumstances. The intuitionistic fuzzy number associated with a point is framed by two parameters, namely membership and non-membership degrees. The membership degree determines its acceptance level, while the non-membership measures its non-belongingness (rejection level). However, a person, because of some hesitation, instead of giving a fixed real number to the acceptance and rejection levels, may assign them intervals. This new construction not only generalizes the concept of intuitionistic fuzzy theory but also gives wider scope with more flexibility. In the present article, a balanced transportation problem having all the parameters and variables as interval-valued intuitionistic fuzzy numbers is formulated. Then, a solution methodology based on goal programming approach is proposed. This algorithm not only cares to maximize the acceptance level of the objective functions but simultaneously minimizes the deviational variables attached with each goal. To tackle the interval-valued intuitionistic fuzzy constraints corresponding to each objective function, three membership and non-membership functions, linear, exponential and hyperbolic, are used. Further, a numerical example is solved to demonstrate the computational steps of the algorithm, and a comparison is drawn amidst linear, exponential and hyperbolic membership functions.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13955	13977		10.1007/s00500-020-04770-6		FEB 2020											
J								Autonomous mobile robot path planning in unknown dynamic environments using neural dynamics	SOFT COMPUTING										Obstacle avoidance; Path planning; Neural dynamics	OBSTACLE AVOIDANCE; NETWORK; MANIPULATORS; NAVIGATION; STRATEGY	In this paper, a novel variant of bio-inspired planning algorithms is presented for robot collision-free path planning in dynamic environments without prior information. The first contribution of this paper is that, with mild technical analysis, the traditional neural dynamic model almost always returns a sub-optimal choice in some challenging scenarios, such as the boundary map and the narrow pathway map. Second, the proposed planning algorithm, namely the padding mean neural dynamic model, is a topologically organized network with connections among neighbouring neurons and is good for spreading nerve impulses such as a waves without coupling effects. The signal transduction method within a network is based on a dynamic neural activity field, which propagates high neural activity from the target state to the whole field, excluding obstacle regions. Third, simulation studies are conducted to compare the performance of the proposed planning algorithm and other popular planning algorithms in terms of effectiveness and efficiency. As a result, the proposed method can drive a robot to find more reasonable paths in both static maps and unknown dynamic scenarios with moving obstacles and a moving target. Finally, the novel excitatory input design of the proposed algorithm is discussed and analysed to explore the neural stimulus propagation mechanism within the network.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13979	13995		10.1007/s00500-020-04771-5		FEB 2020											
J								Study on the algorithm for smart community sensor network routing with adaptive optimization via cluster head election	COMPUTATIONAL INTELLIGENCE										adaptive ant colony; intelligent community; network life; sensor networks; three-dimensional clustering routing		To reduce the uneven energy consumption for the data transmission and extend network life of intelligent community sensor network, an adaptive routing optimized algorithm for intelligent community sensor networks with cluster head election is proposed. In this algorithm, a three-dimensional clustering method adapted to the structure of intelligent community sensor network is proposed. The three-dimensional clustering method uses the cluster head election mechanism based on minimizing the total transmission loss to optimize the energy of the intelligent community sensor network. Second, an adaptive ant colony propagation method is proposed to solve the problem of intercluster data propagation after clustering. With the best path finding algorithm of ant colony algorithm, energy balance routing with lower energy loss and lower packet error rate is proposed. Finally, the simulation results show that the algorithm has better performance in reducing energy consumption and delay, improving transmission efficiency and node survival time.																	0824-7935	1467-8640															10.1111/coin.12304		FEB 2020											
J								Similarity measure with indeterminate parameters regarding cubic hesitant neutrosophic numbers and its risk grade assessment approach for prostate cancer patients	APPLIED INTELLIGENCE										Prostate cancer; Risk grade assessment; Cubic hesitant neutrosophic number; Parameterized cubic hesitant fuzzy set; Generalized distance; Similarity measure	DECISION-MAKING; AGGREGATION OPERATORS; FUZZY-SETS; ANTIGEN; ENZALUTAMIDE	The prostate cancer (PC) of older men has become an important problem with the increment of aging degree in current society. Then, it is necessary to give the timely detection and reasonable risk assessment of a PC patient so as to provide a suitable treatment option for the PC patient. However, there may exist the hybrid information of an interval-valued fuzzy number (IVFN) and a hesitant indeterminate number/hesitant neutrosophic number (HNN) together in risk data of PC patients, while existing PC risk grade assessment approaches with the cubic hesitant fuzzy set (CHFS) composed of an IVFN and a hesitant fuzzy set cannot carry out such a risk grade assessment problem with the hybrid information of both IVFN and HNN. To solve the issue, for the first time this study proposes a cubic hesitant neutrosophic number (CHNN) to express the hybrid information of both IVFN and HNN and then introduces a similarity measure with an indeterminate parameter regarding CHNNs for the risk grade assessment of PC patients. In this paper, therefore, a concept of a CHNN set is firstly presented to suitably express the hybrid information of both IVFN and HNN as the generalization of CHFS. Then, the CHNN set is transformed into the parameterized CHFS (P-CHFS) for de-neutrosophication by means of an indeterminate parameter. Next, generalized distance and similarity measures between P-CHFSs are proposed based on the least common multiple cardinality/number (LCMC) extension method. Thus, a PC risk assessment approach is developed by using the similarity measure of P-CHFSs with the physician's optimistic, moderate and pessimistic attitudes under CHNN environment. Finally, sixteen clinical cases are provided as risk assessment examples of PC patients to indicate the effectiveness and applicability of the proposed PC risk assessment approach in CHNN setting. However, the proposed assessment approach can effectively and flexibly deal with assessment problems of PC risk grades regarding the physicians' optimistic, moderate and pessimistic attitudes in CHNN setting, which shows its advantage.																	0924-669X	1573-7497				JUL	2020	50	7					2120	2131		10.1007/s10489-020-01653-z		FEB 2020											
J								Research on the multiple fuzzy parametric fuzzy sets and its framework of clustering algorithm	EVOLUTIONARY INTELLIGENCE										Multiple fuzzy parametric fuzzy sets; Density peaks-based clustering; Minimum granularity-based key parametric method; Clustering algorithm for multiple fuzzy parametric fuzzy sets	TERMINOLOGICAL DIFFICULTIES; TYPE-2; SYSTEMS	This article first introduces the current research situation of the theory of fuzzy sets, and then, on the basis of type-1 fuzzy sets, according to the different forms and numbers of the parameters of membership functions, put forward the definitions of classical parametric fuzzy sets, multiple classical parametric fuzzy sets, semi fuzzy parametric fuzzy sets, fuzzy parametric fuzzy sets, multiple fuzzy parametric fuzzy sets, etc. Their mathematical expressions are given. On this basis, the multiple parameter fuzzy sets are compared with type-2 fuzzy sets, interval-valued type-2 fuzzy sets, intuitionistic fuzzy sets and interval-valued intuitionistic fuzzy sets. Subsequently, based on clustering by fast search and find of density peaks (CFSFDP), the method and steps of multiple fuzzy parametric fuzzy sets clustering, called "clustering for multiple fuzzy parametric fuzzy sets by fast search and find of density peaks" (MFPFS-CFSFDP), which is further discussed and proposed. In order to reduce the computational complexity of MFPFS-CFSFDP algorithm, referring to the concept of granularity and combining the characteristics of multiple fuzzy parametric fuzzy sets, we propose a "minimum granularity-based key parametric method" to improve the MFPFS-CFSFDP algorithm. Furthermore, two clustering algorithms, which called "clustering plus for multiple fuzzy parametric fuzzy sets by fast search and find of density peaks" (MFPFS-CFSFDP+) and "clustering plus plus for multiple fuzzy parametric fuzzy sets by fast search and find of density peaks" (MFPFS-CFSFDP++) are proposed. Finally, taking the multiple normal fuzzy parametric normal fuzzy sets as an example, the algorithm framework tables of these three clustering algorithms are given, and their advantages and disadvantages are summarized, they provide a theoretical basis for further research on clustering for multiple fuzzy parametric fuzzy sets.																	1864-5909	1864-5917				JUN	2020	13	2			SI		159	183		10.1007/s12065-020-00354-3		FEB 2020											
J								An evolution strategy based approach for cover scheduling problem in wireless sensor networks	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Wireless sensor networks; Cover scheduling; Evolution strategy; Evolutionary algorithm	BEE COLONY ALGORITHM; LIFETIME; SCHEME	Cover scheduling problem in wireless sensor networks (WSN-CSP) aims to find a schedule of covers which minimizes the longest continuous duration of time for which no sensor in the network is able to monitor a target. This problem arises in those sensing environments which permit the coverage breach, i.e., at any instant of time, all targets need not be monitored. The coverage breach may occur owing to either technical restrictions or intentionally. It is an NP-hard problem. This paper presents a (1+1)-evolution strategy based approach to address WSN-CSP problem. We have compared our approach with the state-of-art approaches available in literature. Computational results show that our approach is significantly superior in comparison to the existing approaches for WSN-CSP.																	1868-8071	1868-808X				SEP	2020	11	9					1981	2006		10.1007/s13042-020-01088-5		FEB 2020											
J								Prediction and failure analysis of composite resin restorations in the posterior sector applied in teaching dental students	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Dental caries; Failure; Competency based education; Assessment; CBR; Classifiers; Statistical test	PERFORMANCE; EDUCATION; CARIES; DIAGNOSIS; SUPPORT; LESIONS; SYSTEM; CBR	The failure of composite resin restorations in the posterior region is an ongoing concern in current clinical practice This study assesses possible factors and causes of the failure of restoration 1 year after their placement by fourth year dental students (on a 5-year degree course). While the systematic assessment of dental students does not appear to have received much attention in the field, this study asserts the need and benefit of an assessment methodology that can (1) predict the success or failure of restorations placed by dental students and (2) assist the clinical instructor in identifying the performance profile of each student. Eighty-one patients aged 26-77 years were treated by 81 undergraduates in a prospective cohort study from November 2013 to December 2015. One year after treatment, restorations were assessed by the same staff member who acted as the supervisor during the restoration placement. A CBR system was applied to make predictions about restorations. The CBR includes different machine learning techniques and statistical tests in the CBR cycle. The system calculates the relevant variables, which are used to predict failures. The accuracy of the system is measured with the AUC and the accuracy. The AUC obtained is 0.935 while the Kappa index and the accuracy are 91.36 and 0.75, respectively. In conclusion, factors related to the patient and to the treatment are associated to the failure of the restorative treatment. Of particular interest, the CBR was useful for the performance of a predictive model to estimate the probability of failure of resin restorations placed by students.																	1868-5137	1868-5145															10.1007/s12652-020-01804-7		FEB 2020											
J								Localization and segmentation of metal cracks using deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Segmentation; Convolutional neural networks; UNet; Morphological operations	IMAGE; ALGORITHM; TEXTURE; COLOR	Detection and quantification of defects from metal and metal-coated surfaces is one of the main challenges in computer vision-based semantic segmentation. Manual inspection is not a practically feasible solution for identifying defects such as surface-level cracks, scratches, and manufacturing mistakes, especially when we have to deal with large number of test subjects. The metal defects can be of different size, shape, and texture and often show close resemblance to the possible artefacts due to normal wear and tear and brush markings of metal coating. Hence it will be quite challenging to come up with an efficient segmentation method for quantifying such defects. In this work, we propose an automatic segmentation and quantification approach for inspecting defects from digital images of titanium-coated metal surfaces with a customized Deep learning architecture: UNet. The scheme uses a supervised learning approach with convolutional neural network (CNN) and can learn the suitable representations and features from the training data without any handcrafted features or human intervention. The proposed image segmentation method also uses appropriate pre-processing and post-processing stages. The input images are filtered using median filter for eliminating possible impulse noises, and the output mask generated from the CNN model is post-processed using suitable morphological operations for eliminating false detections. The detection and segmentation performance is evaluated using standard benchmarks, and the overall Dice score of the proposed model is 91.67% with a precision of 93.46%.																	1868-5137	1868-5145															10.1007/s12652-020-01803-8		FEB 2020											
J								Design of smart objects of fear with a taxonomy of factors affecting the user experience of exposure therapy systems for small-animal phobias	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mixed reality systems; Taxonomy; User experience; Exposure therapies; Small-animal phobias	AUGMENTED REALITY SYSTEM; DISORDERS; ANXIETY; PREVALENCE	In vivo exposure therapy (IVET) is the treatment of choice for specific phobias, including animal phobia. Recently virtual reality exposure therapy (VRET) and Augmented reality exposure therapy (ARET) have been presented and validated as suitable tools, however, to the best of our knowledge, no other work has identified nor provided an explicit classification or taxonomy to classify, compare and even inform the design of these kind of systems. In this work we propose a taxonomy of feedback factors that affect the experience of use of mixed reality systems for small-animal phobias. The taxonomy comprises feedback factors in three categories, namely realism, interaction, and intensity; while considering: auditory, haptic and visual dimensions for each. To illustrate this taxonomy in use, we conducted a survey to compare mixed reality applications for small-animal phobias, analysing their features according to the factors proposed in the taxonomy. In addition, we developed the concept of smart objects of fear by means of a design fiction world. In this world, in addition to IVET, VRET and ARET, where the objects of fear are either real (in the former) or virtual (in the two later), we envision a new kind of mixed reality therapy, where the objects of fear are represented using context-aware smart objects, capable of inferring the emotional state of the patient, and of adapting their behavior to improve the system's response during the therapy; which in turn, may enrich the patient's therapy experience. We validated the perceived usability of this design fiction world with an expert in exposure therapy for small-animal phobias, whom perceived it as useful and correct. Finally, we show how the use of the taxonomy allowed us to discuss the features of the proposed future technologies in terms of how they furnish each of the factors of the taxonomy.																	1868-5137	1868-5145															10.1007/s12652-020-01802-9		FEB 2020											
J								An efficient codebook generation using firefly algorithm for optimum medical image compression	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Firefly algorithm; Linde-Buzo-Gray; Medical image compression; Particle swarm optimization; Vector quantization	VECTOR QUANTIZATION	In recent times, the medical imaging becomes an indispensable tool in clinical practice. Due to the large volume of medical images, compression is needed to lessen the redundancies in the image and also to represent the image in shorter manner for effective transmission. In this paper, Linde-Buzo-Gray (LBG) algorithm was developed with vector quantization (VQ) for compressing the images, and it results in decent image quality. To further increase the image quality, optimization techniques [particle swarm optimization (PSO) and firefly algorithm (FA)] were used in LBG method to optimize the codebook for generating the global codebook. In the proposed work, LBG method was used to get the local codebooks and the obtained local codebooks were optimized by utilizing PSO. The optimized codebooks from PSO were again optimized by using FA that results in good quality of the image. In the experimental phase, the performance of the proposed work was compared with individual optimization techniques like PSO and FA. From the experimental study, the proposed work showed 1.2-6 dB improvement in image compression related to other existing approaches.																	1868-5137	1868-5145															10.1007/s12652-020-01782-w		FEB 2020											
J								IECA: an efficient IoT friendly image encryption technique using programmable cellular automata	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image cipher; IoT; Programmable cellular automata; Lightweight; Block cipher	NUMBER GENERATION; CHAOTIC SYSTEM; ALGORITHM; CRYPTOGRAPHY	Digital images play a vital role in multimedia communications in the modern era. With the advent of internet-of-things (IoT) applications, multimedia transfers are happening at a rapid pace. However, providing security to these data is essential if the data is sensitive. It is a challenging task in case of IoT applications due to the limitations of the sensors in terms of memory and computational efficiency. Therefore, conventional ciphers cannot be applied in the IoT devices. However, cellular automata (CA) can be used in this resource-constrained environment for providing security to the IoT devices, as it is inherently capable of creating complex patterns and pseudo-random sequences. It is also easy to implement in hardware. In this work, an IoT friendly programmable cellular automata (PCA) based block cipher called IECA is proposed. Further, randomness in the generated cipher-image has been tested using various statistical testings present in NIST test suite and DIEHARD test suite. The test results show that IECA generates high degree of randomness in the produced cipher-image. In addition to this, correlation, entropy and differential analysis of the proposed scheme justifies the robustness against different types of attacks. Experimental results show the efficiency of IECA as compared to the existing block ciphers.																	1868-5137	1868-5145															10.1007/s12652-020-01813-6		FEB 2020											
J								Lossless medical image compression algorithm using tetrolet transformation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Tetrolet transformation; Haar function; Wavelet technique; Lossless image; Lossy image		In many analytical applications the reuse of medical images plays an important role. In order to store the images, image compression is the technique used to save the storage space and the storage time. The image compression is used in image transmission for the fast transmission. Also in health care sector the compressed image helps to make it complete computerized. The noise interference is the major problem in the image compression. Thus some compression techniques are used to remove the noise added in medical images. This will improve the quality of the compressed image. In image compression the medical image can be processed in deep degree by de-noising, edge preservation and high compression rate. The aim of the paper is develop an efficient computational tetrolet transform, which can be used in lossless compression. For analysis purpose various wavelet techniques are used and result is compared with this techniques. From the analysis it is inferred that by proposed technique is able to achieve a high quality images with low noise.																	1868-5137	1868-5145															10.1007/s12652-020-01792-8		FEB 2020											
J								Joint maximization of accuracy and information for learning the structure of a Bayesian network classifier	MACHINE LEARNING										0/1 loss function; Bayesian network classifiers; Class imbalance; Information measures; Ordinal classification; Structure learning	PERFORMANCE-MEASURES; MAXIMIZING ACCURACY	Although recent studies have shown that a Bayesian network classifier (BNC) that maximizes the classification accuracy (i.e., minimizes the 0/1 loss function) is a powerful tool in both knowledge representation and classification, this classifier: (1) focuses on the majority class and, therefore, misclassifies minority classes; (2) is usually uninformative about the distribution of misclassifications; and (3) is insensitive to error severity (making no distinction between misclassification types). In this study, we propose to learn the structure of a BNC using an information measure (IM) that jointly maximizes the classification accuracy and information, motivate this measure theoretically, and evaluate it compared with six common measures using various datasets. Using synthesized confusion matrices, twenty-three artificial datasets, seventeen UCI datasets, and different performance measures, we show that an IM-based BNC is superior to BNCs learned using the other measures-especially for ordinal classification (for which accounting for the error severity is important) and/or imbalanced problems (which are most real-life classification problems)-and that it does not fall behind state-of-the-art classifiers with respect to accuracy and amount of information provided. To further demonstrate its ability, we tested the IM-based BNC in predicting the severity of motorcycle accidents of young drivers and the disease state of ALS patients-two class-imbalance ordinal classification problems-and show that the IM-based BNC is accurate also for the minority classes (fatal accidents and severe patients) and not only for the majority class (mild accidents and mild patients) as are other classifiers, providing more informative and practical classification results. Based on the many experiments we report on here, we expect these advantages to exist for other problems in which both accuracy and information should be maximized, the data is imbalanced, and/or the problem is ordinal, whether the classifier is a BNC or not. Our code, datasets, and results are publicly available http://www.ee.bgu.ac.il/similar to boaz/software.																	0885-6125	1573-0565				MAY	2020	109	5					1039	1099		10.1007/s10994-020-05869-5		FEB 2020											
J								Classification with costly features as a sequential decision-making problem	MACHINE LEARNING										Sequential classification; Costly features; Adaptive feature acquisition; Datum-Wise classification; Prediction on budget		This work focuses on a specific classification problem, where the information about a sample is not readily available, but has to be acquired for a cost, and there is a per-sample budget. Inspired by real-world use-cases, we analyze average and hard variations of a directly specified budget. We postulate the problem in its explicit formulation and then convert it into an equivalent MDP, that can be solved with deep reinforcement learning. Also, we evaluate a real-world inspired setting with sparse training datasets with missing features. The presented method performs robustly well in all settings across several distinct datasets, outperforming other prior-art algorithms. The method is flexible, as showcased with all mentioned modifications and can be improved with any domain independent advancement in RL.																	0885-6125	1573-0565				AUG	2020	109	8					1587	1615		10.1007/s10994-020-05874-8		FEB 2020											
J								Research on intelligent knowledge representation method and algorithm based on basic-element theory	NEURAL COMPUTING & APPLICATIONS										Basic-element theory; Knowledge representation method; Expand		At present, there are many ways to express knowledge, all of which play an important role in information system. However, these knowledge representation methods cannot reduce the contradiction between knowledge, nor can they effectively extend more new knowledge from existing knowledge. Therefore, this paper puts forward the basic-elements theory and studies the intelligent knowledge representation method of the basic-elements theory. This method studies things, features of things and their corresponding feature quantities as a whole, and USES basic elements to formally describe things, behaviors and relationships, and establishes extended models to express knowledge. The knowledge representation method based on the theory of basic elements can not only express knowledge more accurately and reduce the contradiction between knowledge, but also extend more knowledge from existing knowledge and systematically describe the development of things according to the development nature of basic elements. Experiments show that this method is superior to the common knowledge representation method in terms of expansion rate, clarity and simplification rate.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5353	5365		10.1007/s00521-020-04703-2		FEB 2020											
J								Neural Network-Based Optimal Tracking Control of Continuous-Time Uncertain Nonlinear System via Reinforcement Learning	NEURAL PROCESSING LETTERS										Neural network; Optimal tracking control; Continuous-time; Uncertain nonlinear system; Reinforcement learning	ADAPTIVE OPTIMAL-CONTROL; LINEAR-SYSTEMS; CONTROL SCHEME	In this note, optimal tracking control for uncertain continuous-time nonlinear system is investigated by using a novel reinforcement learning (RL) scheme. The uncertainty here refers to unknown system drift dynamics. Based on the nonlinear system and reference signal, we firstly formulate the tracking problem by constructing an augmented system. The optimal tracking control problem for original nonlinear system is thus transformed into solving the Hamilton-Jacobi-Bellman (HJB) equation of the augmented system. A new single neural network (NN)-based online RL method is proposed to learn the solution of tracking HJB equation while the corresponding optimal control input that minimizes the tracking HJB equation is calculated in a forward-in-time manner without requiring any value, policy iterations and the system drift dynamics. In order to relax the dependence of the RL method on traditional Persistence of Excitation (PE) conditions, a concurrent learning technique is adopted to design the NN tuning laws. The Uniformly Ultimately Boundedness of NN weight errors and closed-loop augmented system states are rigorous proved. Three numerical simulation examples are given to demonstrate the effectiveness of the proposed scheme.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2513	2530		10.1007/s11063-020-10220-z		FEB 2020											
J								Intelligent detection of edge inconsistency for mechanical workpiece by machine vision with deep learning and variable geometry model	APPLIED INTELLIGENCE										Inconsistent features; Deep learning; Contour edge; Variable parametric geometric model	AUTOMATIC DETECTION; DEFECT DETECTION; SURFACE DEFECT; IMAGES; ALGORITHM; OPTIMIZATION; ENHANCEMENT	Inconsistent edges of mechanical workpieces in the same batch are one of the main reasons that lead to different machining performance. A full recognition of the inconsistent features has significant impact on enhancement of their intelligent machining ability. An intelligent hybrid strategy is proposed for edge inconsistent feature detection by machine vision, in which deep learning is combined with variable geometric model together to conduct the function. A deep convolutional neural network based feature classification model is established with K-Means clustering tactic. Supported on the classification model, a variable geometric model for specific edge inconsistency is given with an inconsistency evaluation function to investigate the match degree between the geometric model and the actual detected edge, and then particle swarm optimization algorithm is applied to find the solution of this geometric model. Detection experiments are carried out on a domestic servo-driven vision measuring platform to verify the performance of the proposed approach. The results show that the combined scheme can classify the different type of geometric contour of edge features with 100% correctness, and better evaluation performance by dice similarity index and Hausdorff distance in comparisons with other recent candidate methodologies. It is also indicated that the presented method provides a good recognition of the geometrical shape with less than 0.06mm maximum error for workpiece with 142 x 119mm size in the visual field.																	0924-669X	1573-7497				JUL	2020	50	7					2105	2119		10.1007/s10489-020-01641-3		FEB 2020											
J								Modified gradient neural networks for solving the time-varying Sylvester equation with adaptive coefficients and elimination of matrix inversion	NEUROCOMPUTING										Time-varying sylvester equation; Modified gradient-based recurrent neural network (MGRNN); Adaptive coefficients; Elimination of the matrix inversion	ITERATIVE ALGORITHMS; SYSTEMS; CONVERGENCE; DESIGN	In scientific and engineering fields, the solutions to many problems can be transformed into finding the solutions to Sylvester equation, for which various computational methods (e.g., recurrent neural network, RNN) have been presented and investigated. RNN models are frequently used to solve computational problems due to the prevalent exploitation of the gradient-based RNN. However, the overlong convergent time and the too large residual error restrict the widespread applications of the RNN model in solving time-varying problems. Further, a special type of RNN named zeroing neural network (ZNN) is able to solve the time-varying Sylvester equation, which breaks the limitations mentioned above, but fails to handle complex time-varying problems owing to the sharp increment of the calculated amount in matrix inversion involved. To remedy the limitation, a modified gradient-based RNN (MGRNN) model is proposed to generate more accurate computational solutions with less convergent time and adaptive coefficients for solving the time-varying Sylvester equation, which replaces the matrix inversion problem with the matrix transposition problem. Besides, theoretical analyses and mathematical verifications are presented to validate the efficiency and superiority of the proposed MGRNN model compared with the traditional gradient-based recurrent neural network (GRNN) and ZNN models. Furthermore, simulation experiments are conducted to substantiate the properties of the newly proposed MGRNN model for solving the time-varying Sylvester equation. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						1	11		10.1016/j.neucom.2019.10.080													
J								A renewable fusion fault diagnosis network for the variable speed conditions under unbalanced samples	NEUROCOMPUTING										Deep learning; Domain invariant features; Fault diagnosis; Renewable; Unbalanced samples; Variable speed	DEEP NEURAL-NETWORK; EXTRACTION	Deep learning technology has been gradually applied to solve a variety of fault diagnosis problems because of its outstanding feature learning and nonlinear classification abilities. However, few deep learning network models can be applied to both variable speed conditions and unbalanced samples scenarios in fault diagnosis, especially in extreme cases where fault samples are missing. And most of the fault diagnosis models do not have the ability to update automatically as the collected fault data increases. To deal with the above problems, a deep learning model named renewable fusion fault diagnosis network (RFFDN) is proposed. The network has three main parts: improved feature classification network; second order statistics fusion network and unbalanced feature comparison network. Moreover, these three networks are simultaneously organized on a two-branch convolutional neural network (CNN) architecture with fused data input, so as to facilitate the network to learn the depth nonlinear domain invariant features. Finally, the RFFDN model and other mainstream fault diagnosis models are tested on two different datasets. The results show that the RFFDN model not simply achieves high diagnostic accuracy in diagnosis results, but also extracts the domain invariant features at variable speed conditions under unbalanced samples, and accurately classifies the new faults. These prove that the model can not only be applied to a variety of operating modes, but can be updated as more data are collected as well, which is of great significance to the field of fault diagnosis. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						12	29		10.1016/j.neucom.2019.08.099													
J								Introspection unit in memory network: Learning to generalize inference in OOV scenarios	NEUROCOMPUTING										Memory network; Out of vocabulary; Introspection unit; Language inference		Inference in natural language processing (NLP) is a tough task. Although plenty of models have been proposed in recent years, they are usually restricted to infer within a limited vocabulary or handcrafted training templates. In this paper, we propose the introspection unit (IU), a new neural module which can be incorporated with memory networks to deal with inference tasks in out of vocabulary (OOV) and rare named entities (RNEs) scenarios. Specifically, when encountering a new word, IU compares its part-of-speech context with the training dataset to extract a similar sample, and then embeds the new word into a target position to construct a simulated sample. The target position is located by the result of part-of-speech tagging. Finally, using the simulated sample, IU helps memory networks to learn the context and characteristic of the new word. In experiments, we evaluate the effectiveness of IU with the memory network on four inference datasets: a name OOV dataset, a place OOV dataset, a more challenging synthetical mixture OOV dataset and a realistic dialogue dataset. The experimental results demonstrate that IU effectively generalizes the inference ability of memory networks to OOV scenarios and improves the inference accuracies significantly. Furthermore, we visualize both the introspection process and the effect of IU in word embeddings and memories. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						30	40		10.1016/j.neucom.2019.07.111													
J								Neural network based integral sliding mode optimal flight control of near space hypersonic vehicle	NEUROCOMPUTING										Near space hypersonic vehicle; Integral sliding mode control; Optimal tracking control; Adaptive dynamic programming; Auxiliary system method; Neural network	UNCERTAIN NONLINEAR-SYSTEMS; CONTINUOUS-TIME SYSTEMS; TRACKING CONTROL; ADAPTIVE-CONTROL; ATTITUDE-CONTROL; DESIGN	In this paper, based on the integral sliding mode method and adaptive dynamic programming (ADP) algorithm, a robust optimal tracking control scheme is presented for near space hypersonic vehicle (NSHV) system in the presence of unknown modeling error, external disturbance, and input saturation. Firstly, combining neural network, auxiliary system and integral sliding mode methods, an adaptive integral sliding mode control (AISMC) law is designed to guarantee system trajectories tend to a defined integral sliding surface and the effects of modeling uncertainty, external disturbance, and control input saturation are eliminated. Then, the robust optimal tracking control problem of original system is converted into the optimal control problem of a nominal system, and an ADP method with single critic network is utilized to acquire the corresponding optimal controller. Furthermore, Lyapunov analysis method shows that the overall control input which contains AISMC law and optimal controller can ensure all the signals in closed-loop system are stable in the sense of uniform ultimate boundedness (UUB). Finally, simulation results about attitude flight control of NSHV are given to verify the effectiveness of the proposed control scheme. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 28	2020	379						41	52		10.1016/j.neucom.2019.10.038													
J								SARPNET: Shape attention regional proposal network for liDAR-based 3D object detection	NEUROCOMPUTING										Shape attention; 3D shape priors; Feature encoder; 3D object detection; LiDAR point cloud		Real-time 3D object detection is a fundamental technique in numerous applications, such as autonomous driving, unmanned aerial vehicles (UAV) and robot vision. However, current LiDAR-based 3D object detection algorithms allocate inadequate attention to the inhomogeneity of LiDAR point clouds and the shape encoding capability of regional proposal schemes. This paper introduces a novel 3D object detection network called the Shape Attention Regional Proposal Network (SARPNET), which deploys a new low-level feature encoder to remedy the sparsity and inhomogeneity of LiDAR point clouds with an even sample method, and embodies a shape attention mechanism that learns the statistic 3D shape priors of objects and uses them to spatially enhance semantic embeddings. Experimental results show that the proposed one-stage method outperforms state-of-the-art one-stage and even two-stage methods on the KITTI 3D object detection benchmark. It achieved a BEV AP of (87.26%, 62.80%), 3D AP of (75.64%, 60.43%), and orientation AP of (88.86%, 71.01%) for the detection of cars and cyclists, respectively. Besides, the method is the third winner in the nuScenes 3D Detection challenge of CVPR2019 Workshop on Autonomous Driving (WAD). (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						53	63		10.1016/j.neucom.2019.09.086													
J								Bayesian capsule networks for 3D human pose estimation from single 2D images	NEUROCOMPUTING										Deep bayesian networks; Capsule networks; 3D Human pose estimation	FRAMEWORK	Deep Bayesian Networks are a hot topic in Deep Learning because this approach makes it possible to minimize both the epistemic and the homoscedastic uncertainty at the same time self balancing multiple and complementary losses for a given task, simply by employing standard operations such as dropout, mean squared error or cross-entropy. On the other hand, Capsule networks are a novel DNN architecture that offer a richer representation because each concept is represented by a number of different vectors. The bayesian formulation of the Capsule networks is still an open problem that we address in this paper. We present a bayesian formulation of Capsule networks and compare its performance against the state-of-the-art for the ill-posed regression problem of estimating the 3D human pose from a single 2D image. The results show that our network is very competitive with a much more straightforward solution. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						64	73		10.1016/j.neucom.2019.09.101													
J								Automatic processing of Z-transform artificial neural networks using parallel programming	NEUROCOMPUTING										Parallel programming; Z-Transform; Z-Transform artificial neural network; ZTANN		The article describes the process of computing the Z-transform neural network on the basis of input and output signals of analyzed object. Parallel algorithms for performing these calculations are presented and different parallel architectures with different number of processors showing their advantages and limitations are analyzed. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						74	88		10.1016/j.neucom.2019.10.078													
J								Feature concatenation multi-view subspace clustering	NEUROCOMPUTING										Multi-view clustering; Subspace clustering; Low-rank representation; Feature concatenation	MAXIMUM-ENTROPY DISCRIMINATION; ALGORITHM; REPRESENTATION; FACTORIZATION; SCALE	Multi-view clustering is a learning paradigm based on multi-view data. Since statistic properties of different views are diverse, even incompatible, few approaches implement multi-view clustering based on the concatenated features straightforward. However, feature concatenation is a natural way to combine multiview data. To this end, this paper proposes a novel multi-view subspace clustering approach dubbed Feature Concatenation Multi-view Subspace Clustering (FCMSC), which boosts the clustering performance by exploring the consensus information of multi-view data. Specifically, multi-view data are concatenated into a joint representation firstly, then, l(2,1)-norm is integrated into the objective function to deal with the sample-specific and cluster-specific corruptions of multiple views. Moreover, a graph regularized FCMSC is also proposed in this paper to explore both the consensus information and complementary information of multi-view data for clustering. It is noteworthy that the obtained coefficient matrix is not derived by simply applying the Low-Rank Representation (LRR) to concatenated features directly. Finally, an effective algorithm based on the Augmented Lagrangian Multiplier (ALM) is designed to optimize the objective functions. Comprehensive experiments on six real-world datasets illustrate the superiority of the proposed methods over several state-of-the-art approaches for multi-view clustering. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						89	102		10.1016/j.neucom.2019.10.074													
J								Bootstrap dual complementary hashing with semi-supervised re-ranking for image retrieval	NEUROCOMPUTING										Multi-hashing; dual complementary hashing; image retrieval; semi-supervised	APPROXIMATE NEAREST-NEIGHBOR; ALGORITHMS; SPARSE	With the rapid growth of multimedia data on the Internet, content-based image retrieval becomes a key technique for the Internet development. Hashing methods are efficient and effective for image retrieval. Dual Complementary Hashing (DCH) is one such method, which uses multiple hash tables and has good performance. However, DCH utilizes wrongly hashed image pairs to train the following hash table and discards correctly hashed image pairs. Therefore, the number of image pairs utilized for training the following hash tables will decrease rapidly. Moreover, each hash function in a hash table of DCH is trained by correcting the errors caused by its preceding one instead of holistically considering errors made by all previous hash functions. These restrictions significantly reduce the training efficiency and the overall performance of DCH. In this paper, we propose a new hashing method for image retrieval, Bootstrap Dual Complementary Hashing with semi-supervised Re-ranking (BDCHR). It is a semi-supervised multi-hashing method consisting of two parts: bootstrap DCH and semi-supervised re-ranking. The first part relieves the restrictions of DCH while the second part further enhances the image retrieval performance. Experimental results show that BDCHR yields better performance than other state-of-the-art multi-hashing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						103	116		10.1016/j.neucom.2019.10.073													
J								Recurrent convolutional neural network: A new framework for remaining useful life prediction of machinery	NEUROCOMPUTING										Deep learning; Convolutional neural network; Recurrent connection; Remaining useful life prediction; Uncertainty quantification		Deep learning is becoming more appealing in remaining useful life (RUL) prediction of machines, because it is able to automatically build the mapping relationship between the raw data and the corresponding RUL by representation learning. Among deep learning models, convolutional neural networks (CNNs) are gaining special attention because of its powerful ability in dealing with time-series signals, and have achieved promising results in current studies. These studies, however, suffer from the two limitations: (1) The temporal dependencies of different degradation states are not considered during network construction; and (2) The uncertainty of RUL prediction results cannot be obtained. To overcome the above-mentioned limitations, a new framework named recurrent convolutional neural network (RCNN) is proposed in this paper for RUL prediction of machinery. In RCNN, recurrent convolutional layers are first constructed to model the temporal dependencies of different degradation states. Then, variational inference is used to quantify the uncertainty of RCNN in RUL prediction. The proposed RCNN is evaluated using vibration data from accelerated degradation tests of rolling element bearings and sensor data from life testing of milling cutters, and compared with some state-of-the-art prognostics approaches. Experimental results demonstrate the effectiveness and superiority of RCNN in improving the accuracy and convergence of RUL prediction. More importantly, RCNN is able to provide a probabilistic RUL prediction result, which breaks the inherent limitation of CNNs and facilitates maintenance decision making. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						117	129		10.1016/j.neucom.2019.10.064													
J								Data-driven simulation of pedestrian collision avoidance with a nonparametric neural network	NEUROCOMPUTING										Pedestrian dynamics; Data-driven simulation; Navigation; Steering; Generalized regression neural network; Artificial intelligence	RULES; MODEL	Data-driven simulation of pedestrian dynamics is an incipient and promising approach for building reliable microscopic pedestrian models. We propose a methodology based on generalized regression neural networks, which does not have to deal with a huge number of free parameters as in the case of multi-layer neural networks. Although the method is general, we focus on the one pedestrian - one obstacle problem. Experimental data were collected in a motion capture laboratory providing high-precision trajectories. The proposed model allows us to simulate the trajectory of a pedestrian avoiding an obstacle from any direction. Together with the methodology specifications, we provide the data set needed for performing the simulations of this kind of pedestrian dynamic system. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						130	140		10.1016/j.neucom.2019.10.062													
J								Adaptive neural dynamic surface control of mechanical systems using integral terminal sliding mode	NEUROCOMPUTING										Dynamic surface control; Mechanical systems; Neural networks; Raised-cosine radial basis functions; Terminal sliding mode	NONLINEAR-SYSTEMS; TRACKING	This paper studies the robust tracking control problem of fully-actuated mechanical systems using a novel integral dynamics surface control (DSC) method. We replace the conventional DSC error surfaces with new nonlinear integral surfaces to generate a quasi-terminal sliding mode (TSM) in the tracking error trajectories. Then, we follow the recursive, Lyapunov-based design procedure of the DSC to obtain the control law. The resultant quasi-TSM adjusts the error convergence rate according to the distance from the origin. To achieve robustness against structural variations of the mechanical system as well as external disturbances, we use nonlinear damping combined with a radial basis function neural network (RBFNN) approximator. The RBFNN adaptively identifies the upper-bound of the uncertainty/disturbances to prevent conservative, high-gain control inputs. Moreover, we use raised-cosine basis functions, which have compact supports, to improve the computational efficiency of the RBFNN. Through Lyapunov-based stability analysis, we show the boundedness and ultimate boundedness of the closed-loop system as well as the TSM-induced convergence of the tracking errors. Detailed numerical simulations support the efficacy of the proposed control method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						141	151		10.1016/j.neucom.2019.10.046													
J								Autonomous deep learning: A genetic DCNN designer for image classification	NEUROCOMPUTING										Deep convolutional neural networks (DCNNs); Neural architecture search; Genetic algorithm (GA); Image classification		Recent years have witnessed the breakthrough success of deep convolutional neural networks (DCNNs) in image classification and other vision applications. DCNNs have distinct advantages over traditional solutions in providing a uniform feature extraction-classification framework to free users from troublesome handcrafted feature extraction. However, DCNNs are far from autonomous, since their performance relies heavily on the handcrafted architectures, which also requires a lot expertise and experience to design, and cannot be continuously improved once the tuning of hyper-parameters converges. In this paper, we propose an autonomous and continuous learning (ACL) algorithm to generate automatically a DCNN architecture for each given vision task. We first partition a DCNN into multiple stacked meta convolutional blocks and fully connected blocks, each of which may contain the operations of convolution, pooling, fully connection, batch normalization, activation and drop out, and thus convert the architecture into an integer code. Then, we use genetic evolutionary operations, including selection, mutation and crossover to evolve a population of DCNN architectures. We have evaluated this algorithm on six image classification tasks, i.e., MNIST, Fashion-MNIST, EMNIST-Letters, EMNIST-Digits, CIFAR10 and CIFAR100. Our results indicate that the proposed ACL algorithm is able to evolve the DCNN architecture continuously if more time cost is allowed and can find a suboptimal DCNN architecture, whose performance is comparable to the state of the art. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						152	161		10.1016/j.neucom.2019.10.007													
J								Food det: Detecting foods in refrigerator with supervised transformer network	NEUROCOMPUTING										Food detection; Spatial transformer; Object detection	CONTEXT	Most of existing methods mainly focus on the food image recognition which assumes that one food image contains only one food item. However, in this paper, we present a system to detect a diversity of foods in refrigerator where multiple food items may exist. In view of the refrigerator environment, we propose a food detection framework based on the supervised transformer network. More specifically, the supervised transformer network, dotted as RectNet, is first proposed to automatically select the irregular food regions and transform them to the frontal views. Then, based on the rectified food images, we further propose an end-to-end detection network that predicts the categories and locations of food items. The proposed detection network, called Lite Fully Convolutional Network (LiteFCN), is evolved from the advanced object detection algorithm Faster R-CNN while several significant improvements are tailored to achieve a higher accuracy and keep inference time efficiency. To validate the effectiveness of each component of our method, we build a real-world refrigerator dataset with 80 classes. Extensive experiments demonstrate that our methods achieve the state-of-the-art results, which improves the baseline by a large margin, e.g., 3-5% in terms of F-measure. We also show that the proposed detection network achieve a competitive result on the public PASCAL VOC2007 dataset, which outperforms the Faster R-CNN by 2.3% with a higher speed. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						162	171		10.1016/j.neucom.2019.10.106													
J								Progressive Operational Perceptrons with Memory	NEUROCOMPUTING										Generalized operational perceptron; Progressive learning; Neural architecture learning	NETWORKS	Generalized Operational Perceptron (GOP) was proposed to generalize the linear neuron model used in the traditional Multilayer Perceptron (MLP) by mimicking the synaptic connections of biological neurons showing nonlinear neurochemical behaviours. Previously, Progressive Operational Perceptron (POP) was proposed to train a multilayer network of GOPs which is formed layer-wise in a progressive manner. While achieving superior learning performance over other types of networks, POP has a high computational complexity. In this work, we propose POPfast, an improved variant of POP that signicantly reduces the computational complexity of POP, thus accelerating the training time of GOP networks. In addition, we also propose major architectural modications of POPfast that can augment the progressive learning process of POP by incorporating an information preserving, linear projection path from the input to the output layer at each progressive step. The proposed extensions can be interpreted as a mechanism that provides direct information extracted from the previously learned layers to the network, hence the term "memory". This allows the network to learn deeper architectures and better data representations. An extensive set of experiments in human action, object, facial identity and scene recognition problems demonstrates that the proposed algorithms can train GOP networks much faster than POPs while achieving better performance compared to original POPs and other related algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						172	181		10.1016/j.neucom.2019.10.079													
J								Integrating manifold ranking with boundary expansion and corners clustering for saliency detection of home scene	NEUROCOMPUTING										Home scene; Saliency detection; Manifold ranking; Boundary expansion; Corners clustering; Saliency map	VISUAL-ATTENTION; OBJECT DETECTION; OPTIMIZATION	In this paper, we propose a novel framework for saliency detection of home scene by exploiting manifold ranking, boundary expansion, and corners clustering. Our proposed method firstly combines color cues in RGB and CIELab to select image boundary seeds, and exclude the ones which might be located at salient objects as much as possible. Then, we utilize the boundary seeds on each image boundary as the queries of manifold ranking to compute saliency and integrate them for a background-based saliency map. For the foreground-based saliency detection. Boundary expansion combined with background-based saliency map highlights foreground regions, which are regarded as queries for a foreground-based saliency map. Moreover, we achieve center prior saliency map through multi-scale Harris corner detection and corners clustering to further highlight salient regions and suppress background regions. Finally, we integrate the three saliency maps via the proposed unified framework for a more accurate and smooth saliency map. Both qualitative and quantitative experimental results indicate that our proposed method can deliver better performance than several state-of-the-art saliency detection methods as a whole. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						182	196		10.1016/j.neucom.2019.10.063													
J								A model with length-variable attention for spoken language understanding	NEUROCOMPUTING										Intent detection; Slot filling; Dialogue system; Encoder-decoder; Attention mechanism		Intent detection (ID) and slot filling (SF) are important components in spoken language understanding (SLU) of a dialogue system. The most widely used method is pipeline manner which detects the user's intent at first, then labels the slots. For the purpose of addressing error propagate, some researchers combine these two tasks together by ID and SF joint model. However, the joint models usually perform well only on one of these tasks due to the different values of the trade-off-parameter. We therefore propose an encoder-decoder model with a new tag scheme which unifies these two tasks into one sequence labeling task. In our model, the process of slot filling can receive an intent information and the performance about multiple tags of a word has been improved. Moreover, we show a length-variable attention which can selectively look at a subset of source sentence in the sequence labeling model. Experimental results on two datasets display that the proposed model with length-variable attention outperforms over other joint models. Besides, our method will automatically find the balance between two tasks and achieve better overall performances. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						197	202		10.1016/j.neucom.2019.07.112													
J								Adaptive NN event-triggered control for path following of underactuated vessels with finite-time convergence	NEUROCOMPUTING										Radial basis function neural network; Underactuated marine surface vessel; Line-of-sight; Event-triggered control; Finite-time convergence; Path following	TRACKING CONTROL; CONTAINMENT CONTROL; STABILIZATION; SHIPS	This paper investigates the problem of path following of underactuated marine surface vessels (MSVs) with uncertain nonlinear dynamics. First, the tracking target of the vessel is reduced from tracking the earth-fixed position to tracking the line-of-sight (LOS) angle by LOS method. Then, by employing the radial basis function neural network (RBFNN) to deal with the uncertain nonlinear dynamics, an adaptive NN fast power reaching law is developed for the path following problem based on the backstepping design methodology. Thereafter, the event-triggered technique is incorporated into the control design to synthesize an adaptive NN event-triggered controller with the fast power reaching convergence rate. By combining with the presented event-triggered mechanism, the controller is only updated when the triggering condition is satisfied. Therefore, both the update frequency of the controller and actuator loss are greatly reduced comparing with the traditional time-triggered controller. Theoretical analysis via Lyapunov method indicates that the tracking error can converge to zero within a finite time, meanwhile it also shows that Zeno behavior can be avoided. Simulation results with comparations illustrate the validity and superiority of the proposed controller. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						203	213		10.1016/j.neucom.2019.10.044													
J								Fixed-time event-triggered synchronization of a multilayer Kuramoto-oscillator network	NEUROCOMPUTING										Kuramoto-oscillator; Complex network; Fixed-time control; Event-triggered control; Synchronization control	COMPLEX DYNAMICAL NETWORK; FINITE-TIME; MULTIAGENT SYSTEMS; EXPONENTIAL SYNCHRONIZATION; SECURITY REGIONS; PHASE-LOCKING; CONSENSUS; STABILITY; MODEL	This paper investigates the synchronization problem of the Kuramoto-oscillator network with nonidentical oscillators. The fixed-time event-triggered synchronization control strategies are developed for phase agreement and frequency synchronization under both continuous and intermittent communication. With the developed fixed-time controller, the synchronization can be achieved within a pre-defined time for any initial phase of each oscillator. The event-triggered mechanism avoids continuous controller update and data transmission, which significantly saves the computation and communication resources. Furthermore, theoretical analysis shows that the fixed-time convergence can be guaranteed and the Zeno behavior is avoided by the proposed methods. The numerical simulations of each situation also verify the effectiveness of the proposed synchronization control strategies. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						214	226		10.1016/j.neucom.2019.10.040													
J								Global exponential stability analysis of discrete-time BAM neural networks with delays: A mathematical induction approach	NEUROCOMPUTING										Discrete-time delayed BAM neural network; Global exponential stability; Mathematical induction approach; Spectral abscissa; Spectral radius	LURE SYSTEMS; CRITERIA	The problem of global exponential stability analysis for discrete-time bidirectional associative memory (BAM) neural networks with time-varying delays is investigated. By using the mathematical induction method, a novel exponential stability criterion in the form of linear matrix inequalities is firstly established. Then stability criteria depending upon only the system parameters are derived, which can easily checked by using the standard toolbox software (e.g., MATLAB). The proposed approach is directly based on the definition of global exponential stability, and it does not involve the construct of any Lyapunov-Krasovskii functional or auxiliary function. For a class of special cases, it is theoretical proven that the less conservative stability criteria can be obtained by using the proposed approach than ones in literature. Moreover, several numerical examples are also provided to demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						227	235		10.1016/j.neucom.2019.10.089													
J								Fuzzy granularity neighborhood extreme clustering	NEUROCOMPUTING										Extreme learning machine; Neighborhood rough set; Cluster analysis; Granular computing; Fuzzy set	LEARNING-MACHINE	Clustering is an important method for data analysis. Up to now, how to develop an efficient clustering algorithm is still a critical issue. Unsupervised extreme learning machine is an effective neural network learning method which has a fast training speed. In this paper, a fuzzy granularity neighborhood extreme clustering algorithm which is based on extreme learning machine is proposed. We use fuzzy neighborhood rough set to develop a new feature selection method to eliminate redundant attributes and introduce the adaptive adjustment mechanism to solve the parameters of unsupervised extreme learning machine. Different from the existing clustering algorithms, the proposed algorithm can obtain a clustering result with minimum intra-cluster distance and maximum inter-cluster distance. The proposed algorithm and comparison algorithms are executed on the synthetic data sets and real data sets. The experimental results show that the proposed algorithm outperforms the comparison algorithms on the most data sets and the proposed algorithm is effective for clustering task. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						236	249		10.1016/j.neucom.2019.10.108													
J								Unsupervised depth estimation from monocular videos with hybrid geometric-refined loss and contextual attention	NEUROCOMPUTING										Unsupervised; Monocular video; Attention; Hybrid geometric-refined loss		Most existing methods based on convolutional neural networks (CNNs) are supervised, which require a large amount of ground-truth data for training. Recently, some unsupervised methods utilize stereo image pairs as input by transforming depth estimation into a view synthesis problem, but need stereo camera as an additional equipment for data acquisition. Therefore, we use more available monocular videos captured from monocular camera as our input, and propose an unsupervised learning framework to predict scene depth maps from monocular video frames. First, we design a novel unsupervised hybrid geometricrefined loss, which can explicitly explore more accurate geometric relationship between the input color image and the predicted depth map, and preserve depth boundaries and fine structures in depth maps. Then, we design a contextual attention module to capture nonlocal dependencies along the spatial and channel dimensions in a dual path, which can improve the ability of feature representation and further preserve fine depth details. In addition, we also utilize an adversarial loss to discriminate synthetic or realistic color images by training a discriminator so as to produce realistic results. Experimental results demonstrate that the proposed framework achieves comparable or even better results than those trained with monocular videos or stereo image pairs. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						250	261		10.1016/j.neucom.2019.10.107													
J								Dynamic interaction networks for image-text multimodal learning	NEUROCOMPUTING										Multimodal learning; Dynamic parameters prediction; Deep neural networks		Recently, there is a surge of interest in image-text multimodal representation learning, and many neural network based models have been proposed aiming to capture the interaction between two modalities with different forms of functions. Despite their success, a potential limitation of these methods is insufficient to model all kinds of interactions with a set of static parameters. To alleviate this problem, we present a dynamic interaction network, in which the parameters of the interaction function are dynamically generated by a meta network. Additionally, to provide necessary multimodal features that the meta network needs, we propose a new neural module called Multimodal Transformer. Experimentally, we not only make a comprehensively quantitative evaluation on four image-text tasks, but also show some interpretable analyses of our models, revealing the internal working mechanism of the dynamic parameter learning. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						262	272		10.1016/j.neucom.2019.10.103													
J								Comprehensive design and analysis of time-varying delayed zeroing neural network and its application to matrix inversion	NEUROCOMPUTING										Zeroing neural network (ZNN); Matrix inversion; Time-varying delay; Linear matrix inequality approach; Exponential convergence; Time-varying parameter	SYLVESTER EQUATION; STABILITY; CONVERGENCE	Time delays, which inevitably occur in the circuit implementations of zeroing neural networks (ZNNs), are believed to be one of the most primary causes for their instability and oscillation. To handle this problem, a time-varying delayed zeroing neural network (TVDZNN) is for the first time proposed in this paper to consider the potential influence of time-varying delays for matrix inversion. Moreover, the proposed TVDZNN model is theoretically proved to achieve the global exponential convergence for solving matrix inversion under the Lipschitz condition. Besides, a less conservative sufficient condition is presented to relax the restriction on design parameter.. Moreover, for expediting the convergence speed of the TVDZNN model, two different time-varying parameters have been used to replace its fixed parameter., and the improved version is called the varying-parameter TVDZNN (VP-TVDZNN) model. At last, computer simulations further demonstrate the efficiency of the proposed TVDZNN and VP-TVDZNN models for matrix inversion in a time-varying delayed environment. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						273	283		10.1016/j.neucom.2019.10.101													
J								Event-triggered bipartite consensus for high-order multi-agent systems with input saturation	NEUROCOMPUTING										Multi-agent systems; Bipartite consensus; Input saturation; Event-triggered control; Low-gain feedback; Signed graph	LINEAR-SYSTEMS; NETWORKS	In this paper, the bipartite consensus problem for high-order multi-agent systems (MASs) with input saturation is investigated by distributed event-triggered control and low-gain feedback technique. The underlying topological graph of the system is divided into undirected graph and directed graph. Distributed event-triggered control strategy is proposed to guarantee the semi-global bipartite consensus of MASs. The lower bound of time interval between any two consecutive triggering instants ensures that the Zeno behavior can be excluded for each agent. Finally, two simulation examples are presented to illustrate the effectiveness of the theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						284	295		10.1016/j.neucom.2019.10.095													
J								Memristive autapse involving magnetic coupling and excitatory autapse enhance firing	NEUROCOMPUTING										Memristive system; Excitatory autapse; Magnetic coupling; Firing	NEURON MODEL; TRANSITION; DYNAMICS; NETWORK; SPIKING; SYNCHRONIZATION; INTERNEURONS; CURRENTS; SYNAPSE	Autapse is the synaptic coupling of a neuron's axon to its own dendrite. Considering recent experimental evidence regarding functional excitatory autapses, we explored how the excitatory autapse influences electrical activity in a neuronal model with ion-channel effects. We found that the excitatory autaptic current is able to enhance firing rates. Based on the chemical features of the autapse, we further proposed a memristive autapse involving magnetic coupling, and compared the memristive autapse and the excitatory autapse using the bifurcation analysis and fast/slow decomposition. Our results identified that both of these two types of autapses exhibit similar geometric dynamic properties in terms of burst regulation. Comparing with the excitatory autapse, the memristive autapse involves in a stronger spiking modulation capability, ensuring the neuron to accommodate strong external inputs. Overall, these findings suggested that the memristive system is expected to be usable to mimic biological synapses for advances in neuromorphic computing. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						296	304		10.1016/j.neucom.2019.10.093													
J								DeepANF: A deep attentive neural framework with distributed representation for chromatin accessibility prediction	NEUROCOMPUTING										Chromatin accessibility; Attention mechanism; Convolutional neural networks; Gated recurrent units; Distributed representation	DNA	The identification of chromatin accessibility is a significant part of the genomics and genetics. However, high-throughput experimental techniques are costly and impractical for systematic identification of accessibility. Many computational methods were proposed to predict the functional regions of chromatin purely relying on DNA sequences, but they could not take full advantage of sequence information to capture hidden complex motifs among DNA sequences. Recently, deep learning algorithms have been incorporated into the chromatin accessibility predication and achieved the remarkable results. Nevertheless, there still exists a problem in chromatin accessibility prediction as how to effectively represent the complex features merely from DNA sequences. Thus, developing efficient computational methods is becoming increasingly urgent to identify functional regions of the genome. In this paper, combining convolutional and gated recurrent unit neural networks with attention mechanism, we develop a discriminative computational framework DeepANF to adaptively extract hidden pattern features and identify the chromatin accessibility based on distributed representation of DNA sequences. To verify the efficacy of the DeepANF framework, we conduct extensive experiments on five large scale datasets, and experimental results reveal that our framework not only consistently outperforms these published methods for chromatin accessibility prediction tasks, but also extracts more discriminative features from pure DNA sequences than published methods, especially on MCF-7 dataset. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						305	318		10.1016/j.neucom.2019.10.091													
J								Robust approximations of low-rank minimization for tensor completion	NEUROCOMPUTING										Tensor completion; Block coordinate descent; Nonconvex optimization; Iterative thresholding	MATRIX COMPLETION; NUCLEAR NORM; FACTORIZATION	Motivated by the nuclear norm of tensors and nonconvex approximations of matrix rank, we propose three robust approximations of multi-linear rank for tensor completion. For each method, we develop an efficient algorithm to solve the corresponding optimization problem. Besides, we prove that every cluster point of the sequence, generated by the respective algorithm, is a stationary point. To obtain a more robust reconstruction, we design an updating rule of parameters for each method. Our empirical experiments on real-world data show that the proposed methods deliver state-of-the-art performance in the reconstruction of low-rank tensors. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						319	333		10.1016/j.neucom.2019.10.086													
J								Multi-scale feature fusion residual network for Single Image Super-Resolution	NEUROCOMPUTING										Single Image Super-Resolution; Multi-scale feature fusion; Residual network	INTERPOLATION	We have witnessed great success of Single Image Super-Resolution (SISR) with convolutional neural networks (CNNs) in recent years. However, most existing Super-Resolution (SR) networks fail to utilize the multi-scale features of low-resolution (LR) images to further improve the representation capability for more accurate SR. In addition, most of them do not exploit the hierarchical features across networks for the final reconstruction. In this paper, we propose a novel multi-scale feature fusion residual network (MSFFRN) to fully exploit image features for SISR. Based on the residual learning, we propose a multi-scale feature fusion residual block (MSFFRB) with multiple intertwined paths to adaptively detect and fuse image features at different scales. Furthermore, the outputs of each MSFFRB and the shallow features are used as the hierarchical features for global feature fusion. Finally, we recover the high-resolution image based on the fused global features. Extensive experiments on four standard benchmarks demonstrate that our MSFFRN achieves better accuracy and visually pleasing than the current state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						334	342		10.1016/j.neucom.2019.10.076													
J								Spike train analysis in a digital neuromorphic system of cutaneous mechanoreceptor	NEUROCOMPUTING										Tactile sensing; Spiking model; Neural coding; Hardware implementation	NEURAL-NETWORKS; FPGA; REPRESENTATION; RESPONSES; MODELS	In this research, we develop a neuromorphic system to study neural signaling at the level of first order tactile afferents which are slowly adapting type I (SA1) and rapidly adapting type I (RA1) mechanoreceptors. Considering, the linearized Izhikevich model, two digital circuits are developed for both afferents and are executed on the field programmable gate array (FPGA). After implementation of the digital circuits, we investigate how much information is encoded by this hardware-based neuromorphic system. Indeed, the artificial spiking sequences are evoked by applying different force profiles to the sensor connected to the FPGA. Next, the obtained neural responses are classified based on the two fundamental neural coding for brain information processing: spike timing and rate coding. Considering temporal coding, k-nearest neighbors (kNN), support vector machine (SVM) and Decision Tree algorithms are used for forces recognition using acquired artificial spike patterns. The results of classification show that the digital RA1 is susceptible to signal variations, while the digital SA1, on the other hand, is sensitive to the ramp and hold inputs. Furthermore, these responses are better distinguishable to different stimuli when both artificial SA1 and RA1 afferents are regarded. These results, which are functionally compatible with biological observations, yield the promise for fabrication and development of new tactile sensing modules to be employed in bio-robotic and prosthetic applications. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						343	355		10.1016/j.neucom.2019.09.043													
J								Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint	NEUROCOMPUTING										Person re-identification; Dictionary learning; Label consistency constraint; Stretch regularization	IMAGE; PROJECTION; ALGORITHM	Person re-identification (PRID) is a rather challenging task due to the ambiguity of visual appearance. In this paper, we develop a dictionary-based projection transformation learning approach, where the idea of metric learning and dictionary learning are introduced into a unified framework to make full use of their respective advantages. More specifically, to cope with the challenge caused by dramatic changes in visual appearance, we first project the image features of pedestrian into a discriminative subspace to make the same person from different views with the same coding coefficients. Moreover, we develop a new stretch regularization to make the distance between different pedestrian images larger than that of the same pedestrian images so as to reduce the similarity exhibited by different pedestrian images. Additionally, we develop a label consistency constraint and integrate it into the dictionary learning and then we obtain the ensemble learning model of identity discriminator and dictionary. As a result, the coding coefficient and the corresponding label are bridged and the supervision from the labeled samples is also better exploited. Experimental results on five popular person re-identification benchmarks indicate that the approach developed in this paper has higher identification performance than some state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						356	369		10.1016/j.neucom.2019.11.001													
J								Anatomical context protects deep learning from adversarial perturbations in medical imaging	NEUROCOMPUTING										Medical image processing; Adversarial deep learning		Deep learning has achieved impressive performance across a variety of tasks, including medical image processing. However, recent research has shown that deep neural networks are susceptible to small adversarial perturbations in the image. We study the impact of such adversarial perturbations in medical image processing where the goal is to predict an individual's age based on a 3D MRI brain image. We consider two models: a conventional deep neural network, and a hybrid deep learning model which additionally uses features informed by anatomical context. We find that we can introduce significant errors in predicted age by adding imperceptible noise to an image, can accomplish this even for large batches of images using a single perturbation, and that the hybrid model is much more robust to adversarial perturbations than the conventional deep neural network. Our work highlights limitations of current deep learning techniques in clinical applications, and suggests a path forward. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						370	378		10.1016/j.neucom.2019.10.085													
J								PAC-Bayes and domain adaptation	NEUROCOMPUTING										Domain adaptation; PAC-Bayesian theory	BOUNDS	We provide two main contributions in PAC-Bayesian theory for domain adaptation where the objective is to learn, from a source distribution, a well-performing majority vote on a different, but related, target distribution. Firstly, we propose an improvement of the previous approach we proposed in [1], which relies on a novel distribution pseudodistance based on a disagreement averaging, allowing us to derive a new tighter domain adaptation bound for the target risk. While this bound stands in the spirit of common domain adaptation works, we derive a second bound (introduced in [2]) that brings a new perspective on domain adaptation by deriving an upper bound on the target risk where the distributions' divergence-expressed as a ratio-controls the trade-off between a source error measure and the target voters' disagreement. We discuss and compare both results, from which we obtain PAC-Bayesian generalization bounds. Furthermore, from the PAC-Bayesian specialization to linear classifiers, we infer two learning algorithms, and we evaluate them on real data. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 28	2020	379						379	397		10.1016/j.neucom.2019.10.105													
J								Robust stochastic block model	NEUROCOMPUTING										Stochastic block model; Network analysis; Robust; Variational Bayesian inference; Model selection	COMMUNITY STRUCTURES; BLOCKMODELS; PREDICTION; NETWORKS; NOISE	The family of stochastic block model (SBM) is a mainstay to detect network structures, especially for the exploratory networks analysis without any prior. However, the real-world networks often contain many noisy nodes that have abnormal behaviors or go against the certain patterns. This creates the so-called noise problem, resulting in lower performance of SBMs in real applications. To alleviate this problem, we propose a novel Robust Stochastic Block Model (RSBM). The proposed method can model the noisy nodes in the network and maintain the ability of SBM in structure analysis. RSBM is inferred using variational Bayesian expectation maximization. We evaluate RSBM on both synthetic and real-world networks, and empirical results demonstrate that our RSBM outperforms the state-of-the-art baseline models in the structural partitioning task. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						398	412		10.1016/j.neucom.2019.10.069													
J								A forest of trees with principal direction specified oblique split on random subspace	NEUROCOMPUTING										Ensemble learning; Decision forests; PCA; Random forest	DECISION TREES; ENSEMBLE; CLASSIFICATION; CLASSIFIERS; REGRESSION; RECOGNITION	No matter whether they are univariate or multivariate decision forests, most of previous decision forests determine their partition hyperplanes at split nodes by exhaustive search from candidates or by random generation, which makes some dent in either efficiency or accuracy. In this paper, we propose a new oblique/multivariate decision forest, a forest of trees with principal direction specified oblique split on random subspace (FPDS), where each split of trees is uniquely deterministic once the random feature subspace is determined, the largest principal direction of Principal Component Analysis (PCA) on the sample data at the corresponding split node and the median value of all the current sample points' projections on the largest principal direction directly specified as the normal direction and the cut-point of the partition hyperplane. This method avoids either tediously searching for the optimal split or casually randomly generating the split. The heuristic method to obtain the hyperplanes guarantees accuracy of trees, and the random feature subspace selection adequately ensures the diversity among individual trees in the forest. In addition, each tree of the FPDS uses the whole training set instead of the sampling subset. Therefore, the only randomness factor in the FPDS derives from the random feature subspace selection, which to some extent enhances the robustness. It proves that the proposed forest FPDS is an alternative classifier which can match or even outperform the existing ensemble classifiers or other classifiers. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 28	2020	379						413	425		10.1016/j.neucom.2019.10.045													
J								Online Natural Myocontrol of Combined Hand and Wrist Actions Using Tactile Myography and the Biomechanics of Grasping	FRONTIERS IN NEUROROBOTICS										myocontrol; tactile myography; prosthetics; combined actions; grip strength; high-density force myography (HD-FMG); biomechanics of grasping	PROPORTIONAL MYOELECTRIC CONTROL; PATTERN-RECOGNITION CONTROL; EMG RECENT PROGRESS; PROSTHESIS CONTROL; FOREARM POSITION; FORCE MYOGRAPHY; MOTION; ELECTROMYOGRAPHY; COMBINATIONS; POSTURE	Objective: Despite numerous recent advances in the field of rehabilitation robotics, simultaneous, and proportional control of hand and/or wrist prostheses is still unsolved. In this work we concentrate on myocontrol of combined actions, for instance power grasping while rotating the wrist, by only using training data gathered from single actions. This is highly desirable since gathering data for all possible combined actions would be unfeasibly long and demanding for the amputee. Approach: We first investigated physiologically feasible limits for muscle activation during combined actions. Using these limits we involved 12 intact participants and one amputee in a Target Achievement Control test, showing that tactile myography, i.e., high-density force myography, solves the problem of combined actions to a remarkable extent using simple linear regression. Since real-time usage of many sensors can be computationally demanding, we compare this approach with another one using a reduced feature set. These reduced features are obtained using a fast, spatial first-order approximation of the sensor values. Main results: By using the training data of single actions only, i.e., power grasp or wrist movements, subjects achieved an average success rate of 70.0% in the target achievement test using ridge regression. When combining wrist actions, e.g., pronating and flexing the wrist simultaneously, similar results were obtained with an average of 68.1%. If a power grasp is added to the pool of actions, combined actions are much more difficult to achieve (36.1%).																	1662-5218					FEB 27	2020	14								11	10.3389/fnbot.2020.00011													
J								A neighborhood search based cat swarm optimization algorithm for clustering problems	EVOLUTIONARY INTELLIGENCE										Cat swarm optimization; Clustering; Machine learning; Meta-heuristics	ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; SYSTEM SEARCH; HYBRIDIZATION; EVOLUTIONARY	Clustering is an unsupervised technique that groups the similar data objects into a single subset using a distance function. It is also used to find the optimal set of clusters in a given dataset and each cluster consists of homogenous data objects. In present work, an algorithm based on cat swarm optimization (CSO) is adopted for finding the optimal set of cluster centers for allocating the data objects. Further, some improvements are also incorporated in CSO algorithm for improving clustering performance. These modifications are described as an improved solution search equation to improve convergence rate and an accelerated velocity equation for balancing exploration and exploitation processes of CSO algorithm. Moreover, a neighborhood-based search strategy is introduced to handle local optima problem. The performance of proposed algorithm is tested on eight real-life datasets and compared with well-known clustering algorithms. The simulation results showed that proposed algorithm provides quality results in comparison to existing clustering algorithms.																	1864-5909	1864-5917				DEC	2020	13	4					593	609		10.1007/s12065-020-00373-0		FEB 2020											
J								Discovery of redundant free maximum disjoint Set-k-Covers for WSN life enhancement with evolutionary ensemble architecture	EVOLUTIONARY INTELLIGENCE										Wireless sensor network; Set K-Cover; Life enhancement; Extinction; Genetic algorithm; Differential evolution; Particle Swarm Optimization; Evolutionary ensemble	WIRELESS SENSOR NETWORKS; DIFFERENTIAL EVOLUTION; ENERGY-EFFICIENT; PARTICLE SWARM; OPTIMIZATION; PERFORMANCE; ALGORITHM	In the wireless sensor network, lifetime enhancement of network is a critical design issue for a wide number of applications. Along with ultra-low power technology, the computational approach in the development of several disjoint covers of sensors, such that each cover must provide the coverage of all targets, can be a more effective means for increasing the life span of the network. This enforces to maximize the number of possible disjoint covers, among available sensors in the network. Effectively this problem can be treated as a Set-K-Cover problem, which has been proven to be NP-complete. To make the solution more power-efficient, in this paper, the complexity of the problem has increased at a further level by expecting not only the upper bound of covers but also making the covers redundant free and formed with the minimal number of sensors. This problem can be considered as a search of multiple components where each component itself carried a multi-dimensional characteristic. In a natural system where evolution is the fundamental principle in the development of any entity, such kind of multi-component complex problem doesn't handle at one stage. First, an integral approximated solution evolves and later each component evolves separately. This is the reason that for the Set-K-Cover problem, single stages of the various successful evolutionary algorithms have shown their limitation in achieving the upper bound of covers and in delivering redundant free solutions. In the past, some kind of sequential local scanning process has been integrated with evolutionary computation in the iteration for each cover to improve the performance. But discovered covers fail to meet objectives of upper bound and/or redundant free covers with the minimal number of sensors. Hence this research has explored the possibility of finding the solution through more closer to natural way i.e. only through evolution only (without local scanning process). To meet the desired objectives, in this paper an ensemble evolutionary concept has proposed which has delivered redundant free, the upper bound of covers with the minimum number of sensors. Evolutionary Ensemble architecture works at a different level in a cascaded manner to maximize the total possible number of disjoint covers and their refinement along with that there is a feedback mechanism to explore the new covers further if there is any. Based on the natural extinction process, an extinct operator has also introduced in the Genetic algorithm to increase the convergence rate and better exploration. Instead of fitness-oriented, equal opportunity for every parent in offspring creation was introduced to increase the diversity level. The performance results on different simulated networks confirm that without fail the proposed solution has achieved all objectives whereas various variants of Differential evolution and Genetic algorithms and Particle Swarm Optimization fail to deliver desired performances.																	1864-5909	1864-5917				DEC	2020	13	4					611	630		10.1007/s12065-020-00374-z		FEB 2020											
J								An improved long short-term memory networks with Takagi-Sugeno fuzzy for traffic speed prediction considering abnormal traffic situation	COMPUTATIONAL INTELLIGENCE										agglomerative hierarchical K-means clustering; fuzzy optimized long short-term memory; Gaussian bell-shaped membership function; traffic speed prediction; whale optimization algorithm	NEURAL-NETWORK; WHALE OPTIMIZATION; MODEL; VOLUME; TIME	Traffic speed prediction is an emerging paradigm for achieving a better transportation system in smart cities and improving the heavy traffic management in the intelligent transportation system (ITS). The accurate traffic speed prediction is affected by many contextual factors such as abnormal traffic conditions, traffic incidents, lane closures due to construction or events, and traffic congestion. To overcome these problems, we propose a new method named fuzzy optimized long short-term memory (FOLSTM) neural network for long-term traffic speed prediction. FOLSTM technique is a hybrid method composed of computational intelligence (CI), machine learning (ML), and metaheuristic techniques, capable of predicting the speed for macroscopic traffic key parameters. First, the proposed hybrid unsupervised learning method, agglomerated hierarchical K-means (AHK) clustering, divides the input samples into a group of clusters. Second, based on parameters the Gaussian bell-shaped fuzzy membership function calculates the degree of membership (high, low, and medium) for each cluster using Takagi-Sugeno fuzzy rules. Finally, the whale optimization algorithm (WOA) is used in LSTM to optimize the parameters obtained by fuzzy rules and calculate the optimal weight value. FOLSTM evaluates the accurate traffic speed from the abnormal traffic data to overcome the nonlinear characteristics. Experimental results demonstrated that our proposed method outperforms the state-of-the-art approaches in terms of metrics such as mean square error (MSE), root mean square error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).																	0824-7935	1467-8640				AUG	2020	36	3					964	993		10.1111/coin.12291		FEB 2020											
J								Deep hashing for multi-label image retrieval: a survey	ARTIFICIAL INTELLIGENCE REVIEW										Content-based image retrieval; Fast similarity search; Hashing; Multi-label learning; Deep learning; Deep hash	LEARNING BINARY-CODES	Content-based image retrieval (CBIR) aims to display, as a result of a search, images with the same visual contents as a query. This problem has attracted increasing attention in the area of computer vision. Learning-based hashing techniques are amongst the most studied search approaches for approximate nearest neighbors in large-scale image retrieval. With the advance of deep neural networks in image representation, hashing methods for CBIR have started using deep learning to build binary codes. Such strategies are generally known as deep hashing techniques. In this paper, we present a comprehensive deep hashing survey for the task of image retrieval with multiple labels, categorizing the methods according to how the input images are treated: pointwise, pairwise, tripletwise and listwise, as well as their relationships. In addition, we present discussions regarding the cost of space, efficiency and search quality of the described models, as well as open issues and future work opportunities.																	0269-2821	1573-7462				OCT	2020	53	7					5261	5307		10.1007/s10462-020-09820-x		FEB 2020											
J								An overview of distance and similarity functions for structured data	ARTIFICIAL INTELLIGENCE REVIEW										Distance; Similarity; Structured data; Relational learning	MAXIMUM COMMON SUBGRAPH; GRAPH EDIT DISTANCE; DESCRIPTION LOGICS; EMPIRICAL-EVALUATION; SEMANTIC SIMILARITY; KERNELS; SYSTEM; PREDICTION; RETRIEVAL; FRAMEWORK	The notions of distance and similarity play a key role in many machine learning approaches, and artificial intelligence in general, since they can serve as an organizing principle by which individuals classify objects, form concepts and make generalizations. While distance functions for propositional representations have been thoroughly studied, work on distance functions for structured representations, such as graphs, frames or logical clauses, has been carried out in different communities and is much less understood. Specifically, a significant amount of work that requires the use of a distance or similarity function for structured representations of data usually employs ad-hoc functions for specific applications. Therefore, the goal of this paper is to provide an overview of this work to identify connections between the work carried out in different areas and point out directions for future work.																	0269-2821	1573-7462				OCT	2020	53	7					5309	5351		10.1007/s10462-020-09821-w		FEB 2020											
J								Modified online Newton step based on elementwise multiplication	COMPUTATIONAL INTELLIGENCE										machine learning; online learning; online Newton step; elementwise multiplication; second-order online learning	PERCEPTRON	The second-order method using a Newton step is a suitable technique in online learning to guarantee a regret bound. The large data are a challenge in the Newton method to store second-order matrices such as the hessian. In this article, we have proposed a modified online Newton step that stores first- and second-order matrices of dimension m (classes) by d (features). We have used elementwise arithmetic operations to maintain the size of matrices. The modified second-order matrix size results in faster computations. Also, the mistake rate is on par with respect to popular methods in the literature. The experimental outcome indicates that proposed method could be helpful to handle large multiclass datasets on common desktop machines using second-order method as the Newton step.																	0824-7935	1467-8640				AUG	2020	36	3					1010	1025		10.1111/coin.12298		FEB 2020											
J								Malicious node detection using heterogeneous cluster based secure routing protocol (HCBS) in wireless adhoc sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Malicious nodes; Adhoc network; Security; HCBS; Energy consumption; Sensor nodes	ATTACKS	In wireless, every device can moves anywhere without any infrastructure also the information can be maintained constantly for routing the traffic. The open issues of wireless Adhoc network the attacks which are chosen the forwarding attack that is dropped by malicious node to corrupt the network performance then the information integrity exposure. Aim of the problem that existing methods in Adhoc network for malicious node detection which cannot assure the traceability of the node as well as the fairness of node detection. In this paper, the proposed heterogeneous cluster based secure routing scheme provides trust based secure network for detection of attacks such as wormhole and black hole caused by malicious nodes presence in wireless Adhoc network. The simulation result shows that the proposed model is detect the malicious nodes effectively in wireless Adhoc networks. The malicious node detection efficiency can be achieved 96% also energy consumption also 10% better than existing method.																	1868-5137	1868-5145															10.1007/s12652-020-01797-3		FEB 2020											
J								Value creation system in the connected home ecosystem	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Value creation system; Internet of things ecosystem; Connected home; Value offer; White goods industry	INFORMATION; SMART	The creation of value in the IoT ecosystems of the connected home has for years been a topic of interest for the white goods industry; however, the limitations of information of the new technologies have been a barrier to the adoption of these ecosystems, in this sense, this research aims to model the behavior of the value creation system in the connected home ecosystem. With this in mind, through a prospective study, the system was modeled and the key variables identified, that is, variables on which efforts should be focused in the medium term. Subsequently, as no theoretical method is known to model the problem, it was applied fuzzy logic and dynamic systems to understand the behavior of value creation in the ecosystem. Last a mathematical formula is proposed to estimate value creation. It is discovered in the analysis of results that the correct combination of technologies is a relevant point for the creation of value, as well as the measurement of the effectiveness of the strategy through the concept of digital value offer.																	1868-5137	1868-5145															10.1007/s12652-020-01805-6		FEB 2020											
J								Adaptive image enhancement method using contrast limitation based on multiple layers BOHE	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multiple layers block overlapped histogram equalization; Detail; Image fusion; Contrast limitation	HISTOGRAM EQUALIZATION	Multiple layers block overlapped histogram equalization (MLBOHE) is a classic image enhancement method. However, median filter is used in it to reduce noises which cause the degeneration of the local information. Moreover, the hidden details are not revealed effectively during the image fusion processes. To solve these drawbacks, an adaptive image enhancement method using contrast limitation is proposed in this paper. Based on MLBOHE, the proposed method employs a contrast limited method to suppress noises before BOHE is performed in each layer sub-blocks. Then an improved image fusion mode is applied to adaptively merge the multilayered BOHE images. The way to obtain the fusion weights by this fusion mode is according to the entropy value of each layer sub-blocks. In addition, four Image Quality Measures (IQMs), namely peak signal-to-noise ratio (PSNR), image clarity, contrast measure (EME) and feature similarity index metric (FSIM), are used to analyze the effectiveness of the proposed method. Simulation results show that the proposed method has high performance in suppressing noises and displaying more trustworthy details. Besides, this method outperforms the existing methods in weakening the excessive enhancement for low illumination and foggy images.																	1868-5137	1868-5145															10.1007/s12652-020-01810-9		FEB 2020											
J								Adaptive sliding mode control of robot based on fuzzy neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wheeled mobile robot; Trajectory tracking control; Neural network controller; Adaptive control	TRACKING CONTROL; TRAJECTORY TRACKING; MANIPULATORS; SYSTEMS	The robot is a very complex multi-input multi-output nonlinear system. Due to the inaccuracy of measurement and modeling, coupled with changes in load and the effects of external disturbances, it is virtually impossible to obtain a complete kinetic model. The strong robustness of sliding mode variable structure control makes it particularly suitable for solving the trajectory tracking problem of robots. In this paper, a better real-time adaptive control framework is designed to realize the robust target tracking of mobile robots. Based on the TSK neural network, an efficient control framework combining neural network controller and compensation controller is proposed. Based on the new framework research and improvement of neural network controller, considering the factors of emotional influence decision-making, the existing brain emotional learning neural network is improved and researched, and the brain-emotion learning neural network with radial basis function is proposed. The simulation results show that the trajectory tracking ability, anti-disturbance ability and robustness of the mobile robot are improved to some extent, which verifies the feasibility and efficiency of the proposed control method.																	1868-5137	1868-5145															10.1007/s12652-020-01809-2		FEB 2020											
J								Fast clustering-based weighted twin support vector regression	SOFT COMPUTING										Machine learning; Twin support vector regression; Fast clustering; Prior structural information; Weighted strategy	MACHINE; ROBUST	Construction of an effective model for regression to fit data samples with noise or outlier is a challenging work. In this paper, in order to reduce the influence of noise or outlier on regression and further improve the prediction performance of standard twin support vector regression (TSVR), we proposed a fast clustering-based weighted TSVR, termed as FC-WTSVR. First, we use a fast clustering algorithm to quickly classify samples into different categories based on their similarities. Secondly, to reflect the prior structural information and distinguish contributions of samples located at different positions to regression, we introduce the covariance matrix and weighted diagonal matrix into the primal problems of FC-WTSVR, respectively. Finally, to shorten the training time, we adopt the successive over-relaxation algorithm to solve the quadratic programming problems. The results show that the proposed FC-WTSVR can obtain better prediction performance and anti-interference capability than some state-of-the-art algorithms.																	1432-7643	1433-7479				APR	2020	24	8			SI		6101	6117		10.1007/s00500-020-04746-6		FEB 2020											
J								State-of-the-art fuzzy active contour models for image segmentation	SOFT COMPUTING										Segmentation; Active contour; Fuzzy energy; Blur; Intensity in-homogeneity; Noise and low contrast	LOCAL INFORMATION; FITTING ENERGY; KERNEL; MOTION	Image segmentation is the initial step for every image analysis task. A large variety of segmentation algorithm has been proposed in the literature during several decades with some mixed success. Among them, the fuzzy energy-based active contour models get attention to the researchers during last decade which results in development of various methods. A good segmentation algorithm should perform well in a large number of images containing noise, blur, low contrast, region in-homogeneity, etc. However, the performances of the most of the existing fuzzy energy-based active contour models have been evaluated typically on the limited number of images. In this article, our aim is to review the existing fuzzy active contour models from the theoretical point of view and also evaluate them experimentally on a large set of images under the various conditions. The analysis under a large variety of images provides objective insight into the strengths and weaknesses of various fuzzy active contour models. Finally, we discuss several issues and future research direction on this particular topic.																	1432-7643	1433-7479				OCT	2020	24	19					14411	14427		10.1007/s00500-020-04794-y		FEB 2020											
J								Recommender systems and their ethical challenges	AI & SOCIETY										Algorithms; Artificial intelligence; Digital ethics; Ethical trade-offs; Ethics of recommendation; Machine learning; Recommender systems		This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders-as opposed to just the receivers of a recommendation-in assessing the ethical impacts of a recommender system.																	0951-5666	1435-5655															10.1007/s00146-020-00950-y		FEB 2020											
J								Problem Specific Variable Selection Rules for Constraint Programming: A Type II Mixed Model Assembly Line Balancing Problem Case	APPLIED ARTIFICIAL INTELLIGENCE											GENETIC ALGORITHM; OPERATIONAL OBJECTIVES; OPTIMIZATION; STRATEGIES; SOLVE; FORMULATION; SIMULATION; SEARCH	ABSRACT The main idea of constraint programming (CP) is to determine a solution (or solutions) of a problem assigning values to decision variables satisfying all constraints. Two sub processes, an enumeration strategy and a consistency, run under the constraint programming main algorithm. The enumeration strategy which is managing the order of variables and values to build a search tree and possible solutions is crucial process in CP. In this study problem-based specific variable selection rules are studied on a mixed model assembly line balancing problem. The 18 variable selection rules are generated in three main categories by considering the problem input parameters. These rules are tested with benchmark problems in the literature and experimental results are compared with the results of mathematical model and standard CP algorithm. Also, benchmark problems are run with two CP rules to compare experimental results. In conclusion, experimental results are shown that the outperform rules are listed and also their specifications are defined to guide to researchers who solve optimization problems with CP.																	0883-9514	1087-6545				JUN 6	2020	34	7					564	584		10.1080/08839514.2020.1731782		FEB 2020											
J								Superpixel via coarse-to-fine boundary shift	APPLIED INTELLIGENCE										Superpixel; Slic; K-means	SEGMENTATION	K-means is used by numerous superpixel algorithms, such as SLIC, MSLIC and LSC, because of its simplicity and efficiency. Yet those k-means based algorithm failed to perform well on connectivity and accuracy. In this paper, we propose a coarse-to-fine boundary shift strategy (CFBS) as a replacement of k-means. The CFBS solves the superpixel segmentation problem by shifting boundries rather than clustering pixels. In other words, it can be defined as a special k-means algorithm optimized for superpixel segmentation. By replacing k-means with CFBS, SLIC and LSC are upgraded to NeoSLIC and NeoLSC. Experiments show that NeoSLIC and NeoLSC outperform SLIC and LSC in accuracy and efficiency respectively, and NeoSLIC and NeoLSC alleviate dis-connectivity. In addition, experiments also show that CFBS achieves great improvements on semantic segmentation, class segmentation and segmented flow.																	0924-669X	1573-7497				JUL	2020	50	7					2079	2092		10.1007/s10489-019-01595-1		FEB 2020											
J								A self controlled RDP approach for feature extraction in online handwriting recognition using deep learning	APPLIED INTELLIGENCE										Online handwriting recognition; Feature extraction; Ramer-Douglas-Peucker; Gurmukhi; Unipen; Deep learning	DOMINANT POINT DETECTION; DIGITAL PLANAR CURVES; POLYGONAL-APPROXIMATION; CHINESE CHARACTERS; MODELS	The identification of accurate features is the initial task for benchmarked handwriting recognition. For handwriting recognition, the objective of feature computation is to find those characteristics of a handwritten stroke that depict the class of a stroke and make it separable from the rest of the stroke classes. The present study proposes a feature extraction technique for online handwritten strokes based on a self controlled Ramer-Douglas-Peucker (RDP) algorithm. This novel approach prepares a smaller length feature vector for different shaped online handwritten strokes without preprocessing and without any control parameter to RDP. Thus, it also overcomes the shortcomings of the traditional chain code based feature extraction approach that requires preprocessing of data, and the original RDP algorithm that requires a control parameter as an input to RDP. We further propose a deep learning network of 1-dimensional convolutional neural networks (Conv1Ds) for recognition, which trains in few minutes due to the smaller dimension of the convolution combined with smaller length feature vectors. The proposed approach can be applied to different scripts and different writing styles. The key aim of the present study is to provide a script independent feature extraction technique that is well suited for smaller devices. It improves the recognition over the best reported accuracy in the literature which was achieved using hidden Markov models with directional features, from 87.67% to 95.61% on a Gurmukhi dataset. For Unipen online handwriting datasets the results are at par with the literature.																	0924-669X	1573-7497				JUL	2020	50	7					2093	2104		10.1007/s10489-020-01632-4		FEB 2020											
J								A multicriteria fuzzy pattern recognition approach for assessing the vulnerability to drought: Mediterranean region	EVOLVING SYSTEMS										Vulnerability; Fuzzy pattern recognition; tau OPSIS; Classification to non-ordered categories; Meta-multicriteria methods; Drought	WATER SCARCITY; CLIMATE-CHANGE; CATEGORIZATION; REGRESSION; TOPSIS; MODEL; RISK	The first objective of this paper is to explore a new integrated approach to estimate drought vulnerability taking into account the characteristics of a system that make it prone to be affected by an external hazard. The second objective is to investigate the link between fuzzy pattern recognition and distance based multi-criteria categorization oriented to the assessment of the vulnerability to drought. Firstly, relevant information is grouped into drought sensitivity and adaptive capacity criteria. Instead of the estimation of a unique score for the vulnerability, we propose a classification of the vulnerability to drought into several, in general, non ordered categories. Initially, only the ideal and the anti-ideal points are considered. The link with the multicriteria technique for order preference by similarity to ideal solution (TOPSIS) is investigated. Next, many non-ordered categories are considered which are modulated from all the combinations of the extreme points. Finally, the original fuzzy pattern recognition is considered where the centres are not selected a priori but based on the sample itself. A choice that strengthens the meta-multicriteria character of the proposed approaches is that the categories are not ordered, but they are modulated from all the combinations of the extreme points.																	1868-6478	1868-6486															10.1007/s12530-020-09332-7		FEB 2020											
J								Knowledge granularity based incremental attribute reduction for incomplete decision systems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Incremental attribute reduction; Knowledge granularity; Incomplete decision system; Rough sets	SUPERVISED FEATURE-SELECTION; ROUGH SET-THEORY; UNCERTAINTY; PREDICTION; ALGORITHM	Attribute reduction is an important application of rough set theory. With the dynamic changes of data becoming more and more common, traditional attribute reduction, also called static attribute reduction, is no longer efficient. How to update attribute reducts efficiently gets more and more attention. In the light of the variation about the number of objects, we focus on incremental attribute reduction approaches based on knowledge granularity which can be used to measure the uncertainty in incomplete decision systems. We first introduce incremental mechanisms to calculate knowledge granularity for incomplete decision systems when multiple objects vary dynamically. Then, incremental attribute reduction algorithms for incomplete decision systems when adding multiple objects and when deleting multiple objects are proposed respectively. Finally, comparative experiments on different real-life data sets are conducted to demonstrate the effectiveness and efficiency of the proposed incremental algorithms for updating attribute reducts with the variation of multiple objects in incomplete decision systems.																	1868-8071	1868-808X				MAY	2020	11	5					1141	1157		10.1007/s13042-020-01089-4		FEB 2020											
J								Least squares support vector machines with fast leave-one-out AUC optimization on imbalanced prostate cancer data	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Prostate cancer detection; Imbalanced data; AUC performance index; Least squares support vector machines; Leave-one-out cross validation	NEURAL-NETWORKS; ANTIGEN; SELECTION; MODEL	Quite often, the available pre-biopsy data for early prostate cancer detection are imbalanced. When the least squares support vector machines (LS-SVMs) are applied to such scenarios, it becomes naturally desirable for us to introduce the well-known AUC performance index into the LS-SVMs framework to avoid bias towards majority classes. However, this may result in high computational complexity for the minimal leave-one-out error. In this paper, by introducing the parameter lambda, a generalized Area under the ROC curve (AUC) performance index R-AUCLS is developed to theoretically guarantee that R-AUCLS linearly depends on the classical AUC performance index R-AUC. Based on both R-AUCLS and the classical LS-SVM, a new AUC-based least squares support vector machine called AUC-LS-SVMs is proposed for directly and effectively classifying imbalanced prostate cancer data. The distinctive advantage of the proposed classifier AUC-LS-SVMs exists in that it can achieve the minimal leave-one-out error by quickly optimizing the parameter lambda in R-AUCLS using the proposed fast leave-one-out cross validation (LOOCV) strategy. The proposed classifier is first evaluated using generic public datasets. Further experiments are then conducted on a real-world prostate cancer dataset to demonstrate the efficacy of our proposed classifier for early prostate cancer detection.																	1868-8071	1868-808X				AUG	2020	11	8					1909	1922		10.1007/s13042-020-01081-y		FEB 2020											
J								Performance of single hop and multi hop relaying protocols in cognitive radio networks over Weibull fading channel	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cognitive radio; AF; DAF; CF; Single hop; Multi hop; Weibull fading channels; Cognitive relay networks	COOPERATIVE DIVERSITY; EFFICIENT PROTOCOLS	With the rapid growth of wireless communication, there is an extensive growth in the demand of Radio frequency through electromagnetic spectrum allocation. Cognitive radio is considered to be a potential technology which helps to overcome the demand in radio frequency. Relaying protocol is applied in this paper in order to assist the transmission, improve the efficacy and to minimize the bit error rate. The performance of single hop (Amplify and Forward-AF) and multi-hop (Decode and Forward-DAF and Compress and Forward-CF) relaying protocols over a Weibull fading channels in cognitive radio are analyzed. The numerical outcomes are evidence that the bit error rate and Signal to Noise ratio proves that multi-hop is more promising as compared to single-hop relaying protocol.																	1868-5137	1868-5145															10.1007/s12652-020-01739-z		FEB 2020											
J								Constructing interval-valued generalized partitioned Bonferroni mean operator with several extensions for MAGDM	NEURAL COMPUTING & APPLICATIONS										Group decision making; Partitioned Bonferroni mean; Interval-valued fuzzy sets; Fuzzy numbers; Clustering	GROUP DECISION-MAKING; FUZZY AGGREGATION OPERATORS; SIMILARITY MEASURE; NUMBERS; SETS; PREFERENCE; TOPSIS; ISSUES	In group decision making, each expert's background and the level of knowledge and ability differ, which makes the expert's information inputs to the decision-making process heterogeneous. Such heterogeneity in the information can affect the outcome of the selection of the decision alternatives. This paper therefore attempts to partition the heterogeneous information into homogeneous groups to elicit similar (related) and dissimilar (unrelated) data using a clustering algorithm. We then develop an aggregation approach to gather the collective opinions from the homogeneous clusters to accurately model the decision problem in a group setting. The proposed aggregation approach, labeled as the generalized partitioned Bonferroni mean (GPBM), is studied to investigate the characteristics of the aggregation operator. Further, we extend the GPBM concept to an interval-valued fuzzy set context using the additive generators of the strict t-conorms and we develop two other new aggregation operators: the interval-valued GPBM (IVGPBM) and the weighted IVGPBM (WIVGPBM). We analyze the aggregation of fuzzy numbers by the IVGPBM operator using interval arithmetic involving alpha-cuts and the alpha-cut-based decomposition principle of fuzzy numbers. Two practical examples are presented to illustrate the applicability of these operators, and a comparison is conducted to highlight the effects of the confidence level and the sensitivity of the parameters chosen, analyzing the results with the parameterized strict t-conorm. Finally, we compare the experimental results of the proposed method with existing methods.																	0941-0643	1433-3058				SEP	2020	32	17					13537	13564		10.1007/s00521-020-04765-2		FEB 2020											
J								Development of fuzzy-GMDH model optimized by GSA to predict rock tensile strength based on experimental datasets	NEURAL COMPUTING & APPLICATIONS										Tensile strength; Gravitational search algorithm; Optimization algorithm; Group method of data handling; Fuzzy system	PENETRATION RATE; BRITTLENESS; MACHINE; DESIGN; WATER; ANN; REGRESSION; SELECTION; ACCURACY; NETWORKS	The tensile strength (TS) of the rock is one the most key parameters in designing process of foundations and tunnels structures. However, direct techniques for TS determination (laboratory investigations) are not efficient with respect to cost and time. This investigation attempts to develop an innovative hybrid intelligent model, i.e. fuzzy-group method of data handling (GMDH) optimized by the gravitational search algorithm (GSA), fuzzy-GMDH-GSA, for prediction of the rock TS. To establish a database, the rock samples collected from a tunnel site were evaluated in the laboratory and a database (with the Schmidt hammer test, dry density test, and point load test as inputs and Brazilian tensile strength, BTS, as output) was prepared for modelling. Then, a fuzzy-GMDH-GSA model was developed to predict BTS of the rock considering the most influential of this predictive model. In addition, a fuzzy model as well as a GMDH model were constructed to predict BTS for comparison purposes. The performances of the proposed predictive models were evaluated by comparing the values of several statistical metrics such as correlation coefficient (R). R values of 0.90, 0.86, and 0.86 were obtained for testing datasets of fuzzy-GMDH-GSA, GMDH, and fuzzy models, respectively, which show that the fuzzy-GMDH-GSA predictive model is able to deliver greater prediction performance compared to other constructed models. The results confirmed the effective role of the GSA, as a powerful optimization algorithm in efficiency of hybrid fuzzy-GMDH-GSA model. Moreover, results of sensitivity analysis showed that the point load index is the most effective input on output of this study.																	0941-0643	1433-3058				SEP	2020	32	17					14047	14067		10.1007/s00521-020-04803-z		FEB 2020											
J								Fusing hotel ratings and reviews with hesitant terms and consensus measures	NEURAL COMPUTING & APPLICATIONS										Hesitant fuzzy linguistic term sets; Linguistic decision making; Consensus models; Tourism; Reviews	GROUP DECISION-MAKING; WORD-OF-MOUTH; MODEL; ASSESSMENTS; SEMANTICS; SETS	People have come to refer to reviews for valuable information on products before making a purchase. Digesting relevant opinions regarding a product by reading all the reviews is challenging. An automated methodology which aggregates opinions across all the reviews for a single product to help differentiate any two products having the same overall rating is defined. In order to facilitate this process, rating values, which capture the overall satisfaction, and written reviews, which contain the sentiment of the experience with a product, are fused together. In this manner, each reviewer's opinion is expressed as an interval rating by means of hesitant fuzzy linguistic term sets. These new expressions of opinion are then aggregated and expressed in terms of a central opinion and degree of consensus representing the agreement among the sentiment of the reviewers for an individual product. A real case example based on 2506 TripAdvisor reviews of hotels in Rome during 2017 is provided. The efficiency of the proposed methodology when discriminating between two hotels is compared with the TripAdvisor rating and median of reviews. The proposed methodology obtains significant differentiation between product rankings.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15301	15311		10.1007/s00521-020-04778-x		FEB 2020											
J								Semantically Smooth Bilingual Phrase Embeddings Based on Recursive Autoencoders	NEURAL PROCESSING LETTERS										Bilingual phrase embeddings; Similarity constraints; Machine translation		In this paper, we propose Semantically Smooth Bilingual Recursive Autoencoders to learn bilingual phrase embeddings. The intuition behind our work is to exploit the intrinsic geometric structure of the embedding space and enforce the learned phrase embeddings to be semantically smooth. Specifically, we extend the conventional bilingual recursive autoencoders by preserving the translation and paraphrase probability distributions via regularization terms to simultaneously exploit richer explicit and implicit similarity constraints for bilingual phrase embeddings. To examine the effectiveness of our model, we incorporate two phrase-level similarity features based on the proposed model into a state-of-the-art phrase-based statistical machine translation system. Experiments on NIST Chinese-English test sets show that our model achieves substantial improvements over the baseline.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2497	2512		10.1007/s11063-020-10210-1		FEB 2020											
J								Local binary pattern-based on-road vehicle detection in urban traffic scene	PATTERN ANALYSIS AND APPLICATIONS										Intelligent traffic monitoring; Vehicle detection; Local binary pattern; Saliency map; Clustering forests	DETECTION ALGORITHM; CLASSIFICATION; TRACKING; SURVEILLANCE; RECOGNITION; SYSTEM; FEATURES; SCALE; COLOR; FACE	For intelligent traffic monitoring systems and related applications, detecting vehicles on roads is a vital step. However, robust and efficient vehicles detection is still a challenging problem due to variations in the appearance of the vehicles and complicated background of the roads. In this paper, we propose a simple and effective vehicle detection method based on local vehicle's texture and appearance histograms feed into clustering forests. The interdependency of vehicle's parts locations is incorporating within a clustering forests framework. Local binary pattern-like descriptors are utilized for texture feature extraction. Through utilizing the LBP descriptors, the local structures of vehicles, such as edge, contour and flat region can be effectively depicted. The align set of histograms generated concurrence with LBPs spatial for random sampled local regions are used to measure the dissimilarity between regions of all training images. Evaluating the fit between histograms is built in clustering forests. That is, clustering discriminative codebooks of latent features are used to search between different LBP features of the random regions utilizing the Chi-square dissimilarity measure. Besides, saliency maps built by the learnt latent features are adopted to determine the vehicles locations in test image. Effectiveness of the proposed method is evaluated on different car datasets stressing various imaging conditions and the obtained results show that the method achieves significant improvements compared to published methods.																	1433-7541	1433-755X				NOV	2020	23	4					1505	1521		10.1007/s10044-020-00874-9		FEB 2020											
J								Coaching: accelerating reinforcement learning through human-assisted approach	PROGRESS IN ARTIFICIAL INTELLIGENCE										Reinforcement learning; Human-assisted learning; Semi-supervised learning; Robot coaching	BEHAVIOR; ROBOTS; GAME; GO	The learning process in reinforcement learning is time-consuming because on early episodes agent relies too much on exploration. The proposed "coaching" approach focused on helping to accelerate learning for the system with a sparse environmental reward setting. This approach works well with linear epsilon-greedy Q-learning with eligibility traces. To coach an agent, an intermediate target is given by a human coach as a sub-goal for the agent to pursue. This sub-goal provides an additional clue that guides the agent toward the actual terminal state. In the coaching phase, the agent pursues an intermediate target with an aggressive policy. The aggressive reward from this intermediate target would not be used to update the state-action value directly but the environmental reward is used. After a small number of coaching episodes, the learning would proceed normally with an epsilon-greedy policy. In this way, the agent will end up with an optimal policy which is not under influence or supervision of a human coach. The proposed method has been tested on three experimental tasks: mountain car, ball following, and obstacle avoidance. Even with the human coach of various skill levels, the experimental results show that this method could speed up the learning process of an agent in all tasks.																	2192-6352	2192-6360				JUN	2020	9	2					155	169		10.1007/s13748-020-00204-4		FEB 2020											
J								Soft computing model using cluster-PCA in port model for throughput forecasting	SOFT COMPUTING										Port throughput; Forecasting; Hyperchaotic; Principal component analysis; Soft computing	TRANSPORT; CHOICE; COMPETITION; SAFETY	In the sequence of port throughput analysis, many nonlinear and fluctuation signals are included in order to find the accuracy of port. Besides the socioeconomic factors, the virtually decision making and execution are considered as some kind of forecast. The seasonality and volatility are the critical issues in predicting the efficiency. The forecasting is a useful tool to cross these issues. The forecasting uses many qualitative and casual models and performs time series analysis to find the information about events, pattern changes, relationship between the system elements. It assumes two different kinds of phenomena share the same model of behavior. One is to promote new issues and another is to predict the outcome of the analysis. The judgmental forecasting technique is based on present situation and past situation in order to predict the issues in port. To deal with these issues, this paper addresses a method of hyperchaotic model for optimizing the throughput based on PCA. We review the latest models to provide the theoretical basis and propose novel ideas; the proposed methodology is simulated compared with the other state-of-the-art approaches. The experimental analysis proves the robustness of the model. In the future, more scenarios will be tested.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14167	14177		10.1007/s00500-020-04786-y		FEB 2020											
J								A binary differential evolution algorithm for airline revenue management: a case study	SOFT COMPUTING										Revenue management; Airline industry; Optimization; Binary differential evolution; Booking; Overloading; Cancelation	OPTIMIZATION; FRAMEWORK; MODEL; PRICE; CANCELLATIONS; DECOMPOSITION; ALLOCATION; FLIGHTS	In the current highly competitive airline market, many companies have failed due to their low revenue rates. For this reason, many of them have to develop strategies to increase their revenue. In this study, we develop revenue management (RM) strategy for the Iranian airline industry. More specifically, we present a mathematical model that considers some conditions not studied in previous research in order to provide a more realistic RM modeling of airlines that fits well for the special characteristics of Iranian Airways. A binary differential evolution algorithm is employed to solve the model due to the stochastic nature of data and the NP-hardness of the considered problem. To generate maximum revenue among the six types of airplanes that fly the four capital cities of Iran, the airline under investigation is advised to operate only 21 flights to those cities and cancel the rest of the flights.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14221	14234		10.1007/s00500-020-04790-2		FEB 2020											
J								Entailment for intuitionistic fuzzy sets based on generalized belief structures	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										belief function; Dempster-Shafer evidence theory; entailment; generalized belief structures; intuitionistic fuzzy sets	DECISION-MAKING; FAILURE MODE; SMART CARD; AGGREGATION; METHODOLOGY	Entailment for measure-based belief structures can extend the possible probability value range of variables on a space and obtain more information from variables. However, if the variable space comes from intuitionistic fuzzy sets, the classical entailment for measure-based belief structures will not work in this issue. To deal with this situation, we propose the entailment for intuitionistic fuzzy sets based on generalized belief structures in this paper to apply the entailment for measure based belief structures on space, which is made up of non-membership degree, membership degree and hesitancy degree of a given intuitionistic fuzzy sets. Numerical examples are mentioned to prove the effectively and flexibility of this proposed entailment model. The experimental results indicate that the proposed algorithm can extend the possible probability value range of variables of space efficiently and obtain more information from intuitionistic fuzzy sets.																	0884-8173	1098-111X				JUN	2020	35	6					963	982		10.1002/int.22232		FEB 2020											
J								Small-Sized Reconfigurable Quadruped Robot With Multiple Sensory Feedback for Studying Adaptive and Versatile Behaviors	FRONTIERS IN NEUROROBOTICS										quadruped robot; multiple sensory feedback; self-organized locomotion; vestibular reflexes; compliant control; flexible configuration	NEURAL-CONTROL; LOCOMOTION; DESIGN; EFFICIENT; WALKING	Self-organization of locomotion characterizes the feature of automatically spontaneous gait generation without preprogrammed limb movement coordination. To study this feature in quadruped locomotion, we propose here a new open-source, small-sized reconfigurable quadruped robot, called Lilibot, with multiple sensory feedback and its physical simulation. Lilibot was designed as a friendly quadrupedal platform with unique characteristics, including light weight, easy handling, modular components, and multiple real-time sensory feedback. Its modular components can be flexibly reconfigured to obtain features, such as different leg orientations for testing the effectiveness and generalization of self-organized locomotion control. Its multiple sensory feedback (i.e., joint angles, joint velocities, joint currents, joint voltages, and body inclination) can support vestibular reflexes and compliant control mechanisms for body posture stabilization and compliant behavior, respectively. To evaluate the performance of Lilibot, we implemented our developed adaptive neural controller on it. The experimental results demonstrated that Lilibot can autonomously and rapidly exhibit adaptive and versatile behaviors, including spontaneous self-organized locomotion (i.e., adaptive locomotion) under different leg orientations, body posture stabilization on a tiltable plane, and leg compliance for unexpected external load compensation. To this end, we successfully developed an open-source, friendly, small-sized, and lightweight quadruped robot with reconfigurable legs and multiple sensory feedback that can serve as a generic quadrupedal platform for research and education in the fields of locomotion, vestibular reflex-based, and compliant control.																	1662-5218					FEB 26	2020	14								14	10.3389/fnbot.2020.00014													
J								Season wise bike sharing demand analysis using random forest algorithm	COMPUTATIONAL INTELLIGENCE										bike sharing demand; data analytics; data mining; ensemble model; random forest	CLASSIFICATION; PREDICTION	Rental bike sharing is an urban mobility model that is affordable and ecofriendly. The public bike sharing model is widely used in several cities across the world over the past decade. Because bike use is rising constantly, understanding the system demand in prediction is important to boost the operating system readiness. This article presents a prediction model to meet user demands and efficient operations for rental bikes using Random Forest (RF), which is a homogeneous ensemble method. The approach is carried out in Seoul, South Korea to predict the hourly use of rental bikes. RF is compared with Support Vector Machine with Radial Basis Function Kernel, k-nearest neighbor and Classification and Regression Trees to verify RF supremacy in rental bike demand prediction. Performance Index measures the efficiency of RF compared to the other predictive models. Also, the variable importance analysis is performed to assess the most important characteristics during different seasons by creating a predictive model using RF for each season. The results show that the influence of variables changes depending on the seasons that suggest different operating conditions. RF models trained with yearly and seasonwise models show that bike sharing demand can be further improved by considering seasonal change.																	0824-7935	1467-8640															10.1111/coin.12287		FEB 2020											
J								Artificial intelligence snapchat: Visual conversation agent	APPLIED INTELLIGENCE										AISVCA; Visual conversation; Neural networks; Chat platforms	QUALITY	Visual conversation is a dialog in which parties exchange visual information. The key novelty presented in this paper is an artificial intelligence-driven visual conversation automation method. We will present a state of the art Artificial Intelligence Snapchat Visual Conversation Agent (AISVCA). AISVCA uses our proposed artificial intelligence-driven visual conversation automation method to create received image caption and generate an appropriate reasonable visual response. These functionalities are achieved by using a combination of Convolutional Neural Network (CNN), Long Short-Term Memory Neural Network (LSTM) and, Latent Semantic Indexing method (LSI). CNN and LSTM are used to create image captions and, LSI is used to assess the semantic similarity between captions generated from personalized image dataset, and captions that are extracted from the received image content. We will show that AISVCA, using the proposed method can generate a visual response that is basically indistinguishable from a human visual response. To evaluate the proposed approach, we measured the accuracy of the proposed system and, conducted a user study to test communication quality. In the user study, we analyzed source credibility and interpersonal attraction of the AISVCA. The user study results showed that there are no significant differences in communication quality between a visual conversation with AISVCA and visual conversation with the human agent.																	0924-669X	1573-7497				JUL	2020	50	7					2040	2049		10.1007/s10489-019-01621-2		FEB 2020											
J								Joint discriminative subspace and distribution adaptation for unsupervised domain adaptation	APPLIED INTELLIGENCE										Unsupervised domain adaptation; Discriminative subspace alignment; Classification model; Image classification; Domain shift	REGULARIZATION; FRAMEWORK	In traditional machine learning algorithms, the classification models are learned on the training data (source domain) to reuse for labelling the test data (target domain) where the training and test samples are from the same distributions. However in nowadays applications, the existence of distribution shift across the source and target doamins degrades the model performance, significantly. Domain adaptation methods have been proposed to compensate domain shift problem by aligning the distributions across the source and target domains under various adaptation strategies. This paper addresses the robust image classification problem for unsupervised domain adaptation. Specifically, following three methods are proposed: Discriminative Subspace Learning (DSL), Joint Geometrical and Statistical Distribution Adaptation (GSDA), and Joint Subspace and Distribution Adaptation (DSL-GSDA). DSL is a subspace centric method that aligns the specific and shared features across domains. Indeed, DSL finds two projections to map the source and target data into independent subspaces by aligning the discriminant and global structures of domains. GSDA trends to find an adaptive classifier through statistical and geometrical distribution alignment and minimizes the prediction error. DSL-GSDA, as a combination of DSL and GSDA, consists of two subspace and distribution adaptation levels. DSL-GSDA uses DSL to build two aligned subspaces of source and target domains. The distributions of source and target data in new subspaces is adapted via GSDA. The proposed methods are evaluated on benchmark visual datasets for object, digit and face recongnition tasks. Visual datasets consist of image domains that have been captured under various real-world conditions where the domain shift is unavoidable. The experiment results show that DSL, GSDA and DSL-GSDA outperform other state-of-the-art domain adaptation methods by 6.19%, 1.48% and 1.99% improvement, respectively. Our source code is available at ().																	0924-669X	1573-7497				JUL	2020	50	7					2050	2066		10.1007/s10489-019-01610-5		FEB 2020											
J								Almost periodic solutions of quaternion-valued neutral type high-order Hopfield neural networks with state-dependent delays and leakage delays	APPLIED INTELLIGENCE										Almost periodic solution; Global exponential stability; Quaternion-valued high-order Hopfield neural network; State-dependent delay	TIME-VARYING DELAYS; STABILITY ANALYSIS; SYNCHRONIZATION; STABILIZATION; COEFFICIENTS; DISCRETE; FEEDBACK; SICNNS	In this paper, we consider the existence and global exponential stability of almost periodic solutions for a class of quaternion-valued neutral type neural networks with state-dependent delays and leakage delays by a direct approach. That is, without decomposing the considered quaternion-valued systems into real-valued systems, by using the contraction mapping fixed point theorem, we obtain the existence of almost periodic solutions of the considered networks with state-dependent delays and leakage delays, and by the counter-evidence method, we obtain the global exponential stability of almost periodic solutions of the considered networks with discrete delays and leakage delays. When the time delays in the considered systems are proportional time delays, our existence results are still true. At the same time, when the considered systems degenerate into real-valued or complex-valued systems, our results are still new.																	0924-669X	1573-7497				JUL	2020	50	7					2067	2078		10.1007/s10489-020-01634-2		FEB 2020											
J								Robustness to adversarial examples can be improved with overfitting	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Adversarial examples; Deep learning; Bioinspired learning		Deep learning (henceforth DL) has become most powerful machine learning methodology. Under specific circumstances recognition rates even surpass those obtained by humans. Despite this, several works have shown that deep learning produces outputs that are very far from human responses when confronted with the same task. This the case of the so-called "adversarial examples" (henceforth AE). The fact that such implausible misclassifications exist points to a fundamental difference between machine and human learning. This paper focuses on the possible causes of this intriguing phenomenon. We first argue that the error in adversarial examples is caused by high bias, i.e. by regularization that has local negative effects. This idea is supported by our experiments in which the robustness to adversarial examples is measured with respect to the level of fitting to training samples. Higher fitting was associated to higher robustness to adversarial examples. This ties the phenomenon to the trade-off that exists in machine learning between fitting and generalization.																	1868-8071	1868-808X				APR	2020	11	4					935	944		10.1007/s13042-020-01097-4		FEB 2020											
J								Three-way decisions: beyond rough sets and granular computing	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Three-way decisions; Rough sets; Granular computing; Multi-level; Multi-view	OPTIMAL SCALE SELECTION; ATTRIBUTE REDUCTION; MODEL	With the fast developments of three-way decisions (3WD), this paper systematically summarizes the development track and evolution process of 3WD in recent decades. Firstly, the historical context, internal connections and relations between 3WD and rough sets are carefully investigated. Then, we discuss the methodology of 3WD via granular computing with "multi-level" strategy and "multi-view" strategy. Two novel 3WD generalized models with multilevel structure and multiview structure, as well as an integration framework of multilevel and multiview, are analyzed and discussed detailedly. Finally, this paper presents the research status and future research topics of 3WD.																	1868-8071	1868-808X				MAY	2020	11	5					989	1002		10.1007/s13042-020-01095-6		FEB 2020											
J								A co-training approach for sequential three-way decisions	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Three-way decisions; Cost-sensitive; Boundary region; Co-training; Confidence	SYSTEMS	In recent years, three-way decisions have received much attention in uncertain decision and cost-sensitive learning communities. However, in many real applications, labeled samples are usually far from sufficient. In this case, it is a reasonable choice to defer the decision rather than make an immediate decision without sufficient supported information, thus it constructs a boundary region. In order to label more available samples, a traditional co-training method employs two classifiers on two complementary views to extend the existing training sets. However, the wrong predictions of new labels may lead to a high misclassification cost, especially when few labeled samples are available. To address this problem, a co-training method is incorporated into three-way decisions, which can label new samples with higher confidence. When we obtain sufficient labeled samples, the non-commitment decisions are directly decided to a positive or a negative region, which finally generates a two-way decisions result. Experiments on several face databases are conducted to validate the effectiveness of the proposed approach.																	1868-8071	1868-808X				MAY	2020	11	5					1129	1139		10.1007/s13042-020-01086-7		FEB 2020											
J								An improved artificial bee colony algorithm for balancing local and global search behaviors in continuous optimization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Artificial bee colony; Continuous optimization; Numeric function; Search strategy	PARTICLE SWARM OPTIMIZATION; ABC ALGORITHM; STRATEGY; PERFORMANCE	The artificial bee colony, ABC for short, algorithm is population-based iterative optimization algorithm proposed for solving the optimization problems with continuously-structured solution space. Although ABC has been equipped with powerful global search capability, this capability can cause poor intensification on found solutions and slow convergence problem. The occurrence of these issues is originated from the search equations proposed for employed and onlooker bees, which only updates one decision variable at each trial. In order to address these drawbacks of the basic ABC algorithm, we introduce six search equations for the algorithm and three of them are used by employed bees and the rest of equations are used by onlooker bees. Moreover, each onlooker agent can modify three dimensions or decision variables of a food source at each attempt, which represents a possible solution for the optimization problems. The proposed variant of ABC algorithm is applied to solve basic, CEC2005, CEC2014 and CEC2015 benchmark functions. The obtained results are compared with results of the state-of-art variants of the basic ABC algorithm, artificial algae algorithm, particle swarm optimization algorithm and its variants, gravitation search algorithm and its variants and etc. Comparisons are conducted for measurement of the solution quality, robustness and convergence characteristics of the algorithms. The obtained results and comparisons show the experimentally validation of the proposed ABC variant and success in solving the continuous optimization problems dealt with the study.																	1868-8071	1868-808X				SEP	2020	11	9					2051	2076		10.1007/s13042-020-01094-7		FEB 2020											
J								A conceptual framework for smart device-based notifications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Notifications; Interruptions; Smart devices; Internet of things		The amount of information available for a person at a given time is growing at a disproportioned rate. Moreover, smart devices are becoming more and more popular. To avoid that human's attention becomes a bottleneck and improve the number of notifications delivered to users, we believe that smart devices could serve as notifications mechanisms. This paper summarizes and concludes the efforts of a research project focusing on the use of smart devices as notification mechanisms. The goal of the framework is to assist researchers and developers in the conceptualization of smart device-based notifications. Furthermore, this paper presents the results of three evaluations. First, it describes the results of a survey on the need for novel notification mechanisms and the utility of smart devices as notification delivery mechanisms. Second, the results of a quantitative assessment on the impact of notifications delivered through smart devices compared with notifications delivered through smartphones. Results show that smart devices are useful to deliver valuable information and the quantitative assessment discovered that there is no significant difference between smart devices and smartphones delivering notifications. Finally, the applicability of the framework was evaluated and considered useful by software developers.																	1868-5137	1868-5145															10.1007/s12652-020-01801-w		FEB 2020											
J								Development of a Comfort-Based Motion Guidance System for a Robot Walking Helper	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Comfort-based path planning; Robot walking helper; Motion guidance	WALKER	In research on providing motion assistance for elderly care, the robot walking helper is considered to be able to maintain their vitality. For its practicality, one issue of interest is its feasible path planning for guidance. Inspired by the concept of including human factors for path planning previously proposed, in this paper, we develop such a motion guidance system for the robot walking helper. We first selected the human factors most vital for the elderly and also public via an Internet survey, and then developed a corresponding path planning algorithm and control strategy for its realization. Experiments are conducted to demonstrate the effectiveness of the proposed system. Key contributions of the paper lie on (a) one of the few studies that include human factors into path planning of the robot walking helper and (b) a more thorough consideration on comfort with both physical and psychological factors corresponding to elderly and public preference included.																	0921-0296	1573-0409				NOV	2020	100	2					379	388		10.1007/s10846-020-01168-2		FEB 2020											
J								Reconfigurable Mobile Robot with Adjustable Width and Length: Conceptual Design, Motion Equations and Simulation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Time-variant dimensions of mobile robot; Wheeled robot; Adaptable robot; Obstacle crossing; Omnidirectional wheels	MANIPULATORS; JOINTS	In this article, the dynamic equations of a reconfigurable non-holonomic mobile robot used for space explorations and rescue operations are presented. Dimensions of mobile robots usually remain unchanged while they are programmed to determine a new path to bypass obstacles. The aim of developing this robot is to upgrade the mechanical structure so it can adapt its structure to pass the obstacles without path deviation. Furthermore, by means of mentioned reconfigurations, less motor power is needed, leading to optimized energy consumption. To this end, longitudinal and transverse adjustments are defined for the robot. In view of the motion restrictions existing in conventional wheels, omnidirectional wheels are used in robot structure and their features are considered when deriving motion eqs. Accordingly, the robot is able to move in the direction of wheels' axis. To evaluate the designed mechanism, the system is simulated in ADAMS and the results are compared with the derived motion equations. The findings prove that not only is this system implementable, but the results are also consistent with ADAMS with an acceptable error. Additionally, the calculated energy consumption in a robot that uses transverse adjustment to cross obstacles decreases by 12% compared to a robot that climbs the obstacle and 10% decrease occurs compared to path planning method. As the system dynamics are obtained in a general form, the derived equations can be applied in various applications and different configurations.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		797	814		10.1007/s10846-020-01163-7		FEB 2020											
J								Hydrodynamic Parameter Estimation for an Anguilliform-inspired Robot	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Parameter estimation; Fish-inspired robotics; Underwater robotics; Optimization; Anguilliform-inspired robot	ELONGATED-BODY THEORY; UNDERWATER ROBOTS; IDENTIFICATION; MODEL; LOCOMOTION; DYNAMICS; PREDICTION; PROPULSION; NAVIGATION; VEHICLE	This paper presents an optimization-based approach to estimate the hydrodynamic parameters namely drag and added mass coefficients from free-running experiments conducted on an in-house developed Anguilliform-inspired robot. The objective of the optimization problem is to estimate the hydrodynamic parameters that minimize the differences between the trajectories obtained from the simulations and the physical experiments when operated for identical gait parameters and controller gains for both the straight and the turning motions. The hydrodynamic parameters obtained from the developed approach leads to a maximum root-mean-square (RMS) position error of 0.183 BL and a maximum RMS velocity error of 0.03 BL/s between the trajectories obtained from simulations and experiments. Experimental results suggest that the parameters estimated using the developed approach can be useful in predicting the robot's motion accurately. Accurate robot motion prediction is the fundamental requirement for localization, collision prediction, and motion planning algorithms which in turn are required for automated inspection, maintenance, and repair of sub-sea structures using Anguilliform-inspired robots.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		837	857		10.1007/s10846-020-01154-8		FEB 2020											
J								An improved brainstorm optimization using chaotic opposite-based learning with disruption operator for global optimization and feature selection	SOFT COMPUTING										Brainstorm optimization (BSO); Opposition-based learning (OBL); Swarm algorithms (SA); Evolutionary algorithms (EA); Feature selection	PARTICLE SWARM OPTIMIZATION; ALGORITHM	Optimization has increased its use in different domains for accurately solving challenging problems. Complex optimization problems require the use of methods that possess the capabilities to properly explore the search spaces. The traditional algorithms commonly tend to fail in suboptimal values during the optimization process; this fact affects the quality of the solutions. This situation occurs for different reasons, but the lack of diversity due to the use of exploitation operators is the most common. Brainstorm optimization is an alternative method based on the social strategy to generate new innovative ideas in work groups. In brainstorm optimization, each solution representing an idea and brainstorm process is performed using clustering algorithms. However, brainstorm optimization is not able to thoroughly explore the search space, and its diversity is reduced. It does not possess any mechanism to escape from suboptimal solutions. Besides, the computational effort is also increased in the iterative process. This paper presents a modified version of brainstorm optimization that improves its performance. In the proposed algorithm, chaotic maps and opposition-based learning are applied to initialize the solutions for a given problem. Moreover, in the optimization process, the positions of the initial population are updated using the disruptor operator. After updating the population, opposition-based learning is used again to analyze the opposite solutions. The combination of chaotic maps, opposition-based learning and disruption operator improve the exploration ability of brainstorm optimization by increasing the diversity of the population. The proposed method has been evaluated using a set of benchmark functions, and it has been also used for feature selection in data mining. The results show the high efficacy of the proposed method to determine the optimal solutions of the tested functions.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14051	14072		10.1007/s00500-020-04781-3		FEB 2020											
J								Hybrid Multi-Evolutionary Algorithm to Solve Optimization Problems	APPLIED ARTIFICIAL INTELLIGENCE											NUMERICAL FUNCTION OPTIMIZATION	The article presents a Hybrid Multi-Evolutionary Algorithm designed to solve optimization problems. The Genetic Algorithm and Evolutionary Strategy work together to improve the efficiency of optimization and increase resistance to getting stuck to sub-optimal solutions. Genetic Algorithm and Evolutionary Strategy can periodically exchange the best individuals from each other. The algorithm combines the ability of the Genetic Algorithm to explore the search space and the ability of the Evolutionary Strategy to exploit the search space. It maintains the right balance between the exploration and exploitation of the search space. The results of the experiments suggest that the proposed algorithm is more effective than the Genetic Algorithms and Evolutionary Strategy used separately, and can be an effective tool in solving complex optimization problems.																	0883-9514	1087-6545				JUN 6	2020	34	7					550	563		10.1080/08839514.2020.1730631		FEB 2020											
J								Pythagorean linguistic preference relations and their applications to group decision making using group recommendations based on consistency matrices and feedback mechanism	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										group decision making; linguistic sets; Pythagorean fuzzy sets; Pythagorean linguistic preference relations; Pythagorean linguistic sets	AGGREGATION OPERATORS; CONSENSUS	In this paper, we introduce a new type of fuzzy set, called Pythagorean linguistic sets (PLSs), to address the preferred and nonpreferred degrees of linguistic variables. Moreover, it allows decision makers to offer effectively handle uncertain information more flexible than intuitionistic linguistic sets (ILSs) when one compares two alternatives in the process of decision making. Some of the fundamental operational laws, score, accuracy, and aggregation operators are defined, and their properties are investigated. Preference relation (PR) is a useful and efficient tool for decision making that only requires the decision makers to compare two alternatives at one time. Taking the advantages of PLSs and PRs, this paper also introduces Pythagorean linguistic preference relations (PLPRs) and studies their application. We propose an approach for group decision making using group recommendations based on consistency matrices and feedback mechanism. First, the proposed method constructs the collective consistency matrix, the weight collective PRs, and the group collective PRs. Then, it constructs a consensus relation for each expert and determines the group consensus degree (GCD) for all experts. If the GCD is smaller than a predefined threshold value, then a feedback mechanism is activated to update the PLPRs. Finally, after the GCD is greater than or equal to the predefined threshold value, we calculate the arithmetic mathematical average values of the updated group collective PR to select the most appropriate alternative.																	0884-8173	1098-111X				MAY	2020	35	5					826	849		10.1002/int.22226		FEB 2020											
J								A vector and geometry interpretation of basic probability assignment in Dempster-Shafer theory	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										basic probability assignment; data fusion; dempster-shafer theory; distance; entropy	USER AUTHENTICATION SCHEME; DECISION-MAKING; D NUMBERS; UNCERTAINTY MEASURE; BELIEF FUNCTIONS; SMART CARD; ENTROPY; RULE; APPROXIMATION; COMBINATION	Because of the superiority in dealing with uncertainty expression, Dempster-Shafer theory (D-S theory) is widely used in decision theory. In D-S theory, the basic probability assignment (BPA) is the basis and core. Recently, some researchers represent BPA on a N-dimension frame of discernment (FOD) as 2 N-dimension vector in Descartes coordinate system. This representation treats a BPA as a point in the 2 N-dimensional space. A new vector and geometry interpretation of BPA is proposed in this paper. The BPA on a N-dimension FOD is represented as N-dimension vector with parameters in this method. Then BPA is expressed as subset of N-dimension Cartesian space rather than a point. The proposed method is a new way to represent BPA with vector and geometry. The essence of this method is to convert BPA to probability distribution with parameters. The applications of this representation method in D-S theory have been studied. Based on this method, problems in D-S theory can be solved, which include the fusion of BPAs, the distance between BPAs, the correspondence between BPA and probability, and the entropy of BPAs.																	0884-8173	1098-111X				JUN	2020	35	6					944	962		10.1002/int.22231		FEB 2020											
J								Improved formulation of the latent variable model inversion-based optimization problem for quality by design applications	JOURNAL OF CHEMOMETRICS										latent variable modelling; latent variable model inversion; optimization in the latent space; partial least-squares (PLS); quality by design (QbD)	BATCH OPERATING POLICIES; PRODUCT DESIGN; SPACE	Latent variable regression model (LVRM) inversion is a relevant tool for finding, if they exist, different combinations of manufacturing conditions that yield the desired process outputs. Finding the best manufacturing conditions can be done by optimizing an appropriately formulated objective function using nonlinear programming. To this end, different formulations of the optimization problem based on LVRM inversion have been proposed in the literature that allow the use of happenstance data (eg, historical data) for this purpose, present lower computational costs than optimizing in the space of the original variables, and guarantee that the solution will conform to the correlation structure of available data from the past. However, these approaches, as presented, suffer from some limitations, such as having to actively modify the constraints imposed on the solution to achieve different sets of conditions to those available in the LVRM calibration dataset, or the lack of a standardized approach for optimizing a linear combination of variables. Furthermore, when minimizing or maximizing one or more outputs, a severe handicap is also present related to the definition of arbitrarily low or high "desired" values. This paper aims at tackling all of these issues. The resulting proposed formulation of the optimization problem is illustrated with three case studies.																	0886-9383	1099-128X				JUN	2020	34	6							e3230	10.1002/cem.3230		FEB 2020											
J								Modeling and Control of a Cable-Driven Rotary Series Elastic Actuator for an Upper Limb Rehabilitation Robot	FRONTIERS IN NEUROROBOTICS										series elastic actuator (SEA); rehabilitation robot; bowden cable; torque control; impedance control; disturbance observer (DOB)	IMPEDANCE CONTROL; DESIGN; EXOSKELETON	This paper focuses on the design, modeling, and control of a novel remote actuation, including a compact rotary series elastic actuator (SEA) and Bowden cable. This kind of remote actuation is used for an upper limb rehabilitation robot (ULRR) with four powered degrees of freedom (DOFs). The SEA mainly consists of a DC motor with planetary gearheads, inner/outer sleeves, and eight linearly translational springs. The key innovations include (1) an encoder for direct spring displacement measurement, which can be used to calculate the output torque of SEA equivalently, (2) the embedded springs can absorb the negative impact of backlash on SEA control performance, (3) and the Bowden cable enables long-distance actuation and reduces the bulky structure on the robotic joint. In modeling of this actuation, the SEA's stiffness coefficient, the dynamics of the SEA, and the force transmission of the Bowden cable are considered for computing the inputs on each powered joint of the robot. Then, both torque and impedance controllers consisting of proportional-derivative (PD) feedback, disturbance observer (DOB), and feedforward compensation terms are developed. Simulation and experimental results verify the performance of these controllers. The preliminary results show that this new kind of actuation can not only implement stable and friendly actuation over a long distance but also be customized to meet the requirements of other robotic system design.																	1662-5218					FEB 25	2020	14								13	10.3389/fnbot.2020.00013													
J								Cognitive System Framework for Brain-Training Exercise Based on Human-Robot Interaction	COGNITIVE COMPUTATION										Cognitive robotic system; Cognitive training; HRI; Robot; Safety; Socially assistive robotics; Adaptive robot	OLDER-ADULTS; MANIPULATION; SIMULATION	Every 3 seconds, someone develops dementia worldwide. Brain-training exercises, preferably involving also physical activity, have shown their potential to monitor and improve the brain function of people affected by Alzheimer disease (AD) or mild cognitive impairment (MCI). This paper presents a cognitive robotic system designed to assist mild dementia patients during brain-training sessions of sorting tokens, an exercise inspired by the Syndrom KurzTest neuropsychological test (SKT). The system is able to perceive, learn and adapt to the user's behaviour and is composed of two main modules. The adaptive module based on representing the human-robot interaction as a planning problem, that can adapt to the user performance offering different encouragement and recommendation actions using both verbal and gesture communication in order to minimize the time spent to solve the exercise. As safety is a very important issue, the cognitive system is enriched with a safety module that monitors the possibility of physical contact and reacts accordingly. The cognitive system is presented as well as its embodiment in a real robot. Simulated experiments are performed to (i) evaluate the adaptability of the system to different patient use-cases and (ii) validate the coherence of the proposed safety module. A real experiment in the lab, with able users, is used as preliminary evaluation to validate the overall approach. Results in laboratory conditions show that the two presented modules effectively provide additional and essential functionalities to the system, although further work is necessary to guarantee robustness and timely response of the robot before testing it with patients.																	1866-9956	1866-9964				JUL	2020	12	4					793	810		10.1007/s12559-019-09696-2		FEB 2020											
J								Research on relation extraction of named entity on social media in smart cities	SOFT COMPUTING										Smart cities; Social media; Named entity relation extraction; Interaction scenario; Distant supervision	KRILL HERD ALGORITHM	Social media make significant contribution to the evolution of smart cities. The key issue of smart cities is to develop a series of automatic methods to support smart applications. As one of the basic techniques of smart cities, the task of relation extraction of named entities on social media provides an indispensable means to construct and expand knowledge map and contributes to the utilization of information resources in smart cities. What's more, it is conducive to improve the efficiency of network supervision. This paper proposes an automatic method to extract entity relations via deep learning techniques on a two-level neural network named Bi-CLSTM. The research conquers some challenges of relation extraction on social media. To extract entity relations on conversation scenarios, Bi-CLSTM represents texts with the strategy of "word embedding + position embedding + shortest dependency path" and extracts relations via a hybrid model of LSTM and PCNN. The nodes and networks of Bi-CLSTM are designed to adapt to the scenarios of conversation and over-sentence. To reduce the dependency on training data, distant supervised strategy is employed and a two-level attention mechanism is used to prevent noise signals. Experiments are carried out on Sina Microblog corpus, and the results show that Bi-CLSTM model makes outstanding performance.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11135	11147		10.1007/s00500-020-04742-w		FEB 2020											
J								An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications	SOFT COMPUTING										Search-based software engineering (SBSE); Fault finding; System reliability; Software testing; Hyper-heuristics	PARTICLE SWARM OPTIMIZATION; TEST SUITE; COMBINATORIAL; SEARCH; ALGORITHM; EFFICIENT; STRATEGY; DESIGN; SYSTEM; GENERATION	Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram (FBD) language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results show that Q-EMCQ is an efficient technique for test case generation. Addition- ally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach. The results show the Q-EMCQ is also capable of outperforming the original EMCQ as well as several recent meta/hyper-heuristic including modified choice function, Tabu high-level hyper-heuristic, teaching learning-based optimization, sine cosine algorithm, and symbiotic optimization search in clustering quality within comparable execution time.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13929	13954		10.1007/s00500-020-04769-z		FEB 2020											
J								Attribute reduction for multi-label classification based on labels of positive region	SOFT COMPUTING										Attribute reductions; Rough set; Multi-label classification; Positive region		In this paper, on the basis of the rough set theory, four attribute reduction algorithms are proposed for multi-label data. In order to improve the computational efficiency, the proposed algorithms utilize the lower approximations of the label information set instead of the decision class to evaluate the importance of attributes. The relationship between the proposed methods and two classical attribute reductions is analyzed and shows that the proposed methods are more applicable to multi-label classification. Experimental results reveal that the proposed algorithms can remove redundant attributes without reducing classification accuracy for most data.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14039	14049		10.1007/s00500-020-04780-4		FEB 2020											
J								A sanitization approach for big data with improved data utility	APPLIED INTELLIGENCE										Big data; Data utility; Parallel processing; Privacy preservation; Sensitive patterns; Spark		The process of collaborative data mining may sometimes expose the sensitive patterns present inside the data which may be undesirable to the data owner. Sensitive Pattern Hiding (SPH) is a subfield of data mining that addresses this problem. However, most of the existing approaches used for hiding sensitive patterns cause high side-effect on non-sensitive patterns which in-turn reduces the utility of the sanitized dataset. Furthermore, most of them are sequential in nature and are not able to cope with massive amounts of data and often results in high execution time. To resolve these identified challenges of utility and non-feasibility, two parallelized approaches have been proposed named PGVIR and PHCR based on spark parallel computing framework which modifies the data such that no sensitive patterns can be extracted while maintaining the utility of the sanitized dataset. Experiments performed using benchmark dataset shows that PGVIR scales better and PHCR causes fewer side-effects to the data compared to the existing techniques.																	0924-669X	1573-7497				JUL	2020	50	7					2025	2039		10.1007/s10489-020-01640-4		FEB 2020											
J								A three-way decision method in a hybrid decision information system and its application in medical diagnosis	ARTIFICIAL INTELLIGENCE REVIEW										Three-way decision; Method; Hybrid information system; Decision-theoretic rough set; Probability measure; Medical diagnosis; Feasibility	THEORETIC ROUGH SET; FEATURE-SELECTION; RULES; MODEL; GRANULATION; (I	In the traditional two-way decision, there are only two kinds of decisions (i.e., acceptance and rejection). It will sometimes pay unnecessary costs when one makes decisions in this way. Therefore, a three-way decision is proposed to avoid losses that caused by error acceptance or false rejection in decision-making process. An information system is a database that represents relationships between objects and attributes. A hybrid information system is an information system where there exist many kinds of data (e.g., boolean, categorical, real-valued and set-valued data) and missing data. This paper proposes a three-way decision method in a hybrid decision information system. First, the hybrid distance between two objects based on the conditional attribute set in a given hybrid decision information system is developed. Then, the tolerance relation on the object set of this hybrid decision information system is obtained by using the hybrid distance. Next, as a natural extension of decision-theoretic rough set model in an information system, decision-theoretic rough set model in this hybrid decision information system is presented. Moreover, a three-way decision method based on this decision-theoretic rough set model is proposed by means of probability measure. Finally, an example of medical diagnosis is employed to illustrate the feasibility of the proposed method, which may provide an effective method for hybrid data analysis in real applications.																	0269-2821	1573-7462				OCT	2020	53	7					4707	4736		10.1007/s10462-020-09805-w		FEB 2020											
J								Research on learning mechanism designing for equilibrated bipolar spiking neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Artificial intelligence; Spiking neural network; Supervised learning method; Ensemble learning; Excitatory and inhibitory	CLASSIFICATION; ALGORITHM	Artificial Intelligence (AI) has become very popular due to both the increasing demands from applications and the booming of computer techniques. Spiking Neural Network (SNN), as the third generation of Artificial Neural Network, receives more and more attention in the field of AI. With the high similarity to biological neural network, SNN has the potential to break through the barriers of strong AI. However, the using of SNNs on practical scenarios is rather limited, as a result of the lack of high efficient learning algorithms. Nowadays, learning methods of SNNs are designed mainly based on previous biological discoveries. The fact that there are both excitatory neurons and inhibitory neurons in the biological neural network has stimulated the motive of this research. The existence of inhibitory neurons could strengthen the self-regulation ability of neural networks and improve learning efficiency. Inspired by the ancient Chinese "Yin and Yang" Theory, we first presented our effort at constructing SNN structure with equilibrated excitatory neurons and inhibitory neurons. Then an ensemble learning optimized supervised learning method is designed and tailored for this SNN structure. Experiments are conducted using MNIST data sets, and results show that, with the designed learning mechanism, our equilibrated bipolar SNN structure could gain reasonable accuracy with much more compact structure and much more sparse synapse connections.																	0269-2821	1573-7462				OCT	2020	53	7					5189	5215		10.1007/s10462-020-09818-5		FEB 2020											
J								Extreme learning machine with hybrid cost function of G-mean and probability for imbalance learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Extreme learning machine; Imbalance learning; G-mean	NETWORKS; ALGORITHM; STRATEGY; ELM	Extreme learning machine(ELM) is a simple and fast machine learning algorithm. However, similar to other conventional learning algorithms, the classical ELM can not well process the problem of imbalanced data distribution. In this paper, in order to improve the learning performance of classical ELM for imbalanced data learning, we present a novel variant of the ELM algorithm based on a hybrid cost function which employs the probability that given training sample belong in each class to calculate the G-mean. We perform comparable experiments for our approach and the state-of-the-arts methods on standard classification datasets which consist of 58 binary datasets and 9 multiclass datasets under different degrees of imbalance ratio. Experimental results show that our proposed algorithm can improve the classification performance significantly compared with other state-of-the-art methods.																	1868-8071	1868-808X				SEP	2020	11	9					2007	2020		10.1007/s13042-020-01090-x		FEB 2020											
J								Unsupervised attribute reduction based on alpha-approximate equal relation in interval-valued information systems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Interval-valued information system; alpha-approximate equal relation; Information entropy; Unsupervised; Attribute reduction	ROUGH SET APPROACH; UNCERTAINTY MEASURES; FEATURE-SELECTION; ENTROPY MEASURES; SIMILARITY; DISTANCES	As generalizations of single-valued information systems, interval-valued information systems (IVISs) can better express real data. At present, numerous unsupervised attribute reduction approaches for single-valued information systems have been considered, but there are few researches on unsupervised attribute reduction for IVISs. In this article, we investigate a new fuzzy relation by means of similarity between interval values, and propose the concept of alpha-approximate equal relation in view of the fuzzy similarity class. Then the equivalence relation induced by alpha-approximate equal relation is used to define the information entropy, which is used to construct the unsupervised attribute reduction method together with mutual information for IVISs. Finally, experiments demonstrate that the advanced unsupervised attribute reduction method is effective and feasible in IVISs.																	1868-8071	1868-808X				SEP	2020	11	9					2021	2038		10.1007/s13042-020-01091-w		FEB 2020											
J								Efficient and optimized design of a stacked patch microstrip antenna for next generation network applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Stacked patch microstrip antenna; Aperture coupling; Bandwidth; Ansoft HFSS	WIMAX	WiMAX technology is an emerging technology for next generation network. It is a wireless broadband communications technology based around the IEE 802.16 standard providing high speed data over a wide area. Although initially seen as a candidate for 4G, its use is decreasing, although it is used as WiMAX broadband and also for last mile links. In this paper, An Aperture coupled Stacked Patch Microstrip Antenna for WiMAX application is proposed. This antenna is designed to have H-shape patch with V shape slot at the ground to provide the WiMAX band (3.3-3.6 GHz) which is a licensed spectrum that has been allocated for broadband wireless access in numerous countries around the world and widely used in military applications. FR4 is used as the substrate material. The designed antenna is simulated using Ansoft High Frequency Simulation Software. The simulated and measured results are showing that the proposed antenna is producing the bandwidth of 300 MHz with the return loss value of - 25.1 dB.																	1868-5137	1868-5145															10.1007/s12652-020-01788-4		FEB 2020											
J								SPASS-AR: A First-Order Theorem Prover Based on Approximation-Refinement into the Monadic Shallow Linear Fragment	JOURNAL OF AUTOMATED REASONING										First-order theorem proving; Model generation; Decidability; Approximation refinement		We introduce FO-AR, an approximation-refinement approach for first-order theorem proving based on counterexample-guided abstraction refinement. A given first-order clause set N is transformed into an over-approximation N in a decidable first-order fragment. That means if N is satisfiable so is N. However, if N is unsatisfiable, then the approximation provides a lifting terminology for the found refutation which is step-wise transformed into a proof of unsatisfiability for N. If this fails, the cause is analyzed to refine the original clause set such that the found refutation is ruled out for the future and the procedure repeats. The target fragment of the transformation is the monadic shallow linear fragment with straight dismatching constraints, whichwe prove to be decidable via ordered resolution with selection. We further discuss practical aspects of SPASS-AR, a first-order theorem prover implementing FO-AR. We focus in particularly on effective algorithms for lifting and refinement.																	0168-7433	1573-0670				MAR	2020	64	3			SI		611	640		10.1007/s10817-020-09546-z		FEB 2020											
J								Quality analysis in metal additive manufacturing with deep learning	JOURNAL OF INTELLIGENT MANUFACTURING										Additive manufacturing; Process monitoring; Quality identification; Deep learning; Low-quality data	MELT POOL; DIAGNOSIS	As a promising modern technology, additive manufacturing (AM) has been receiving increasing research and industrial attention in the recent years. With its rapid development, the importance of quality monitoring in AM process has been recognized, which significantly affects the property of the manufactured parts. Since the conventional hand-crafted features for quality identification are generally costly, time-consuming and sensitive to noises, the intelligent data-driven automatic process monitoring methods are becoming more and more popular at present. This paper proposes a deep learning-based quality identification method for metal AM process. To alleviate the requirement for large amounts of high-quality labeled training data by most existing data-driven methods, an identification consistency-based approach is proposed to better explore the semi-supervised training data. The proposed method is able to achieve promising performance using limited supervised samples with low quality, such as noisy and blurred images. Experiments on a real-world metal AM dataset are implemented to validate the effectiveness of the proposed method, which offers a promising tool for real industrial applications.																	0956-5515	1572-8145															10.1007/s10845-020-01549-2		FEB 2020											
J								AI4SAFE-IoT: an AI-powered secure architecture for edge layer of Internet of things	NEURAL COMPUTING & APPLICATIONS										Internet of things; IoT; Service-oriented architecture; Secure architecture; Artificial intelligence; Fog computing; Edge layer	EXECUTION ENVIRONMENT; IOT; SERVICES; TAXONOMY	With the increasing use of the Internet of things (IoT) in diverse domains, security concerns and IoT threats are constantly rising. The computational and memory limitations of IoT devices have resulted in emerging vulnerabilities in most IoT-run environments. Due to the low processing ability, IoT devices are often not capable of running complex defensive mechanisms. Lack of an architecture for a safer IoT environment is referred to as the most important barrier in developing a secure IoT system. In this paper, we propose a secure architecture for IoT edge layer infrastructure, called AI4SAFE-IoT. This architecture is built upon AI-powered security modules at the edge layer for protecting IoT infrastructure. Cyber threat attribution, intelligent web application firewall, cyber threat hunting, and cyber threat intelligence are the main modules proposed in our architecture. The proposed modules detect, attribute, and further identify the stage of an attack life cycle based on the Cyber Kill Chain model. In the proposed architecture, we define each security module and show its functionality against different threats in real-world applications. Moreover, due to the integration of AI security modules in a different layer of AI4SAFE-IoT, each threat in the edge layer will be handled by its corresponding security module delivered by a service. We compared the proposed architecture with the existing models and discussed our architecture independence of the underlying IoT layer and its comparatively low overhead according to delivering security as service for the edge layer of IoT architecture instead of embed implementation. Overall, we evaluated our proposed architecture based on the IoT service management score. The proposed architecture obtained 84.7 out of 100 which is the highest score among peer IoT edge layer security architectures.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16119	16133		10.1007/s00521-020-04772-3		FEB 2020											
J								Distributed Pinning Impulsive Control for Inner-Outer Synchronization of Dynamical Networks on Time Scales	NEURAL PROCESSING LETTERS										Inner-outer synchronization; Identical and non-identical topologies; Distributed pinning impulsive control; Time scale	RECURRENT NEURAL-NETWORKS; COMPLEX NETWORKS; ADAPTIVE-CONTROL; COUPLING DELAYS; STABILITY; EXISTENCE; CONSENSUS; SYSTEMS; DESIGN	In this paper, inner-outer synchronization problem of dynamical networks on time scales is studied. This kind of network synchronization means that two dynamical networks can achieve inner/outer synchronization simultaneously. By designing suitable distributed pinning impulsive controllers, the inner-outer synchronization target is realized. Based on the Lyapunov function method and the mathematical induction approach, two sufficient criteria are given for inner-outer synchronization of two networks with identical and non-identical topologies. Due to the structure of time scales, the derived results can be applied to study the inner-outer synchronization problems of continuous/discrete networks and networks on hybrid time domains. A numerical simulation example is given to illustrate the effectiveness of the derived results.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2481	2495		10.1007/s11063-020-10204-z		FEB 2020											
J								Scene classification using a new radial basis function classifier and integrated SIFT-LBP features	PATTERN ANALYSIS AND APPLICATIONS										Scene classification; Bag of visual words (BoVW); Radial basis function neural networks (RBFNNs); Extended PSO-OSD (EPSO-OSD); SIFT; LBP	IMAGE RETRIEVAL; SCALE; CATEGORIZATION; DESCRIPTOR; BAG	Scene classification is one of the most significant and challenging tasks in computer vision. This paper presents a new method for scene classification using bag of visual words and a particle swarm optimization (PSO)-based artificial neural network classifier. Contributions of this paper are introducing a novel feature integration method using scale invariant feature transform (SIFT) and local binary pattern (LBP) and a new framework for training radial basis function neural network, combining optimum steepest decent method with a specially designed PSO-based optimizer for center adjustment of radial basis function neural network. Our study shows that using LBP increases the performance of classification task compared to using SIFT only. In addition, our experiments on Proben1 dataset demonstrate improvements in classification performance (averagely about 6.04%) and convergence speed of the proposed radial basis function neural network. The proposed radial basis function neural network is then employed in scene classification task. Results are reported for classification of the Oliva and Torralba, Fei-Fei and Perona and Lazebnik et al. datasets. We compare the performance of the proposed classifier with a multi-way SVM classifier. Experimental results show the superiority of the proposed classifier over the state-of-the-art on the three datasets.																	1433-7541	1433-755X				AUG	2020	23	3					1071	1084		10.1007/s10044-020-00868-7		FEB 2020											
J								Web service composition on IoT reliability test based on cross entropy	COMPUTATIONAL INTELLIGENCE										cross entropy; MDP model; QoS; test; web service composite		Web service has developed the managed IoT application to let connected devices easily and securely interact with cloud applications and other devices. As an important factor for web service, the reliability of web services refers to the probability of web service running success. For modeling web service composition, we should abstract the process of web service composition. Due to the diversity and complexity of web service composition, it is unlikely to do exhaustive testing. In order to improve the quality of web service composition test cases and find out which path leads to the greatest probability of service combination failure, heuristic test case generation method is adopted to obtain the optimal test path. First, the web service composition test is abstracted into the MDP model. The QoS of the web service composition is taken as the software test optimization goal, and the cross-entropy strategy is used to optimize the test case. The experimental results show that the test profile given by the cross-strategy is better than the random test strategy. Detect and exclude the same number of software defects. Cross-entropy strategy can significantly reduce the number of test cases, reduce test costs, and improve defect detection efficiency.																	0824-7935	1467-8640															10.1111/coin.12302		FEB 2020											
J								Early diagnosis of glaucoma using multi-feature analysis and DBN based classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Deep belief network; Glaucoma early detection; Multi-feature analysis		In this new era, the advancement towards the diagnosis of disease in its early-stage has improved. The medical field is not only equipped with the new generation devices but also the approach. The ultimate aim of this research is to diagnose a Glaucoma disorder in the optical never of the eye. Glaucoma diagnosis is a complex process as it produces no sign until it affects the eye and causes a loss of vision partially or entirely. This research provides a solution to find this disorder at the early stage by analysis the retinal parameters or features extracted using high-resolution imaging process. In this screening process, the hunt for diseases such as retinal detachment, retinopathy, retinoblastoma, and age-related molecular degeneration is processed. The classification of these diseases is complicated as many of these diseases share identical characteristics which cause the doctors to confuse in identifying the particular illness for the treatment. In this research, the multi-feature analysis methodology of classification is introduced enhanced with the machine learning algorithm and a DWT (discrete wavelet transform) to classify the diseases for the unique treatment procedures. This research shows promising results as the methodology provides more than 95 percentage of Accuracy for the Glaucoma identification.																	1868-5137	1868-5145															10.1007/s12652-020-01771-z		FEB 2020											
J								Record linkage of banks and municipalities through multiple criteria and neural networks	PEERJ COMPUTER SCIENCE										Record Linkage; Entity resolution; Neural networks; Feature extraction; Deduplication	INTEGRATION	Record linkage aims to identify records from multiple data sources that refer to the same entity of the real world. It is a well known data quality process studied since the second half of the last century, with an established pipeline and a rich literature of case studies mainly covering census, administrative or health domains. In this paper, a method to recognize matching records from real municipalities and banks through multiple similarity criteria and a Neural Network classifier is proposed: starting from a labeled subset of the available data, first several similarity measures are combined and weighted to build a feature vector, then a Multi-Layer Perceptron (MLP) network is trained and tested to find matching pairs. For validation, seven real datasets have been used (three from banks and four from municipalities), purposely chosen in the same geographical area to increase the probability of matches. The training only involved two municipalities, while testing involved all sources (municipalities vs. municipalities, banks vs banks and and municipalities vs. banks). The proposed method scored remarkable results in terms of both precision and recall, dearly outperforming threshold-based competitors.																	2376-5992					FEB 24	2020									e258	10.7717/peerj-cs.258													
J								Grayscale images colorization with convolutional neural networks	SOFT COMPUTING										Colorization; Computer vision; Convolutional neural network; Self-supervised learning	COLOR; ALGORITHM	Previous approaches to the colorization of grayscale images rely on human manual labor and often produce desaturated results that are not likely to be true colorizations. Inspired by Matias Richart's paper, we proposed an automatic approach based on deep neural networks to color the image in grayscale. We have studied several models, approaches and loss functions to understand the best practices for producing a plausible colorization. By noting that some loss functions work better than others, we used the VGG-16 CNN model based on the classification with the loss of cross-entropy. The experiment shows that our model can produce a plausible colorization.																	1432-7643	1433-7479				APR	2020	24	7			SI		4751	4758		10.1007/s00500-020-04711-3		FEB 2020											
J								An efficient key authentication procedure for IND-CCA2 secure Paillier-based cryptosystem	SOFT COMPUTING										Public key cryptosystem; Authentication procedure; Decisional composite residuosity assumption; Certificate-based procedure	MUTUAL AUTHENTICATION; AGREEMENT PROTOCOL; DISCRETE LOGARITHM; PUBLIC-KEY; SCHEME; CRYPTANALYSIS; GDLP; MODEL	Public key cryptosystems more recently developed have to be strong against newer and more advanced forms of attacks. The security protection of a public key cryptosystem relies heavily on the design of the public key. The key authentication procedure is one of the easiest and most advantageous authentication mechanisms used over insecure networks and widely applied for the remote login with various operation systems, computer networks, wireless networks, database management systems, and many others. In a typical key authentication procedure, however, there is at least one authority involved to authenticate the keys. In this paper, we shall propose a new key authentication procedure built on the basis of the decisional composite residuosity assumption. As with ordinary certificate-based procedures, the proposed procedure involves no authorities. With the certificate of the public key of a client being a blend of his/her private key and password, the proposed procedure is exceptionally secure, and the authentication process is very simple.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6531	6537		10.1007/s00500-020-04768-0		FEB 2020											
J								Single axioms for (S, T)-fuzzy rough approximation operators with fuzzy product operations	SOFT COMPUTING										Fuzzy relations; Fuzzy rough sets; Triangular norms; Fuzzy product operations	SETS; REDUCTION	There are two different perspectives to study single axioms for (S, T)-fuzzy rough approximation operators, that is, ordinary fuzzy operations and fuzzy product operations. However, it is too complex and tedious to characterize (S, T)-fuzzy rough approximation operators with ordinary fuzzy operations, such as intersection, union and so on. To remedy these defects, this paper further investigates single axioms for (S, T)-fuzzy rough approximation operators with fuzzy product operations, where fuzzy relation is not limited into either a general fuzzy relation or a symmetric one. Considering a left-continuous t-norm T, we describe T-upper fuzzy rough approximation operators with fuzzy product operations by only one axiom. When t-conorm S is right-continuous and fuzzy negation N is strict, S-lower fuzzy rough approximation operators are characterized with fuzzy product operations by a single axiom.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6539	6551		10.1007/s00500-020-04774-2		FEB 2020											
J								Joint label completion and label-specific features for multi-label learning algorithm	SOFT COMPUTING										Multi-label learning; Label correlations; Label completion; Label-specific features	MISSING LABELS; CLASSIFICATION; MACHINE	Label correlations have always been one of the hotspots of multi-label learning. Using label correlations to complete the original label can enrich the information of the label matrix. At the same time, label-specific features give a thought that different labels have inherent characteristics that can be distinguished, and we can use label correlations to enhance the learning process of label-specific features among similar labels. At present, most of the algorithms combine label correlations and label-specific features to improve the multi-label learning effect, but do not consider the impact of label marking errors or defaults in data sets. In fact, the label completion method can further enrich the information of label matrix, and then the joint learning framework of joint label-specific features can effectively improve the robustness of the multi-label learning algorithm. Based on this, this paper proposes a multi-label learning algorithm for joint label completion and label-specific features, and constructs a new multi-label learning algorithm framework by means of joint label completion and label-specific features. Completion matrix and label-specific features are obtained by alternating iteration method, and the label matrix updating the optimization framework fully considers the label correlations. The algorithm in this paper has been demonstrated and trained on several benchmark multi-label data sets by extensive experiments, which verifies the effectiveness of the algorithm.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6553	6569		10.1007/s00500-020-04775-1		FEB 2020											
J								Prediction research of financial time series based on deep learning	SOFT COMPUTING										Deep learning; Convolutional neural networks; Financial prediction; Time series; Stock prediction	ARTIFICIAL NEURAL-NETWORK; MULTISCALE ENTROPY; MODEL	Currently, the world economics develops rapidly, and the finance business also develops promptly. As there are more financial activities, the uncertainty of change trend in financial activities is also increased constantly. How to study and grasp the laws of banking activity and calculate their coming tendency has grown into the concentrate and major study substance of scientific and monetary ring. On the one hand, available finance prediction can supply base for making finance plans and relevant decisions, thus ensuring the laudable expansion of the finance market and maximizing the benefits of profit organizations. However, on the other hand, convolution neural network (CNN) is a multilayer neural network composition that can simulate the operation machine-made of biological field system, which can be used to obtain effective feature description. Meanwhile, the features are extracted from the original data. Now, CNN has turned into a study hot point in the fields of giving a lecture discriminate, figure distinguishing, and classifying, and natural language handling. Moreover, it is widely used in these fields, and its application effect has been recognized by most people. Consequently, CNN composition is adopted to predict the finance time succession data. Firstly, the research means of financial time series are summarized, and then, the artificial neural network (ANN) and deep learning methods are briefly introduced. Afterward, the prediction model of stock index according to CNN model is proposed, and the influences of historical factors on model are analyzed. Finally, a few stock indexes are predicted to verify validity and effectiveness of the proposed CNN model through experimental comparison. And a hybrid model combined with CNN is found, thus further improving the cable CNN network model.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8295	8312		10.1007/s00500-020-04788-w		FEB 2020											
J								Deep learning-based data imputation on time-variant data using recurrent neural network	SOFT COMPUTING										Blood glucose prediction; RNN; EM approach; HSIC	BLOOD; PREDICTION	In general, numerous inbuilt diagnosis complications are due to improper or missing data. Thus, it becomes mandatory to perform proper imputation of the missed values to predict the diseases accurately. Imputation operations will be crucial when we encounter incompletely recorded patient data. The measurement of blood glucose level is considered to be the most important health-conscious effort that one does periodically since the false diagnosis of it leads to misinterpretation of patient health conditions that might cause fatal outcomes. But predicting those measures has become a tedious task in the course of diabetic treatment of these days. This paper focuses on the aim of the imputation of the missing patient-specific diabetic data, especially to overcome the existing methods' demerits of yielding lesser accuracy and more time. This work attempts to predict the blood glucose levels by analyzing time-series data along with the patient activities. The patient activities are being thoroughly investigated here in this work; for instance, with the first 20-day diabetic data of a patient, the diabetic forecast for the next 10 days is made in the considered month. This prediction of patient diabetic conditions is done by proposing a novel approach for predicting the blood glucose levels with the aid of Maclaurin series-based expectation maximization, estimation of correlation relationship and dissimilarities, kernel-based Hilbert-Schmidt optimization, optimized features, and classification using the deep learning methodology of RNN-recurrent neural network. Finally, we make the performance analysis with the performance metrics like accuracy, Kappa, TN, TP, FN, FP, precision, recall, Jaccard coefficient, F1-measure, and error.																	1432-7643	1433-7479				SEP	2020	24	17					13369	13380		10.1007/s00500-020-04755-5		FEB 2020											
J								Liquidity transmission and the subprime mortgage crisis: a multivariate GARCH approach	SOFT COMPUTING										Financial crisis; Multivariate GARCH; Liquidity transmission; Subprime mortgage crisis	DYNAMIC CONDITIONAL CORRELATION; VOLATILITY SPILLOVERS; STOCK-MARKET; CRUDE-OIL; MODEL; COMPLEXITY; SHOCKS; INDEX	This paper examines the liquidity transmission across the interbank money market by investigating four liquidity measurements. We detect an empirical evidence of the increase in conditional correlation across different liquidity channels during the subprime mortgage crisis. Two structural breaks are observed, and the break dates correspond to the critical events that happened at the beginning of the subprime mortgage crisis. Furthermore, two out of three significant pairwise liquidity transmissions involved the TED liquidity spread.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13871	13878		10.1007/s00500-020-04772-4		FEB 2020											
J								Flash flood potential prioritization of sub-basins in an ungauged basin in Turkey using traditional multi-criteria decision-making methods	SOFT COMPUTING										Multi-criteria decision-making methods; Morphometric parameters; Prioritization; Flash flood potential; Western Black Sea Basin	MORPHOMETRIC PARAMETERS; RISK; ALGORITHM; VIKOR; SUSCEPTIBILITY; OPERATORS; DIAGNOSIS; REGIONS; TOPSIS; SET	Morphometric analysis of watersheds based on morphometric parameters is the most widely accepted method for watershed prioritization. However, traditional methods adopted for prioritization of sub-basins lack a standard classification of the morphometric parameters and ranges of their values. So, in this study several multi-criteria decision-making (MCDM) methods and a number of traditional methods are used for watershed prioritization regarding the flash flood potential of the sub-basins. Akcay, a small ungauged basin in Turkey, was chosen as the study area, and 12 morphometric parameters were determined for the basin. The geomorphological instantaneous unit hydrograph concept coupled with Monte Carlo analysis was used to estimate the flood yield of the basin due to the lack of flow data. Kendall tau and Spearman correlation coefficient tests and receiver operating characteristics analysis were performed to validate the results of the traditional methods and the MCDM approaches for prioritization of the sub-basins. Results showed that the AHP method could well predict the sub-basins with higher flood potential, while the methodology adopted in the study to determine the criteria weights obtained from ANP method in MCDM improved the prediction capability of those approaches, especially VIKOR. The initial values of criteria weights were determined to be effective on the predictions and sensitivity analysis. When the results of traditional methods and MCDM approaches were compared, the MCDM approaches were found to give improved results. This study showed that MCDM approaches can be used to provide an efficient management of basins regarding conservation of water resources and soil.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14251	14263		10.1007/s00500-020-04792-0		FEB 2020											
J								Efficient attribute selection technique for leukaemia prediction using microarray gene data	SOFT COMPUTING										Microarray; Gene expression data; Machine learning; Attribute selection; Effective correlation-based attribute selection (ECFS); Ant colony optimization (ACO); Ant lion optimization (ALO); Support vector machine (SVM)	CLASSIFICATION; MACHINE; DISCOVERY; ALGORITHM; CANCER	The recent advancements in today's medical sciences regarding Data Analytics have made it possible for the use of efficient techniques for analysis. For prognosis, diagnosis and cancer treatment, a microarray-based gene expression profiling is considered. Informative genes causing cancer are determined through the deoxyribonucleic acid microarray technique. Dimensionality is the utmost concern while working with multi-dimensional data analysis which acts as a barrier in extracting information from a dataset which leads to costly computational complexity. Thus, an imperative task in the selection of relevant features in the analysis of cancer microarray datasets is crucial towards effective classification. This work focuses on variable selection techniques by utilizing effective correlation for attribute selection along with ant colony optimization. The criterion of a given classifier is maximized through wrapper-based attribute selection, and so it needs efficient searching techniques in finding optimal feature combinations. A new wrapper-based selection technique which uses ant lion optimization (ALO) in finding optimal feature set is proposed in this work which maximizes classification performance. The natural shooting procedure of ant lions is imitated in the proposed ALO algorithm. Support vector machine technique was utilized for the classification of chosen marker genes.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14265	14274		10.1007/s00500-020-04793-z		FEB 2020											
J								Optimized PSO algorithm based on the simplicial algorithm of fixed point theory	APPLIED INTELLIGENCE										PSO algorithm; FP-PSO algorithm; Simplicial algorithm; Fixed point; Initial population	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; PERFORMANCE; IDENTIFICATION; PLACEMENT; INERTIA; SEARCH; DESIGN	Particle swarm optimization algorithm (PSO) has been optimized from various aspects since it was proposed. Optimization of PSO can be realized by optimizing its iterative process or the initial parameters and heuristic methods have been combined with the initial PSO algorithm to improve its performance. In this paper, we introduce the Simplicial Algorithm (SA) of fixed point theory into the optimization of PSO and proposed a FP-PSO (Fixed-point PSO) improved algorithm. In FP-PSO algorithm, the optimization of target function is converted into the problem of solving a fixed point equation set, and the solution set obtained by Simplicial Algorithm (SA) of fixed point theory is used as the initial population of PSO algorithm, then the remaining parameters can be obtained accordingly with classical PSO algorithm. Since the fixed point method has sound mathematical properties, the initial population obtained with FP-PSO include nearly all the approximate local extremes which maintain the diversity of population and can optimize the flight direction of particles, and shows their advantages on setting other initial parameters. We make an experimental study with five commonly used testing functions from UCI (University of California Irvine) which include two single-peak functions and three multi-peak functions. The results indicate that the convergence accuracy, stability, and robustness of FP-PSO algorithm are significantly superior to existing improve strategies which also optimize PSO algorithm by optimizing initial population, especially when dealing with complex situations. In addition, we nest the FP-PSO algorithm with four classical improved PSO algorithms that improve PSO by optimizing iterative processes, and carry out contrast experiments on three multi-peak functions under different conditions (rotating or non-rotating). The experimental results show that the performance of the improved algorithm using nested strategy are also significantly enhanced compared with these original algorithms.																	0924-669X	1573-7497				JUL	2020	50	7					2009	2024		10.1007/s10489-020-01630-6		FEB 2020											
J								Layout2image: Image Generation from Layout	INTERNATIONAL JOURNAL OF COMPUTER VISION										Scene image generation; Image translation; Image generation; Generative adversarial networks		Despite significant recent progress on generative models, controlled generation of images depicting multiple and complex object layouts is still a difficult problem. Among the core challenges are the diversity of appearance a given object may possess and, as a result, exponential set of images consistent with a specified layout. To address these challenges, we propose a novel approach for layout-based image generation; we call it Layout2Im. Given the coarse spatial layout (bounding boxes + object categories), our model can generate a set of realistic images which have the correct objects in the desired locations. The representation of each object is disentangled into a specified/certain part (category) and an unspecified/uncertain part (appearance). The category is encoded using a word embedding and the appearance is distilled into a low-dimensional vector sampled from a normal distribution. Individual object representations are composed together using convolutional LSTM, to obtain an encoding of the complete layout, and then decoded to an image. Several loss terms are introduced to encourage accurate and diverse image generation. The proposed Layout2Im model significantly outperforms the previous state-of-the-art, boosting the best reported inception score by 24.66% and 28.57% on the very challenging COCO-Stuff and Visual Genome datasets, respectively. Extensive experiments also demonstrate our model's ability to generate complex and diverse images with many objects.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2418	2435		10.1007/s11263-020-01300-7		FEB 2020											
J								Respiratory signal and human stress: non-contact detection of stress with a low-cost depth sensing camera	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Stress detection; Stress classification; Remote sensing respiration signal; Physiological features of stress	PSYCHOLOGICAL STRESS; PHYSICAL STRESS; RECOGNITION; EMOTION; PATTERNS	Psychological stress may cause various health problems. To prevent the potential chronic illness that long-term psychological stress could cause, it is important to detect and monitor the psychological stress at its initial stage. In this paper, we present a framework for remotely detecting and classifying human stress by using a KINECT sensor that is portable and affordable enough for ordinary users in everyday life. Unlike most of emotion recognition tasks in which respiratory signals (RSPS) are usually used only as an aiding analysis, the whole task proposed is based on RSPS. Thus, the main contribution of this paper is that not only the non-contact devices is used to identify human stress, but also the relationship between RSPS and stress recognition is analyzed in detail. Experimental results on 84 volunteers show that the recognition accuracy for recognizing psychological stress, physical stress, and relaxing state are 93.90%, 93.40%, and 89.05% respectively. These results suggest that the proposed framework is effective for monitoring human stress, and RSPS could be used for stress recognition.																	1868-8071	1868-808X				AUG	2020	11	8					1825	1837		10.1007/s13042-020-01074-x		FEB 2020											
J								Fully-connected LSTM-CRF on medical concept extraction	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Medical concept extraction; Named entity recognition; CRF; LSTM; Deep learning		Patient symptoms, test results, and treatment information have been taking down in extensive electronic records. Specifically, the named entity recognition of these medical concepts has high application value in the clinical field. However, due to issues like patient privacy, labeled data is expensive and difficult to find. In 2010, the i2b2/VA Natural Language Processing Challenge started a conceptual extraction task for electronic medical records. One of the task requirements is to classify natural language descriptions as corresponding concept types. In this paper, we proposed a new fully-connected LSTM network, while the LSTM + CRF model is used as the framework to test the effects of various LSTM structures. The real-data experiments demonstrate that the proposed fully-connected LSTM outperforms many of the mainstream LSTM structures in the quantitative evaluation. It is confirmed that the multi-layer bidirectional fully-connected LSTM cooperates with the character level word vector and the pre-trained word embedding, which achieves similar performance compared with the state-of-the-art methods, avoiding the using of prior knowledge data and ultra-high dimensional feature representation. Moreover, this end-to-end training method saves a lot of feature engineering work and storage spaces.																	1868-8071	1868-808X				SEP	2020	11	9					1971	1979		10.1007/s13042-020-01087-6		FEB 2020											
J								Unsupervised image-to-image translation using intra-domain reconstruction loss	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Generative adversarial networks; Image-to-image translation; Deep learning; Intra-domain reconstruction loss		Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, GANs are often accompanied by training instability and mode collapse in the process of image-to-image translation, which leads to the generation of low-quality images. To address the aforementioned problem, by combining CycleGAN and intra-domain reconstruction loss (IDRL), we propose an unsupervised image-to-image translation network named "Cycle-IDRL". Specifically, the generator adopts the U-Net network with skip connections, which merges the coarse-grained and fine-grained features and the least squares loss in LSGAN is used to improve the stability of training process. Especially, the target domain features extracted from the discriminator are used as input of generator to generate reconstructed samples. Then, we construct the IDRL between the target domain samples and the reconstructed samples by using L-1 norm. The experimental results on multiple datasets show that the proposed method performs better than the existing methods.																	1868-8071	1868-808X				SEP	2020	11	9					2077	2088		10.1007/s13042-020-01098-3		FEB 2020											
J								An adaptive ensemble classification framework for real-time data streams by distributed control systems	NEURAL COMPUTING & APPLICATIONS										Smart energy grids; Kappa architecture; Ensemble learning; Advanced persistent threats; Industrial control systems		Smart Grids are critical infrastructure networks. They play a critical role in the survival of our postmodern economies, as all other areas depend on their availability. An interruption in their operation may have a direct impact on the availability of other services (e.g., health, transportation). The problem is particularly intense when no backup networks are available, or the required recovery time is beyond backup autonomy. The transition to a decentralized management and control system for these networks requires digital technologies, advanced interconnected system communications, and Internet access. These technologies expose critical infrastructure networks to external threats that require careful assessment of cyber-security risks and appropriate countermeasures. An important factor that enhances the range of threats is the heterogeneity of Smart Grids, which incorporate industrial control systems such as the SCADA, distributed control system, and programmable logic controllers to which security improvements may not have been made since they were installed. & x3a5;et, another serious problem arises from the fact that older technologies were designed at times when cyber-security was not part of their technical design specifications. At the same time, it should be seriously considered that many of the systems of these networks that can be cyber-attacked may not be easily disconnected, as this could potentially lead to generalized operational problems. In this scientific research, a sophisticated active security framework is proposed, which is based solely on advanced computational intelligence methods and concerns the digital security of critical infrastructure networks. Specifically, this research introduces a sophisticated adaptive ensemble classification framework for real-time data streams by distributed control systems. It is a "Kappa" architecture framework that is based on a two-step online ensemble learning model based on bagging and boosting methods. The aim is performance of real-time analysis and evaluation of data flows from Smart Grids, toward the effective identification of APT attacks.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4139	4149		10.1007/s00521-020-04759-0		FEB 2020											
J								Critical infrastructure protection based on memory-augmented meta-learning framework	NEURAL COMPUTING & APPLICATIONS										Critical infrastructure protection; Meta-learning; Advanced persistent threats; Memory-augmented architecture		Critical infrastructures are related to systems which are essential for sustaining the important functions of a society. Their potential failures can cause serious problems not only to the population and economy but also to national security as well. The importance of these infrastructures calls for measures toward their security and protection. The aim was the reduction of risk related to natural disasters, terrorist acts, and cyberthreats. Traditional security systems, even those that employ intelligent algorithms, fail to prevent advanced zero-day attacks as they require constant training. This research proposes a novel meta-learning architecture that considers the neural turing machines as the approach upon which the model is founded. The introduced model allows for the memorization of useful data from past processes, by integrating external storage memory. Moreover, it facilitates the rapid integration of new information without the need for retraining. In particular, the proposed novel architecture is called memory-augmented neural network (M-ANN) whose core is a sophisticated, very fast, and highly efficient extreme learning machine. The M-ANN is assisted by a series of original modifications, related to fine-tuning of training, to memory retrieval mechanisms, to addressing techniques, and to ways of attention-weight allocation to memory vectors. The efficiency of the proposed system has been successfully tested using an extremely complex scenario for the protection of critical infrastructures. According to the testing scenario, memory could quickly encode and record information about new types of attacks, while any stored representation from previous experience was easily and consistently accessible, to maximize the detection efficiency.																	0941-0643	1433-3058															10.1007/s00521-020-04760-7		FEB 2020											
J								Intra- and Inter-modal Multilinear Pooling with Multitask Learning for Video Grounding	NEURAL PROCESSING LETTERS										Video grounding; Multimodal learning; Multimedia data analysis; Deep learning		Video grounding aims to temporally localize an action in an untrimmed video referred to by a query in natural language, which plays an important role in fine-grained video understanding. Given temporal proposals of limited granularity, the task is challenging that it requires fusing multi-modal features from questions and videos effectively, and localizing the referred action accurately. For multimodal feature fusion, we present an Intra- and Inter-modal Multilinear pooling (IIM) model to effectively combine the multi-modal features with considering both the intra- and inter-modal feature interactions. Compared to existing multimodal fusion models, IIM can capture high-order interactions and is more capable for modeling temporal information of videos. For action localization, we propose a simple yet effective multi-task learning framework to simultaneously predict the action label, alignment score and refined location in an end-to-end manner. Experimental results on real-world TaCoS and Charades-STA datasets demonstrate the superiority of the proposed approach over existing state-of-the-art methods.																	1370-4621	1573-773X															10.1007/s11063-020-10205-y		FEB 2020											
J								Two-sided matching decision-making model with hesitant fuzzy preference information for configuring cloud manufacturing tasks and resources	JOURNAL OF INTELLIGENT MANUFACTURING										Two-sided matching; Hesitant fuzzy preference information; Matching model; Cloud manufacturing; Configuration of tasks and resources	ENERGY MANAGEMENT; SETS; ALLOCATION; EFFICIENCY; STABILITY; SELECTION; INTERNET; SMART	Owing to the complexity of socio-economic environments and the fuzziness of human cognition, information of cognitive preference provided by decision-making organizations composed of many experts is often hesitant and fuzzy. In consequence, for the sake of addressing the hesitance and fuzziness of preference information for the configuration of tasks and resources in cloud manufacturing, a decision-making model of a two-sided matching considering a bidirectional projection under preference information of hesitant fuzzy is put forward. Primarily, this paper describes the problem of two-sided matching and introduces the hesitant fuzzy set. Afterwards, according to preference information given by matching agents using the hesitant fuzzy element, the evaluation matrix is constructed. Meanwhile, the bidirectional projection technology and TOPSIS method are combined to calculate the closeness degree matrix. Further, by introducing the constraint of the stable matching, a decision-making model of a two-sided matching for maximizing the closeness degree of two-sided matching agents is constructed, and the optimal configuration results are obtained by the solving of the model. Subsequently, the illustrative case is provided to validate the rationality and effectiveness of the presented model in solving the configuration for cloud manufacturing tasks and resources. Also, the stability in the proposed configuration results is illustrated by making a sensitivity analysis. Further, the reliability in the given solving process is demonstrated through performing a comparative analysis, as well as the advantages in the proposed model is also discussed. It shows that this developed model can give stable configuration results and also provide a matching approach for different agents under an uncertain environment.																	0956-5515	1572-8145															10.1007/s10845-020-01552-7		FEB 2020											
J								Various realization methods of machine-part classification based on deep learning	JOURNAL OF INTELLIGENT MANUFACTURING										Artificial intelligence; Artificial neural networks; Feature extraction; Industrial informatics; Optimized production technology; Classification algorithms; Image classification	3D OBJECT RECOGNITION; INFORMATION; FEATURES	Parts classification can improve the efficacy of the manufacturing process in a computer-aided process planning system. In this study, we investigate various methodologies to assist with parts classification based on deep learning technologies, including a two-dimensional convolutional neural network (2D-CNN) trained using both picture data and CSV files; and a three-dimensional convolutional neural network (3D-CNN) trained using voxel data. Additionally, two novel methods are proposed: (1) feature recognition for the processing parts based on syntactic patterns, where their feature quantities are computed and saved to comma-separated variable (CSV) files that are subsequently employed to train the 2D-CNN model; and (2) voxelization of parts, wherein the voxel data of the parts is obtained for training the 3D-CNN model. The two methods are compared with a 2D-CNN model trained with the images of parts to classify. Results indicated that the 2D-CNN model trained with CSV data yielded the best performance and highest accuracy, followed by the 3D-CNN model, which was simpler and easier to implement and utilized better learning ability for the parts' details. The 2D-CNN model trained with picture files evinced the lowest accuracy and a complex training network.																	0956-5515	1572-8145															10.1007/s10845-020-01550-9		FEB 2020											
J								Reliable Dissipative Interval Type-2 Fuzzy Control for Nonlinear Systems with Stochastic Incomplete Communication Route and Actuator Failure	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Random incomplete transmission; Interval type-2 (IT2) fuzzy control; Sensor saturation; Quantization; Actuator failure; Dissipativity	NETWORKED CONTROL-SYSTEMS; STABILITY ANALYSIS; NEURAL-NETWORKS; DESIGN; STABILIZATION	This paper investigates the dissipative interval type-2 (IT2) fuzzy control issue for a group of uncertain nonlinear systems subject to randomly occurring sensor saturation and quantization. By utilizing the both lower and upper membership functions with relevant tradeoff moduli, uncertainties, which exist in the considered systems, can be captured effectively. During the design of the IT2 fuzzy controller, we take the actuator failure into consideration. Considering the randomicities for the occurrences of sensor saturation and quantization, two separate Bernoulli stochastic distributed courses are utilized to construct the corresponding complicated transmission route. In term of Lyapunov stability theory, sufficient criteria to design dissipative IT2 fuzzy controller are presented to assure the stochastic stability with strict dissipativity of resulting closed-loop systems. Finally, illustrative example verifies the effectiveness and merits of the control strategy proposed in this paper.																	1562-2479	2199-3211				MAR	2020	22	2			SI		368	379		10.1007/s40815-020-00807-y		FEB 2020											
J								Output Feedback Adaptive Fuzzy Control for Uncertain Fractional-Order Nonlinear Switched System with Output Quantization	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Adaptive fuzzy control; Output quantization; Fractional-order switched nonlinear systems; Fuzzy logic systems (FLSs)	LYAPUNOV FUNCTIONS; ROBUST; STABILIZATION; STABILITY; MODEL	In this paper, a novel adaptive fuzzy controller is developed for the uncertain fractional-order switched nonlinear systems whose output is quantized by a class of sector-bounded quantizers. Since the states are not completely measurable, an observer with quantized output signal is designed to estimate the unknown system states. Meanwhile, based on the fractional Lyapunov stability criterion, the Lyapunov function with sum functions and the virtual control function with hyperbolic tangent functions are designed. Besides, in order to improve the approximation accuracy of the unknown nonlinear functions generated by fractional differential, the prediction errors and auxiliary variables of series-parallel estimation model are introduced in backstepping procedures. The simulation results show that the control scheme ensures that all the signals of the considered system remain semi-globally uniformly ultimately bounded, and the tracking error converges to a small neighborhood of the origin regardless of arbitrary switching.																	1562-2479	2199-3211				APR	2020	22	3					943	955		10.1007/s40815-020-00814-z		FEB 2020											
J								Diagnosis of cerebral microbleed via VGG and extreme learning machine trained by Gaussian map bat algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cerebral microbleed; Magnetic resonance image; Deep learning; Extreme learning machine; Gaussian map	CONVOLUTIONAL NEURAL-NETWORK; COMPUTER-AIDED DETECTION; VOXELWISE DETECTION; CLASSIFICATION	Cerebral microbleed (CMB) is a serious public health concern. It is associated with dementia, which can be detected with brain magnetic resonance image (MRI). CMBs often appear as tiny round dots on MRIs, and they can be spotted anywhere over brain. Therefore, manual inspection is tedious and lengthy, and the results are often short in reproducible. In this paper, a novel automatic CMB diagnosis method was proposed based on deep learning and optimization algorithms, which used the brain MRI as the input and output the diagnosis results as CMB and non-CMB. Firstly, sliding window processing was employed to generate the dataset from brain MRIs. Then, a pre-trained VGG was employed to obtain the image features from the dataset. Finally, an ELM was trained by Gaussian-map bat algorithm (GBA) for identification. Results showed that the proposed method VGG-ELM-GBA provided better generalization performance than several state-of-the-art approaches.																	1868-5137	1868-5145															10.1007/s12652-020-01789-3		FEB 2020											
J								Path Planning of Multiple Unmanned Marine Vehicles for Adaptive Ocean Sampling Using Elite Group-Based Evolutionary Algorithms	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Path planning; Multiple unmanned marine vehicles; Adaptive ocean sampling; Simulated annealing algorithm; Particle swarm optimization	AUTONOMOUS UNDERWATER VEHICLES; DYNAMIC TASK ASSIGNMENT; INITIAL TEMPERATURE; AUV SYSTEM; OPTIMIZATION; DECISION	This paper presents elite group-based evolutionary algorithms (EGEA) for adaptive ocean sampling using multiple unmanned marine vehicles (UMVs). The EGEA integrate a group-based framework and elitist selection methods into evolutionary path planner, which combine main advantages of these two techniques.The group-based framework allows each offspring individual of evolutionary algorithm to generate its own group of new solutions with a certain probability. Two elitist selection methods, herein referred to as group individual elitist selection (GIES) and whole population elitist selection (WPES), are proposed to facilitate selecting preferable solutions to be passed on to the next generation in the procedure of evolutionary algorithms. The EGEA path planners based on simulated annealing algorithm (SA) and particle swarm optimization (PSO) are tested to find trajectories for multiple UMVs to collect maximum interested ocean information from regions under investigation. The mixed integer linear programming (MILP) is also described and evaluated with the proposed EGEA for solving the adaptive sampling problem. Simulation results show that the whole elite group-based simulated annealing algorithm (WEGSA) is able to generate trajectories with more information gain from regions of high scientific interest with constrained energy of multiple UMVs than other techniques. Monte Carlo simulations demonstrate the inherent robustness and superiority of the proposed planner based on the EGEA in comparison with other techniques.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		875	889		10.1007/s10846-020-01155-7		FEB 2020											
J								Learning decision trees through Monte Carlo tree search: An empirical evaluation	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										data mining; decision trees; Monte Carlo tree search	EVOLUTIONARY ALGORITHMS; SOFTWARE TOOL; CLASSIFIERS; GAME; KEEL	Decision trees (DTs) are a widely used prediction tool, owing to their interpretability. Standard learning methods follow a locally optimal approach that trades off prediction performance for computational efficiency. Such methods can however be far from optimal, and it may pay off to spend more computational resources to increase performance. Monte Carlo tree search (MCTS) is an approach to approximate optimal choices in exponentially large search spaces. We propose a DT learning approach based on the Upper Confidence Bound applied to tree (UCT) algorithm, including procedures to expand and explore the space of DTs. To mitigate the computational cost of our method, we employ search pruning strategies that discard some branches of the search tree. The experiments show that proposed approach outperformed the C4.5 algorithm in 20 out of 31 datasets, with statistically significant improvements in the trade-off between prediction performance and DT complexity. The approach improved locally optimal search for datasets with more than 1,000 instances, or for smaller datasets likely arising from complex distributions. This article is categorized under: Algorithmic Development > Hierarchies and Trees Application Areas > Data Mining Software Tools Fundamental Concepts of Data and Knowledge > Data Concepts																	1942-4787	1942-4795				MAY	2020	10	3							e1348	10.1002/widm.1348		FEB 2020											
J								Allocating the fixed cost based on data envelopment analysis in view of the Shapley value	EXPERT SYSTEMS										coalition efficiency; data envelopment analysis; fixed cost allocation; Shapley value	RESOURCE-ALLOCATION; EFFICIENCY INVARIANCE; SHARED COSTS; EQUITABLE ALLOCATION; DEA; WEIGHTS; UNITS	This paper considers fixed cost allocation in view of cooperative game theory and proposes an approach based on data envelopment analysis while incorporating the perspectives of coalition efficiency and the Shapley value. To do this, we first build two models to evaluate coalition efficiencies before and after cost allocation, and we prove that all coalitions can be efficient after fixed cost allocation. Then, following the premise that each coalition makes itself efficient without reducing the efficiencies of other decision-making units' preallocation efficiency, we propose a model that determines the acceptable range of each coalition's allocated fixed cost. Furthermore, a model is constructed to determine the final cost allocation based on three principles: efficiency, monotonicity, and similarity. Moreover, the Shapley value is employed to obtain the cost allocated to each decision-making unit (DMU). The proposed approach considers the relationships among DMUs across their forming coalitions to determine their interaction types and then generates a fixed cost allocation result that possesses the features of the Shapley value. This process makes the fixed cost allocation more acceptable. Finally, a simple numerical example and an empirical case are provided to illustrate the calculation process of the proposed approach and compare our approach with the traditional methods.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12539	10.1111/exsy.12539		FEB 2020											
J								A Puzzle concerning Compositionality in Machines	MINDS AND MACHINES										Compositionality; Deep Neural Networks; Deep learning; machine learning; Epistemic opacity; Artificial Intelligence	NETWORKS; GAME; GO	This paper attempts to describe and address a specific puzzle related to compositionality in artificial networks such as Deep Neural Networks and machine learning in general. The puzzle identified here touches on a larger debate in Artificial Intelligence related to epistemic opacity but specifically focuses on computational applications of human level linguistic abilities or properties and a special difficulty with relation to these. Thus, the resulting issue is both general and unique. A partial solution is suggested.																	0924-6495	1572-8641				MAR	2020	30	1					47	75		10.1007/s11023-020-09519-6		FEB 2020											
J								A density-core-based clustering algorithm with local resultant force	SOFT COMPUTING										Clustering; Density core; Natural neighbor; Local resultant force	OPTIMIZATION; SEARCH	Clustering analysis has been widely used in image segmentation, face recognition, protein identification, intrusion detection, document clustering and so on. Most of the previous clustering algorithms are not suitable for complex situations with manifold structure and large variations in density. Clustering by density core (DCore) turns out to be a very effective clustering method for complex structure. However, DCore must set too many parameters for better results, which often fails when the shape of data is complex and the density of data varies too much. Inspired by universal gravitation, we propose a novel clustering algorithm (called DCLRF) based on density core and local resultant force. In this algorithm, each data point is viewed as an object with a local resultant force (LRF) generated by its neighbors and a local measure named centrality is proposed based on LRF and natural neighbors. Firstly, we extract core points using the CE value. Then, we use the natural neighbor structure information of core points to get the final clustering results. Excluding the influence of noise, core points can well represent the structure of clusters. Therefore, DCLRF can obtain the optimal cluster numbers for the datasets which contain clusters of arbitrary shapes. Both synthetic datasets and real datasets are used for experiments to verify the efficiency and accuracy of the DCLRF.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6571	6590		10.1007/s00500-020-04777-z		FEB 2020											
J								An optimal pruning algorithm of classifier ensembles: dynamic programming approach	NEURAL COMPUTING & APPLICATIONS										Machine learning; Classification; Classifier ensembles; Dynamic programming; Diversity; Game theory; Cooperative game	DIVERSITY; SELECTION; INTEGRATION	In recent years, classifier ensemble techniques have drawn the attention of many researchers in the machine learning research community. The ultimate goal of these researches is to improve the accuracy of the ensemble compared to the individual classifiers. In this paper, a novel algorithm for building ensembles called dynamic programming-based ensemble design algorithm (DPED) is introduced and studied in detail. The underlying theory behind DPED is based on cooperative game theory in the first phase and applying a dynamic programming approach in the second phase. The main objective of DPED is to reduce the size of the ensemble while encouraging extra diversity in order to improve the accuracy. The performance of the DPED algorithm is compared empirically with the classical ensemble model and with a well-known algorithm called "the most diverse." The experiments were carried out with 13 datasets from UCI and three ensemble models. Each ensemble model is constructed from 15 different base classifiers. The experimental results demonstrate that DPED outperforms the classical ensembles on all datasets in terms of both accuracy and size of the ensemble. Regarding the comparison with the most diverse algorithm, the number of selected classifiers by DPED across all datasets and all domains is less than or equal to the number selected by the most diverse algorithm. Experiment on blog spam dataset, for instance, shows that DPED provides an accuracy of 96.47 compared to 93.87 obtained by the most diverse using 40% training size. Finally, the experimental results verify the reliability, stability, and effectiveness of the proposed DPED algorithm.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16091	16107		10.1007/s00521-020-04761-6		FEB 2020											
J								A self-adaptive synthetic over-sampling technique for imbalanced classification	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										fairness; imbalanced classification; performance boosting; synthetic data generation	DECISION TREE; SMOTE	Traditionally, in supervised machine learning, (a significant) part of the available data (usually 50%-80%) is used for training and the rest-for validation. In many problems, however, the data are highly imbalanced in regard to different classes or does not have good coverage of the feasible data space which, in turn, creates problems in validation and usage phase. In this paper, we propose a technique for synthesizing feasible and likely data to help balance the classes as well as to boost the performance in terms of confusion matrix as well as overall. The idea, in a nutshell, is to synthesize data samples in close vicinity to the actual data samples specifically for the less represented (minority) classes. This has also implications to the so-called fairness of machine learning. In this paper, we propose a specific method for synthesizing data in a way to balance the classes and boost the performance, especially of the minority classes. It is generic and can be applied to different base algorithms, for example, support vector machines, k-nearest neighbour classifiers deep neural, rule-based classifiers, decision trees, and so forth. The results demonstrated that (a) a significantly more balanced (and fair) classification results can be achieved and (b) that the overall performance as well as the performance per class measured by confusion matrix can be boosted. In addition, this approach can be very valuable for the cases when the number of actual available labelled data is small which itself is one of the problems of the contemporary machine learning.																	0884-8173	1098-111X				JUN	2020	35	6					923	943		10.1002/int.22230		FEB 2020											
J								Enhanced zippy restricted Boltzmann machine for feature expansion and improved classification of analytical data	JOURNAL OF CHEMOMETRICS										autoencoder; classification; Copiosity Principle; EZRBM; feature augmentation; restricted Boltzmann machine	GAS-CHROMATOGRAPHY; VALIDATION; SPECTRA; MODELS	Restricted Boltzmann machines (RBMs) are components found in many deep learning algorithms. RBMs originally were designed for binary image data. Some advances in RBM algorithms have been made so that they may accept real-valued inputs that are typical for analytical chemistry measurements. However, these algorithms are difficult to train and require fine-tuning of the parameters. The RBM algorithm was modified to furnish the enhanced zippy RBM (EZRBM) that trains reliably and robustly with respect to the parameters. In addition, feature augmentation (ie, fusing the RBM linear inputs and nonlinear outputs) improves the classification rate while reducing the dependence of the RBM training parameters. Two different classifiers were used, the support vector classifier (SVC) and super partial least squares-discriminant analysis (sPLS-DA), to evaluate the performance. Classifiers built from the EZRBM outputs performed better than those built from the continuous RBM and the Gaussian RBM (GRBM) outputs when validated using 100 bootstraps with two Latin partitions. Three datasets were used. The first was an overdetermined set of eight fatty acid concentrations for 572 olive oils from nine regions of Italy. The second was 75 UV spectra of 15 Cannabis extracts with 101 measurements made from 200 to 400 nm. The third set comprised 60 proton nuclear magnetic resonance (NMR) spectra of 12 tea extracts that had 1000 chemical shift measurements from 0.5 to 7.0 ppm. In every evaluation, the augmented EZRBM had better classification performance than the classifiers without the RBM. The classifiers built with EZRBM outperformed the other RBM algorithms except for a single instance. Recently, RBMs have been considered a transform into a dual feature space.																	0886-9383	1099-128X				MAR	2020	34	3			SI				e3228	10.1002/cem.3228		FEB 2020											
J								Deconvolution of Gaussian peaks with mixed real and discrete-integer optimization based on evolutionary computing	JOURNAL OF CHEMOMETRICS										absorption spectroscopy; data fitting; genetic algorithm; GPU programming; peak overlap	GENETIC ALGORITHM APPROACH; GEOMETRY OPTIMIZATION; PARALLEL	This study describes an alternative method for deconvolution of overlapping characteristic Gauss peaks with the help of optimization of a mixed variable genetic algorithm. Continuous and discrete variables and nonlinear discrete variables in optimization problems cause solution complexity. The processing and analysis of complex analytical signals is important not only in analytical chemistry but also in other fields of science. As the amount of data increases and linearity decreases, high-performance computations are needed to solve analytical signals. It takes a long time to perform these calculations with traditional processor systems and algorithms. We have used NVIDIA graphical processing units (GPUs) to shorten the duration of these calculations. Solving such analytical signals with genetic algorithms is widely used in computational sciences. In this study, we present a new curve-fitting method using a genetic algorithm based on Gauss functions used to deconvolve overlapping peaks and find the exact peak number in absorption spectroscopy. The deconvolution of individual bands in the UV-VIS region is a complex task, because the absorption bands are broad and often strongly overlap. Useful information about the molecular structure and environment can only be obtained by appropriate and truthful separation of these peaks.																	0886-9383	1099-128X				JUN	2020	34	6							e3229	10.1002/cem.3229		FEB 2020											
J								Local transform directional pattern and optimization driven DBN for age estimation	EVOLUTIONARY INTELLIGENCE										Age estimation; Face image; AAM features; LDP; DBN	FACE; FEATURES; IMAGE	Age estimation is an interesting and challenging research area, gaining significant importance in the recent era and is employed in various applications, such as intelligent surveillance, face recognition, biometrics, and so on. Various techniques are employed in the literature for age estimation from the face images. This paper introduces the age estimation scheme from the face image, and estimation is done by defining a novel feature extraction strategy, named Local Transform Directional Pattern (LTDP). The database containing the input images has many unwanted regions, and thus, the Viola Jones algorithm detects the required face region. After detecting the face regions, active appearance model extracts the active appearance features, and the proposed LTDP extracts the texture features. The proposed LTDP feature extraction model modifies the existing Local Directional Pattern (LDP) with several other texture feature extraction models. After the feature extraction, the Cuckoo search based Deep Belief Network (CDBN) classifier estimates the age of the person from the face image based on the extracted features. The simulation results reveal that the proposed LTDP with the CDBN classifier achieved high performance with the values of 2.3416, 0.9803, and 0.9724 for MAE, AEO, and AEM, respectively.																	1864-5909	1864-5917															10.1007/s12065-020-00363-2		FEB 2020											
J								An integrated group FWA-ELECTRE III approach based on interval type-2 fuzzy sets for solving the MCDM problems using limit distance mean	COMPLEX & INTELLIGENT SYSTEMS										Material handling equipment selection problem; Gaussian interval type-2 fuzzy sets; FWA; ELECTRE III	MATERIAL HANDLING EQUIPMENT; GROUP DECISION-MAKING; KNOWLEDGE-BASED SYSTEM; SUPPORT-SYSTEM; LOGIC SYSTEMS; SELECTION; DESIGN; CRITERIA; RANKING; ENERGY	The material handling equipment (MHE) has a close connection with layout of machinery and plays the important role in productivity of servicing or manufacturing systems. Since each of MHE has distinct characteristics than the others with respect to conflicting criteria and design experts may state the different subjective judgments with respect to qualitative criteria, the material handling equipment selection problem (MHESP) can be taken into account as a group multi-criteria decision-making (GMCDM) problem. In this paper, a version of type-2 fuzzy sets (T2FSs), named Gaussian interval type-2 fuzzy sets (GIT2FSs), is first used as an alternative to the traditional triangular membership functions (MFs) to weight criteria and sub-criteria and also evaluate of alternatives with respect to sub-criteria. The synthetic value method of GIT2FSs is then carried out to convert the assessments stated as GIT2FSs for each alternative with respect to each sub-criterion and also weights of criteria (sub-criteria) into the single fuzzy rating and weight, respectively. Then, the fuzzy weighted average (FWA) approach is adopted to integrate the single fuzzy ratings of each alternative with respect to sub-criteria and the single fuzzy weights of sub-criteria under each criterion with the aggregated weighted ratings. In next stage, ELECTRE III (ELimination Et Choix Traduisant la Realite '-elimination and choice translation reality) is generalized with GIT2FSs to select the optimal MHE through a new ranking approach. Moreover, some arithmetic operations and properties are extended to GIT2FSs. In addition, to demonstrate its potential applications, the proposed methodology is implemented in a real case study and an illustrative example, and then, the ranking results are compared with those of the others in the literature. Finally, the sensitivity analysis is carried out to show robustness and stability of the obtained results.																	2199-4536	2198-6053				JUL	2020	6	2					355	389		10.1007/s40747-020-00130-x		FEB 2020											
J								A deep granular network with adaptive unequal-length granulation strategy for long-term time series forecasting and its industrial applications	ARTIFICIAL INTELLIGENCE REVIEW										Time series; Deep learning; Adaptive granulation; Granular computing; Long-term forecasting	ECHO STATE NETWORK; NEURAL-NETWORKS; PREDICTION; FUZZY; MODEL; REPRESENTATIONS; MULTIVARIATE; OPTIMIZATION; CONSUMPTION; MACHINE	An accurate long-term forecasting for some time series in industrial production is substantially significant for improving the economic efficiency of industry enterprise. In this study, a granular computing (GrC)-based deep learning framework is proposed for long-term time series forecasting, which consists of two stages i.e., the adaptive data granulation and the GrC-based deep model construction. In the first stage, for automatically generating the basic information granules with unequal time span adaptively from data, a stacked sparse auto-encoders granulation network is designed, in which the starting and ending points of a granule are adaptively determined by setting a single neuron in the last hidden layer after multi-layer feature extraction. Then, the second stage sees a definition of a partially overlapping sub-block basis (POSB) matrix to extract the features of these granules, based on which a deep sparse coding feature decomposition-based long-term forecasting model is constructed to transform the unequal-length granules into a product of a POSB matrix and a coefficient matrix layer by layer to serve for forecasting. To verify the effectiveness of the proposed method, two synthetic datasets, two real-world datasets and two practical industrial datasets are employed. The experimental results demonstrate that the proposed method outperforms other data-driven ones on long-term time series forecasting, particularly in an industrial case.																	0269-2821	1573-7462				OCT	2020	53	7					5353	5381		10.1007/s10462-020-09822-9		FEB 2020											
J								Deep quantification down-plain-upsampling residual learning for single image super-resolution	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Super-resolution; Deep convolutional neural networks; U-Net; Residual learning		Deep convolutional neural networks have been widely used in single image super-resolution (SISR) with great success. However, the performance and efficiency of such models need to be improved for practical applications. In this paper, a novel deep quantification down-plain-upsampling (QDPU) network for SISR is proposed. In the framework, a down-plain-upsampling (DPU) residual block based on U-Net is firstly designed to reduce the computational cost by transforming the spatial scale of feature maps without sacrificing the reconstruction performance. Then, to better transmit low-level features to the reconstruction layer, we construct quantification skip-connection modules to integrate the outputs of the DPU residual blocks. Finally, QDPU is developed by stacking the DPU residual blocks with multiple skip-connections to reconstruct high-resolution images and reduce the computational burden. Quantitative and qualitative evaluations of the reconstruction results on four benchmark datasets show that the proposed method can achieve better performance compared with several state-of-the-art SISR methods.																	1868-8071	1868-808X				AUG	2020	11	8					1923	1937		10.1007/s13042-020-01083-w		FEB 2020											
J								Deep Neural Network Augmentation: Generating Faces for Affect Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION										Dimensional; Categorical affect; Valence; Arousal; Basic emotions; Facial affect synthesis; 4DFAB; Blendshape models; 3DMM fitting; DNNs; StarGAN; GANimation; Data augmentation; Affect recognition; Facial expression transfer	DATABASE	This paper presents a novel approach for synthesizing facial affect; either in terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness and surprise), or in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the emotion activation). The proposed approach accepts the following inputs:(i) a neutral 2D image of a person; (ii) a basic facial expression or a pair of valence-arousal (VA) emotional state descriptors to be generated, or a path of affect in the 2D VA space to be generated as an image sequence. In order to synthesize affect in terms of VA, for this person, 600,000 frames from the 4DFAB database were annotated. The affect synthesis is implemented by fitting a 3D Morphable Model on the neutral image, then deforming the reconstructed face and adding the inputted affect, and blending the new face with the given affect into the original image. Qualitative experiments illustrate the generation of realistic images, when the neutral image is sampled from fifteen well known lab-controlled or in-the-wild databases, including Aff-Wild, AffectNet, RAF-DB; comparisons with generative adversarial networks (GANs) show the higher quality achieved by the proposed approach. Then, quantitative experiments are conducted, in which the synthesized images are used for data augmentation in training deep neural networks to perform affect recognition over all databases; greatly improved performances are achieved when compared with state-of-the-art methods, as well as with GAN-based data augmentation, in all cases.																	0920-5691	1573-1405				MAY	2020	128	5					1455	1484		10.1007/s11263-020-01304-3		FEB 2020											
J								Feature construction as a bi-level optimization problem	NEURAL COMPUTING & APPLICATIONS										Feature construction; Data classification; Bi-level optimization; Evolutionary algorithms	WRAPPER EVOLUTIONARY APPROACH; MULTIPLE-FEATURE CONSTRUCTION; FEATURE-SELECTION; CLASSIFICATION	Feature selection and construction are important preprocessing techniques in data mining. They allow not only dimensionality reduction but also classification accuracy and efficiency improvement. While feature selection consists in selecting a subset of relevant feature from the original feature set, feature construction corresponds to the generation of new high-level features, called constructed features, where each one of them is a combination of a subset of original features. Based on these definitions, feature construction could be seen as a bi-level optimization problem where the feature subset should be defined first and then the corresponding (near) optimal combination of the selected features should be found. Motivated by this observation, we propose, in this paper, a bi-level evolutionary approach for feature construction. The basic idea of our algorithm, named bi-level feature construction genetic algorithm (BFC-GA), is to evolve an upper-level population for the task of feature selection, while optimizing the feature combinations at the lower level by evolving a follower population. It is worth noting that for each upper-level individual (feature subset), a whole lower-level population is optimized to find the corresponding (near) optimal feature combination (constructed feature). In this way, BFC-GA would be able to output a set of optimized constructed features that could be very informative to the considered classifier. A detailed experimental study has been conducted on a set of commonly used datasets with varying dimensions. The statistical analysis of the obtained results shows the competitiveness and the outperformance of our bi-level feature construction approach with respect to many state-of-the-art algorithms.																	0941-0643	1433-3058				SEP	2020	32	17					13783	13804		10.1007/s00521-020-04784-z		FEB 2020											
J								A weighted SVM ensemble predictor based on AdaBoost for blast furnace Ironmaking process	APPLIED INTELLIGENCE										Weighted support vector machine; AdaBoost; Imbalanced classification; Blast furnace ironmaking process	EXTREME LEARNING-MACHINE; SUPPORT VECTOR MACHINE; SILICON CONTENT; IDENTIFICATION; MODEL	As one of the most complex industrial reactors, there remain some urgent issues for blast furnace (BF), such as BF automation, prediction of the inner thermal state, etc. In this work, the prediction of BF inner thermal state, which is represented by the silicon content in BF hot metal, is taken as an imbalanced binary classification problem and a Weighted Support Vector Machine (W-SVM) ensemble predictor based on AdaBoost is presented for the prediction task. Compared with the traditional W-SVM algorithm, the proposed predictor dynamically adjusts the weight distribution of training samples according to the performance of weak classifier, in this way to mine information lurked in the samples. The prediction can act as a guide to aid the operators for judging the thermal state of BF in time. Experiments results on five benchmark datasets and two real-world BFs datasets demonstrate the efficiency of the proposed W-SVM ensemble predictor.																	0924-669X	1573-7497				JUL	2020	50	7					1997	2008		10.1007/s10489-020-01662-y		FEB 2020											
J								Digitally fabricated aesthetic enhancements and enrichments	AI & SOCIETY										Bio inspired design; Aesthetic enhancements; Digital fabrication; Art and technology		In this paper, we explore digitally fabricated aesthetic enhancements and modifications of the body as well as digitally fabricated fauna habitats. We will address how we utilize speculative works through our bio inspired digitally fabricated designs via two of our most recent projects: {skin} D.E.E.P. and in silico et in situ. Through these two projects we explore cultural implications of the intersection of technology and biologically inspired art/design. Technology has provided an ever increasing amount of data which has facilitated the way data can be used in aesthetic explorations. Technology can redefine the human experience by enhancing our senses, experiences, and forms of expression. In our work {skin} D.E.E.P., we explore these ideas via augmentation through wearables and biomimetic expressions on the skin through digitally fabricated prosthetics. {skin} D.E.E.P. employs temporary biomimetic skin patterns via wearable 3D printed exoskeletons. The first series of this work mimics the patterns and textures of animal skin via ephemeral impressions onto human skin. Taking inspiration from shedding snakeskin, we shed the outer layer by removing the 3D printed prosthesis. The epidermis retains the negative imprint of the prosthetic mimicking the look of serpent skin. The dermis reestablishes the smooth form of the human skin as it heals itself within the hour, erasing the ephemeral imprint, symbolically representing rebirth and renewal. Investigating the ephemeral aesthetic of textured patterns on human skin will not by any means provide a solution to an engineering problem, yet acts as a larger philosophical basis to conduct a discussion about the possibilities of blending bio inspired designs. The manifestation of data into physical form occurs via digital fabrication in our most recent series in silico et in situ: fauna habitats in the age of voxels, where we aim to create a new ecosystem for animals and to potentially augment their habitat. The habitats include pollinator waterers, spider habitats (in form of glow up rings) and turtle basking platforms. This project pushes forward with a new piece-the development of a 20-foot avian sculpture that will solely exist to support the habitat of the Chimney Swift species. The Chimney Swift is a near-threatened bird species in the US. in silico et in situ investigates the interplay of art, nature and technology by contextualizing sculptural works and placing these sculptures into nature. The project serves as a breeding ground for a larger dialogue about nature, culture, human intervention, design art and technology. Providing the artwork in the public sphere, barrier free in nature, fosters this discourse.																	0951-5666	1435-5655															10.1007/s00146-020-00938-8		FEB 2020											
J								Artificial wisdom: a philosophical framework	AI & SOCIETY										Practical wisdom; Artificial narrow intelligence; Artificial general intelligence; Specificationism; Well-being		Human excellences such as intelligence, morality, and consciousness are investigated by philosophers as well as artificial intelligence researchers. One excellence that has not been widely discussed by AI researchers is practical wisdom, the highest human excellence, or the highest, seventh, stage in Dreyfus's model of skill acquisition. In this paper, I explain why artificial wisdom matters and how artificial wisdom is possible (in principle and in practice) by responding to two philosophical challenges for building artificial wisdom systems. The result is a conceptual framework that guides future research on creating artificial wisdom.																	0951-5666	1435-5655															10.1007/s00146-020-00949-5		FEB 2020											
J								Hidden Markov Model Approach for Software Reliability Estimation with Logic Error	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Hidden Markov model (HMM); reliability; logic error; safety critical; software failure	PREDICTION; PROPAGATION	To ensure the safe operation of any software controlled critical systems, quality factors like reliability and safety are given utmost importance. In this paper, we have chosen to analyze the impact of logic error that is one of the contributors to the above factors. In view of this, we propose a novel framework based on a data driven approach known as software failure estimation with logic error (SFELE). Here, the probabilistic nature of software error is explored by observing the operation of a safety critical system by injecting logic fault. The occurrence of error, its propagations and transformations are analyzed from its inception to end of its execution cycle through the hidden Markov model (HMM) technique. We found that the proposed framework SFELE supports in labeling and quantifying the behavioral properties of selected errors in a safety critical system while traversing across its system components in addition to reliability estimation of the system. Our attempt at the design level can help the design engineers to improve their system quality in a cost-effective manner.																	1476-8186	1751-8520				APR	2020	17	2					305	320		10.1007/s11633-019-1214-7		FEB 2020											
J								Evaluation of the Performance of Search and Rescue Robots Using T-spherical Fuzzy Hamacher Aggregation Operators	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Hamacher aggregation operators; Multi-attribute decision-making; Picture fuzzy sets; Spherical fuzzy sets; T-spherical fuzzy sets	SETS	Multi-attribute decision-making approach is a widely used algorithm that needs some aggregation tools and several such aggregation operators have been developed in past decades to serve the purpose. Hamacher aggregation operator is one such operator which is based on Hamacher t-norm and t-conorm. It is observed that the Hamacher aggregation operators of intuitionistic fuzzy set, Pythagorean fuzzy set and that of picture fuzzy set has some limitations in their applicability. To serve the purpose, in this paper, some Hamacher aggregation operators based on T-spherical fuzzy numbers are introduced. The concepts of T-spherical fuzzy Hamacher-weighted averaging and T-spherical fuzzy Hamacher-weighted geometric aggregation operators are proposed which described four aspects of human opinion including yes, no, abstinence and refusal with no limitations. Such type of aggregation operators efficiently describes the cases that left unsolved by the existing aggregation operators. The validity of the proposed aggregation operators is examined, and some basic properties are discussed. The proposed new Hamacher aggregation operators are used to analyze the performance of search and rescue robots using a multi-attribute decision-making approach as their performance in an emergency is eminent. The proposed Hamacher aggregation operators have two variable parameters, namely q and gamma which affects the decision-making process and their sensitivity towards decision-making results is analyzed. A comparative analysis of the results obtained using proposed Hamacher aggregation operators in view of the variable parameters q and gamma is established to discuss any advantages or disadvantages.																	1562-2479	2199-3211				MAR	2020	22	2			SI		570	582		10.1007/s40815-020-00803-2		FEB 2020											
J								Pricing strategy and system performance in a cloud-based manufacturing system built on blockchain technology	JOURNAL OF INTELLIGENT MANUFACTURING										Blockchain; Cloud manufacturing; Additive manufacturing; Game pricing; Fuzzy algorithm	ALGORITHM; SERVICE; DEMAND; MODEL	Blockchain technology has been enjoying rapid development in numerous areas where trust, safety, and global payments are deemed important. In this research, we introduce a framework that integrates blockchain technology with the emerging concept of cloud manufacturing. It has the potential to dramatically improve the feasibility and applicability of cloud manufacturing. Our focus is on the pricing strategies for service providers in bidding for jobs in such a system. A scenario is developed based on cloud additive manufacturing services. A game theoretic approach for pricing simulation is proposed in the research, and a fuzzy algorithm is employed in the pricing decision. Two pricing policies are considered: one is that the information of players is unknown to each other, and the other is that the information of players who are within a limit of Euclidean space is known to each other. The effects of system load and the weight coefficients in KNN recommendation algorithm are also studied. The results show that the pricing policy has significant effect on revenue generation for providers, and the increase of system load helps to grow revenue for providers only when the machine utilization is low and job waiting time is short. In addition, the system seems to be less sensitive to the change of KNN weights in terms of individual provider and system revenues.																	0956-5515	1572-8145															10.1007/s10845-020-01548-3		FEB 2020											
J								Applications of fuzzy rho-ideals in rho-algebras	SOFT COMPUTING										Fuzzy BCK-algebra; Fuzzy d-algebra; Fuzzy d-ideal; Pre-image; Strongest fuzzy relation		Some new notions of fuzzy algebras like fuzzy.-subalgebra, fuzzy.-ideal, and fuzzy.-ideal are introduced in this work. Moreover, the relationships between our new notions and other types of fuzzy algebras like fuzzy d-subalgebra, fuzzy d-ideal, fuzzy BCK-subalgebra, and fuzzy BCK-ideal are investigated. Also, some basic characterizations of fuzzy.-ideal with their applications on images, Cartesian product, upper level, characteristic function, and strongest fuzzy relation are studied and discussed in this paper. Furthermore, several examples are presented to expound our notions in this work.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13997	14004		10.1007/s00500-020-04773-3		FEB 2020											
