PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Interestingness Improvement of Face Images by Learning Visual Saliency	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										computer vision; visual attention; face image; personal characteristics; interestingness	MEMORABILITY; ATTENTION	Connecting features of face images with the interestingness of a face may assist in a range of applications such as intelligent visual human-machine communication. To enable the connection, we use interestingness and image features in combination with machine learning techniques. In this paper, we use visual saliency of face images as learning features to classify the interestingness of the images. Applying multiple saliency detection techniques specifically to objects in the images allows us to create a database of saliency-based features. Consistent estimation of facial interestingness and using multiple saliency methods contribute to estimate, and exclusively, to modify the interestingness of the image. To investigate interestingness - one of the personal characteristics in a face image, a large benchmark face database is tested using our method. Taken together, the method may advance prospects for further research incorporating other personal characteristics and visual attention related to face images.																	1343-0130	1883-8014				SEP	2020	24	5					630	637															
J								Projection with Gaussian Kernel for Person Re-Identification	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										person re-identification; manifold; Gaussian kernel		Person re-identification (ReID), the task of associating the detected images of a person as he/she moves in a non-overlapping camera network, is faced with different challenges including variations in the illumination, view-point and occlusion. To ensure good performance for person ReID, the state-of-the-art methods have leveraged different characteristics for person representation. As a result, a high-dimensional feature vector is extracted and used in the person matching step. However, each feature plays a specific role for distinguishing one person from the others. This paper proposes a method for person ReID wherein the correspondences between descriptors in high-dimensional space can be achieved via explicit feature selection and appropriate projection with a Gaussian kernel. The advantage of the proposed method is that it allows simultaneous matching of the descriptors while preserving the local geometry of the manifolds. Different experiments were conducted on both single-shot and multi-shot person ReID datasets. The experimental results demonstrates that the proposed method outperforms the state-of-the-art methods.																	1343-0130	1883-8014				SEP	2020	24	5					638	647															
J								Feature Analysis for Imbalanced Learning	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										feature analysis; class imbalance; Bayesian inference	SMOTE	Based on the results of artificial samples generated in the minority class and through the label regulation of the neighbor samples of the majority class, the precision of the classification prediction for imbalanced learning has clearly been enhanced. This article presents a unified solution combining learning factors to improve the learning performance. The proposed method solves this imbalance through a feature selection incorporating the generation of artificial samples and label regulation. A probabilistic representation is used for all aspects of learning: class, sample, and feature. A Bayesian inference is applied to the learning model to interpret the imbalance occurring in the training data and to describe solutions for recovering the balance. We show that the generation of artificial samples is sample based approach and label regulation is class based approach. We discovered that feature selection achieves surprisingly good results when combined with a sample- or class-based solution.																	1343-0130	1883-8014				SEP	2020	24	5					648	655															
J								Building a Fuzzy System for Pulse Based Disease Diagnosis and Acupuncture Therapy	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										pulse-based diagnosis; expert systems; acupuncture therapy; fuzzy logic; traditional medicine	LOGIC	Pulse-based disease diagnosis and acupuncture therapy are the key components of traditional Oriental medicine. This study aims to model the thinking of medical doctors with regard to their use of pulse-based diagnosis and acupuncture therapy. This paper focuses on a fuzzy inference and knowledge base, which are the main components of the system for pulse based disease diagnosis and acupuncture therapy. The input of the system is the pulse symptoms of the patient with fuzzy degrees, whereas the output is the disease diagnosis and acupuncture therapy prescription. In this system, the knowledge base consists of nearly 1,200 rules for diagnosis and treatment. An evaluation of a group of traditional medical doctors indicates that the results of the newly proposed system are in good accordance with those of doctors practicing traditional medicine. This approach leads to better results than previous approaches because it uses fuzzy logic, which is an appropriate tool here because most entities in traditional medicine are fuzzy in nature. The system of pulse-based disease diagnosis and acupuncture therapy can mimic the thinking of traditional practitioners, and it can be a "good teacher" for medical students who want to learn traditional Vietnamese medicine.																	1343-0130	1883-8014				SEP	2020	24	5					656	661															
J								Mixed Duopoly Stochastic Sales Model with Advertising and Experience Gains for the Public and Foreign Competitors	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										mixed duopoly; stochastic control model; experience gaining; advertising	RESEARCH-AND-DEVELOPMENT; PRIVATIZATION; SPILLOVERS; PERFORMANCE	In this research, we propose a stochastic model with the finite horizon of time for sales competition between the state-owned company and private (foreign) competitor. We assume that the foreign company objective function is to maximize revenues and the state-owned agent is concerned about welfare maximization. There are many stochastic models for sales, but what is new in our case is that we assume mixed oligopoly and have different types of firms: private and state owned. They have somewhat different objective functions. As a control variable, we take the advertisement expenses of the private firm. Sale bursts rate depends and the advertisement expenditure and experience stock gained. For the public firm, we assume that advertising efforts are fixed. It means that the optimal control is to maximize private firm revenues taking into account possible uncertainties of stochastic profit flow using Bellman's optimality condition. We can find out that the Advertisement-Experience (AE) efforts of the private firm are increasing if sales are increasing. Next, the AE might decrease if the experience level of the private firm increases and we have a sales burst. To optimize the governmental policies, we check for optimal AE effort of the public firm so the social welfare achieves the maximum value.																	1343-0130	1883-8014				SEP	2020	24	5					662	667															
J								Adaptive Personalized Multiple Machine Learning Architecture for Estimating Human Emotional States	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										affective computing; human robot interaction; non-verbal; multi-modal learning; personal modeling		Robots have the potential to facilitate the future education of all generations, particularly children. However, existing robots are limited in their ability to automatically perceive and respond to a human emotional states. We hypothesize that these sophisticated models suffer from individual differences in human personality. Therefore, we proposed a multi-characteristic model architecture that combines personalized machine learning models and utilizes the prediction score of each model. This architecture is formed with reference to an ensemble machine learning architecture. In this study, we presented a method for calculating the weighted average in a multi-characteristic architecture by using the similarities between a new sample and the trained characteristics. We estimated the degree of confidence during a communication as a human internal state. Empirical results demonstrate that using the multi-model training of each person's information to account for individual differences provides improvements over a traditional machine learning system and insight into dealing with various individual differences.																	1343-0130	1883-8014				SEP	2020	24	5					668	675															
J								Visualization Method Corresponding to Regression Problems and Its Application to Deep Learning-Based Gaze Estimation Model	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										CNN; eye tracking; Grad-CAM; regression problem		The human gaze contains substantial personal information and can be extensively employed in several applications if its relevant factors can be accurately measured. Further, several fields could be substantially innovated if the gaze could be analyzed using popular and familiar smart devices. Deep learning-based methods are robust, making them crucial for gaze estimation on smart devices. However, because internal functions in deep learning are black boxes, deep learning systems often make estimations for unclear reasons. In this paper, we propose a visualization method corresponding to a regression problem to solve the black box problem of the deep learning-based gaze estimation model. The proposed visualization method can clarify which region of an image contributes to deep learning-based gaze estimation. We visualized the gaze estimationmodel proposed by a research group at the Massachusetts Institute of Technology. The accuracy of the estimation was low, even when the facial features important for gaze estimation were recognized correctly. The effectiveness of the proposed method was further determined through quantitative evaluation using the area over the MoRF perturbation curve (AOPC).																	1343-0130	1883-8014				SEP	2020	24	5					676	684															
J								Smartphone Naive Bayes Human Activity Recognition Using Personalized Datasets	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										tilt angles; signal magnitude vector; real-time; Gaussian distribution function; personalized dataset	PHYSICAL-ACTIVITY; STROKE; WOMEN	Recognizing human activity in real time with a limited dataset is possible on a resource-constrained device. However, most classification algorithms such as Support Vector Machines, C4.5, and K Nearest Neighbor require a large dataset to accurately predict human activities. In this paper, we present a novel real-time human activity recognition model based on Gaussian Naive Bayes (GNB) algorithm using a personalized JavaScript object notation dataset extracted from the publicly available Physical Activity Monitoring for Aging People dataset and University of Southern California Human Activity dataset. With the proposed method, the personalized JSON training dataset is extracted and compressed into a 12 x 8 multi dimensional array of the time-domain features extracted using a signal magnitude vector and tilt angles from tri-axial accelerometer sensor data. The algorithm is implemented on the Android platform using the Cordova cross-platform framework with HTML5 and JavaScript. Leave-one-activity-out cross validation is implemented as a testTrainer() function, the results of which are presented using a confusion matrix. The testTrainer() function leaves category K as the testing subset and the remaining K-1 as the training dataset to validate the proposed GNB algorithm. The proposed model is inexpensive in terms of memory and computational power owing to the use of a compressed small training dataset. Each K category was repeated five times and the algorithm consistently produced the same result for each test. The result of the simulation using the tilted angle features shows overall precision, recall, F-measure, and accuracy rates of 90%, 99.6%, 94.18%, and 89.51% respectively, in comparison to rates of 36.9%, 75%, 42%, and 36.9% when the signal magnitude vector features were used. The results of the simulations confirmed and proved that when using the tilt angle dataset, the GNB algorithm is superior to Support Vector Machines, C4.5, and K Nearest Neighbor algorithms.																	1343-0130	1883-8014				SEP	2020	24	5					685	702															
J								Artificial Intelligence Research Community and Associations in Poland	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES												In last years Artificial Intelligence presented a tremendous progress by offering a variety of novel methods, tools and their spectacular applications. Besides showing scientific breakthroughs it attracted interest both of the general public and industry. It also opened heated debates on the impact of Artificial Intelligence on changing the economy and society. Having in mind this international landscape, in this short paper we discuss the Polish AI research community, some of its main achievements, opportunities and limitations. We put this discussion in the context of the current developments in the international AI community. Moreover, we refer to activities of Polish scientific associations and their initiative of founding Polish Alliance for the Development of Artificial Intelligence (PP-RAI). Finally two last editions of PP-RAI joint conferences are summarized.																	0867-6356	2300-3405				SEP	2020	45	3					159	177		10.2478/fcds-2020-0009													
J								Transfer Learning Methods as a New Approach in Computer Vision Tasks with Small Datasets	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										Deep neural networks; Transfer learning; Signal processing; Image analysis; Anomaly detection		Deep learning methods, used in machine vision challenges, often face the problem of the amount and quality of data. To address this issue, we investigate the transfer learning method. In this study, we briefly describe the idea and introduce two main strategies of transfer learning. We also present the widely-used neural network models, that in recent years performed best in ImageNet classification challenges. Furthermore, we shortly describe three different experiments from computer vision field, that confirm the developed algorithms ability to classify images with overall accuracy 87.2-95%. Achieved numbers are state-of-the-art results in melanoma thickness prediction, anomaly detection and Clostridium difficile cytotoxicity classification problems.																	0867-6356	2300-3405				SEP	2020	45	3					179	193		10.2478/fcds-2020-0010													
J								Mining Cardinality Restrictions in OWL	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										Semantic Web; ontology learning; frequency estimation; kernel density estimation; cardinality restrictions	FRAMEWORK; ONTOLOGY	We present an approach to mine cardinality restriction axioms from an existing knowledge graph, in order to extend an ontology describing the graph. We compare frequency estimation with kernel density estimation as approaches to obtain the cardinalities in restrictions. We also propose numerous strategies for filtering obtained axioms in order to make them more available for the ontology engineer. We report the results of experimental evaluation on DBpedia 2016-10 and show that using kernel density estimation to compute the cardinalities in cardinality restrictions yields more robust results that using frequency estimation. We also show that while filtering is of limited usability for minimum cardinality restrictions, it is much more important for maximum cardinality restrictions. The presented findings can be used to extend existing ontology engineering tools in order to support ontology construction and enable more efficient creation of knowledge-intensive artificial intelligence systems.																	0867-6356	2300-3405				SEP	2020	45	3					195	216		10.2478/fcds-2020-0011													
J								Application of Machine Learning Algorithms for Traffic Forecasting in Dynamic Optical Networks with Service Function Chains	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										Dynamic Optical Networks; Traffic Prediction; Service Function Chaining; Machine Learning		Knowledge about future optical network traffic can be beneficial for network operators in terms of decreasing an operational cost due to efficient resource management. Machine Learning (ML) algorithms can be employed for forecasting traffic with high accuracy. In this paper we describe a methodology for predicting traffic in a dynamic optical network with service function chains (SFC). We assume that SFC is based on the Network Function Virtualization (NFV) paradigm. Moreover, other type of traffic, i.e. regular traffic, can also occur in the network. As a proof of effectiveness of our methodology we present and discuss numerical results of experiments run on three benchmark networks. We examine six ML classifiers. Our research shows that it is possible to predict a future traffic in an optical network, where SFC can be distinguished. However, there is no one universal classifier that can be used for each network. Choice of an ML algorithm should be done based on a network traffic characteristics analysis.																	0867-6356	2300-3405				SEP	2020	45	3					217	232		10.2478/fcds-2020-0012													
J								Analysis of statistical model-based optimization enhancements in Generalized Self-Adapting Particle Swarm Optimization framework	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										Particle Swarm Optimization; global optimization; metaheuristic	EVOLUTION	This paper presents characteristics of model-based optimization methods utilized within the Generalized Self-Adapting Particle Swarm Optimization (GA-PSO) - a hybrid global optimization framework proposed by the authors. GAPSO has been designed as a generalization of a Particle Swarm Optimization (PSO) algorithm on the foundations of a large degree of independence of individual particles. GAPSO serves as a platform for studying optimization algorithms in the context of the following research hypothesis: (1) it is possible to improve the performance of an optimization algorithm through utilization of more function samples than standard PSO sample-based memory, (2) combining specialized sampling methods (i.e. PSO, Differential Evolution, model-based optimization) will result in a better algorithm performance than using each of them separately. The inclusion of model-based enhancements resulted in the necessity of extending the GAPSO framework by means of an external samples memory - this enhanced model is referred to as M-GAPSO in the paper. We investigate the features of two model-based optimizers: one utilizing a quadratic function and the other one utilizing a polynomial function. We analyze the conditions under which those model-based approaches provide an effective sampling strategy. Proposed model-based optimizers are evaluated on the functions from the COCO BBOB benchmark set.																	0867-6356	2300-3405				SEP	2020	45	3					233	254		10.2478/fcds-2020-0013													
J								Improving coordination in small-scale multi-agent deep reinforcement learning through memory-driven communication	MACHINE LEARNING										Reinforcement learning; Multi-agent systems; Artificial neural networks	EMERGENCE; WORLD	Deep reinforcement learning algorithms have recently been used to train multiple interacting agents in a centralised manner whilst keeping their execution decentralised. When the agents can only acquire partial observations and are faced with tasks requiring coordination and synchronisation skills, inter-agent communication plays an essential role. In this work, we propose a framework for multi-agent training using deep deterministic policy gradients that enables concurrent, end-to-end learning of an explicit communication protocol through a memory device. During training, the agents learn to perform read and write operations enabling them to infer a shared representation of the world. We empirically demonstrate that concurrent learning of the communication device and individual policies can improve inter-agent coordination and performance in small-scale systems. Our experimental results show that the proposed method achieves superior performance in scenarios with up to six agents. We illustrate how different communication patterns can emerge on six different tasks of increasing complexity. Furthermore, we study the effects of corrupting the communication channel, provide a visualisation of the time-varying memory content as the underlying task is being solved and validate the building blocks of the proposed memory device through ablation studies.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1727	1747		10.1007/s10994-019-05864-5													
J								Detection outliers on internet of things using big data technology	EGYPTIAN INFORMATICS JOURNAL										Internet of things; IoT; Big data; Data quality; Outliers Detection; DBSCAN; RDDs	CLUSTERING-ALGORITHM; MR	Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.																	1110-8665	2090-4754				SEP	2020	21	3					131	138		10.1016/j.eij.2019.12.001													
J								The impacts of social media on University students in Iraq	EGYPTIAN INFORMATICS JOURNAL										Social media; University students; Education; Business; Political		The rapid increase in the era of the technological revolution and the Internet, especially the social media, have created a new reality in the daily life of the societies in general and of the university students in particular, so this new reality without any doubt imposes on us the general effects of this increasing use of social media has affected all areas and societies The effects are positive and negative. These social means have become a vast space for exchanging ideas, making new friends, proposals, sources of information, business and e-shopping. In this study, we have decided to shed light on the academic, political and economic effects of the study by comparing them to the general effects of social media by looking at the demographic variables of Iraqi university students. Three universities (Kerbela University in the Middle Euphrates, UOITC University in the capital Baghdad and Tikrit University in western Iraq) where we considered spatial, cultural and social differences. Data collected through a survey consisting of four categories, General Influences, Academic Influences, Political Influences and Influences Business (distributed over 40 questions) We tried to cover most of the students' common uses of social media and their impact on them. The questionnaire was distributed through 100 questionnaires to each university. The total number of participants was (201) distributed on (77) participants from Karbala University, (50) participants from Tikrit University and (74) participants from the University of Information and Communication Technology (UOITC). (C) 2020 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.																	1110-8665	2090-4754				SEP	2020	21	3					139	144		10.1016/j.eij.2019.12.003													
J								Extractive multi-document text summarization based on graph independent sets	EGYPTIAN INFORMATICS JOURNAL										Graph independent set; Graph-based document summarization; Generic document summarization; Extractive text summarization; Multi document text summarization		We propose a novel methodology for extractive, generic summarization of text documents. The Maximum Independent Set, which has not been used previously in any summarization study, has been utilized within the context of this study. In addition, a text processing tool, which we named KUSH, is suggested in order to preserve the semantic cohesion between sentences in the representation stage of introductory texts. Our anticipation was that the set of sentences corresponding to the nodes in the independent set should be excluded from the summary. Based on this anticipation, the nodes forming the Independent Set on the graphs are identified and removed from the graph. Thus, prior to quantification of the effect of the nodes on the global graph, a limitation is applied on the documents to be summarized. This limitation prevents repetition of word groups to be included in the summary. Performance of the proposed approach on the Document Understanding Conference (DUC-2002 and DUC-2004) datasets was calculated using ROUGE evaluation metrics. The developed model achieved a 0.38072 ROUGE performance value for 100-word summaries, 0.51954 for 200-word summaries, and 0.59208 for 400-word summaries. The values reported throughout the experimental processes of the study reveal the contribution of this innovative method. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.																	1110-8665	2090-4754				SEP	2020	21	3					145	157		10.1016/j.eij.2019.12.002													
J								Gender identification for Egyptian Arabic dialect in twitter using deep learning models	EGYPTIAN INFORMATICS JOURNAL										Gender identification; Egyptian Arabic text classification; Deep learning; Natural language processing; Social Media analysis and mining		Although the number of Arabic language writers in social media is increasing, the research work targeting Author Profiling (AP) is at the initial development phase. This paper investigates Gender Identification (GI) (male or female) of authors posting Egyptian dialect tweets using Neural Networks (NN) models. Various architectures of NN are explored with extensive parameters' selection such as simple Artificial Neural Network (ANN), Convolutional Neural Network (CNN), Long-Short Term Memory (LSTM), Convolutional Bidirectional Long-Short Term Memory (C-Bi-LSTM) and Convolutional Bidirectional Gated Recurrent Units (C-Bi-GRU) NN which is tuned for the GI problem at hand. The best acquired GI accuracy using C-Bi-GRU multichannel model is 91.37%. It is worth noting that the presence of the bidirectional layer as well as the convolutional layer in the NN models has significantly enhanced the GI accuracy. (C) 2020 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.																	1110-8665	2090-4754				SEP	2020	21	3					159	167		10.1016/j.eij.2020.04.001													
J								Formal verification for a PMQTT protocol	EGYPTIAN INFORMATICS JOURNAL										IoT; MQTT; Elliptic Curve Digital Signature Algorithm; Elliptic Curve Diffie Hellman; Formal verification; ProVerif		The future of Internet of Things (IoT) foresees a world of interconnected people with every physical object in a seamless manner. The security related aspects for the IoT world are still an open field of discussion and research. The Message Queue Telemetry Transport (MQTT) application layer protocol is widely used in IoT networks. Since, MQTT standard has no mandatory requirements regarding the security services, therefore, manipulating the security related issues is different in MQTT platforms. This paper proposes a novel security protocol. It is the Protected Message Queue Telemetry Transport (PMQTT) protocol which is based on MQTT with added cryptographic primitives to offer security services for IoT systems. Moreover, a formal verification for a PMQTT protocol is conducted using the ProVerif cryptographic automated verifier tool to prove that the PMQTT protocol satisfies the intended security properties. (C) 2020 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.																	1110-8665	2090-4754				SEP	2020	21	3					169	182		10.1016/j.eij.2020.01.001													
J								GDD: Geometrical driven diagnosis based on biomedical data	EGYPTIAN INFORMATICS JOURNAL										GDD; Medical diagnosis; Classification; Machine learning; Diabetes; Big data; Bioinformatics		Modern medical diagnosis heavily rely on bio-medical and clinical data. Machine learning algorithms have proven effectiveness in mining this data to provide an aid to the physicians in supporting their decisions. In response, machine learning based approaches were developed to address this problem. These approaches vary in terms of effectiveness and computational cost. Attention has been paid towards non-communicable diseases as they are very common and have life threatening risk factors. The diagnosis of diabetes or breast cancer can be considered a binary classification problem. This paper proposes a new machine learning based algorithm, Geometrical Driven Diagnosis (GDD), to diagnose diabetes and breast cancer with accuracy up to 99.96% and 95.8% respectively. (C) 2020 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.																	1110-8665	2090-4754				SEP	2020	21	3					183	190		10.1016/j.eij.2020.04.002													
J								Anytime Frequent Itemset Mining of Transactional Data Streams	BIG DATA RESEARCH										Data streams; Frequent itemset mining; Anytime frequent itemset mining	SIZE SLIDING WINDOW; BIG DATA; ALGORITHM; PATTERNS	Mining frequent itemsets from transactional data streams has become very essential in today's world with many applications such as stock market analysis, retail chain analysis, web log analysis, etc. Various algorithms have been proposed to efficiently mine single-port and multi-port transactional streams within the constraints of limited time and memory. However, all of them are budget algorithms, i.e., they are not capable of handling varying inter-arrival rate of transactions and high speed streams. They are constrained by a maximum limit to the inter-arrival rate of transactions, beyond which they fail to process. Also, these algorithms are not capable of giving immediate mining results, even with compromised accuracy if required. The above two properties characterize an anytime algorithm. We propose ANYFI, which is the first anytime frequent itemset mining algorithm for data streams. ANYFI uses a novel data structure BFI-FOREST, which is capable of handling transactions arriving at variable rate. It maintains itemsets in BFI-forest in such a way that it can give a mining result almost immediately when the time allowance to mine is very less and can refine its accuracy with increase in time allowance. We also propose MPANYFI which extends ANYFI into a parallel framework for anytime frequent itemset mining of multi-port data streams over commodity clusters. It uses ANYFI at each computing node of the cluster. Our extensive experimental analysis shows that ANYFI can handle high stream speeds close to 60,000 trans/sec with recall close to 100%. They also show the efficiency of MPANYFI. (C) 2020 Elsevier Inc. All rights reserved.																	2214-5796					SEP	2020	21								100146	10.1016/j.bdr.2020.100146													
J								Data-Driven Computational Social Science: A Survey	BIG DATA RESEARCH										Computational social science; Human dynamics; Individual; Collective; Relationship; Machine learning	COMMUNITY STRUCTURE; TRAFFIC CONGESTION; SCHOLARLY DATA; EVOLUTION; NETWORKS; PREDICTION; MODEL; PREDICTABILITY; ORGANIZATION; COOPERATION	Social science concerns issues on individuals, relationships, and the whole society. The complexity of research topics in social science makes it the amalgamation of multiple disciplines, such as economics, political science, and sociology, etc. For centuries, scientists have conducted many studies to understand the mechanisms of the society. However, due to the limitations of traditional research methods, there exist many critical social issues to be explored. To solve those issues, computational social science emerges due to the rapid advancements of computation technologies and the profound studies on social science. With the aids of the advanced research techniques, various kinds of data from diverse areas can be acquired nowadays, and they can help us look into social problems with a new eye. As a result, utilizing various data to reveal issues derived from computational social science area has attracted more and more attentions. In this paper, to the best of our knowledge, we present a survey on datadriven computational social science for the first time which primarily focuses on reviewing application domains involving human dynamics. The state-of-the-art research on human dynamics is reviewed from three aspects: individuals, relationships, and collectives. Specifically, the research methodologies used to address research challenges in aforementioned application domains are summarized. In addition, some important open challenges with respect to both emerging research topics and research methods are discussed. (C) 2020 Elsevier Inc. All rights reserved.																	2214-5796					SEP	2020	21								100145	10.1016/j.bdr.2020.100145													
J								Combining instance and feature neighbours for extreme multi-label classification	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Extreme multi-label classification; Item-based collaborative filtering; k-nearest neighbours; Top-kqueries; Information retrieval	STRATEGIES	Extreme multi-label classification problems occur in different applications such as prediction of tags or advertisements. We propose a new algorithm that predicts labels using a linear ensemble of labels from instance- and feature-based nearest neighbours. In the feature-based nearest neighbours method, we precompute a matrix containing the similarities between each feature and label. For the instance-based nearest neighbourhood, we create an algorithm that uses an inverted index to compute cosine similarity on sparse datasets efficiently. We extend this baseline with a new top-kquery algorithm that combines term-at-a-time and document-at-a-time traversal with tighter pruning based on a partition of the dataset. On ten real-world datasets, we find that our method outperforms state-of-the-art methods such as multi-labelk-nearest neighbours, instance-based logistic regression, binary relevance with support vector machines and FastXml on different evaluation metrics. We also find that our algorithm is orders of magnitude faster than these baseline algorithms on sparse datasets and requires less than 20 ms per instance to predict labels for extreme datasets without the need for expensive hardware.																	2364-415X	2364-4168				SEP	2020	10	3					215	231		10.1007/s41060-020-00209-1													
J								Clustering of mixed-type data considering concept hierarchies: problem specification and algorithm	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Mixed-type data; Information-theoretic clustering		Most clustering algorithms have been designed only for pure numerical or pure categorical data sets, while nowadays many applications generate mixed data. It raises the question how to integrate various types of attributes so that one could efficiently group objects without loss of information. It is already well understood that a simple conversion of categorical attributes into a numerical domain is not sufficient since relationships between values such as a certain order are artificially introduced. Leveraging the natural conceptual hierarchy among categorical information, concept trees summarize the categorical attributes. In this paper, we introduce the algorithmClicoT(CLustering mixed-type dataIncludingCOnceptTrees) as reported by Behzadi et al. (Advances in Knowledge Discovery and Data Mining, Springer, Cham, 2019) which is based on the minimum description length principle. Profiting of the conceptual hierarchies, ClicoT integrates categorical and numerical attributes by means of a MDL-based objective function. The result of ClicoT is well interpretable since concept trees provide insights into categorical data. Extensive experiments on synthetic and real data sets illustrate that ClicoT is noise-robust and yields well-interpretable results in a short runtime. Moreover, we investigate the impact of concept hierarchies as well as various data characteristics in this paper.																	2364-415X	2364-4168				SEP	2020	10	3					233	248		10.1007/s41060-020-00216-2													
J								A fast scalable distributed kriging algorithm using Spark framework	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Linear algebra; Matrix multiplication; Strassen's algorithm; Apache Spark	PRECIPITATION; INTERPOLATION; TEMPERATURE; CREATION; DATASETS; NETWORK; MAXIMUM	Environmental and climate models used for weather prediction require evenly spaced meteorological datasets at a very high spatial and temporal resolution to facilitate the analysis of recent climatic changes. However, due to the small number of weather stations available, often the data collected from them are scattered and inadequate for such model creation. For this reason, very high-resolution gridded meteorological surface is developed by interpolating the available scattered data points to fulfill the need of various ecological and climatic applications. Among various interpolation techniques, Ordinary Kriging (OK) is one of the most popular and widely used gridding methodologies with a sound statistical basis providing a possibility to obtain highly accurate results. However, OK interpolation on large unevenly spaced data points is computationally demanding and has a computational cost that scales as the cube of the number of data points as it involves multiplication and inversion of matrices of large cardinalities infeasible for computation on a single node. Additionally, its standard implementation involves complex model fitting and function minimization steps which make automatic kriging analysis from raw data a considerable challenge. Meanwhile, Apache Spark has emerged as a large-scale data processing engine with a dedicated Machine Learning Library (MLLib) for processing large matrices and thereby can be used for large-scale kriging analysis with considerable time. In this paper, we present a new fast distributed OK algorithm on Apache Spark framework and provide an efficient and simple distributed matrix inversion scheme to accelerate the execution of distributed OK algorithm. We have employed Strassen's direct method for matrix inversion and the acceleration is achieved by exploiting the symmetry nature of the variance-covariance matrix of the OK equation to invert the matrix. We show experimentally that our distributed inversion scheme enables us to invert a16,000x16,000 matrix with 51% and 38% less wall clock time than distributed Spark-basedLUand Strassen's inversion scheme, respectively.																	2364-415X	2364-4168				SEP	2020	10	3					249	264		10.1007/s41060-020-00215-3													
J								Classifying sensitive content in online advertisements with deep learning	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Online advertising; Advertising technology; Computer vision; Image classification; Deep learning; Convolutional neural networks		In online advertising, an important quality control step is to audit advertising images ("creatives") before they appear on publishers' Web pages. This ensures that advertisements only appear on Web pages where the ad is appropriate. If a creative with sensitive content such as gambling and pornography is displayed on the wrong Web page, it can ruin the user's experience, the publisher's reputation, and may have legal implications. To protect against this, humans must audit every creative before it is displayed through our ad exchange; this process is costly and time-consuming. To detect sensitive content, we use a pre-trained deep convolutional neural network (Xception Chollet in: The IEEE conference on computer vision and pattern recognition (CVPR), 2017) to process the creative image, and merge its features with the historical distribution of categories associated with the creative's landing page (the Web page that loads when the ad is clicked, which may also contain sensitive content). This representation is then passed through a series of fully connected layers to predict the sensitive category. The trained model achieves slightly better than human performance (model accuracy 99.92%; human accuracy 99.88%) on a large fraction of creatives (61%), while making 3.5 times fewer mistakes in very sensitive categories. The main challenges we faced were to detect, with high accuracy, creatives from 10 "very sensitive" categories as determined by our Creative Audit team, along with a highly imbalanced data set with 95% of creatives having no sensitive categories. This paper extends the work we described in Austin et al. (in: Proceedings of the 2018 IEEE international conference on data science and advanced analytics (DSAA), DSAA'18, 2018). It demonstrates the successful usage of deep learning in production as a method for detecting sensitive creatives, while respecting the constraints set by business.																	2364-415X	2364-4168				SEP	2020	10	3					265	276		10.1007/s41060-020-00212-6													
J								Identifying Pareto-based solutions for regression subset selection via a feasible solution algorithm	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Pareto; Optimal; Feasible solution; Multiple; Objective; Subset selection; Regression	OPTIMALITY	The concept of Pareto optimality has been utilized in fields such as engineering and economics to understand fluid dynamics and consumer behavior. In machine learning contexts, Pareto-optimality has been used to identify tuning parameters that best optimize a set ofmcriteria (multi-objective optimization). During the process of regression model selection, data scientists are often concerned with choosing a model which has the best single criterion (e.g., Akaike information criterion (AIC) orR-squared (R-2)) before continuing to check a number of other regression model characteristics (e.g., model size, form, diagnostics, and interpretability). This strategy is multi-objective in nature but single objective in its numeric execution. This paper will first introduce a feasible solution algorithm (FSA) and explain how it can be applied to multi-objective problems for regression subset selection. Then we introduce the general framework of Pareto optimality within the regression setting. We then apply the algorithm in a simulation setting where we seek to estimate the first four Pareto boundaries for regression models using two model fit criteria. Finally, we present an application where we use a US communities and crime dataset.																	2364-415X	2364-4168				SEP	2020	10	3					277	284		10.1007/s41060-020-00218-0													
J								Modelling the electrical energy profile of a batch manufacturing pharmaceutical facility	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Sustainable manufacturing; Energy modelling; Data science; Machine learning	RANDOM FOREST; DATA SCIENCE; CONSUMPTION; PREDICTION; SELECTION; SYSTEMS	Sustainable manufacturing practices are a dominating consideration for legacy factories. Major attention is being applied to improving current practices to more sustainable ones. This research provides a case study of a batch manufacturing pharmaceutical facility and compares a number of approaches to modelling the electrical energy consumption in the plant. An accurate model of the electrical energy in the facility will allow more sustainable approaches to be developed. This can be achieved by improving current processes to reduce the electrical load. Historical electrical energy data were modelled using traditional time series methods. Historical manufacturing data and the electrical energy data were used to develop machine learning models using a feedforward neural network and a random forest. All of the approaches were then compared. The major challenge posed in model development and validation was acquiring data suitable for machine learning. The manufacturing data were stored in hand-written records. These records needed to be digitised and then go through a number of transformative steps before the data were suitable for modelling. The random forest model successfully modelled the energy profile of the facility. The model can be used to predict and better manage the plant electrical energy load.																	2364-415X	2364-4168				SEP	2020	10	3					285	300		10.1007/s41060-020-00217-1													
J								Estimating Designers' Performance considering Personal Characteristics and External Factors Together	ADVANCES IN HUMAN-COMPUTER INTERACTION											TEAM PERFORMANCE; JOB-PERFORMANCE; COGNITIVE-ABILITY; COMMUNICATION; PREDICTORS; TRAITS; MODELS	Design team performance evaluation can occur in different ways, all of them requiring considerations on interactions among team members; in turn, these considerations should count on as many pieces of information as possible about individuals. The literature already explains how personal characteristics and/or external factors influence designers' performance; nevertheless, a way to evaluate performance considering several personal characteristics and external factors together is missing. This research tries to fill the gap by developing the Designer's Performance Estimator (DPE), a ready-to-use tool for researchers and practitioners who need to make information about team members as richer as possible.																	1687-5893	1687-5907				SEP 1	2020	2020								1823291	10.1155/2020/1823291													
J								Wide Bezel Televisions Decrease Immersive Experiences	ADVANCES IN HUMAN-COMPUTER INTERACTION											VIRTUAL ENVIRONMENTS; INTERACTIVITY; TELEPRESENCE; ENJOYMENT; FRAMEWORK; REALITY; SIZE	This study explored how telepresence could be affected by stimuli from reality that distracts people while they are watching television. The sample comprised of 36 undergraduate and graduate students from a university in South Korea (age range: 18-38 years,M = 22.61, and SD = 4.12). A between-subjects experimental design was employed with two types of viewing equipment (a television screen vs. a television screen with side screens that act as stimuli from reality) and two bezel widths (2 cm vs. 10 cm) to examine how each condition influenced the viewers' perceived telepresence. The results revealed that participants' perception of telepresence was not affected by the type of viewing equipment. However, the level of telepresence was affected by the bezel width: the thinner the bezel, the more telepresence felt by the viewers. These findings provide important insights that can guide the future designs of screen bezels for televisions and other devices in order to more effectively create immersive virtual worlds. Future studies are needed to examine the relationship between central vision and telepresence.																	1687-5893	1687-5907				SEP 1	2020	2020								9349560	10.1155/2020/9349560													
J								Chemobrionics: From Self-Assembled Material Architectures to the Origin of Life	ARTIFICIAL LIFE										Chemical garden; chemobrionics; origin of life; biomimetics; submarine alkaline vent theory	CHEMICAL GARDENS; PRECIPITATION; GROWTH; EMERGENCE; PHOSPHATE; REDOX; TUBES	Self-organizing precipitation processes, such as chemical gardens forming biomimetic micro- and nanotubular forms, have the potential to show us new fundamental science to explore, quantify, and understand nonequilibrium physicochemical systems, and shed light on the conditions for life's emergence. The physics and chemistry of these phenomena, due to the assembly of material architectures under a flux of ions, and their exploitation in applications, have recently been termed chemobrionics. Advances in understanding in this area require a combination of expertise in physics, chemistry, mathematical modeling, biology, and nanoengineering, as well as in complex systems and nonlinear and materials sciences, giving rise to this new synergistic discipline of chemobrionics.																	1064-5462	1530-9185				SEP	2020	26	3					315	326		10.1162/artl_a_00323													
J								Side Reactions Do Not Completely Disrupt Linear Self-Replicating Chemical Reaction Systems	ARTIFICIAL LIFE										Autocatalytic network; origin of life; chemical reaction network; mass action kinetics; artificial chemistry	NETWORK FORMATION; SETS	A crucial question within the fields of origins of life and metabolic networks is whether or not a self-replicating chemical reaction system is able to persist in the presence of side reactions. Due to the strong nonlinear effects involved in such systems, they are often difficult to study analytically. There are however certain conditions that allow for a wide range of these reaction systems to be well described by a set of linear ordinary differential equations. In this article, we elucidate these conditions and present a method to construct and solve such equations. For those linear self-replicating systems, we quantitatively find that the growth rate of the system is simply proportional to the sum of all the rate constants of the reactions that constitute the system (but is nontrivially determined by the relative values). We also give quantitative descriptions of how strongly side reactions need to be coupled with the system in order to completely disrupt the system.																	1064-5462	1530-9185				SEP	2020	26	3					327	337		10.1162/artl_a_00327													
J								Symbiosis Promotes Fitness Improvements in the Game of Life	ARTIFICIAL LIFE										Symbiosis; cooperation; open-ended evolution; Game of Life; Immigration Game; levels of selection	EVOLUTION; AUTOPOIESIS; MODEL	We present a computational simulation of evolving entities that includes symbiosis with shifting levels of selection. Evolution by natural selection shifts from the level of the original entities to the level of the new symbiotic entity. In the simulation, the fitness of an entity is measured by a series of one-on-one competitions in the Immigration Game, a two-player variation of Conway's Game of Life. Mutation, reproduction, and symbiosis are implemented as operations that are external to the Immigration Game. Because these operations are external to the game, we can freely manipulate the operations and observe the effects of the manipulations. The simulation is composed of four layers, each layer building on the previous layer. The first layer implements a simple form of asexual reproduction, the second layer introduces a more sophisticated form of asexual reproduction, the third layer adds sexual reproduction, and the fourth layer adds symbiosis. The experiments show that a small amount of symbiosis, added to the other layers, significantly increases the fitness of the population. We suggest that the model may provide new insights into symbiosis in biological and cultural evolution.																	1064-5462	1530-9185				SEP	2020	26	3					338	365		10.1162/artl_a_00326													
J								Genetic Music System with Synthetic Biology	ARTIFICIAL LIFE										Artificial life music; genetic music; generative music; DNA music; Miranda machine		This article introduces GeMS, a system for music composition informed by synthetic biology. GeMS generates music with simulations of genetic processes, such as transcription, translation, and protein folding, with which biological systems render chains of amino acids from DNA strands. The system comprises the following components: theMiranda machine, therhythmator, and thepitch processor. The Miranda machine is an abstract Turing-machine-like processor, which manipulates a sequence of DNA symbols according to a set of programming instructions. This process generates a pool of new DNA strands, which are subsequently translated into rhythms. GeMS represents the musical equivalent of amino acids in terms of rhythms, referred to as rhythmic codons. This enables the rhythmator to convert DNA sequences into rhythmic sequences. The pitch processor generates pitches for such rhythmic sequences. It is inspired by the phenomenon of protein folding. The pitch processor considers orientation information of DNA instructions yielded by the Miranda machine in order to activate algorithms for generating pitches. A musical composition, entitledArtibiotics, for percussion ensemble and electronic instruments, is presented to demonstrate the system.																	1064-5462	1530-9185				SEP	2020	26	3					366	390		10.1162/artl_a_00325													
J								Self-Organization and Artificial Life	ARTIFICIAL LIFE										Self-organization; review; classification; soft ALife; hard ALife; wet ALife	CELLULAR-AUTOMATA; REPLICATING MICELLES; AGGREGATION BEHAVIOR; DECISION-MAKING; DESIGN PATTERNS; SYSTEM; MODELS; ROBOT; COMPLEXITY; EVOLUTION	Self-organization can be broadly defined as the ability of a system to display ordered spatiotemporal patterns solely as the result of the interactions among the system components. Processes of this kind characterize both living and artificial systems, making self-organization a concept that is at the basis of several disciplines, from physics to biology and engineering. Placed at the frontiers between disciplines, artificial life (ALife) has heavily borrowed concepts and tools from the study of self-organization, providing mechanistic interpretations of lifelike phenomena as well as useful constructivist approaches to artificial system design. Despite its broad usage within ALife, the concept of self-organization has been often excessively stretched or misinterpreted, calling for a clarification that could help with tracing the borders between what can and cannot be considered self-organization. In this review, we discuss the fundamental aspects of self-organization and list the main usages within three primary ALife domains, namely "soft" (mathematical/computational modeling), "hard" (physical robots), and "wet" (chemical/biological systems) ALife. We also provide a classification to locate this research. Finally, we discuss the usefulness of self-organization and related concepts within ALife studies, point to perspectives and challenges for future research, and list open questions. We hope that this work will motivate discussions related to self-organization in ALife and related fields.																	1064-5462	1530-9185				SEP	2020	26	3					391	408		10.1162/artl_a_00324													
J								Virtual Environment Positioning Utilizing Play-Script Spatiotemporal Reasoning	IEEE TRANSACTIONS ON GAMES										Games; Markup languages; Broadcasting; Natural language processing; Engines; Robots; Spatiotemporal phenomena; Force-directed graphs; natural language processing; play-scripts; rules; spatiotemporal reasoning; virtual environments	LANGUAGE	Automatically staging characters in order to facilitate the performance and blocking of a scene in a virtual environment is a difficult task today. There is only a limited set of techniques used in practice. Some general methods include Behavior Markup Language (BML) and motion capture replay (the most popular); however they require either detailed technical knowledge or are not adaptable to different environmental configurations. In this work, we block and perform scenes with synthetic actors utilizing only a text-based, standard play-script as the primary input to the positioning of these characters in an environment. Using natural language processing techniques, we extract the annotated movements from the script, then add additional movements and adjustments from our rules engine built with theater, stage performance, and human interaction spatiotemporal relationships. In addition, we incorporate force-directed graph algorithms to adjust positions of the artificially intelligent (AI)-driven characters based on human-controlled character movements for interaction. These techniques have been quantitatively and qualitatively evaluated, revealing both similar blocking, and indistinguishably good performances from a human's perspective, when compared to an actual human performance.																	2475-1502	2475-1510				SEPT	2020	12	3					225	235		10.1109/TG.2019.2927706													
J								Testing a Protocol for Characterizing Game Playing Agents Trained via Evolution on a New Game	IEEE TRANSACTIONS ON GAMES										Games; Registers; Genetic programming; Protocols; Measurement; Sociology; Resource management; Agent resources; evolutionary computation; mathematical games; representation	PRISONERS-DILEMMA; BEHAVIORS	A large series of studies on evolving agents to play mathematical games has demonstrated that many factors can significantly impact which agents arise, when those agents arise during evolution, and how robust they are in their play against other agents. Some or all of these factors have been shown to be relevant in the iterated prisoner's dilemma, the snowdrift game, and a fairly complex game called divide-the-dollar. This study demonstrates the impact or representation and agent resource allocation for a new game called coordination prisoner's dilemma. This paper demonstrates protocols from a recently published book for analysis of agent behavior and extends the work to another game, the first three-move game so treated. A new representation for agents playing mathematical games is introduced, a linear genetic programming register machine. New metrics for agent behavior including total exploitation, strategic variability, and action entropy are introduced. It is found that varying the representation and resource levels within a representation changes the types of game playing agents produced by evolution for coordination prisoner's dilemma.																	2475-1502	2475-1510				SEPT	2020	12	3					236	245		10.1109/TG.2019.2910642													
J								Video Game Development in a Rush: A Survey of the Global Game Jam Participants	IEEE TRANSACTIONS ON GAMES										Games; Software; Software engineering; Industries; Quality assurance; Correlation; Requirements engineering; Game development; game jam; opinion survey; software engineering; time pressure	SOFTWARE-DEVELOPMENT; VALIDITY; QUALITY	Video game development is a complex endeavor, often involving complex software, large organizations, and aggressive release deadlines. Several studies have reported that periods of "crunch time" are prevalent in the video game industry, but there are few studies on the effects of time pressure. We conducted a survey with participants of the Global Game Jam (GGJ), a 48-h hackathon. Based on 198 responses, the results suggest the following: iterative brainstorming is the most popular method for conceptualizing initial requirements; continuous integration, minimum viable product, scope management, version control, and stand-up meetings are frequently applied development practices; regular communication, internal playtesting, and dynamic and proactive planning are the most common quality assurance activities; and familiarity with agile development has a weak correlation with perception of success in the GGJ. We conclude that GGJ teams rely on ad hoc approaches to the development and face-to-face communication, and recommend some complementary practices with limited overhead. Furthermore, as our findings are similar to recommendations for software startups, we posit that game jams and the startup scene share contextual similarities. Finally, we discuss the drawbacks of systemic "crunch time" and argue that game jam organizers are in a good position to problematize the phenomenon.																	2475-1502	2475-1510				SEPT	2020	12	3					246	259		10.1109/TG.2019.2910248													
J								Improving Solvability for Procedurally Generated Challenges in Physical Solitaire Games Through Entangled Components	IEEE TRANSACTIONS ON GAMES										Games; Color; Image color analysis; Monte Carlo methods; Heuristic algorithms; Terminology; Approximation algorithms; Board games; game design; Monte Carlo methods; procedural content		Challenges for physical solitaire puzzle games are typically designed in advance by humans and limited in number. Alternatively, some games incorporate rules for stochastic setup, where the game board is randomly configured before the challenge is solved. These setup rules greatly increase the number of possible challenges, but can often generate unsolvable or uninteresting challenges. To better understand the compromises involved in minimizing undesirable challenges, we examine three games where component design choices can influence the stochastic nature of the resulting challenge generation algorithms. We evaluate the effect of these components and algorithms on challenge solvability and challenge engagement. We find that algorithms which control randomness through entangling components based on subelements of the puzzle mechanics can generate interesting challenges with a high probability of being solvable.																	2475-1502	2475-1510				SEPT	2020	12	3					260	269		10.1109/TG.2019.2918223													
J								Adaptive Music Composition for Games	IEEE TRANSACTIONS ON GAMES										Games; Music; Adaptation models; Adaptive systems; Context modeling; Instruments; Computational modeling; Agent-based modeling; computer generated music; neural networks	ACTIVATION; EMOTIONS; MODELS	The generation of music that adapts dynamically to content and actions has an important role in building more immersive, memorable, and emotive game experiences. To date, the development of adaptive music systems (AMSs) for video games is limited both by the nature of algorithms used for real-time music generation and the limited modeling of player action, game-world context, and emotion in current games. We propose that these issues must be addressed in tandem for the quality and flexibility of adaptive game music to significantly improve. Cognitive models of knowledge organization and emotional effect are integrated with multimodal, multiagent composition techniques to produce a novel AMS. The system is integrated into two stylistically distinct games. Gamers reported an overall higher immersion and correlation of music with game-world concepts with the AMS than that with the original game soundtracks in both the games.																	2475-1502	2475-1510				SEPT	2020	12	3					270	280		10.1109/TG.2019.2921979													
J								REAL: Reality-Enhanced Applied Games	IEEE TRANSACTIONS ON GAMES										Games; Collaboration; Automobiles; Cloud computing; Prototypes; Fuels; Applied games; automotive; Internet of Things (IoT); pervasive games; RESTful API; serious games	DRIVING BEHAVIOR	Pervasive games are an emerging genre combining reality and computing. This article presents a suite of simple pervasive serious games we have developed to explore the concept of "reality-enhanced gaming," a pattern to tie game play mechanics to the outcomes/measurements of real-world activities. The prototype games were realized in the context of TEAM, an industrial research project aimed at developing apps for flexible and collaborative mobility. The proposed games are examples of different user interfaces we considered useful to meet various significant scenarios, goals and user typologies, especially for improving car driving styles. Given the variety of information sources, contexts of use, and target users, we abstracted a game-oriented framework [Reality-Enhanced AppLied (REAL) games], in order to support reuse and scalability. Through a set of RESTful application programming interfaces (APIs), the REAL framework separates sensor data from actual game implementations, so as to provide different experiences to users, according to their specific needs and preferences. This concept-which allows serious game developers to focus on their specific game logic while seamlessly exploiting a variety of field sensors-is general and may be applied to a variety of domains. We validated REAL developing and field testing five typologies of serious games. Subjective evaluation results show a good level of satisfaction and perceived usefulness. More tests are needed, especially in terms of different application contexts, impact on developers, number and variety of users, and exposure time. However, outcomes confirm the significant potential of reality-enhanced game design and the importance of tools for supporting their development.																	2475-1502	2475-1510				SEPT	2020	12	3					281	290		10.1109/TG.2019.2940108													
J								AutoRhythm: A Music Game With Automatic Hit-Timing Generation and Percussion Identification	IEEE TRANSACTIONS ON GAMES										Games; Rhythm; Timing; Real-time systems; Object recognition; Servers; Active noise cancellation; automatic hit-timing generation; music game; percussion identification; rhythm game	ONSET DETECTION	This article describes a music rhythm game called AutoRhythm, which can automatically generate the hit timing as game contents from a given piece of music, and identify user-defined percussion of real objects in real time for gameplay. More specifically, AutoRhythm can generate the hit timing of a piece of music based on onset detection, so the user can use any music from their own collection for the rhythm game. Moreover, to make the game more realistic, AutoRhythm also allows the user to interact with the game via any object that can produce percussion sounds, such as a pen or a chopstick hitting against a table. AutoRhythm can identify the percussions in real time to replace tapping on the screen. This real-time user percussion identification is achieved based on the frame-based power spectrum of the filtered recording after background music reduction, which is performed based on the concept of active noise cancellation, with the estimated noisy playback music being subtracted from the original recording. Based on a test data set of 100 recordings, our experiment indicates that our system can achieve an F-measure of 78.22%, which outperforms other well-known classifiers and is quite satisfactory for the purpose of gameplay.																	2475-1502	2475-1510				SEPT	2020	12	3					291	301		10.1109/TG.2019.2936033													
J								Solving Sudoku With Ant Colony Optimization	IEEE TRANSACTIONS ON GAMES										Ant colony optimization; puzzle games; Sudoku	COMPLEXITY; ALGORITHM; PUZZLES	In this article, we present a new algorithm for the well-known and computationally challenging Sudoku puzzle game. Our ant-colony-optimization-based method significantly outperforms the state-of-the-art algorithm on the hardest, large instances of Sudoku. We provide evidence that-compared to traditional backtracking methods-our algorithm offers a much more efficient search of the solution space, and demonstrate the utility of a novel antistagnation operator. This work lays the foundation for future work on a general-purpose puzzle solver, and establishes Japanese pencil puzzles as a suitable platform for benchmarking a wide range of algorithms.																	2475-1502	2475-1510				SEPT	2020	12	3					302	311		10.1109/TG.2019.2942773													
J								A Game Design Plot: Exploring the Educational Potential of History-Based Video Games	IEEE TRANSACTIONS ON GAMES										Games; History; Buildings; Media; Motion pictures; Systematics; Entertainment industry; Communications technology; e-learning; history-based video games; web-based communication services		The number of video games that are developed based on real historical events and evidence is increasing. These history-based video games provide learning opportunities to players, but a certain type of such games-first- and third-person shooters-has not been carefully examined for their potentials. Knowing what players say about their game experience-even if the information and knowledge are inaccurate-helps researchers understand what type of learning could happen with such games. In this article, we propose a systematic approach to assessing games as learning environments, using the method of comparing the authenticity of popular history-based video games. Through a qualitative data analysis, we studied players' comments on the web-based communication services, such as game forums, digital distribution platforms, and discussion websites. Casual players' conversations on these websites showed that there exist several learning potentials in the games for players, including building their understanding about history and historical forces of the time, through personally relating to specific events, social artifacts, and places.																	2475-1502	2475-1510				SEPT	2020	12	3					312	322		10.1109/TG.2019.2954880													
J								Procedural Generation of Multistory Buildings With Interior	IEEE TRANSACTIONS ON GAMES										Buildings; Games; Grammar; Topology; Shape; Data models; Architecture; Architecture; games; procedural content generation; serious games; virtual worlds	ARCHITECTURE; ALGORITHM	This article is about the procedural generation of accessible multistory buildings including stairwells. We present an algorithm featuring a natural bottom-up approach, where each building is defined by a set of rooms that can be connected by doors or stairways and are equipped with windows. Our approach is robust so that neither the rooms, nor the connections, nor the objects can be placed in invalid positions. In the second part, we enhance the process by a procedural growth algorithm with automatic corridor detection and stairwell placement. Finally, we propose an automatic mesh generation for either procedurally or manually designed buildings. We evaluate our approach by creating a full implementation of the algorithm, and judge it based on the resulting realism, accessibility, and diversity of a generated test set of buildings.																	2475-1502	2475-1510				SEPT	2020	12	3					323	336		10.1109/TG.2019.2957733													
J								Interval-Valued Intuitionistic Uncertain Linguistic Multi-attribute Decision-Making Method for Plant Location Selection with Partitioned Hamy Mean	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multi-attribute group decision making; Interval-valued intuitionistic uncertain linguistic variable; Linguistic scale function; Partitioned Hamy mean; Plant location selection	OPERATORS; 2-TUPLE; MODEL; SETS	Plant location selection (PLS) is a typical and complex multi-attribute group decision-making (MAGDM) problem, which requires consideration of several alternatives and multiple attributes. However, existing research results on PLS lack sufficient consideration of complex relationship patterns between attributes, which often leads to inaccurate decision results. Thus, in this paper, a new MAGDM technique is proposed to solve the problem of PLS, in which complex interrelationship structures exist between multi-inputs and attribute values are represented as interval-valued intuitionistic uncertain linguistic variables (IVIULVs). First, to solve the shortcomings of the existing operations, some new operational rules are redefined for IVIULVs based on linguistic scale functions and Archimedean T-conorms and T-norms. These new rules of operation hold the closedness, and are flexible for semantic transformation processing. Then, to more accurately reflect the partition structure and the complex interrelationship pattern among multi-input arguments, we extend the traditional Hamy mean and propose the partitioned Hamy mean (PHAM), which can eliminate the impact of unrelated attributes on the results and meet the semantic conversion needs of different decision makers. Furthermore, we develop two new fuzzy linguistic aggregation operators: interval-valued intuitionistic uncertain linguistic PHAM (IVIULPHAM) and its weighted version (IVIULWPHAM). Finally, a novel MAGDM technique (IVIUL-MAGDM) based on the IVIULWPHAM operator is proposed to solve the problem of PLS. Several examples of PLS are also illustrated to show the validity and advantages of the IVIUL-MAGDM method by comparing with the other existing MAGDM methods.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1993	2010		10.1007/s40815-019-00736-5													
J								Morphological Characterization of Functional Brain Imaging by Isosurface Analysis in Parkinson's Disease	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Parkinson's disease; neuroimaging; machine learning; isosurfaces; Parkinson's Progression Markers Initiative (PPMI); Single Photon Emission Computed Tomography (SPECT); Computer-Aided-Diagnosis (CAD); supervised learning	SPECT; CLASSIFICATION; DIAGNOSIS	Finding new biomarkers to model Parkinson's Disease (PD) is a challenge not only to help discerning between Healthy Control (HC) subjects and patients with potential PD but also as a way to measure quantitatively the loss of dopaminergic neurons mainly concentrated at substantia nigra. Within this context, this work presented here tries to provide a set of imaging features based on morphological characteristics extracted from I[123]-Ioflupane SPECT scans to discern between HC and PD participants in a balanced set of 386 scans from Parkinson's Progression Markers Initiative (PPMI) database. These features, obtained from isosurfaces of each scan at different intensity levels, have been classified through the use of classical Machine Learning classifiers such as Support-Vector-Machines (SVM) or Naive Bayesian and compared with the results obtained using a Multi-Layer Perceptron (MLP). The proposed system, based on a Mann-Whitney-Wilcoxon U-Test for feature selection and the SVM approach, yielded a 97.04% balanced accuracy when the performance was evaluated using a 10-fold cross-validation. This proves the reliability of these biomarkers, especially those related to sphericity, center of mass, number of vertices, 2D-projected perimeter or the 2D-projected eccentricity, among others, but including both internal and external isosurfaces.																	0129-0657	1793-6462				SEP	2020	30	9							2050044	10.1142/S0129065720500446													
J								Enhancement of Hippocampal Spatial Decoding Using a Dynamic Q-Learning Method With a Relative Reward Using Theta Phase Precession	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Place cell; interneuron; dynamical Q-learning; phase precession; adaptive reward function; goal-direction navigation	PLACE CELLS; GRID CELLS; SPACE; MODEL; ENSEMBLE; POSITION; INTERNEURONS; NAVIGATION; RAT; RECONSTRUCTION	Hippocampal place cells and interneurons in mammals have stable place fields and theta phase precession profiles that encode spatial environmental information. Hippocampal CA1 neurons can represent the animal's location and prospective information about the goal location. Reinforcement learning (RL) algorithms such as Q-learning have been used to build the navigation models. However, the traditional Q-learning (tQ-learning) limits the reward function once the animals arrive at the goal location, leading to unsatisfactory location accuracy and convergence rates. Therefore, we proposed a revised version of the Q-learning algorithm, dynamical Q-learning (dQ-learning), which assigns the reward function adaptively to improve the decoding performance. Firing rate was the input of the neural network of dQ-learning and was used to predict the movement direction. On the other hand, phase precession was the input of the reward function to update the weights of dQ-learning. Trajectory predictions using dQ- and tQ-learning were compared by the root mean squared error (RMSE) between the actual and predicted rat trajectories. Using dQ-learning, significantly higher prediction accuracy and faster convergence rate were obtained compared with tQ-learning in all cell types. Moreover, combining place cells and interneurons with theta phase precession improved the convergence rate and prediction accuracy. The proposed dQ-learning algorithm is a quick and more accurate method to perform trajectory reconstruction and prediction.																	0129-0657	1793-6462				SEP	2020	30	9							2050048	10.1142/S0129065720500483													
J								Neurolight: A Deep Learning Neural Interface for Cortical Visual Prostheses	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Visual neuroprosthesis; neural encoding; computational models; deep learning; artificial vision	ELECTRICAL-STIMULATION; COCHLEAR IMPLANTS; BLIND; VISION; RESTORATION; MODEL	Visual neuroprosthesis, that provide electrical stimulation along several sites of the human visual system, constitute a potential tool for vision restoration for the blind. Scientific and technological progress in the fields of neural engineering and artificial vision comes with new theories and tools that, along with the dawn of modern artificial intelligence, constitute a promising framework for the further development of neurotechnology. In the framework of the development of a Cortical Visual Neuroprosthesis for the blind (CORTIVIS), we are now facing the challenge of developing not only computationally powerful tools and flexible approaches that will allow us to provide some degree of functional vision to individuals who are profoundly blind. In this work, we propose a general neuroprosthesis framework composed of several task-oriented and visual encoding modules. We address the development and implementation of computational models of the firing rates of retinal ganglion cells and design a tool - Neurolight - that allows these models to be interfaced with intracortical microelectrodes in order to create electrical stimulation patterns that can evoke useful perceptions. In addition, the developed framework allows the deployment of a diverse array of state-of-the-art deep-learning techniques for task-oriented and general image pre-processing, such as semantic segmentation and object detection in our system's pipeline. To the best of our knowledge, this constitutes the first deep-learning-based system designed to directly interface with the visual brain through an intracortical microelectrode array. We implement the complete pipeline, from obtaining a video stream to developing and deploying task-oriented deep-learning models and predictive models of retinal ganglion cells' encoding of visual inputs under the control of a neurostimulation device able to send electrical train pulses to a microelectrode array implanted at the visual cortex.																	0129-0657	1793-6462				SEP	2020	30	9							2050045	10.1142/S0129065720500458													
J								Intrinsic Synchronization Analysis of Brain Activity in Obsessive-compulsive Disorders	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Neuropsychiatric; synchronization; MEMD; EEG; OCD	GENERALIZED SYNCHRONIZATION; ALZHEIMERS-DISEASE; QUANTITATIVE EEG; PHASE-LOCKING; METHODOLOGY; LIKELIHOOD; COHERENCE; DIAGNOSIS; DEPRESSION; ASYMMETRY	Obsessive-compulsive disorder (OCD) is one of the neuropsychiatric disorders qualified by intrusive and iterative annoying thoughts and mental attitudes that are activated by these thoughts. In recent studies, advanced signal processing techniques have been favored to diagnose OCD. This research suggests four different measurements; intrinsic phase-locked value, intrinsic coherence, intrinsic synchronization likelihood, and intrinsic visibility graph similarity that quantifies the synchronization level and complexity in electroencephalography (EEG) signals. This intrinsic synchronization is achieved by utilizing Multivariate Empirical Mode Decomposition (MEMD), a data-driven method that resolves nonlinear and nonstationary data into their intrinsic mode functions. Our intrinsic technique in this study demonstrates that MEMD-based synchronization analysis gives us much more detailed knowledge rather than utilizing the synchronization method alone. Furthermore, the nonlinear synchronization method presents more consistent results considering OCD heterogeneity. Statistical evaluation using sample t-test and U-test has shown the significance of such new methodology.																	0129-0657	1793-6462				SEP	2020	30	9							2050046	10.1142/S012906572050046X													
J								Response to Discussion on "Improved Overlap-Based Undersampling for Imbalanced Dataset Classification with Application to Epilepsy and Parkinson's Disease,"	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Class overlap; imbalanced data; undersampling; classification; medical; Fuzzy C-means		In the paper Improved Overlap-Based Undersampling for Imbalanced Dataset Classification with Application to Epilepsy and Parkinson's Disease, the authors introduced two new methods that address the class overlap problem in imbalanced datasets. The methods involve identification and removal of potentially overlapped majority class instances. Extensive evaluations were carried out using 136 datasets and compared against several state-of-the-art methods. Results showed competitive performance with those methods, and statistical tests proved significant improvement in classification results. The discussion on the paper related to the behavioral analysis of class overlap and method validation was raised by Fern ' andez. In this article, the response to the discussion is delivered. Detailed clarification and supporting evidence to answer all the points raised are provided.																	0129-0657	1793-6462				SEP	2020	30	9							2075002	10.1142/S0129065720750027													
J								Schizophrenia Using Intrinsic Functional Connectivity	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Schizophrenia; multivariate pattern analysis; functional connectivity; ultra-high risk; symptom severity	ULTRA-HIGH-RISK; DEFAULT MODE NETWORK; PATTERN-CLASSIFICATION; NEURAL-NETWORK; BRAIN; PSYCHOSIS; PREDICTION; STATE; DYSCONNECTIVITY; DYSFUNCTION	Past studies have consistently shown functional dysconnectivity of large-scale brain networks in schizophrenia. In this study, we aimed to further assess whether multivariate pattern analysis (MVPA) could yield a sensitive predictor of patient symptoms, as well as identify ultra-high risk (UHR) stage of schizophrenia from intrinsic functional connectivity of whole-brain networks. We first combined rank-based feature selection and support vector machine methods to distinguish between 43 schizophrenia patients and 52 healthy controls. The constructed classifier was then applied to examine functional connectivity profiles of 18 UHR individuals. The classifier indicated reliable relationship between MVPA measures and symptom severity, with higher classification accuracy in more severely affected schizophrenia patients. The UHR subjects had classification scores falling between those of healthy controls and patients, suggesting an intermediate level of functional brain abnormalities. Moreover, UHR individuals with schizophrenia-like connectivity profiles at baseline presented higher rate of conversion to full-blown illness in the follow-up visits. Spatial maps of discriminative brain regions implicated increases of functional connectivity in the default mode network, whereas decreases of functional connectivity in the cerebellum, thalamus and visual areas in schizophrenia. The findings may have potential utility in the early diagnosis and intervention of schizophrenia.																	0129-0657	1793-6462				SEP	2020	30	9							2050047	10.1142/S0129065720500471													
J								Correction Tower: A General Embedding Method of the Error Recognition for the Knowledge Graph Correction	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Correction tower; erroneous relations; inconsistency; knowledge graph correction; error recognition; outliers	QUALITY; WEB	Today, knowledge graphs (KGs) are growing by enrichment and refinement methods. The enrichment and refinement can be gained using the correction and completion of the KG. The studies of the KG completion are rich, but less attention has been paid to the methods of the KG error correction. The correction methods are divided into embedding and nonembedding methods. Embedding correction methods have been recently introduced in which a KG is embedded into a vector space. Also, existing correction approaches focused on the recognition of the three types of errors, the outliers, inconsistencies and erroneous relations. One of the challenges is that most outlier correction methods can recognize only numeric outlier entities by nonembedding methods. On the other hand, inconsistency errors are recognized during the knowledge extraction step and existing methods of this field do not pay attention to the recognition of these errors as post-correction by embedding methods. Also, to correct erroneous relations, new embedding techniques have not been used. Since the errors of a KG are variant and there is no method to cover all of them, a new general correction method is proposed in this paper. This method is called correction tower in which these three error types are corrected in three trays. In this correction tower, a new configuration will be suggested to solve the above challenges. For this aim, a new embedding method is proposed for each tray. Finally, the evaluation results show that the proposed correction tower can improve the KG error correction methods and proposed configuration can outperform previous results.																	0218-0014	1793-6381				SEP	2020	34	10							2059034	10.1142/S021800142059034X													
J								Real-Time Mobile-Based Electrocardiogram System for Remote Monitoring of Patients with Cardiac Arrhythmias	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Real-time and remote ECG monitoring; multiple cardiac patients; ECG wearable sensor; real-time support vector machine	QRS DETECTION; NEURAL-NETWORK; ECG ENHANCEMENT; HEART-FAILURE; CLASSIFICATION; SIGNAL; COMPRESSION; ALGORITHM; CARE	In this study, we propose an electrocardiogram (ECG) system for the simultaneous and remote monitoring of multiple heart patients. It consists of three main components: patient, sever, and monitoring units. The patient unit uses a wearable miniature sensor that continuously measures ECG signals and sends them to a smart mobile phone via a Bluetooth connection. In the mobile device, the ECG signals can be stored, displayed on screen, and automatically transmitted to a distant server unit over the internet; the server stores ECG data from several patients. Health care stakeholders use a monitoring unit to retrieve the ECG signals of multiple patients at any time from the server for display and real-time automatic analysis. The analysis includes segmentation of the ECG signal into separate heartbeats followed by arrhythmia detection and classification. When compared to existing real-time ECG systems, where the detection of abnormalities is usually performed using simple rules, the proposed system implements a real-time classification module that is based on a support vector machine (SVM) classifier. Extensive experimental results on ECG data obtained from a TechPatient (TM) simulator, a real person, and 20 records from the MIT arrhythmia database are reported and discussed.																	0218-0014	1793-6381				SEP	2020	34	10							2058013	10.1142/S0218001420580136													
J								Particle Swarm Optimization Algorithm with Mutation Operator for Particle Filter Noise Reduction in Mechanical Fault Diagnosis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Mutation operator; particle swarm optimization; particle filter; noise reduction		In this paper, a new particle swarm optimization particle filter (NPSO-PF) algorithm is proposed, which is called particle cluster optimization particle filter algorithm with mutation operator, and is used for real-time filtering and noise reduction of nonlinear vibration signals. Because of its introduction of mutation operator, this algorithm overcomes the problem where by particle swarm optimization (PSO) algorithm easily falls into local optimal value, with a low calculation accuracy. At the same time, the distribution and diversity of particles in the sampling process are improved through the mutation operation. The defect of particle filter (PF) algorithm where the particles are poor and the utilization rate is not high is also solved. The mutation control function makes the particle set optimization process happen in the early and late stages, and improves the convergence speed of the particle set, which greatly reduces the running time of the whole algorithm. Simulation experiments show that compared with PF and PSO-PF algorithms, the proposed NPSO-PF algorithm has lower root mean square error, shorter running time, higher signal-to-noise ratio and more stable filtering performance. It is proved that the algorithm is suitable for real-time filtering and noise reduction processing of nonlinear signals.																	0218-0014	1793-6381				SEP	2020	34	10							2058012	10.1142/S0218001420580124													
J								Fractional Fourier Transform for Digital Image Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image recognition; correlation; fractional correlation; peak to correlation energy	INVARIANT CORRELATION; POSITION; CLASSIFICATION; SIGNATURES; ROTATION; OBJECTS; SYSTEM	In the image recognition field, there are several techniques that allow identifying patterns in digital images, correlation being one of them. In a correlation, you have to obtain an output plane that is as clean as possible. To measure the sharpness of the correlation peak and the cleanliness of the output plane, a performance metric called Peak to Correlation Energy (PCE) is used. In this paper, the fractional correlation is applied to recognize real phytoplankton images. This fractional correlation guarantees a higher PCE compared to the conventional correlation. The results of PCE are two-orders of magnitude higher than those obtained with the conventional correlation and manage to identify 91.23% of the images, while the conventional correlation only manages to identify 87.42% of them. This methodology was tested using images in salt and pepper or Gaussian noise, and the fractional correlation output plane always is cleaner and generates a better-defined correlation peak when compared with the classical correlation.																	0218-0014	1793-6381				SEP	2020	34	10							2054025	10.1142/S0218001420540257													
J								A Building Energy Consumption Prediction Method Based on Integration of a Deep Neural Network and Transfer Reinforcement Learning	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										DNN-TRL; feature transfer; denoising autoencoder; building energy prediction		With respect to the problem of the low accuracy of traditional building energy prediction methods, this paper proposes a novel prediction method for building energy consumption, which is based on the seamless integration of the deep neural network and transfer reinforcement learning (DNN-TRL). The method introduces a stack denoising autoencoder to extract the deep features of the building energy consumption, and shares the hidden layer structure to transfer the common information between different building energy consumption problems. The output of the DNN model is used as the input of the Sarsa algorithm to improve the prediction performance of the target building energy consumption. To verify the performance of the DNN-TRL algorithm, based on the data recorded by American Power Balti Gas and Electric Power Company, and compared with Sarsa, ADE-BPNN, and BP-Adaboost algorithms, the experimental results show that the DNN-TRL algorithm can effectively improve the prediction accuracy of the building energy consumption.																	0218-0014	1793-6381				SEP	2020	34	10							2052005	10.1142/S0218001420520059													
J								Personalized Knowledge Recommendation Based on Knowledge Graph in Petroleum Exploration and Development	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Recommendation algorithm; knowledge graph; petroleum exploration and development		Firstly, this paper designs the process of personalized recommendation method based on knowledge graph, and constructs user interest model. Second, the traditional personalized recommendation algorithms are studied and their advantages and disadvantages are analyzed. Finally, this paper focuses on the combination of knowledge graph and collaborative filtering recommendation algorithm. They are effective to solve the problem where K value is difficult to be determined in the clustering process of traditional collaborative filtering recommendation algorithm as well as data sparsity and cold start, utilizing the ample semantic relation in knowledge graph. If we use RDF data, which is distributed by the E and P (Exploration and Development) database based on the petroleum E and P, to verify the validity of the algorithm, the result shows that collaborative filtering algorithm based on knowledge graph can build the users' potential intentions by knowledge graph. It is enlightening to query the information of users. In this way, it expands the mind of users to accomplish the goal of recommendation. In this paper, a collaborative filtering algorithm based on domain knowledge atlas is proposed. By using knowledge graph to effectively classify and describe domain knowledge, the problems are solved including clustering and the cold start in traditional collaborative filtering recommendation algorithm. The better recommendation effect has been achieved.																	0218-0014	1793-6381				SEP	2020	34	10							2059033	10.1142/S0218001420590338													
J								Key Technologies of Intelligent Transportation Based on Image Recognition and Optimization Control	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image recognition; intelligent traffic; Hadoop; AdaBoost algorithm		With the development of digital image processing technology, the application scope of image recognition is more and more wide, involving all aspects of life. In particular, the rapid development of urbanization and the popularization and application of automobiles in recent years have led to a sharp increase in traffic problems in various countries, resulting in intelligent transportation technology based on image processing optimization control becoming an important research field of intelligent systems. Aiming at the application demand analysis of intelligent transportation system, this paper designs a set of high-definition bayonet systems for intelligent transportation. It combines data mining technology and distributed parallel Hadoop technology to design the architecture and analysis of intelligent traffic operation state data analysis. The mining algorithm suitable for the system proves the feasibility of the intelligent traffic operation state data analysis system with the actual traffic big data experiment, and aims to provide decision-making opinions for the traffic state. Using the deployed Hadoop server cluster and the AdaBoost algorithm of the improved MapReduce programming model, the example runs large traffic data, performs traffic analysis and speed-overspeed analysis, and extracts information conducive to traffic control. It proves the feasibility and effectiveness of using Hadoop platform to mine massive traffic information.																	0218-0014	1793-6381				SEP	2020	34	10							2054024	10.1142/S0218001420540245													
J								Speaker Classification with Support Vector Machine and Crossover-Based Particle Swarm Optimization	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Speaker classification; particle swarm optimization; support vector machine; machine learning	NEURAL-NETWORKS; SPEECH; ADAPTATION; GMM	It has been observed from the literature that speech is the most natural means of communication between humans. Human beings start speaking without any tool or any explicit education. The environment surrounding them helps them to learn the art of speaking. From the existing literature, it is found that the existing speaker classification techniques suffer from over-fitting and parameter tuning issues. An efficient tuning of machine learning techniques can improve the classification accuracy of speaker classification. To overcome this issue, in this paper, an efficient particle swarm optimization-based support vector machine is proposed. The proposed and the competitive speaker classification techniques are tested on the speaker classification data of Punjabi persons. The comparative analysis of the proposed technique reveals that it outperforms existing techniques in terms of accuracy, F-measure, specificity and sensitivity.																	0218-0014	1793-6381				SEP	2020	34	10							2051010	10.1142/S0218001420510106													
J								Vehicle Vision Robust Detection and Recognition Method	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Intelligent vehicle; visual; robust detection; sign recognition	PATTERN-RECOGNITION	With the rapid growth of the global economy, the global car ownership is also increasing year by year, which has caused a series of problems, the most prominent of which is traffic congestion and traffic accidents. In order to solve the traffic problem, all countries are actively studying the intelligent transportation system, and one of the important research contents of the intelligent transportation system is vehicle detection. Vehicle detection based on vision is to capture vehicle images in the driving environment through a camera, and then use computer vision recognition technology for vehicle detection and recognition. Although computer vision recognition technology has made great progress, how to improve the detection accuracy of the image to be detected is still an important content of visual recognition technology research. Intelligent vehicle visual robust detection and identification of methods of research to reduce the growing incidence of traffic accidents, improve the existing road traffic safety and transportation efficiency, alleviate the degree of driver fatigue problem are of great significance. This paper considers the intelligent vehicle environmental awareness of the key technology to the goal of robust detection and recognition based on machine vision problems for further research. The particle filter is used to extract the local energy of the image to realize the fast segmentation of the region of interest (ROI). In order to further verify the ROI, a measure learning method based on multi-core embedding is proposed, and the semantic classification of ROI is realized by integrating the color, shape and geometric features of ROI. Experimental results show that the algorithm can effectively eliminate false sexy ROI interest, and the algorithm is robust to complex background, illumination changes, perspective changes and other conditions.																	0218-0014	1793-6381				SEP	2020	34	10							2055020	10.1142/S0218001420550204													
J								An Indoor WLAN Location Algorithm Based on Fingerprint Database Processing	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										WLAN; principal component analysis method; k-d tree; best bin first	POSITIONING SYSTEM; SEARCH	Indoor positioning technology based on the Wireless Local Area Network (WLAN) fingerprinting method is becoming a promising choice as for ubiquitous WLAN infrastructure. The technology mainly compares the received signal strength (RSS) of a mobile device with an RSS fingerprint in the fingerprint database, and uses the matching rule to find the closest match as the estimated position of the device. The quality of the fingerprint database construction can directly affect the positioning results. This work proposes a three-stage fingerprint database processing method. In the first stage, the original fingerprint database is divided into several small sub-fingerprint databases according to the specified rules. In the second stage, every sub-fingerprint database is processed using the principal component analysis method to achieve a reduced dimension fingerprint dataset. In the third stage, the k-d tree method is used to process each dimension-reduced sub-fingerprint database for obtaining a hierarchical sub-fingerprint database. In addition, in the online phase, the best bin first (BBF) method is applied to the search engine of sub-fingerprint database to complete the location determination of the device. This method can improve positioning performance through simulation research.																	0218-0014	1793-6381				SEP	2020	34	10							2050026	10.1142/S0218001420500263													
J								An Attribute-Weighted Bayes Classifier Based on Asymmetric Correlation Coefficient	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Bayes classifier; asymmetric correlation; attribute weighting	SUPPORT VECTOR MACHINES; NAIVE BAYES; LOGISTIC-REGRESSION	In this research, an attribute-weighted one-dependence Bayes estimation algorithm based on the asymmetric correlation coefficient is proposed. The asymmetric correlation coefficients Tau_y and Lambda-y, respectively, are used to calculate the correlation between parent attributes and category labels, then the result of calculation is regarded as weight to the parent attribute. The algorithm is applied to eight types of different datasets including binary classification and multiple classification from the UCI database. By comparing the time complexity and classification accuracy, experimental results show that the algorithm can significantly improve the classification performance with less prediction error. In addition, several baseline methods such as KNN, ANN, logistic regression and SVM are used for comparison with the proposed method.																	0218-0014	1793-6381				SEP	2020	34	10							2050025	10.1142/S0218001420500251													
J								Learning Regular Expressions Using XCS-Based Classifier System	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Learning classifier system; evolutionary machine learning; XCS; genetic programming; regular expressions; genetic algorithm	COMPLEX	Evolutionary machine learning research aims to develop classifier systems that can solve complex and hard tasks. This paper addresses the problem of inferring a regular expression from a given set of strings for automating the task of information extraction. To the best of our knowledge, this paper is the first to propose the extension of accuracy-based classifier system XCS to learn the regular expressions for text extraction. This new system named as XCSREA includes tree-like code fragments to learn regular expressions. The genetic algorithm in action sets uses two-point crossover with uniform mutation and Roulette wheel parent selection method. Seven different datasets, each with three different lengths, are used to compare the performance of the proposed model with standard genetic programming (GP) approach. The experimental results demonstrate that XCSREA outperforms standard GP approach when sufficiently large numbers of classifiers are used.																	0218-0014	1793-6381				SEP	2020	34	10							2051011	10.1142/S0218001420510118													
J								Prediction of Cuttings Transport Behavior under Drill String Rotation Conditions in High-Inclination Section	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Cuttings transport; drill string rotation; high-inclination section; pressure loss	PIPE ROTATION; HOLE; CFD	Cuttings are likely to accumulate and eventually form a cuttings bed in the highly-deviated section, which usually lead to high friction and torque, slower rate of penetration, pipe stuck and other problems. It is therefore necessary to study cuttings transport mechanism and improve hole cleaning efficiency. In this study, the cuttings-transport behaviors with pipe rotation under turbulent flow conditions in the highly deviated eccentric section were numerically simulated based on Euler solid-fluid model and Realizable k-e model. The resulted numerical results were compared with available experimental data in reported literature to validate the algorithm, and good agreement was found. Under the conditions of drill string rotation, cuttings bed surface tilts in the direction of rotation and distributes asymmetrically in annulus. Drill string rotation, drilling fluid flow rate, cuttings diameter, cuttings injection concentration and drilling fluid viscosity affect the axial velocity of drilling fluid; whereas drilling fluid tangential velocity is mainly controlled by the rotational speed of drill string. Increase in value of drill string rotation, drilling fluid flow rate or hole inclination will increase cuttings migration velocity. Notably, drill string rotation reduces cuttings concentration and solid-fluid pressure loss, and their variations are dependent on inclination, cuttings injection concentration, cuttings diameter, drilling fluid velocity and viscosity. However, when a critical rotation speed is reached, no additional contribution is observed. The results can provide theoretical support for optimizing hole cleaning and realizing safety drilling of horizontal wells and extended reach wells.																	0218-0014	1793-6381				SEP	2020	34	10							2059035	10.1142/S0218001420590351													
J								Foreground Extraction and Motion Recognition Technology for Intelligent Video Surveillance	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Intelligent video surveillance; foreground extraction; background modeling; GMM; Weizmann		With the rapid development of computer technology and network technology, it has become possible to build a large-scale networked video surveillance system. The video surveillance system has become a new type of infrastructure necessary for modern cities. In this paper, the problem of foreground extraction and motion recognition in intelligent video surveillance is studied. The three key sub-problems, namely the extraction of motion foreground in video, the deblurring of motion foreground and the recognition of human motion, are studied and corresponding solutions are proposed. A background modeling technique based on video block is proposed. The background is modeled at the block level, which greatly reduces the spatial complexity of the algorithm. It solves the problem that the traditional Gaussian model (GMM) moving target enters the static state and is integrated into the background process. The target starts to move for a long time and there are ghosts and other problems, which reduce the processing efficiency of the lifting algorithm. The test results on the Weizmann dataset show that the proposed algorithm can achieve high human motion recognition accuracy and recognition with low computational complexity. The rate can reach 100%; the local constrained group sparse representation classification (LGSRC) model is used to classify it. The experimental results on Weizmann, KTH, UCF sports and other test datasets confirm the validity of the algorithm in this chapter. KNN, SRC voting classification accuracy.																	0218-0014	1793-6381				SEP	2020	34	10							2055021	10.1142/S0218001420550216													
J								Efficiency Optimization of Capsule Network Model Based on Vector Element	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image identification; deep learning; convolutional neural network; CapsNet; capsule network	DROPOUT	Currently, Deep Learning and Convolutional Neural Network (CNN) have been widely used in many fields and have generated very high value in these fields, especially in the field of image recognition. But there are some deficiencies in certain issues of image recognition. For example, CNN's recognizing performance is not good at different angles of objects and overlapping objects. Also, CNN is sometimes very sensitive to slight perturbations, modifying one pixel of a recognized image may cause recognition errors. For these problems, the capsule network (CapsNet) proposed by Geoffrey Hinton can solve the problems of traditional convolutional networks. Shortly after CapsNet was proposed, the model structure was relatively simple, and many aspects could be explored for improvement. This paper will optimize CapsNet from two aspects: "optimization of routing mechanism" and "increase Dropout operation." And carry out experiments and results analysis on these optimizations.																	0218-0014	1793-6381				SEP	2020	34	10							2052006	10.1142/S0218001420520060													
J								Human Action Recognition Based on Multiple Features and Modified Deep Learning Model	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Human action recognition; human 13 joint points; critical points of optical flow; deep belief nets; support vector machine	CATEGORIES	In order to improve the accuracy of human action recognition in video and the computational efficiency of large data sets, an action recognition algorithm based on multiple features and modified deep learning model is proposed. First, the deep network pre-training process is used to learn and optimize the RBM parameters, and the deep belief nets (DBN) model is constructed through deep learning. Then, human 13 joint points and critical points of optical flow are automatically extracted by DBN model. Second, these more abstract and more effective human motion features are combined to represent human actions. Ultimately, the entire DBN network structure is fine-tuned by support vector machine (SVM) algorithm to classify human actions. We demonstrate that human 13 joint points and critical points of optical flow are two very effective human action characterizations, our proposed approach greatly reduces the required samples, and shortens the training time of the samples, can efficiently process large data sets and can effectively recognize novel actions. We performed experiments on the KTH data set, Weizmann data set, the ballet data set and UCF101 data set to evaluate the proposed method, the experiment results show that the average recognition accuracy is over 98%, which validates its effectiveness, and show that our results are stable, reliable, and significantly better than the results of two state-of-the-art approaches on four different data sets. So, it lays a good theoretical foundation for practical applications.																	0218-0014	1793-6381				SEP	2020	34	10							2055022	10.1142/S0218001420550228													
J								Disentangling automatic and semi-automatic approaches to the optimization-based design of control software for robot swarms	NATURE MACHINE INTELLIGENCE											SELF-ORGANIZATION; EVOLUTION	Optimization-based design is an effective and promising approach to realizing collective behaviours for robot swarms. Unfortunately, the domain literature often remains vague about the exact role played by the human designer, if any. It is our contention that two cases should be disentangled: semi-automatic design, in which a human designer operates and steers an optimization process (for example, by fine-tuning the parameters of the optimization algorithm); and (fully) automatic design, in which the optimization process does not involve, need or allow any human intervention. In this Perspective, we briefly review the relevant literature; illustrate the hypotheses, characteristics and core challenges of semi-automatic and automatic design; and sketch out the context in which they could be ideally applied. Developing swarm robots for a specific application is a time consuming process and can be alleviated by automated optimization of the behaviour. Birattari and colleagues discuss that there are two fundamentally different design approaches; a semi-autonomous one, which allows for situation specific tuning from human engineers and one that needs to be entirely autonomous.																		2522-5839				SEP	2020	2	9					494	499		10.1038/s42256-020-0215-0													
J								Using online verification to prevent autonomous vehicles from causing accidents	NATURE MACHINE INTELLIGENCE											DECISION-MAKING; SAFETY	Ensuring that autonomous vehicles do not cause accidents remains a challenge. We present a formal verification technique for guaranteeing legal safety in arbitrary urban traffic situations. Legal safety means that autonomous vehicles never cause accidents although other traffic participants are allowed to perform any behaviour in accordance with traffic rules. Our technique serves as a safety layer for existing motion planning frameworks that provide intended trajectories for autonomous vehicles. We verify whether intended trajectories comply with legal safety and provide fallback solutions in safety-critical situations. The benefits of our verification technique are demonstrated in critical urban scenarios, which have been recorded in real traffic. The autonomous vehicle executed only safe trajectories, even when using an intended trajectory planner that was not aware of other traffic participants. Our results indicate that our online verification technique can drastically reduce the number of traffic accidents. Recent accidents with autonomous test vehicles have eroded trust in such self-driving cars. A shift in approach is required to ensure autonomous vehicles can never be the cause of accidents. An online verification technique is presented that guarantees provably safe motions, including fallback solutions in safety-critical situations, for any intended trajectory calculated by the underlying motion planner.																		2522-5839				SEP	2020	2	9					518	528		10.1038/s42256-020-0225-y													
J								Remote explainability faces the bouncer problem	NATURE MACHINE INTELLIGENCE												The concept of explainability is envisioned to satisfy society's demands for transparency about machine learning decisions. The concept is simple: like humans, algorithms should explain the rationale behind their decisions so that their fairness can be assessed. Although this approach is promising in a local context (for example, the model creator explains it during debugging at the time of training), we argue that this reasoning cannot simply be transposed to a remote context, where a model trained by a service provider is only accessible to a user through a network and its application programming interface. This is problematic, as it constitutes precisely the target use case requiring transparency from a societal perspective. Through an analogy with a club bouncer (who may provide untruthful explanations upon customer rejection), we show that providing explanations cannot prevent a remote service from lying about the true reasons leading to its decisions. More precisely, we observe the impossibility of remote explainability for single explanations by constructing an attack on explanations that hides discriminatory features from the querying user. We provide an example implementation of this attack. We then show that the probability that an observer spots the attack, using several explanations for attempting to find incoherences, is low in practical settings. This undermines the very concept of remote explainability in general. When automated decisions are provided by a company without providing the full model, users and law makers might demand a 'right to an explanation'. Le Merrer and Tredan show that malicious manipulations of these explanations are hard to detect, even for simple strategies to obscure the model's decisions.																		2522-5839				SEP	2020	2	9					529	539		10.1038/s42256-020-0216-z													
J								Constructing big panorama from video sequence based on deep local feature	IMAGE AND VISION COMPUTING										Panorama; Deep local feature; Feature tracking; Image stitching; Image blending	IMAGE; WARPS	Constructing high-quality panorama is a fundamental task in both computer vision and computer graphics communities, thus, leading to many image stitching approaches for panorama construction. However, panorama constructed by traditional image stitching has a limited angle of view and has an expensively computational cost. To defend the issues, in this paper, we proposed to use video sequence as input for constructing big panorama, resulting in a high-quality panoramic image, the presented method is built on the stabilized video and robust feature tracking method. Specifically, the input video sequences are captured by moving hand cameras which can be any type of consumer-level camera. To mitigate the affections from rolling shutters in videos, a novel video stabilization method is introduced to filter the unstable camera's path, then resulting in a stabilized video for panorama construction. Additionally, a deep local feature-based feature tracking method is proposed to produce feature correspondences between consecutive video frames for camera motion estimation used in both video stabilization and image stitching. Finally, a comprehensive experiment conducted on the benchmarking datasets is presented to demonstrate the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103972	10.1016/j.imavis.2020.103972													
J								A novel co-attention computation block for deep learning based image co-segmentation	IMAGE AND VISION COMPUTING										Visualco-attention; Imageco-segmentation; Deeplearning; Correlationcalculation; Averagepooling	COSEGMENTATION	The correlation between images is crucial for solving the image co-segmentation problem that is segmenting common and salient objects from a set of related images. This paper proposes a novel co-attention computation block to compute the visual correlation between images for improving the co-segmentation performance. Here 'co-attention' means that we obtain the co-attention features in encoded features of an image to guide the attention in another image. To this purpose, we firstly introduce top-k average pooling to compute the channel co-attention descriptor. Then we explore the correlation between features in different spatial positions to get the spatial co-attention descriptor. Finally, these two types of co-attention descriptors are multiplied to generate a fused one. We obtain such a fused co-attention descriptor for each image and use it to produce the co-attention augmented feature map for the following processing in the applications. We embed the proposed co-attention block into a U-shaped Siamese network for fulfilling the image co-segmentation. It is proven to be able to improve the performance effectively in the experiments. To our best knowledge, it leads to the currently best results on Internet dataset and iCoseg dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103973	10.1016/j.imavis.2020.103973													
J								Person search: New paradigm of person re-identification: A survey and outlook of recent works	IMAGE AND VISION COMPUTING										Person re-identification; Person search; Literature survey; Metric learning; Loss functions	NETWORK; ATTENTION	Person Search (PS) has become a major field because of its need in community and in the field of research among researchers. This task aims to find a probe person from whole scene which shows great significance in video surveillance field to track lost people, re-identification, and verification of person. In last few years, deep learning has played unremarkable role for the solution of re-identification problem. Deep learning shows incredible performance in person (re-ID) and search. Researchers experience more flexibility in proposing new methods and solve challenging issues such as low resolution, pose variation, background clutter, occlusion, viewpoints, and low illumination. Specially, convolutional neural network (CNN) achieves breakthrough performance and extracts useful patterns and characteristics. Development of new framework takes substantial efforts; hard work and computation cost are required to acquire excellent results. This survey paper includes brief discussion about feature representation learning and deep metric learning with novel loss functions. We thoroughly review datasets with performance analysis on existing datasets. Finally, we are reviewing current solutions for further consideration. (C) 2020 The Author. Published by Elsevier B.V.																	0262-8856	1872-8138				SEP	2020	101								103970	10.1016/j.imavis.2020.103970													
J								Cross-modal feature extraction and integration based RGBD saliency detection	IMAGE AND VISION COMPUTING										RGBD; Saliency; Cross-modal; Feature extraction; Integration	OBJECT DETECTION; FUSION	In RGBD saliency detection research field, RGB and depth cues are generally given the same status by RGBD saliency models. However, they ignore that both modalities are significantly different in inherent attribution so that effective features cannot be drawn from depth maps. In order to address this issue, this paper proposes a novel RGBD saliency model including two key components: the contrast-guided depth feature extraction (CDFE) module and the cross-modal feature integration (CFI) module. Specifically, considering the specific properties of depth information, we first design a targeted CDFE module, which learns multi-level deep depth features by strengthening the depth contrast between foreground and background, to provide multi-level deep depth features. Then, to sufficiently and reasonably integrate multi-level cross-modal features, namely the multi-level deep RGB and depth features, we equip the saliency inference branch with the CFI module, which contains two successive steps, i.e. information enrichment and feature enhancement. Extensive experiments are conducted on five challenging RGBD datasets, and the experimental results clearly demonstrate the effectiveness and superiority of the proposed model against the state-of-the-art RGBD saliency models. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103964	10.1016/j.imavis.2020.103964													
J								Feature based video stabilization based on boosted HAAR Cascade and representative point matching algorithm	IMAGE AND VISION COMPUTING										Digital image stabilization; Representative point matching; Boosted HAAR Cascade; Foreground feature selection; Motion compensation	IMAGE; SCHEME	The success of handheld video capturing devices has further fueled the need of improved video stabilization. The videos often contain many foreground facial features like eyes, nose etc. These foreground features can be considered as feature points and may be used to stabilize videos. This paper proposes an innovative and effective digital video stabilization technique, which utilizes foreground features present in the video to produce consistent and stabilized output. It uses successive stages of Boosted HAAR cascade and representative point matching digital motion stabilization algorithm to identify and stabilize the video. The feature based tracking of object improves motion estimation accuracy between two frames thereby increasing the correlation calculation and compensation motion vector. This work achieves a significantly smoother sequence after the motion compensation. It also improves the robustness, precision and quality of the video when compared to traditional digital stabilization algorithms. The simulation results compared with pre-existing techniques reflect distinct improvements in Inter-Frame Transformation Fidelity values and Structural Similarity Index along with lesser standard deviation between image frames. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103957	10.1016/j.imavis.2020.103957													
J								Deep manifold clustering based optimal pseudo pose representation (DMC-OPPR) for unsupervised person re-identification	IMAGE AND VISION COMPUTING										Person re-identification; Clustering; Pose estimation; Representation; Computer vision; Deep learning	NETWORK	Person re-identification (re-ID) is highly complex in a diverse surveillance environment. The existing person re ID methods are evaluated as a closed set problem with limited environmental variation. It is highly challenging to estimate the diverse poses of a dynamically crowded environment using the traditional unsupervised person re ID methods. To resolve this issue of handling complex diverse poses and camera angles, a contextual incremental multi-clustering based unsupervised person re-ID method have been proposed. Cam-pose based optimal similarity distance threshold is determined to label the unlabeled person re-ID images efficiently. Frequent intra and inter-camera pseudo pose sequences are represented with optimal distance threshold. This resolves the over fitting issue created by the dominant samples of an identity and reduces the source-target domain gap. The experimental results show the supremacy of our proposed method over the existing unsupervised person re-ID methods in handling complex poses and camera angles in an incremental self-learning diverse surveillance environment. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103956	10.1016/j.imavis.2020.103956													
J								Collaborative representation of blur invariant deep sparse features for periocular recognition from smartphones	IMAGE AND VISION COMPUTING										Periocular recognition; Visible Spectrum; Smartphone; Biometrics; Deep sparse filtering	IRIS RECOGNITION; FACE; PHASE	The periocular region is used for authentication in the recent days under unconstrained acquisition in biometrics. This work presents two new feature extraction techniques to achieve robust and blur invariant biometric verification using periocular images captured using smartphones (1) Deep Sparse Features (DSF) and (2) Deep Sparse Time Frequency Features (DeSTiFF). Both the approaches are based on extracting features via convolution of periocular images with a set of filters also referred as Deep Sparse Filters. The filters are learnt using natural image patches and sparse filtering approach. The DSF is obtained through convolution via Deep Sparse Filters. Further, convoluted responses are analyzed using Short Term Fourier Transform (STFT) to obtain time and frequency features of the images referred as DeSTIFF. The features obtained from the newly proposed feature extraction techniques are further represented in a collaborative subspace to achieve better verification performance. Both of the proposed feature extraction schemes are evaluated on two publicly available smartphone periocular databases and a new database (Visible Spectrum Periocular Image (VISPI) database) released with this article. The robustness of the proposed feature extraction is exemplified by comparing it with state-of-art approaches along with multiple deep networks where the improvement is evidently seen on large scale database with an average verification accuracy of Genuine Match Rate approximate to 98% at False Match Rate = 0.01%. We further support reproducible research by making the code and the database available for the academic research. (C) 2020 The Author(s). Published by Elsevier B.V.																	0262-8856	1872-8138				SEP	2020	101								103979	10.1016/j.imavis.2020.103979													
J								Automated repair of fragmented tracks with 1D CNNs	IMAGE AND VISION COMPUTING										Multiple object tracking; Fragmented track repair; Occlusion handling	SOCCER PLAYER TRACKING; CAMERA	Multiple object tracking is an important but challenging computer vision problem. The complex motion of objects makes tracking difficult during long periods of object occlusion, and as a result occlusions frequently cause fragmented tracks with gaps. Previous works use linear interpolation to fill in such gaps, a technique which is only able to model simple motion. As a result, tracked bounding box locations can be quite poor in these situations. In this paper, we propose a 1D CNN based solution to filling gaps which models complex motion in a data-driven way. Our proposed solution uses only bounding box coordinates as input, and as such does not incur the computational cost of processing image features directly. We show that our model significantly outperforms linear interpolation on dynamic sports datasets in terms of mean intersection over union between predicted and ground truth bounding boxes. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103982	10.1016/j.imavis.2020.103982													
J								Explaining VQA predictions using visual grounding and a knowledge base	IMAGE AND VISION COMPUTING										Deep Learning; Attention; Supervision; Knowledge Base; Interpretability; Explainability		In this work, we focus on the Visual Question Answering (VQA) task, where a model must answer a question based on an image, and the VQA-Explanations task, where an explanation is produced to support the answer. We introduce an interpretable model capable of pointing out and consuming information from a novel Knowledge Base (KB) composed of real-world relationships between objects, along with labels mined from available region descriptions and object annotations. Furthermore, this model provides a visual and textual explanations to complement the KB visualization. The use of a KB brings two important consequences: enhance predictions and improve interpretability. We achieve this by introducing a mechanism that can extract relevant information from this KB, and can point out the relations better suited for predicting the answer. A supervised attention map is generated over the KB to select the relevant relationships from it for each question-image pair. Moreover, we add image attention supervision on the explanations module to generate better visual and textual explanations. We quantitatively show that the predicted answers improve when using the KB; similarly, explanations improve with this and when adding image attention supervision. Also, we qualitatively show that the KB attention helps to improve interpretability and enhance explanations. Overall, the results support the benefits of having multiple tasks to enhance the interpretability and performance of the model. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				SEP	2020	101								103968	10.1016/j.imavis.2020.103968													
J								Adaptive HTF-MPR: An Adaptive Heterogeneous TensorFlow Mapper Utilizing Bayesian Optimization and Genetic Algorithms	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Task mapping; neural networks; Bayesian optimization; genetic algorithms; computational graphs; adaptivity; gradient boosting regressor		Deep neural networks are widely used in many artificial intelligence applications. They have demonstrated state-of-the-art accuracy on many artificial intelligence tasks. For this high accuracy to occur, deep neural networks require the right parameter values. This is achieved by a process known as training. The training of large amounts of data via many iterations comes at a high cost in regard to computation time and energy. Optimal resource allocation would therefore reduce the training time. TensorFlow, a computational graph library developed by Google, alleviates the development of neural network models and provides the means to train these networks. In this article, we propose Adaptive HTF-MPR to carry out the resource allocation, or mapping, on TensorFlow. Adaptive HTF-MPR searches for the best mapping in a hybrid approach. We applied the proposed methodology on two well-known image classifiers: VGG-16 and AlexNet. We also performed a full analysis of the solution space of MNIST Softmax. Our results demonstrate that Adaptive HTF-MPR outperforms the default homogeneous TensorFlow mapping. In addition to the speed up, Adaptive HTF-MPR can react to changes in the state of the system and adjust to an improved mapping.																	2157-6904	2157-6912				SEP	2020	11	5							55	10.1145/3396949													
J								Practical Privacy Preserving POI Recommendation	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Privacy preserving; decentralization; local differential privacy; secret sharing; POI recommendation	SYSTEM	Point-of-Interest (POI) recommendation has been extensively studied and successfully applied in industry recently. However, most existing approaches build centralized models on the basis of collecting users' data. Both private data and models are held by the recommender, which causes serious privacy concerns. In this article, we propose a novel Privacy preserving POI Recommendation (PriRec) framework. First, to protect data privacy, users' private data (features and actions) are kept on their own side, e.g., Cellphone or Pad. Meanwhile, the public data that need to be accessed by all the users are kept by the recommender to reduce the storage costs of users' devices. Those public data include: (1) static data only related to the status of POI, such as POI categories, and (2) dynamic data dependent on user-POI actions such as visited counts. The dynamic data could be sensitive, and we develop local differential privacy techniques to release such data to the public with privacy guarantees. Second, PriRec follows the representations of Factorization Machine (FM) that consists of a linear model and the feature interaction model. To protect the model privacy, the linear models are saved on the users' side, and we propose a secure decentralized gradient descent protocol for users to learn it collaboratively. The feature interaction model is kept by the recommender since there is no privacy risk, and we adopt a secure aggregation strategy in a federated learning paradigm to learn it. To this end, PriRec keeps users' private raw data and models in users' own hands, and protects user privacy to a large extent. We apply PriRec in real-world datasets, and comprehensive experiments demonstrate that, compared with FM, PriRec achieves comparable or even better recommendation accuracy.																	2157-6904	2157-6912				SEP	2020	11	5							52	10.1145/3394138													
J								Shapelet-transformed Multi-channel EEG Channel Selection	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										EEG channel selection; EEG shapelets; channel contribution; shapelet similarity minimization	BRAIN COMPUTER INTERFACES; TIME-SERIES; CLASSIFICATION; ALGORITHM	This article proposes an approach to select EEG channels based on EEG shapelet transformation, aiming to reduce the setup time and inconvenience for subjects and to improve the applicable performance of Brain-Computer Interfaces (BCIs). In detail, the method selects top-k EEG channels by solving a logistic loss-embedded minimization problem with respect to EEG shapelet learning, hyperplane learning, and EEG channel weight learning simultaneously. Especially, to learn distinguished EEG shapelets for weighting contributions of each EEG channel to the logistic loss, EEG shapelet similarity is also minimized during the procedure. Furthermore, the gradient descent strategy is adopted in the article to solve the non-convex optimization problem, which finally leads to the algorithm termed StEEGCS. In a result, classification accuracy, with those EEG channels selected by StEEGCS, is improved compared to that with all EEG channels, and classification time consumption is reduced as well. Additionally, the comparisons with several state-of-the-art EEG channel selection methods on several real-world EEG datasets also demonstrate the efficacy and superiority of StEEGCS.																	2157-6904	2157-6912				SEP	2020	11	5							58	10.1145/3397850													
J								Multi-Task Learning for Entity Recommendation and Document Ranking in Web Search	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Entity recommendation; document ranking; context-aware; personalized; multi-task learning; neural networks; Web search		Entity recommendation, providing users with an improved search experience by proactively recommending related entities to a given query, has become an indispensable feature of today's Web search engine. Existing studies typically only consider the query issued at the current timestep while ignoring the in-session user search behavior (short-term search history) or historical user search behavior across all sessions (long-term search history) when generating entity recommendations. As a consequence, they may fail to recommend entities of interest relevant to a user's actual information need. In this work, we believe that both short-term and long-term search history convey valuable evidence that could help understand the user's search intent behind a query, and take both of them into consideration for entity recommendation. Furthermore, there has been little work on exploring whether the use of other companion tasks in Web search such as document ranking as auxiliary tasks could improve the performance of entity recommendation. To this end, we propose a multi-task learning framework with deep neural networks (DNNs) to jointly learn and optimize two companion tasks in Web search engines: entity recommendation and document ranking, which can be easily trained in an end-to-end manner. Specifically, we regard document ranking as an auxiliary task to improve the main task of entity recommendation, where the representations of queries, sessions, and users are shared across all tasks and optimized by the multi-task objective during training. We evaluate our approach using large-scale, real-world search logs of a widely-used commercial Web search engine. We also performed extensive ablation experiments over a number of facets of the proposed multi-task DNN model to figure out their relative importance. The experimental results show that both short-term and long-term search history can bring significant improvements in recommendation effectiveness, and the combination of both outperforms using either of them individually. In addition, the experiments show that the performance of both entity recommendation and document ranking can be significantly improved, which demonstrates the effectiveness of using multi-task learning to jointly optimize the two companion tasks in Web search.																	2157-6904	2157-6912				SEP	2020	11	5							54	10.1145/3396501													
J								STARS: Defending against Sockpuppet-Based Targeted Attacks on Reviewing Systems	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Data mining and knowledge discovery; online commerce and recommendation systems; social and information networks	FAKE	Customers of virtually all online marketplaces rely upon reviews in order to select the product or service they wish to buy. These market places in turn deploy review fraud detection systems so that the integrity of reviews is preserved. A well-known problem with review fraud detection systems is their underlying assumption that the majority of reviews are honest-this assumption leads to a vulnerability where an attacker can try to generate many fake reviews of a product. In this article, we consider the case where a company wishes to fraudulently promote its product through fake reviews and propose the Sockpuppet-based Targeted Attack on Reviewing Systems (STARS for short). STARS enables an attacker to enter fake reviews for a product from multiple, apparently independent, sockpuppet accounts. We show that the STARS attack enables companies to successfully promote their product against seven recent, well-known review fraud detectors on four datasets (Amazon, Epinions, and the BitcoinAlpha and OTC exchanges) by significant margins. To protect against the STARS attack, we propose a new fraud detection algorithm called RTV. RTV introduces a new class of users (called trusted users) and also considers reviews left by verified users which were not considered in existing review fraud detectors. We show that RTV significantly mitigates the impact of the STARS attack across the four datasets listed above.																	2157-6904	2157-6912				SEP	2020	11	5							56	10.1145/3397463													
J								Mapping Points of Interest Through Street View Imagery and Paid Crowdsourcing	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Crowdsourcing; geographic information; geospatial information; micro-tasks; urban auditing; mapping	GEOGRAPHIC INFORMATION; VOLUNTEERS	We present the Virtual City Explorer (VCE), an online crowdsourcing platform for the collection of rich geotagged information in urban environments. Compared to other volunteered geographic information approaches, which are constrained by the number and availability of mapping enthusiasts on the ground, the VCE uses digital street imagery to allow people to virtually explore a city from anywhere in the world, using a browser or a mobile phone. In addition, contributions in VCE are designed as paid microtasks-small jobs that can be carried out without any specific knowledge of the local area or previous mapping expertise in exchange for a fee. We tested the VCE in two cities to map points of interest (PoIs) in transport and mobility, using FigureEight to recruit participants. We were able to show that our platform enables crowdworkers to submit PoI location seamlessly, cover almost all of the tested areas, and discover several PoIs not reported by other approaches. This allows the VCE to complement existing approaches that leverage experts or grassroot communities.																	2157-6904	2157-6912				SEP	2020	11	5							63	10.1145/3403931													
J								Cut-n-Reveal: Time Series Segmentations with Explanations	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Multivariate time series; spatio-temporal segmentation	MODELS; ALGORITHM	Recent hurricane events have caused unprecedented amounts of damage on critical infrastructure systems and have severely threatened our public safety and economic health. The most observable (and severe) impact of these hurricanes is the loss of electric power in many regions, which causes breakdowns in essential public services. Understanding power outages and how they evolve during a hurricane provides insights on how to reduce outages in the future, and how to improve the robustness of the underlying critical infrastructure systems. In this article, we propose a novel scalable segmentation with explanations framework to help experts understand such datasets. Ourmethod, CnR (Cut-n-Reveal), first finds a segmentation of the outage sequences based on the temporal variations of the power outage failure process so as to capture major pattern changes. This temporal segmentation procedure is capable of accounting for both the spatial and temporal correlations of the underlying power outage process. We then propose a novel explanation optimization formulation to find an intuitive explanation of the segmentation such that the explanation highlights the culprit time series of the change in each segment. Through extensive experiments, we show that our method consistently outperforms competitors in multiple real datasets with ground truth. We further study real county-level power outage data from several recent hurricanes (Matthew, Harvey, Irma) and show that CnR recovers important, non-trivial, and actionable patterns for domain experts, whereas baselines typically do not give meaningful results.																	2157-6904	2157-6912				SEP	2020	11	5							53	10.1145/3394118													
J								Moment-Guided Discriminative Manifold Correlation Learning on Ordinal Data	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Canonical correlation analysis; ordinal regression; l(p)-norm centers; manifold learning; moment	FEATURE-EXTRACTION; FACE RECOGNITION; REGRESSION; FUSION; SETS	Canonical correlation analysis (CCA) is a typical and useful learning paradigm in big data analysis for capturing correlation across multiple views of the same objects. When dealing with data with additional ordinal information, traditional CCA suffers from poor performance due to ignoring the ordinal relationships within the data. Such data is becoming increasingly common, as either temporal or sequential information is often associated with the data collection process. To incorporate the ordinal information into the objective function of CCA, the so-called ordinal discriminative CCA has been presented in the literature. Although ordinal discriminative CCA can yield better ordinal regression results, its performance deteriorates when data is corrupted with noise and outliers, as it tends to smear the order information contained in class centers. To address this issue, in this article we construct a robust manifold-preserved ordinal discriminative correlation regression (rmODCR). The robustness is achieved by replacing the traditional (l(2)-norm) class centers with lp-norm centers, where p is efficiently estimated according to the moments of the data distributions, as well as by incorporating the manifold distribution information of the data in the objective optimization. In addition, we further extend the robust manifold-preserved ordinal discriminative correlation regression to deep convolutional architectures. Extensive experimental evaluations have demonstrated the superiority of the proposed methods.																	2157-6904	2157-6912				SEP	2020	11	5							61	10.1145/3402445													
J								Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Deep learning; computer vision; face anti-spoofing; face recognition; domain adaptation		Face anti-spoofing aims to detect presentation attack to face recognition-based authentication systems. It has drawn growing attention due to the high security demand. The widely adopted CNN-based methods usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of novel patterns or unseen scenes, leading to poor generalization performance. Furthermore, almost all current methods treat face anti-spoofing as a prior step to face recognition, which prolongs the response time and makes face authentication inefficient. In this article, we try to boost the generalizability and applicability of face anti-spoofing methods by designing a new generalizable face authentication CNN (GFA-CNN) model with three novelties. First, GFA-CNN introduces a simple yet effective total pairwise confusion loss for CNN training that properly balances contributions of all spoofing patterns for recognizing the spoofing faces. Second, it incorporate a fast domain adaptation component to alleviate negative effects brought by domain variation. Third, it deploys filter diversification learning to make the learned representations more adaptable to new scenes. In addition, the proposed GFA-CNN works in a multi-task manner-it performs face anti-spoofing and face recognition simultaneously. Experimental results on five popular face anti-spoofing and face recognition benchmarks show that GFA-CNN outperforms previous face anti-spoofing methods on cross-test protocols significantly and also well preserves the identity information of input face images.																	2157-6904	2157-6912				SEP	2020	11	5							60	10.1145/3402446													
J								A Survey of Unsupervised Deep Domain Adaptation	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Domain adaptation; deep learning; generative adversarial networks	ROBUST	Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.																	2157-6904	2157-6912				SEP	2020	11	5							51	10.1145/3400066													
J								Querying Recurrent Convoys over Trajectory Data	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Recurrent convoy query; co-moving pattern; spatio-temporal index	GATHERING PATTERNS; DISCOVERY	Moving objects equipped with location-positioning devices continuously generate a large amount of spatiotemporal trajectory data. An interesting finding over a trajectory stream is a group of objects that are travelling together for a certain period of time. We observe that existing studies on mining co-moving objects do not consider an important correlation between co-moving objects, which is the reoccurrence of the co-moving pattern. In this study, we propose the problem of finding recurrent co-moving patterns from streaming trajectories, enabling us to discover recent co-moving patterns that are repeated within a given time period. Experimental results on real-life trajectory data verify the efficiency and effectiveness of our method.																	2157-6904	2157-6912				SEP	2020	11	5							59	10.1145/3400730													
J								Dancing with Trump in the Stock Market: A Deep Information Echoing Model	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Stock market prediction; information echoing; Trump; Twitter; deep learning	MEDIA; SENTIMENT; EARNINGS	It is always deemed crucial to identify the key factors that could have significant impact on the stock market trend. Recently, an interesting phenomenon has emerged that some of President Trump's posts in Twitter can surge into a dominant role on the stock market for a certain time period, although studies along this line are still in their infancy. Therefore, in this article, we study whether and how this new-rising information can help boost the performance of stock market prediction. Specifically, we have found that the echoing reinforced effect of financial news with Trump's market-related tweets can influence the market movement-that is, some of Trump's tweets directly impact the stock market in a short time, and the impact can be further intensified when it echoes with other financial news reports. Along this line, we propose a deep information echoing model to predict the hourly stock market trend, such as the rise and fall of the Dow Jones Industrial Average. In particular, to model the discovered echoing reinforced impact, we design a novel information echoing module with a gating mechanism in a sequential deep learning framework to capture the fused knowledge from both Trump's tweets and financial news. Extensive experiments have been conducted on the real-world U.S. stock market data to validate the effectiveness of our model and its interpretability in understanding the usability of Trump's posts. Our proposed deep echoing model outperforms other baselines by achieving the best accuracy of 60.42% and obtains remarkable accumulated profits in a trading simulation, which confirms our assumption that Trump's tweets contain indicative information for short-term market trends. Furthermore, we find that Trump's tweets about trade and political events are more likely to be associated with short-term market movement, and it seems interesting that the impact would not degrade as time passes.																	2157-6904	2157-6912				SEP	2020	11	5							62	10.1145/3403578													
J								A Discriminative Convolutional Neural Network with Context-aware Attention	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Text mining; convolution neural networks; attention method	FEATURE REPRESENTATION; DISTANCE; FUSION	Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting n-gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual n-gram features adaptively. In this article, we propose a discriminative CNN with context-aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model.																	2157-6904	2157-6912				SEP	2020	11	5							57	10.1145/3397464													
J								Emergent Jaw Predominance in Vocal Development Through Stochastic Optimization	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Optimization; Speech; Computational modeling; Robot sensing systems; Stochastic processes; Lips; Jaw predominance; sensorimotor control; stochastic optimization; vocal babbling; vocal development	SPEECH; EVOLUTION; LIP	Infant vocal babbling strongly relies on jaw oscillations, especially at the stage of canonical babbling, which underlies the syllabic structure of world languages. In this paper, we propose, model and analyze an hypothesis to explain this predominance of the jaw in early babbling. This hypothesis states that general stochastic optimization principles, when applied to learning sensorimotor control, automatically generate ordered babbling stages with a predominant exploration of jaw movements in early stages. The reason is that those movements impact the auditory effects more than other articulators. In previous computational models, such general principles were shown to selectively freeze and free degrees of freedom in a model reproducing the proximo-distal development observed in infant arm reaching. The contribution of this paper is to show how, using the same methods, we are able to explain such patterns in vocal development. We present three experiments. The two first ones show that the recruitment order of articulators emerging from stochastic optimization depends on the target sound to be achieved but that on average the jaw is largely chosen as the first recruited articulator. The third experiment analyses in more detail how the emerging recruitment order is shaped by the dynamics of the optimization process.																	2379-8920	2379-8939				SEPT	2020	12	3					378	389		10.1109/TCDS.2017.2704912													
J								The Role of Criticality of Gene Regulatory Networks in Morphogenesis	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Cell fate; criticality; gene regulatory network (GRN); morphogenetic pattern; morphogenetic system; random Boolean network (RBN)	BOOLEAN NETWORK; MODELS; ROBUSTNESS; EXPRESSION; DYNAMICS; GROWTH	Gene regulatory network (GRN)-based morphogenetic models have recently gained an increasing attention. However, the relationship between microscopic properties of intracellular GRNs and macroscopic properties of morphogenetic systems has not been fully understood yet. Here, we propose a theoretical morphogenetic model representing an aggregation of cells, and reveal the relationship between criticality of GRNs and morphogenetic pattern formation. In our model, the positions of the cells are determined by spring-mass-damper kinetics. Each cell has an identical Kauffman's NK random Boolean network (RBN) as its GRN. We varied the properties of GRNs from ordered, through critical, to chaotic by adjusting node in-degree K. We randomly assigned four cell fates to the attractors of RBNs for cellular behaviors. By comparing diverse morphologies generated in our morphogenetic systems, we investigated what the role of the criticality of GRNs is in forming morphologies. We found that nontrivial spatial patterns were generated most frequently when GRNs were at criticality. Our finding indicates that the criticality of GRNs facilitates the formation of nontrivial morphologies in GRN-based morphogenetic systems.																	2379-8920	2379-8939				SEPT	2020	12	3					390	400		10.1109/TCDS.2018.2876090													
J								A Computational Model for Child Inferences of Word Meanings via Syntactic Categories for Different Ages and Languages	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Syntactics; Hidden Markov models; Computational modeling; Semantics; Motion pictures; Neurons; Visualization; Computational model; cross-linguistic difference; hidden Markov model (HMM); learning of word meanings; syntactic category	YOUNG-CHILDREN; MOTHERS SPEECH; ACQUISITION; VERBS; INFANTS; NOUNS; CONSTRAINTS; LIMITATIONS; VOCABULARY; EMERGENCE	Children exploit their morphosyntactic knowledge in order to infer the meanings of words. A recent behavioral study has reported developmental changes in word learning from three to five years of age, with respect to a child's native language. To understand the computational basis of this phenomenon, we propose a model based on a hidden Markov model (HMM). The HMM acquires syntactic categories of given words as its hidden states, which are associated with observed features. Then, the model infers the syntactic category of a new word, which facilitates the selection of an appropriate visual feature. We hypothesize that using this model with different numbers of categories can replicate the manner in which children of different ages learn words. We perform simulation experiments in three native language environments (English, Japanese, and Chinese), which demonstrate that the model produces similar performances as the children in each environment. Allowing a larger number of categories means that the model can acquire a sufficient number of obvious categories, which results in the successful inference of visual features for novel words. In addition, cross-linguistic differences originating from the acquisition of language-specific syntactic categories are identified, i.e., the syntactic categories learned from English and Chinese corpora are relatively reliant on word orders, whereas the Japanese-trained model exploits morphological cues to infer the syntactic categories.																	2379-8920	2379-8939				SEPT	2020	12	3					401	416		10.1109/TCDS.2018.2883048													
J								Adaptive Image-Based Visual Servoing for Hovering Control of Quad-Rotor	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Hovering control; image-based visual servoing (IBVS); quad-rotor; reinforcement learning (RL)	POSITION	Image-based visual servoing (IBVS) achieves precise positioning and motion control for a relatively stationary target by visual feedback, but problems persist with convergence and stability. Appropriate servoing gains for the IBVS are critical to the convergence and stability, but this control gain is heuristically a constant for most IBVS applications. This paper proposes an integrated method that allows adaptive adjustment of the servoing gain by reinforcement learning (RL) for IBVS control. The proposed method learns a policy to determine the value of the servoing gain on the fly. To ensure rapid convergence for the RL, truncating Q-learning (TQL) with faster convergence is used as learning algorithm, which uses truncated temporal differences (TDs) to update the TD. A nonuniform state space partitioning as a state encoder for RL allows more efficient policy. A strategy that uses the Metropolis derived from the simulated annealing is introduced for selecting the action, in order to balance exploration and exploitation so as to accelerate the learning speed. The integrated IBVS control system is tested using experiments involving a quad-rotor helicopter hovering control. The results of simulation and experiment show that the integrated IBVS method increases stability and ensures more rapid convergence than other methods.																	2379-8920	2379-8939				SEPT	2020	12	3					417	426		10.1109/TCDS.2019.2908923													
J								Optimization of Output Spike Train Encoding for a Spiking Neuron Based on its Spatio-Temporal Input Pattern	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Neurons; Task analysis; Encoding; Spatiotemporal phenomena; Biological information theory; Firing; Classification algorithms; Encoding; learning; spatio-temporal patterns; spike trace; spike train; spiking neural network (SNN)	NETWORKS; CLASSIFICATION; RESUME	A common learning task for a spiking neuron is to map a spatio-temporal input pattern to a target output spike train. There is no prescribed method for selection of the target output spike train. However, the precise spiking pattern of the target output spike train (output encoding) can affect the learning performance of the spiking neuron. Therefore, systematic methods of finding the optimum spiking pattern for a target output spike train that can be learned by spiking neurons are needed. Here, a method is proposed to adaptively adjust an initial suboptimal output encoding during different learning epochs to find the optimal output encoding. A time varying value of a local event called a spike trace is used to calculate the amount of a required adjustment. The remote supervised method (ReSuMe) learning algorithm is used to train the weights, and the proposed method is used for finding optimized output encoding (optimized desired spikes). Experimental results show that optimizing the output encoding during the learning phase increases the accuracy. The proposed method was applied to find optimized output encoding in classification tasks and the results revealed improvements up to 16.5% in accuracy compared to when using the non-adapted method. It also increases the accuracy in a classification task from 90% to 100%.																	2379-8920	2379-8939				SEPT	2020	12	3					427	438		10.1109/TCDS.2019.2909355													
J								Interactive Question-Posing System for Robot-Assisted Reminiscence From Personal Photographs	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Task analysis; Visualization; Robot sensing systems; Knowledge based systems; Data models; Interactive question-posing; knowledge graph; Markov random fields (MRFs); reminiscence; social companion robot		Reminiscence is a lifelong activity that happens throughout our lifespan. While memories can serve as topics in people's everyday conversations, recalling the past can also help us build self-esteem and increase our level of happiness. In this paper, we aim to develop a robot companion that helps people to recollect their memories from personal photographs. We focus on how a robot can associate concepts relevant to the content in the photographs and evoke people's memories by asking questions that are both relatable and engaging. To understand the content of a picture, we applied deep learning techniques in order to recognize events, objects, and scenes in it. Then, these observations and any user utterances are considered in a Markov random field (MRF)-based algorithm that contains common sense knowledge of a number of events, with loopy belief propagation being used to infer possible associated concepts and topics. Afterward, the robot poses appropriate questions about the selected topics, guiding the user to reminisce through conversation. Our results show that the proposed system can pose related and appropriate questions to interact with the user, and has the potential guide the user to recall the past in an organized way.																	2379-8920	2379-8939				SEPT	2020	12	3					439	450		10.1109/TCDS.2019.2917030													
J								Four-Dimensional Modeling of fMRI Data via Spatio-Temporal Convolutional Neural Networks (ST-CNNs)	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Functional magnetic resonance imaging; Brain modeling; Dictionaries; Data models; Task analysis; Machine learning; Sparse matrices; Deep learning; functional brain networks; functional magnetic resonance imaging (fMRI)	DEFAULT-MODE; INDEPENDENT COMPONENT; FUNCTIONAL CONNECTIVITY; BRAIN	Since the human brain functional mechanism has been enabled for investigation by the functional magnetic resonance imaging (fMRI) technology, simultaneous modeling of both the spatial and temporal patterns of brain functional networks from 4-D fMRI data has been a fundamental but still challenging research topic for neuroimaging and medical image analysis fields. Currently, general linear model (GLM), independent component analysis (ICA), sparse dictionary learning, and recently deep learning models, are major methods for fMRI data analysis in either spatial or temporal domains, but there are few joint spatial-temporal methods proposed, as far as we know. As a result, the 4-D nature of fMRI data has not been effectively investigated due to this methodological gap. The recent success of deep learning applications for functional brain decoding and encoding greatly inspired us in this paper to propose a novel framework called spatio-temporal convolutional neural network (ST-CNN) to extract both spatial and temporal characteristics from targeted networks jointly and automatically identify of functional networks. The identification of default mode network (DMN) from fMRI data was used for evaluation of the proposed framework. Results show that only training the framework on one fMRI data set is sufficiently generalizable to identify the DMN from different data sets of different cognitive tasks and resting state. Further investigation of the results shows that the joint-learning scheme can capture the intrinsic relationship between the spatial and temporal characteristics of DMN and thus it ensures the accurate identification of DMN from independent data sets. The ST-CNN model brings new tools and insights for fMRI analysis in cognitive and clinical neuroscience studies.																	2379-8920	2379-8939				SEPT	2020	12	3					451	460		10.1109/TCDS.2019.2916916													
J								Deep Residual Network With Adaptive Learning Framework for Fingerprint Liveness Detection	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Deep convolutional neural network (DCNN); fingerprint liveness detection (FLD); LivDet; local gradient pattern (LGP); multilayer perceptron (MLP); region of interest (ROI)	TEXTURE CLASSIFICATION; FEATURES	Today, fingerprint recognition technology has aroused wide attention in the society, especially in the application of identity authentication with a smartphone as a carrier. However, the disadvantage of these devices is that the identification sensors are vulnerable to spoofing attacks from artificial replicas made from clay, gelatin, silicon, etc. To resolve it, a feasible anti-deception countermeasure, called fingerprint liveness detection (FLD), has been proposed. Different from most shallow feature methods, the deep convolutional neural network (DCNN)-based FLD methods have been widely explored with the properties of fast operation, few parameters, and end-to-end feature self-learning. Meanwhile, DCNN faces a pair of contradictory problems, on the one hand, the training accuracy will keep rising with the increasement of multilayer perceptron (MLP), finally tends to a stable value. Continue to increase the number of MLP, results will decline. Much research, on the other hand, shows that the number of MLP is the foundation for realizing a high performance detection. Hereby, we apply deep residual network (DRN) to FLD for the first time to solve the contradiction mentioned in this paper. Next, to eliminate the interference of invalid regions of given images, a region-of-interest (ROI) extraction algorithm is put forward. Afterward, to avoid the parameters learned plunging into local optimization, adaptive learning-based DRNs (ALDRNs), which automatically adjust the learning rate if those monitoring parameters (verification accuracy) are stable, are explored. Finally, we propose a novel texture enhancement based on the local gradient pattern (LGP) method to improve the generalization of a model classifier as well. Experimental results on three benchmark data sets: LivDet 2011, 2013, and 2015, show that our results outperform the state-of-the-art FLD methods.																	2379-8920	2379-8939				SEPT	2020	12	3					461	473		10.1109/TCDS.2019.2920364													
J								Policy Sharing Using Aggregation Trees for Q-Learning in a Continuous State and Action Spaces	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Multiagent system; policy sharing; Q-learning; tree structure		Q-learning is a generic approach that uses a finite discrete state and an action domain to estimate action values using tabular or function approximation methods. An intelligent agent eventually learns policies from continuous sensory inputs and encodes these environmental inputs onto a discrete state space. The application of Q-learning in a continuous state/action domain is the subject of many studies. This paper uses a tree structure to approximate a Q-function using in a continuous state domain. The agent selects a discretized action with a maximum Q-value and this discretized action is then extended to a continuous action using an action bias function. Reinforcement learning is difficult for a single agent when the state space is huge. This proposed architecture is also applied to a multiagent system, wherein an individual agent transfers its useful Q-values to other agents to accelerate the learning process. Policy is shared between agents by grafting the branches of trees in which Q-values are stored to other trees. The results for simulation show that the proposed architecture performs better than tabular Q-learning and significantly accelerates the learning process because all agents use the sharing mechanisms to cooperate with each other.																	2379-8920	2379-8939				SEPT	2020	12	3					474	485		10.1109/TCDS.2019.2926477													
J								Affective EEG-Based Person Identification Using the Deep Learning Approach	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Electroencephalography; Logic gates; Task analysis; Deep learning; Feature extraction; Brain modeling; Biometrics (access control); Affective computing; biometrics; convolutional neural networks (CNNs); deep learning (DL); electroencephalography (EEG); long short-term memory (LSTM); personal identification (PI); recurrent neural networks (RNNs)	NEURAL-NETWORKS; SIGNALS; RECOGNITION; BIOMETRICS	Electroencephalography (EEG) is another method for performing person identification (PI). Due to the nature of the EEG signals, EEG-based PI is typically done while a person is performing a mental task such as motor control. However, few studies used EEG-based PI while the person is in different mental states (affective EEG). The aim of this paper is to improve the performance of affective EEG-based PI using a deep learning (DL) approach. We proposed a cascade of DL using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). CNNs are used to handle the spatial information from the EEG while RNNs extract the temporal information. We evaluated two types of RNNs, namely long short-term memory (LSTM) and gated recurrent unit (GRU). The proposed method is evaluated on the state-of-the-art affective data set DEAP. The results indicate that CNN-GRU and CNN-LSTM can perform PI from different affective states and reach up to 99.90%-100% mean correct recognition rate. This significantly outperformed a support vector machine baseline system that used power spectral density features. Notably, the 100% mean CRR came from 32 subjects in DEAP data set. Even after the reduction of the number of EEG electrodes from 32 to 5 for more practical applications, the model could still maintain an optimal result obtained from the frontal region, reaching up to 99.17%. Amongst the two DL models, we found that CNN-GRU and CNN-LSTM performed similarly while CNN-GRU expended faster training time. In conclusion, the studied DL approaches overcame the influence of affective states in EEG-Based PI reported in the previous works.																	2379-8920	2379-8939				SEPT	2020	12	3					486	496		10.1109/TCDS.2019.2924648													
J								Sampling-Tree Model: Efficient Implementation of Distributed Bayesian Inference in Neural Networks	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Bayes methods; Neurons; Biological neural networks; Computational modeling; Brain modeling; Probabilistic logic; Integrated circuit modeling; Bayesian inference; importance sampling; neural network; probabilistic population coding (PPC); sampling-tree model (STM)	COMPUTATIONAL PRINCIPLES; BELIEF PROPAGATION; NEURONAL GAIN; CIRCUIT; PROBABILITY; UNCERTAINTY; CONFIDENCE	Experimental observations from neuroscience have suggested that the cognitive process of human brain is realized as probabilistic reasoning and further modeled as Bayesian inference. However, it remains unclear how Bayesian inference could be implemented by network of neurons in the brain. Here a novel implementation of neural circuit, named the sampling-tree model, is proposed to fulfill this aim. By using a deep tree structure to implement sampling with simple and stackable basic neural network motifs for any given Bayesian networks, one can perform local inference while guaranteeing the accuracy of global inference. We show that these task-independent motifs can be used in parallel for fast inference without intensive iteration and scale-limitation. As a result, this model utilizes the structure benefit of neuronal system, i.e., neuronal abundance and multihierarchy, to perform fast inference in an extendable way.																	2379-8920	2379-8939				SEPT	2020	12	3					497	510		10.1109/TCDS.2019.2927808													
J								Scene-Specific Multiple Cues Integration for Multiperson Tracking	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Target tracking; Color; Trajectory; Supervised learning; Task analysis; Feature extraction; Shape; Data association; multicommodity network; multiperson tracking; multiple cues integration; scene-specific	MULTIOBJECT TRACKING; MULTITARGET TRACKING; CONTEXT	Robust multiperson tracking requires the correct associations of online detection responses with existing trajectories. In this paper, we propose to integrate multiple cues to resolve the ambiguities in data association for multiperson tracking. Unlike most existing algorithms which integrate multiple cues in the same manner for different scenes, we learn scene-specific parameters to integrate multiple cues for different scenes, as the discriminative power of each cue may vary in different scenes. The scene-specific integration parameters are learned offline by supervised learning method. Min-cost multicommodity flow is employed to solve the data association task. The edge cost of the multicommodity network, which is crucial for the data association, is determined by integrating the multiple cues extracted from the detection response based on the learned scene-specific integration parameters. The experimental results on public multiperson tracking data set demonstrate the effectiveness of the proposed scene-specific integration method.																	2379-8920	2379-8939				SEPT	2020	12	3					511	518		10.1109/TCDS.2019.2928338													
J								Decentralized Energy-Aware Co-Planning of Motion and Communication Strategies for Networked Mobile Robots	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Mobile robots; Task analysis; Collision avoidance; Robot kinematics; Planning; Wireless communication; Decentralized control; energy conservation; networked mobile robots; path planning; wireless communication	CONNECTIVITY; SYSTEMS	In this article, a decentralized planning scheme is proposed to determine simultaneously communication and motion strategies for a team of mobile robots. These robots accomplish a collection of target visiting tasks in a complex environment with optimal energy consumption and guaranteed end-to-end connectivity. Information generated during the team deployment is transmitted to an operation center via a multihop wireless network whose channels are modeled by stochastic variables. For each announced task, mobile robots adopt different roles depending on the task's nature and the team's current configuration; then, each robot determines its communication and motion policies by solving a convex optimization problem. Avoiding inter-robot collisions and obstacles is also taken into account. The suggested approach leads to the efficient use of available robots and their energy resources compared to the rival methods in the literature. Effectiveness of the proposed algorithm is illustrated by computer simulations.																	2379-8920	2379-8939				SEPT	2020	12	3					519	528		10.1109/TCDS.2019.2932751													
J								An Associative-Memory-Based Reconfigurable Memristive Neuromorphic System With Synchronous Weight Training	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Neuromorphics; Neurons; Associative memory; Memristors; Logic gates; Threshold voltage; Training; Associative memory; backpropagation (BP) algorithm; memristive neuromorphic system; memristor; reconfigurable neuromorphic system; synchronous weight training	MULTILAYER NEURAL-NETWORKS; WITHDRAWAL REFLEX; IMPLEMENTATION; ARCHITECTURE; CROSSBAR; FPGA	Memristive neuromorphic systems are emerging potential hardware platforms to implement artificial neural networks. Combining features of memristive neuromorphic systems with associative memory, this article proposes an associative-memory-based reconfigurable memristive neuromorphic system. In the proposed system, there are two neural networks: 1) the neural network for computing acceleration and 2) the neural network mimicking associative memory. Then, a case study of the system is presented, which includes an associative memory network to realize apple recognition and a computing acceleration network for iris classification. The associative memory network depends on associative learning to achieve the recognition function. In addition, during the corresponding forgetting process, the connections of the related synaptic circuits are cut off and sent to a synaptic circuit block, realizing variable circuit topology. Further, the synaptic circuits in the block are applied to construct the iris classification network, accomplishing the reconfiguration of the proposed system. The circuit structure of this classification network matches backpropagation (BP) algorithm well. Meanwhile, the network reaches a relatively high classification accuracy after training. In an iteration of the training, all the synaptic circuits that need to change can adjust weights synchronously, which improves training speed.																	2379-8920	2379-8939				SEPT	2020	12	3					529	540		10.1109/TCDS.2019.2932179													
J								Sentiment Cognition From Words Shortlisted by Fuzzy Entropy	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Fuzzy entropy; human cognition; long short-term memory (LSTM); sentiment analysis (SA)	CLASSIFICATION	Sentiment analysis (SA) is the process of evaluating human emotions, opinions, and reviews expressed in text to detect the writer's mental outlook toward a particular event, topic, product, service, etc., and assign a relevant sentiment. In SA, to highlight the correct words which contribute toward sentiment cognition is very difficult. Simulating this task of shortlisting of words by human observers is challenging due to complexity of human mind's processing. The use of fuzzy entropy is proposed in this article as an innovative step to tap sentiment quotients of online movie reviews. We have proposed a novel approach of shortlisting of words that help in sentiment cognition using a combination of fuzzy entropy, k-means clustering, and sentiment lexicon SentiWordNet. We have addressed this challenging task of simulating the human cognition of words by developing a model that recognizes sentiment based on fuzzy scores derived from SentiWordNet in an automatic manner. Experiments on two benchmark movie review data sets-IMDB and polarity data sets by Pang and Lee-with training by long short-term memory neural networks, yields high accuracy for our approach as compared to other state-of-the-art-methods of SA.																	2379-8920	2379-8939				SEPT	2020	12	3					541	550		10.1109/TCDS.2019.2937796													
J								Weighted Ensemble of Deep Convolution Neural Networks for Single-Trial Character Detection in Devanagari-Script-Based P300 Speller	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Brain-computer interface (BCI); deep convolution neural network (DCNN); Devanagari-script-input-based P300 speller (DS-P3S); ensemble learning; single trial	BCI COMPETITION 2003; DATA SET IIB; CLASSIFICATION; PERFORMANCE; SIGNAL; MODEL	The existing Devanagari-script-input-based P300 speller (DS-P3S) performs better mostly with 3-15 trials. This leads to poor information transfer rate (ITR) and a major concern in its real-time adaptation. In DS-P3S, the display paradigm is a matrix of 8x8 size which has 28 more characters than the 6x6 English paradigm. The increased number of characters leads to user-related issues such as a crowding effect, double flashing, adjacency distraction, task difficulty, and fatigue which increases the false detection rate. To tackle this, we propose an efficient single-trial character detection approach for DS-P3S using weighted ensemble of deep convolution neural networks (WE-DCNNs). The weighted strategy is constructed based on measured ensemble diversity to counter the instability by the individual classifier. Additionally, to reduce the false detection rate arising from a single trial, a new channel dropout-based character detection approach is introduced first time in this article. The ITR of 55.45 b/min and an average P300 classification accuracy of 92.64% achieved are comparatively higher than existing methods of DS-P3S. The significant reduction in tradeoff between bias and variance for the different subjects affirms the ease of applicability of the proposed model with just a single trial.																	2379-8920	2379-8939				SEPT	2020	12	3					551	560		10.1109/TCDS.2019.2942437													
J								Path Planning in Multiple-AUV Systems for Difficult Target Traveling Missions: A Hybrid Metaheuristic Approach	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Task analysis; Planning; Resource management; Trajectory; Sea surface; Computer science; Ant colony optimization (ACO); autonomous robotics; autonomous underwater vehicle (AUV); differential evolution (DE); target traveling mission	AUTONOMOUS UNDERWATER VEHICLES; ANT COLONY OPTIMIZATION; DIFFERENTIAL EVOLUTION; LOCAL SEARCH	Multiple autonomous underwater vehicles (AUVs) are popular for challenging submarine missions. In this article, we focus on the multi-AUV path planning for a common class of missions that need to traverse lots of mission targets in large and complex environments. Given that AUVs are often launched from a movable surface vehicle, e.g., a ship, a multi-AUV target traveling problem is formulated with a requirement of surface point location. Thereafter, a hybrid metaheuristic approach is developed by sequentially performing a cube-based environment modeling, cost map building, voyage planning, and detailed trajectory planning. Specifically, the shortest path faster algorithm (SPFA) is adopted to build a cost map among targets and candidate surface points, and the A* search is utilized for trajectory planning. The optimality of both SPFA and A* can indeed be guaranteed. Thus, voyage planning becomes critical and an algorithm namely DE-C-ACO is proposed by combining the ant colony optimization (ACO) and the differential evolution (DE) with a cluster-based adjustment strategy, i.e., DE-C. DE-C and ACO evolve in parallel for surface point location and voyage generation, respectively. Experiments based on realistic bathymetries are conducted and the results validate the effectiveness and efficiency of the proposed DE-C-ACO.																	2379-8920	2379-8939				SEPT	2020	12	3					561	574		10.1109/TCDS.2019.2944945													
J								A Circadian Rhythms Learning Network for Resisting Cognitive Periodic Noises of Time-Varying Dynamic System and Applications to Robots	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Mathematical model; Time-varying systems; Biological neural networks; Circadian rhythm; Noise reduction; Circadian rhythms; convergence; large errors; neural network; online equation solving; time-varying problem	NEURAL-NETWORKS; EQUATION; INEQUALITY; STABILITY	Time-varying dynamic system contaminated by cognitive noises is universal in the fields of engineering and science. In this article, a circadian rhythms learning network (CRLN) is proposed and investigated for disposing the noise disturbed time-varying dynamic system. To do so, a vector-error function is first defined. Second, a neural dynamic model is formulated. Third, a co-state matrix is integrated into the model, of which the states are the linear combination of the previous periodic states and errors, which can effectively suppress periodic noises. Theoretical analysis and mathematical derivation prove the global exponential convergence performance of the proposed CRLN model. Finally, a practical noise disturbed time-varying dynamic system example with four different noises illustrates the accuracy and efficacy of the proposed CRLN model. Comparisons with traditional zeroing neural network further verify the advantages of the proposed CRLN model.																	2379-8920	2379-8939				SEPT	2020	12	3					575	587		10.1109/TCDS.2019.2948066													
J								Real-Time Fall Detection Using Uncalibrated Fisheye Cameras	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Cameras; Calibration; Feature extraction; Three-dimensional displays; Detection algorithms; Hidden Markov models; Head; Computer vision; fall detection event; fisheye cameras; multicamera feature extraction; rule-based recognition	DETECTION SYSTEM; VOXEL PERSON; VIDEO	In this article, we describe an approach for the problem of fall detection among the several other everyday activities in indoor environment, using three uncalibrated fisheye cameras. The proposed methodology requires the input segmented silhouettes from the three simultaneously acquired frames, and it is capable of detecting fall events in every location of the imaged environment. The presented algorithm uses the model of fisheye image formation that is based on the spherical projection followed by central projection. Under this model, vertical structures are imaged as straight lines passing through the center of field of view, by a camera with approximately vertical optical axis. The main advantages of this article are the simplicity of the detecting rule, the speed of execution, and the utilization of heterogeneous omnidirectional cameras that allows simultaneous imaging along any direction. The proposed algorithm is designed and parameterized using an extensive data set of synthetic frames. The results from the real videos are presented using the frame statistics and the event-based statistics. The proposed algorithm correctly detects the fall events within standing or walking, as well as other nonfalling activities.																	2379-8920	2379-8939				SEPT	2020	12	3					588	600		10.1109/TCDS.2019.2948786													
J								Reducing Redundancy of Musculoskeletal Robot With Convex Hull Vertexes Selection	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Muscles; Robots; Mathematical model; Hardware; Redundancy; Numerical models; Convex hull vertexes; human-like robot; motion experiment; musculoskeletal system; reduce redundancy	COMPUTED MUSCLE CONTROL; DYNAMIC SIMULATIONS; UPPER EXTREMITY; MODEL; OPTIMIZATION; SYNERGIES	It has been a long-term dream of roboticist to create a robot that has similar behavior and appearance to human. With highly redundant joints and actuators, the musculoskeletal system of human can fulfill movement and operation with high precision, flexibility, and reliability. However, the redundancy of the musculoskeletal system also brings great difficulties to the control and fabrication of a human-like robot. Therefore, we propose an algorithm based on the convex hull theory to effectively reduce the number of redundant muscles. Specifically, taking the strength limitation and state-dependent characteristics of muscles into consideration, by analyzing the torque contribution of muscles for the discrete motion of joints, each muscle can be represented by a feature vector, and the convex hulls are formed by these vectors in high-dimensional space. By applying the proposed algorithm, the muscles corresponding to the vertexes of the convex hull are reserved while the redundant muscles within the convex hull are deleted, and a simplified model can be obtained. Sets of motion experiments demonstrate that the simplified model can achieve high-precision movement as a complete model. Based on the simplified model, a hardware platform of a robotic arm is constructed, which consists of six degrees of freedom and 11 pneumatic muscles.																	2379-8920	2379-8939				SEPT	2020	12	3					601	617		10.1109/TCDS.2019.2953642													
J								Exploration of Subjective Color Perceptual-Ability by EEG-Induced Type-2 Fuzzy Classifiers	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Color; Colored noise; Electroencephalography; Feature extraction; Image color analysis; Fuzzy sets; Uncertainty; Color perceptual-ability; exact low-resolution electromagnetic topographic analysis (e-LORETA); type-2 fuzzy classifiers; type-2 fuzzy sets	INTERVAL TYPE-2; LOGIC SYSTEMS; SETS; OPTIMIZATION; PERFORMANCE	Perceptual-ability informally refers to the ability of a person to recognize a stimulus. This article deals with color perceptual-ability measurement of subjects using brain response to basic color (red, green, and blue) stimuli. It also attempts to determine the subjective ability to recognize the base colors in the presence of noise tolerance of the base colors, referred to as recognition tolerance. Because of intrasession and intersession variations in subjective brain signal features for a given color stimulus, there exists uncertainty in perceptual-ability. In addition, small variations in the color stimulus result in wide variations in brain signal features, introducing uncertainty in the perceptual-ability of the subject. Type-2 fuzzy logic has been employed to handle the uncertainty in color perceptual-ability measurements due to: 1) variations in brain signal features for a given color and 2) the presence of colored noise on the base colors. Because of the limited power of uncertainty management of the interval type-2 fuzzy sets and high computational overhead of its general type-2 counterpart, we develop a semi-general type-2 fuzzy classifier to recognize the base color. It is important to note that the proposed technique transforms a vertical slice-based general type-2 fuzzy set (GT2FS) into an equivalent interval type-2 counterpart to reduce the computational overhead, without losing the contributions of the secondary memberships. The proposed Semi-GT2FSs-induced classifier yields superior performance in classification accuracy with respect to existing type-1, type-2, and other well-known classifiers. The brain-understanding of a perceived base or noisy base colors is also obtained by exact low-resolution electromagnetic topographic analysis (e-LORETA) software. This is used as the reference for our experimental results of the semi-general type-2 classifier in color perceptual-ability detection. Statistical tests undertaken confirm the superiority of the proposed classifier over its competitors. The proposed technique is expected to have interesting applications in identifying people with excellent color perceptual-ability for chemical, pharmaceutical, and textile industries.																	2379-8920	2379-8939				SEPT	2020	12	3					618	635		10.1109/TCDS.2019.2959138													
J								A Multitier Reinforcement Learning Model for a Cooperative Multiagent System	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Cooperation game; dilemma; multiagent systems; Nash bargaining solution (NBS); Q-learning; reinforcement learning		In multiagent cooperative systems with value-based reinforcement learning, agents learn how to complete the task by an optimal policy learned through value-policy improvement iterations. But how to design a policy that avoids cooperation dilemmas and comes to a common consensus between agents is an important issue. A method that improves the coordination ability of agents in cooperative systems by assessing the cooperative tendency and increases the collective payoff by candidate policy is proposed in this article. The method learns the cooperative rules by recording the cooperation probabilities for agents in a multitier reinforcement learning model. The candidate action sets are selected through the candidate policy which considers the payoff of the coalition. Then, the optimal strategy is selected through the Nash bargaining solution (NBS) from these candidate action sets. The method is tested using two cooperative tasks. The results show that the proposed algorithm, which addresses the instability and ambiguity in a win or learning fast policy hill-climbing (WoLF-PHC) and requires significantly less memory space than the NBS, is more stable and more efficient than other methods.																	2379-8920	2379-8939				SEPT	2020	12	3					636	644		10.1109/TCDS.2020.2970487													
J								A Method for Napping Time Recommendation Using Electrical Brain Activity	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Sleep; Task analysis; Electroencephalography; Electromyography; Schedules; Standards; Electrooculography; Electroencephalogram (EEG); enhancement; finger tapping; motor; napping; procedural memory; sequential finger movements; sleep inertia; sleepiness	NOCTURNAL SLEEP; AUTOMATIC-ANALYSIS; MEMORY; NAP; EEG; PERFORMANCE; INCREASES; AFTERNOON; SPINDLES	Napping in the workplace has become popular. Knowing how to nap for brain benefits is important. We designed a nap experiment to investigate how napping after different sleep stages impacts procedural memory and sleepiness. In total, 45 nonhabitual nappers were randomly assigned to the Wake group (no napping), N2 group (napping and being woken after enough N2 sleep), and slow-wave sleep (SWS) group (napping and being woken after the end of the first cycle of slow-wave sleep). The results show that the N2 group produces benefits in procedural memory consolidation and sleepiness reduction. In contrast, the SWS group had a lower behavioral performance than the N2 group and their sleepiness. The Wake group had lower performance and higher sleepiness score than the other groups. The results suggest that the ideal napping time is 10-20 min of N2 sleep. Considering that people's sleep-onset time might be different, we developed a napping time suggestion system using a single-channel electroencephalogram signal. The testing results show that the difference between a 10-min nap of N2 sleep calculated by our system and by an expert is only 0.45 min on average, which demonstrates the feasibility of waking people up at the right time.																	2379-8920	2379-8939				SEPT	2020	12	3					645	657		10.1109/TCDS.2020.2991176													
J								Models of Cross-Situational and Crossmodal Word Learning in Task-Oriented Scenarios	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Visualization; Robots; Task analysis; Data models; Linguistics; Cameras; Tracking; Artificial intelligence; cognitive robotics; intelligent robots; intelligent systems; multimodal word learning	LANGUAGE-ACQUISITION; MULTIMODAL MOTHERESE; DIRECTED SPEECH; VERBS; DISCOURSE; OBJECT	We present two related but different cross-situational and crossmodal models of incremental word learning. Model 1 is a Bayesian approach for co-learning object-word mappings and referential intention which allows for incremental learning from only a few situations where the display of referents to the learning system is systematically varied. We demonstrate the robustness of the model with respect to sensory noise, including errors in the visual (object recognition) and auditory (recognition of words) systems. The model is then integrated with a cognitive robotic architecture in order to realize cross-situational word learning on a robot. A different approach to word learning is demonstrated with Model 2, an information-theoretic model for the object- and action-word learning from modality rich input data based on pointwise mutual information. The approach is inspired by insights from language development and learning where the caregiver/teacher typically shows objects and performs actions to the infant while naming what the teacher is doing. We demonstrate the word learning capabilities of the model, feeding it with crossmodal input data from two German multimodal corpora which comprise visual scenes of performed actions and related utterances.																	2379-8920	2379-8939				SEPT	2020	12	3					658	668		10.1109/TCDS.2020.2995045													
J								Robot Manipulation in Open Environments: New Perspectives	IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS										Robots; Task analysis; Training; Deep learning; Tools; Cognition; Commonsense reasoning; knowledge representation; robot manipulation	TOOL USE; KNOWLEDGE	The problem of performing everyday manipulation tasks robustly in open environments is currently beyond the capabilities of artificially intelligent robots; humans are required. The difficulty arises from the high variability in open environments; it is not feasible to program for, or train for, every variation. This correspondence paper presents the case for a new approach to the problem, based on three mutually dependent ideas: 1) highly transferable manipulation skills; 2) choice of representation: a scene can be modeled in several different ways; and 3) top-down processes by which the robot's task can influence the bottom-up processes interpreting a scene. The approach we advocate is supported by evidence from what we know about humans, and also the approach is implicitly taken by human designers in designing representations for robots. We present brief results of an implementation of these ideas in robot vision, and give some guidelines for how the key ideas can be implemented more generally in practical robot systems.																	2379-8920	2379-8939				SEPT	2020	12	3					669	675		10.1109/TCDS.2019.2921098													
J								Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Training; Task analysis; Robustness; Cost function; Neurons; Neural networks; Deep learning; information bottleneck; representation learning; regularization; classification; neural networks; stochastic neural networks	DIMENSION	In this theory paper, we investigate training deep neural networks (DNNs) for classification via minimizing the information bottleneck (IB) functional. We show that the resulting optimization problem suffers from two severe issues: First, for deterministic DNNs, either the IB functional is infinite for almost all values of network parameters, making the optimization problem ill-posed, or it is piecewise constant, hence not admitting gradient-based optimization methods. Second, the invariance of the IB functional under bijections prevents it from capturing properties of the learned representation that are desirable for classification, such as robustness and simplicity. We argue that these issues are partly resolved for stochastic DNNs, DNNs that include a (hard or soft) decision rule, or by replacing the IB functional with related, but more well-behaved cost functions. We conclude that recent successes reported about training DNNs using the IB framework must be attributed to such solutions. As a side effect, our results indicate limitations of the IB framework for the analysis of DNNs. We also note that rather than trying to repair the inherent problems in the IB functional, a better approach may be to design regularizers on latent representation enforcing the desired properties directly.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2225	2239		10.1109/TPAMI.2019.2909031													
J								CrawlSN: community-aware data acquisition with maximum willingness in online social networks	DATA MINING AND KNOWLEDGE DISCOVERY										Social networks; Graph algorithm; Data acquisition	GROUP QUERIES; SEARCH	Real social network datasets with community structures are critical for evaluating various algorithms in Online Social Networks (OSNs). However, obtaining such community data from OSNs has recently become increasingly challenging due to privacy issues and government regulations. In this paper, we thus make our first attempt to address two important factors, i.e., user willingness and existence of community structure, to obtain more complete OSN data. We formulate a new research problem, namelyCommunity-aware Data Acquisition with Maximum Willingness in Online Social Networks(CrawlSN), to identify a group of users from an OSN, such that the group is a socially tight community and the users' willingness to contribute data is maximized. We prove that CrawlSN is NP-hard and inapproximable within any factor unless, and propose an effective algorithm, namedCommunity-aware Group Identification with Maximum Willingness(CIW) with various processing strategies. We conduct an evaluation study with 1093 volunteers to validate our problem formulation and demonstrate that CrawlSN outperforms the other alternatives. We also perform extensive experiments on 7 real datasets and show that the proposed CIW outperforms the other baselines in both solution quality and efficiency.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1589	1620		10.1007/s10618-020-00709-5													
J								A hybrid Type-2 Fuzzy Logic System and Extreme Learning Machine for low-cost INS/GPS in high-speed vehicular navigation system	APPLIED SOFT COMPUTING										ANFIS; EKF; ELM; High-speed; Low-cost INS/GPS; Navigation; T2-FLS	INTERVAL TYPE-2; GPS/INS INTEGRATION; SETS	Due to the combined navigation system consisting of both Inertial Navigation System (INS) and Global Positioning System (GPS) in a complementary mode which assure a reliable, accurate, and continuous navigation system, we use a GPS/INS navigation system in our research. Because of the conditions of navigation system such as low-cost MEMS-based inertial sensors with considerable uncertainty in INS sensors, a highly noisy real data, and a long term outage of GPS signals during our flight tests, we enhance the positioning speed and accuracy by an Extreme Learning Machine (ELM) with the features of excellent generalization performance and fast learning speed. However, the generalization capability of ELM usually destabilizes with uncertainty existing in the dataset. In order to fix this limitation, first, a Type-2 Fuzzy Logic System (T2-FLS) handles the uncertainties in GPS/INS data, and then the final output ends up to the ELM to train and predict INS positioning error. We verify the efficiency of the suggested method in the estimation of speed and accuracy in INS sensors error during GPS satellites outage, particularly in real-time applications with a high-speed vehicle. Then, to evaluate the overall performance of the proposed method, the achieved results are discussed and compared to other methods like Extended Kalman Filter (EKF), wavelet-ELM, and Adaptive Neuro-Fuzzy Inference System (ANFIS). The results present considerable achievement and open the door to the application of T2-FLS and ELM in GPS/INS integration even in severe conditions. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106447	10.1016/j.asoc.2020.106447													
J								A class of new Support Vector Regression models	APPLIED SOFT COMPUTING										Support Vector Machine; Regression; epsilon-insensitive loss function	MACHINES	We propose a novel convex loss function termed as 'epsilon-penalty loss function', to be used in Support Vector Regression (SVR) model. The proposed epsilon-penalty loss function is shown to be optimal for a more general noise distribution. The popular epsilon-insensitive loss function and the Laplace loss function are particular cases of the proposed loss function. Making the use of the proposed loss function, we have proposed two new Support Vector Regression models in this paper. The first model which we have termed with 'epsilon-Penalty Support Vector Regression' (epsilon-PSVR) model minimizes the proposed loss function with L-2-norm regularization. The second model minimizes the proposed loss function with L-1-Norm regularization and has been termed as 'L-1-Norm Penalty Support Vector Regression' (L-1-Norm PSVR) model. The proposed loss function can offer different rates of penalization inside and outside of the epsilon-tube. This strategy enables the proposed SVR models to use the full information of the training set which make them to generalize well. Further, the numerical results obtained from the experiments carried out on various artificial, benchmark datasets and financial time series datasets show that the proposed SVR models own better generalization ability than existing SVR models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106446	10.1016/j.asoc.2020.106446													
J								Optimization of drop ejection frequency in EHD inkjet printing system using an improved Firefly Algorithm	APPLIED SOFT COMPUTING										Global optimization; Swarm intelligence; Firefly algorithm; Electrohydrodynamic inkjet printing	PARTICLE SWARM OPTIMIZATION; CONE-JET MODE; MULTIOBJECTIVE OPTIMIZATION; DESIGN OPTIMIZATION; NEURAL-NETWORK; STABILITY; NANOSCALE; STRATEGY; PATTERNS; REGION	Swarm intelligence approaches have been used to solve various optimization real-world applications in recent years. Firefly Algorithm (FA) is one of the popular stochastic swarm intelligence paradigms developed in the recent past. In order to solve the slow convergence speed of the standard FA, a new improved Firefly Algorithm (iFA) is proposed in this research. Instead of keeping a constant initial brightness coefficient, a new rule has been proposed for updating the brightness of the fireflies based on a selection probability during generations which leads to a better balance between exploration and exploitation. The efficiency of the iFA has been tested by solving benchmark mathematical functions as well as in a real-world engineering problems. In order to comprehensively compare the performance of the iFA, several other metaheuristic algorithms were used for solving the same benchmark functions and the real-world problems. The iFA shows its superiority in almost all the cases for finding better optima in terms of the objective function value. The droplet ejection speed of an electrohydrodynamic inkjet printing system has been significantly improved by the iFA, which hence fully demonstrates its potential to solve real-world problems. Additionally, the iFA proves its efficiency in solving some challenging, classic engineering design problems with unknown search space. The source codes of the iFA are publicly available at https://www.amitball.com/projects/iFA. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106438	10.1016/j.asoc.2020.106438													
J								An ant colony hyperheuristic approach for matrix bandwidth reduction	APPLIED SOFT COMPUTING										Bandwidth reduction; Sparse matrix; Ant colony optimization; Hyperheuristic; Reordering algorithms; Renumbering; Ordering; Graph labeling; Graph algorithm	ALGORITHM; OPTIMIZATION; HEURISTICS; PROFILE; DESIGN; MATLAB; COST	This paper considers the bandwidth reduction problem for large-scale matrices in serial computations. A heuristic for bandwidth reduction reorders the rows and columns of a given sparse matrix so that the method places entries with a nonzero value as close to the main diagonal as possible. Bandwidth optimization is a critical issue for many scientific and engineering applications. In this regard, this paper proposes an ant colony hyperheuristic approach for the bandwidth reduction of symmetric and nonsymmetric matrices. The ant colony hyperheuristic approach evolves and selects graph theory bandwidth reduction algorithms for application areas. This paper evaluates the resulting heuristics for bandwidth reduction in each application area against the most promising low-cost heuristics for bandwidth reduction. This paper also includes a numerical examination of the current state-of-the-art metaheuristic algorithms for matrix bandwidth reduction. The results yielded on a wide-ranging set of standard benchmark matrices showed that the proposed approach outperformed state-of-the-art low-cost heuristics for bandwidth reduction when applied to problem cases arising from several application areas, clearly indicating the promise of the proposal. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106434	10.1016/j.asoc.2020.106434													
J								Constructing parsimonious analytic models for dynamic systems via symbolic regression	APPLIED SOFT COMPUTING										Symbolic regression; Genetic programming; Model learning; Reinforcement learning		Developing mathematical models of dynamic systems is central to many disciplines of engineering and science. Models facilitate simulations, analysis of the system's behavior, decision making and design of automatic control algorithms. Even inherently model-free control techniques such as reinforcement learning (RL) have been shown to benefit from the use of models, typically learned online. Any model construction method must address the tradeoff between the accuracy of the model and its complexity, which is difficult to strike. In this paper, we propose to employ symbolic regression (SR) to construct parsimonious process models described by analytic equations. We have equipped our method with two different state-of-the-art SR algorithms which automatically search for equations that fit the measured data: Single Node Genetic Programming (SNGP) and Multi-Gene Genetic Programming (MGGP). In addition to the standard problem formulation in the state-space domain, we show how the method can also be applied to input-output models of the NARX (nonlinear autoregressive with exogenous input) type. We present the approach on three simulated examples with up to 14-dimensional state space: an inverted pendulum, a mobile robot, and a bipedal walking robot. A comparison with deep neural networks and local linear regression shows that SR in most cases outperforms these commonly used alternative methods. We demonstrate on a real pendulum system that the analytic model found enables a RL controller to successfully perform the swing-up task, based on a model constructed from only 100 data samples. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106432	10.1016/j.asoc.2020.106432													
J								A modified salp swarm algorithm for task assignment problem	APPLIED SOFT COMPUTING										Task assignment problem; Salp swarm algorithm; Swarm intelligence algorithms; Task interchange graph; Local refinement heuristic	OPTIMIZATION ALGORITHM; GENETIC ALGORITHM; ALLOCATION; PROCESSORS; HEURISTICS; CHAINS; TIME	The task assignment problem (TAP) is one of the standard combinatorial optimization problems (COPs) in the field of discrete optimization. TAP is known to be an NP-complete problem due to the difficulty to obtain the exact solution of it in polynomial time. Thus, it is essential to develop and/or propose an optimization algorithm to solve this problem. In TAPs, a set of tasks is assigned to a set of machines to both effectively minimize the communication and execution cost. This paper presents a modified Salp Swarm Algorithm (SSA), to solve not only task assignment problems but also, fundamental combinatorial optimization problems in engineering and real-world scientific domains. The modified salp algorithm takes place in the integration with the Local Refinement Heuristic (LRH) approach to enhance a given assignment along with operators. The modified algorithm is named Modified Salp Local Refinement Heuristic (MSLRH). To the best of our knowledge, this paper is the first of its kind to attempt using the SSA in task assignment problems. The MSLRH algorithm is tested on different benchmark datasets (tree structure and general graph), including various tasks and machines for each dataset. In addition, it compared with the most known meta-heuristic algorithms such as the Genetic Algorithm (GA), Particle Swarm Optimization (PSO) algorithm, and Jaya algorithm (JAYA) to investigate the effectiveness of the MSLRH algorithm in terms of average assignment allocation cost. From tree structure dataset (e.g., 200 tasks assigned to 8 machines), the proposed MSLRH algorithm has achieved a minimum average assignment cost better than GA by 62% and better than PSO and JAYA by 42%. From the general graph dataset (e.g., 209 tasks assigned to 16 machines), the proposed MSLRH algorithm has better results than other algorithms up to 60%. From various and extensive experimental results, the proposed algorithm has proven the effectiveness in solving the task assignment problem. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106445	10.1016/j.asoc.2020.106445													
J								A novel randomized machine learning approach: Reservoir computing extreme learning machine	APPLIED SOFT COMPUTING										Reservoir computing; Extreme learning machine; Randomization in machine learning; Randomized artificial neural network	ECHO STATE NETWORKS; NEURAL-NETWORKS; ALGORITHMS	In this study, a novel approach that is based on reservoir computing, which is a successful method in modeling sequential datasets, and extreme learning machines, which has a high generalization capacity, was proposed to model a non-sequential dataset or system. The proposed approach does not require any optimization stage; each weight (except weights in the output layer), biases, the number of neurons in the reservoir, activation functions and the parameters of activation functions were determined arbitrarily and the weights in the output layer were calculated based on these arbitrarily assigned parameters. The proposed approach was evaluated and validated with 60 different benchmark datasets. Obtained results were compared with literature findings and results obtained by each of the extreme learning machine (ELM), randomized artificial neural network, random vector functional link, stochastic ELM, and pruned stochastic ELM methods. Achieved results are successful enough to be employed in classification and regression. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106433	10.1016/j.asoc.2020.106433													
J								Solving green supplier selection problem using q-rung orthopair fuzzy-based decision framework with unknown weight information	APPLIED SOFT COMPUTING										Evidence-based bayes approximation; Group decision-making; Maclaurin symmetric mean; q-rung orthopair fuzzy set; Statistical variance and VIKOR method	VIKOR	As a powerful generalization to intuitionistic fuzzy set (IFS), q-rung orthopair fuzzy set (q-ROFS) is proposed by Yager, which can effectively mitigate the weakness of IFS and provide wider space for preference elicitation. Based on the literature analysis on q-ROFS, a comprehensive decision framework for promoting rational decision-making is lacking. Motivated by the superiority of q-ROFS and to circumvent the issue, in this paper, a new decision framework with minimum subjective randomness is proposed under q-ROFS context. Initially, decision makers' (DMs') relative importance is systematically calculated by extending evidence-based Bayes approximation to q-ROFS. Later, a new operator is proposed for aggregating DMs' preferences by extending generalized Maclaurin symmetric mean (GMSM) to q-ROFS context. Attributes' weight values are calculated by using newly proposed q-rung orthopair fuzzy statistical variance (q-ROFSV) method and objects are prioritized by extending the popular VIKOR method to q-ROFS context. Finally, the practical use of the proposed decision framework is validated by using a green supplier selection problem and the strengths and weaknesses of the framework are discussed by using comparative analysis with other methods. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				SEP	2020	94								106431	10.1016/j.asoc.2020.106431													
J								User reviews: Sentiment analysis using lexicon integrated two-channel CNN-LSTM family models	APPLIED SOFT COMPUTING										Sentiment analysis; Long short-term memory; Convolutional neural network; Sentiment padding; Sentiment lexicon		Sentiment analysis, which refers to the task of detecting whether a textual item (e.g., a product review and a blog post) expresses a positive or negative opinion in general or about a given entity (e.g., a product, person, or policy), has received increasing attention in recent years. It serves as an important role in natural language processing. User generated content, like tourism reviews, developed dramatically during the past years, generating a large amount of unstructured data from which it is hard to obtain useful information. Due to the changes in textual order, sequence length and complicated logic, it is still a challenging task to predict the exact sentiment polarities of the user reviews, especially for fine-grained sentiment classification. In this paper, we first propose sentiment padding, a novel padding method compared with zero padding, making the input data sample of a consistent size and improving the proportion of sentiment information in each review. Inspired by the most recent studies with respect to neural networks, we propose deep learning based sentiment analysis models named lexicon integrated two-channel CNN-LSTM family models, combining CNN and LSTM/BiLSTM branches in a parallel manner. Experiments on several challenging datasets, like Stanford Sentiment Treebank, demonstrate that the proposed method outperforms many baseline methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106435	10.1016/j.asoc.2020.106435													
J								A decision making model based on the leading principal submatrices of a reciprocal preference relation	APPLIED SOFT COMPUTING										Decision making; Leading principal submatrix (LPSM); Particle swarm optimization (PSO); Consistency; Analytic hierarchy process (AHP)	IMPROVING CONSISTENCY; COMPARISON MATRIX; PAIRWISE; CONSENSUS; INFORMATION; AHP; ALGORITHMS; ALLOCATION; JUDGMENTS; PSO	A completed comparison matrix is the basic tool in the analytic hierarchy process (AHP). In practice, the process of forming a comparison matrix is complex and it is worth being investigated carefully. In this study, by decomposing pairwise comparison process of alternatives, the leading principal submatrices (LPSMs) of a completed comparison matrix are used as the basis for decision analysis. Based on the particle swarm optimization (PSO), a novel method for improving consistency of inconsistent comparison matrices is proposed. The fitness function is constructed by considering the acceptable consistency of pairwise comparison matrices and the similarity degree between the initial and the adjusted decision-making information. A new algorithm is elaborated on for solving a decision making problem with multiplicative reciprocal matrices. Numerical results are reported to show the advantages of the proposed model by comparing with the other methods. The observations reveal that the proposed method and algorithm are effective when dealing with inconsistent comparison matrices and the corresponding decision making problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106448	10.1016/j.asoc.2020.106448													
J								Retinal vessel segmentation using multifractal characterization	APPLIED SOFT COMPUTING										Multifractal; Holder exponent; Fundus; Retinal vessel segmentation; Gabor wavelet; Morphological reconstruction	NEURAL-NETWORKS; BLOOD-VESSELS; MATCHED-FILTER; IMAGES; MODEL; VASCULATURE; RETINOPATHY; ALGORITHM; LEVEL	This study presents a supervised classification method for the segmentation of retinal vessels using fundus images. This work proposes a novel retinal vasculature segmentation method based on multifractal characterization of the vessels to minimize the noise and enhance the vessels during segmentation. The Holder exponent, a multifractal measure is employed for the first time to segment the retinal vessels. The Holder exponent is used to quantify the local regularity of the vessels. The Holder exponents are computed from the Gabor wavelet responses for the effective segmentation of vessels, which is a novel feature of the method. The Gaussian mixture model (GMM) classifier is used for the classification of pixels. Different multifractal measures used to compute the Holder exponents are evaluated for the output quality. The effectiveness of the method is evaluated using three publicly available datasets for fundus images namely, DRIVE, STARE and CHASE_DB1. The proposed method provides robust segmentation of retinal vessels for both normal and abnormal images (i.e., images with pathologies) at a reasonable segmentation speed and the method is also simple to configure. It achieves an average accuracy and area under the receiver operating characteristic curve of 0.948 and 0.959 respectively on the DRIVE dataset; 0.9542 and 0.9711 respectively on the STARE dataset; 0.9459 and 0.9592 respectively on the CHASE_DB1 dataset; 0.9500 and 0.9623 respectively on the abnormal images. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106439	10.1016/j.asoc.2020.106439													
J								An extended Pythagorean fuzzy complex proportional assessment approach with new entropy and score function: Application in pharmacological therapy selection for type 2 diabetes	APPLIED SOFT COMPUTING										Pythagorean Fuzzy Sets; Type 2 Diabetes; Entropy measure; Score function; Pharmacological therapy selection; COPRAS	GROUP DECISION-MAKING; AGGREGATION OPERATORS; SIMILARITY MEASURES; SUPPLIER SELECTION; EINSTEIN OPERATIONS; MEMBERSHIP GRADES; ACCURACY FUNCTION; DISTANCE MEASURE; EXPERT-SYSTEM; SETS	In the context of medical decision making, the Type 2 Diabetes (T2D) pharmacological therapy selection problem involves several medications that can be stipulated to manage the blood glucose level of patients. The extensive range of hyperglycemia lowering agents with varying outcomes and several side effects makes the decision quite complicated and uncertain. Pythagorean Fuzzy Sets (PFSs) are proven as one of the valuable tools to handle ambiguous and ill-defined problems. This paper introduces an innovative Complex Proportional Assessment (COPRAS) to solve the T2D medication selection problem under the Pythagorean fuzzy environment. In this methodology, a new formula based on entropy measure and score function is introduced to evaluate the unknown criteria weights. To doing so, the new entropy measure and score function are developed under the PFSs context. Further, an illustrative case study of the T2D pharmacological therapy selection problem is taken to express the feasibility and usefulness of the proposed method in real-world decision-making problems. Finally, the obtained result is compared with some existing methods, which confirms the strength and stability of the proposed COPRAS method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106441	10.1016/j.asoc.2020.106441													
J								A multi-objective artificial butterfly optimization approach for feature selection	APPLIED SOFT COMPUTING										Meta-heuristic algorithms; Machine learning; Pattern recognition; Many-objective optimization	CLASSIFICATION; ALGORITHM; DESIGN	Feature selection plays an essential role in machine learning since high dimensional real-world datasets are becoming more popular nowadays. The very basic idea consists in selecting a compact but representative set of features that reduce the computational cost and minimize the classification error. In this paper, the authors propose single, multi- and many-objective binary versions of the Artificial Butterfly Optimization (ABO) in the context of feature selection. The authors also propose two different approaches: (i) the first one (MO-I) aims at optimizing the classification accuracy of each class individually, while (ii) the second one (MO-II) considers the feature set minimization in the process either. The experiments were conducted over eight public datasets, and the proposed approaches are compared against the well-known Particle Swarm Optimization, Firefly Algorithm, Flower Pollination Algorithm, Brainstorm Optimization, and the Black Hole Algorithm. The results showed that the binary single-objective ABO performed better than the other meta-heuristic techniques, selecting fewer features and also figuring a lower computational burden. Concerning multi- and many-objective feature selection, both MO-I and MO-II approaches performed better than their single-objective meta-heuristic counterparts. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106442	10.1016/j.asoc.2020.106442													
J								Identifying influential spreaders using multi-objective artificial bee colony optimization	APPLIED SOFT COMPUTING										Complex network; Influential nodes; Artificial bee colony; Spreading ability; Influence maximization	SOCIAL NETWORKS; INFLUENCE MAXIMIZATION; COMPLEX NETWORKS; NODE RANKING; IDENTIFICATION; CENTRALITY; USERS; ALGORITHM; INDEX	Advertisement over social networks is critical for many businesses, and selecting the initial set of influential nodes for which the advertising message is passed, is regarded as an important issue in this regard. Although various measures have been proposed for specifying the influentiality of a set, it is affected by different factors. In this paper, a multi-objective function is first defined as an influentiality measure, and finding such an initial is framed as an optimization problem. Then, using artificial bee colony optimization two approaches are proposed to solve the problem. In the first approach, influentiality of nodes is only taken into account, while in the second method a budget constraint is also considered. Different experiments on real networks are conducted to evaluate the proposed methods, where the obtained results show their outperformance over state-of-the-art influence maximization methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106436	10.1016/j.asoc.2020.106436													
J								Autonomous Learning Multiple-Model zero-order classifier for heart sound classification	APPLIED SOFT COMPUTING										Autonomous Learning; Data clouds; Evolving fuzzy systems; Heart sound classification; Rule-based system	PERFORMANCE; BRAIN	This paper proposes a new extended zero-order Autonomous Learning Multiple-Model (ALMMo-0*) neuro-fuzzy approach in order to classify different heart disorders through sounds. ALMMo-0* is build upon the recently introduced ALMMo-0. In this paper ALMMo-0 is extended by adding a preprocessing structure which improves the performance of the proposed method. ALMMo-0* has as a learning engine composed of hierarchical a massively parallel set of 0-order fuzzy rules, which are able to self-adapt and provide transparent and human understandable IF ... THEN representation. The heart sound recordings considered in the analysis were sourced from several contributors around the world. Data were collected from both clinical and nonclinical environment, and from healthy and pathological patients. Differently from mainstream machine learning approaches, ALMMo-0* is able to learn from unseen data. The main goal of the proposed method is to provide highly accurate models with high transparency, interpretability, and explainability for heart disorder diagnosis. Experiments demonstrated that the proposed neuro-fuzzy-based modeling is an efficient framework for these challenging classification tasks surpassing its state-of-the-art competitors in terms of classification accuracy. Additionally, ALMMo-0* produced transparent AnYa type fuzzy rules, which are human interpretable, and may help specialists to provide more accurate diagnosis. Medical doctors can easily identify abnormal heart sounds by comparing a patient's sample with the identified prototypes from abnormal samples by ALMMo-0*. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106449	10.1016/j.asoc.2020.106449													
J								A parallel compact cuckoo search algorithm for three-dimensional path planning	APPLIED SOFT COMPUTING										3D path planning; Cuckoo search algorithm; Parallel communication strategy; Compact strategy	OPTIMIZATION	The three-dimensional (3D) path planning of unmanned robots focuses on avoiding collisions with obstacles and finding an optimized path to the target location in a complex three-dimensional environment. An improved cuckoo search algorithm based on compact and parallel techniques for three-dimensional path planning problems is proposed. This paper implements the compact cuckoo search algorithm, and then, a new parallel communication strategy is proposed. The compact scheme can effectively save the memory of the unmanned robot. The parallel scheme can increase the accuracy and achieve faster convergence. The proposed algorithm is tested on several selected functions and three-dimensional path planning. Results compared with other methods show that the proposed algorithm can provide more competitive results and achieve more efficient execution. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106443	10.1016/j.asoc.2020.106443													
J								New peer effect-based approach for service matching in Cloud Manufacturing under uncertain preferences	APPLIED SOFT COMPUTING										Cloud manufacturing; Two-sided matching; Uncertain preferences; Peer effect; Cloud model	DEMAND	Cloud manufacturing is a kind of sharing manufacturing, and the supply-demand matching between manufacturing services and customers has become one of the most important issues for a platform. Because of the increasing complexity of customer personalization, the cognitive information from both sides becomes uncertain and fuzzy. Linguistics is used to describe uncertain preferences, especially in platforms. Also, the cloud model is adopted to convert the linguistics to reflect the randomness and fuzziness. Meanwhile, as the rapid development of communication techniques has strengthened the connections between different agents, the final matching results are affected by connections. Hence, the peer effect, which describes the mutual influences among individuals, is introduced in our study. In addition, considering the different strengths of the connections, the peer effect is improved by integrating grey relations, which are used to evaluate the diverse connections. Finally, the workload is introduced in the form of adjustment parameters. Consequently, we establish a bi-objective model that aims to maximize satisfaction and minimize the differences among individuals. To solve the mathematical model, an improved cuckoo algorithm is proposed. Additionally, the design of an air outlet grille for new-energy vehicles is taken as an example, and the effectiveness of the proposed method is verified. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106444	10.1016/j.asoc.2020.106444													
J								An inverse-distance weighting genetic algorithm for optimizing the wafer exposure pattern for enhancing OWE for smart manufacturing	APPLIED SOFT COMPUTING										Semiconductor industry; Yield enhancement; Inverse distance weighting estimation genetic algorithm (GA); Overall wafer effectiveness (OWE); Total Resource Management (TRM)	BIG DATA; SEMICONDUCTOR; YIELD; STRATEGY; SYSTEM	Wafer exposure pattern will determine the number of gross dies fabricated on the wafer and also affect the yield. Although a number of studies have addressed the wafer exposure pattern problem for maximizing the number of gross dies, little research has considered both the yield and gross dies simultaneously. To fill the gap, this study aims to develop an inverse distance weighting genetic algorithm (IDWGA) that simultaneously maximizes the total number of exposed gross dies and minimizes the deviation of die-estimated measurement from the target for yield enhancement and smart manufacturing. This study developed a novel approach for estimating the die yield from a few measurement points and a three-dimensional (3D) contour plot of die estimates for verifying the measurement pattern among the dies. The proposed IDWGA can detect the die yield pattern during the wafer exposure stage and thus optimize the exposure pattern to maximize the number of gross dies and minimize potential yield loss. On the basis of realistic data, experiments were designed to estimate the validity of the proposed approach. The results have shown practical viability of the proposed approach to optimize overall wafer effectiveness for total resource management. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106430	10.1016/j.asoc.2020.106430													
J								Why Black-Scholes Equations Are Effective Beyond Their Usual Assumptions: Symmetry-Based Explanation	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Financial options; Black-Scholes equations; decision making under uncertainty; scale invariance		Nobel-Prize-winning Black-Scholes equations are actively used to estimate the price of options and other financial instruments. In practice, they provide a good estimate for the price, but the problem is that their original derivation is based on many simplifying statistical assumptions which are, in general, not valid for financial time series. The fact that these equations are effective way beyond their usual assumptions leads to a natural conclusion that there must be an alternative derivation for these equations, a derivation that does not use the usual too-strong assumptions. In this paper, we provide such a derivation in which the only substantial assumption is a natural symmetry: namely, scale-invariance of the corresponding processes. Scale-invariance also allows us to describe possible generalizations of Black-Scholes equations, generalizations that we hope will lead to even more accurate estimates for the corresponding prices.																	0218-4885	1793-6411				SEP	2020	28			1	SI		1	10		10.1142/S0218488520400012													
J								Uncertainty in Information Market Games	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Interval uncertainty; information market; game; interval core; interval T-value		A new product can be produced and sold in a market thanks to the entrance of a patent holder into the market. This market is divided into submarkets controlled by only some firms and the profit attainable in each submarket is uncertain. In this paper, this situation is studied by means of cooperative games under interval uncertainty. We consider different ways of allocating the interval profit among the firms. One of these is the interval T-value, which is defined for interval games satisfying some conditions. Efficient interval solutions in terms of the market data are provided.																	0218-4885	1793-6411				SEP	2020	28			1	SI		11	29		10.1142/S0218488520400024													
J								Beyond Deep Learning: An Econometric Example	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Deep learning; econometrics; currency exchange rate	NETWORKS	In the past, in many areas, the best prediction models were linear and nonlinear parametric models. In the last decade, in many application areas, deep learning has shown to lead to more accurate predictions than the parametric models. Deep learning-based predictions are reasonably accurate, but not perfect. How can we achieve better accuracy? To achieve this objective, we propose to combine neural networks with parametric model: namely, to train neural networks not on the original data, but on the differences between the actual data and the predictions of the parametric model. On the example of predicting currency exchange rate, we show that this idea indeed leads to more accurate predictions.																	0218-4885	1793-6411				SEP	2020	28			1	SI		31	38		10.1142/S0218488520400036													
J								How to Take Both Non-Linearity and Asymmetry (Skewness) into Account in Binary Decision Making: Skew-Probit and Skew-Logit in Binary Kink Regression	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										binary decisions; binary regression; kink regression; logit; probit; skew-normal distribution; skew-logistic distribution	MODEL	In many practical situations, it is desirable to predict binary ("yes"-"no") decisions made by people. The traditional approach to this prediction assumes that the utility linearly depends on the corresponding parameters, and that the distribution of the difference between predicted and actual utility is symmetric - usually normal or logistic; the corresponding techniques are known as, correspondingly, probit and logit. In real life, utility often non-linearly depends on the parameters, and the corresponding distributions are asymmetric (skewed). There are techniques for dealing with non-linearity; the most widely used such technique - called kink regression - uses piece-wise linear approximations to the utility. There are also techniques that take into account the distribution's asymmetry; usually, they are based on using special asymmetric distributions: skew-normal and skew-logistic. In this paper, we show how these two techniques to be combined to take into account both non-linearity and asymmetry. On a real-life example, we show that the new technique indeed leads to a better description of human binary decision-making.																	0218-4885	1793-6411				SEP	2020	28			1	SI		39	49		10.1142/S0218488520400048													
J								Comparing Medical Care Costs using Bayesian Credible Intervals for the Ratio of Means of Delta-Lognormal Distributions	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Bayesian credible interval; delta-lognormal distribution; medical care cost; uniform-beta prior; zero observation	CONFIDENCE-INTERVALS; VARIANCES	When considering the medical care costs data with a high proportion of zero items of two inpatient groups, comparing them can be estimated using confidence intervals for the ratio of the means of two delta-lognormal distributions. The Bayesian credible interval-based uniform-beta prior (BCIh-UB) is proposed and compared with the generalized confidence interval (GCI), fiducial GCI (FGCI), the method of variance estimates recovery (MOVER), BCIh based on Jeffreys' rule prior (BCIh-JR), and BCIh based on the normal-gamma prior (BCIh-NG). The coverage probability (CP), average length (AL), and lower and upper error rates were the performance measures applied for assessing the methods through a Monte Carlo simulation. A numerical evaluation showed that BCIh-UB had much better CP and AL than the others even with a large difference between the variances and with a high proportion of zero. Finally, to illustrate the efficacy of BCIh-UB, the methods were applied to two sets of medical care costs data.																	0218-4885	1793-6411				SEP	2020	28			1	SI		51	68		10.1142/S021848852040005X													
J								Ruin Probabilities of Continuous-Time Risk Model with Dependent Claim Sizes and Interarrival Times	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Cramer-Lundberg model; risk model with continuous time; m-dependent random variables; ruin probabilities		In this paper we investigate an insurance continuous-time risk model when the claim sizes and inter-arrival times are m-dependent random variables. We provide an upper exponential bound for the ruin probability.																	0218-4885	1793-6411				SEP	2020	28			1	SI		69	80		10.1142/S0218488520400061													
J								Why the Use of Convex Combinations Works Well for Interval Data: A Theoretical Explanation	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Interval-valued data; artificial neural network; convex combination method	REGRESSION	One of the main objectives of econometrics is to predict future values of important economics-related quantities, such as unemployment level, stock prices, currency exchange rates, etc. - and especially to predict how different possible economy-boosting measures will affect these quantities. To perform this prediction, we design a model of such effect and train it on the available data. Usually, the daily (or weekly) data are used for this training. However, economics-related quantities fluctuate all the time. So, for each moment of time (e.g., for each day) instead of a single value of the corresponding quantity, we have smallest and largest daily values - i.e., the interval range of daily values. At first glance, it may seem that using both endpoints of this interval for training will lead to more accurate predictions, but in reality, predictions become less accurate. Predictions become more accurate if we only use midpoints of the corresponding intervals. Several recent papers showed that even more accurate predictions are possible if we allow general convex combinations of the intervals' endpoints - and select the corresponding coefficients so as to best fit the data. In this paper, we provide a theoretical explanation for these results.																	0218-4885	1793-6411				SEP	2020	28			1	SI		81	85		10.1142/S0218488520400073													
J								On Statistics of Random Sets for Partial Identification of Econometric Structures	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Coarse data; copulas; econometric structures; identification; partial identification; random sets; random set statistics; set parameters		In this paper, we emphasize and elaborate on two important and relatively new aspects in uncertainty analysis in order to increase the credibility of empirical results in statistics in general, and in econometrics in particular, namely, the problem of partial identification, and the use of random set statistics. We elaborate on the current interests in partially identified models, exemplified by econometric structures involving copulas. We spell out the rationale and the statistical methods based upon random set theory for analyzing partial identification problem towards credible econometrics.																	0218-4885	1793-6411				SEP	2020	28			1	SI		87	98		10.1142/S0218488520400085													
J								The Black-Litterman Model for Portfolio Optimization on Vietnam Stock Market	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Black-Litterman model; portfolio optimization; Markowitz model; ARIMA model	VIEWS	The Black-Litterman asset allocation model is an extended portfolio management model to construct optimal portfolios by combining the market equilibrium with investor views into asset allocation decisions. In this paper we apply Black-Litterman model for portfolio optimization on Vietnames stock market. We chose ARIMA methodology utilized in financial econonometrics to predict the views of investor which are used as inputs of the Black-Litterman asset allocation process to find optimal portfolio and weights.																	0218-4885	1793-6411				SEP	2020	28			1	SI		99	111		10.1142/S0218488520400097													
J								A Mixed Copula-Based Vector Autoregressive Model for Econometric Analysis	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Copula; mixed copula; vector autoregressive model	INFERENCE	In many practical applications, the dynamics of different quantities is reasonably well described by linear equations. In economics, such linear dynamical models are known as vector autoregressive (VAR) models. These linear models are, however, only approximate. The deviations of the actual value of each quantity from the predictions of the linear model are usually well described by normal or Student-t distributions. To complete the description of the joint distribution of all these deviations, we need to supplement these marginal distributions with the information about the corresponding copula. To describe this dependence, in the past, researchers followed the usual idea of trying copulas from several standard families: Gaussian, Student, Clayton, Frank, Gumbel, and Joe families. To get a better description, we propose to also use convex combinations of copulas from different families; such convex combinations are known as mixed copulas. On the example of the dynamics of US macroeconomic data, including GDP, unemployment, consumer price index, and the real effective exchange rate, we show that mixed copulas indeed lead to a better description of the actual data. Specifically, it turns out that the best description is obtained if we use a convex combination of Student and Frank copulas.																	0218-4885	1793-6411				SEP	2020	28			1	SI		113	121		10.1142/S0218488520400103													
J								Supervised machine learning based gait classification system for early detection and stage classification of Parkinson's disease	APPLIED SOFT COMPUTING										Parkinson's disease (PD); Machine learning algorithms; Classifiers; Confusion matrix; Temporal and spatial features; H& Y scale	DISORDERS; PATTERNS	While diagnosing Parkinson's disease (PD), neurologists often use several clinical manifestations of the subject and rate the severity level based on the Unified Parkinson Disease Rating Scale (UPDRS). This kind of rating largely depends on the expertise of the doctors, which is not only subjective but also inefficient. Hence, in this paper, a machine learning based gait classification system which can assist the clinician to diagnose the stages of PD is presented. Gait pattern, which plays a significant role in assessing the human mobility, is a significant biomarker to classify whether the subject is healthy or affected with PD. Hence, we utilize the vertical ground reaction force (VGRF) gait dataset and extract the minimal feature vector using the statistical analysis. Subsequently, the normal distribution of the data is validated using the Shapiro-Wilk test, and from the spatial and temporal features of gait pattern, the salient biomarkers are identified using the correlation based feature selection technique. Four supervised machine learning algorithms namely decision tree (DT), support vector machine (SVM), ensemble classifier (EC) and Bayes classifier (BC) are used for statistical and kinematic analyses which predict the severity of PD. The classifier efficacy quantified using the accuracy, sensitivity and specificity highlights that the proposed framework can effectively rate the severity of PD based on Hohen and Yahr (H&Y) scale. Moreover, comparing the accuracy of the proposed PD classification approach with those of the other state-of-the-art approaches, which utilized the same gait dataset, reveal that the proposed method outperforms several other PD classification methods. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106494	10.1016/j.asoc.2020.106494													
J								Neuro-genetic programming for multigenre classification of music content	APPLIED SOFT COMPUTING										Genetic programming; Artificial neural networks; Music genre recognition; Multi-label classifiers; Fuzzy classification	GENRE CLASSIFICATION	A machine learning approach based on hybridization of genetic programming and neural networks is used to derive mathematical models for music genre classification. We design three multi-label classifiers with different trade-offs between complexity and accuracy, which are able to identify the degree of belonging of music content to ten different music genres. Our approach is innovative as it entirely relies on simple analytical functions and a reduced number of features. Resulting classifiers have an extremely low computational complexity and are suitable to be easily integrated in low-cost embedded systems for real-time applications. The GTZAN dataset is used for model training and to evaluate the accuracy of the proposed classifiers. Despite of the reduced number of features used in our approach, the accuracy of our models is found to be similar to that of more complex music genre classification tools previously published in the literature. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106488	10.1016/j.asoc.2020.106488													
J								A multi-response optimal design of bistable compliant mechanism using efficient approach of desirability, fuzzy logic, ANFIS and LAPO algorithm	APPLIED SOFT COMPUTING										Bistable compliant mechanism; Desirability function approach; Fuzzy logic system; Adaptive neuro-fuzzy inference system; Multi-objective optimization; Lightning attachment procedure optimization; Statistical analysis	MULTIOBJECTIVE OPTIMIZATION; TOPOLOGY OPTIMIZATION; PARALLEL MECHANISM; MILLIMETER-RANGE; MACHINE; ACCELEROMETER; METHODOLOGY; PARAMETERS; MODEL; STAGE	Compliant mechanisms are promising candidates in precision engineering, soft robotics, space, and bioengineering due to their advantages of free friction, free lubricant, no backlash, monolithic structure, and minimal assembly. However, designing and analyzing of compliant mechanisms are facing the high complexity due to a coupling of kinematic and mechanical behavior in comparison to rigid-body mechanisms. Especially, considering a multi-objective optimization design for compliant mechanisms, the problem is more complicated. Thus, this paper presents a new efficient hybrid methodology for solving the multi-objective optimization design. A hybridization is developed through a combination of finite element method, statistical technique, desirability function approach, fuzzy logic system, adaptive neuro-fuzzy inference system (ANFIS), and Lightning attachment procedure optimization (LAPO). A bistable compliant mechanism is investigated as an application example of the proposed method. First, design variables of the mechanism are determined, and then central composite design is employed to construct a numerically experimental matrix. Though using analysis of variance and Taguchi approach, the design variables are refined to make new populations. Subsequently, desirability values of two performances of the mechanism are computed, and the results are transferred into the fuzzy logic system. The output of fuzzy logic system is considered as single combined objective function. By developing the ANFIS model, the relation between the refined design variables and the output of fuzzy logic system is established. Finally, LAPO algorithm is adopted for solving the multi objective optimization problem for the mechanism. Three numerical examples are investigated to validate the performance efficiency of the proposed method. The results demonstrate that the proposed method is more efficient than Taguchi-based fuzzy logic. Besides, through Wilcoxon signed rank test and Friedman test, it reveals that the performances of proposed approach are superior to those of the Jaya algorithm and TLBO algorithm. The results of this article can be extended for other complex compliant mechanisms as well as optimization problems with multiple objective functions and more complex constraints. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106486	10.1016/j.asoc.2020.106486													
J								A fast self-attention cascaded network for object detection in large scene remote sensing images	APPLIED SOFT COMPUTING										Remote sensing images; Deep learning; Neural network; Visual perception; Object detection	HIERARCHICAL SALIENCY	Aiming at the real-time detection of multiple objects and micro-objects in large-scene remote sensing images, a cascaded convolutional neural network real-time object-detection framework for remote sensing images is proposed, which integrates visual perception and convolutional memory network reasoning. The detection framework is composed of two fully convolutional networks, namely, the strengthened object self-attention pre-screening fully convolutional network (SOSA-FCN) and the object accurate detection fully convolutional network (AD-FCN). SOSA-FCN introduces a self-attention module to extract attention feature maps and constructs a deep feature pyramid to optimize the attention feature maps by combining convolutional long-term and short-term memory networks. It guides the acquisition of potential sub-regions of the object in the scene, reduces the computational complexity, and enhances the network's ability to extract multi-scale object features. It adapts to the complex background and small object characteristics of a large-scene remote sensing image. In ADFCN, the object mask and object orientation estimation layer are designed to achieve fine positioning of candidate frames. The performance of the proposed algorithm is compared with that of other advanced methods on NWPU_VHR-10, DOTA, UCAS-AOD, and other open datasets. The experimental results show that the proposed algorithm significantly improves the efficiency of object detection while ensuring detection accuracy and has high adaptability. It has extensive engineering application prospects. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106495	10.1016/j.asoc.2020.106495													
J								Feature-based hesitant fuzzy aggregation method for satisfaction with life scale	APPLIED SOFT COMPUTING										Satisfaction with life scale; Hesitant fuzzy element; Hesitant fuzzy feature pair; Aggregation method	DECISION-MAKING METHOD; PREFERENCE-RELATION; OPERATORS; MODEL; SETS; ENVIRONMENTS; SUPPORT	Satisfaction with life scale is a comprehensive cognitive judgement of individual's own life and becomes the dominant measure of life satisfaction. However, the individual may hesitate to assess his/her life satisfaction with a single value due to multiple criteria. Consequently, we propose the hesitant fuzzy satisfaction with life scale (HFSWLS) with the same form as hesitant fuzzy element (HFE), consisting of several possible values of the cognitive judgements to describe the uncertainties and hesitancies. In this paper, we propose a novel aggregation method for large scale HFEs and apply it to HFSWLSs. Primarily, we present the necessary requirements of the extension rules for HFEs to guarantee the reversibility and linearity of the proposed operators. In the proposed aggregation method, we cluster the individuals based on their feature values and further define the hesitant fuzzy feature pair to find out the abnormal HFEs. We generate each cluster's center HFE by the proposed operators in the first-round aggregation. Based on the orness degree relating to the cluster's feature value and the decision makers' preferences to cluster's feature value, we derive the weights of the center HFEs by O'Hagan's nonlinear optimization, leading to the final aggregation result taking into account the actual demand. Finally, the validity and effectiveness of the proposed aggregation method for HFEs are testified by the application of HFSWLSs of the citizens. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106493	10.1016/j.asoc.2020.106493													
J								A hybrid Grey Wolf Optimization and Particle Swarm Optimization with C4.5 approach for prediction of Rheumatoid Arthritis	APPLIED SOFT COMPUTING										Feature selection; Particle Swarm Optimization; Grey Wolf Optimization; Predictive model; Rheumatoid Arthritis	CLASSIFICATION	Rheumatoid Arthritis (RA) is a type of dreadful autoimmune disease that affects the entire human body, especially joints. Early diagnosis of RA is a challenging task for General Physicians, since the actual triggering mechanism is unpredictable. The capability of C4.5 was explored using the hybridization of Grey Wolf Optimization (GWO) Particle Swarm Optimization (PSO) to develop an effective RA prediction system. In this work, firstly, PSO was applied for selecting the diversified initial positions. Secondly GWO was used to update the current positions of the population from the search space to get the optimal features for better classification. Subsequently, the selected features were given as an input to the C4.5 approach and developed a final RA predictor model. The proposed HGWO-C4.5 was meticulously examined based on real time patient's data, which included factors that influence RA prediction by utilizing both RA and Non-RA information. To validate the proposed HGWO-C4.5, with other meta-heuristics based methods including GWO based C4.5, PSO based C4.5 and individual C4.5 method. The Cross-validation results show that HGWO-C4.5 has achieved an overall average accuracy of 86.36% from three other approaches, which was similar to 6%-14% higher than those attainable using the individual predictors. Furthermore, HGWO-C4.5 has achieved an overall average accuracy of 84% on independent datasets evaluation, which was 8.61% higher than those yielded by the state-of-the-art predictors. This is the first predictor model that includes feature selection and classification for RA prediction to the best of our knowledge. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106500	10.1016/j.asoc.2020.106500													
J								Artificial neural network based software fault detection and correction prediction models considering testing effort	APPLIED SOFT COMPUTING										Software reliability; Fault detection process; Fault correction process; Testing effort function; Artificial neural network	RELIABILITY GROWTH-MODEL; UNCERTAINTY; FAILURE; RELEASE	Software reliability is an important attribute of software quality. To achieve higher reliability, software development must include a testing phase in which faults can be detected and corrected. The software reliability growth model (SRGM) has evolved from modeling merely the fault detection process (FDP) into incorporating the fault correction process (FCP) as well. However, restricted by mathematical tractability, it is difficult to incorporate into analytical models with more complicated factors, such as the dependency between faults and the influence of staffing levels. This limits the application of analytical models. Therefore, it is promising to adopt data-driven methods such as the artificial neural network (ANN) to model the FDP and the FCP as no specific assumptions are needed. In this study, a stepwise prediction model is proposed to model the FDP and the FCP based on the ANN. Testing effort is considered in our model since it has a great influence on fault detection and correction process. Using real data, the performance of different types of neural networks are compared with the analytical model. The empirical study has confirmed the effectiveness of the proposed models. Further, the optimal policy of the software release time is also presented to illustrate the applications. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106491	10.1016/j.asoc.2020.106491													
J								Difficulty Adjustable and Scalable Constrained Multiobjective Test Problem Toolkit	EVOLUTIONARY COMPUTATION										Constrained problems; evolutionary multiobjective optimization; test problems; controlled difficulties	NONDOMINATED SORTING APPROACH; EVOLUTIONARY ALGORITHM; GENETIC ALGORITHM; OPTIMIZATION; DECOMPOSITION; EFFICIENT; SELECTION; SEARCH	Multiobjective evolutionary algorithms (MOEAs) have progressed significantly in recent decades, but most of them are designed to solve unconstrained multiobjective optimization problems. In fact, many real-world multiobjective problems contain a number of constraints. To promote research on constrained multiobjective optimization, we first propose a problem classification scheme with three primary types of difficulty, which reflect various types of challenges presented by real-world optimization problems, in order to characterize the constraint functions in constrained multiobjective optimization problems (CMOPs). These are feasibility-hardness, convergence-hardness, and diversity-hardness. We then develop a general toolkit to construct difficulty adjustable and scalable CMOPs (DAS-CMOPs, or DAS-CMaOPs when the number of objectives is greater than three) with three types of parameterized constraint functions developed to capture the three proposed types of difficulty. In fact, the combination of the three primary constraint functions with different parameters allows the construction of a large variety of CMOPs, with difficulty that can be defined by a triplet, with each of its parameters specifying the level of one of the types of primary difficulty. Furthermore, the number of objectives in this toolkit can be scaled beyond three. Based on this toolkit, we suggest nine difficulty adjustable and scalable CMOPs and nine CMaOPs, to be called DAS-CMOP1-9 and DAS-CMaOP1-9, respectively. To evaluate the proposed test problems, two popular CMOEAs-MOEA/D-CDP (MOEA/D with constraint dominance principle) and NSGA-II-CDP (NSGA-II with constraint dominance principle) and two popular constrained many-objective evolutionary algorithms (CMaOEAs)-C-MOEA/DD and C-NSGA-III-are used to compare performance on DAS-CMOP1-9 and DAS-CMaOP1-9 with a variety of difficulty triplets, respectively. The experimental results reveal that mechanisms in MOEA/D-CDP may be more effective in solving convergence-hard DAS-CMOPs, while mechanisms of NSGA-II-CDP may be more effective in solving DAS-CMOPs with simultaneous diversity-, feasibility-, and convergence-hardness. Mechanisms in C-NSGA-III may be more effective in solving feasibility-hard CMaOPs, while mechanisms of C-MOEA/DD may be more effective in solving CMaOPs with convergence-hardness. In addition, none of them can solve these problems efficiently, which stimulates us to continue to develop new CMOEAs and CMaOEAs to solve the suggested DAS-CMOPs and DAS-CMaOPs.																	1063-6560	1530-9304				SEP	2020	28	3					339	378		10.1162/evco_a_00259													
J								Generating New Space-Filling Test Instances for Continuous Black-Box Optimization	EVOLUTIONARY COMPUTATION										Algorithm selection; benchmarking; black-box continuous optimization; exploratory landscape analysis; instance generator	ALGORITHM PERFORMANCE; BENCHMARKING; OPTIMIZERS	This article presents a method to generate diverse and challenging new test instances for continuous black-box optimization. Each instance is represented as a feature vector of exploratory landscape analysis measures. By projecting the features into a two-dimensional instance space, the location of existing test instances can be visualized, and their similarities and differences revealed. New instances are generated through genetic programming which evolves functions with controllable characteristics. Convergence to selected target points in the instance space is used to drive the evolutionary process, such that the new instances span the entire space more comprehensively. We demonstrate the method by generating two-dimensional functions to visualize its success, and ten-dimensional functions to test its scalability. We show that the method can recreate existing test functions when target points are co-located with existing functions, and can generate new functions with entirely different characteristics when target points are located in empty regions of the instance space. Moreover, we test the effectiveness of three state-of-the-art algorithms on the new set of instances. The results demonstrate that the new set is not only more diverse than a well-known benchmark set, but also more challenging for the tested algorithms. Hence, the method opens up a new avenue for developing test instances with controllable characteristics, necessary to expose the strengths and weaknesses of algorithms, and drive algorithm development.																	1063-6560	1530-9304				SEP	2020	28	3					379	404		10.1162/evco_a_00262													
J								Diagonal Acceleration for Covariance Matrix Adaptation Evolution Strategies	EVOLUTIONARY COMPUTATION										Evolution strategies; covariance matrix adaptation; adaptive diagonal decoding; active covariance matrix update; default strategy parameters	SELF-ADAPTATION; CMA-ES; TIME	We introduce an acceleration for covariance matrix adaptation evolution strategies (CMA-ES) by means ofadaptive diagonal decoding(dd-CMA). This diagonal acceleration endows the default CMA-ES with the advantages of separable CMA-ES without inheriting its drawbacks. Technically, we introduce a diagonal matrixDthat expresses coordinate-wise variances of the sampling distribution inDCDform. The diagonal matrix can learn a rescaling of the problem in the coordinates within a linear number of function evaluations. Diagonal decoding can also exploit separability of the problem, but, crucially, does not compromise the performance on nonseparable problems. The latter is accomplished by modulating the learning rate for the diagonal matrix based on the condition number of the underlying correlation matrix. dd-CMA-ES not only combines the advantages of default and separable CMA-ES, but may achieve overadditive speedup: it improves the performance, and even the scaling, of the better of default and separable CMA-ES on classes of nonseparable test functions that reflect, arguably, a landscape feature commonly observed in practice. The article makes two further secondary contributions: we introduce two different approaches to guarantee positive definiteness of the covariance matrix with active CMA, which is valuable in particular with large population size; we revise the default parameter setting in CMA-ES, proposing accelerated settings in particular for large dimension. All our contributions can be viewed as independent improvements of CMA-ES, yet they are also complementary and can be seamlessly combined. In numerical experiments with dd-CMA-ES up to dimension 5120, we observe remarkable improvements over the original covariance matrix adaptation on functions with coordinate-wise ill-conditioning. The improvement is observed also for large population sizes up to about dimension squared.																	1063-6560	1530-9304				SEP	2020	28	3					405	435		10.1162/evco_a_00260													
J								Simple Hyper-Heuristics Control the Neighbourhood Size of Randomised Local Search Optimally for LeadingOnes	EVOLUTIONARY COMPUTATION										Hyper-heuristics; online algorithm selection; runtime analysis; theory; mutation; selection	RUNTIME ANALYSIS; GENETIC ALGORITHMS; COMPLEXITY; MUTATION	Selection hyper-heuristics (HHs) are randomised search methodologies which choose and execute heuristics during the optimisation process from a set of low-level heuristics. A machine learning mechanism is generally used to decide which low-level heuristic should be applied in each decision step. In this article, we analyse whether sophisticated learning mechanisms are always necessary for HHs to perform well. To this end we consider the most simple HHs from the literature and rigorously analyse their performance for theLeadingOnesbenchmark function. Our analysis shows that the standard Simple Random, Permutation, Greedy, and Random Gradient HHs show no signs of learning. While the former HHs do not attempt to learn from the past performance of low-level heuristics, the idea behind the Random Gradient HH is to continue to exploit the currently selected heuristic as long as it is successful. Hence, it is embedded with a reinforcement learning mechanism with the shortest possible memory. However, the probability that a promising heuristic is successful in the next step is relatively low when perturbing a reasonable solution to a combinatorial optimisation problem. We generalise the "simple" Random Gradient HH so success can be measured over a fixed period of time tau, instead of a single iteration. ForLeadingOneswe prove that theGeneralised Random Gradient (GRG)HH can learn to adapt the neighbourhood size of Randomised Local Search to optimality during the run. As a result, we prove it has the best possible performance achievable with the low-level heuristics (Randomised Local Search with different neighbourhood sizes), up to lower-order terms. We also prove that the performance of the HH improves as the number of low-level local search heuristics to choose from increases. In particular, with access toklow-level local search heuristics, it outperforms the best-possible algorithm using any subset of thekheuristics. Finally, we show that the advantages of GRG over Randomised Local Search and Evolutionary Algorithms using standard bit mutation increase if the anytime performance is considered (i.e., the performance gap is larger if approximate solutions are sought rather than exact ones). Experimental analyses confirm these results for different problem sizes (up ton=10(8)) and shed some light on the best choices for the parameter tau in various situations.																	1063-6560	1530-9304				SEP	2020	28	3					437	461		10.1162/evco_a_00258													
J								Analysis of the (mu/mu(I), lambda)-CSA-ES with Repair by Projection Applied to a Conically Constrained Problem	EVOLUTIONARY COMPUTATION										Evolution strategy; constraint handling; repair by projection; cumulative step size adaptation; conically constrained problem	SELF-ADAPTATION	Theoretical analyses of evolution strategies are indispensable for gaining a deep understanding of their inner workings. For constrained problems, rather simple problems are of interest in the current research. This work presents a theoretical analysis of a multi-recombinative evolution strategy with cumulative step size adaptation applied to a conically constrained linear optimization problem. The state of the strategy is modeled by random variables and a stochastic iterative mapping is introduced. For the analytical treatment, fluctuations are neglected and the mean value iterative system is considered. Nonlinear difference equations are derived based on one-generation progress rates. Based on that, expressions for the steady state of the mean value iterative system are derived. By comparison with real algorithm runs, it is shown that for the considered assumptions, the theoretical derivations are able to predict the dynamics and the steady state values of the real runs.																	1063-6560	1530-9304				SEP	2020	28	3					463	488		10.1162/evco_a_00261													
J								EvoComposer: An Evolutionary Algorithm for 4-Voice Music Compositions	EVOLUTIONARY COMPUTATION										Evolutionary algorithms; automatic music composition; evolutionary music	CONSENSUAL ASSESSMENT TECHNIQUE; CONTENT GENERATION; CREATIVITY	Evolutionary algorithms mimic evolutionary behaviors in order to solve problems. They have been successfully applied in many areas and appear to have a special relationship with creative problems; such a relationship, over the last two decades, has resulted in a long list of applications, including several in the field of music. In this article, we provide an evolutionary algorithm able to compose music. More specifically we consider the following 4-voice harmonization problem: one of the 4 voices (which are bass, tenor, alto, and soprano) is given as input and the composer has to write the other 3 voices in order to have a complete 4-voice piece of music with a 4-note chord for each input note. Solving such a problem means finding appropriate chords to use for each input note and also finding a placement of the notes within each chord so that melodic concerns are addressed. Such a problem is known as theunfigured harmonization problem. The proposed algorithm for the unfigured harmonization problem, namedEvoComposer, uses a novel representation of the solutions in terms of chromosomes (that allows to handle both harmonic and nonharmonic tones), specialized operators (that exploit musical information to improve the quality of the produced individuals), and a novelhybridmultiobjective evaluation function (based on an original statistical analysis of a large corpus of Bach's music). Moreover EvoComposer is the first evolutionary algorithm for this specific problem. EvoComposer is a multiobjective evolutionary algorithm, based on the well-known NSGA-II strategy, and takes into consideration two objectives: the harmonic objective, that is finding appropriate chords, and the melodic objective, that is finding appropriate melodic lines. The composing process is totally automatic, without any human intervention. We also provide an evaluation study showing that EvoComposer outperforms other metaheuristics by producing better solutions in terms of both well-known measures ofperformance, such as hypervolume, Delta index, coverage of two sets, and standard measures ofmusic creativity. We conjecture that a similar approach can be useful also for similar musical problems.																	1063-6560	1530-9304				SEP	2020	28	3					489	530		10.1162/evco_a_00265													
J								Granular Modeling of Fuzzy Color Categories	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Image color analysis; Color; Prototypes; Computational modeling; Semantics; Proposals; Linguistics; Color modeling; fuzzy color; fuzzy color space; human perception; image analysis	COMPUTATIONAL MODEL; WEIGHTING METHOD; RETRIEVAL; GENERATION; QUANTIZATION; FEATURES; SPACE	In this paper, we introduce fuzzy granular colors for modeling color categories. Fuzzy granular colors are built by aggregating fuzzy colors having semantic relationships with regard to a certain color category. Our proposal allows us to model color categories which comprise disjoint fuzzy subsets of colors, as well as those having a nonconvex representation in the color space, among other advantages. Such categories are used very often by humans in different real contexts. Fuzzy granular colors are appropriate to provide color models able to deal with ill-defined boundaries, subjectivity, and context-dependence. We illustrate the advantages of our approach with respect to current state of the art with several experiments.																	1063-6706	1941-0034				SEPT	2020	28	9					1897	1909		10.1109/TFUZZ.2019.2923966													
J								An Improved Fuzzy Min-Max Neural Network for Data Classification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptation models; Brain modeling; Artificial neural networks; Training; Biological neural networks; Pattern classification; Adaptive systems; Enhanced fuzzy min-max (EFMM) model; fuzzy min-max (FMM) neural network; histopathological images; hyperbox classifier; pattern classification; semiperimeter	ADAPTIVE PATTERN-CLASSIFICATION; FAULT-DETECTION; ALGORITHM; OPTIMIZATION; SYSTEMS; RULE; IDENTIFICATION; PREDICTION; MODELS	Hyperbox classifier is an efficient tool for modern pattern classification problems due to its transparency and rigorous use of Euclidian geometry. Fuzzy min-max (FMM) network efficiently implements the hyperbox classifier, and has been modified several times to yield better classification accuracy. However, the obtained accuracy is not up to the mark. Therefore, in this paper, a new improved FMM (IFMM) network is proposed to increase the accuracy rate. In the proposed IFMM network, a modified constraint is employed to check the expandability of a hyperbox. It also uses semiperimeter of the hyperbox along with k-nearest mechanism to select the expandable hyperbox. In the proposed IFMM, the contraction rules of conventional FMM and enhanced FMM (EFMM) are also modified using semiperimeter of a hyperbox in order to balance the size of both overlapped hyperboxes. Experimental results show that the proposed IFMM network outperforms the FMM, k-nearest FMM, and EFMM by yielding more accuracy rate with less number of hyperboxes. The proposed methods are also applied to histopathological images to know the best magnification factor for classification.																	1063-6706	1941-0034				SEPT	2020	28	9					1910	1924		10.1109/TFUZZ.2019.2924396													
J								Interval Type-2 Fuzzy Local Enhancement Based Rough K-Means Clustering Considering Imbalanced Clusters	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Clustering algorithms; Approximation algorithms; Fuzzy sets; Graphical models; Distribution functions; Rough sets; Uncertainty; Imbalanced clusters; interval type-2 fuzzy sets (IT2FS); local fuzzy enhancement; rough k-means (RKM)	C-MEANS ALGORITHM; UNDERSAMPLING METHOD; CLASSIFICATION	Rough K-Means (RKM) is an efficient clustering algorithm for overlapping datasets, and has captured increasing attention in recent years. RKM algorithms are the main focus on the further description of uncertain objects located in boundary regions in order to improve the performance. However, most available RKM algorithms fail to pay attention to the influence of imbalanced clusters, together with imbalanced spatial distributions (i.e., the cluster density) and differing cluster sizes (i.e., the number of object ratios). This paper seeks to address this deficiency and examines in detail some adverse effects caused by imbalanced clusters. To mitigate adverse effects of imbalanced clusters and decrease the computational cost, an interval type-2 fuzzy local measure for the RKM clustering is proposed, on the basis of which, a novel RKM clustering algorithm has been developed that specifically gives due consideration to imbalanced clusters. The effectiveness and superiority of this algorithm are demonstrated through simulation and experimental analysis.																	1063-6706	1941-0034				SEPT	2020	28	9					1925	1939		10.1109/TFUZZ.2019.2924402													
J								An Interval Type-3 Fuzzy System and a New Online Fractional-Order Learning Algorithm: Theory and Practice	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy sets; Stability analysis; Approximation algorithms; Uncertainty; Heuristic algorithms; Fuzzy neural networks; Approximation performance; fractional order; interval type-3 fuzzy system (IT3FS); stable learning algorithm; type-2 fuzzy systems (T2FS)	LOGIC SYSTEMS; CHAOTIC SYSTEMS; NEURAL-NETWORK; IDENTIFICATION; SYNCHRONIZATION; REGRESSION; REDUCTION	The main reason of the extensive usage of the fuzzy systems in many branches of science is their approximation ability. In this paper, an interval type-3 fuzzy system (IT3FS) is proposed. The uncertainty modeling capability of the proposed IT3FS is improved in contrast to type-1 and type-2 fuzzy systems (T1FS and T2FS). Because in the proposed IT3FS, the membership is defined as an interval type-2 fuzzy set, whereas in T1FS and T2FS, the membership is crisp value and type-1 fuzzy set, respectively. An online fractional-order learning algorithm is given to optimize the consequent parameters of the IT3FS. The stability of the learning algorithm is proved by utilizing the Lyapunov method. The validity of the proposed fuzzy system is illustrated by both simulation and the experimental studies. It is shown that the proposed fuzzy system and associated learning algorithm result in better approximation performance in comparison with the other well-known approaches.																	1063-6706	1941-0034				SEPT	2020	28	9					1940	1950		10.1109/TFUZZ.2019.2928509													
J								Adaptive Sliding Mode Observer Design for a Class of T-S Fuzzy Descriptor Fractional Order Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Observers; Fuzzy systems; Control systems; Adaptive systems; Nonlinear systems; Linear matrix inequalities; Tools; Adaptive sliding mode control strategy; descriptor fractional order systems (FOSs); linear matrix inequalities (LMIs); sliding mode observer (SMO)	MARKOVIAN JUMP SYSTEMS; SUFFICIENT CONDITIONS; DYNAMICAL-SYSTEMS; STABILIZATION; ACTUATOR; CONTROLLER; LMI	This paper investigates the problem of sliding mode observer (SMO) design for Takagi-Sugeno (T-S) fuzzy descriptor fractional order systems (FOSs) with fractional order 0 < alpha < 1. First, an SMO is designed for the T-S fuzzy descriptor FOS. Then, a fuzzy fractional order integral-type sliding surface is constructed. By utilizing the fuzzy sliding surface, the assumption that all local input matrices are identical in most existing results about sliding mode control (SMC) for T-S fuzzy systems, can be removed. Then, a new sufficient condition is proposed in terms of linear matrix inequalities (LMIs), which guarantees the admissibility of the sliding mode dynamics. Moreover, adaptive SMC strategy is used such that the reachability condition can be guaranteed. Finally, three examples are given to verify the effectiveness of our results.																	1063-6706	1941-0034				SEPT	2020	28	9					1951	1960		10.1109/TFUZZ.2019.2928511													
J								Generalizations of Weighted Means and OWA Operators by Using Unimodal Weighting Vectors	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Open wireless architecture; Games; Indexes; Closed-form solutions; Distance measurement; Convex functions; Choquet integral; ordered weighted averaging (OWA) operators; semiuninorm-based ordered weighted averaging (SUOWA) operators; semi-SUOWA operators; the crescent method; unimodal weighting vectors; weighted means	SUOWA OPERATORS; AGGREGATION	Weighted means and ordered weighted averaging (OWA) operators are two families of functions well known in the literature. Given that both are specific cases of the Choquet integral, several procedures for constructing capacities that generalize simultaneously those of the weighted means and the OWA operators have been suggested in recent years. In this paper, we propose two methods that allow us to address the previous issue and that provide us with a wide variety of capacities when the weighting vector associated with the OWA operator is unimodal.																	1063-6706	1941-0034				SEPT	2020	28	9					1961	1970		10.1109/TFUZZ.2019.2928513													
J								A General Joint Matrix Factorization Framework for Data Integration and Its Systematic Algorithmic Exploration	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Sparse matrices; Pattern recognition; Data integration; Prediction algorithms; Data models; Matrix decomposition; Signal processing algorithms; Bioinformatics; data integration; network-regularized constraint; nonnegative matrix factorization (NMF); pattern recognition	CONSTRAINED LEAST-SQUARES; TRIPLE-HELIX REPEAT; BREAST-CANCER; GENOMIC DATA; EXPRESSION; DISCOVERY; FUSION; MODEL	Nonnegative matrix factorization (NMF) is a powerful tool in data exploratory analysis by discovering hidden features and part-based patterns from high-dimensional data. NMF and its variants have been successfully applied into diverse fields such as pattern recognition, signal processing, data mining, bioinformatics, and so on. Recently, NMF has been extended to analyze multiple matrices simultaneously. However, a general framework and its systematic algorithmic exploration are still lacking. In this paper, we first introduce a sparse multiple relationship data regularized joint matrix factorization (JMF) framework and two adapted prediction models for pattern recognition and data integration. Next, we present four update algorithms to solve this framework in a very comprehensive manner. The merits and demerits of these algorithms are systematically explored. Furthermore, extensive computational experiments using both synthetic data and real data demonstrate the effectiveness of JMF framework and related algorithms on pattern recognition and data mining.																	1063-6706	1941-0034				SEPT	2020	28	9					1971	1983		10.1109/TFUZZ.2019.2928518													
J								Collective Scenario Understanding in a Multivehicle System by Consensus Decision Making	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Ontologies; Decision making; Reliability; Linguistics; Task analysis; Knowledge based systems; Optimization; Consensus measures; fuzzy ontology; group decision making (GDM); situation awareness; unmanned vehicles (UVs)	FUZZY ONTOLOGY; AGGREGATION OPERATORS; PREFERENCE RELATIONS; COOPERATIVE SEARCH; SENTIMENT ANALYSIS; CONSISTENCY; UAVS; OPTIMIZATION; MODEL	In recent years, unmanned vehicles (UVs) have been largely employed in many applications. They, enhanced with computer vision and artificial intelligence, can autonomously recognize targets in an environment and detect events occurring in a real-world scenario. The employment of cooperative UVs can provide multiple interpretations supporting a multiperspective view of the scene. However, UV multiple interpretations often diverge, therefore, UVs need to find an agreed interpretation of the scenario. To this purpose, this paper proposes a novel consensus-based approach to lead multi-UV systems to find agreement on what they observe and build a group situation-based description of the scenario. UVs are modeled as experts in a group decision making problem that must decide on which situations best describe the scenario. First, the approach allows each UV to build high-level situations from the detected events through a fuzzy-based event aggregation. The event aggregation is modeled with a fuzzy ontology which allows each UV to express preferences on the situations. Then, a collective interpretation of situations is achieved by consensing each UV interpretation. Finally, consensus and proximity measures support the evaluation of the final group decision reliability. The assessed consensus reflects how much the collective scenario interpretation fits each UV perspective. The proximity measures support the detection of reliable and unreliable UVs to serve many tasks (i.e., mission replanning, damaged UV detection, etc.).																	1063-6706	1941-0034				SEPT	2020	28	9					1984	1995		10.1109/TFUZZ.2019.2928787													
J								Patch Learning	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Training; Data models; Machine learning; Computational modeling; Training data; Two dimensional displays; Ensemble learning; fuzzy system; patch learning (PL); regression		There have been different strategies to improve the performance of a machine learning model, e.g., increasing the depth, width, and/or nonlinearity of the model, and using ensemble learning to aggregate multiple base/weak learners in parallel or in series. This article proposes a novel strategy called patch learning (PL) for this problem. It consists of three steps: first, train an initial global model using all training data; second, identify from the initial global model the patches that contribute the most to the learning error, and train a (local) patch model for each such patch; and, third, update the global model using training data that do not fall into any patch. To use a PL model, we first determine if the input falls into any patch. If yes, then the corresponding patch model is used to compute the output. Otherwise, the global model is used. We explain in detail how PL can be implemented using fuzzy systems. Five regression problems on one-dimensional (1-D)/2-D/3-D curve fitting, nonlinear system identification, and chaotic time-series prediction, verified its effectiveness. To our knowledge, the PL idea has not appeared in the literature before, and it opens up a promising new line of research in machine learning.																	1063-6706	1941-0034				SEPT	2020	28	9					1996	2008		10.1109/TFUZZ.2019.2930022													
J								A Novel Fuzzy Clustering-Based Histogram Model for Image Contrast Enhancement	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Contrast enhancement; fuzzy clustering; histogram equalization; image histogram; lowess smoothing; probability estimate function	CUCKOO SEARCH ALGORITHM; MODIFICATION SCHEME; GAMMA CORRECTION; MEAN BRIGHTNESS; EQUALIZATION; SEGMENTATION	Histogram equalization is a famous method for enhancing the contrast and image features. However, in few cases, it causes the overenhancement, and hence demolishes the natural display of the image. Therefore, in this article, a new fuzzy clustering based subhistogram scheme using discrete cosine transform (DCT) for contrast enhancement has been proposed. For preserving the distinctive appearance of the image, histogram division and separate histogram equalization is done on each subhistogram. The way of dividing histogram and calculating the numbers of parts for histogram division are the major problems which directly affects the quality of the output image. The proposed fuzzy-DCT scheme includes automatic calculation of a number of parts in which histogram is divided. Histogram division has done on the basis of density function and histogram separation is computed in such a way that each main peak can be divided in a different segment. The proposed scheme consists of four stages. The first stage includes the automatic calculation of number of clusters for image brightness levels. The second stage includes clustering of brightness levels by the fuzzy c-means clustering method and utilizing the given transfer function of histogram equalization. In the third stage, contrast enhancement is computed on each individual cluster separately. In the final stage, DCT is employed on the resulting image of the third step for better contrast and brightness preservation. The simulation results of the proposed scheme reveal not only clearer features along with a contrast enhancement, but also remarkably more natural look in the images.																	1063-6706	1941-0034				SEPT	2020	28	9					2009	2021		10.1109/TFUZZ.2019.2930028													
J								Global Synchronization of Fuzzy Memristive Neural Networks With Discrete and Distributed Delays	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Delays; Synchronization; Fuzzy systems; Artificial neural networks; Memristors; Biological neural networks; Adaptive control; Discrete delays; distributed delays; fuzzy memristive neural networks (FMNNs); image encryption; synchronization	EXPONENTIAL SYNCHRONIZATION; TIME-DELAY; STABILITY; STABILIZATION; SYNAPSE	This paper investigates the synchronization problem of Takagi-Sugeno fuzzy memristive neural networks (FMNNs) with mixed delays, in which the bounded distributed and unbounded discrete time-varying delays are involved. Then, under the nonsmooth analysis and Lyapunov stability theory, several easily verified algebraic criteria are established to guarantee the global synchronization of FMNNs via a designed fuzzy feedback controller. Moreover, to show the superiority of the theoretical results, several discussions and comparisons with existing work are provided, indicating that derived results in this paper are general and include several existing ones as special cases. Finally, two numerical examples and two applications in psuedorandom number generation and image encryption are presented to show the validity and practicability of the theoretical results.																	1063-6706	1941-0034				SEPT	2020	28	9					2022	2034		10.1109/TFUZZ.2019.2930032													
J								Fuzzy Adaptive Robust Control for Stochastic Switched Nonlinear Systems With Full-State-Dependent Nonlinearities	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Switches; Nonlinear systems; Adaptive systems; Output feedback; Backstepping; Stochastic processes; Average dwell time (ADT); full-state dependent nonlinearities; fuzzy adaptive robust control; output feedback; stochastic switched nonlinear systems	DYNAMIC SURFACE CONTROL; OUTPUT-FEEDBACK STABILIZATION; H-INFINITY CONTROL; UNIVERSAL APPROXIMATION; PRESCRIBED PERFORMANCE; CONTROL DESIGN; STABILITY	This paper solves the problem of fuzzy adaptive robust output feedback control using average dwell-time switching for a class of stochastic switched nonstrict feedback nonlinear systems with more general uncertainties, including unmodeled dynamics, unknown control coefficients, unknown drift/diffusion terms, and unknown output functions. The unmodeled dynamics and drift/diffusion terms are allowed to depend on the whole unmeasurable states, whose effect is difficult to counteract. By virtue of the fuzzy approximation capability and variable partition technique, an adaptive backstepping procedure is proposed to compensate these uncertainties. Based on the constructed switched observer, which shows its strong robustness to the unknown control coefficients and output functions, an output feedback controller is designed to render all the signals of the closed-loop system bounded in probability under an appropriately chosen average dwell time.																	1063-6706	1941-0034				SEPT	2020	28	9					2035	2047		10.1109/TFUZZ.2019.2930034													
J								Fuzzy Descriptor Sliding Mode Observer Design: A Canonical Form-Based Method	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Observers; Switches; Design methodology; Fuzzy systems; Linear matrix inequalities; Nonlinear systems; Sliding mode control; Integral-type switching function; linear switching function; sliding mode observer; T-S fuzzy descriptor systems	LYAPUNOV FUNCTION-APPROACH; FAULT-TOLERANT CONTROL; CONTROLLER-DESIGN; SYSTEMS; STABILIZATION; RECONSTRUCTION; STABILITY; VEHICLE	Linear switching function and integral-type switching function are two typical switching functions in the sliding mode control field. Based on these, this paper investigates the design problem of fuzzy descriptor sliding mode observers. Two canonical forms, which are the T-S fuzzy descriptor systems' counterparts of the canonical form in normal systems, are first proposed. In terms of the proposed canonical forms, a linear switching function-based fuzzy descriptor sliding mode observer and an integral-type switching function-based fuzzy descriptor sliding mode observer are designed, respectively. It is shown that although the linear switching function-based fuzzy descriptor sliding mode observer has a much simpler design structure than the integral-type switching function-based fuzzy descriptor sliding mode observer, the integral-type switching function-based design method can deal with a much larger range of T-S fuzzy descriptor systems than the linear switching function-based design method. Finally, three simulation examples are provided to verify the effectiveness and merits of the proposed method.																	1063-6706	1941-0034				SEPT	2020	28	9					2048	2062		10.1109/TFUZZ.2019.2930036													
J								Improved Fuzzy Bayesian Network-Based Risk Analysis With Interval-Valued Fuzzy Sets and D-S Evidence Theory	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Uncertainty; Risk analysis; Bayes methods; Fuzzy sets; Safety; Complex systems; Modeling; Dempster-Shafer (D-S) evidence theory; fuzzy Bayesian network (BN); interval-valued fuzzy set (IVFS); Latin hypercube sampling; risk analysis	SAFETY ANALYSIS; PROCESS SYSTEMS; BELIEF NETWORK; UNCERTAINTY; COMBINATION; EXTENSION; MODEL	A novel risk analysis approach is developed by merging interval-valued fuzzy sets (IVFSs), improved Dempster-Shafer (D-S) evidence theory, and fuzzy Bayesian networks (BNs), acting as a systematic decision support approach for safety insurance for the entire life cycle of a complex system under uncertainty. Aiming to alleviate the problem of insufficient and imprecise data collected from the complicated environment, the expert judgment in linguistic expressions is employed to describe the risk levels for all risk factors, which are represented by IVFSs using Gaussian membership function to fully consider such fuzziness and uncertainty. In regard to interval fusion and highly conflicting data, an improved combination rule based on the D-S evidence theory is developed. Then, fuzzy prior probability for each risk factor can be generated from fused intervals and fed into a fuzzy BN model for fuzzy-based Bayesian inference, including predictive, sensitivity, and diagnosis analysis. Furthermore, a case study is used to demonstrate the feasibility of the proposed risk analysis. A comparison of risk analysis based upon the hybrid improved D-S, classical D-S, and arithmetic average method is illustrated to show the outstanding performance of the developed approach in fusing multisource information with ubiquitous uncertainty and conflicts in an efficient manner, leading to more reliable risk evaluation. It is concluded that the proposed risk analysis provides a deep insight on risk control, especially for complex project environment, which enables to not only reduce the likelihood of failure ahead of time but also mitigate risk magnitudes to some degree after the occurrence of a failure.																	1063-6706	1941-0034				SEPT	2020	28	9					2063	2077		10.1109/TFUZZ.2019.2929024													
J								Automatic Fuzzy Clustering Framework for Image Segmentation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Clustering algorithms; Image segmentation; Computational complexity; Linear programming; Color; Entropy; Histograms; Density peak (DP) algorithm; fuzzy clustering; image segmentation; superpixel	LOCAL INFORMATION; ALGORITHM; FCM	Clustering algorithms by minimizing an objective function share a clear drawback of having to set the number of clusters manually. Although density peak clustering is able to find the number of clusters, it suffers from memory overflow when it is used for image segmentation because a moderate-size image usually includes a large number of pixels leading to a huge similarity matrix. To address this issue, here we proposed an automatic fuzzy clustering framework (AFCF) for image segmentation. The proposed framework has threefold contributions. First, the idea of superpixel is used for the density peak (DP) algorithm, which efficiently reduces the size of the similarity matrix and thus improves the computational efficiency of the DP algorithm. Second, we employ a density balance algorithm to obtain a robust decision-graph that helps the DP algorithm achieve fully automatic clustering. Finally, a fuzzy c-means clustering based on prior entropy is used in the framework to improve image segmentation results. Because the spatial neighboring information of both the pixels and membership are considered, the final segmentation result is improved effectively. Experiments show that the proposed framework not only achieves automatic image segmentation, but also provides better segmentation results than state-of-the-art algorithms.																	1063-6706	1941-0034				SEPT	2020	28	9					2078	2092		10.1109/TFUZZ.2019.2930030													
J								Multiple-Surface-Approximation-Based FCM With Interval Memberships for Bias Correction and Segmentation of Brain MRI	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Magnetic resonance imaging; Image segmentation; Linear programming; Brain; Surface fitting; Nonhomogeneous media; Fitting; Bias field correction; fuzzy c-means (FCM); magnetic resonance image (MRI) segmentation	INTENSITY INHOMOGENEITY CORRECTION; IMAGE SEGMENTATION; FIELD ESTIMATION; FUZZY; FUSION	Fuzzy c-means (FCM) is a popular clustering method for image segmentation. However, FCM has difficulties in handling artifacts in brain magnetic resonance imaging (MRI), especially when it comes to bias field and noise. We propose a novel multiple-surface-approximation-based FCM with interval membership method for simultaneous bias correction and segmentation of Brain MRI. First, multiple surface representation of bias field is embedded into FCM to estimate and correct bias field. Then memberships of the improved FCM are extended to intervals. After the extension, clustering centers of different MR brain tissues could be solved more properly by the proposed method. Moreover, the proposed method is less sensitive to noise by introducing effects of neighboring pixels. Experiments conducted on artificial images and synthetic and real clinical Brain MRI show that the proposed method is effective and obtains better results of both bias field correction and segmentation than comparing methods.																	1063-6706	1941-0034				SEPT	2020	28	9					2093	2106		10.1109/TFUZZ.2019.2930478													
J								State and Fault Observer Design for Switched Systems via an Adaptive Fuzzy Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Observers; Switched systems; Adaptive systems; Switches; Fuzzy logic; Actuators; Adaptive observer; fuzzy logic systems (FLSs); state and fault estimation; switched systems	DISCRETE-EVENT SYSTEMS; SLIDING-MODE OBSERVER; SUPERVISORY CONTROL; RECONSTRUCTION; ACTUATOR; SENSOR	This article investigates the state and fault estimation problem for switched systems with external disturbances, unknown nonlinear dynamics, and actuator and sensor faults. In this article, a new adaptive fuzzy-based observer is designed, where the prior knowledge of the nonlinearities, disturbances, and faults is not required. In contrast to the sliding mode observer (SMO), the proposed observer can guarantee the convergence of the error trajectory through the online adaptive mechanism, avoiding the sliding surface switching problem in SMO design. The accurate state asymptotic estimations can also be provided by the developed design approach subject to the simultaneous additive and multiplicative faults. Finally, a simulation example is presented to show the effectiveness of the proposed method.																	1063-6706	1941-0034				SEPT	2020	28	9					2107	2118		10.1109/TFUZZ.2019.2930485													
J								Envelopment Analysis, Preference Fusion, and Membership Improvement of Intuitionistic Fuzzy Numbers	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Decision making; Fuzzy sets; Indexes; Biological system modeling; Data envelopment analysis; Companies; Decision making; dual form; intuitionistic fuzzy envelopment analysis (IFEA); membership improvement; preference fusion	DECISION-MAKING PROBLEMS; GEOMETRIC AGGREGATION OPERATORS; TECHNICAL EFFICIENCY; SIMILARITY MEASURES; BONFERRONI MEANS; ANALYSIS MODEL; ANALYSIS DEA; PERFORMANCE; UNITS; SETS	In this article, we propose a novel intuitionistic fuzzy decision making approach from the perspective of envelopment analysis. This method helps make a decision by calculating the intuitionistic fuzzy efficiencies of all the alternatives. The calculated efficiency values are relative indexes instead of stock indexes such as the aggregated and measured values. A prominent advantage is that this method can distinguish the efficient and inefficient alternatives and further improve the inefficient ones by modifying their membership and nonmembership degrees. Consequently, we can optimize the inefficient alternatives in the future, which cannot be achieved by other similar methods. To do this, we construct two intuitionistic fuzzy envelopment analysis (IFEA) models based on the membership and nonmembership degrees, respectively, which are different from the data envelopment analysis (DEA) and the fuzzy DEA. Their dual forms are derived so that these models can be transformed into the linear programming forms. With respect to the attributes' differences, two preference IFEA models are developed by integrating subjective preference relations. Moreover, we provide the membership improvement formulas and summarize the specific decision making and membership improvement processes for practical applications. Finally, an example of ranking and improving the six listed companies is presented to demonstrate these approaches.																	1063-6706	1941-0034				SEPT	2020	28	9					2119	2130		10.1109/TFUZZ.2019.2930483													
J								A Weighted Similarity Measure Between Z-Numbers and Bow-Tie Quantification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy sets; Safety; Weight measurement; Loss measurement; Accidents; Reliability theory; Bow-tie; fuzzy sets (FSs); Hausdorff distance; safety analytics; Z-numbers; Z-similarity	RISK ANALYSIS; TREE ANALYSIS; FUZZY; COMPONENTS; ENTROPY; LEAKAGE; OIL	In this article, we propose a new weighted Z-similarity measure between two Z-numbers, which is able to retain the original information provided by experts in linguistic terms. This measure takes care of the directional aspect of reliability of information by leveraging the Hausdorff distance. Probabilistic-approach-based statistical distance measure is used to characterize the internal relationship between two parts of a Z-number. Some properties of the Z-similarity measure are proved. The measure is capable of quantifying the probability of basic events in bow-tie analysis. It is experimentally shown that the Z-similarity measure is able to overcome the lacuna of the available techniques. Sensitivity analysis is done to verify its feasibility and applicability. We further propose another measure, called Z-similarity-based basic event contribution (Z-BEC), to quantify the contribution of basic events to the occurrence of different accidents. The performance of the Z-BEC measure is compared with that of Fussell-Vesely index and Birnbaum's structural index.																	1063-6706	1941-0034				SEPT	2020	28	9					2131	2142		10.1109/TFUZZ.2019.2930935													
J								IOWA-SVM: A Density-Based Weighting Strategy for SVM Classification via OWA Operators	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Support vector machines; Open wireless architecture; Anomaly detection; Fasteners; Entropy; Training; Kernel; Density-based clustering; fuzzy clustering; induced ordered weighted averaging (OWA) (IOWA); OWA operators; support vector machines (SVMs)	SUPPORT VECTOR REGRESSION; AGGREGATION OPERATORS; DISTANCE MEASURES; DECISION-MAKING; MOVING AVERAGES; MACHINES	A weighting strategy for handling outliers in binary classification using support vector machine (SVM) is proposed in this article. The traditional SVM model is modified by introducing an induced ordered weighted averaging (IOWA) operator, in which the hinge loss function becomes an ordered weighted sum of the SVM slack variables. These weights are defined using IOWA quantifiers, while the order is induced via fuzzy density-based methods for outlier detection. The proposal is developed for both linear and kernel-based classification using the duality theory and the kernel trick. Our experimental results on well known benchmark datasets demonstrate the virtues of the proposed IOWA-SVM, which achieved the best average performance compared to other machine learning approaches of similar complexity.																	1063-6706	1941-0034				SEPT	2020	28	9					2143	2150		10.1109/TFUZZ.2019.2930942													
J								Interval Type-2 Fuzzy Set and Theory of Weak Continuity Constraints for Accurate Multiclass Image Segmentation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Image segmentation; Uncertainty; Fuzzy sets; Tools; Minimization; Benchmark testing; Linear programming; Center of gravity (COG); image segmentation; thresholding; type-2 fuzzy set (T2FS); weak continuity constraints	C-MEANS ALGORITHM; ENTROPY; MODEL	Multiclass image segmentation is a challenging task due to the uncertainties involved with the process of segmentation. To handle those uncertainties, we propose an automatic multiclass image segmentation method based on an interval type-2 fuzzy set (IT2FS). In the proposed method in this article, the accurate multiclass segmentation is achieved by minimizing an energy function. This energy function is based on IT2FS and weak continuity constraints present in the membership values. The theory of weak continuity constraints helps to localize the segmentation boundaries between the classes accurately with the minimization of the energy. The proper localization of segmentation boundaries helps to minimize the uncertainties in the segmentation process. We also theoretically show that the minimization of the energy function reduces the uncertainties present in the segmentation process. Furthermore, the method automatically determines the number of clusters without a priori knowledge. The proposed method is found to be superior to the existing conventional, fuzzy type-1 and fuzzy type-2 based segmentation techniques. The superiority is verified using synthetic and benchmark datasets. The noise immunity of the proposed method is found to be better than that of the state-of-the-art methods when benchmark against the modified Cramer-Rao bound.																	1063-6706	1941-0034				SEPT	2020	28	9					2151	2163		10.1109/TFUZZ.2019.2930932													
J								Finite-Time Adaptive Fuzzy Control for Nonstrict-Feedback Nonlinear Systems Via an Event-Triggered Strategy	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Nonlinear systems; Fuzzy control; Fuzzy logic; Uncertainty; Control systems; Adaptive control; Adaptive fuzzy control; event-triggered control; finite-time stability; nonstrict-feedback nonlinear systems	LINEAR MULTIAGENT SYSTEMS; FAULT-TOLERANT CONTROL; OUTPUT CONSENSUS; STABILIZATION; DESIGN; STABILITY; NETWORKS	This article addresses the finite-time adaptive fuzzy control problem for a class of nonstrict-feedback uncertain nonlinear systems via an event-triggered strategy. A novel design scheme, consisting of finite-time adaptive fuzzy controller and event-triggering mechanism (ETM), is proposed to decrease the number of data transmission and the number of control actuation updates. With the proposed event-triggered adaptive fuzzy control scheme, all the solutions of the resulting closed-loop system are guaranteed to be semi-globally bounded within finite time. Moreover, the feasibility of the proposed ETM is verified by excluding Zeno behavior. In contrast to existing results on similar problems, the restrictions on nonlinearities are relaxed and the more general uncertain nonlinear systems are considered. Finally, an example is provided to illustrate our theoretical results.																	1063-6706	1941-0034				SEPT	2020	28	9					2164	2174		10.1109/TFUZZ.2019.2931228													
J								Bridging the Gap Between Probabilistic and Fuzzy Entropy	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Entropy; Uncertainty; Fuzzy sets; Probabilistic logic; Decision making; Measurement uncertainty; Additives; Decision making; fuzzy entropy; probabilistic-fuzzy; subjective uncertainty; uncertainty	DISTANCE MEASURE; SETS	The real-world decision making often involves a comparison of uncertain systems or alternatives based on fuzzy evaluations. The concept of fuzzy entropy is quite useful in such situations. However, fuzzy entropy and the conventional probabilistic entropy differ in their semantics. This article critically examines the existing fuzzy entropy functions and redefine them to bring them closer to the probabilistic entropy. More specifically, new variants of the extant Luca and Termini, and Pal and Pal fuzzy entropy functions are proposed. The proposed fuzzy entropy functions are extended for the probabilistic-fuzzy uncertainty, commonly observed in the real world. A real application is included to demonstrate the usefulness of the proposed entropy functions in decision making applications.																	1063-6706	1941-0034				SEPT	2020	28	9					2175	2184		10.1109/TFUZZ.2019.2931232													
J								Online Self-Learning Fuzzy Discrete Event Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Learning automata; Fuzzy sets; Automata; Discrete-event systems; Fuzzy logic; Input variables; Mathematical model; Fuzzy automata; fuzzy discrete event systems (FDES); machine learning	SUPERVISORY CONTROL	The fuzzy discrete event system theory is unique in that it is capable of modeling a class of event-driven systems as fuzzy automata with states and event-invoked state transitions being ambiguous. At present, the theory lacks a self-learning component, an important topic that has hardly been touched in the literature. In this article, we use stochastic gradient descent to develop online learning algorithms for the fuzzy automata. We uncover an inherent obstacle in the initial derived algorithms that fundamentally restricts their learning capability owing to dependences of the model parameters to be learned. We develop a novel mechanism to not only overcome the obstacle but also make the learning adaptive. Our final algorithms can learn an event transition matrix based on automaton's states before and after the occurrence of a fuzzy event, and learn the transition matrix and multidimensional Gaussian fuzzy sets yielding initial automaton states from relevant input variables and target states. Computer simulation results are presented to show learning performance of the final algorithms.																	1063-6706	1941-0034				SEPT	2020	28	9					2185	2194		10.1109/TFUZZ.2019.2931254													
J								Cross-Network Learning With Fuzzy Labels for Seed Selection and Graph Sparsification in Influence Maximization	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Predictive models; Greedy algorithms; Adaptation models; Knowledge engineering; Prediction algorithms; Task analysis; Integrated circuit modeling; Fuzzy domain adaptation; graph sparsification; influence maximization; negative transfer; self-training		To maximize the influence across multiple heterogeneous networks, we propose an innovative cross-network learning model to study the influence maximization problem from two perspectives, namely, seed selection and graph sparsification. On one hand, we consider seed selection as a cross-network node prediction task, by leveraging the greedy seed selection knowledge prelearned in a smaller source network, to heuristically select the nodes most likely to act as seed for the target networks. On the other hand, we consider graph sparsification as a cross-network edge prediction problem, by adapting the influence propagation knowledge previously acquired in the source network to remove the edges least likely to contribute to influence propagation in the target networks. To address domain discrepancy, a fuzzy self-learning algorithm is proposed to iteratively train the prediction model by leveraging not only the fully labeled data in the source network, but also the most confident predicted instances with their predicted fuzzy labels in the target network. With such fuzzy labels, we can differentiate the confident levels of predictions generated by different self-training iterations, thus lowering the negative effects caused by less confident predictions. The performance of the proposed model is benchmarked with the popular influence maximization algorithms for seed selection; and also competed with several graph sparsification algorithms for inactive edge prediction. Experimental results on the real-world datasets show that the proposed cross-network learning model can achieve a good tradeoff between the efficiency and effectiveness of the influence maximization task in the target networks.																	1063-6706	1941-0034				SEPT	2020	28	9					2195	2208		10.1109/TFUZZ.2019.2931272													
J								Data-Knowledge-Based Fuzzy Neural Network for Nonlinear System Identification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Data-knowledge; fuzzy neural network (FNN); mutual attraction strategy (MAS); nonlinear dynamical system identification; transfer learning	REGRESSION; FRAMEWORK; PREDICTION; RULES	Many nonlinear dynamical systems are usually lack of abundant datasets since the data acquiring process is time consuming. It is difficult to utilize the incomplete datasets to build an effective data-driven model to improve the industry productivity. To overcome this problem, a data-knowledge-based fuzzy neural network (DK-FNN) was developed in this article. Compared with the existing methods, the proposed DK-FNN consists of the following obvious advantages. First, through the multilayered connectionist structure, this proposed DK-FNN could not only make full use of the data from the current scene, but also use the existing knowledge from the source scene to improve the learning performance. Second, an integrated-form transfer learning (ITL) method was developed to improve the learning performance of DK-FNN. This first reported ITL method was able to integrate the internal information from the datasets in the source scene and the knowledge from the current scene to offset the data shortage in the learning process. Third, a mutual attraction strategy (MAS) was designed to balance the difference of data distributions to reduce the identification errors of DK-FNN. Then, the proposed DK-FNN was able to satisfy the nonlinear dynamical systems. Finally, the effectiveness and the merit of DK-FNN were validated by applying it to several practical systems.																	1063-6706	1941-0034				SEPT	2020	28	9					2209	2221		10.1109/TFUZZ.2019.2931870													
J								Cluster-Volume-Based Merging Approach for Incrementally Evolving Fuzzy Gaussian Clustering-eGAUSS	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Merging; Covariance matrices; Clustering algorithms; Data models; Partitioning algorithms; Eigenvalues and eigenfunctions; Computational modeling; Data stream; dynamic merging; evolving clustering; evolving cluster models; incremental learning; volume of hyperellipsoids	MODEL; IDENTIFICATION; CLASSIFIERS; REGRESSION	In this article, a new dynamic merging approach for incrementally evolving clustering is presented. This means that the cluster partitions are incrementally learned on-line from streams of data. The criterion of merging is based on the comparison between the sum of volumes of two clusters that fulfill the criteria of a minimal number of samples in the cluster and the expected volume of the newly generated merged cluster. The newly generated merged cluster is conducted by using the weighted averaging of cluster centers and the calculation of the joint covariance matrix from the covariance matrices of the clusters. It has been shown that the proposed new evolving algorithm eGAUSS+ together with the new merging concept is very easy to implement, can work on higher-dimensional data sets, can perform all necessary computation on-line, and can produce reliable clusters.																	1063-6706	1941-0034				SEPT	2020	28	9					2222	2231		10.1109/TFUZZ.2019.2931874													
J								Fuzzy Logic Control for Doppler Search in DSSS Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Doppler effect; Fuzzy logic; Search problems; Frequency estimation; Estimation error; Signal processing algorithms; Doppler frequency search; estimation error; fuzzy logic (FL); false detection; signal acquisition	FREQUENCY ESTIMATION; ACQUISITION; NONCOHERENT; INFERENCE; COHERENT; SIGNALS; PHASE	The study on the Doppler frequency search strategy in direct sequence spread spectrum (DSSS) systems has been addressed in this article. To reduce the estimation error and alleviate the possible false detection, a novel Doppler search algorithm that combines the serial-based search and the fuzzy-logic (FL)-based search is presented. At first, the serial-based search is used until the decision variable exceeds a preset threshold; the FL-based search is then performed for a second search and the final decision is made. In the second search stage, Doppler search step size is adaptively adjusted by an FL controller. To design a robust FL controller, the characteristics of the spectrum distribution and several possible abnormal conditions are analyzed, input and output fuzzy membership functions are derived, and a set of fuzzy rules is formulated. Finally, the effectiveness of the proposed approach is evaluated through Monte Carlo simulations. The obtained results show that a smaller estimation error is achieved when the proposed algorithm is used; further, reasonable mean acquisition times are attained compared with other existing search techniques. Furthermore, the proposed algorithm can alleviate possible false detection in the abnormal conditions.																	1063-6706	1941-0034				SEPT	2020	28	9					2232	2243		10.1109/TFUZZ.2019.2932676													
J								Comparing Performance Potentials of Classical and Intuitionistic Fuzzy Systems in Terms of Sculpting the State Space	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Measurement; Firing; Fuzzy sets; Decision making; Forecasting; Performance analysis; Intuitionistic fuzzy sets (I-FS); intuitionistic fuzzy systems; rule partitions; sculpting the state space; Takagi-Sugeno-Kang (TSK) fuzzy systems	SETS	This article provides new application-independent perspectives about the performance potential of an intuitionistic (I-) fuzzy system over a (classical) Takagi-Sugeno-Kang (TSK) fuzzy system. It does this by extending sculpting the state-space works from a TSK fuzzy system to an I-fuzzy system. It demonstrates that, for piecewise-linear membership functions (trapezoids and triangles), an I-fuzzy system always has significantly more first-order rule partitions of the state space-the coarse sculpting of the state space-than does a TSK fuzzy system, and that some I-fuzzy systems also have more second-order rule partitions of the state space-the fine sculpting of the state space-than does a TSK fuzzy system. It is the author's conjecture that for piecewise-linear membership functions (trapezoids and triangles): it is the always significantly greater coarse (and possibly fine) sculpting of the state space that provides an I-fuzzy system with the potential to outperform a TSK fuzzy system, and that a type-1 I-fuzzy system has the potential to outperform an interval type-2 fuzzy system.																	1063-6706	1941-0034				SEPT	2020	28	9					2244	2254		10.1109/TFUZZ.2019.2933786													
J								Interpolation With Just Two Nearest Neighboring Weighted Fuzzy Rules	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Interpolation; Fuzzy sets; Cognition; Feature extraction; Computer science; Benchmark testing; Business; Attribute weights; fuzzy interpolative reasoning; nearest neighboring rules; weighted rule interpolation	SCALE	Fuzzy rule interpolation (FRI) enables sparse fuzzy rule-based systems to derive an interpolated conclusion using neighboring rules, when presented with an observation that matches none of the given rules. The efficacy of FRI has been further empowered by the recent development of weighted FRI techniques, particularly the one that introduces attribute weights of rule antecedents from the given rule base, removing the conventional assumption of antecedent attributes having equal weighting or significance. However, such work was carried out within the specific transformation-based FRI mechanism. This short paper reports the results of generalizing it through enhancing two alternative representative FRI methods. The resultant weighted FRI algorithms facilitate the individual attribute weights to be integrated throughout the corresponding procedures of the conventional unweighted methods. With systematical comparative evaluations over benchmark classification problems, it is empirically demonstrated that these algorithms work effectively and efficiently using just two nearest neighboring rules.																	1063-6706	1941-0034				SEPT	2020	28	9					2255	2262		10.1109/TFUZZ.2019.2928496													
J								A New Family of OWA Operators Featuring Constant Orness	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Open wireless architecture; Weight measurement; Aggregates; Dispersion; Buoyancy; Sociology; Statistics; Inverse hypergeometric distribution; ordered weighted averaging (OWA) operators; orness	AGGREGATION OPERATORS; ANALYTIC APPROACH; WEIGHTS; MODEL	Determination of the weights of an ordered weighted averaging (OWA) operator is a vital constituent of the operator's aggregation procedure. Consequently, several weight generation techniques have evolved in the literature. Here, using inverse hypergeometric distribution, we use a novel parametric function to generate the weight vector of an OWA operator. The proposed inverse hypergeometric OWA (InHyp-OWA) operator generates a unique weight vector for every choice of its parameter. Also, using the proposed weight function, OWA weight vectors can be generated without solving any complicated optimization problem. An important property of InHyp-OWA operator is that for a given parameter value, its orness value remains constant, regardless of the number of objectives aggregated. Hence, InHyp-OWA operator is a new member of the family of OWA operators with constant orness. This class of OWA operators can utilize a prejudiced preference to determine the corresponding weight vector. The proposed approach provides a number of advantages over existing weight generating methods for OWA operators. One of the remarkable advantages of the InHyp-OWA operator is the fact that it generates weight vectors with strictly monotonic components. It also generalizes the well-known Borda-Kendall OWA operator.																	1063-6706	1941-0034				SEPT	2020	28	9					2263	2269		10.1109/TFUZZ.2019.2928519													
J								A Matrix Method of Basic Belief Assignment's Negation in Dempster-Shafer Theory	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Entropy; Uncertainty; Measurement uncertainty; Probability distribution; Cognition; Resource management; Decision making; Basic belief assignment (BBA); belief function; belief entropy; Dempster-Shafer (D-S) theory; matrix operator; negation	UNCERTAINTY MEASURE; PROBABILITY; ENTROPY; FUSION; FUZZINESS; EVIDENCES	Negation is a new perspective to represent knowledge. The negation of probability distribution has been proposed, and it has a lot of interesting properties, which can reach a maximum entropy. Because of the defects of the classical probability theory in the expression of uncertainty, the basic belief assignment (BBA) in the Dempster-Shafer theory (D-S theory) are widely used in decision theory. Thus, negation provides a new perspective for D-S theory to measure fuzziness. In this paper, a new definition of negation of BBA is presented. In the proposed negation, BBAs are represented as vectors, and negation is realized by matrix operators. This method has a good interpretation of the matrix operators and has the merit of simplifying the problem. With several different definitions of entropy to determinate the uncertainty of BBA, the proposed negation of BBA can reach a maximum belief entropy when the entropies satisfy a certain property.																	1063-6706	1941-0034				SEPT	2020	28	9					2270	2276		10.1109/TFUZZ.2019.2930027													
J								Robust and Noise-Insensitive Recursive Maximum Correntropy-Based Evolving Fuzzy System	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Steady-state; Fuzzy systems; Convergence; Gaussian noise; Kernel; Stability analysis; Noise measurement; Correntropy; evolving fuzzy system (EFS); excess mean square error (EMSE); recursive	INFERENCE SYSTEM; IDENTIFICATION	In this article, a novel recursive maximum correntropy-based evolving fuzzy system (RMCEFS) is proposed. The proposed system has the capability of reorganizing the structure and adapting itself in a dynamically changing environment with non-Gaussian noises. The system generates a new rule based on the correntropy criterion which represents a robust nonlinear similarity measure between two random variables and avoids recruiting the noises as the rules. Maximizing the cross-correntropy between the system output and the desired response leads to the maximum correntropy criterion for system self-adaptation. In our article, a recursive solution of the maximum correntropy criterion is derived to update the parameters of the evolving rules. This avoids the convergence problem produced by the learning size in the gradient-based learning. Also, the steady-state convergence performance of the proposed RMCEFS is studied, where the analytical solutions of the steady-state excess mean square error for the Gaussian noise and non-Gaussian noises are derived. The simulation studies show that the proposed RMCEFS using the recursive maximum correntropy converges much faster and is more accurate than the existing evolving fuzzy systems in the case of noise-free and noisy conditions.																	1063-6706	1941-0034				SEPT	2020	28	9					2277	2284		10.1109/TFUZZ.2019.2931871													
J								Cross-Batch Reference Learning for Deep Retrieval	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Task analysis; Image retrieval; Training; Object detection; Measurement; Proposals; Semantics; Convolutional neural networks (CNNs); deep learning; image retrieval; mean average precision (mAP)	FEATURES	Learning effective representations that exhibit semantic content is crucial to image retrieval applications. Recent advances in deep learning have made significant improvements in performance on a number of visual recognition tasks. Studies have also revealed that visual features extracted from a deep network learned on a large-scale image data set (e.g., ImageNet) for classification are generic and perform well on new recognition tasks in different domains. Nevertheless, when applied to image retrieval, such deep representations do not attain performance as impressive as used for classification. This is mainly because the deep features are optimized for classification rather than for the desired retrieval task. We introduce the cross-batch reference (CBR), a novel training mechanism that enables the optimization of deep networks with a retrieval criterion. With the CBR, the networks leverage both the samples in a single minibatch and the samples in the others for weight updates, enhancing the stochastic gradient descent (SGD) training by enabling interbatch information passing. This interbatch communication is implemented as a cross-batch retrieval process in which the networks are trained to maximize the mean average precision (mAP) that is a popular performance measure in retrieval. Maximizing the cross-batch mAP is equivalent to centralizing the samples relevant to each other in the feature space and separating the samples irrelevant to each other. The learned features can discriminate between relevant and irrelevant samples and thus are suitable for retrieval. To circumvent the discrete, nondifferentiable mAP maximization, we derive an approximate, differentiable lower bound that can be easily optimized in deep networks. Furthermore, the mAP loss can be used alone or with a classification loss. Experiments on several data sets demonstrate that our CBR learning provides favorable performance, validating its effectiveness.																	2162-237X	2162-2388				SEPT	2020	31	9					3145	3158		10.1109/TNNLS.2019.2936876													
J								Memory Augmented Deep Recurrent Neural Network for Video Question Answering	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Task analysis; Knowledge discovery; Computational modeling; Recurrent neural networks; Data models; Semantics; Deep learning; differentiable neural computer (DNC); memory augmented neural network; recurrent neural network (RNN); video question answering (VideoQA)		Video question answering (VideoQA) is a very important but challenging multimedia task, which automatically analyzes questions and videos and generates accurate answers. However, research on VideoQA is still in its infancy. In this article, we propose a novel memory augmented deep recurrent neural network (MA-DRNN) model for VideoQA, which features a new method for encoding videos and questions, and memory augmentation using the emerging differentiable neural computer (DNC). Specifically, we encode textual (questions) information before visual (videos) information, which leads to better visual-textual representations. Moreover, we leverage DNC (with an external memory) for storing and retrieving useful information in questions and videos, and modeling the long-term visual-textual dependence. To evaluate the proposed model, we conducted extensive experiments using the VTW data set and MSVD-QA data set, which are both Widely used large-scale video data sets for language-level understanding. The experimental results have well validated the proposed model and showed that it outperforms the state-of-the-art in terms of various accuracy-related metrics.																	2162-237X	2162-2388				SEPT	2020	31	9					3159	3167		10.1109/TNNLS.2019.2938015													
J								Exponential State Estimation for Stochastically Disturbed Discrete-Time Memristive Neural Networks: Multiobjective Approach	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Memristors; State estimation; Analytical models; Stochastic processes; Linear matrix inequalities; Optimization; Learning systems; Discrete-time; memristive neural networks; multiobjective approach; state estimation; stochastic disturbances	H-INFINITY; VARYING DELAYS; STABILITY; SYNCHRONIZATION; SYSTEMS	The state estimation of the discrete-time memristive model is studied in this article. By applying the stochastic analysis technique, sufficient formulas are established to ensure the exponentially mean-square stability of the error model. Moreover, the derived control gain matrix can be calculated via the linear matrix inequality (LMI). It should be mentioned that, by extending the derived conclusion to a multiobjective optimization problem, the maximum bound of the active function and the minimum bound of the disturbance attenuation are derived. The corresponding simulation figures are provided in the end.																	2162-237X	2162-2388				SEPT	2020	31	9					3168	3177		10.1109/TNNLS.2019.2938774													
J								Harnessing Side Information for Classification Under Label Noise	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Noise measurement; Matrix decomposition; Training; Computer science; Task analysis; Learning systems; Risk management; Classification; generalization bound; label noise; matrix recovery; side information		Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled. Such noisy labels will mislead the classifier training and severely decrease the classification performance. Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empirical risk minimization. However, they are only suitable for binary classification and also require strong prior knowledge. Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem. We denote our proposed method as "label noise handling via side information" (LNSI). Specifically, the observed label matrix is decomposed as the sum of two parts, in which the first part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix. The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufficiently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems. Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classification error of LNSI is upper bounded. The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets confirm the superiority of LNSI to state-of-the-art approaches on label noise handling.																	2162-237X	2162-2388				SEPT	2020	31	9					3178	3192		10.1109/TNNLS.2019.2938782													
J								Modeling and Clustering Positive Vectors via Nonparametric Mixture Models of Liouville Distributions	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Mixture models; Data models; Inference algorithms; Clustering algorithms; Fans; Bayes methods; Averaged collapsed variational Bayes (ACVB); clustering; Dirichlet process; inverted Beta-Liouville (IBL) distribution; mixture models; nonparametric Bayesian; positive vectors	SIMULTANEOUS FEATURE-SELECTION; VARIATIONAL INFERENCE; DIRICHLET	In this article, we propose an effective mixture model-based approach to modeling and clustering positive data vectors. Our mixture model is based on the inverted Beta-Liouville (IBL) distribution which is extracted from the family of Liouville distributions. To cope with the problem of determining the appropriate number of clusters in our approach, a nonparametric Bayesian framework is used to extend the IBL mixture to an infinite mixture model in which the number of clusters is assumed to be infinite initially and will be inferred automatically during the learning process. To optimize the proposed model, we propose a convergence-guaranteed learning algorithm based on the averaged collapsed variational Bayes inference that can effectively learn model parameters with closed-form solutions. The effectiveness of the proposed infinite IBL mixture model for modeling and clustering positive vectors is validated through both synthetic and real-world data sets.																	2162-237X	2162-2388				SEPT	2020	31	9					3193	3203		10.1109/TNNLS.2019.2938830													
J								General 7-Instant DCZNN Model Solving Future Different-Level System of Nonlinear Inequality and Linear Equation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Continuous combined zeroing neural network (CCZNN) model; continuous different-level system of nonlinear inequality and linear equation (CDLSNILE); discrete combined zeroing neural network (DCZNN) model; future different-level system of nonlinear inequality and linear equation (FDLSNILE); general 7-instant Zhang et al. discretization (ZeaD) formula	RECURRENT NEURAL-NETWORK; OVERRELAXATION METHODS; DESIGN; SCHEME	In this article, a novel and challenging problem called future different-level system of nonlinear inequality and linear equation (FDLSNILE) is proposed and investigated. To solve FDLSNILE, the corresponding continuous different-level system of nonlinear inequality and linear equation (CDLSNILE) is first analyzed, and then, a continuous combined zeroing neural network (CCZNN) model for solving CDLSNILE is proposed. To obtain a discrete combined zeroing neural network (DCZNN) model for solving FDLSNILE, a high-precision general 7-instant Zhang et al. discretization (ZeaD) formula for the first-order time derivative approximation is proposed. Furthermore, by applying the general 7-instant ZeaD formula to discretize the CCZNN model, a general 7-instant DCZNN (7IDCZNN) model is thus proposed for solving FDLSNILE. For comparison, by using three conventional ZeaD formulas, three conventional DCZNN models are also developed. Meanwhile, theoretical analyses and results guarantee the efficacy and superiority of the general 7IDCZNN model compared with the other three conventional DCZNN models for solving FDLSNILE. Finally, several comparative numerical experiments, including the motion control of a 5-link redundant manipulator, are provided to substantiate the efficacy and superiority of the general 7-instant ZeaD formula and the corresponding 7IDCZNN model.																	2162-237X	2162-2388				SEPT	2020	31	9					3204	3214		10.1109/TNNLS.2019.2938866													
J								Automatically Design Convolutional Neural Networks by Optimization With Submodularity and Supermodularity	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimization; Computer architecture; Kernel; Greedy algorithms; Bayes methods; Neural networks; Meters; Convolutional neural network (CNN); Greedy algorithm; submodular; supermodular	SET FUNCTION SUBJECT; GREEDY ALGORITHM	The architecture of convolutional neural networks (CNNs) is a key factor of influencing their performance. Although deep CNNs perform well in many difficult problems, how to intelligently design the architecture is still a challenging problem. Focusing on two practical architectural design problems: to maximize the accuracy with a given forward running time and to minimize the forward running time with a given accuracy requirement, we innovatively utilize prior knowledge to convert architecture optimization problems into submodular optimization problems. We propose efficient Greedy algorithms to solve them and give theoretical bounds of our algorithms. Specifically, we employ the techniques on some public data sets and compare our algorithms with some other hyperparameter optimization methods. Experiments show our algorithms' efficiency.																	2162-237X	2162-2388				SEPT	2020	31	9					3215	3229		10.1109/TNNLS.2019.2939157													
J								A Robust Distance Measure for Similarity-Based Classification on the SPD Manifold	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Manifolds; Visualization; Measurement; Covariance matrices; Geometry; Kernel; Learning systems; Metric learning; similarity-based classification; symmetric positive definite (SPD) manifold; visual information	ALPHA-BETA; DIVERGENCES	The symmetric positive definite (SPD) matrices, forming a Riemannian manifold, are commonly used as visual representations. The non-Euclidean geometry of the manifold often makes developing learning algorithms (e.g., classifiers) difficult and complicated. The concept of similarity-based learning has been shown to be effective to address various problems on SPD manifolds. This is mainly because the similarity-based algorithms are agnostic to the geometry and purely work based on the notion of similarities/distances. However, existing similarity-based models on SPD manifolds opt for holistic representations, ignoring characteristics of information captured by SPD matrices. To circumvent this limitation, we propose a novel SPD distance measure for the similarity-based algorithm. Specifically, we introduce the concept of point-to-set transformation, which enables us to learn multiple lower dimensional and discriminative SPD manifolds from a higher dimensional one. For lower dimensional SPD manifolds obtained by the point-to-set transformation, we propose a tailored set-to-set distance measure by making use of the family of alpha-beta divergences. We further propose to learn the point-to-set transformation and the set-to-set distance measure jointly, yielding a powerful similarity-based algorithm on SPD manifolds. Our thorough evaluations on several visual recognition tasks (e.g., action classification and face recognition) suggest that our algorithm comfortably outperforms various state-of-the-art algorithms.																	2162-237X	2162-2388				SEPT	2020	31	9					3230	3244		10.1109/TNNLS.2019.2939177													
J								Semi-Supervised Graph Regularized Deep NMF With Bi-Orthogonal Constraints for Data Representation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Matrix decomposition; Data mining; Dimensionality reduction; Linear programming; Deep learning; Laplace equations; Feature extraction; Bi-orthogonal constraints; deep non-negative matrix factorization (NMF); dimensionality reduction; dual-hypergraph Laplacian regularization; semi-supervised learning	NONNEGATIVE MATRIX FACTORIZATION; SPARSE; DECOMPOSITION	Semi-supervised non-negative matrix factorization (NMF) exploits the strengths of NMF in effectively learning local information contained in data and is also able to achieve effective learning when only a small fraction of data is labeled. NMF is particularly useful for dimensionality reduction of high-dimensional data. However, the mapping between the low-dimensional representation, learned by semi-supervised NMF, and the original high-dimensional data contains complex hierarchical and structural information, which is hard to extract by using only single-layer clustering methods. Therefore, in this article, we propose a new deep learning method, called semi-supervised graph regularized deep NMF with bi-orthogonal constraints (SGDNMF). SGDNMF learns a representation from the hidden layers of a deep network for clustering, which contains varied and unknown attributes. Bi-orthogonal constraints on two factor matrices are introduced into our SGDNMF model, which can make the solution unique and improve clustering performance. This improves the effect of dimensionality reduction because it only requires a small fraction of data to be labeled. In addition, SGDNMF incorporates dual-hypergraph Laplacian regularization, which can reinforce high-order relationships in both data and feature spaces and fully retain the intrinsic geometric structure of the original data. This article presents the details of the SGDNMF algorithm, including the objective function and the iterative updating rules. Empirical experiments on four different data sets demonstrate state-of-the-art performance of SGDNMF in comparison with six other prominent algorithms.																	2162-237X	2162-2388				SEPT	2020	31	9					3245	3258		10.1109/TNNLS.2019.2939637													
J								Exact Passive-Aggressive Algorithms for Ordinal Regression Using Interval Labels	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Prediction algorithms; Optimization; Training data; Training; Learning systems; Convex functions; Indexes; Interval labels; online learning; ordinal regression; passive-aggressive (PA)		In this article, we propose exact passive-aggressive (PA) online algorithms for ordinal regression. The proposed algorithms can be used even when we have interval labels instead of actual labels for example. The proposed algorithms solve a convex optimization problem at every trial. We find an exact solution to those optimization problems to determine the updated parameters. We propose a support class algorithm (SCA) that finds the active constraints using the Karush-Kuhn-Tucker (KKT) conditions of the optimization problems. These active constraints form a support set, which determines the set of thresholds that need to be updated. We derive update rules for PA, PA-I, and PA-II. We show that the proposed algorithms maintain the ordering of the thresholds after every trial. We provide the mistake bounds of the proposed algorithms in both ideal and general settings. We also show experimentally that the proposed algorithms successfully learn accurate classifiers using interval labels as well as exact labels. The proposed algorithms also do well compared to other approaches.																	2162-237X	2162-2388				SEPT	2020	31	9					3259	3268		10.1109/TNNLS.2019.2939861													
J								Supervised Dimensionality Reduction Methods via Recursive Regression	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Dimensionality reduction; Linear discriminant analysis; Eigenvalues and eigenfunctions; Learning systems; Computer science; Optical imaging; Optics; Optimal scaling (OS); orthogonal least squares regression (OLSR); orthogonal linear discriminant analysis (OLDA); recursive regression; supervised dimensionality reduction	RIDGE-REGRESSION; EXTRACTION; FRAMEWORK	In this article, the recursive problems of both orthogonal linear discriminant analysis (OLDA) and orthogonal least squares regression (OLSR) are investigated. Different from other works, the associated recursive problems are addressed via a novel recursive regression method, which achieves the dimensionality reduction in the orthogonal complement space heuristically. As for the OLDA, an efficient method is developed to obtain the associated optimal subspace, which is closely related to the orthonormal basis of the optimal solution to the ridge regression. As for the OLSR, the scalable subspace is introduced to build up an original OLSR with optimal scaling (OS). Through further relaxing the proposed problem into a convex parameterized orthogonal quadratic problem, an effective approach is derived, such that not only the optimal subspace can be achieved but also the OS could be obtained automatically. Accordingly, two supervised dimensionality reduction methods are proposed via obtaining the heuristic solutions to the recursive problems of the OLDA and the OLSR.																	2162-237X	2162-2388				SEPT	2020	31	9					3269	3279		10.1109/TNNLS.2019.2940088													
J								Region Stabilization of Switched Neural Networks With Multiple Modes and Multiple Equilibria: A Pole Assignment Method	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Switches; Linear systems; Artificial neural networks; Stability analysis; Switched systems; Asymptotic stability; Neurons; Multiple equilibria (ME); multiple modes (MMs); pole assignment; region stability; region stabilization; switched neural networks (SNNs); switched systems	OBSERVER-BASED STABILIZATION; STABILITY ANALYSIS; EXPONENTIAL STABILITY; STOCHASTIC-SYSTEMS; NONLINEAR-SYSTEMS; TIME; DISSIPATIVITY; PASSIVITY; STORAGE	This article investigates region stabilization issue of switched neural networks (SNNs) with multiple modes (MMs) and multiple equilibria (ME) via a pole assignment method. In such an SNN, every neuron is observed with more than one mode and unstable equilibrium point. First, SNNs with MMs and ME are modeled in terms of switched systems with unstable subsystems and ME. Second, a necessary and sufficient condition and a sufficient condition are, respectively, proposed for arbitrary switching paths pole assignment and arbitrary periodic/quasi-periodic switching paths (PSPs/QSPs) asymptotically region stabilizing pole assignment of switched linear time-invariant (LTI) systems with ME. It is shown that to stabilize a switched LTI system, some/all poles of all/some linear subsystems can be assigned to suitable locations of the right-half side of the complex plane. Third, based on the obtained pole assignment results, an asymptotical-region-stabilizing-control law observed as distributed state feedback controllers of MMs, asymptotical-region-stabilizing PSPs/QSPs, and a corresponding algorithm are all designed for asymptotical region stabilization of switched linear/nonlinear neural networks with MMs and ME. Finally, a numeral example is given to illustrate the effectiveness and practicality of the new results.																	2162-237X	2162-2388				SEPT	2020	31	9					3280	3293		10.1109/TNNLS.2019.2940466													
J								On the Working Principle of the Hopfield Neural Networks and its Equivalence to the GADIA in Optimization	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimization; Neurons; Biological neural networks; Interference; Associative memory; Learning systems; Partitioning algorithms; Associative memory; graph partitioning; greedy asynchronous distributed interference avoidance algorithm (GADIA); Hopfield neural networks (HNNs); matrix trace maximization; optimization	DESIGN METHOD; SYSTEMS; MEMORY	Hopfield neural networks (HNNs) are one of the most well-known and widely used kinds of neural networks in optimization. In this article, the author focuses on building a deeper understanding of the working principle of the HNN during an optimization process. Our investigations yield several novel results giving some important insights into the working principle of both continuous and discrete HNNs. This article shows that what the traditional HNN actually does as energy function decreases is to divide the neurons into two classes in such a way that the sum of biased class volumes is minimized (or maximized) regardless of the types of the optimization problems. Introducing neuron-specific class labels, the author concludes that the traditional discrete HNN is actually a special case of the greedy asynchronous distributed interference avoidance algorithm (GADIA) [17] of Babadi and Tarokh for the 2-class optimization problems. The computer results confirm the findings.																	2162-237X	2162-2388				SEPT	2020	31	9					3294	3304		10.1109/TNNLS.2019.2940920													
J								Parameter Optimization and Learning in a Spiking Neural Network for UAV Obstacle Avoidance Targeting Neuromorphic Processors	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimization; Neuromorphics; Neurons; Adaptation models; Mathematical model; Robot sensing systems; Program processors; Bayesian optimization (BO); differential evolution (DE); neuromorphic engineering; self-adaptation (SA); spike-timing-dependent plasticity (STDP)	COLLISION DETECTION MECHANISM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; LOCUST; MODEL; SYSTEM; CARS	The Lobula giant movement detector (LGMD) is an identified neuron of the locust that detects looming objects and triggers the insect's escape responses. Understanding the neural principles and network structure that leads to these fast and robust responses can facilitate the design of efficient obstacle avoidance strategies for robotic applications. Here, we present a neuromorphic spiking neural network model of the LGMD driven by the output of a neuromorphic dynamic vision sensor (DVS), which incorporates spiking frequency adaptation and synaptic plasticity mechanisms, and which can be mapped onto existing neuromorphic processor chips. However, as the model has a wide range of parameters and the mixed-signal analog-digital circuits used to implement the model are affected by variability and noise, it is necessary to optimize the parameters to produce robust and reliable responses. Here, we propose to use differential evolution (DE) and Bayesian optimization (BO) techniques to optimize the parameter space and investigate the use of self-adaptive DE (SADE) to ameliorate the difficulties of finding appropriate input parameters for the DE technique. We quantify the performance of the methods proposed with a comprehensive comparison of different optimizers applied to the model and demonstrate the validity of the approach proposed using recordings made from a DVS sensor mounted on an unmanned aerial vehicle (UAV).																	2162-237X	2162-2388				SEPT	2020	31	9					3305	3318		10.1109/TNNLS.2019.2941506													
J								AlphaSeq: Sequence Discovery With Deep Reinforcement Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Games; Radar; Tools; Multiaccess communication; Machine learning algorithms; Approximation algorithms; Learning systems; AlphaGo; deep reinforcement learning (DRL); Monte Carlo tree search (MCTS); multi-carrier code-division multiple access (MC-CDMA); pulse compression radar	MERIT FACTOR; GAME; GO	Sequences play an important role in many applications and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This article puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) techniques. AlphaSeq treats the sequence discovery problem as an episodic symbol-filling game, in which a player fills symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely filled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov decision process (MDP) and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, and learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier code-division multiple access (CDMA) systems and 2) AlphaSeq discovers new sequences that triple the signal-to-interference ratio-benchmarked against the well-known Legendre sequence-of a mismatched filter (MMF) estimator in pulse compression radar systems.																	2162-237X	2162-2388				SEPT	2020	31	9					3319	3333		10.1109/TNNLS.2019.2942951													
J								Secure Communication Based on Quantized Synchronization of Chaotic Neural Networks Under an Event-Triggered Strategy	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Synchronization; Chaotic communication; Quantization (signal); Output feedback; Biological neural networks; Security; Chaotic neural networks; event-triggered strategy; quantized synchronization; secure communication	MULTIAGENT SYSTEMS; CONTINUOUS-TIME; CONSENSUS; STATE	This article presents a secure communication scheme based on the quantized synchronization of master-slave neural networks under an event-triggered strategy. First, a dynamic event-triggered strategy is proposed based on a quantized output feedback, for which a quantized output feedback controller is formed. Second, theoretical criteria are derived to ensure the bounded synchronization of master-slave neural networks. With these criteria, an explicit upper bound is given for the synchronization error. Sufficient conditions are also provided on the existence of quantized output feedback controllers. A Chua's circuit is chosen to illustrate the effectiveness of our theoretical results. Third, a secure communication scheme is presented based on the synchronization of master-slave neural networks by combining the basic principle of cryptology. Then, a secure image communication is studied to verify the feasibility and security performance of the proposed secure communication scheme. The impact of the quantization level and the event-triggered control (ETC) on image decryption is investigated through experiments.																	2162-237X	2162-2388				SEPT	2020	31	9					3334	3345		10.1109/TNNLS.2019.2943548													
J								Adaptive Control of Noncanonical Neural-Network Nonlinear Systems With Unknown Input Dead-Zone Characteristics	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Nonlinear systems; Adaptive control; Backstepping; Actuators; Robustness; Adaptation models; Dead-zone characteristics; neural networks; noncanonical nonlinear systems; robust adaptive control	DELAY SYSTEMS; DESIGN	Most of the available results on adaptive control of uncertain nonlinear systems with input dead-zone characteristics are for canonical nonlinear systems whose relative degrees are explicit and for which a Lyapunov-based backstepping design is directly applicable. However, those results cannot be applied to noncanonical form nonlinear systems whose relative degrees are implicit and for which a Lyapunov-based backstepping design may not be applicable. This article solves the adaptive control problem of a class of noncanonical neural-network nonlinear systems with unknown input dead-zones. A complete solution framework is developed, using a new gradient-based design which is applicable to noncanonical nonlinear systems with input dead-zones. Signal boundedness of the closed-loop system and the desired tracking performance are ensured with the developed control schemes. Their effectiveness is illustrated by an application example of speed control of dc motors. This article can be readily extended to handle general parametrizable noncanonical nonlinear systems with unknown dynamics and input dead-zones, to solve such an open problem.																	2162-237X	2162-2388				SEPT	2020	31	9					3346	3360		10.1109/TNNLS.2019.2943637													
J								Projected Neural Network for a Class of Non-Lipschitz Optimization Problems With Linear Constraints	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimization; Neural networks; Smoothing methods; Numerical models; Linear programming; Mathematical model; Generalized stationary point; neural network; non-Lipschitz optimization problem; sparse optimization	VARIABLE SELECTION; IMAGE-RESTORATION; NONSMOOTH; CONVERGENCE; SIGNALS; RECONSTRUCTION; MINIMIZATION	In this article, we consider a class of nonsmooth, nonconvex, and non-Lipschitz optimization problems, which have wide applications in sparse optimization. We generalize the Clarke stationary point and define a kind of generalized stationary point of the problems with a stronger optimal capability. Based on the smoothing method, we propose a projected neural network for solving this kind of optimization problem. Under the condition that the level set of objective function in the feasible region is bounded, we prove that the solution of the proposed neural network is globally existent and bounded. The uniqueness of the solution of the proposed network is also analyzed. When the feasible region is bounded, any accumulation point of the proposed neural network is a generalized stationary point of the optimization model. Based on some suitable conditions, any solution of the proposed neural network is asymptotic convergent to one stationary point. In particular, we give some deep analysis on the proposed network for solving a special class of the non-Lipschitz optimization problem, which indicates a lower bound property and the unify identification for the nonzero elements of all accumulation points. Finally, some numerical results are presented to show the efficiency of the proposed neural network for solving some kinds of sparse optimization models.																	2162-237X	2162-2388				SEPT	2020	31	9					3361	3373		10.1109/TNNLS.2019.2944388													
J								Guide Subspace Learning for Unsupervised Domain Adaptation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Learning systems; Task analysis; Adaptation models; Image reconstruction; Machine learning; Domain adaptation; subspace learning; transfer learning	RECOGNITION; ALGORITHM; KERNEL	A prevailing problem in many machine learning tasks is that the training (i.e., source domain) and test data (i.e., target domain) have different distribution [i.e., non-independent identical distribution (i.i.d.)]. Unsupervised domain adaptation (UDA) was proposed to learn the unlabeled target data by leveraging the labeled source data. In this article, we propose a guide subspace learning (GSL) method for UDA, in which an invariant, discriminative, and domain-agnostic subspace is learned by three guidance terms through a two-stage progressive training strategy. First, the subspace-guided term reduces the discrepancy between the domains by moving the source closer to the target subspace. Second, the data-guided term uses the coupled projections to map both domains to a unified subspace, where each target sample can be represented by the source samples with a low-rank coefficient matrix that can preserve the global structure of data. In this way, the data from both domains can be well interlaced and the domain-invariant features can be obtained. Third, for improving the discrimination of the subspaces, the label-guided term is constructed for prediction based on source labels and pseudo-target labels. To further improve the model tolerance to label noise, a label relaxation matrix is introduced. For the solver, a two-stage learning strategy with teacher teaches and student feedbacks mode is proposed to obtain the discriminative domain-agnostic subspace. In addition, for handling nonlinear domain shift, a nonlinear GSL (NGSL) framework is formulated with kernel embedding, such that the unified subspace is imposed with nonlinearity. Experiments on various cross-domain visual benchmark databases show that our methods outperform many state-of-the-art UDA methods. The source code is available at https://github.com/Fjr9516/GSL.																	2162-237X	2162-2388				SEPT	2020	31	9					3374	3388		10.1109/TNNLS.2019.2944455													
J								Neuroadaptive Control Design for Pure-Feedback Nonlinear Systems: A One-Step Design Approach	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Backstepping; Control design; Complexity theory; Lyapunov methods; Artificial neural networks; Explosions; Dynamic surface control (DSC); neuroadaptive control; one-step design; pure-feedback systems	DYNAMIC SURFACE CONTROL; ADAPTIVE NEURAL-CONTROL; TRACKING CONTROL; NETWORK CONTROL	In this article, we propose a one-step control design approach for pure-feedback nonlinear systems in the presence of unmatched and nonvanishing external disturbances. Different from the commonly utilized backstepping design, the proposed method, integrated with the dynamic surface control (DSC) technique, only involves one-step design with one single Lyapunov function in the whole control synthesis, which derives the actual control and the intermediate controls simultaneously in a collective way, avoiding the repetitive design procedures and multiple Lyapunov functions, yet circumventing the issue of "explosion of complexity." Furthermore, with this method, the increase in system order does not increase the design and analysis complexity. Numerical simulation examples confirm and validate the effectiveness of the proposed method.																	2162-237X	2162-2388				SEPT	2020	31	9					3389	3399		10.1109/TNNLS.2019.2944459													
J								Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Data models; Servers; Deep learning; Protocols; Training data; Distributed databases; Deep learning; distributed learning; efficient communication; federated learning; privacy-preserving machine learning		Federated learning allows multiple parties to jointly train a deep learning model on their combined data, without any of the participants having to reveal their local data to a centralized server. This form of privacy-preserving collaborative learning, however, comes at the cost of a significant communication overhead during training. To address this problem, several compression methods have been proposed in the distributed training literature that can reduce the amount of required communication by up to three orders of magnitude. These existing methods, however, are only of limited utility in the federated learning setting, as they either only compress the upstream communication from the clients to the server (leaving the downstream communication uncompressed) or only perform well under idealized conditions, such as i.i.d. distribution of the client data, which typically cannot be found in federated learning. In this article, we propose sparse ternary compression (STC), a new compression framework that is specifically designed to meet the requirements of the federated learning environment. STC extends the existing compression technique of top-k gradient sparsification with a novel mechanism to enable downstream compression as well as ternarization and optimal Golomb encoding of the weight updates. Our experiments on four different learning tasks demonstrate that STC distinctively outperforms federated averaging in common federated learning scenarios. These results advocate for a paradigm shift in federated optimization toward high-frequency low-bitwidth communication, in particular in the bandwidth-constrained learning environments.																	2162-237X	2162-2388				SEPT	2020	31	9					3400	3413		10.1109/TNNLS.2019.2944481													
J								An Exponential-Type Anti-Noise Varying-Gain Network for Solving Disturbed Time-Varying Inversion Systems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convergence; Recurrent neural networks; Mathematical model; Time-varying systems; Analytical models; Learning systems; Simulation; Anti-noise; matrix inversion; time-varying problem; varying-gain network		To solve the disturbed time-varying inversion problem, an exponential-type anti-noise varying-gain network (EAVGN) is proposed and analyzed. To do so, a vector-based error function is first defined. By using the varying-gain neural dynamic design method, an EAVGN model is then formulated. Furthermore, the differentiation error and the model-implementation error are considered into the model, and the perturbed EAVGN model is obtained. For better illustrations, comparisons between the EAVGN and the conventional fixed-parameter recurrent neural network (FP-RNN) are conducted to illustrate the advantages of the proposed EAVGN. Mathematical proof demonstrates that the proposed EAVGN has much better anti-noise properties than FP-RNN. On one hand, the residual error of EAVGN can be reduced to zero in any case, but that of FP-RNN is large and cannot be convergent, in particular when the bound of Frobenius norm of the exact solution is large or the noise is large. On the other hand, the bound of the residual error of EAVGN is always smaller than that of FP-RNN. Simulation results verify that when different types of noises exist, the proposed EAVGN owns better anti-noise property compared with the state-of-the-art methods. In addition, a practical application is presented to illustrate the implementation process and the practical benefits of the EAVGN.																	2162-237X	2162-2388				SEPT	2020	31	9					3414	3427		10.1109/TNNLS.2019.2944485													
J								Self-Weighted Clustering With Adaptive Neighbors	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Adaptation models; Covariance matrices; Kernel; Laplace equations; Learning systems; Support vector machines; Adaptive neighbors; block-diagonal similarity matrix; clustering; weighted features		Many modern clustering models can be divided into two separated steps, i.e., constructing a similarity graph (SG) upon samples and partitioning each sample into the corresponding cluster based on SG. Therefore, learning a reasonable SG has become a hot issue in the clustering field. Many previous works that focus on constructing better SG have been proposed. However, most of them follow an ideal assumption that the importance of different features is equal, which is not adapted in practical applications. To alleviate this problem, this article proposes a self-weighted clustering with adaptive neighbors (SWCAN) model that can assign weights for different features, learn an SG, and partition samples into clusters simultaneously. In experiments, we observe that the SWCAN can assign weights for different features reasonably and outperform than comparison clustering models on synthetic and practical data sets.																	2162-237X	2162-2388				SEPT	2020	31	9					3428	3441		10.1109/TNNLS.2019.2944565													
J								Multiview Uncorrelated Locality Preserving Projection	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Correlation; Dimensionality reduction; Feature extraction; Pairwise error probability; Principal component analysis; Optimization; Canonical correlation analysis (CCA); high-order correlation; multiview learning; pairwise correlation; uncorrelated feature extraction	CANONICAL CORRELATION-ANALYSIS; ALTERNATING LEAST-SQUARES; SETS; CONSISTENCY; EXTENSIONS; FRAMEWORK; VIEW	Canonical Correlation Analysis (CCA) is a popular multiview dimension reduction method, which aims to maximize the correlation between two views to find the common subspace shared by these two views. However, it can only deal with two-view data, while the number of views frequently exceeds two in many real applications. To handle data with more than two views, in the previous studies, either the pairwise correlation or the high-order correlation was employed. These two types of correlation define the relation of multiview data from different viewpoints, and both have special effects for view consistency. To obtain flexible view consistency, in this article, we propose multiview uncorrelated locality preserving projection (MULPP), which considers two types of correlation simultaneously. The MULPP also considers the complementary property of different views by preserving the local structures of all the views. To obtain multiple projections and minimize the redundancy of low-dimensional features, for each view, the MULPP makes the features extracted by different projections uncorrelated. The MULPP is solved by an iteration algorithm, and the convergence of the algorithm is proven. The experiments on Multiple Feature, Coil-100, 3Sources, and NUS-WIDE data sets demonstrate the effectiveness of MULPP.																	2162-237X	2162-2388				SEPT	2020	31	9					3442	3455		10.1109/TNNLS.2019.2944664													
J								Extracting Relational Explanations From Deep Neural Networks: A Survey From a Neural-Symbolic Perspective	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Knowledge engineering; Biological neural networks; Neurons; Market research; Explainable artificial intelligence; knowledge extraction; neural networks; neural-symbolic integration (NSI)	KNOWLEDGE EXTRACTION; RULE EXTRACTION; INTERPRETABILITY; REPRESENTATIONS; AUTOMATA; LOGIC	The term "explainable AI" refers to the goal of producing artificially intelligent agents that are capable of providing explanations for their decisions. Some models (e.g., rule-based systems) are designed to be explainable, while others are less explicit "black boxes" for which their reasoning remains a mystery. One example of the latter is the neural network, and over the past few decades, researchers in the field of neural-symbolic integration (NSI) have sought to extract relational knowledge from such networks. Extraction from deep neural networks, however, has remained a challenge until recent years in which many methods of extracting distinct, salient features from input or hidden feature spaces of deep neural networks have been proposed. Furthermore, methods of identifying relationships between these features have also emerged. This article presents examples of old and new developments in extracting relational explanations in order to argue that the latter have analogies in the former and, as such, can be described in terms of long-established taxonomies and frameworks presented in early neural-symbolic literature. We also outline potential future research directions that come to light from this refreshed perspective.																	2162-237X	2162-2388				SEPT	2020	31	9					3456	3470		10.1109/TNNLS.2019.2944672													
J								Adaptive Neural Quantized Control of MIMO Nonlinear Systems Under Actuation Faults and Time-Varying Output Constraints	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										MIMO communication; Nonlinear systems; Quantization (signal); Time-varying systems; Stability analysis; Adaptive systems; Control design; Actuation faults; asymmetric yet time-varying barrier function; input quantization; multi-input multi-output (MIMO) nonlinear systems; neuroadaptive control; output constraints	BARRIER LYAPUNOV FUNCTIONS; TRACKING CONTROL; FEEDBACK-CONTROL; DESIGN	In this article, a neural network (NN)-based robust adaptive fault-tolerant control (FTC) algorithm is proposed for a class of multi-input multi-output (MIMO) strict-feedback nonlinear systems with input quantization and actuation faults as well as asymmetric yet time-varying output constraints. By introducing a key nonlinear decomposition for quantized input, the developed control scheme does not require the detailed information of quantization parameters. By imposing a reasonable condition on the gain matrix under actuation faults, together with the inherent approximation capability of NN, the difficulty of FTC design caused by anomaly actuation can be handled gracefully, and the normally used yet rigorous assumption on control gain matrix in most existing results is significantly relaxed. Furthermore, a brand new barrier function is constructed to handle the asymmetric yet time-varying output constraints such that the analysis and design are extremely simplified compared with the traditional barrier Lyapunov function (BLF)-based methods. NNs are used to approximate the unknown nonlinear continuous functions. The stability of the closed-loop system is analyzed by using the Lyapunov method and is verified through a simulation example.																	2162-237X	2162-2388				SEPT	2020	31	9					3471	3481		10.1109/TNNLS.2019.2944690													
J								MVStream: Multiview Data Stream Clustering	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Clustering algorithms; Shape; Task analysis; Support vector machines; Indexes; Data models; Computer science; Clustering; clusters of arbitrary shapes; data stream; multiview; support vector (SV)	ALGORITHM	This article studies a new problem of data stream clustering, namely, multiview data stream (MVStream) clustering. Although many data stream clustering algorithms have been developed, they are restricted to the single-view streaming data, and clustering MVStreams still remains largely unsolved. In addition to the many issues encountered by the conventional single-view data stream clustering, such as capturing cluster evolution and discovering clusters of arbitrary shapes under the limited computational resources, the main challenge of MVStream clustering lies in integrating information from multiple views in a streaming manner and abstracting summary statistics from the integrated features simultaneously. In this article, we propose a novel MVStream clustering algorithm for the first time. The main idea is to design a multiview support vector domain description (MVSVDD) model, by which the information from multiple insufficient views can be integrated, and the outputting support vectors (SVs) are utilized to abstract the summary statistics of the historical multiview data objects. Based on the MVSVDD model, a new multiview cluster labeling method is designed, whereby clusters of arbitrary shapes can be discovered for each view. By tracking the cluster labels of SVs in each view, the cluster evolution associated with concept drift can be captured. Since the SVs occupy only a small portion of data objects, the proposed MVStream algorithm is quite efficient with the limited computational resources. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the proposed method.																	2162-237X	2162-2388				SEPT	2020	31	9					3482	3496		10.1109/TNNLS.2019.2944851													
J								Adaptive Neural Output Feedback Compensation Control for Intermittent Actuator Faults Using Command-Filtered Backstepping	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Actuators; Output feedback; Nonlinear systems; Transient analysis; Backstepping; Adaptive systems; Lyapunov methods; Adaptive neural control; command-filtered backstepping; decentralized systems; intermittent actuator faults	UNCERTAIN NONLINEAR-SYSTEMS; EXTREME LEARNING-MACHINE; DYNAMIC SURFACE CONTROL; LARGE-SCALE SYSTEMS; FAILURE COMPENSATION; INTERCONNECTED SYSTEMS; TOLERANT CONTROL; NETWORKS; DESIGN; STABILIZATION	Effectively compensating unknown intermittent actuator faults in uncertain decentralized nonlinear systems is a very difficult problem, and very few results have been obtained. In this article, to address this issue, an adaptive neural output feedback compensation control scheme based on command-filtered backstepping is developed. First, we design a bank of observers to estimate the system states and utilize neural networks with random hidden nodes to approximate the unknown functions of these observers. Second, a smooth projection algorithm is used to online update estimated parameters in the controllers such that the possible ceaseless increase in the estimated parameters caused by intermittent actuator faults can be eliminated. Due to the presence of intermittent jumps of unknown parameters, a modified Lyapunov function is developed to analyze the system stability. It is proved that the boundedness of all closed-loop system signals is ensured and the ultimate bound of the tracking error depends on design parameters, adjustable jumping amplitude of Lyapunov function, and minimum fault time interval. Third, by analyzing the system transient performance, the peaking phenomenon at the starting instant of the system operation can be removed, and a root mean square type of bound is established to illustrate that the transient tracking error performance is tunable by design parameters. Finally, simulations studies are done to illustrate the effectiveness of the theoretical results.																	2162-237X	2162-2388				SEPT	2020	31	9					3497	3511		10.1109/TNNLS.2019.2944897													
J								Sketch Kernel Ridge Regression Using Circulant Matrix: Algorithm and Theory	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Kernel; Training; Time complexity; Convergence; Acceleration; Approximation algorithms; Circulant matrix; kernel ridge regression (KRR); large scale; random sketch	RANDOMIZED SKETCHES; NYSTROM METHOD; RATES	Kernel ridge regression (KRR) is a powerful method for nonparametric regression. The time and space complexity of computing the KRR estimate directly are O(n(3)) and O(n(2)) , respectively, which are prohibitive for large-scale data sets, where n is the number of data. In this article, we propose a novel random sketch technique based on the circulant matrix that achieves savings in storage space and accelerates the solution of the KRR approximation. The circulant matrix has the following advantages: It can save time complexity by using the fast Fourier transform (FFT) to compute the product of matrix and vector, its space complexity is linear, and the circulant matrix, whose entries in the first column are independent of each other and obey the Gaussian distribution, is almost as effective as the i.i.d. Gaussian random matrix for approximating KRR. Combining the characteristics of the circulant matrix and our careful design, theoretical analysis and experimental results demonstrate that our proposed sketch method, making the estimate kernel methods scalable and practical for large-scale data problems, outperforms the state-of-the-art KRR estimates in time complexity while retaining similar accuracies. Meanwhile, our sketch method provides the theoretical bound that keeps the optimal convergence rate for approximating KRR.																	2162-237X	2162-2388				SEPT	2020	31	9					3512	3524		10.1109/TNNLS.2019.2944959													
J								Bayes Imbalance Impact Index: A Measure of Class Imbalanced Data Set for Classification Problem	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Indexes; Complexity theory; Learning systems; Training; Benchmark testing; Technological innovation; Computer science; Bayes classifier; class imbalance learning; data complexity; imbalance measure; imbalance recovery methods	SAMPLING METHOD; SMOTE; COMPLEXITY; NOISY	Recent studies of imbalanced data classification have shown that the imbalance ratio (IR) is not the only cause of performance loss in a classifier, as other data factors, such as small disjuncts, noise, and overlapping, can also make the problem difficult. The relationship between the IR and other data factors has been demonstrated, but to the best of our knowledge, there is no measurement of the extent to which class imbalance influences the classification performance of imbalanced data. In addition, it is also unknown which data factor serves as the main barrier for classification in a data set. In this article, we focus on the Bayes optimal classifier and examine the influence of class imbalance from a theoretical perspective. We propose an instance measure called the Individual Bayes Imbalance Impact Index (IBI3) and a data measure called the Bayes Imbalance Impact Index (BI3). IBI3 and BI3 reflect the extent of influence using only the imbalance factor, in terms of each minority class sample and the whole data set, respectively. Therefore, IBI3 can be used as an instance complexity measure of imbalance and BI3 as a criterion to demonstrate the degree to which imbalance deteriorates the classification of a data set. We can, therefore, use BI3 to access whether it is worth using imbalance recovery methods, such as sampling or cost-sensitive methods, to recover the performance loss of a classifier. The experiments show that IBI3 is highly consistent with the increase of the prediction score obtained by the imbalance recovery methods and that BI3 is highly consistent with the improvement in the F1 score obtained by the imbalance recovery methods on both synthetic and real benchmark data sets.																	2162-237X	2162-2388				SEPT	2020	31	9					3525	3539		10.1109/TNNLS.2019.2944962													
J								Collocating Clothes With Generative Adversarial Networks Cosupervised by Categories and Attributes: A Multidiscriminator Framework	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Clothing; Gallium nitride; Generative adversarial networks; Generators; Task analysis; Visualization; Computer science; Clothes collocation; clothing attributes; fashion data; generative adversarial network (GAN); image translation		The choice of which clothes to wear affects how one is perceived, as well as constitutes an expression of one's personal style. Based on the recent advances in image-to-image translation by the conditional generative adversarial network (cGAN), we propose a new framework with a multidiscriminator by incorporating different types of conditional information into the discriminator of cGAN for clothing matches. In contrast with most extant frameworks under cGAN, with one generator and one discriminator, the proposed framework investigates the potential of utilizing conditional information delivered by multidiscriminators to guide the generator. Under this framework, we propose an Attribute-GAN with two discriminators and a category-attribute GAN (CA-GAN) with three discriminators. In order to evaluate the performance of our proposed models, we built a large-scale data set that consists of 19 081 pairs of collocation clothing images with 90 manually labeled attributes. Experimental results demonstrate that with supervision of the additional attribute discriminator or category discriminator, the quality of the generated clothing images by GANs is consistently improved in comparison with the state-of-the-art methods.																	2162-237X	2162-2388				SEPT	2020	31	9					3540	3554		10.1109/TNNLS.2019.2944979													
J								Complex-Valued Discrete-Time Neural Dynamics for Perturbed Time-Dependent Complex Quadratic Programming With Applications	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Computational modeling; Convergence; Mathematical model; Recurrent neural networks; Perturbation methods; Robots; Numerical models; Complex domain; discrete-time neural dynamics (DTND); quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS); quadratic programming (QP)	REDUNDANT MANIPULATORS; NETWORK	It has been reported that some specially designed recurrent neural networks and their related neural dynamics are efficient for solving quadratic programming (QP) problems in the real domain. A complex-valued QP problem is generated if its variable vector is composed of the magnitude and phase information, which is often depicted in a time-dependent form. Given the important role that complex-valued problems play in cybernetics and engineering, computational models with high accuracy and strong robustness are urgently needed, especially for time-dependent problems. However, the research on the online solution of time-dependent complex-valued problems has been much less investigated compared to time-dependent real-valued problems. In this article, to solve the online time-dependent complex-valued QP problems subject to linear constraints, two new discrete-time neural dynamics models, which can achieve global convergence performance in the presence of perturbations with the provided theoretical analyses, are proposed and investigated. In addition, the second proposed model is developed to eliminate the operation of explicit matrix inversion by introducing the quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. Moreover, computer simulation results and applications in robotics and filters are provided to illustrate the feasibility and superiority of the proposed models in comparison with the existing solutions.																	2162-237X	2162-2388				SEPT	2020	31	9					3555	3569		10.1109/TNNLS.2019.2944992													
J								Reinforcement Learning Tracking Control for Robotic Manipulator With Kernel-Based Dynamic Model	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Manipulator dynamics; Heuristic algorithms; Task analysis; Kernel; Adaptation models; Kernel function; reinforcement learning (RL); reward function; robotics tracking control	NEURAL-NETWORK; SYSTEMS	Reinforcement learning (RL) is an efficient learning approach to solving control problems for a robot by interacting with the environment to acquire the optimal control policy. However, there are many challenges for RL to execute continuous control tasks. In this article, without the need to know and learn the dynamic model of a robotic manipulator, a kernel-based dynamic model for RL is proposed. In addition, a new tuple is formed through kernel function sampling to describe a robotic RL control problem. In this algorithm, a reward function is defined according to the features of tracking control in order to speed up the learning process, and then an RL tracking controller with a kernel-based transition dynamic model is proposed. Finally, a critic system is presented to evaluate the policy whether it is good or bad to the RL control tasks. The simulation results illustrate that the proposed method can fulfill the robotic tracking tasks effectively and achieve similar and even better tracking performance with much smaller inputs of force/torque compared with other learning algorithms, demonstrating the effectiveness and efficiency of the proposed RL algorithm.																	2162-237X	2162-2388				SEPT	2020	31	9					3570	3578		10.1109/TNNLS.2019.2945019													
J								Geometric Matrix Completion With Deep Conditional Random Fields	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Predictive models; Computational modeling; Neural networks; Measurement; Benchmark testing; Deep learning; Deep conditional random field (CRF); deep learning; geometric matrix completion (GMC); mean-field inference; probabilistic graphical model	NEURAL-NETWORKS; MODELS	The problem of completing high-dimensional matrices from a limited set of observations arises in many big data applications, especially recommender systems. The existing matrix completion models generally follow either a memory- or a model-based approach, whereas geometric matrix completion (GMC) models combine the best from both approaches. Existing deep-learning-based geometric models yield good performance, but, in order to operate, they require a fixed structure graph capturing the relationships among the users and items. This graph is typically constructed by evaluating a pre-defined similarity metric on the available observations or by using side information, e.g., user profiles. In contrast, Markov-random-fields-based models do not require a fixed structure graph but rely on handcrafted features to make predictions. When no side information is available and the number of available observations becomes very low, existing solutions are pushed to their limits. In this article, we propose a GMC approach that addresses these challenges. We consider matrix completion as a structured prediction problem in a conditional random field (CRF), which is characterized by a maximum a posteriori (MAP) inference, and we propose a deep model that predicts the missing entries by solving the MAP inference problem. The proposed model simultaneously learns the similarities among matrix entries, computes the CRF potentials, and solves the inference problem. Its training is performed in an end-to-end manner, with a method to supervise the learning of entry similarities. Comprehensive experiments demonstrate the superior performance of the proposed model compared to various state-of-the-art models on popular benchmark data sets and underline its superior capacity to deal with highly incomplete matrices.																	2162-237X	2162-2388				SEPT	2020	31	9					3579	3593		10.1109/TNNLS.2019.2945111													
J								Singular Values for ReLU Layers	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neural networks; Tools; Harmonic analysis; Learning systems; Task analysis; Measurement; Gaussian mean width; n-width; neural networks; rectified linear unit (ReLU); singular values		Despite their prevalence in neural networks, we still lack a thorough theoretical characterization of rectified linear unit (ReLU) layers. This article aims to further our understanding of ReLU layers by studying how the activation function ReLU interacts with the linear component of the layer and what role this interaction plays in the success of the neural network in achieving its intended task. To this end, we introduce two new tools: ReLU singular values of operators and the Gaussian mean width of operators. By presenting, on the one hand, theoretical justifications, results, and interpretations of these two concepts and, on the other hand, numerical experiments and results of the ReLU singular values and the Gaussian mean width being applied to trained neural networks, we hope to give a comprehensive, singular-value-centric view of ReLU layers. We find that ReLU singular values and the Gaussian mean width do not only enable theoretical insights but also provide one with metrics that seem promising for practical applications. In particular, these measures can be used to distinguish correctly and incorrectly classified data as it traverses the network. We conclude by introducing two tools based on our findings: double layers and harmonic pruning.																	2162-237X	2162-2388				SEPT	2020	31	9					3594	3605		10.1109/TNNLS.2019.2945113													
J								Development and Analysis of Neural Networks Realized in the Presence of Granular Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neural networks; Training; Numerical models; Computational modeling; Computer architecture; Task analysis; Fuzzy sets; Granular computing (GrC); granular neural network; information granule; principle of justifiable granularity	HIERARCHICAL GENETIC ALGORITHM; OPTIMAL PREDICTION INTERVALS; OPTIMIZATION; DESIGN; PRINCIPLE; LOAD	In this article, we propose a design and evaluation framework of granular neural networks realized in the presence of information granules. Neural networks realized in this manner are able to process both nonnumerical data, such as information granules as well as numerical data. Information granules are meaningful and semantically sound entities formed by organizing existing knowledge and available experimental data. The directional nature of mapping between the input and output data needs to be considered when building information granules. The development of neural networks advocated in this article is realized as a two-phase process. First, a collection of information granules is formed through granulation of numeric data in the input and output spaces. Second, neural networks are constructed on the basis of information granules rather than original (numeric) data. The proposed method leads to the construction of neural networks in a completely new way. In comparison with traditional (numeric) neural networks, the networks developed in the presence of granular data require shorter learning time. They also produce the results (outputs) that are information granules rather than numeric entities. The quality of granular outputs generated by our neural networks is evaluated in terms of the coverage and specificity criteria that are pertinent to the characterization of the information granules.																	2162-237X	2162-2388				SEPT	2020	31	9					3606	3619		10.1109/TNNLS.2019.2945307													
J								Laplacian-Uniform Mixture-Driven Iterative Robust Coding With Applications to Face Recognition Against Dense Errors	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Encoding; Face recognition; Minimization; Training; Face; Robustness; Approximation algorithms; Coding residual; face recognition; non-convexity; outlier; sparse representation	SPARSE REPRESENTATION; THRESHOLDING ALGORITHM; CLASSIFICATION; EIGENFACES; EXTRACTION; REGRESSION; SHRINKAGE	Outliers due to occlusion, pixel corruption, and so on pose serious challenges to face recognition despite the recent progress brought by sparse representation. In this article, we show that robust statistics implemented by the state-of-the-art methods are insufficient for robustness against dense gross errors. By modeling the distribution of coding residuals with a Laplacian-uniform mixture, we obtain a sparse representation that is significantly more robust than the previous methods. The nonconvex error term of the implemented objective function is nondifferentiable at zero and cannot be properly addressed by the usual iteratively reweighted least-squares formulation. We show that an iterative robust coding algorithm can be derived by local linear approximation of the nonconvex error term, which is both effective and efficient. With iteratively reweighted l(1) minimization of the error term, the proposed algorithm is capable of handling the sparsity assumption of the coding errors more appropriately than the previous methods. Notably, it has the distinct property of addressing error detection and error correction cooperatively in the robust coding process. The proposed method demonstrates significantly improved robustness for face recognition against dense gross errors, either contiguous or discontiguous, as verified by extensive experiments.																	2162-237X	2162-2388				SEPT	2020	31	9					3620	3633		10.1109/TNNLS.2019.2945372													
J								Cross-Media Semantic Correlation Learning Based on Deep Hash Network and Semantic Expansion for Social Network Cross-Media Search	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Semantics; Correlation; Social networking (online); Quantization (signal); Search problems; Big Data; Cross-media search; cross-media semantic correlation learning; cross-media similarity calculation; deep hash network; semantic expansion		Cross-media search from large-scale social network big data has become increasingly valuable in our daily life because it can support querying different data modalities. Deep hash networks have shown high potential in achieving efficient and effective cross-media search performance. However, due to the fact that social network data often exhibit text sparsity, diversity, and noise characteristics, the search performance of existing methods often degrades when dealing with this data. In order to address this problem, this article proposes a novel end-to-end cross-media semantic correlation learning model based on a deep hash network and semantic expansion for social network cross-media search (DHNS). The approach combines deep network feature learning and hash-code quantization learning for multimodal data into a unified optimization architecture, which successfully preserves both intramedia similarity and intermedia correlation, by minimizing both cross-media correlation loss and binary hash quantization loss. In addition, our approach realizes semantic relationship expansion by constructing the image-word relation graph and mining the potential semantic relationship between images and words, and obtaining the semantic embedding based on both internal graph deep walk and an external knowledge base. Experimental results demonstrate that DHNS yields better cross-media search performance on standard benchmarks.																	2162-237X	2162-2388				SEPT	2020	31	9					3634	3648		10.1109/TNNLS.2019.2945567													
J								An Event-Driven Categorization Model for AER Image Sensors Using Multispike Encoding and Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Computational modeling; Biological neural networks; Image sensors; Data models; Object recognition; Visualization; Event-based vision; neuromorphic computing; object recognition; spiking neural networks (SNNs)	OBJECT RECOGNITION; VISION; FEATURES; CODE	In this article, we present a systematic computational model to explore brain-based computation for object recognition. The model extracts temporal features embedded in address-event representation (AER) data and discriminates different objects by using spiking neural networks (SNNs). We use multispike encoding to extract temporal features contained in the AER data. These temporal patterns are then learned through the tempotron learning rule. The presented model is consistently implemented in a temporal learning framework, where the precise timing of spikes is considered in the feature-encoding and learning process. A noise-reduction method is also proposed by calculating the correlation of an event with the surrounding spatial neighborhood based on the recently proposed time-surface technique. The model evaluated on wide spectrum data sets (MNIST, N-MNIST, MNIST-DVS, AER Posture, and Poker Card) demonstrates its superior recognition performance, especially for the events with noise.																	2162-237X	2162-2388				SEPT	2020	31	9					3649	3657		10.1109/TNNLS.2019.2945630													
J								Design Principles for Central Pattern Generators With Preset Rhythms	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Mice; Synapses; Legged locomotion; Biological system modeling; Mathematical model; Cells (biology); Bifurcation analysis; central pattern generators; neuronal models; parameter optimization	LOCOMOTOR SPEED; ORGANIZATION; CIRCUITS; MODEL; INTERNEURONS; OSCILLATOR; STABILITY; SYSTEM	This article is concerned with the design of synthetic central pattern generators (CPGs). Biological CPGs are neural circuits that determine a variety of rhythmic activities, including locomotion, in animals. A synthetic CPG is a network of dynamical elements (here called cells) properly coupled by various synapses to emulate rhythms produced by a biological CPG. We focus on CPGs for locomotion of quadrupeds and present our design approach, based on the principles of nonlinear dynamics, bifurcation theory, and parameter optimization. This approach lets us design the synthetic CPG with a set of desired rhythms and switch between them as the parameter representing the control actions from the brain is varied. The developed four-cell CPG can produce four distinct gaits: walk, trot, gallop, and bound, similar to the mouse locomotion. The robustness and adaptability of the network design principles are verified using different cell and synapse models.																	2162-237X	2162-2388				SEPT	2020	31	9					3658	3669		10.1109/TNNLS.2019.2945637													
J								Nonstationary Discrete Convolution Kernel for Multimodal Process Monitoring	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Kernel; Principal component analysis; Training; Convolution; Data models; Fault detection; kernel-based learning methods; multivariate statistics; nonstationary kernels; process monitoring	PRINCIPAL COMPONENT ANALYSIS; DATA-DEPENDENT KERNEL; FAULT-DETECTION; MACHINE	Data-driven process monitoring has benefited from the development and application of kernel transformations, especially when various types of nonlinearity exist in the data. However, when dealing with the multimodality behavior that is frequently observed in the process operations, the most widely used radial basis function (RBF) kernel has limitations in describing process data collected from multiple normal operating modes. In this article, we highlight this limitation via a synthesized example. In order to account for the multimodality behavior and improve the fault detection performance accordingly, we propose a novel nonstationary discrete convolution kernel, which derives from the convolution kernel structure, as an alternative to the RBF kernel. By assuming the training samples to be the support of the discrete convolution, this new kernel can properly address these training samples from different operating modes with diverse properties and, therefore, can improve the data description and fault detection performance. Its performance is compared with RBF kernels under a standard kernel principal component analysis framework and with other methods proposed for multimode process monitoring via numerical examples. Moreover, a benchmark data set collected from a pilot-scale multiphase flow facility is used to demonstrate the advantages of the new kernel when applied to an experimental data set.																	2162-237X	2162-2388				SEPT	2020	31	9					3670	3681		10.1109/TNNLS.2019.2945847													
J								MLNE: Multi-Label Network Embedding	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Correlation; Neural networks; Network topology; Feature extraction; Topology; Learning systems; Task analysis; Label correlation; multi-label learning; network embedding; network representation learning; neural network		Network embedding aims to preserve topological structures of a network using low-dimensional vectors and has shown to be effective for driving a myriad of graph mining tasks (e.g., link prediction or classification) free of the stressful feature extraction procedure. Many methods have been proposed to integrate node content and/or label information, with nodes sharing similar content/labels being close to each other in the learned latent space. To date, existing methods either consider networked instances with a single label or consider a set of labels as a whole for node representation learning. Therefore, they cannot handle network of instances containing multiple labels (i.e. multi-labels), which are ubiquitous in describing complex concepts of instances. In this article, we formulate a new multi-label network embedding (MLNE) problem to learn feature representation for networked multi-label instances. We argue that the key to MLNE learning is to aggregate node topology structures, node content, and multi-label correlations. We propose a two-layer network embedding framework to couple information for effective learning. To capture higher order label correlations, we use labels to form a high-level label-label network over a low-level node-node network, in which the label network interacts with the node network through multi-labeling relations. The low-level node-node network can be enhanced by latent label-specific features from high-level label network with well-captured high-order correlations between labels. To enable the multi-label informed network embedding, we force both node and label representations being optimized under the same low-dimensional latent space by a unified training objective. Experiments on real-world data sets demonstrate that MLNE achieves better performance compared with methods with or without considering label information.																	2162-237X	2162-2388				SEPT	2020	31	9					3682	3695		10.1109/TNNLS.2019.2945869													
J								Neural Network-Based Adaptive Control for Spacecraft Under Actuator Failures and Input Saturations	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Attitude tracking; fault-tolerant control (FTC); finite-time control; input saturations; neural network (NN) control	FAULT-TOLERANT CONTROL; ATTITUDE TRACKING; RIGID SPACECRAFT	In this article, we develop attitude tracking control methods for spacecraft as rigid bodies against model uncertainties, external disturbances, subsystem faults/failures, and limited resources. A new intelligent control algorithm is proposed using approximations based on radial basis function neural networks (RBFNNs) and adopting the tunable parameter-based variable structure (TPVS) control techniques. By choosing different adaptation parameters elaborately, a series of control strategies are constructed to handle the challenging effects due to actuator faults/failures and input saturations. With the help of the Lyapunov theory, we show that our proposed methods guarantee both finite-time convergence and fault-tolerance capability of the closed-loop systems. Finally, benefits of the proposed control methods are illustrated through five numerical examples.																	2162-237X	2162-2388				SEPT	2020	31	9					3696	3710		10.1109/TNNLS.2019.2945920													
J								XFlow: Cross-Modal Deep Neural Networks for Audiovisual Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Task analysis; Videos; Correlation; Visualization; Neural networks; Deep learning; Audiovisual; cross modality; deep learning; integration; machine learning; multimodal		In recent years, there have been numerous developments toward solving multimodal tasks, aiming to learn a stronger representation than through a single modality. Certain aspects of the data can be particularly useful in this case-for example, correlations in the space or time domain across modalities-but should be wisely exploited in order to benefit from their full predictive potential. We propose two deep learning architectures with multimodal cross connections that allow for dataflow between several feature extractors (XFlow). Our models derive more interpretable features and achieve better performances than models that do not exchange representations, usefully exploiting correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. This article improves on the existing multimodal deep learning algorithms in two essential ways: 1) it presents a novel method for performing cross modality (before features are learned from individual modalities) and 2) extends the previously proposed cross connections that only transfer information between the streams that process compatible data. Illustrating some of the representations learned by the connections, we analyze their contribution to the increase in discrimination ability and reveal their compatibility with a lip-reading network intermediate representation. We provide the research community with Digits, a new data set consisting of three data types extracted from videos of people saying the digits 0-9. Results show that both cross-modal architectures outperform their baselines (by up to 11.5%) when evaluated on the AVletters, CUAVE, and Digits data sets, achieving the state-of-the-art results.																	2162-237X	2162-2388				SEPT	2020	31	9					3711	3720		10.1109/TNNLS.2019.2945992													
J								A Wide-Deep-Sequence Model-Based Quality Prediction Method in Industrial Process Analysis	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Predictive models; Data models; Quality assessment; Product design; Data mining; Analytical models; Industrial artificial intelligence (AI); industrial big data; Industrial Internet of Things; product quality prediction; wide-deep-sequence (WDS) model		Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.																	2162-237X	2162-2388				SEPT	2020	31	9					3721	3731		10.1109/TNNLS.2020.3001602													
J								Teacher-Student Curriculum Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Task analysis; Training; Reinforcement learning; Supervised learning; Robots; Navigation; Active learning; curriculum learning; deep reinforcement learning; learning progress		We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e., where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with long short-term memory (LSTM) and navigation in Minecraft. Our automatically ordered curriculum of submazes enabled to solve a Minecraft maze that could not be solved at all when training directly on that maze, and the learning was an order of magnitude faster than a uniform sampling of those submazes.																	2162-237X	2162-2388				SEPT	2020	31	9					3732	3740		10.1109/TNNLS.2019.2934906													
J								Implementing Any Nonlinear Quantum Neuron	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Qubit; Neurons; Registers; Integrated circuit modeling; Computational modeling; Biological neural networks; Quantum computing; quantum Fourier transform (QFT); quantum neural networks; quantum neuron	ALGORITHMS; NETWORK	The ability of artificial neural networks (ANNs) to adapt to input data and perform generalizations is intimately connected to the use of nonlinear activation and propagation functions. Quantum versions of ANN have been proposed to take advantage of the possible supremacy of quantum over classical computing. To date, all proposals faced the difficulty of implementing nonlinear activation functions since quantum operators are linear. This brief presents an architecture to simulate the computation of an arbitrary nonlinear function as a quantum circuit. This computation is performed on the phase of an adequately designed quantum state, and quantum phase estimation recovers the result, given a fixed precision, in a circuit with linear complexity in function of ANN input size.																	2162-237X	2162-2388				SEPT	2020	31	9					3741	3746		10.1109/TNNLS.2019.2938899													
J								Partial-Neurons-Based Passivity-Guaranteed State Estimation for Neural Networks With Randomly Occurring Time Delays	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neurons; Delay effects; Estimation; Delays; Stability criteria; Biological neural networks; Neural network; partial-neurons-based (PNB) estimation; passivity; randomly occurring time delays; state estimation (SE)	H-INFINITY; STABILITY	In this brief, the partial-neurons-based passivity-guaranteed state estimation (SE) problem is examined for a class of discrete-time artificial neural networks with randomly occurring time delays. The measurement outputs available utilized for the SE are allowed to be available only at a fraction of neurons in the networks. A Bernoulli-distributed random variable is employed to characterize the random nature of the occurrence of time delays. By resorting to the Lyapunov-Krasovskii functional method as well as the stochastic analysis technique, sufficient criteria are provided for the existence of the desired state estimators ensuring the estimation error dynamics to achieve the asymptotic stability in the mean square with a guaranteed passivity performance level. In addition, the parameterization of the estimator gain is acquired by solving a convex optimization problem. Finally, the validity of the obtained theoretical results is illustrated via a numerical simulation example.																	2162-237X	2162-2388				SEPT	2020	31	9					3747	3753		10.1109/TNNLS.2019.2944552													
J								Adaptive Robust Low-Rank 2-D Reconstruction With Steerable Sparsity	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Image reconstruction; Robustness; Adaptation models; Principal component analysis; Learning systems; Technological innovation; Analytical models; Adaptive weight; global robustness; low-rank 2-D image reconstruction; steerable sparsity		Existing image reconstruction methods frequently improve their robustness by using various nonsquared loss functions, which are still potentially sensitive to the outliers. More specifically, when certain samples in data sets encounter severe contamination, these methods cannot identify and filter out the ill ones, and thus lead to the functional degeneration of the associated models. To address this issue, we propose a general framework, named robust and sparse weight learning (RSWL), to compute the adaptive weights based on an objective for robustness and sparsity. More importantly, the degree of the sparsity is steerable, such that only k well-reserved samples are activated during the optimization of our model. As a result, the severely polluted or damaged samples are eliminated, and the robustness is ensured. The framework is further leveraged against a 2-D image reconstruction task. Theoretical analysis and extensive experiments are presented to demonstrate the superiority of the proposed method.																	2162-237X	2162-2388				SEPT	2020	31	9					3754	3759		10.1109/TNNLS.2019.2944650													
J								Deep Adaptive Input Normalization for Time Series Forecasting	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Time series analysis; Task analysis; Data models; Forecasting; Training; Predictive models; Adaptation models; Data normalization; deep learning (DL); limit order book data; time series forecasting		Deep learning (DL) models can be used to tackle time series analysis tasks with great success. However, the performance of DL models can degenerate rapidly if the data are not appropriately normalized. This issue is even more apparent when DL is used for financial time series forecasting tasks, where the nonstationary and multimodal nature of the data pose significant challenges and severely affect the performance of DL models. In this brief, a simple, yet effective, neural layer that is capable of adaptively normalizing the input time series, while taking into account the distribution of the data, is proposed. The proposed layer is trained in an end-to-end fashion using backpropagation and leads to significant performance improvements compared to other evaluated normalization schemes. The proposed method differs from traditional normalization methods since it learns how to perform normalization for a given task instead of using a fixed normalization scheme. At the same time, it can be directly applied to any new time series without requiring retraining. The effectiveness of the proposed method is demonstrated using a large-scale limit order book data set, as well as a load forecasting data set.																	2162-237X	2162-2388				SEPT	2020	31	9					3760	3765		10.1109/TNNLS.2019.2944933													
J								Ensemble Pruning Based on Objection Maximization With a General Distributed Framework	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Information entropy; Diversity reception; Optimization; Degradation; Entropy; Measurement uncertainty; Learning systems; Composable core-sets; diversity; ensemble learning; ensemble pruning; information entropy	NEURAL-NETWORKS; DIVERSITY; SELECTION; CLASSIFIERS; ALGORITHM; STATISTICS	Ensemble pruning, selecting a subset of individual learners from an original ensemble, alleviates the deficiencies of ensemble learning on the cost of time and space. Accuracy and diversity serve as two crucial factors, while they usually conflict with each other. To balance both of them, we formalize the ensemble pruning problem as an objection maximization problem based on information entropy. Then we propose an ensemble pruning method, including a centralized version and a distributed version, in which the latter is to speed up the former. Finally, we extract a general distributed framework for ensemble pruning, which can be widely suitable for most of the existing ensemble pruning methods and achieve less time-consuming without much accuracy degradation. Experimental results validate the efficiency of our framework and methods, particularly concerning a remarkable improvement of the execution speed, accompanied by gratifying accuracy performance.																	2162-237X	2162-2388				SEPT	2020	31	9					3766	3774		10.1109/TNNLS.2019.2945116													
J								A new asymmetric epsilon-insensitive pinball loss function based support vector quantile regression model	APPLIED SOFT COMPUTING										Quantile regression; Pinball loss function; Support vector machine; epsilon-insensitive loss function		In this paper, we propose a novel asymmetric epsilon-insensitive pinball loss function for quantile estimation. There exists some pinball loss functions which attempt to incorporate the epsilon-insensitive zone approach in it but, they fail to extend the epsilon-insensitive approach for quantile estimation in true sense. The proposed asymmetric epsilon-insensitive pinball loss function can make an asymmetric epsilon- insensitive zone of fixed width around the data and divide it using tau value for the estimation of the tau th quantile. The use of the proposed asymmetric epsilon-insensitive pinball loss function in Support Vector Quantile Regression (SVQR) model improves its prediction ability significantly. It also brings the sparsity back in SVQR model. Further, the numerical results obtained by several experiments carried on simulated and real world datasets empirically show the efficacy of the proposed 'epsilon-Support Vector Quantile Regression' (epsilon-SVQR) model over other existing SVQR models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106473	10.1016/j.asoc.2020.106473													
J								Capability-based distributed layout formation with or without demand and process flow information	APPLIED SOFT COMPUTING										Distributed machine layout; Capability-based machine layout; Integer programming; Metaheuristics; Weighted superposition attraction algorithm	SWARM INTELLIGENCE ALGORITHM; SUPERPOSITION ATTRACTION WSA; DESIGN; OPTIMIZATION	In this paper, a binary integer programming model of an unbiased capability-based distributed layout (UBCB-DL) problem without demand and process flow information is first developed. Then, it is extended to a mixed-integer program for a biased capability-based distributed layout (BCB-DL) problem where the demand information and processing requirements of several parts are taken into account. Since the complex nature of the problems, a recently proposed new generation metaheuristic optimizer namely, weighted superposition attraction (WSA) algorithm is also applied. In order to show validity and practicality of the proposed WSA algorithm and compare its performance with the proposed mathematical programs, a real-life case study is presented. The computational experiments have shown that both of the proposed binary integer program and WSA algorithm are able to find alternative optimal solutions for the UBCB-DL problem under reasonable computation times. However, just a feasible solution with 5.93% optimality gap is found by the proposed mixed-integer program for the BCB-DL problem under 24-hour running time limit. Fortunately, its optimal solution is achieved by the proposed WSA algorithm. Consequently, the proposed WSA algorithm provided the most effective solutions for both UBCB-DL and BCB-DL problems under shortest computation times. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106469	10.1016/j.asoc.2020.106469													
J								Similarity-based Particle Filter for Remaining Useful Life prediction with enhanced performance	APPLIED SOFT COMPUTING										Remaining Useful Life; Rao-Blackwellized Particle Filter; Maximum Mean Discrepancy; Kernel Two Sample Test; NASA C-MAPSS Dataset	IDENTIFICATION; PROGNOSIS; MACHINE; SIGNAL	This paper proposes a similarity-based Particle Filter (PF) method for Remaining Useful Life (RUL) prediction with improved performance. In the proposed methodology, Maximum Mean Discrepancy (MMD) and Kernel Two Sample Test are firstly adopted to query similar Run-To-Failure (R2F) profiles from historical data library. The states and parameters of degradation are initialized based on the similar R2F profiles. Next, Rao-Blackwellized Particle Filter (RBPF) is employed to update the degradation states based on the initialization. The RUL prediction results are obtained by extrapolating the degradation states updated by RBPF. The proposed RUL prediction method holds several advantages: (1) compared with other PF methods, the proposed model includes historical knowledge from similar R2F profiles; (2) compared with similarity-based methods, the proposed model presents good probabilistic interpretation of prediction uncertainties based on RUL distribution. The effectiveness and superiority over other peer algorithms are justified based on a public aero-engine dataset for prognostics. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106474	10.1016/j.asoc.2020.106474													
J								Modeling operation problem of active distribution networks with retailers and microgrids: A multi-objective bi-level approach	APPLIED SOFT COMPUTING										Bi-level optimization; Micro-grids (MGs); Multi-objective policy; Particle swarm optimization (PSO); Retailers	ENERGY MANAGEMENT; DISTRIBUTION COMPANY; DECISION-MAKING; OPTIMIZATION; SYSTEM; ELECTRICITY; FRAMEWORK; GENERATION; GRIDS	Implementation of distributed energy resources (DERs) has led to a decrement in the cost of supplying demand in distribution networks. Integration of DERs in the forms of micro-grids (MGs) is a solution to enhance the operation of these resources in the low voltage networks. To meet the demand by MG operator, both technical and economic characteristics as well as the prices offered by retailers are considered to schedule DERs optimally. In these networks, the profit of retailers is maximized by power trading with MGs and optimally purchasing the energy from wholesale markets. Due to the existence of several retailers and MGs in active distribution networks (ADNs), hierarchical decision-making frameworks are needed to model their operation problem. For this purpose, a bi-level optimization technique is proposed in this paper to model the operation problem of retailers and MGs as decision-making variables in distribution networks in the upper and lower levels, respectively. To solve the proposed model, multi-objective particle swarm optimization (MOPSO) algorithm is used. The proposed model and its solution method are applied to a hypothetical distribution network with several retailers and MGs to validate the theories and discussions. Numerical results show that the maximum capacity of DG and the amount of demand have an important effect on this decision and the prices of purchased power from wholesale markets determine the amount of retailers' offers to MGs. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106484	10.1016/j.asoc.2020.106484													
J								Joint resource allocation algorithm based on multi-objective optimization for wireless sensor networks	APPLIED SOFT COMPUTING										Multi-radio multi-channel wireless sensor networks; Resource allocation; Graph coloring; Multi-objective optimization; Hybrid particle swarm optimization	VARIABLE NEIGHBORHOOD SEARCH; PARTICLE SWARM OPTIMIZATION; POWER-CONTROL; ENERGY EFFICIENCY; ASSIGNMENT; MULTIHOP	With the limitations of the network resources and battery energy of wireless sensors, the competition of resources in the process of communication will increase the network energy consumption and reduce the Quality of Service (QoS), resulting in that the application of Multi-Radio Multi-Channel (MRMC) Wireless Sensor Networks (WSNs) face many challenges. In this paper, we concentrate on the resource allocation of joint time slot assignment, channel allocation and power control for MRMC WSNs. Due to the diversity of research objectives and the computational complexity of the non-convex problem, this paper develops a two-stage resource allocation optimization algorithm by analyzing the interdependence of various resources. Specifically, to exchange information with conflict-free transmission among all sensors, a graph coloring algorithm for time slot assignment is designed firstly. Then based on the first stage of this algorithm, the problem of joint power control and channel allocation is studied and formulated as a multi-objective optimization problem to achieve the trade-off between energy efficiency and network capacity maximization under the constraints of link interference and load balance. Multi-objective hybrid particle swarm optimization is introduced to obtain the Pareto optimal solutions. The simulation results show that the proposed algorithm significantly performs better in terms of achieving the trade-off of multi-performance. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106470	10.1016/j.asoc.2020.106470													
J								Grey Wolf production scheduling for the capital goods industry	APPLIED SOFT COMPUTING										Capital goods; Grey Wolf Optimisation; Fixed-position assembly; Production scheduling; Complex products	ASSEMBLY JOB-SHOP; BIOGEOGRAPHY-BASED OPTIMIZATION; SIMULATED ANNEALING ALGORITHM; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION; COMPLEX PRODUCTS; IMMUNE-SYSTEM; MODEL; SEQUENCE	The capital goods industry produces physical assets used for current and future production. Capital goods are highly customised. Production scheduling aims to synchronise material supply, component manufacturing, sub-assembly and final assembly processes to minimise the total costs of earliness and tardiness, whilst satisfying finite capacity, machining and assembly precedence constraints. This paper presents the first application of Grey Wolf Optimisation (GWO) together with modified and hybridised versions for solving the capital goods scheduling problem. A novel GWO-based production scheduling tool was developed and validated using four realistic case studies obtained from a collaborating company. The first experiment identified appropriate parameter settings for the GWO. The performance of the GWO was then evaluated and compared with a modified GWO and a hybridised GWO. The computational results obtained from the proposed methods were statistical analysed. The outperformed other metaheuristics. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106480	10.1016/j.asoc.2020.106480													
J								Forecasting the monthly iron ore import of China using a model combining empirical mode decomposition, non-linear autoregressive neural network, and autoregressive integrated moving average	APPLIED SOFT COMPUTING										Empirical mode decomposition; Non-linear autoregressive neural network; ARIMA; Iron ore import; Forecasting	TIME-SERIES; CONSUMPTION; OPTIMIZATION; DEMAND; ENERGY	Considering that the non-linear path of monthly time-series for the iron ore imported to China is under reciprocal influences of multiple factors, the process of data generation is not easily represented in a time-series model. Based on the decomposition-integration method, superiorities of empirical mode decomposition (EMD), non-linear autoregressive neural network (NARNN), and autoregressive integrated moving average (ARIMA) models are integrated to establish a combined model EMD-NARNN-ARIMA. The empirical results show that, compared with the NARNN or seasonal autoregressive integrated moving average (SARIMA) models, the proposed model is more suitable for predicting data pertaining to the import of iron ore to China. The prediction error of EMD-NARNN-ARIMA is significantly lower than that of NAR and SARIMA, and, more importantly, it does not increase the time-complexity. The predicted result attained through the proposed model reveals that the import of iron ore to China from January 2019 to December 2020 will gradually decrease, accompanied by reasonable seasonal fluctuations, which is consistent with the decreasing trend in the demand for iron and steel as a result of the adjustment of China's current industrial structure. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106475	10.1016/j.asoc.2020.106475													
J								Modeling and performance evaluation of wind turbine based on ant colony optimization-extreme learning machine	APPLIED SOFT COMPUTING										Ant colony optimization algorithm; Extreme learning machine network; Generator's actual output power; Performance evaluation; Wind turbine	SUPPORT-VECTOR-MACHINE; SHORT-TERM PREDICTION; POWER PREDICTION; NEURAL-NETWORK; DEGRADATION ASSESSMENT; QUANTILE REGRESSION; SPEED; GENERATION; DECOMPOSITION; ALGORITHM	In this paper, an innovative hybrid multi-variable generator's actual-output-power predicting model is proposed based on ant colony optimization algorithm and extreme learning machine network, and a data-driven performance evaluation model is presented based on the two indices, K-means clustering algorithm and Markov chain for the performance evaluation of the wind turbines. Ant colony optimization algorithm is used to optimize the initial weights and thresholds of the extreme learning machine network, then the optimized combinations of weights and thresholds are provided into the extreme learning machine models to overcome the sensitivity problem of initialization setting and the disadvantage of easily falling into local optimum. Through the actual-output-power prediction of the WTs in a wind farm, the results show that the proposed model has more higher prediction accuracy than other methods mentioned in this paper. The optimization process also shows that the prediction accuracy is sensitive to the number of hidden-layer nodes and is relatively insensitive to other model parameters. Then, the data-driven performance evaluation models are proposed based on the error sequences obtained above. The case study is conducted and the results show that the method can evaluate the operating performance of the wind turbines correctly. The effectiveness of the evaluation results is also verified by the actual operation results. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106476	10.1016/j.asoc.2020.106476													
J								Total Bregman divergence-based fuzzy local information C-means clustering for robust image segmentation	APPLIED SOFT COMPUTING										Image segmentation; Fuzzy clustering; Total Bregman divergence; Fuzzy local information C-means clustering; Sample weighting	ALGORITHM; FCM	The fuzzy local information C-means clustering algorithm (FLICM) is an important robust fuzzy clustering segmentation method, which has attracted considerable attention over the years. However, it lacks certain robustness to high noise or severe outliers. To improve the accuracy and robustness of the FLICM algorithm for images corrupted by high noise, a novel fuzzy local information c-means clustering utilizing total Bregman divergence (TFLICM) is proposed in this paper. The total Bregman divergence is modified by the local neighborhood information of sample to further enhance the ability to suppress noise, and then modified total Bregman divergence is introduced into the FLICM to construct a new objective function of robust fuzzy clustering, and the iterative clustering algorithm with high robustness is obtained through optimization theory. The convergence of the TFLICM algorithm is proved by the Zangwill theorem. In addition, the validity of the TFLICM algorithm applied in noise image segmentation is explained by means of sample weighting fuzzy clustering. Meanwhile, the generalized total Bregman divergence unifies the Bregman divergence with the total Bregman divergence and enhances the universality of the TFLICM algorithm applied in segmenting complex medical and remote sensing images. Some experimental results show that the TFLICM algorithm can obtain better segmentation quality and stronger anti-noise robustness than the existing FLICM algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106468	10.1016/j.asoc.2020.106468													
J								Identifying expertise through semantic modeling: A modified BBPSO algorithm for the reviewer assignment problem	APPLIED SOFT COMPUTING										MBBPSO; LDA; Reviewer assignment; Dimension reduction; Research project selection	PARTICLE SWARM OPTIMIZATION; LANGUAGE MODELS; ALLOCATION; INTELLIGENCE	Reviewers play a significant role in academic peer review activities, including conference paper assignment and funding selection, because their evaluation of proposals impacts the final decision. Several studies have proposed reviewer selection strategies or reviewer evaluation methods for solving the problem of selecting appropriate reviewers. Identifying reviewers who are familiar with the proposals to be reviewed is the objective of the reviewer assignment problem. However, the majority of the existing studies ignore quantitative constraints with respect to the articles assigned to the reviewers during the review process. In this study, we propose a novel optimization model with several review condition constraints to address the reviewer assignment problem. In the proposed model, the expertise and research areas of the candidate reviewers and proposals are identified using semantic topic models, which are demonstrated to be effective when measuring the relevance of the reviewers with respect to the proposals to be reviewed; further, the computational efficiency is improved owing to the reduced representation dimensionality. Herein, an improved heuristic algorithm is proposed to match reviewers and papers based on specific topic areas, and candidate reviewers are assigned to each proposal under the global optimum condition based on their overall performance values. Subsequently, an empirical test is conducted using a conference reviewer dataset. The obtained results show that the proposed model can help the managers to efficiently and effectively select reviewers in terms of the convergence rate and convergence level when compared with several classic benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106483	10.1016/j.asoc.2020.106483													
J								The two-echelon multi-objective location routing problem inspired by realistic waste collection applications: The composable model and a metaheuristic algorithm	APPLIED SOFT COMPUTING										Location routing problem; Multi-objective optimization; Multi-echelon; Genetic algorithm	MATHEMATICAL-MODEL; TABU SEARCH; DELIVERY; DESIGN; MOEA/D	Waste collection has always been a major research area in waste management. It plays an important role in social development and environmental sustainability. However, the past research often makes great efforts to formulate dedicated models to some specific waste collection applications, and relatively speaking, fewer efforts have been devoted to relevant method development. Inspired by these issues, in this work, we first develop a more general two-echelon multi-objective location routing problem model (2E-MOLRP) in consideration of the inherent similarities of many realistic waste collection applications. In the model, various commonly-seen and potential costs are classified in a straightforward way and different objectives can hence be flexibly defined to satisfy different requirements. Furthermore, to solve the model, an improved non-dominated sorting genetic algorithm with directed local search (INSGA-dLS) is proposed. In order to validate its effectiveness, experiments are conducted in comparison with existing representative metaheuristics and the results show that our proposed algorithm can achieve better performance even without using local search. Also, we prove that the specially-designed directed locate search is able to further improve our algorithm's performance significantly in experiments. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106477	10.1016/j.asoc.2020.106477													
J								How does thinking relate to tool making?	ADAPTIVE BEHAVIOR										Cognitive archaeology; stone tool making; Material Engagement Theory; cognitive boundaries	HANDAXE REDUCTION; COGNITION; ARCHAEOLOGY; SKILL; METAPLASTICITY; DIFFICULTIES; TECHNOLOGY; TOOLMAKING; ARTIFACTS; GESTURES	How the boundaries of the mind should be drawn with respect to action and the material world is a core research question that cognitive archaeology shares with contemporary cognitive sciences. The study of hominin technical thinking, as in the case of stone tool making, is a good way to bring that question to the fore. This article argues that archaeologists who study lithic artefacts and their transformations over the course of human evolution are uniquely well positioned to contribute to the ongoing debate about the marks of the mental. Adopting the material engagement approach, I propose to replace the internalist vision of mentality, that is, the vision of a brain-bound mind that is using the body to execute and externalise preconceived mental plan through the stone, with an ecological-enactive vision of participatory mentality where bodily acts and materials act together to generate rather than merely execute thought processes. I argue that the latter participatory view changes the geography of the cognitive and offers a better description for the continuity of mind and matter that we see in the lithic record.																	1059-7123	1741-2633														1059712320950539	10.1177/1059712320950539		SEP 2020											
J								Universal Functions Originator	APPLIED SOFT COMPUTING										Universal functions originator; Regression analysis; Neural networks; Support vector machines; Artificial intelligence; Machine learning		Nowadays, couples of computing systems have been introduced to perform many applications, such as function approximation, pattern classification, categorization/clustering, forecasting/prediction, control, and optimization. Linear regression (LR) is commonly used for simple data where the relation between its coefficients is linear, while nonlinear regression (NLR) is used when that relation is nonlinear. Artificial neural networks (ANNs) and support vector machines (SVMs) are more efficient and they can be used for complex applications. However, each one of these approaches has its own strengths and weaknesses. This study introduces a new computing system called "universal functions originator (UFO)". This system is a new symbolic regression (SR) technique that can generate mathematical models universally through two independent optimization algorithms. Different arithmetic operators can be entered into the search pool. Also, any analytic function can be dragged into that pool. UFO has been mathematically designed and practically tested with function approximation problems. However, UFO can also be used for the applications listed above, including anomaly detection, function complication, function simplification, dimension expansion, dimension reduction, and highdimensional function visualization. This novel computing system shows an impressive performance with many promising uses and distinct capabilities. This study reveals the mechanism of UFO and solves some numerical problems via an advanced graphical user interface (GUI) designed just to validate the process of this computing system. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106417	10.1016/j.asoc.2020.106417													
J								A hybrid genetic and Lagrangian relaxation algorithm for resource-constrained project scheduling under nonrenewable resources	APPLIED SOFT COMPUTING										Project scheduling; Nonrenewable resources; Pre-scheduled procurement; Genetic algorithm; Lagrangian Relaxation	RCPSP; MANAGEMENT	Scheduling under nonrenewable resources is one of the challenging issues in project scheduling problems. There are many cases where the projects are subject to some nonrenewable resources. In most of the literature, nonrenewable resources are assumed to be available in full amount at the beginning of the project. However, in practice, it is prevalent that these resources are procured along the project horizon. This paper studies the generalized resource-constrained project scheduling problem (RCPSP) where, in addition to renewable resources, nonrenewable resources are considered, such as budget or consuming materials by the project activities. As the problem is NP-hard, some sub-algorithm elements are developed, which can be used in the structure of inexact approaches for solving the problem. These elements include constraint propagation, priority rules, schedule generation schemes, and local search improvement procedures. Also, a lower bounding algorithm is developed based on the Lagrangian Relaxation (LR) approach, and the problem is optimized by the Genetic Algorithm (GA). The hybrid GA-LR algorithm produces a result reasonably near to optimum solutions. Comprehensive computational experiments based on standard project scheduling problems are performed to evaluate these developments. The experiments showed the performance and robustness of the proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106482	10.1016/j.asoc.2020.106482													
J								A fuzzy collaborative forecasting approach considering experts' unequal levels of authority	APPLIED SOFT COMPUTING										Fuzzy collaborative forecasting; Dynamic random access memory; Fuzzy weighted intersection	GROUP DECISION-MAKING; LINEAR-REGRESSION; INTELLIGENCE APPROACH; TIME; AGGREGATION	Experts typically have unequal authority levels in collaborative forecasting tasks. Most current fuzzy collaborative forecasting methods address this problem by applying a (fuzzy) weighted average to aggregate experts' fuzzy forecasts. However, the aggregation result may be unreasonable, hence fuzzy weighted intersection operators have been proposed for fuzzy collaborative forecasting. This paper proposes that unequal expert authority levels are considered when deriving the membership function rather than the aggregation value. Therefore, the membership of a value in the aggregation result cannot exceed those in experts' fuzzy forecasts. The proposed approach was applied to forecast the yield of a dynamic random access memory product to validate its effectiveness. Experimental results showed that the proposed methodology outperformed all current best-practice methods considered in every aspect, and in particular achieving 65% mean root mean square error reduction. Thus, a high expert authority level increased the likelihood for the forecast, which could not be satisfactorily addressed by simply applying a higher weight to the forecast. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106455	10.1016/j.asoc.2020.106455													
J								A rough-fuzzy approach integrating best-worst method and data envelopment analysis to multi-criteria selection of smart product service module	APPLIED SOFT COMPUTING										Smart product service system; Smart product service module selection; Intrapersonal and interpersonal uncertainty; Rough-fuzzy best-worst method; Rough-fuzzy data envelopment analysis	CUSTOMER SATISFACTION; SUPPLIER SELECTION; SUSTAINABILITY; SYSTEMS; DESIGN; MODEL; TECHNOLOGIES; EFFICIENCY; FRAMEWORK; INTERNET	The revolutionary development and implementation of smart technologies have triggered the manufacturers' servitization trend towards smart product service system (PSS). Accurate selection of smart product service (SPS) module is critical to successful planning and development of smart PSS concept. This study constructs a list of criteria for SPS module selection from the perspectives of service implementation, value symbiosis and smart capability. The selection can be deemed as a multi-criteria decision-making process including two parts: weight determination of criteria and module ranking, in which the intrapersonal linguistic ambiguousness and interpersonal preference randomness are involved. The best-worst method (BWM) is widely acknowledged as an efficient method for weight determination due to its superiority in quickly finding optimal weight with scant decision data. The data envelopment analysis (DEA) method is proven feasible to prioritize alternatives with cost-based and benefit-based criteria. However, these two methods cannot handle the uncertainties involved in the selection process which may lead to imprecise results. Moreover, the previous research rarely studies simultaneous handling of these two types of uncertainty in the realm of BWM and DEA. Therefore, the current study proposes a novel rough-fuzzy BWM-DEA approach to SPS module selection, with fully capturing both the intrapersonal and interpersonal uncertainties. The application of the proposed approach in the smart vehicle service module selection and the comparisons with other methods demonstrate the validity and effectiveness of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106479	10.1016/j.asoc.2020.106479													
J								Z-number integrated weighted VIKOR technique for hazard prioritization and its application in virtual prototype based EOT crane operations	APPLIED SOFT COMPUTING										EOT crane; Hazard identification; Virtual reality; Zadeh's Z-number; VIKOR; Shannon entropy principle	MODIFIED FAILURE MODE; RISK-EVALUATION; FUZZY; SAFETY	Hazard identification and its ranking is one of the foundation steps of safety management. Nowadays, failure mode and effect analysis (FMEA), a reliability analysis tool, is widely used to identify, assess, and rank hazards/failure events related to products, processes, and services because of its simplistic nature. Traditional FMEA focuses on computing the risk priority number (RPN) to rank the identified hazards based on three risk factors, occurrence (O), severity (S), and detectability (D) of a failure mode (FM). However, the traditional RPN method has been criticized because of its limitations in the assessment of hazards/failure events, weighting of the risk factors, and prioritization of the FMs. Furthermore, experts usually provide rating on the risk factors based on their expertise, which introduces impreciseness, uncertainty, and vagueness in the subsequent assessments. In this study, Z-number is used to capture the uncertainty and unreliability associated with experts' evaluation. Also, the concept of objectivity is deployed along with subjectivity in the weight calculation of the risk factors in the information assessment process. Classic VIKOR (ViseKriterijum-ska Optimizacija I Kompromisno Resenje) method is also extended in Z-environment to rank the identified hazards by incorporating the concepts of maximum group utility and minimum regret. For the illustration purpose, we have applied our proposed method in virtual prototyping based EOT crane operations. Sensitivity analysis and comparative evaluation with the existing methods are also presented to validate our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106419	10.1016/j.asoc.2020.106419													
J								A nature-inspired feature selection approach based on hypercomplex information	APPLIED SOFT COMPUTING										Meta-heuristic optimization; Hypercomplex spaces; Feature selection	FIREFLY ALGORITHM; OPTIMIZATION; MODEL	Feature selection for a given model can be transformed into an optimization task. The essential idea behind it is to find the most suitable subset of features according to some criterion. Nature-inspired optimization can mitigate this problem by producing compelling yet straightforward solutions when dealing with complicated fitness functions. Additionally, new mathematical representations, such as quaternions and octonions, are being used to handle higher-dimensional spaces. In this context, we are introducing a meta-heuristic optimization framework in a hypercomplex-based feature selection, where hypercomplex numbers are mapped to real-valued solutions and then transferred onto a boolean hypercube by a sigmoid function. The intended hypercomplex feature selection is tested for several meta-heuristic algorithms and hypercomplex representations, achieving results comparable to some state-of-the-art approaches. The good results achieved by the proposed approach make it a promising tool amongst feature selection research. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106453	10.1016/j.asoc.2020.106453													
J								Kriging-assisted Discrete Global Optimization (KDGO) for black-box problems with costly objective and constraints	APPLIED SOFT COMPUTING										Kriging; Computationally expensive; Discrete; Constrained; Global optimization	MULTIDISCIPLINARY DESIGN OPTIMIZATION; VARIABLE NEIGHBORHOOD SEARCH; SURROGATE MODEL ALGORITHM; ADAPTIVE DIRECT SEARCH; INSPIRED ALGORITHM; PARTICLE SWARM	In this paper, a Kriging-assisted discrete global optimization method is presented for computationally expensive black-box problems. KDGO employs Kriging to approximate the landscape of a black-box model, and utilizes a novel infilling strategy to capture the promising discrete samples. In the infilling strategy, a multi-start knowledge mining approach is introduced, including Optimization, Projection, Sampling and Selection. Firstly, a multi-start optimization is used to capture the promising solutions in the continuous design range. Secondly, all these potential solutions are projected to a predefined matrix and a grid sampling method suitable for low and high-dimensional space is proposed to get the promising discrete samples. Thereafter, the k-nearest neighbors (KNN) search strategy and expected improvement (EI) criterion are jointly used to select the candidate samples. The algorithm keeps running to update Kriging and find the most promising samples until the satisfactory solution is obtained. KDGO is primarily developed to solve time-consuming black-box problems with various discrete cases including binary, integer, non-integer, uni/multimodal and box/inequality-constrained types. After the comparison tests on 20 representative benchmark cases, KDGO proves that it can build a reasonable balance between exploitation and exploration. Besides, compared with the existing 6 methods, KDGO has significant advantages on computational efficiency and robustness. Finally, KDGO is used for structure optimization of a blended-wing-body underwater glider, and gets the satisfactory design. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106429	10.1016/j.asoc.2020.106429													
J								Green vehicle routing and scheduling problem with heterogeneous fleet including reverse logistics in the form of collecting returned goods	APPLIED SOFT COMPUTING										Green vehicle routing and scheduling; Heterogeneous fleet; Reverse logistics; Time-dependent traffic patterns; CO2 emissions	NEIGHBORHOOD SEARCH; GENETIC ALGORITHM; OPTIMIZATION	Vehicle routing problem (VRP) is about finding optimal routes for a fixed fleet of vehicles in order that they can meet the demands for a set of given customers by traveling through those paths. This problem and its numerous expansions are one of the most important and most applicable transportation and logistics problems. In this study, the green vehicle routing and scheduling problem with heterogeneous fleet including reverse logistics in the form of collecting returned goods along with weighted earliness and tardiness costs is studied to establish a trade-off between operational and environmental costs and to minimize both simultaneously. In this regard, a mixed integer non-linear programming (MINLP) model is proposed. Since the problem is categorized as NP-hard, two meta-heuristics, a simulated annealing (SA) and a genetic algorithm (GA) are suggested in order to find near-optimal solutions for large instances in a reasonable computational time. The performances of the proposed algorithms are evaluated in comparison with the mathematical model for small-sized problems and with each other for problems of all size using a set of defined test problems. Analysis of the results considering two criteria: solutions quality and computational times, indicates the satisfactory performance of the presented algorithms in a proper computational time. Meanwhile, a statistical hypothesis testing (T test) is conducted. It can generally be observed that SA achieves relatively better results in terms of solution quality, while GA spends less computational time for all-sized test problems. Eventually, sensitivity analysis is conducted to investigate the effect of collecting returned goods on the cost of total CO2 emissions, variable costs of the fleet and the objective function value. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106462	10.1016/j.asoc.2020.106462													
J								Multi-frame decision fusion based on evidential association rule mining for target identification	APPLIED SOFT COMPUTING										Multi-sensor target identification; Data-driven decision-making; Evidential support and confidence; Association rule mining; Belief function theory	FREQUENT ITEMSETS; INFORMATION; ALGORITHM; MODEL	In the multi-sensor target identification problem involving multiple frames, it is important to fuse the potential information characterizing inherent relations among frames with uncertain decision inputs for enhancing the decision-making process. However, due to the influence of environments or other interference factors, the priori knowledge that accurately represents these relations is usually hard to obtain. To overcome this difficulty, we propose a rule mining-based multi-frame decision fusion (abbreviated as RMDF) method, in which the unknown relations can be discovered from a series of historical sensor reports in the framework of belief functions. First, to accommodate data uncertainty, new measures of evidential support and confidence are defined for a constructed multi-frame evidential database, which are generalizations of the support and confidence measures in binary and probabilistic databases. Then, with these measures, an evidential association rule mining algorithm is developed to discover the relations among frames from a series of historical reports. Finally, how these mined rules are properly combined with uncertain decision information using belief function theory is explored. The key benefit of the RMDF method is that it enables modeling the uncertain relations among frames for deriving more accurate decision results. To demonstrate the feasibility and effectiveness of our proposal, an airborne target identification problem is studied under different conditions and the numerical results show that the identification performance of our method is significantly better than the traditional expert knowledge-based method where the available knowledge is inevitably incomplete or inaccurate. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106460	10.1016/j.asoc.2020.106460													
J								Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm	APPLIED SOFT COMPUTING										Emotion recognition; Feature selection; Firefly algorithm; EEG; Classification	FEATURE-SELECTION; TENSOR DECOMPOSITION; SIGNALS; CLASSIFICATION; PSO	Emotion recognition is helpful for human to enhance self-awareness and respond appropriately towards the happenings around them. Due to the complexity and diversity of emotions, EEG-based emotion recognition is still a challenging task in pattern recognition. In order to recognize diverse emotions, we propose a novel firefly integrated optimization algorithm (FIOA) in this paper. It can simultaneously accomplish multiple tasks, i.e. the optimal feature selection, parameter setting and classifier selection according to different EEG-based emotion datasets. The FIOA utilizes a ranking probability objection function to guarantee the high accuracy recognition with less features. Moreover, the hybrid encoding expression and the dual updating strategy are developed in the FIOA so as to realize the optimal selection of feature subset and classifier without stagnating in the local optimum. In addition to the public DEAP datasets, we also conducted an EEG-based music emotion experiment involving 20 subjects for the validation of the proposed FIOA. After filtering and segmentation, three categories of features were extracted from every EEG signal. Then FIOA was applied to every subject dataset for two pattern recognition of emotions. The results show that the FIOA can automatically find the optimal features, parameter and classifier for different emotion datasets, which greatly reduces the artificial selection workload. Furthermore, comparing with the binary particle swarm optimization (PSObinary) and the binary firefly (FAbinary), the FIOA can achieve the higher accuracy with less features in the emotion recognition. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106426	10.1016/j.asoc.2020.106426													
J								A multi-stage multi-criteria hierarchical decision-making approach for sustainable supplier selection	APPLIED SOFT COMPUTING										Fuzzy best-worst method; Sustainable supplier selection; Supply chain management; Fuzzy logic	BEST-WORST METHOD; FUZZY TOPSIS; NETWORK; MANAGEMENT; MODEL; PERFORMANCE; ENVIRONMENT; EXTENSION; FRAMEWORK; RISK	Sustainable supplier selection is known as a crucial objective in supply chains due to its impact on profitability, adorability, flexibility, and agility of the system. This study proposes a new multi-stage hierarchical fuzzy index-based approach with which decision-makers are empowered to select the most sustainable supplier based on sustainability triple bottom line criteria. Besides, a new fuzzy extension for the best-worst method is proposed considering trapezoidal fuzzy membership functions that can cover uncertainty under imprecise environments. This study makes a contribution to the literature of sustainable supply chains in the sense that it facilitates the computational complexity of previous decision-making approaches by collecting the most relevant criteria and a straightforward fuzzy process. Besides, the graded mean integration representation method has been adopted for prioritizing the supplier based on their performance of sustainable development, which enhances the accuracy of selection compared with previous ranking methods. The proposed study can be utilized as a benchmark for sustainability evaluations between suppliers. A real-world case study is resolved to illustrate the superiority and broad application of the proposed model. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106456	10.1016/j.asoc.2020.106456													
J								IECT: A methodology for identifying critical products using purchase transactions	APPLIED SOFT COMPUTING										Critical products; Data mining; Vertical database; Frequent itemsets; RFM	CUSTOMER SEGMENTATION; MINING FREQUENT; ALGORITHMS; ITEMSETS; INFREQUENT; RFM	Identifying critical products and key customers to strengthen company performance is vitally important in the digital transformation era. Critical products are the itemsets that are preferred by vip customers and yet not popular among ordinary customers. As a result, critical products should be kept on the shelf despite its sales volumes may be lower than other popular items. However, few studies have considered identifying critical products or their potentially valuable patterns. Therefore an innovative algorithm taking advantage of vertical databases to identify critical products was designed. The proposed algorithm is applied to a transaction database of a midsize supermarket to verify the performance. The result showed that precision can reach 80.55% and 82.15% for two different filtering criteria. To the best of our knowledge, this study is the first to apply the concept of critical products to real retail industry transaction records. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106420	10.1016/j.asoc.2020.106420													
J								Incomplete data classification with view-based decision tree	APPLIED SOFT COMPUTING										Incomplete data; Missing value; Classification; Decision tree	MISSING VALUE ESTIMATION; GENETIC ALGORITHMS; DATA IMPUTATION; REGRESSION; NETWORK	Data quality issues may bring serious problems in data analysis. For instance, missing values could decrease the accuracy of the classification. As traditional classification approaches can only be applied to complete data sets, we present a generic classification model for incomplete data where existing classification methods can be effectively incorporated. Firstly, we generate complete views from the incomplete data by choosing proper subsets of attributes based on Information Gain measure. Then we use these selected views to obtain multiple base classifiers. Finally, the base classifies are effectively combined as a final classifier with a decision tree. Extensive experiments results on real data sets demonstrate that the proposed method outperforms existing approaches. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106437	10.1016/j.asoc.2020.106437													
J								A memetic algorithm with novel semi-constructive evolution operators for permutation flowshop scheduling problem	APPLIED SOFT COMPUTING										Flowshop scheduling; Genetic algorithm; Simulated annealing; Memetic algorithm; NEH constructive heuristic	ITERATED GREEDY ALGORITHM; CUCKOO SEARCH ALGORITHM; GENETIC ALGORITHMS; MINIMIZE MAKESPAN; COLONY ALGORITHM; M-MACHINE; OPTIMIZATION; HEURISTICS; METAHEURISTICS; CLASSIFICATION	This paper proposes a memetic algorithm (MA) with novel semi-constructive crossover and mutation operators (MASC) to minimize makespan in permutation flowshop scheduling problem (PFSP). MASC combines the strengths of genetic algorithm (GA), simulated annealing (SA), and Nawaz-Enscore-Ham (NEH) algorithm. The aim is to enhance GA in identifying promising areas in the search space, whose local optima will be subsequently located by SA. This is achieved by means of novel crossover and mutation operators that construct chromosomes by using two different types of genes: static and dynamic genes. MASC is tested on the well-known Taillard's benchmark instances. The proposed operators are compared with traditional operators. The results show that the proposed operators produce considerable improvements. These improvements reach up to 20.79% in the average relative error of best solution and 11.86% in the average relative error of average solution. MASC is compared with fourteen well-known and state-of-the-art algorithms. These algorithms include MA, whale optimization, ant colony optimization, particle swarm optimization, artificial bee colony, monkey search, and iterated greedy. The results show that MASC outperforms all the compared algorithms except three iterated greedy algorithms. Moreover, the improvement in the average relative error of best solution achieved on the best-so-far MA is 37.92%. Therefore, MASC can be considered as one of the best-so-far methods for PFSP. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106458	10.1016/j.asoc.2020.106458													
J								Continual learning classification method with new labeled data based on the artificial immune system	APPLIED SOFT COMPUTING										Artificial immune system; Classification; Continual learning; Machine learning; New labeled data	NEGATIVE SELECTION ALGORITHM; RECOGNITION; DIAGNOSIS	In this paper, a new supervised learning classification method, continual learning classification method with new labeled data based on the artificial immune system (CLCMNLD), is proposed as a new way to improve the classification performance in real-time by continually learning the new labeled data during the testing stage. It is inspired by the mechanism that vaccines can enhance immunity. New types of memory cells were continuously cultured by learning new labeled data during the testing stage. CLCMNLD will degenerate into a common supervised learning classification method when there is no new labeled data comes out during the testing stage. The effectiveness of the proposed CLCMNLD is tested on twenty well-known datasets from the UCI Machine Learning Repository that are commonly used in the domain of data classification. The experiments reveal that CLCMNLD has better classification performance when it degenerates into a common supervised learning classification method, and it outperforms the other methods when there are some new labeled data comes out during the testing stage. The more types of new labeled data, the more advantages it has. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106423	10.1016/j.asoc.2020.106423													
J								A new method for multivariable nonlinear coupling relations analysis in complex electromechanical system	APPLIED SOFT COMPUTING										Complex electromechanical system; Nonlinear relationship of coupling; Granger causality; Causality analysis; Nonlinear fitting; RBF neural network	CROSS-CORRELATION ANALYSIS; PARTIAL GRANGER CAUSALITY; SIGNED DIRECTED GRAPH; EXOGENOUS INPUTS; FAULT-DIAGNOSIS; TIME-SERIES; CONNECTIVITY; DEPENDENCE; NETWORKS; MODEL	Coupling relations analysis of monitoring variables in complex electromechanical system is a powerful and useful means for abnormal detection, false monitoring information identification and fault diagnosis. However, due to the characteristics of multivariable, nonlinear and non-stationarity in the actual production process, it is difficult to achieve multivariable coupling modeling of complex electromechanical systems. In this paper, a new coupling modeling method is proposed by combining causality analysis with RBF neural network. First, considering the multivariate and nonlinearity of complex system, the conditional Granger and nonlinear Granger is combined to analyze the causal relations of process variables and obtain the cause variable set of any variable in complex system. Second, the RBF neural network is applied to achieve the nonlinear fitting and the parameter is optimized to ensure the fitting precision. Finally, the effectiveness of the proposed method is verified by an analysis of one case study of real compressor groups data set in chemical production system. This new approach can handle general coupling modeling problems and obtain a quantitative nonlinear coupling model by determine the dependent and independent variables in system and the functional relationship between them. Which dedicated to studying quantitative functional relationship of variable coupling, not just considering the direction or strength of coupling as in the existing, and does not need any prior knowledge about the physical structure. Thus, the proposed method can be effectively used in coupling modeling of complex electromechanical systems and formulate the foundation of anomaly detection, information quality assessment, and failure propagation mechanism, as well as other engineering applications. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				SEP	2020	94								106457	10.1016/j.asoc.2020.106457													
J								Towards the behavior analysis of chemical reactors utilizing data-driven trend analysis and machine learning techniques	APPLIED SOFT COMPUTING										Machine learning; Industry 4.0; Process control; Monitoring; Malfunction prognosis; Real-time systems	FAULT-DETECTION; ACCIDENTS; DIAGNOSIS; COSTS; SCALE	The concept of modeling the behavior of industrial processes is of great importance as it describes the possible states of equipment used in large industries, which once damaged, it usually costs both in time and money. In this paper, we propose a data-driven methodology for depicting three distinct states of a chemical reactor, (1) normal, (2) warning, (3) alert, by using machine learning techniques. A method for predicting the classification of data input, assists in prevention (early prognosis) of possible malfunctions. This method uses a combined linear trend analysis of the involved data which form the warning state of the reactor where the pre-incident conditions are fulfilled. Afterwards, it checks the possibility of the subsequent input to be classified in the alert state which is an indication that the reactor's active equipment, such as heating resistance, will start malfunctioning. The objective of the three main steps of the proposed methodology are: first, to reveal the number of clusters based on past data, second to train normal, warning and alert behavior-models and validate them and third to test them as well as verify the accuracy of linear trend analysis. The proposed methodology is based on the analysis of real data sets derived from the automation system of a chemical process located at CERTH/CPERI in order to identify real-life models for prognostic behavior for malfunction prevention. This approach is especially suitable for modern industrial systems that follow Industry 4.0 principles. The results reveal a robust modeling of the reactor's behavior with accuracy reaching 88,94%. (C) 2020 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				SEP	2020	94								106464	10.1016/j.asoc.2020.106464													
J								Flexible job shop scheduling problem with reconfigurable machine tools: An improved differential evolution algorithm	APPLIED SOFT COMPUTING										Flexible job shop; Configuration-dependent setup times; Industry 4.0; Self-adaptive differential evolution; Nelder-Mead mutation strategy	SEQUENCE-DEPENDENT SETUP; NELDER-MEAD ALGORITHM; GLOBAL OPTIMIZATION; TABU SEARCH; DESIGN; SYSTEMS; MODEL	Developing reconfigurable machine tools (RMTs) has attracted increasing attention recently. An RMT can be utilized as a group of machines, which can obtain different configurations to satisfy manufacturing requirements. This paper deals with a production scheduling problem in a shop-floor with RMTs as an extension of a flexible job shop scheduling problem (FJSSP). To begin with, two mixed-integer linear programming models with the positionand sequence-based decision variables are formulated to minimize the maximum completion time (i.e., makespan). The CPLEX solver is used to solve the smalland medium-sized instances. The computational experiments show that the sequence-based model significantly outperforms the other one. Since even the sequence-based model cannot optimally solve most of the medium-sized problems, a self-adaptive differential evolution (DE) algorithm is proposed to efficiently solve the given problem. Moreover, the effectiveness of the proposed algorithm is enhanced by introducing a new mutation strategy based on a searching approach hired from a Nelder-Mead method. The performance of the proposed method and three other well-known variants of the DE algorithm are first validated by comparing their results with the results of the sequence-based model. Additional experiments on another data set including large-sized problems also confirm that the proposed algorithm is extremely efficient and effective. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106416	10.1016/j.asoc.2020.106416													
J								Scheduling different types of vehicles in distribution centers with fixed due dates and packed shipments	APPLIED SOFT COMPUTING										Distribution center; Cross docking; Taguchi method; Non-dominated sorting genetic algorithm; Particle swarm optimization; Meta-heuristic	DOCK-DOOR ASSIGNMENT; CROSS-DOCKING; OUTBOUND TRUCKS; NETWORK DESIGN; ALGORITHM; DELIVERY; SYSTEMS	This paper proposes a novel non-dominated sorting genetic algorithm-based method for scheduling simultaneously different types of vehicles with different capacities in distribution centers working as cross dock. In this paper, a three-objective model is proposed, which minimizes operational time, lateness and earliness of delivering products. In many of real life cases, packs of products with different number of products must be unloaded instead of arbitrary number of products because of products' nature and/or physical limitations. It means that the optional amount of unloading is not prohibited in many of real life cases unlike to previous researches; therefore, scheduling vehicles by considering this limitation is different from previous works that allow optional amount of unloading. Another advantage of this paper is consideration of the frequent unloading pattern of inbound vehicles. This consideration better synchronizes the inbound and outbound vehicles compared to nonfrequent pattern, and as a result, it helps to reduce the operational time of cross docking as well as the shipping cycle. This paper proposes a new scheduling method for considering the mentioned novelties. Furthermore, Taguchi design is used for regulating the proposed algorithm's parameters. Several numerical examples are solved by the proposed method, and the obtained results are compared to multi-objective particle swarm optimization. The numerical results show the superiority of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106450	10.1016/j.asoc.2020.106450													
J								Improving class noise detection and classification performance: A new two-filter CNDC model	APPLIED SOFT COMPUTING										Class noise detection; Ensemble filtering; Distance filtering; Classification	LABEL NOISE; ATTRIBUTE NOISE; ELIMINATION; ENSEMBLES	Class noise is an important issue in classification with a lot of potential consequences. It can decrease the overall accuracy and increase the complexity of the induced model. This study investigates ensemble filtering, removing and relabeling noisy instances issues and proposes a new two-filter model for Class Noise Detection and Classification (CNDC). The proposed two-filter CNDC model comprises two major parts, which are noise detection and noise classification. The noise detection part involves ensemble and distance filtering to overcome ensemble issues. In latter part, a Removing-Relabeling (REM-REL) technique is proposed to enhance overall performance of noise classification. To evaluate the performance of the proposed model, several experiments were conducted on six real data sets. The proposed REM-REL technique was found to be successful to classify noisy instances. The final results showed that the proposed model led to a significant performance improvement compared with ensemble filtering. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106428	10.1016/j.asoc.2020.106428													
J								Fuzzy based image edge detection algorithm for blood vessel detection in retinal images	APPLIED SOFT COMPUTING										Fuzzy logic; Retinal fundus images; Retinal blood vessels; Retinal vessel segmentation; Edge detection; Medical image processing	NEURAL-NETWORKS; FUNDUS IMAGES; SEGMENTATION; SYSTEM; MODEL	We developed a contour detection based image processing algorithm based on Mamdani (Type-2) fuzzy rules for detection of blood vessels in retinal fundus images. The method uses the green channel data from eye fundus images as input, Contrast-Limited Adaptive Histogram Equalization (CLAHE) for contrast enhancement, and median filter for background exclusion. The Mamdani (Type-2) fuzzy rules applied on image gradient value are used for edge detection. The results of experiments on the Digital Retinal Images for Vessel Extraction (DRIVE), STructured Analysis of the Retina (STARE) and CHASEdb datasets show the applicability of the proposed method as a flexible approach which can be adapted to numerous edge detection/contour based applications. We achieved an accuracy of 0.865 for STARE dataset, an accuracy of 0.939 for the DRIVE dataset, and the accuracy of 0.950 for the ChaseDB dataset. In relation to works of other authors, our method offered a similar performance, but it offers an improved dynamics and flexibility in formulation of the linguistic threshold criteria, which can be a leading factor in design of image processing systems with dynamic and flexible rules, such as Type 2 fuzzy rules would allow, offering an interesting alternative to currently widespread deep learning applications. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106452	10.1016/j.asoc.2020.106452													
J								Fuzzy expert analysis of the severity of mining machinery failure	APPLIED SOFT COMPUTING										Severity of failure; Severity of failure assessment; Mining machinery; Fuzzy sets; Fuzzy logic	RISK-ASSESSMENT; RELIABILITY-ANALYSIS; MODE; EQUIPMENT; METHODOLOGY; SYSTEMS	Mining machinery failure is almost an everyday occurrence. Usually the failures bare certain consequences, which require additional financial costs to repair and restore the system to its operational state. The consequences are viewed through negative and damaging effects a failure has on the machine, health and safety of the employees, work environment, and on the environment. The removal of the consequences of the failure requires additional financial investment, which has a negative impact on the company's business. In order to prevent this, it is necessary to have Risk Centered Maintenance, where the risk assessment would include all the negative consequences of the risky event. The fuzzy expert assessment is presented in this paper, as well as the failure severity assessment based on the harmful effects of the failure. The negative effects of the machine component failure, such as the time needed for the repair, the possibility of workplace injury caused by the failure, and the impact it has on the environment, are analyzed in this paper as well. This approach to the severity of failure assessment enables a more comprehensive view of this risk indicator. It also enables the risk indicator "severity of failure"to gain greater significance in combination with other risk indicators. The developed model was presented on the example of typical failures of the hydraulic subsystems of a mobile crushing machine Lokotrack LT 1213S. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106459	10.1016/j.asoc.2020.106459													
J								A dynamic interval type-2 fuzzy customer segmentation model and its application in E-commerce	APPLIED SOFT COMPUTING										Customer relationship management; Customer segmentation; Interval type-2 fuzzy linguistic labels; Fuzzy equivalence relation clustering analysis	GROUP DECISION-MAKING; CLUSTERING APPROACH; SELECTION; SETS	Internet-based services and retail are growing rapidly. To manage online customer relationships with linguistic comments, we propose a dynamic interval type-2 fuzzy customer segmentation model. Interval type-2 fuzzy linguistic labels (IT2FLLs) are used to model customer comments. The similarity of IT2FLLs is computed based on an extended distance method. Customers are segmented dynamically according to the fuzzy equivalence relations of similarity. A case study in E-commerce shows the application of the proposed model, and a comparative analysis shows its effectiveness. The dynamic customer segmentation can help managers to have a deep understanding on customers' purchasing behaviors and to make accurate recommendations. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106366	10.1016/j.asoc.2020.106366													
J								Hypergraph membrane system based F-2 fully convolutional neural network for brain tumor segmentation	APPLIED SOFT COMPUTING										Brain tumor segmentation; Fully convolutional neural network; Membrane system	P-SYSTEMS; POWER	Accurate segmentation is a necessary step in the clinical management of brain tumors. However, the task remains challenging due to not only large variations in the sizes and shapes of brain tumors but also wide variations among individuals. In this paper, we develop a novel fully convolutional neural network with a feature reuse module and feature conformity module (F-2 FCN) to alleviate the above challenges and further improve the accuracy of segmentation. Specifically, to extract more valuable features, we present a feature reuse module to repeatedly utilize features from different layers. We also provide a feature conformity module to eliminate possible noise and enhance the fusion of different feature map levels. However, the difficult selection of multiple parameters and the long training time of a single model make CNNs less effective. To solve these problems, a new distributed and parallel computing model, a hypergraph membrane system, is designed to implement the F-2 FCN. In particular, we develop a hypergraph membrane structure with three new kinds of rules to implement several F-2 FCNs with different settings simultaneously to leverage the ensemble learning of F-2 FCNs and save time. Experimental results on two datasets show promotive performance in terms of the Dice similarity coefficient (DSC), positive predictive value (PPV) and sensitivity compared with the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106454	10.1016/j.asoc.2020.106454													
J								An improved network structural balance approach based on weighted node-to-node influence with evolutionary algorithm	APPLIED SOFT COMPUTING										Signed social network; Node-to-node influence; Structural balance; Evolutionary algorithm	SIGNED NETWORKS; MULTIOBJECTIVE OPTIMIZATION	The network structural balance is a challenging task in social networks. The difficulty is how to determine the unbalanced degree of a network and make the network balanced with the least cost. Aiming at this issue, this paper proposes a new weighted node-to-node influence (W2NI) model, which integrates two important node-to-node influence factors, i.e., weights and influences between nodes. Then, an EA-based weighted influence structural balance (WISB) algorithm is devised deliberately to optimize W2NI model. In WISB algorithm, a neighbors-based initialization and a random greedy based local search strategies are proposed to enhance its convergence performance. Comprehensive experiments on a set of generated networks and real social networks demonstrate the effectiveness and efficiency of the proposed model. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				SEP	2020	94								106323	10.1016/j.asoc.2020.106323													
J								A Neural network enhanced hidden Markov model for tourism demand forecasting	APPLIED SOFT COMPUTING										Autoregressive neural network; Hidden Markovian model; Low-pass filter; Forecasting	TIME-SERIES; BUSINESS CYCLES; VOLATILITY; ACCURACY; ARRIVALS; PERFORMANCE; FREQUENCY	In recent years, tourism demand forecasting has attracted more interests not only in tourism area but in data science field. In this study, we follow the previous relevant data science literatures and propose a new neural network enhanced hidden Markovian structural time series model (NehM-STSM). This model takes a multiplicative error structure of a trend and a seasonal element. The trend is modelled by an artificial neural network while the seasonal element is captured by a tailor-made hidden Markovian model with four components: a persistence replicative cycle, a jump component capturing an unexpected event, an amplitude component reflecting the event strength and a random error term. The empirical research is conducted using US incoming tourism data from twelve major source countries across January 1996-September 2017. The proposed NehM-STSM achieves a better performance than the chosen benchmark models for two error measures and most forecasting horizons. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106465	10.1016/j.asoc.2020.106465													
J								Bibliometric analysis of rough sets research	APPLIED SOFT COMPUTING										Bibliometric analysis; Rough sets; Co-citation; Co-occurrence; Burst detection	INFORMATION SCIENCES; FEATURE-SELECTION; 3-WAY DECISIONS; APPROXIMATION; MANAGEMENT; REDUCTION; MODEL	Rough set (RS) is a mathematical framework used to deal with incomplete and uncertain information. It has been widely used in decision analysis, data mining, artificial intelligence, economic management and many other fields. Up to now, there have been tens of thousands of research papers on this topic, and the area has made a rapid growth. In light of these factors, a comprehensive and systematic review of this area becomes essential. The purpose of this study is to present a coherent overview of the theory and applications of the RS, reveal its current research focal points, and identify future development trends. We conduct a thorough bibliometric review and perform co-occurrence and co-citation analysis. First, the fundamental characteristics, productive authors, preferred journals and leading countries in the field of RS are identified. Second, the co-citation and citation burst detection methods are used to explore research hotspot and trends. In light of the undertaken methodology, this study can offer tangible value to scholars in understanding the content structure and development process of the RS field. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106467	10.1016/j.asoc.2020.106467													
J								An Information Maximization Multi-task Clustering Method for egocentric temporal segmentation	APPLIED SOFT COMPUTING										Temporal segmentation; First person vision; Multi-task clustering	VIDEO; VISION	With the widespread application of vision-based wearable devices, temporal segmentation helps people search for and localize all occurrences quickly in egocentric videos. In the same scenario, the activities are similar to each other, e.g., people staying at home typically cook, clean and watch TV. These relations among videos of different individuals are regarded as auxiliary information to improve task performance. Inspired by this, we propose an Information Maximization Multi-task Clustering (IMMC) algorithm for egocentric temporal segmentation. The algorithm mainly includes two parts: (1) within-task clustering: clustering on each task based on an information maximization approach, and (2) cross-task information transferring: a novel strategy is presented to transfer correlation information between tasks, which balances the correlation among clusters in different tasks to improve the performance of the individual task. A draw-merge method is designed to address the optimization problem. Experiments are performed on three publicly available first-person vision data sets and a new data set we construct (Outdoor data set). The results show that IMMC consistently outperforms the state-of-the-art clustering methods in multiple evaluation metrics. Moreover, it achieves relatively good performance on runtime cost and convergence. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106425	10.1016/j.asoc.2020.106425													
J								Ameliorated moth-flame algorithm and its application for modeling of silicon content in liquid iron of blast furnace based fast learning network	APPLIED SOFT COMPUTING										Moth-flame optimization algorithm; Fast learning network; Silicon content; Blast furnace	OPTIMIZATION ALGORITHM; IDENTIFICATION; PREDICTION	Moth-Flame Optimization (MFO) algorithm is a widely used nature-inspired optimization algorithm. However, for some complex optimization problems, such as high dimensional and multimodal problems, the MFO may fall into the local optimal solution. Hence, in this paper an ameliorated Moth-Flame Optimization (AMFO) algorithm is presented to improve the solution quality and global optimization capability. The key features of the proposed algorithm are the Gaussian mutation produce flames and the modified position updating mechanism of moths, which can improve the ability of MFO to jump out of local optimum solutions. In addition, opposition-based learning is adopted to initialize the population. The AMFO algorithm is compared with 9 state-of-the-art algorithms (such as Levy Moth-Flame Optimization (LMFO), Grey Wolf Optimization (GWO), Sine Cosine Algorithm (SCA), Heterogeneous Comprehensive Learning Particle Swarm Optimization (HCLPSO)) on 23 classical benchmark functions. The comparative results show that the AMFO is effective and has good performance in terms of jumping out of local optimum, balancing exploitation ability and exploration ability. Furthermore, the AMFO is adopted to optimize the parameters of fast learning network (FLN) to build the prediction model of silicon content in liquid iron for blast furnace, and simulation experiment results from field data show that the root mean square error of the AMFO-FLN model is 0.0542, hit ratio is 91 and the relative error is relatively stable, the main fluctuation is between-0.1 and 0.1; compared with other ten silicon content in liquid iron models, the AMFO-FLN model has better predictive performance. (C) 2020 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				SEP	2020	94								106418	10.1016/j.asoc.2020.106418													
J								Memetic search for composing medical crews with equity and efficiency	APPLIED SOFT COMPUTING										Health care service management; Memetic algorithm; Hybrid search; Tabu search; Heuristics	TABU SEARCH; ALGORITHM; MODELS	Composing medical crews with equity and efficiency is an important practical problem commonly arising from health care system management. This work presents the first hybrid memetic algorithm for this problem. The proposed approach integrates an original backbone-based crossover for generating promising offspring solutions and a tabu search based local optimization algorithm exploring both feasible and infeasible search regions. Computational experiments on two sets of benchmark instances in the literature are conducted to assess the proposed algorithm with reference to existing methods. This study advances the state-of-the-art of solving this relevant practical problem and is expected to inspire new solution methods to similar problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106440	10.1016/j.asoc.2020.106440													
J								A combined forecasting system based on modified multi-objective optimization and sub-model selection strategy for short-term wind speed	APPLIED SOFT COMPUTING										Novel combined forecasting system; Modified multi-objective optimization; Wind farm; Sub-models selection	ARTIFICIAL NEURAL-NETWORKS; PREDICTION; ALGORITHM; ARCHITECTURE; RESOURCE	Forecasting models have been widely used in wind-speed time series forecasting that are often nonlinear, irregular, and non-stationary. Current forecasting models based on artificial neural network can adapt to various wind-speed time series. However, they cannot simultaneously and effectively forecast the entire wind-speed time series of a wind farm. In this paper, a novel combined forecasting system is developed for a wind farm that includes that SSAWD secondary de-noising algorithm is used to pre-process original wind speed data, and then the sub-model selection strategy is used to select five optimal sub models for the combined model. Meanwhile, a modified multi-objective optimization algorithm optimizes weight of the combined model, and the experimental results show that this forecasting system outperforms other traditional systems and can be effectively used to forecast wind-speed time series of a large wind farm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106463	10.1016/j.asoc.2020.106463													
J								A case learning-based differential evolution algorithm for global optimization of interplanetary trajectory design	APPLIED SOFT COMPUTING										Global optimization; Trajectory design; Multiple Gravity Assists; Case learning-based method	LOW-THRUST; SPACECRAFT; CROSSOVER	The problem of optimally designing an interplanetary trajectory for a space mission is considered in this paper. To tackle the extreme non-linearity of the search space, a case learning-based differential evolution algorithm, named CLDE, is proposed. It stores successful control parameters (scaling factor and crossover possibility) and retrieve the available reference information according to a geographic similarity in each generation. To depart from the basin of attraction of a local optimum, CLDE will give up learning from the successful cases once no better offsprings have been obtained within a certain number of generations and generate new control parameters. Two versions of CLDE have been developed, for global optimization (G-CLDE) and local optimization (L-CLDE), respectively. Their performance has been tested on GTOP benchmarks and real mission design. Experimental results show that G-CLDE performs better than related algorithms, including PYGMO algorithms and recently published L-SHADE variants. L-CLDE can improve upon the best known solution for the Messenger benchmark (full version). By connecting G-CLDE and L-CLDE together, CLDE finds promising results in acceptable computational time on the GTOP benchmark. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				SEP	2020	94								106451	10.1016/j.asoc.2020.106451													
J								Persistence codebooks for topological data analysis	ARTIFICIAL INTELLIGENCE REVIEW										Persistent homology; Machine learning; Persistence diagrams; Bag of words; VLAD; Fisher vectors	HOMOLOGY	Persistent homology is a rigorous mathematical theory that provides a robust descriptor of data in the form of persistence diagrams (PDs) which are 2D multisets of points. Their variable size makes them, however, difficult to combine with typical machine learning workflows. In this paper we introduce persistence codebooks, a novel expressive and discriminative fixed-size vectorized representation of PDs that adapts to the inherent sparsity of persistence diagrams. To this end, we adapt bag-of-words, vectors of locally aggregated descriptors and Fischer vectors for the quantization of PDs. Persistence codebooks represent PDs in a convenient way for machine learning and statistical analysis and have a number of favorable practical and theoretical properties including 1-Wasserstein stability. We evaluate the presented representations on several heterogeneous datasets and show their (high) discriminative power. Our approach yields comparable-and partly even higher-performance in much less time than alternative approaches.																	0269-2821	1573-7462															10.1007/s10462-020-09897-4		SEP 2020											
J								Exploring Personalised Autonomous Vehicles to Influence User Trust	COGNITIVE COMPUTATION										Autonomous vehicle; Driving characteristics; Driving style; Personalisation; Trust; User experience; User study; Human factors	DRIVING STYLE; SELF-REPORT; AUTOMATION; DRIVERS; ACCEPTANCE; BEHAVIOR; SYSTEM; METAANALYSIS; EXPERIENCE; COMFORT	Trust is a major determinant of acceptance of an autonomous vehicle (AV), and a lack of appropriate trust could prevent drivers and society in general from taking advantage of such technology. This paper makes a new attempt to explore the effects of personalised AVs as a novel approach to the cognitive underpinnings of drivers' trust in AVs. The personalised AV system is able to identify the driving behaviours of users and thus adapt the driving style of the AV accordingly. A prototype of a personalised AV was designed and evaluated in a lab-based experimental study of 36 human drivers, which investigated the impact of the personalised AV on user trust when compared with manual human driving and non-personalised AVs. The findings show that a personalised AV appears to be significantly more reliable through accepting and understanding each driver's behaviour, which could thereby increase a user's willingness to trust the system. Furthermore, a personalised AV brings a sense of familiarity by making the system more recognisable and easier for users to estimate the quality of the automated system. Personalisation parameters were also explored and discussed to support the design of AV systems to be more socially acceptable and trustworthy.																	1866-9956	1866-9964															10.1007/s12559-020-09757-x		SEP 2020											
J								A stochastic learning-from-data approach to the history-matching problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										History matching; Reservoir simulation; Learning-from-data strategy; Stochastic methods	DISCRETE LATIN HYPERCUBE	History matching is the process whereby the values of uncertain attributes of a reservoir model are changed with the purpose of finding models that match existing reservoir production data. As an inverse and ill-posed problem in engineering, it admits multiple solutions and plays a key role in reservoir management tasks: reservoir models support important and strategic field development decisions and, the more calibrated the models, the higher the confidence on their forecast for the actual reservoir's performance. In this work, we introduce a stochastic learning-from-data approach to the history-matching problem. With a data-driven nature, the proposed algorithm has dedicated components to handle petrophysical and global uncertain attributes, and generates new solutions using the patterns of attributes present in solutions that are judiciously selected among a set of solutions for each well and variable involved in the history-matching process. We apply our approach to the UNISIM-I-H benchmark, a challenging synthetic case based on the Namorado Field, Campos Basin, Brazil. The results indicate the potential of our learning proposal towards generating multiple solutions that not only match the history data but, most importantly, offer acceptable performance while forecasting field production. Compared with history-matching methodologies previously applied to the same benchmark, our approach produces competitive results in terms of matching quality and forecast capacity, using substantially fewer simulations.																	0952-1976	1873-6769				SEP	2020	94								103767	10.1016/j.engappai.2020.103767													
J								Surrogate-assisted parallel tempering for Bayesian neural learning	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Bayesian neural networks; Parallel tempering; MCMC; Surrogate-assisted optimization; Parallel computing	RESPONSE-SURFACE METHODS; CHAIN MONTE-CARLO; VARIATIONAL INFERENCE; INVERSION; ALGORITHM; NETWORKS; OPTIMIZATION; DESIGN	Due to the need for robust uncertainty quantification, Bayesian neural learning has gained attention in the era of deep learning and big data. Markov Chain Monte-Carlo (MCMC) methods typically implement Bayesian inference which faces several challenges given a large number of parameters, complex and multimodal posterior distributions, and computational complexity of large neural network models. Parallel tempering MCMC addresses some of these limitations given that they can sample multimodal posterior distributions and utilize high-performance computing. However, certain challenges remain given large neural network models and big data. Surrogate-assisted optimization features the estimation of an objective function for models which are computationally expensive. In this paper, we address the inefficiency of parallel tempering MCMC for large-scale problems by combining parallel computing features with surrogate assisted likelihood estimation that describes the plausibility of a model parameter value, given specific observed data. Hence, we present surrogate-assisted parallel tempering for Bayesian neural learning for simple to computationally expensive models. Our results demonstrate that the methodology significantly lowers the computational cost while maintaining quality in decision making with Bayesian neural networks. The method has applications for a Bayesian inversion and uncertainty quantification for a broad range of numerical models.																	0952-1976	1873-6769				SEP	2020	94								103700	10.1016/j.engappai.2020.103700													
J								Precise object detection using adversarially augmented local/global feature fusion	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										High spatial resolution (HSR) remote sensing imagery; Geospatial object detection; Super resolution generative adversarial network; Data augmentation; Local/global feature fusion	CONVOLUTIONAL NETWORKS; VISUAL SALIENCY; MODEL	Object detection, which aims at recognizing or locating the objects of interest in remote sensing imagery with high spatial resolutions (HSR), plays a significant role in many real-world scenarios, e.g., environment monitoring, urban planning, civil infrastructure construction, disaster rescuing, and geographic image retrieval. As a long-lasting challenging problem in both machine learning and geoinformatics communities, many approaches have been proposed to tackle it. However, previous methods always overlook the abundant information embedded in the HSR remote sensing images. The effectiveness of these methods, e.g., accuracy of detection, is therefore limited to some extent. To overcome the mentioned challenge, in this paper, we propose a novel two-phase deep framework, dubbed GLGOD-Net, to effectively detect meaningful objects in HSR images. GLGOD-Net firstly attempts to learn the enhanced deep representations from super-resolution image data. Fully utilizing the augmented image representations, GLGOD-Net then learns the fused representations into which both local and global latent features are implanted. Such fused representations learned by GLGOD-Net can be used to precisely detect different objects in remote sensing images. The proposed framework has been extensively tested on a real-world HSR image dataset for object detection and has been compared with several strong baselines. The remarkable experimental results validate the effectiveness of GLGOD-Net. The success of GLGOD-Net not only advances the cutting-edge of image data analytics, but also promotes the corresponding applicability of deep learning in remote sensing imagery.																	0952-1976	1873-6769				SEP	2020	94								103710	10.1016/j.engappai.2020.103710													
J								Levy flight distribution: A new metaheuristic algorithm for solving engineering optimization problems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Engineering optimization problems; Evolutionary computation; Global optimization; Levy flight distribution; Metaheuristic; Wireless sensor networks	WHALE OPTIMIZATION; GENETIC ALGORITHMS; SEARCH; NETWORKS; PROTOCOL	In this paper, we propose a new metaheuristic algorithm based on Levy flight called Levy flight distribution (LFD) for solving real optimization problems. The LFD algorithm is inspired from the Levy flight random walk for exploring unknown large search spaces (e.g., wireless sensor networks (WSNs). To assess the performance of the LFD algorithm, various optimization test bed problems are considered, namely the congress on evolutionary computation (CEC) 2017 suite and three engineering optimization problems: tension/compression spring, the welded beam, and pressure vessel. The statistical simulation results revealed that the LFD algorithm provides better results with superior performance in most tests compared to several well-known metaheuristic algorithms such as simulated annealing (SA), differential evolution (DE), particle swarm optimization (PSO), elephant herding optimization (EHO), the genetic algorithm (GA), moth-flame optimization algorithm (MFO), whale optimization algorithm (WOA), grasshopper optimization algorithm (GOA), and Harris Hawks Optimization (HHO) algorithm. Furthermore, the performance of the LFD algorithm is tested on other different optimization problems of unknown large search spaces such as the area coverage problem in WSNs. The LFD algorithm shows high performance in providing a good deployment schema than energy-efficient connected dominating set (EECDS), A3, and CDS-Rule K topology construction algorithms for solving the area coverage problem in WSNs. Eventually, the LFD algorithm performs successfully achieving a high coverage rate up to 43.16 %, while the A3, EECDS, and CDS-Rule K algorithms achieve low coverage rates up to 40 % based on network sizes used in the simulation experiments. Also, the LFD algorithm succeeded in providing a better deployment schema than A3, EECDS, and CDS-Rule K algorithms and enhancing the detection capability of WSNs by minimizing the overlap between sensor nodes and maximizing the coverage rate. The source code is currently available for public from: https://www.mathworks.com/matlabcentral/fileexchange/76103-lfd.																	0952-1976	1873-6769				SEP	2020	94								103731	10.1016/j.engappai.2020.103731													
J								Fault diagnosis model based on Granular Computing and Echo State Network	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Granular Computing; Echo State Network; Bienenstock-Cooper-Munro rule; L-1/2-norm regularization; Fault diagnosis	L-1/2 REGULARIZATION; VARIABLE SELECTION; ROUGH SET; LASSO	In order to improve the efficiency and accuracy of electronic equipment fault diagnosis, a fault diagnosis model based on Granular Computing and Echo State Network (ESN) is proposed. Firstly, the attribute reduction of test index is carried out based on granular computing model. An attribute distinguishing ability index is defined based on attribute value influence degree. As the basis of similarity measure, a number of attribute granules of similar distinguish are obtained through affinity propagation clustering algorithm, then fault attribute reduction was completed by selecting clustering center attributes. In the stage of fault identification by ESN, in order to improve the dynamic adaptability of ESN reservoir to samples, Bienenstock-Cooper-Munro(BCM) rule is introduced into the reservoir construction to train the connection weight matrix. Meanwhile, the L(1/)2-norm penalty term is added to the objective function in order to improve the sparsification efficiency, and a smoothing L-1/2-norm regularization term is used to overcome the iterative numerical oscillation problem, the model is solved by using the half threshold iteration method at last. The effectiveness and superiority of the proposed method are verified by a fault diagnosis example of terminal guidance radar signal processing module.																	0952-1976	1873-6769				SEP	2020	94								103694	10.1016/j.engappai.2020.103694													
J								The heterogeneous vehicle routing problem with time windows and a limited number of resources	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Vehicle routing problem; VRP with time windows; Fixed heterogeneous fleet; Variable neighborhood descent; Analytical and numerical modeling; Performance analysis	SHUTTLE-BASED STORAGE; VARIABLE NEIGHBORHOOD SEARCH; TABU SEARCH; FLEET SIZE; ALGORITHM; MODEL; PERFORMANCE	This paper introduces the heterogeneous vehicle routing problem with time windows and a limited number of resources (HVRPTW-LR), a practical extension of the classical vehicle routing problem in which routes to be designed share common scarce resources. The HVRPTW-LR arises when a limited number of resources, such as vehicles, drivers, instruments, and so on, are available but are insufficient to serve all customers in a route planning. Therefore, the route design involves the selection of customers to be visited at each route and resources to be used. Applications to this problem are found in real services companies with high seasonal demand which attend to different types of works and have to decide on how to effectively manage their resources. For designing optimal routes, a hierarchical objective function is considered, maximizing the total number of served customers as the primary objective, and minimizing the travel costs as secondary. A mathematical model of linear programming is introduced to describe and understand all constraints clearly. The problem is first heuristically solved by a semi-parallel insertion heuristic. Then, solutions are improved by a hybrid variable neighborhood descent metaheuristic based on a Tabu Search algorithm for the exploration of the neighborhood and a holding list. Experiments are conducted on numerous sets of benchmark instances from the literature to evaluate the performance of the proposed algorithm. Results show that the algorithm proposed in this paper has a good performance and can be easily applied for solving numerous vehicle routing problem variants from the literature. A new set of benchmark cases for the HVRPTW-LR are also presented and solved.																	0952-1976	1873-6769				SEP	2020	94								103745	10.1016/j.engappai.2020.103745													
J								Backtracking search optimization algorithm-based least square support vector machine and its applications	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Backtracking search optimization algorithm; Least square support vector machine; Dynamic fluid level; Benchmark dataset; Classification; Prediction	PSO ALGORITHM; LSSVM; SELECTION; NETWORK; FUSION; MODEL	Based on statistical learning theory, least square support vector machine can effectively solve the learning problem of small samples. However, the parameters of the least square support vector machine model have a great influence on its performance. At the same time, there is no clear theoretical basis for how to choose these parameters. In order to cope with the parameters optimization of the least square support vector machine, a backtracking search optimization algorithm-based least square support vector machine model is proposed. In this model, backtracking search optimization algorithm is introduced to optimize the parameters of the least square support vector machine. Meanwhile, the least square support vector machine model is updated by the prediction error combined with the sliding window strategy to solve the problem of mis-match between the prediction model and the actual sample data in the time-varying system. The performance of the proposed model is verified by classification and regression problems. The classification performance of the model is verified by five Benchmark datasets, and the regression prediction performance is verified by the dynamic liquid level of the oil production process. Compared with genetic algorithm, particle swarm optimization algorithm, and improved free search algorithm optimized least square support vector machine, the simulation results show that the proposed model has higher classification accuracy with less computation time, and higher prediction accuracy and reliability for the dynamic liquid level. The proposed model is effective.																	0952-1976	1873-6769				SEP	2020	94								103801	10.1016/j.engappai.2020.103801													
J								Emotion recognition using speech and neural structured learning to facilitate edge intelligence	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Audio; Emotion; MFCC; LDA; NSL	FALL; CLASSIFICATION; FEATURES; FUSION; SYSTEM; SOUND	Emotions are quite important in our daily communications and recent years have witnessed a lot of research works to develop reliable emotion recognition systems based on various types data sources such as audio and video. Since there is no apparently visual information of human faces, emotion analysis based on only audio data is a very challenging task. In this work, a novel emotion recognition is proposed based on robust features and machine learning from audio speech. For a person independent emotion recognition system, audio data is used as input to the system from which, Mel Frequency Cepstrum Coefficients (MFCC) are calculated as features. The MFCC features are then followed by discriminant analysis to minimize the inner-class scatterings while maximizing the inter-class scatterings. The robust discriminant features are then applied with an efficient and fast deep learning approach Neural Structured Learning (NSL) for emotion training and recognition. The proposed approach of combining MFCC, discriminant analysis and NSL generated superior recognition rates compared to other traditional approaches such as MFCC-DBN, MFCC-CNN, and MFCC-RNN during the experiments on an emotion dataset of audio speeches. The system can be adopted in smart environments such as homes or clinics to provide affective healthcare. Since NSL is fast and easy to implement, it can be tried on edge devices with limited datasets collected from edge sensors. Hence, we can push the decision-making step towards where data resides rather than conventionally processing of data and making decisions from far away of the data sources. The proposed approach can be applied in different practical applications such as understanding peoples' emotions in their daily life and stress from the voice of the pilots or air traffic controllers in air traffic management systems.																	0952-1976	1873-6769				SEP	2020	94								103775	10.1016/j.engappai.2020.103775													
J								Robust master planning of a socially responsible supply chain under fuzzy-stochastic uncertainty (A case study of clothing industry)	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Physical supply channel plan; Integrated transportation and production; Mathematical optimization; Social sustainability; Textile industry; Two-stage stochastic programming	POSSIBILISTIC PROGRAMMING APPROACH; ORDER ALLOCATION; INTEGRATING PROCUREMENT; OPTIMIZATION MODEL; DISTRIBUTION PLANS; SELECTION; AGGREGATE; DESIGN; MANAGEMENT; CRITERIA	This paper addresses an integrated physical supply channel, aggregate production, and transportation planning problem through the lens of social sustainability under uncertainty. The three-echelon supply chain network includes multiple suppliers, a manufacturer and multiple distribution centres. Social-related criteria such as workers' job-security and health (i.e. working conditions) and social investment are contemplated in the supply chain under consideration. In doing so, a chance constraint mixed integer non-linear programming model is built to determine the centralized planning of procurement-production-transportation under fuzzy-stochastic uncertainty over the tactical planning horizon. Moreover, an efficient hybrid solution procedure is developed utilizing convexification and defuzzification strategies. Finally, a real-case study in clothing industry is presented to show the model applicability and the solution procedure efficiency.																	0952-1976	1873-6769				SEP	2020	94								103715	10.1016/j.engappai.2020.103715													
J								Multi-objective based deployment of throwboxes in Delay Tolerant Networks for the Internet of Things environment	EVOLUTIONARY INTELLIGENCE										Delay tolerant networks; Internet of things; Throwboxes; Deployment strategy; Multi-objective optimisation	OPTIMIZATION; ARCHITECTURE; ALGORITHM	Recent advances in Delay Tolerant Networks find its way into the Internet of Things leading to a new framework known as DTN-IoT. Delay Tolerant Network models can be effectively implemented within the Internet of Things framework to overcome intermittent connectivity problems. This could be made possible by deploying a few strategic nodes called throwboxes, which act as intermediate relay nodes and increase communication opportunities among the nodes. Each throwbox is assumed to have a pre-specified transmission range. Increase in the connection opportunities in a throwbox assisted DTN-IoT environment depends mainly on the optimal deployment of throwboxes. The objective of this paper is to identify optimal deployment locations for placement of throwboxes in a throwbox assisted DTN-IoT environment by (1) maximizing the coverage of all the throwboxes (2) minimizing the average delay and (3) maximizing the delivery ratio among all the nodes. We use an efficient Multi-Objective Differential Evolution and a popular Non-Dominated Sorting Genetic Algorithm-II for finding the optimal deployment location of throwboxes. The simulation results are compared to find a preferable strategy in throwbox deployment and enhance the performance of throwbox assisted DTN-IoT environment.																	1864-5909	1864-5917															10.1007/s12065-020-00474-w		SEP 2020											
J								Double-quantitative variable consistency dominance-based rough set approach	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Dominance-based rough set; Double quantification; Ordered information system; Quantitative consistency; Three-way decision	GROUP DECISION-MAKING; ATTRIBUTE REDUCTION; MODEL; APPROXIMATIONS; PRECISION	Rough set model with double quantification satisfies the requirement of quantitative information in practical applications, it has better fault tolerance than probabilistic rough set model considering only relative quantification and graded rough set model considering only absolute quantification. In this paper, two kinds of consistency levels are introduced from the perspective of double quantification in an ordered information system, namely relative quantitative consistency level and absolute quantitative consistency level. The single-quantitative variable consistency dominance-based rough set models based on these two kinds of quantitative consistency levels and their basic properties with the relevant three-way decision rules are discussed respectively in an ordered information system. Moreover, two kinds of double-quantitative variable consistency dominance-based rough set models and their basic properties with the relevant decision rules based on these two kinds of quantitative consistency levels are introduced. A consistency analysis of decision making in a practical case study is used to illustrate and interpret the double-quantitative variable consistency rough set models and the related decision rules in the ordered information system. The obvious shortcomings of dominance-based rough set approach (DRSA) without quantitative information are compared to explain the advantages of the quantitative variable consistency dominance-based rough sets with the two consistency levels in the practical case study. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						1	26		10.1016/j.ijar.2020.05.002													
J								Granulation in Rough Set Theory: A novel perspective	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Multigranularity; Rough sets; Incomplete information systems; Mixed information systems	STATISTICAL COMPARISONS; ATTRIBUTE REDUCTION; DECISIONS; CLASSIFICATION; CLASSIFIERS; SELECTION	Considering data from different perspectives (views or granulations) is very common in several applications nowadays. Unfortunately, Rough Sets lack of effective tools for handling multiple granulation in mixed and incomplete information systems. This paper introduces a novel approach for dealing with such information systems: The Parameterized Granulation. The demonstration of some of the properties of such granulation approach, as well as the theoretical and experimental analysis carried out, show the capabilities of this novel granulation, and result in great utility in the study of mixed and incomplete information systems, not only in the context of a single granulation, but in the context of multiple granulations. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						27	39		10.1016/j.ijar.2020.05.003													
J								Left and right distributivity between semi-uninorms and semi-S-uninorms	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Distributive equations; Semi-uninorms; S-uninorms; Semi-S-uninorms	OPERATORS; EQUATIONS; NORMS	This paper investigates the left and right distributivity between semi-uninorms and semi-S-uninorms without the commutativity and associativity. Firstly, we study the left and right distributivity for special disjunctive (resp. conjunctive) semi-uninorms in N-e(max) (resp. N-e(min)) over S-uninorms with the underlying uninorms in N-e'(min). Secondly, semi-S-uninorms are proposed by omitting the commutativity and associativity of S-uninorms. Thirdly, the left and right distributivity for semi-S-uninorms over semi-uninorms in N-e(min) (resp. N-e(max)) are studied, where the underlying semi-uninorms of semi-S-uninorms are semi-uninorms in N-e'(min). Especially, the distributivity between semi-uninorms and semi-S-uninorms has the distributivity between uninorms and S-uninorms as a special case. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						40	58		10.1016/j.ijar.2020.05.007													
J								On the use of group theory to generalize elements of pairwise comparisons matrix: A cautionary note	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Pairwise comparisons; Inconsistency; Approximate reasoning; Group theory	UNIFIED FRAMEWORK; CONSISTENCY; ALGORITHMS; RANKING	This paper examines the constricted use of group theory in the study of pairwise comparisons. The presented approach is based on the application of the celebrated Levi Theorems of 1942 and 1943 for orderable groups. The theoretical foundation for multiplicative (ratio) pairwise comparisons is provided. Counterexamples are provided to support the theory. In our opinion, the scientific community must be made aware of the limitations of using the group theory in pairwise comparisons. Groups, which are not torsion free, cannot be used for ratios by Levi's theorems. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						59	65		10.1016/j.ijar.2020.05.008													
J								Discriminative training of feed-forward and recurrent sum-product networks by extended Baum-Welch	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Sum-product network; Extended Baum-Welch; Discriminative learning		We present a discriminative learning algorithm for feed-forward Sum-Product Networks (SPNs) [42] and recurrent SPNs [31] based on the Extended Baum-Welch (EBW) algorithm [4]. We formulate the conditional data likelihood in the SPN framework as a rational function, and we use EBW to monotonically maximize it. We derive the algorithm for SPNs and RSPNs with both discrete and continuous variables. The experiments show that this algorithm performs better than both generative Expectation-Maximization, and discriminative gradient descent on a wide variety of applications. We also demonstrate the robustness of the algorithm in the case of missing features by comparing its performance to Support Vector Machines and Neural Networks. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						66	81		10.1016/j.ijar.2020.02.007													
J								New results of fuzzy implications satisfying I(x, I(y, z)) = I(I(x, y), I(x, z))	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Fuzzy implications; (S, N)-implications; (U, N)-implications; Generalized Frege's Law; Fuzzy negation; Fixed point	IMPLICATION OPERATORS; SYSTEM; FAMILY	Cruz et al. (2018) [10] investigated the fuzzy generalization of Frege's Law: x -> (y -> z) (x -> y) -> (x -> z), i.e., I(x, I(y, z)) = I(I(x, y), I(x, z)), which is called generalized Frege's Law. They showed conditions such that the generalized Frege's Law holds for (S, N)-implications (R-, QL-, D-, (T, N)-, H-, respectively). In this paper, firstly, a new necessary condition such that the generalized Frege's Law holds is given: N-I, the natural negation of I, is not continuous or has no fixed point. Based on this result, some propositions in [10] with contradictory assumptions are pointed out, and a correction is given. Secondly, new solutions of the equation I (x, I(y, z)) = I (I(x, y), I(x, z)) in (S, N)-implications are given. Finally, the necessary and sufficient conditions under which the generalized Frege's Law holds for the (U, N)-implications (f-,g-, T-Power based implications, respectively) are studied. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						82	102		10.1016/j.ijar.2020.03.011													
J								Dynamic Lukasiewicz Logic and Dynamic MV-algebras	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										MV-algebra; Dynamic logic; Lukasiewicz logic; Dynamic algebra		Following K. Segerberg [22], D. Kozen [15] and V. Pratt [19], who have been introduced dynamic propositional logic and dynamic algebras, dynamic propositional Lukasiewicz logic DPL (dynamic n-valued propositional Lukasiewicz logic DPLn) and dynamic MV-algebras (dynamic MVn-algebras) are introduced and theories of the logic DPL (DPLn) and dynamic MV-algebras (MVn-algebras) are developed. Dynamic MV-algebras (dynamic MVn-algebras) are algebraic counterparts of the logic D PL (D PLn), that in turn represent two-sorted algebras that combine the varieties of MV-algebras (MVn-algebras) (M, circle plus, circle dot, similar to, 0, 1) and regular algebras (R, boolean OR, ; , *) into a single finitely axiomatized variety (M, R, lozenge) resembling R-module with "scalar" multiplication lozenge. Kripke semantics is developed for dynamic propositional Lukasiewicz logic (dynamic n-valued propositional Lukasiewicz logic DPLn). (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						103	110		10.1016/j.ijar.2020.06.003													
J								Entropy and monotonicity in artificial intelligence	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Entropy; Monotonicity; Measure of fuzziness; Intuitionistic entropy measure; Divergence; Artificial intelligence	INTUITIONISTIC FUZZY-SETS; INFORMATION; UNCERTAINTY; DEFINITION; SELECTION	Entropies and measures of information are extensively used in several domains and applications in Artificial Intelligence. Among the original quantities from Information theory and Probability theory, a lot of extensions have been introduced to take into account fuzzy sets, intuitionistic fuzzy sets and other representation models of uncertainty and imprecision. In this paper, we propose a study of the common property of monotonicity of such measures with regard to a refinement of information, showing that the main differences between these quantities come from the diversity of orders defining such a refinement. Our aim is to propose a clarification of the concept of refinement of information and the underlying monotonicity, and to illustrate this paradigm by the utilisation of such measures in Artificial Intelligence. (C) 2020 Published by Elsevier Inc.																	0888-613X	1873-4731				SEP	2020	124						111	122		10.1016/j.ijar.2020.04.008													
J								Consistent projections and indicators in pairwise comparisons	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Pairwise comparisons; Consistent projections; Consistent weighted formulae; Discrete classical weights	INCONSISTENCY INDICATORS; APPROXIMATION; AXIOMATIZATION; DEFINITION; ALGORITHMS	This study examines several generic properties of weighted consistent projections and indicators of inconsistency in an arbitrary finite dimensional inner product space of square matrices. In the case of weighted Frobenius inner products we present explicit formulae for them in terms of the matrix entries and weights. It extends the recent results, due to Koczkodaj et al. [Fund. Inform. 172 (2020) 379-397], to positive matrices in pairwise comparisons. Moreover, we discuss the possibilities of a proper choice of the inner product weights for numerical applications based on the orthogonal consistent projections. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						123	132		10.1016/j.ijar.2020.06.001													
J								On the approximation of a membership function by empirical quantile functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Possibility distribution; Fuzzy intervals; Quantiles; Average Cumulative Function (ACF)	FUZZY-SETS; FAMILY	The Average Cumulative representation of fuzzy intervals is connected with the possibility theory in the sense that the possibility and necessity functions are substituted by a pair of non decreasing functions defined as the positive and negative variations in the Jordan decomposition of a membership function. In this paper we motivate the crucial role of ACF in determining the membership function from experimental data; some examples and simulations are shown to state the robustness of the proposed construction. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						133	146		10.1016/j.ijar.2020.06.012													
J								Three-way decision models based on multigranulation support intuitionistic fuzzy rough sets	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Multigranulation rough sets; Support intuitionistic fuzzy sets; Support degree; Three-way decisions	ATTRIBUTE REDUCTION; SELECTION; INFORMATION; APPROXIMATIONS	To capture the influence of various uncertain factors during delayed decision-making, support intuitionistic fuzzy sets (SIFSs) are introduced for three-way decisions (TWDs) to study this topic from the perspective of multigranulation. First, the concepts of support intuitionistic fuzzy rough sets are defined, and their related properties are discussed. Then, we combine support intuitionistic fuzzy rough sets with multigranulation rough sets (MRSs), present optimistic/pessimistic multigranulation support intuitionistic fuzzy rough set models, and discuss their corresponding properties. Second, a parameter alpha is introduced to constrain the disjunction and conjunction of multiple support intuitionistic fuzzy relations, and variable optimistic and pessimistic multigranulation support intuitionistic fuzzy rough set models are constructed. Third, we define the similarity measure, positive ideal solution, negative ideal solution, and conditional probability based on multigranulation support intuitionistic fuzzy rough sets. Four kinds of TWD models based on four proposed multigranulation support intuitionistic fuzzy rough set models are established. Finally, decision rules can be obtained from a new score function and accuracy function, and the decision rule extraction algorithm based on multigranulation support intuitionistic fuzzy rough sets is designed. Experimental results on a series of examples demonstrate the effectiveness of our proposed TWD models. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						147	172		10.1016/j.ijar.2020.06.004													
J								Type-2 fuzzy multigranulation rough sets	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Rough sets; Multigranulation rough sets; Type-2 fuzzy sets; Rough measures	2 UNIVERSES; GRANULARITY; REDUCTION; MODEL; DECISIONS; SYSTEMS	Aiming at expanding the application range of the multigranulation rough set (MGRS) theory, a type-2 fuzzy multigranulation rough set (T2FMGRS) model is proposed by combining type-2 fuzzy sets with multigranulation rough sets (MGRSs) in this paper. At first, definitions and properties of optimistic and pessimistic type-2 fuzzy multigranulation rough sets (T2FMGRSs) are introduced. Then, the rough measure is proposed to measure the uncertainty of T2FMGRSs. Finally, T2FMGRSs over two universes are put forward and some examples of decision-making are given to illustrate the applicability of the newly proposed model. In summary, the establishment of T2FMGRSs is a meaningful generalization of the MGRS theory from both theoretical and practical aspects. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				SEP	2020	124						173	193		10.1016/j.ijar.2020.06.007													
J								An interval-valued utility theory for decision making with Dempster-Shafer belief functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Dempster-Shafer theory of evidence; Von Neumann-Morgenstern's utility theory; Interval-valued utility function; Jaffray's linear utility theory; Smets' two-level decision theory; Shafer's constructive decision theory	EXPECTED UTILITY; AMBIGUITY; IGNORANCE; RULE	The main goal of this paper is to describe an axiomatic utility theory for Dempster-Shafer belief function lotteries. The axiomatic framework used is analogous to von Neumann-Morgenstern's utility theory for probabilistic lotteries as described by Luce and Raiffa. Unlike the probabilistic case, our axiomatic framework leads to interval-valued utilities, and therefore, to a partial (incomplete) preference order on the set of all belief function lotteries. If the belief function reference lotteries we use are Bayesian belief functions, then our representation theorem coincides with Jaffray's representation theorem for his linear utility theory for belief functions. We illustrate our representation theorem using some examples discussed in the literature, and we propose a simple model for assessing utilities based on an interval-valued pessimism index representing a decision-maker's attitude to ambiguity and indeterminacy. Finally, we compare our decision theory with those proposed by Jaffray, Smets, Dubois et al., Giang and Shenoy, and Shafer. Published by Elsevier Inc.																	0888-613X	1873-4731				SEP	2020	124						194	216		10.1016/j.ijar.2020.06.008													
J								Innovation and Challenges of Blockchain in Banking: A Scientometric View	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Digital Economy; Blockchain; Banking; Scientometrics; Literature Review	BITCOIN; TECHNOLOGY; EVOLUTION; SYSTEMS; ISSUES	Blockchain has been gaining focus in research and development for diverse industries in recent years. Nevertheless, innovations that impact to the banking nurture a potential for disruptive impact globally for economic reasons; however it has received less scholarly attention. Hence the effect of blockchain technologies on banking industry is systematically reviewed. The relevant literature is extracted from Scopus, Web of Science and bibliometric techniques are applied. While a bulk of earlier papers focuses only on bit coins, a broader framework is envisaged that synthesizes interdisciplinary thematic areas for advancement; hence novelty in current work. A few practical and theoretical implications for stakeholders in view of technology, law and management are discussed.																	1989-1660					SEP	2020	6	3			SI		7	14		10.9781/ijimai.2020.03.004													
J								Blockchain for Healthcare: Securing Patient Data and Enabling Trusted Artificial Intelligence	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Blockchain; Healthcare; Data Privacy And Security; Trusted Artificial Intelligence; Interoperability; Patient-Centric Data Management; Provenance; Traceability		Advances in information technology are digitizing the healthcare domain with the aim of improved medical services, diagnostics, continuous monitoring using wearables, etc., at reduced costs. This digitization improves the ease of computation, storage and access of medical records which enables better treatment experiences for patients. However, it comes with a risk of cyber attacks and security and privacy concerns on this digital data. In this work, we propose a Blockchain based solution for healthcare records to address the security and privacy concerns which are currently not present in existing e-Health systems. This work also explores the potential of building trusted Arti.cial Intelligence models over Blockchain in e-Health, where a transparent platform for consent-based data sharing is designed. Provenance of the consent of individuals and traceability of data sources used for building and training the AI model is captured in an immutable distributed data store. The audit trail of the data access captured using Blockchain provides the data owner to understand the exposure of the data. It also helps the user to understand the revenue models that could be built on top of this framework forcommercial data sharing to build trusted AI models.																	1989-1660					SEP	2020	6	3			SI		15	23		10.9781/ijimai.2020.07.002													
J								Blockverse: A Cloud Blockchain-based Platform for Tracking in Affiliate Systems	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Blockchain; Cloud Computing; Databases; Affiliate Systems; Analytics		Affiliate systems are a crucial piece of today's online advertising. In affiliate systems, web traffic is directed from certain sites displaying ads to the websites of those company whose products or services are advertised. The way in which these ads are monetized is diverse and can respond to different models. In many cases, affiliates establish a cost based on impressions (displays of the ad) or on clicks. However, more intricate models are becoming widespread, such as the cost per action, where the affiliate incomes are due to the users performing certain actions in the target website. In particular, in the world of iGaming, it is frequent that affiliates charges are based on registrations, deposits or money lost on bets. In this scenario, Blockverse is a tool whose objective is to record transactions occurring in affiliate systems at large scale, using a permissioned blockchain implemented atop state-of-the-art cloud technology. Additionally, the system will be able to execute smart deals that generate income for affiliates based on the agreed conditions, and to provide real-time analytics in the context of the affiliate system.																	1989-1660					SEP	2020	6	3			SI		24	31		10.9781/ijimai.2020.06.001													
J								Efficient Method Based on Blockchain Ensuring Data Integrity Auditing with Deduplication in Cloud	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Auditing; Blockchain; Cloud Computing; Deduplication; Integrity		With the rapid development of cloud storage, more and more cloud clients can store and access their data anytime, from anywhere and using any device. Data deduplication may be considered an excellent choice to ensure data storage efficiency. Although cloud technology offers many advantages for storage service, it also introduces security challenges, especially with regards to data integrity, which is one of the most critical elements in any system. A data owner should thus enable data integrity auditing mechanisms. Much research has recently been undertaken to deal with these issues. In this paper, we propose a novel blockchain-based method, which can preserve cloud data integrity checking with data deduplication. In our method, a mediator performs data deduplication on the client side, which permits a reduction in the amount of outsourced data and a decrease in the computation time and the bandwidth used between the enterprise and the cloud service provider. This method supports private and public auditability. Our method also ensures the confidentiality of a client's data against auditors during the auditing process.																	1989-1660					SEP	2020	6	3			SI		32	38		10.9781/ijimai.2020.08.001													
J								Tracking News Stories Using Blockchain to Guarantee their Traceability and Information Analysis	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Blockchain; Smart Contract; Traceability; News Stories; Journalistic; Transparency	TRANSPARENCY	Nowadays, having a mechanism to guarantee the traceability of the information and to monitor the evolution of the news from its origin, and having elements to know the reputation and credibility of the media, analyze the news as well as its evolution and possible manipulation, etc. is becoming increasingly significant. Transparency in journalism is currently a key element in performing serious and rigorous journalism. End-users and fact-checking agencies need to be able to check and verify the information published in different media. This transparency principle enables the tracking of news stories and allows direct access to the source of essential content to contrast the information it contains and to know whether it has been manipulated. Additionally, the traceability of news constitutes another instrument in the fight against the lack of credibility, the manipulation of information, misinformation campaigns and the propagation of fake news. This article aims to show how to use Blockchain to facilitate the tracking and traceability of news so that it can provide support to the automatic indexing and extraction of relevant information from newspaper articles to facilitate the monitoring of the news story and allows users to verify the veracity of what they are reading.																	1989-1660					SEP	2020	6	3			SI		39	46		10.9781/ijimai.2020.06.003													
J								Traceable Ecosystem and Strategic Framework for the Creation of an Integrated Pest Management System for Intensive Farming	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Integrated Pest Management; IPM; Computer Vision; Machine Learning; Artificial Intelligence; Blockchain; Intensive Farming; Greenhouses	HERBICIDE-TOLERANT CROPS; INSECT PEST; GREENHOUSE; PESTICIDES; WHITEFLIES; HEMIPTERA; CANCER	The appearance of pests is one of the major problems that exist in the growth of crops, as they can damage the production if the appropriate measures are not taken. Within the framework of the Integrated Pest Management strategy (IPM), early detection of pests is an essential step in order to provide the most appropriate treatment and avoid losses. This paper proposes the architecture of a system intensive farming in greenhouses featuring the ability to detect environmental variations that may favour the appearance of pests. This system can suggest a plan or treatment that will help mitigate the effects that the identified pest would produce otherwise. Furthermore, the system will learn from the actions carried out by the humans throughout the different stages of crop growing and will add it as knowledge for the prediction of future actions. The data collected from sensors, through computer vision, or the experiences provided by the experts, along with the historical data related to the crop, will allow for the development of a model that contrasts the predictions of the actions that could be implemented with those already performed by technicians. Within the technological ecosystems in which the Integrated Pest Management systems develop their action, traceability models must be incorporated. This will guarantee that the data used for the exploitation of the information and, therefore for the parameterization of the predictive models, are adequate. Thus, the integration of blockchain technologies is considered key to provide them with security and confidence.																	1989-1660					SEP	2020	6	3			SI		47	54		10.9781/ijimai.2020.08.004													
J								Intelligent Detection and Recovery from Cyberattacks for Small and Medium-Sized Enterprises	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Attack Detection; Attack Recovery; Blockchain; Cybersecurity; Machine Learning; SME	TAXONOMY; ATTACKS; TROJANS	Cyberattacks threaten continuously computer security in companies. These attacks evolve everyday, being more and more sophisticated and robust. In addition, they take advantage of security breaches in organizations and companies, both public and private. Small and Medium-sized Enterprises (SME), due to their structure and economic characteristics, are particularly damaged when a cyberattack takes place. Although organizations and companies put lots of efforts in implementing security solutions, they are not always effective. This is specially relevant for SMEs, which do not have enough economic resources to introduce such solutions. Thus, there is a need of providing SMEs with affordable, intelligent security systems with the ability of detecting and recovering from the most detrimental attacks. In this paper, we propose an intelligent cybersecurity platform, which has been designed with the objective of helping SMEs to make their systems and network more secure. The aim of this platform is to provide a solution optimizing detection and recovery from attacks. To do this, we propose the application of proactive security techniques in combination with both Machine Learning (ML) and blockchain. Our proposal is enclosed in the IASEC project, which allows providing security in each of the phases of an attack. Like this, we help SMEs in prevention, avoiding systems and network from being attacked; detection, identifying when there is something potentially harmful for the systems; containment, trying to stop the effects of an attack; and response, helping to recover the systems to a normal state.																	1989-1660					SEP	2020	6	3			SI		55	62		10.9781/ijimai.2020.08.003													
J								Smart Contracts with Blockchain in the Public Sector	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Blockchain; Smart Contract; Public Sector; Transparency; Corruption	SYSTEM; CHALLENGES; INTERNET; THINGS	The appearance of so-called block chains or Blockchain with the promise of transforming trust and the way value is exchanged, joins the expansion of the technological capabilities of organizations to achieve higher levels of productivity and innovation. This is how Blockchain-based techniques are being applied to many fields, focusing in this article on the public sector, as a possible solution to the demands for transparency, participation and citizen cooperation that society demands; due to the possibility of disintermediation based on automated transactions and on the responsibility and security in the management of official blockchain records. This could obstruct corruption and make government services more transparent and efficient. Although, it investigates about applications in the public sector under the Blockchain system, such as transactions, agreements, property registries and innovations, developments and other assets; Special emphasis is placed on the possibility of implementing Smart Contracts (mechanisms that aim to eliminate intermediaries to simplify processes) in public procurement procedures, given that it is in this type of activity where high levels of corruption are generated. It is concluded then that Europe has the largest number of blockchain initiatives worldwide, while Latin America, except for the case of Peru, lacks this type of applications, being this continent exactly where there are the countries with the highest levels of corruption. It concludes with a recommendation to use blockchain along with smart contracts through platforms such as Ethereum or Lisk, mainly given its flexibility and current development on topics with similar functionalities.																	1989-1660					SEP	2020	6	3			SI		63	72		10.9781/ijimai.2020.07.005													
J								Blockchain-Enabled Platforms: Challenges and Recommendations	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Adoption; Blockchain; Collaborative Networks Disruption; Value Proposition	INNOVATION; NETWORK; BITCOIN; COMPETITION; TECHNOLOGY; EVOLUTION	Not even a tenth of blockchain-enabled platforms survive their first anniversary. The volatility of cryptomarkets has brought negative attention and led some to question the applicability of blockchain technology. This paper argues that the challenges for startups and incumbents behind these platforms are numerous, and that the speculative bubble around cryptocurrencies is only one of them. Blockchain still needs to demonstrate fully its disruptive potential and so far, entrepreneurs have not managed to significantly impact incumbents' market shares. This transitory period requires incumbents to let go of traditional control mech-anisms, and startups to scale down their global decentralised hopes. Indeed, whilst the technology can indeed scale fast, starting in a controlled market and managing growth is a counterintuitive but essential strategy for blockchain-enabled platforms to implement. Given the diverging nature of the technology, at present at least, the combined shortage of skills in blockchain and security, and the trust blockchain is built on, rushing to the global market is high risk. Nonetheless, given the potential returns, the risk appetite is high and both entrepreneurs and corporate executives share unrealistic expectations about a technology they cannot fully understand since it has not yet converged. In light of the above, this article identifies the main challenges faced when building blockchainenabled platforms and provides recommendations for startups and incumbents to overcome these. In order to reach these conclusions, the information obtained from twenty semi-structured interviews with leading actors in the field has been fundamental.																	1989-1660					SEP	2020	6	3			SI		73	89		10.9781/ijimai.2020.08.005													
J								A novel reversible ternary coded decimal adder/subtractor	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Quantum realization; Reversible circuit; Ternary coded decimal (TCD); Full-adder; Ternary coded decimal detector	PARALLEL ADDER/SUBTRACTOR; DESIGN	Reversible ternary logic is a promising new research for the future of quantum computing, which has several advantages over the binary ones. In this paper, an effective design of reversible ternary coded decimal (TCD) adder/subtractor is proposed. For this purpose, at first, we propose a new reversible ternary full-adder, called comprehensive reversible ternary full-adder, using the ternary logic capabilities that can sum four ternary values and produce two ternary outputs. Moreover, we implement a 3-qutrit ripple carry adder (RCA). Then, we propose a quantum realization of TCD error detector circuit. Next, a novel quantum reversible TCD adder and a novel quantum reversible TCD subtractor are designed and implemented using the proposed 3-qutrit RCA and the proposed TCD error detector. Finally, by merging these two circuits, we propose an effective quantum realization of reversible TCD adder/subtractor. The results of evaluations show that the proposed circuits are superior or similar to related counterpart works in terms of constant input, garbage outputs, hardware complexity and quantum cost criteria.																	1868-5137	1868-5145															10.1007/s12652-020-02499-6		SEP 2020											
J								MRI brain tumor detection using optimal possibilistic fuzzy C-means clustering algorithm and adaptive k-nearest neighbor classifier	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Median filter; AKNN; OPFCM		Brain tumor characterizes the aggregation of abnormal cells in specific tissues of the brain zone. The prior distinguishing proof of brain tumors has a huge influence on the treatment and recovery of the patient. The identification of a brain tumor and its evaluation is commonly a troublesome and tedious assignment. For effective classification and grading of brain tumor images, in this paper, we present an automatic MRI brain tumor classification system. The proposed work consists of four modules namely, pre-processing, feature extraction, classification, and segmentation. Initially, the noise present in the input image is removed using the Median Filter because the noises present in the input images will affect the accuracy of the classification process. At once, the images are converted into 3 x 3 blocks. Then, the texture features are extracted from the pre-processed image. After the feature extraction process, the features are given to the adaptive k-nearest neighbor classifier to classify an image as normal or abnormal. Later, the tumor regions are segmented with the help of the optimal possibilistic fuzzy C-means clustering algorithm. Both classification and the segmentation appearance technique are evaluated in terms of accuracy; sensitivity as well as specificity. For experimental analysis two dataset are utilized namely, BRATS MICCAI brain tumor dataset and publically available dataset.																	1868-5137	1868-5145															10.1007/s12652-020-02444-7		SEP 2020											
J								CL-IoT: cross-layer Internet of Things protocol for intelligent manufacturing of smart farming	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cross-layer; Clustering; Intelligent manufacturing; Nature-inspired algorithm; Smart farming; Internet of Things	WIRELESS SENSOR NETWORKS; EFFICIENT; AGRICULTURE; ALGORITHM	Internet of Things (IoT) for Intelligent Manufacturing of Smart Farming gained significant attention from researchers to automate various farming applications called Smart Farming (SF). The sensors and actuators deployed across the farm using which farmers receive periodic farm information related to temperature, soil moisture, light intensity, and water used, etc. The clustering-based methods are proven energy-efficient solutions for Wireless Sensor Networks (WSNs). However, by considering long-distance communications and scalable networks of IoT enabled SF; the present clustering solutions cannot be feasible and having higher delay and latency for various SF applications. To focus on requirements SF applications, an efficient and scalable protocol for remote monitoring and decision making of farms in rural regions called CL-IoT protocol proposed. A cross-layer-based clustering and routing algorithms have designed to reduce network communication delay, latency, and energy consumption. The cross-layer-based optimal Cluster Head (CH) selection solution proposed to overcome the energy asymmetry problem in WSN. The parameters of different layers like a physical, medium access control (MAC), and network layer of each sensor used to evaluate and select optimal CH and efficient data transmission. The nature-inspired algorithm proposed with a novel probabilistic decision rule functions as a fitness function to discover the optimal route for data transmission. The performance of the CL-IoT protocol analyzed using NS2 by considering the energy-efficiency, computational-efficiency, and QoS-efficiency factors. Compared to state-of-art IoT-based farming methods, the CL-IoT reduces energy consumption, communication overhead, and end-to-end delay up to a certain extent and maximizes the network throughput.																	1868-5137	1868-5145															10.1007/s12652-020-02502-0		SEP 2020											
J								Nonlinear Output Feedback for HL-20 Flight Control Using Back-Stepping Observer	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Output feedback; Back-stepping observer; Observer-based controller; Free gyroscopes; Trajectory initialization; HL-20 flight vehicle	EXTENDED-STATE-OBSERVER; DISTURBANCE-OBSERVER; ROBUST-CONTROL; MOTION CONTROL; SLIDING MODE; DESIGN; SYSTEMS	This paper investigates the problem of an observer-based controller for the flight control of the HL-20 flight vehicle in the low altitude flight stage. The difficulty arises when the coupling of the control channels effects on each other. Moreover, due to limitations of rate gyroscope drift, free gyroscopes with high-precision Euler angles are considered. Consequently, for a control purpose, output feedback is used and by considering the coupling effects as disturbance, an observer is designed using the back-stepping method to estimate angular rates. These estimations are used in the controller design and with the Lyapunov function and trajectory initialization, globally uniformly bounded is achieved for the observer-based controlled system. The validity and effectiveness of the designed controller for the flight controller are shown by simulation results.																	0921-0296	1573-0409															10.1007/s10846-020-01251-8		SEP 2020											
J								A Multimodal Path Planning Approach to Human Robot Interaction Based on Integrating Action Modeling	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot navigation; Human-robot interaction; Action modeling; Multimodal path planning		To complete a task consisting of a series of actions that involve human-robot interaction, it is necessary to plan a motion that considers each action individually as well as in relation to the following action. We then focus on the specific action of "approaching a group of people" in order to accurately obtain human data that is used to make the performance of tasks involving interactions with multiple people more smooth. The movement depends on the characteristics of the important sensors used for the task and on the placement of people at and around the destination. Considering the multiple tasks and placement of people, the pre-calculation of the destinations and paths is difficult. This paper thus presents a system of navigation that can accurately obtain human data based on sensor characteristics, task content, and real-time sensor data for processes involving human-robot interaction (HRI); this method does not navigate specifically toward a previously determined static point. Our goal was achieved by using a multimodal path planning based on integration of action modeling by considering both voice and image sensing of interacting people as well as obstacle avoidance. We experimentally verified our method by using a robot in a coffee shop environment.																	0921-0296	1573-0409															10.1007/s10846-020-01244-7		SEP 2020											
J								Embedding Values in Artificial Intelligence (AI) Systems	MINDS AND MACHINES										Artificial intelligence; Values; Ethics; Sociotechnical system; Value embedding; Institution; Artificial agent; Norms; Multi-agent system	SOCIOTECHNICAL SYSTEMS; DUAL NATURE; ARTIFACTS; DESIGN	Organizations such as the EU High-Level Expert Group on AI and the IEEE have recently formulated ethical principles and (moral) values that should be adhered to in the design and deployment of artificial intelligence (AI). These include respect for autonomy, non-maleficence, fairness, transparency, explainability, and accountability. But how can we ensure and verify that an AI system actually respects these values? To help answer this question, I propose an account for determining when an AI system can be said to embody certain values. This account understands embodied values as the result of design activities intended to embed those values in such systems. AI systems are here understood as a special kind of sociotechnical system that, like traditional sociotechnical systems, are composed of technical artifacts, human agents, and institutions but-in addition-contain artificial agents and certain technical norms that regulate interactions between artificial agents and other elements of the system. The specific challenges and opportunities of embedding values in AI systems are discussed, and some lessons for better embedding values in AI systems are drawn.																	0924-6495	1572-8641															10.1007/s11023-020-09537-4		SEP 2020											
J								A local-to-global scheme-based multi-objective evolutionary algorithm for overlapping community detection on large-scale complex networks	NEURAL COMPUTING & APPLICATIONS										Multi-objective optimization; Evolutionary algorithm; Overlapping community detection; Large-scale complex network	GENETIC ALGORITHM	Recently, multi-objective evolutionary algorithms (MOEAs) have been shown promising performance for detecting overlapping community structure in complex networks. However, it is still challenging to design MOEAs for overlapping community detection on large-scale complex networks due to the curse of dimensionality. Along this avenue, this paper proposes a local-to-global scheme-based MOEA named LG-MOEA for overlapping community detection on large-scale complex networks, which mainly consists of two stages: a local community structure detection stage and a global community structure determination stage. To be specific, in the local community structure detection stage, the key nodes that are central to community and essential to the connectedness of community are firstly identified. Then for each key node, an MOEA with the proposed community boundary control strategy is suggested to detect a set of local overlapping communities through local expansion around the key node. In the global community structure determination stage, a single objective evolutionary algorithm is adopted to search for a suitable local overlapping community for each key node and combine them as one global community partition of the whole network. The proposed LG-MOEA is compared with several competitive overlapping community detection algorithms on both real-world small-scale and large-scale networks, and the experimental results show its superiority for overlapping community detection in terms of the generalized normalized mutual information gNMI and the extended modularity Q(ov), especially has competitive superiority for large-scale complex networks.																	0941-0643	1433-3058															10.1007/s00521-020-05311-w		SEP 2020											
J								HINDIA: a deep-learning-based model for spell-checking of Hindi language	NEURAL COMPUTING & APPLICATIONS										Spelling; Spell-checker; Deep-learning; Long short-term memory; Encoder-decoder recurrent neural network	ACTIVITY RECOGNITION; SENSORS	The spelling error is a mistake occurred while typing the text document. The applications like search engines, information retrieval, emails, etc., require user typing. In such applications, good spell-checker is essential to rectify the misspelling. Spell-checkers for western languages like English are very powerful and can handle any type of spelling errors, whereas in the case of Indian languages like Hindi, Urdu, Bengali, Kannada, Assamese, etc., the available spell-checkers are very basic ones. These spell-checkers are developed using traditional methods like statistical methods and rule-based methods. This article presents a novel model HINDIA to handle the spelling errors of the Hindi language, one of the most spoken languages in India. It utilizes a deep-learning method for spelling error detection and correction. The proposed spell-checking model works in two phases. In the first phase model identifies the erroneous words in the input sample and in the second phase it replaces the wrong words with the most probable correct words. Model HINDIA is developed using the attention-based encoder-decoder bidirectional recurrent neural network (BiRNN) which uses long short-term memory cells. Several modifications in the BiRNN have been made and network is fine-tuned to process the spelling errors of Hindi language. It uses publicly available dataset 'monolingual corpus' developed by IIT Mumbai for training and testing. The performance of the proposed model is evaluated in two scenarios. In the first scenario where the testing dataset is generated using split function. HINDIA performs significantly well with precision 0.86, recall 0.72,f-measure 0.78 and accuracy 0.80. Further, in the second scenario, where a dataset is manually generated its performance is fairly good with precision 0.81, recall 0.72,f-measure 0.76 and accuracy 0.74. Model HINDIA gives better performance than the deep-learning-based Malayalam spell-checker and some other deep-learning-based correction models present in the literature.																	0941-0643	1433-3058															10.1007/s00521-020-05207-9		SEP 2020											
J								Two-branch encoding and iterative attention decoding network for semantic segmentation	NEURAL COMPUTING & APPLICATIONS										Semantic segmentation; Two-branch encoding; Improved PeleeNet; Iterative attention decoding; Channel position attention; Boundary residual attention		Deep convolutional neural networks(DCNNs) have shown outstanding performance in semantic image segmentation. In this paper, we propose a two-branch encoding and iterative attention decoding semantic segmentation model. In encoding stage, an improved PeleeNet is used as the backbone branch to extract dense image features, and the spatial branch is used to preserve fine-grained information. In decoding stage, the iterative attention decoding is employed to optimize the segmentation results with multi-scale features. Furthermore, we propose a channel position attention module and a boundary residual attention module to learn different position and boundary features, which can enrich the target boundary position information. Finally, we use SegNet as the basic network and conduct some experiments to evaluate the effect of each component in the proposed model with accuracy and mIOU on CamVid dataset. Furthermore, we verify the segmentation performance of the proposed model with comparable experiments on CamVid, Cityscapes and PASCAL VOC 2012 dataset. In particular, the model has achieved 91.7% segmentation accuracy and 67.1% mIOU on the CamVid dataset respectively, which verify the effectiveness of our proposed model. In the future, we can combine target detection with semantic segmentation to further improve the semantic segmentation effect of small objects. We also hope to further optimize the model structure and reduce its time complexities and parameters under the guarantee of effectiveness.																	0941-0643	1433-3058															10.1007/s00521-020-05312-9		SEP 2020											
J								Automated glaucoma detection using GIST and pyramid histogram of oriented gradients (PHOG) descriptors	PATTERN RECOGNITION LETTERS										Glaucoma classification; GIST features; PHOG features; Automated disease diagnosis	RETINAL FUNDUS IMAGES; OPTIC DISC; CUP; SEGMENTATION; LOCALIZATION; EXTRACTION; DIAGNOSIS; SYSTEM	Effective diagnosis of glaucoma mainly relies on the analysis of optic disc characteristics of retina. Glaucoma is considered as second leading cause of blindness and its early detection prevents patients from temporary or permanent blindness. It effects the intensity and shape near optic disc of the retina. Fundus photography has revolutionized the field of ophthalmology and helped in visualizing the structure of optic disc. The proposed work aims to develop an automated diagnostic system based on fundus images for glaucoma disease. It focuses on extraction of GIST and pyramid histogram of oriented gradients (PHOG) features from preprocessed fundus images. The extracted features are ranked and selected through principal component analysis (PCA) to choose significant features. The classification into glaucomatous images is done with SVM classifier on fundus images of Drishti-GS1 and HRF databases. The results obtained from the proposed method are compared with recent glaucoma detection techniques in the literature, including deep learning methodologies, on the basis of accuracy and AUC parameters. The performance of the system is also validated by glaucoma expert from Sharp Sight Group of Eye Hospitals, Delhi-NCR, India. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		3	11		10.1016/j.patrec.2019.04.004													
J								Robust contactless pulse transit time estimation based on signal quality metric	PATTERN RECOGNITION LETTERS										Pulse transit time; Signal quality metric; Remote photoplethysmograpy; Physiological signal processing	BLOOD-PRESSURE	The pulse transit time (PIT) can provide valuable insight into cardiovascular health, specifically regarding arterial stiffness and blood pressure. Traditionally, PTT is derived by calculating the time difference between two photoplethysmography (PPG) measurements, which require a set of body-worn sensors attached to the skin. Recently, remote photoplethysmography (rPPG) has been proposed as a contactless monitoring alternative. The main problem with rPPG based PIT estimation is that motion artifacts affect the shape of waveform leading to the shift or over-detected peaks, which decreases the accuracy of PIT. To overcome this problem, this paper presents a robust pulse-by-pulse PIT estimation framework using a signal quality metric. By exploiting the local temporal information and global periodic characteristics, the metric automatically assesses pulse quality of signal on a pulse-by-pulse basis, and calculates the probabilities of the pulse peak being the actual peak. Furthermore, in order to cope with over-detected and shift pulse peaks, Kalman filter complemented by the proposed signal quality metric is used to adaptively adjust the peaks based on the estimated probability. All the refined peaks are finally used for pulse-by-pulse PIT estimation. The experiment results are promising, suggesting that the proposed framework provides a robust and more accurate PIT estimation in real applications. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		12	16		10.1016/j.patrec.2019.06.016													
J								A projective chirp based stair representation and detection from monocular images and its application for the visually impaired	PATTERN RECOGNITION LETTERS										Color-based stair detection; Chirp pattern; Stair modeling	TO-LINE MAPPINGS; VISION	The most prominent characteristic of a stair is that it has rigid form with periodic pattern of its steps. In this work, we exploit this periodic characteristic in view of geometrical rules. As a stair consists of equidistant nosing lines, under a perspective projection of camera, the projection of these lines on an image follows a projective chirplet transform. We propose to detect a stair by finding a group of lines that best satisfies a projective chirp model. The most advantage of the proposed techniques is that some missed noising lines and thus whole stair could be recovered. We validate the proposed method on both artificial and real datasets. The experimental results show a higher detection rate on different datasets. Finally, a real application alarming the visually impaired about stairs in indoor environments has been conducted and obtained 88.37% of accuracy. The implementations and datasets are made publicly available. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		17	26		10.1016/j.patrec.2019.03.007													
J								Enhancing perception for the visually impaired with deep learning techniques and low-cost wearable sensors	PATTERN RECOGNITION LETTERS										Visual impaired assistant; Deep learning; Outdoors; Depth from monocular frames		As estimated by the World Health Organization, there are millions of people who lives with some form of vision impairment. As a consequence, some of them present mobility problems in outdoor environments. With the aim of helping them, we propose in this work a system which is capable of delivering the position of potential obstacles in outdoor scenarios. Our approach is based on non-intrusive wearable devices and focuses also on being low-cost. First, a depth map of the scene is estimated from a color image, which provides 3D information of the environment. Then, an urban object detector is in charge of detecting the semantics of the objects in the scene. Finally, the three-dimensional and semantic data is summarized in a simpler representation of the potential obstacles the users have in front of them. This information is transmitted to the user through spoken or haptic feedback. Our system is able to run at about 3.8 fps and achieved a 87.99% mean accuracy in obstacle presence detection. Finally, we deployed our system in a pilot test which involved an actual person with vision impairment, who validated the effectiveness of our proposal for improving its navigation capabilities in outdoors. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		27	36		10.1016/j.patrec.2019.03.008													
J								Wearable assistive devices for visually impaired: A state of the art survey	PATTERN RECOGNITION LETTERS										Assistive devices survey; Visually impaired people; Sensorial networks ETAs; Video camera based ETAs	BLIND; NAVIGATION; SYSTEM; ROBUST; AID; TECHNOLOGIES; RECOGNITION; MOBILITY; PEOPLE	Recent statistics of the World Health Organization (WHO), published in October 2017, estimate that more than 253 million people worldwide suffer from visual impairment (VI) with 36 million of blinds and 217 million people with low vision. In the last decade, there was a tremendous amount of work in developing wearable assistive devices dedicated to the visually impaired people, aiming at increasing the user cognition when navigating in known/unknown, indoor/outdoor environments, and designed to improve the VI quality of life. This paper presents a survey of wearable/assistive devices and provides a critical presentation of each system, while emphasizing related strengths and limitations. The paper is designed to inform the research community and the VI people about the capabilities of existing systems, the progress in assistive technologies and provide a glimpse in the possible short/medium term axes of research that can improve existing devices. The survey is based on various features and performance parameters, established with the help of the blind community that allows systems classification using both qualitative and quantitative measures of evaluation. This makes it possible to rank the analyzed systems based on their potential impact on the VI people life. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		37	52		10.1016/j.patrec.2018.10.031													
J								Beyond context: Exploring semantic similarity for small object detection in crowded scenes	PATTERN RECOGNITION LETTERS												Small object detection in crowded scene aims to find those tiny targets with very limited resolution from crowded scenes. Due to very little information available on tiny objects, it is often not suitable to detect them merely based on the information presented inside their bounding boxes, resulting low accuracy. In this paper, we propose to exploit the semantic similarity among all predicted objects' candidates to boost the performance of detectors when handling tiny objects. For this purpose, we construct a pairwise constraint to depict such semantic similarity and propose a new framework based on Discriminative Learning and Graph-Cut techniques. Experiments conducted on three widely used benchmark datasets demonstrate the improvement over the state-of-the-art approaches gained by applying this idea. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		53	60		10.1016/j.patrec.2019.03.009													
J								Human behaviour modelling for welfare technology using hidden Markov models	PATTERN RECOGNITION LETTERS										Ambient assisted living; HMM; Behaviour recognition; Assistive technology; Pattern recognition; Norway; Smart house	ACTIVITY RECOGNITION	Human behaviour modelling for welfare technology is the task of recognizing a person's behaviour patterns in order to construct a safe environment for that person. It is useful in building environments for older adults or to help any person in his or her daily life. The aim of this study is to model the behaviour of a person living in a smart house environment in order to detect abnormal behaviour and assist the person if help is needed. Hidden Markov models, location of the person in the house, posture of the person, and time frame rules are implemented using a real-world, open-source dataset for training and testing. The proposed model presented in this study models the normal behaviour of a person and detects anomalies in the usual pattern. The model shows good results in the identification of abnormal behaviour when tested. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		71	79		10.1016/j.patrec.2019.09.022													
J								COMBAHO: A deep learning system for integrating brain injury patients in society	PATTERN RECOGNITION LETTERS										Robot assistants; Ambient assisted living; Rehabilitation aids	RECOGNITION; SEGMENTATION; NETWORKS	In the last years, the care of dependent people, either by disease, accident, disability, or age, is one of the current priority research topics in developed countries. Moreover, such care is intended to be at patients home, in order to minimize the cost of therapies. Patients rehabilitation will be fulfilled when their integration in society is achieved, either in the family or in a work environment. To address this challenge, we propose the development and evaluation of an assistant for people with acquired brain injury or dependents. This assistant is twofold: in the patient's home is based on the design and use of an intelligent environment with abilities to monitor and active learning, combined with an autonomous social robot for interactive assistance and stimulation. On the other hand, it is complemented with an outdoor assistant, to help patients under disorientation or complex situations. This involves the integration of several existing technologies and provides solutions to a variety of technological challenges. Deep leaning-based techniques are proposed as core technology to solve these problems. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		80	90		10.1016/j.patrec.2019.02.013													
J								Feature mask network for person re-identification	PATTERN RECOGNITION LETTERS										Person re-identification; Image retrieval; Network ensemble		Person re-identification aims at establishing the identity of a pedestrian from a gallery that contains images of people obtained from a multi-camera system, which has many applications in video surveillance for public security and safety. Many challenges such as occlusions, drastic lighting and pose variations across the camera views, and noise make this task highly challenging. While most approaches focus on learning features and metrics to derive better representations, we hypothesize that both local and global contextual cues are crucial for an accurate identity matching. To this end, we propose a Feature Mask Network (FMN) that takes advantage of ResNet high-level features to predict a feature map mask and then imposes it on the low-level features to dynamically re-weight different object parts for a complementary feature representation. This serves as an attention mechanism by allowing the network to focus on local details selectively. We frame the network training as a multi-task objective optimization, which further improves the learned feature descriptions. We conduct experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach respectively achieves significant improvements and competitive results when compared to the state-of-the-art. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		91	98		10.1016/j.patrec.2019.02.015													
J								Deep video-to-video transformations for accessibility with an application to photosensitivity	PATTERN RECOGNITION LETTERS										Photosensitivity; Accessibility; Computer vision; Video-to-video transformation		We demonstrate how to construct a new class of visual assistive technologies that, rather than extract symbolic information, learn to transform the visual environment to make it more accessible. We do so without engineering which transformations are useful allowing for arbitrary modifications of the visual input. As an instantiation of this idea we tackle a problem that affects and hurts millions worldwide: photosensitivity. Any time an affected person opens a website, video, or some other medium that contains an adverse visual stimulus, either intended or unintended, they might experience a seizure with potentially significant consequences. We show how a deep network can learn a video-to-video transformation rendering such stimuli harmless while otherwise preserving the video. This approach uses a specification of the adverse phenomena, the forward transformation, to learn the inverse transformation. We show how such a network generalizes to real-world videos that have triggered numerous seizures, both by mistake and in politically-motivated attacks. A number of complimentary approaches are demonstrated including using a hand-crafted generator and a GAN using a differentiable perceptual metric. Such technology can be deployed offline to protect videos before they are shown or online with assistive glasses or real-time post processing. Other applications of this general technique include helping those with limited vision, attention deficit hyperactivity disorder, and autism. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				SEP	2020	137				SI		99	107		10.1016/j.patrec.2019.01.019													
J								Thermal comfort measurement using thermal-depth images for robotic monitoring	PATTERN RECOGNITION LETTERS										Thermal comfort measurement; Thermal-depth images; Clothing insulation; Mobile assistive robot		This paper describes an application of thermal-depth images to human thermal comfort measurement. A mobile monitoring of the elderly and residents of care houses is one of the promising applications of mobile assistive robots. Monitoring if a person feels comfortable is an important task of such robots. We rely on an established comfort measure in the architecture domain, namely, predicted mean vote (PMV). PMV is calculated mainly by six factors and one of which is the clothing insulation or do-value. Clo-values are usually measured by a thermal mannequin, a specially-designed apparatus for the purpose. We apply human recognition techniques in thermal-depth images to efficiently measure do-values, thereby enabling on-line assessment of thermal comfort. We evaluate the method and develop a mobile robot system for experimental testing. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				SEP	2020	137				SI		108	113		10.1016/j.patrec.2019.02.014													
J								Bayesian reliability estimation for the Topp-Leone distribution under progressively type-II censored samples	SOFT COMPUTING										Approximate methods; Symmetric and asymmetric loss functions; Prior information; Progressively type-II censoring; Reliability	WEIBULL DISTRIBUTION; INFERENCE; MOMENTS; PARAMETER; FAMILY; SHAPE	This study explored the possibility of using the Bayesian estimation for the parameters and reliability characteristics (including reliability function, hazard rate function and reversal hazard rate function) of the two-parametric Topp-Leone distribution when the lifetimes are progressively censored. We assumed the independent gamma prior for the scale parameter and beta prior for the shape parameter of the distribution. Because the expressions for the Bayes estimators for the characteristics under investigation cannot be derived in the closed form, we proposed the Lindley's approximation and Tierney and Kadane's approximation for their numerical estimation. The comparison among the proposed estimators was made by analyzing the simulated and real-life datasets, which elucidated that the estimators are consistent. The proposed estimators, however, are insensitive with respect to change in values of hyper-parameters and model parameters. The employment of censoring schemes with tail behavior along with entropy loss function and Tierney and Kadane's approximation provided the best results, especially in the small-to-moderate samples.																	1432-7643	1433-7479															10.1007/s00500-020-05285-w		SEP 2020											
J								A new opposition crow search optimizer-based two-step approach for controlled intentional islanding in microgrids	SOFT COMPUTING										Opposition; Crow search optimizer; IEEE 69-bus; Intentional islanding; Microgrid	DISTRIBUTION-SYSTEMS; NONLINEAR-SYSTEMS; FEASIBLE METHOD; ALGORITHM; LOAD; STRATEGIES; PLACEMENT; NETWORKS	Crow search optimizer is considered as the latest meta-heuristic algorithm that is influenced by crow's behavior. The proposed oppositional crow search optimizer (OCSO) is intended here, for solving the intentional islanding problem. This paper has proposed a novel two-step method by considering the most significant factors such as the constraints of line capacity, bus voltage, load priority, load controllability, problem occurred due to spacing of solutions as well as the capability of integrating the islands to produce higher intentional islands. Initially, the tree knapsack problem is considered as an intentional islanding issue and therefore the OCSO algorithm is employed in solving such shortcomings. In OCSO approach, the opposition-based generation jumping and population initialization concept are used in crow search optimizer for improving the convergence profile and computational speed. In next process, island feasibility is verified by means of conducting power flow computation and providing significant modifications. Six distributed generations containing IEEE 69-bus test system are employed in validating experimentally the efficiency of the proposed approach, and comparison was done for the obtained results with the other existing approaches. The comparative analysis is evaluated to enhance the level of reliability, particularly critical load.																	1432-7643	1433-7479															10.1007/s00500-020-05280-1		SEP 2020											
J								Two-phase protein folding optimization on a three-dimensional AB off-lattice model	SWARM AND EVOLUTIONARY COMPUTATION										Protein folding optimization; AB off-lattice model; Differential evolution; Two-phase optimization	BEE COLONY ALGORITHM; DIFFERENTIAL EVOLUTION; STRUCTURE PREDICTION; GENETIC ALGORITHM; INTELLIGENCE; ENERGY	This paper presents a two-phase protein folding optimization on a three-dimensional AB off-lattice model. The first phase is responsible for forming conformations with a good hydrophobic core or a set of compact hydrophobic amino acid positions. These conformations are forwarded to the second phase, where an accurate search is performed with the aim of locating conformations with the best energy value. The optimization process switches between these two phases until the stopping condition is satisfied. An auxiliary fitness function was designed for the first phase, while the original fitness function is used in the second phase. The auxiliary fitness function includes an expression about the quality of the hydrophobic core. This expression is crucial for leading the search process to the promising solutions that have a good hydrophobic core and, consequently, improves the efficiency of the whole optimization process. Our differential evolution algorithm was used for demonstrating the efficiency of two-phase optimization. It was analyzed on well-known amino acid sequences that are used frequently in the literature. The obtained experimental results show that the employed two-phase optimization improves the efficiency of our algorithm significantly and that the proposed algorithm is superior to other state-of-the-art algorithms.																	2210-6502	2210-6510				SEP	2020	57								100708	10.1016/j.swevo.2020.100708													
J								Parallel surrogate-assisted optimization: Batched Bayesian Neural Network-assisted GA versus q-EGO	SWARM AND EVOLUTIONARY COMPUTATION										Surrogate-assisted optimization; Bayesian optimization; Efficient global optimization; Simulation; Massively parallel computing; Evolutionary algorithm	EVOLUTIONARY OPTIMIZATION; COMPUTER EXPERIMENTS; ALGORITHM	Surrogate-based optimization is widely used to deal with long-running black-box simulation-based objective functions. Actually, the use of a surrogate model such as Kriging or Artificial Neural Network allows to reduce the number of calls to the CPU time-intensive simulator. Bayesian optimization uses the ability of surrogates to provide useful information to help guiding effectively the optimization process. In this paper, the Efficient Global Optimization (EGO) reference framework is challenged by a Bayesian Neural Network-assisted Genetic Algorithm, namely BNN-GA. The Bayesian Neural Network (BNN) surrogate is chosen for its ability to provide an uncertainty measure of the prediction that allows to compute the Expected Improvement of a candidate solution in order to improve the exploration of the objective space. BNN is also more reliable than Kriging models for high-dimensional problems and faster to set up thanks to its incremental training. In addition, we propose a batch-based approach for the parallelization of BNN-GA that is challenged by a parallel version of EGO, called q-EGO. Parallel computing is a highly important complementary way (to surrogates) to deal with the computational burden of simulation-based optimization. The comparison of the two parallel approaches is experimentally performed through several benchmark functions and two real-world problems within the scope of Tuberculosis Transmission Control (TBTC). The study presented in this paper proves that parallel batched BNN-GA is a viable alternative to q-EGO approaches being more suitable for high-dimensional problems, parallelization impact, bigger data-bases and moderate search budgets. Moreover, a significant improvement of the solutions is obtained for the two TBTC problems tackled.																	2210-6502	2210-6510				SEP	2020	57								100717	10.1016/j.swevo.2020.100717													
J								Quantum-enhanced multiobjective large-scale optimization via parallelism	SWARM AND EVOLUTIONARY COMPUTATION										Quantum mechanics; Multiobjective large-scale optimization; Quantum-inspired evolutionary algorithm (QIEA); Large-scale optimization	INSPIRED EVOLUTIONARY ALGORITHM; GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; SEARCH ALGORITHM; PERFORMANCE; MOEA/D	Traditional quantum-based evolutionary algorithms are intended to solve single-objective optimization problems or multiobjective small-scale optimization problems. However, multiobjective large-scale optimization problems are continuously emerging in the big-data era. Therefore, the research in this paper, which focuses on combining quantum mechanics with multiobjective large-scale optimization algorithms, will be beneficial to the study of quantum-based evolutionary algorithms. In traditional quantum-behaved particle swarm optimization (QPSO), particle position uncertainty prevents the algorithm from easily falling into local optima. Inspired by the uncertainty principle of position, the authors propose quantum-enhanced multiobjective large-scale algorithms, which are parallel multiobjective large-scale evolutionary algorithms (PMLEAs). Specifically, PMLEA-QDE, PMLEA-QjDE and PMLEA-QJADE are proposed by introducing the search mechanism of the individual particle from QPSO into differential evolution (DE), differential evolution with self-adapting control parameters (jDE) and adaptive differential evolution with optional external archive (JADE). Moreover, the proposed algorithms are implemented with parallelism to improve the optimization efficiency. Verifications performed on several test suites indicate that the proposed quantum-enhanced algorithms are superior to the state-of-the-art algorithms in terms of both effectiveness and efficiency.																	2210-6502	2210-6510				SEP	2020	57								100697	10.1016/j.swevo.2020.100697													
J								Surrogate-assisted grey wolf optimization for high-dimensional, computationally expensive black-box problems	SWARM AND EVOLUTIONARY COMPUTATION										Computationally expensive optimization; Grey wolf optimization; Radial basis function; Surrogate model; High-dimensional optimization problems	EFFICIENT GLOBAL OPTIMIZATION; DIFFERENTIAL EVOLUTION; PARTICLE SWARM; NEURAL-NETWORK; ALGORITHM; HYBRID	In this paper, a Surrogate-Assisted Grey Wolf Optimization (SAGWO) algorithm for high-dimensional and computationally expensive problems is presented, where Radial Basis Function (RBF) is employed as the surrogate model. SAGWO conducts the search in three phases, initial exploration, RBF-assisted meta-heuristic exploration, and knowledge mining on RBF. In the initial exploration, the Design of Experiments is carried out to generate a group of well-distributed samples based on which the original wolf pack and wolf leaders are sequentially identified to approximate the high-dimensional space roughly. The knowledge mining on RBF includes a global search that is carried out using the grey wolf optimization and a local search that is performed over a focused local region using a search strategy combining global and multi-start local exploration. In the proposed SAGWO, knowledge gained from the RBF model assists the generation of new wolf leaders in each cycle, and the positions of the wolf pack are iteratively changed following the wolf leaders, thus reaching balanced exploitation and exploration. The new SAGWO algorithm presents superior computation efficiency and robustness as demonstrated by comparison tests with ten representative global optimization algorithms on 30, 50 and 100 design variables.																	2210-6502	2210-6510				SEP	2020	57								100713	10.1016/j.swevo.2020.100713													
J								A survey on meta-heuristics for solving disassembly line balancing, planning and scheduling problems in remanufacturing	SWARM AND EVOLUTIONARY COMPUTATION										Meta-heuristic; Swarm intelligence; Evolutionary algorithms; Disassembly line balancing; Disassembly planning; Disassembly scheduling	ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; ELECTRONIC EQUIPMENT; SEARCH ALGORITHM; SEQUENCE; MAINTENANCE; OPERATIONS; SERVICE; HAZARD; BEES	Recently, meta-heuristics have been employed and improved for solving various scheduling and combinational optimization problems. Disassembly line balancing, planning and scheduling problems (DLBPSP) are typical examples since the high complexity (NP-Hard). Since 2000s, numerous articles have represented the applications of meta-heuristics for solving DLBPSP. This paper aims to review the state-of-the-art of this topic. It can help researchers, especially for new researchers, to identify the current status of meta-heuristics for solving DLBPSP, to obtain the technologies used in various algorithms, and to follow the research trends of this topic. First, the related research articles are summarized, classified, and analyzed. Second, the special meta-heuristics for solving DLBPSP are reviewed. The encoding/decoding rules and improvement strategies are analyzed and discussed. Finally, the current research trends are summarized, and some future research directions are given.																	2210-6502	2210-6510				SEP	2020	57								100719	10.1016/j.swevo.2020.100719													
J								A comparative study of high-productivity high-performance programming languages for parallel metaheuristics	SWARM AND EVOLUTIONARY COMPUTATION										Metaheuristics; Parallel metaheuristics; High-productivity languages; Parallel computing	3-DIMENSIONAL ASSIGNMENT PROBLEM	Parallel metaheuristics require programming languages that provide both, high performance and a high level of programmability. This paper aims at providing a useful data point to help practitioners gauge the difficult question of whether to invest time and effort into learning and using a new programming language. To accomplish this objective, three productivity-aware languages (Chapel, Julia, and Python) are compared in terms of performance, scalability and productivity. To the best of our knowledge, this is the first time such a comparison is performed in the context of parallel metaheuristics. As a test-case, we implement two parallel metaheuristics in three languages for solving the 3D Quadratic Assignment Problem (Q3AP), using thread-based parallelism on a multi-core shared-memory computer. We also evaluate and compare the performance of the three languages for a parallel fitness evaluation loop, using four different test-functions with different computational characteristics. Besides providing a comparative study, we give feedback on the implementation and parallelization process in each language.																	2210-6502	2210-6510				SEP	2020	57								100720	10.1016/j.swevo.2020.100720													
J								Many-objective optimization for scheduling of crude oil operations based on NSGA-III with consideration of energy efficiency	SWARM AND EVOLUTIONARY COMPUTATION										Crude oil operations; Scheduling; NSGA-III; Energy efficiency; Pareto-optimality	NONDOMINATED SORTING APPROACH; TERM SCHEDULABILITY ANALYSIS; CONTINUOUS-TIME; MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; MINLP MODEL; MILP MODEL; REFINERY; TRANSPORTATION; PIPELINE	Crude oil operations in refineries are characterized as a hybrid system since it contains both discrete-event and continuous processes, and it is extremely difficult to schedule such a system. For scheduling such a system, initially the discrete tasks to be performed during the scheduling horizon is unknown such that heuristics and meta-heuristics are not directly applicable, which further complicates its scheduling problem. Moreover, there are large number of objectives to be optimized, including the minimization of energy consumption due to that crude oil operations consume large amount of energy and therefore lead to large amount of emissions. Furthermore, the energy optimization problem is characterized as highly non-linearity. Hence, the scheduling problem of crude oil operations in refineries belongs to the many-objective optimization problems and it is extremely challenging. This paper addresses this challenging scheduling problem of crude oil operations. This scheduling problem is first converted to a discrete dynamic resource allocation problem such that meta-heuristics can be applicable. Then, with the results of large number of experiments, this work innovatively proposes an NSGA-III-based optimization algorithm to efficiently solve the problem for Pareto-optimal solutions. By the proposed method, the genes in a chromosome are generated one by one and, when generating a gene, safeness check is done according to the derived safeness conditions such that each gene is feasible. In this way, the schedule feasibility can be ensured. An industrial case study is given to test its performance and comparison is made with the existing state-of-the-art algorithms for many-objective optimization problems. The results show its good performance in terms of convergence, solution diversity, time efficiency, and its applicability to real-life refinery scheduling problems.																	2210-6502	2210-6510				SEP	2020	57								100714	10.1016/j.swevo.2020.100714													
J								Autonomous detection of collective behaviours in swarms	SWARM AND EVOLUTIONARY COMPUTATION										Collective behaviour; Artificial swarming; Evolutionary framework; Boids model; Computational value systems	VALUE-SYSTEMS; INTELLIGENCE; EXPLORATION; MODEL	Collective behaviours such as swarm formations of autonomous agents offer the advantages of efficient movement redundancy, and potential for human guidance of a single swarm organism. This paper proposes a developmental approach to evolving collective behaviours whereby the evolutionary process is guided by a novel value system. A self-organising map is used at the core of this value system and motion properties of the swarm entities are used as input. Unlike traditional approaches, this value system does not need in advance the precise characteristics of the intended behaviours. We examine the performance of this value system in a series of controlled experiments. Our results demonstrate that the value system can recognise multiple "interesting" structured collective behaviours and distinguish them from random movement patterns. Results show that our value system is most effective distinguishing structured behaviours from random behaviours when using motion properties of individual agents as input. Further variations and modifications to input data such as normalisation and aggregation were also investigated, and it was shown that certain configurations provide better results in distinguishing collective behaviours from random ones.																	2210-6502	2210-6510				SEP	2020	57								100715	10.1016/j.swevo.2020.100715													
J								Optimization of Lennard-Jones clusters by particle swarm optimization with quasi-physical strategy	SWARM AND EVOLUTIONARY COMPUTATION										Particle swarm optimization; Quasi-physical strategy; Multimodal global optimization; Lennard-Jones (LJ) clusters	GLOBAL OPTIMIZATION; GENETIC ALGORITHM; DECOMPOSITION	The goal of Lennard-Jones (LJ) clusters optimization is to find the minimum value of the potential function of a cluster and thereby determine the stable configuration of the cluster. It is essentially a completely inseparable multimodal global optimization problem, and using the traditional particle swarm algorithm to solve it often results in local convergence, which means that the solution accuracy of the algorithm is not high. Thus, in this study, we develop a LJ algorithm using a particle swarm optimization (PSO) method and a physical approach to improve the solution accuracy. In this quasi-physical strategy (QPS), the particle swarm algorithm is used to simulate the real atomic structure and incorporates the interatomic force to construct a convergence model so that the algorithm performs well in both global and local space. The potential energy functions of a variety of LJ cluster systems are selected as test functions, and the improved PSO algorithm (QPS-PSO) is analyzed and compared with a competitive swarm optimizer, cooperative coevolution PSO, and differential-group cooperative coevolution, variable-length PSO for feature selection, heterogeneous comprehensive learning PSO, ensemble PSO and cooperative coevolution with differential optimization. The results show that the PSO algorithm for LJ clusters using the proposed QPS has noticeably superior solution accuracy, especially in high-dimensional spaces.																	2210-6502	2210-6510				SEP	2020	57								100710	10.1016/j.swevo.2020.100710													
J								A firefly algorithm for the environmental prize-collecting vehicle routing problem	SWARM AND EVOLUTIONARY COMPUTATION										Environmental vehicle routing problem; Prize-collecting vehicle routing problem; Firefly algorithm; Coordinates related encoding/decoding process	LOCAL SEARCH ALGORITHM; FUEL CONSUMPTION; DISCRETE; HYBRID; DESIGN; MODEL; INTELLIGENCE; NEIGHBORHOOD; COLONY; SWARM	In the present research, a new variant of the Vehicle Routing Problem (VRP), the Environmental Prize-Collecting Vehicle Routing Problem (E-PCVRP), is introduced. The E-PCVRP is a selective routing problem that focuses on the maximization of the aggregated prize values collected from the visited nodes while minimizing the fixed and variable cost of the formed routes. In terms of variable cost, the CO2 emissions of the vehicles performing the routes are considered as a load-distance function. The presented solution approach is based on the Firefly Algorithm (FA). The FA is an optimization algorithm, designed for the solution of continuous problems, while the proposed E-PCVRP, requires a discrete solution approach. Addressing the above discrepancy, the Firefly Algorithm based on Coordinates (FAC) is introduced, which incorporates the proposed "Coordinates Related" (CR) encoding/decoding process in the original FA scheme. The CR is a novel process that allows for algorithms designed for continuous optimization to by employed in the solution of discrete problems, such as the VRP. Specifically, the CR utilizes auxiliary vectors for solution representation, containing the Cartesian coordinates of each node, that allows for the original movement equation of the FA to be applied directly. The effectiveness of the FAC algorithm is showed over computational experiments and statistical analysis, in comparison to the performance of other bio-inspired algorithms and a mathematical solver.																	2210-6502	2210-6510				SEP	2020	57								100712	10.1016/j.swevo.2020.100712													
J								Energy-efficient distributed permutation flow shop scheduling problem using a multi-objective whale swarm algorithm	SWARM AND EVOLUTIONARY COMPUTATION										Distributed permutation flow shop; Setup times; Energy-efficient scheduling; Whale swarm algorithm	MINIMIZING MAKESPAN; GENETIC ALGORITHM; SEARCH ALGORITHM; OPTIMIZATION; HEURISTICS; EVOLUTION	Production scheduling is of great significance in improving production effectiveness while the energy-efficient problem is one of most concerned problems for researchers and manufacturers. Thus, this study investigates the energy-efficient distributed permutation flow shop scheduling problem (DPFSP) with the objectives of makespan and energy consumption. The DPFSP is an extension of permutation flow shop problem (PFSP) considering a set of identical factories. This paper presents a multi-objective mixed integer programming model based on the three sub-problems: allocating jobs among factories, scheduling the jobs in each factory and determining speed upon each job. A multi-objective whale swarm algorithm (MOWSA) is proposed to solve this energy-efficient DPFSP. A new problem-dependent local search is developed to improve the exploitation capability of MOWSA. Moreover, the updating exploitation mechanism is presented to enhance energy efficiency without affecting production efficiency. Finally, the extensive comparison experiments are designed to demonstrate the effectiveness of proposed MOWSA, problem-dependent local search and updating exploitation mechanism. The results indicate the effectiveness of MOWSA and the superior performance over NSGA-II, SPEA2, PAES and MDEA, and also demonstrate that the proposed algorithm can significantly reduce the energy consumption compared with other algorithms.																	2210-6502	2210-6510				SEP	2020	57								100716	10.1016/j.swevo.2020.100716													
J								Multiple adaptive strategies based particle swarm optimization algorithm	SWARM AND EVOLUTIONARY COMPUTATION										Particle swarm optimization; Multiple adaptive strategies; Learning exemplars; Population size; Multiple swarms	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; STABILITY ANALYSIS; SELECTION; ADAPTATION; ENSEMBLE; DYNAMICS; SEARCH	Although particle swarm optimization algorithm (PSO) has displayed promising performance on many optimization problems, how to balance contradictions between the exploration and the exploitation and rationally allocate computational resource are two crucial problems need to be dealt with in PSO study. In this paper, a PSO variant based on multiple adaptive strategies (MAPSO) is proposed. To efficiently maintain the population diversity, the entire population is split into multiple swarms, which can be regrouped during the evolutionary process. In each generation, different particles in a swarm adaptively select their learning exemplars (ALE) according to the performance of the particles. Thus, different particles in the same swarm can perform distinct search behaviors in each generation, as well as the same particle can conduct various search behaviors in different generations. In addition, aiming to rationally utilize computational resource, an adaptive strategy for population size (APS) is introduced. In APS, the population can adaptively delete unfavorable particles and add promising particles during the evolutionary process. Extensive experiments based on CEC2013 and CEC2017 test suites verify the superior performance of the multiple adaptive strategies on balancing the exploration and exploitation abilities. Furthermore, the performance of the newly introduced strategies is also testified by a set of experiments.																	2210-6502	2210-6510				SEP	2020	57								100731	10.1016/j.swevo.2020.100731													
J								A novel multi-objective genetic algorithm based error correcting output codes	SWARM AND EVOLUTIONARY COMPUTATION										Multi-objective; Genetic algorithm (GA); Error correcting output codes (ECOC); Pairwise diversity; Multiclass classification; Heterogeneous ensemble	CANCER-DIAGNOSIS; DEPENDENT DESIGN; MULTICLASS; CLASSIFICATION; ENSEMBLE; PREDICTION; CLASSIFIERS; SELECTION; ECOC; OPTIMIZATION	Up to now, different genetic algorithm (GA) based error correcting output codes (ECOC) algorithms have been proposed by setting accuracy as the optimization objective. However, it was demonstrated that diversity among learners is of great significance to a robust ensemble. In this paper, we propose a multi-objective GA with setting accuracy and diversity as two objectives. To further promote diversity in an ensemble, a new individual structure is designed to accommodate heterogeneous dichotomizers. Three multi-objective ranking strategies are deployed to balance two objectives respectively. A novel genetic operator is designed to produce ECOC-compatible offspring in the evolutionary process, and a local improvement algorithm is designed to promote individuals' fitness values. To verify the performance of our GA, a single objective ranking strategy and the design of homogeneous learner based GA are also adopted. Ten widely used ECOC algorithms and three famous ensemble algorithms are deployed for performance comparisons based on a set of the UCI data and microarray data sets. Results show that compared with other algorithms, our GA obtains higher performance in most cases due to the trade-off between performance and diversity. Besides, the accommodation of heterogeneous dichotomizers in an ensemble provides higher generalization ability compared with homogeneous ensembles.																	2210-6502	2210-6510				SEP	2020	57								100709	10.1016/j.swevo.2020.100709													
J								A Multi-Agent System for guiding users in on-line social environments	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-Agent System; Social networks; Sentiment analysis; Stress analysis	REPUTATION; SENTIMENT	The present work is a study of the detection of negative affective or emotional states, the high-stress levels that people have using social network sites (SNSs), and the effect that this negative state or stress level has on the repercussions of posted messages. We aim to discover to what extent a user that has a state detected as negative by an analyzer (Sentiment analyzer and Stress analyzer) can affect other users and generate negative repercussions, and also determine whether it is more suitable to predict a future negative situation using different analyzers. We propose two different methods for creating a combined model of sentiment and stress, and we use them in our experimentation to discern which one is more suitable for predicting future negative situations that could arise from the interaction between users, and in what context. Additionally, we designed a Multi-Agent System (MAS) that integrates the analyzers to protect or advise users on a SNS. We have conducted this study to help build future systems that prevent negative situations where a user that has a negative state creates a repercussion in the SNS. This can help users avoid getting into a bad mood or help avoid privacy issues (e.g. a user that has a negative state posting information that the user does not really want to post).																	0952-1976	1873-6769				SEP	2020	94								103740	10.1016/j.engappai.2020.103740													
J								Extra-adaptive robust online subspace tracker for anomaly detection from streaming networks	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Anomaly detection; Robust online subspace tracker; Dynamic network; CP tensor decomposition; Low rank and sparse analysis	PCA	Anomaly detection in time-evolving networks has many applications, for instance, traffic analysis in trans-portation networks and intrusion detection in computer networks. One group of popular methods for anomaly detection from evolving networks are robust online subspace trackers. However, these methods suffer from problem of insensitivity to drastic changes in the evolving subspace. In order to solve this problem, we propose a new robust online subspace and anomaly tracker, which is more adaptive and robust against sudden drastic changes in the subspace. More accurate estimation of low rank and sparse components by this tracker leads to more accurate anomaly detection. We evaluate the accuracy of our method with real-world dynamic network data sets with varying sparsity levels. The result is promising and our method outperforms the state-of-the-art.																	0952-1976	1873-6769				SEP	2020	94								103741	10.1016/j.engappai.2020.103741													
J								Erasable pattern mining based on tree structures with damped window over data streams	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Erasable pattern mining; Damped window; Tree structures; Pruning technique; Stream mining	EFFICIENT ALGORITHM; FREQUENT PATTERNS; ITEMSETS; TIME; NETWORKS	Several pattern mining methods have been proposed to process dynamic data streams because the data generated in industrial fields is continually accumulated. Erasable pattern mining techniques for processing dynamic data streams are needed to discover erasable patterns from dynamic data streams. In previous erasable pattern mining approaches suggested for dynamic data streams, all data are considered to have the same importance regardless of its timestamp. However, dynamic data streams have the characteristic that the new data is relatively more significant than the old data. In erasable pattern mining, one of the desired techniques is an approach in consideration of such characteristic of data streams. For this reason, we propose an erasable pattern mining algorithm over dynamic data streams based on the damped window model. Since the suggested technique considers the new data more important than the previous data, it can find more useful erasable patterns. In addition, erasable pattern mining based on the damped window model is conducted efficiently by employing the tree and table structures. In performance test, we present that our pruning techniques remove unnecessary operations related to invalid erasable patterns efficiently from damped-window-based data streams. Performance evaluation results using real datasets and synthetic datasets show that the proposed approach has good performance with regard to as execution time, pattern generation, and scalability by comparing between the suggested technique and the state of the art algorithms.																	0952-1976	1873-6769				SEP	2020	94								103735	10.1016/j.engappai.2020.103735													
J								Real-time prediction of process forces in milling operations using synchronized data fusion of simulation and sensor data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Machine learning algorithms; Predictive models; Production engineering; Simulation; Time series analysis	NEURAL-NETWORK; TOOL WEAR; CUTTING FORCE; REGRESSION; MACHINES; SELECTION; INTEGRATION; STRATEGIES; PARAMETERS; ENERGY	To prevent undesirable effects during milling processes, online predictions of upcoming events can be used. Process simulations enable the capability to retrieve additional knowledge about the process, since their application allows the generation of data about characteristics, which cannot be measured during the process and can be incorporated as pre-calculated features into the analysis. Furthermore, sensor technologies were used as reasonable data sources for analyzing different monitoring scopes of milling processes. Machine learning-based models utilize data, acquired by various available data sources, to generate predictions of upcoming events in real-time. In this paper, we propose a novel approach for combining simulation data with sensor data to generate online predictions of process forces, which are influenced by tool wear, using an ensemble-based machine learning method. In addition, a methodology was developed in order to synchronize pre-calculated simulation data and streaming sensor measurements in real time. Milling experiments using ball-end milling tools with varying cutting speeds and tooth feeds showed the robustness of the approach in enhancing the prediction accuracy compared to only using one of each data source.																	0952-1976	1873-6769				SEP	2020	94								103753	10.1016/j.engappai.2020.103753													
J								Multi Level Directional Cross Binary Patterns New handcrafted descriptor for SVM-based texture classification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										LBP; Texture recognition; Feature extraction; SVM classifier; MLD-CBP	FACE RECOGNITION; FEATURES	The pattern recognition and computer vision have experienced a prominent progress in feature extraction techniques, judged by the extensive proposed methods in the literature. A big part of these works was devoted to enhance the texture classification performance, regarding the important role of textural analysis in various real-world and challenging applications. Developing discriminant feature extractors requires solid knowledge in machine learning and applied mathematics. However, Local Binary Patterns (LBP) offered much more space to develop enhanced handcrafted descriptors thanks to its simplicity and flexibility. In this paper we introduce a brand new LBP variant referred to as Multi Level Directional Cross Binary Patterns (MLD-CBP). The proposed representation is training-free, low-dimensional, yet discriminative and robust handcrafted operator for texture description. The concept of the proposed MLD-CBP descriptor is based on encoding the most informative directions contained within multi radiuses, which helps in detecting the gray level variations that may occur in different directions. Moreover, the proposed MLD-CBP handcrafted is combined with an automated SVM classifier based on the RBF Kernel, where the.. parameter is calculated automatically according to the training images. Conducted experiments on 15 well known and challenging databases of the literature, demonstrate prominent performance and stability compared to the results achieved by 30 recent and most powerful descriptors of the state-of-the-art. This paper provides also a comparative study on the effect of.. parameter to show the benefits of automatically tuning this parameter value considering the nature of the database and its size.																	0952-1976	1873-6769				SEP	2020	94								103743	10.1016/j.engappai.2020.103743													
J								A novel method for locating the critical slip surface of a soil slope	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Slope stability; Critical slip surface; Minimum safety factor; Improved whale optimization algorithm	WHALE OPTIMIZATION ALGORITHM; NONCIRCULAR FAILURE SURFACE; STABILITY ANALYSIS; GREY WOLF; LANDSLIDE; SEARCH	Calculating the minimum slope safety factor or locating the critical slip surface of a soil slope is a complex optimization problem. This paper describes an improved whale optimization algorithm (IWOA) for locating the critical slip surface of a soil slope. Locating a critical slip surface is transformed into a three-dimensional problem from a high-dimensional optimization. Combined with the Morgenstern-Price method, IWOA is compared against other optimization techniques in an experimental study. Test results using 13 benchmark functions show that IWOA significantly outperforms the conventional whale optimization algorithm (WOA) and particle swarm optimization (PSO). The IWOA method is then used to search for the critical slip surfaces of four slopes. The results show that IWOA again performs better than WOA and PSO in locating the critical slip surface.																	0952-1976	1873-6769				SEP	2020	94								103733	10.1016/j.engappai.2020.103733													
J								Discriminative sparse embedding based on adaptive graph for dimension reduction	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Manifold learning; Discriminative sparse embedding; Dimension reduction; Subspace learning	FACE-RECOGNITION; PRESERVING PROJECTIONS; REGRESSION; SELECTION; EIGENFACES	The traditional manifold learning methods usually utilize the original observed data to directly define the intrinsic structure among data. Because the original samples often contain a deal of redundant information or it is corrupted by noises, it leads to the unreliability of the obtained intrinsic structure. In addition, the intrinsic structure learning and subspace learning are completely separated. For solving above problems, this paper presents a novel dimension reduction method termed discriminative sparse embedding (DSE) based on adaptive graph. By projecting the original samples into a low-dimensional subspace, DSE learns a sparse weight matrix, which can reduce the effects of redundant information and noises of the original data, and uncover essential structural relationship among the data. In DSE, the robust subspace is learned from the original data. Meanwhile, the intrinsic local structure and the optimal subspace can be simultaneously learned, in which they are mutually improved, and the accurate structure can be captured, and the optimal subspace can be obtained. We propose an alternative and iterative method to solve the DSE model. In order to evaluate the performance of DSE, it is compared with some state-of-the-art feature extraction algorithms. Various experiments show that our DSE is effective and feasible.																	0952-1976	1873-6769				SEP	2020	94								103758	10.1016/j.engappai.2020.103758													
J								Trust management and evaluation for edge intelligence in the Internet of Things	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Trust; Internet of Things; Attacks; Denial of Services; Networks; Security; Computer	WIRELESS SENSOR NETWORKS; MECHANISM	Information and Communication Technologies (ICTs) have revolutionised the traditional communication processes, converting the cities into Smart Cities. Internet of Things (IoT) is one of the leading frameworks in Smart Cities realm; it is based on heterogeneous infrastructure, digital systems, smart technologies, and intelligent services. Due to the complex networks supporting the IoT world, smart devices and services are quickly degrading due to various factors. Security is one of the considered factors, and it also represents a difficult challenge. Malicious nodes disrupt the data traffic and integrity of IoT-based networks. This paper presents a novel Cumulative Trust Evaluation based Efficient Technique (CTBET) by singling out numerous viewpoints on governing and implementing the security in edge-based IoT networks. The proposed CTBET is based on the cumulative trust concept, which calculates the direct and indirect trust among nodes considering the packet drop rate and the packet data rate among different transmission nodes. Furthermore, it enforces suitable approaches to implement the trust mechanism based technique to enhance security and privacy. The proposed scheme handles the On-Off, Denial of Service (DoS) and Bad-Mouth attacks and is also able to isolate the malicious nodes in edge-based IoT networks. The provided simulation results show encouraging performance in terms of network life span, level of trustworthiness of nodes, lesser end-to -end delay and high data delivery ratio, during data transmission in the presence of the malicious and selfish nodes in the network.																	0952-1976	1873-6769				SEP	2020	94								103756	10.1016/j.engappai.2020.103756													
J								Towards privacy preserving AI based composition framework in edge networks using fully homomorphic encryption	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Edge-AI; Artificial Intelligence; Privacy in edge networks; Privacy-preserving AI; Privacy-preserving AI-based service composition; Privacy-preserving service composition	SECURITY; CLOUD; ISSUES	We present a privacy-preserving framework for Artificial Intelligence (AI) enabled composition for the edge networks. Edge computing is a very promising technology for provisioning realtime AI services due to low response time and network bandwidth requirements. Due to the lack of computational capabilities, an edge device alone cannot provide the complex AI services. Complex AI tasks should be divided into multiple sub-tasks and distributed among multiple edge devices for efficient service provisioning in the edge network. AI-enabled or automatic service composition is one of the essential AI tasks in the service provisioning. In edge computing-based service provisioning, service composition related tasks need to be offloaded to several edge nodes for efficient service. Edge nodes can be used for monitoring services, storing Quality-of-Service (QoS) data, and composing services to find the best composite service. Existing service composition methods use plaintext QoS data. Hence, attackers may compromise edge devices to reveal QoS data of services and modify them for giving an advantage to particular edge service providers, and the AI-based service composition becomes biased. From that point of view, a privacy-preserving framework for AI-based service composition is required for the edge networks. In our proposed framework, we introduce an AI-based composition model for edge services in the edge networks. Additionally, we present a privacy-preserving AI service composition framework to perform composition on encrypted QoS data using fully homomorphic encryption (FHE) algorithm. We conduct several experiments to evaluate the performance of our proposed privacy-preserving service composition framework using a synthetic QoS dataset.																	0952-1976	1873-6769				SEP	2020	94								103737	10.1016/j.engappai.2020.103737													
J								Boosting algorithms for network intrusion detection: A comparative evaluation of Real AdaBoost, Gentle AdaBoost and Modest AdaBoost	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Malware detection; Intrusion detection systems; Anomaly detection; Machine learning; Ensemble learning; Boosting techniques; AdaBoost	ANOMALY DETECTION; DETECTION SYSTEM; BIG DATA; CLASSIFICATION; INTELLIGENCE; REGRESSION; FEATURES; FUSION; THREAT	Computer networks have been experienced ever-increasing growth since they play a critical role in different aspects of human life. Regarding the vulnerabilities of computer networks, they should be monitored regularly to detect intrusions and attacks by using high-performance Intrusion Detection Systems (IDSs). IDSs try to differentiate between normal and abnormal behaviors to recognize intrusions. Due to the complex behavior of malicious entities, it is crucially important to adopt machine learning methods for intrusion detection with a fine performance and low time complexity. Boosting approach is considered as a way to deal with this challenge. In this paper, we prepare a clear summary of the latest progress in the context of intrusion detection methods, present a technical background on boosting, and demonstrate the ability of the three well-known boosting algorithms (Real Adaboost, Gentle Adaboost, and Modest Adaboost) as IDSs by using five IDS public benchmark datasets. The results show that the Modest AdaBoost has a higher error rate compared to Gentle and Real AdaBoost in IDSs. Besides, in the case of IDSs, Gentle and Real AdaBoost show the same performance as they have about 70% lower error rates compared to Modest Adaboost, however, Modest AdaBoost is about 7% faster than them. In addition, as IDSs need to retrain the model frequently, the results show that Modest AdaBoost has a much lower performance than Gentle and Real AdaBoost in case of error rate stability.																	0952-1976	1873-6769				SEP	2020	94								103770	10.1016/j.engappai.2020.103770													
J								Deep neural network model with Bayesian hyperparameter optimization for prediction of NOx at transient conditions in a diesel engine	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep neural networks; Hyperparameter optimization; Bayesian optimization; NOx prediction; Transient cycle; Diesel engine		Owing to increasing interest in the environment, particularly on air quality, regulations in the automobile industry have become stricter. Test cycles have been substituted to simulate real driving conditions, and they offer opportunities for researchers to satisfy regulations and predict emissions using models. The objective of this study is to develop a deep neural network (DNN) model, optimize its hyperparameters using the Bayesian optimization method, and use hidden-node determination logic to predict engine-out NOx emissions by using the worldwide harmonized light vehicles test procedure (WLTP) of diesel engines. A DNN network learns the internal relationships between inputs and target outputs even though they are complicated. However, the hyperparameters of DNNs are typically determined by researchers before training, and they affected the accuracy of the model. In this study, the hyperparameters of the DNN model such as the number of hidden layers, number of nodes in each hidden layer, learning rate, learning rate decay, and batch size are automatically optimized using the Bayesian optimization method. Some logical equations are combined with the number of nodes in the first hidden layer and the number of hidden layers to realize the model's structure instead of using the number of hidden nodes in each hidden layer. Compared with grid search and random sampling, the Bayesian optimization method is a promising solution to optimize hyperparameters. In addition, a hidden-node determination logic further improved the accuracy of the model. The accuracy of the optimized model is indicated by an R-2 value of 0.9675 with 14 input features. The result of cycle prediction shows that the mean absolute errors are approximately 16-17 ppm for four WLTP cycles, which are 1.6% of the maximum NOx value. These results indicate that the accuracy of the model is comparable to that of a physical NOx measurement device whose linearity is 1% of the full scale (5,000 ppm).																	0952-1976	1873-6769				SEP	2020	94								103761	10.1016/j.engappai.2020.103761													
J								A new Kho-Kho optimization Algorithm: An application to solve combined emission economic dispatch and combined heat and power economic dispatch problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Combined Emission Economic Dispatch problem (CEED); Combined Heat and Power Economic Dispatch problem (CHPED); Optimization technique	PARTICLE SWARM OPTIMIZATION; GROUP SEARCH OPTIMIZATION; ARTIFICIAL BEE COLONY; SCALE COMBINED HEAT; LOAD DISPATCH; GENETIC ALGORITHM; BAT ALGORITHM; BEHAVIOR	In this article, a new optimization technique known as Kho-Kho optimization (KKO) algorithm is presented. This proposed technique is a population based meta-heuristic method which is inspired from the strategies used by players in a well known tag-team game played in India, i.e. Kho-Kho. The performance and superiority of the proposed method with respect to other existing methods is evaluated using twenty nine benchmark functions and real-time optimization problems related to power system i.e. combined emission economic dispatch and combined heat and power economic dispatch problem.																	0952-1976	1873-6769				SEP	2020	94								103763	10.1016/j.engappai.2020.103763													
J								Scoring and assessment in medical VR training simulators with dynamic time series classification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Virtual reality; Simulation; Medical training; Skill assessment; Classification; Time series	SURGICAL SKILL; PERFORMANCE; RETRIEVAL; ALGORITHM; NETWORKS	This research proposes and evaluates scoring and assessment methods for Virtual Reality (VR) training simulators. VR simulators capture detailed n-dimensional human motion data which is useful for performance analysis. Custom made medical haptic VR training simulators were developed and used to record data from 271 trainees of multiple clinical experience levels. DTW Multivariate Prototyping (DTW-MP) is proposed. VR data was classified as Novice, Intermediate or Expert. Accuracy of algorithms applied for time-series classification were: dynamic time warping 1-nearest neighbor (DTW-1NN) 60%, nearest centroid SoftDTW classification 77.5%, Deep Learning: ResNet 85%, FCN 75%, CNN 72.5% and MCDCNN 28.5%. Expert VR data recordings can be used for guidance of novices. Assessment feedback can help trainees to improve skills and consistency. Motion analysis can identify different techniques used by individuals. Mistakes can be detected dynamically in real-time, raising alarms to prevent injuries.																	0952-1976	1873-6769				SEP	2020	94								103760	10.1016/j.engappai.2020.103760													
J								An engine-fault-diagnosis system based on sound intensity analysis and wavelet packet pre-processing neural network	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Engine fault diagnosis; Sound intensity analysis; Noise-based fault recognition; Wavelet package analysis (WPA); Artificial neural network (ANN)	COMBUSTION ENGINES	Based on the techniques of sound intensity analysis, incomplete wavelet packet analysis (WPA) and artificial neural network (ANN), a WPA pre-processing method for noise-based engine fault diagnosis (EFD), so-called WPA-ANN model, is presented in this paper. The noises of an EFI gasoline engine under normal and fault states are measured and their contours of sound intensity level (SIL) are calculated by interpolation approach to initially investigate the possibility of a SIL-based EFD. Furthermore, an incomplete WPA model, which consists of a five-level discrete wavelet transform (DWT) and a four-level WPA, is developed and applied to the measured noise signals for extracting fault features of the engine, as is a multi-layered ANN model for engine failure classification by using the extracted features of the noises. To verify the proposed approach, the WPA-ANN model is extended to recognize other noise-related faults of the engine. The results suggest that the noise-based WPA-ANN models are effective for engine fault diagnosis. Due to its time-frequency characteristics and pattern recognition capacity, the WPA-ANN can be used to process both the stationary and nonstationary signals. In view of the applications, the proposed WPA-ANN model can be directly used in vehicle EFDs, and may be extended to other sound-related fields for failure diagnosis in engineering.																	0952-1976	1873-6769				SEP	2020	94								103765	10.1016/j.engappai.2020.103765													
J								Cross-scale generative adversarial network for crowd density estimation from images	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Generative adversarial network; Crowd density estimation; Deconvolution convolutions; Loss function	SIMULATION; EVACUATION; MULTIPLE; HUMANS; RISK; BEHAVIORS; TRACKING; STATIONS; MODEL	This research develops a cross-scale convolutional spatial generative adversarial network (CSGAN), in order to estimate the crowd density from images accurately. It consists of two similar generators, one for the whole feature extraction, and the other for patch scale feature extraction. An encoder-decoder structure is employed to generate density maps from input images or patches. Additionally, a new objective function for crowd counting called cross-scale consistency pursuit containing an adversarial loss, L2 loss, perceptual loss, and consistency loss, is developed to make the generated density maps more realistic and closer to the ground truth. The effectiveness of the proposed CSGAN is verified in two public datasets. Results indicate that the new objective function is able to reach the most satisfying value of evaluation metrics in both the low-density and high-density crowd scenes when it is compared with other state-of-the-art methods on the test datasets. Moreover, the proposed CSGAN is more practical and flexible due to the smaller computational complexity. Its estimation capability will be significantly improved even in a small size of training data. Overall, this research contributes to the development of a novel computer vision approach together with a new objective function to generate density maps from cross-scale crowd images, enabling the counting process more accurately and efficiently.																	0952-1976	1873-6769				SEP	2020	94								103777	10.1016/j.engappai.2020.103777													
J								Optimized task distribution based on task requirements and time delay in edge computing environments	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Edge computing; Node selection; Bloom filter; Task distribution	FOG; CLOUD; MINIMIZATION; PLACEMENT; SECURITY; INTERNET; SYSTEMS; MODEL	Edge computing is a new technology for completing real-time and complex tasks with low latency. However, due to limited storage, computing and communication capabilities of edge nodes, it is often necessary for multiple edge nodes to share a task's related work load to decrease its overall execution time. To solve the problem, this paper proposes a task distribution method based on the analysis of task requirements and time delay in an edge computing environment. First, the related data is received by a proxy server to obtain the running states of edge nodes. Then, a task-based edge node selection algorithm is designed to select appropriate target edge nodes. It can meet task requirements by using a Bloom filter to filter malicious nodes. Finally, based on the above selected nodes, optimized target edge nodes are selected to achieve the minimum time delay. Based on the selected optimized target edge nodes, this paper proposes an algorithm to optimize task distribution for meeting task requirements and achieve the minimum delay in an edge computing environment. Because the method considers both task requirements and time delay, it can distribute tasks to target nodes at low cost. The experimental results show that the method is feasible and effective and outperforms two commonly-used methods																	0952-1976	1873-6769				SEP	2020	94								103774	10.1016/j.engappai.2020.103774													
J								Designing energy-efficient high-precision multi-pass turning processes via robust optimization and artificial intelligence	JOURNAL OF INTELLIGENT MANUFACTURING										Multi-pass machining; Turning process; Robust optimization; Energy efficiency; Multi-objective optimization; Evolutionary computation	MULTIOBJECTIVE OPTIMIZATION; PROGRAMMING APPROACH; CUTTING PARAMETERS; NETWORK DESIGN; CONSUMPTION; OPERATIONS; ALGORITHMS; PREDICTION	This paper suggests a novel robust formulation designed for optimizing the parameters of the turning process in an uncertain environment for the first time. The aim is to achieve the lowest energy consumption and highest precision. With this aim, the current paper considers uncertain parameters, objective functions, and constraints in the offered mathematical model. We proposed several uncertain models and validated the results in real-world case studies. In addition, several artificial intelligence-based solution techniques are designed to solve the complex nonlinear problem. We determined the most efficient solution approach by solving various test problems. Then, simulated several scenarios to demonstrate the robustness of our results. The results showed that the solutions provided by the offered model significantly reduce energy consumption in different setups. To ensure the reliability of the results, we carried out worst-case sensitivity analyses and found the most critical parameters. The results of the worst-case analyses indicated that the offered robust model is efficient and saves a significant amount of energy comparing to traditional models. It is shown that the provided solution by the presented robust formulation is reliable in all situations and results in the lowest energy and the best machining precision.																	0956-5515	1572-8145															10.1007/s10845-020-01648-0		SEP 2020											
J								Knowledge based approach to ground refuelling optimization of commercial airplanes	EXPERT SYSTEMS										aircraft; expert system; genetic algorithm; knowledge engineering; refuelling	MASS ESTIMATION; AIRCRAFT	This work aims to establish a general and optimized procedure for the initial refuelling of commercial airplanes, as this loading process is strongly related to safety and energy saving issues. The on-ground refuelling is addressed as an optimization problem whose cost function involves expert knowledge about constraints and factors that influence the aircraft stability and performance. Several heterogeneous criteria (fuelling time, structural load, flow transfers, etc.) have been considered and weighted accordance to its importance in terms of stability. This allows us to adapt the strategy to any type and planned trip of the airplane. The priority is the positioning of the centre of mass of the civil aircraft within safety and manoeuvrability margins, and near the optimal position. Evolutive algorithms are applied, keeping feasible solutions by modifying genetic operators. As a case of study, the initial refuelling of a long range type commercial aircraft, the Airbus A330-200, is analysed. Simulation results have proved this methodology to be efficient and optimal. Even more, this heuristic and general approach improves the traditional solution that follows a set of pre-defined rules that are specific for each type of aircraft.																	0266-4720	1468-0394														e12631	10.1111/exsy.12631		SEP 2020											
J								Fingerprint enhancement using multi-scale classification dictionaries with reduced dimensionality	IET BIOMETRICS										image enhancement; principal component analysis; learning (artificial intelligence); image representation; fingerprint identification; reduced dimensionality; fingerprint enhancement method; sparse representation; learned multiscale classification dictionaries; multiscale dictionary; enhancement quality; quality grading scheme; multiscale composite windows; high-quality spectra diffuse; low-quality fingerprint patches; spectra quality	GABOR FILTER; SPARSE REPRESENTATION; DIFFUSION; ALGORITHM; SCHEME; DESIGN	In order to improve the quality of fingerprint with a large noise, this study proposes a fingerprint enhancement method by using a sparse representation of learned multi-scale classification dictionaries with reduced dimensionality. The multi-scale dictionary is used to balance the contradiction between the accuracy and the anti-noise ability, which is an ideal solution to reconcile the demands of enhancement quality and computational performance. The principal component analysis is applied in the authors' technique for dimension reduction of multi-scale classification dictionaries. Under the quality grading scheme and multi-scale composite windows, the fingerprint patches are enhanced by using a sparse representation of learned multi-scale classification dictionaries with reduced dimensionality according to their priorities. In addition, the multi-scale composite windows help the more high-quality spectra diffuse into the low-quality fingerprint patches and this can greatly improve the spectra quality of them. Experimental results and comparisons on FVC 2000 and FVC 2004 databases are reported. And it shows that the proposed method yields better results in terms of the robustness of fingerprint enhancement as compared with the latest techniques. Moreover, the results show that the proposed algorithm can obtain better identification performance.																	2047-4938	2047-4946				SEP	2020	9	5					194	204		10.1049/iet-bmt.2019.0121													
J								Two-tiered face verification with low-memory footprint for mobile devices	IET BIOMETRICS										smart phones; feature extraction; mobile computing; face recognition; convolution; biometrics (access control); neural nets; facial verification method; mobile environment; two-tiered procedure; hand-crafted features; convolutional neural network; CNN; device owner; verification task; hybrid-image input; face images; art face verification methods; recent smartphone models; existing face verification CNNs; low-memory footprint; mobile devices; personal data; knowledge-based procedures; recently biometric traits; secure authentication; effortless authentication	RECOGNITION	Mobile devices have their popularity and affordability greatly increased in recent years. As a consequence of their ubiquity, these devices now carry all sorts of personal data that should be accessed only by their owner. Even though knowledge-based procedures are still the main methods to secure the owner's identity, recently biometric traits have been employed for more secure and effortless authentication. In this work, the authors propose a facial verification method optimised to the mobile environment. It consists of a two-tiered procedure that combines hand-crafted features and a convolutional neural network (CNN) to verify if the person depicted in a photograph corresponds to the device owner. To train a CNN for the verification task, the authors propose a hybrid-image input, which allows the network to process encoded information of a pair of face images. The proposed experiments show that the solution outperforms state of the art face verification methods, providing a 4x speedup when processing an image in recent smartphone models. Additionally, the authors show that the two-tiered procedure can be coupled with existing face verification CNNs improving their accuracy and efficiency. They also present a new data set of selfie pictures - RECOD Selfie data set - that hopefully will support future research in this scenario.																	2047-4938	2047-4946				SEP	2020	9	5					205	215		10.1049/iet-bmt.2020.0031													
J								Constant-Q magnitude-phase coefficients extraction for synthetic speech detection	IET BIOMETRICS										speech synthesis; principal component analysis; feature extraction; discrete cosine transforms; magnitude-phase coefficients extraction; synthetic speech detection; useful discriminative information; magnitude-phase spectrum; phase-spectra information; CMPC; uniform resampling; long-term window; short-term window; feature dimensions; constant-Q magnitude-phase coefficients extraction; CQT; MPS	SPEAKER VERIFICATION	Previous works in synthetic speech detection have focused on features based on magnitude or phase spectrum. In this study, to extract useful discriminative information for synthetic speech detection, the authors propose a feature based on magnitude-phase spectrum (MPS), combining magnitude- and phase-spectra information. The proposed feature is termed as constant-Q magnitude-phase coefficient (CMPC), which is obtained by combining constant-Q transform (CQT), MPS, uniform resampling, and discrete cosine transform. The CQT used in this study is a long-term window transform, which can provide the basis for CMPC to capture important artefacts of synthetic speech. Such artefacts are obtained using a unit selection algorithm, which have difficulties when based on the short-term window transform. Uniform resampling aims to convert MPS from the octave domain into the linear domain. The discrete cosine transform is used when extracting principal components to remove correlations among the feature dimensions. The experimental results on AVspoof and ASVspoof 2015 corpora show that CMPC performs better than some commonly used features based on magnitude or phase spectrum alone. Their system based on CMPC outperforms many known systems.																	2047-4938	2047-4946				SEP	2020	9	5					216	221		10.1049/iet-bmt.2018.5100													
J								Undergraduate teaching audit and evaluation using an extended MABAC method underq-rung orthopair fuzzy environment	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										ITARA method; MABAC method; q-rung orthopair fuzzy set; teaching audit and evaluation; teaching quality evaluation	GROUP DECISION-MAKING; INTERVAL TYPE-2; LOGIC SYSTEMS; QUALITY; SELECTION; AGGREGATION; FRAMEWORK; MODEL; AHP	Undergraduate teaching audit and evaluation (UTAE) is a new type of evaluation pattern, which is extremely important for a university to improve its quality assurance system and enhance teaching quality. Selecting an optimal university for benchmarking through UTAE to promote the quality of teaching can be regarded as a complex multicriteria decision making (MCDM) problem. Furthermore, in the process of UTAE, experts' evaluations over the teaching quality of universities are often imprecise and fuzzy due to the subjective nature of human thinking. In this paper, we propose a new UTAE approach based onq-rung orthopair fuzzy sets and the multiattribute border approximation area comparison (MABAC) method for evaluating and selecting the best university for benchmarking. The introduced method deals with the linguistic assessments given by experts by usingq-ROFSs, assigns the weights of audit elements based on the indifference threshold-based attribute ratio analysis method, and acquires the ranking of universities with an extended MABAC method. The feasibility and effectiveness of the proposedq-rung orthopair fuzzy MABAC method is demonstrated through a realistic UTAE example. Results show that the UTAE method being proposed is valid and practical for UTAE.																	0884-8173	1098-111X				DEC	2020	35	12					1912	1933		10.1002/int.22278		SEP 2020											
J								Optimal mixed block withholding attacks based on reinforcement learning	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										block withholding attacks; Markov chain; reinforcement learning; strategic behaviors	GAME	The vulnerabilities in cryptographic currencies facilitate the adversarial attacks. Therefore, the attackers have incentives to increase their rewards by strategic behaviors. Block withholding attacks (BWH) are such behaviors that attackers withhold blocks in the target pools to subvert the blockchain ecosystem. Furthermore, BWH attacks may dwarf the countermeasures by combining with selfish mining attacks or other strategic behaviors, for example, fork after withholding (FAW) attacks and power adaptive withholding (PAW) attacks. That is, the attackers may be intelligent enough such that they can dynamically gear their behaviors to optimal attacking strategies. In this paper, we propose mixed-BWH attacks with respect to intelligent attackers, who leverage reinforcement learning to pin down optimal strategic behaviors to maximize their rewards. More specifically, the intelligent attackers strategically toggle among BWH, FAW, and PAW attacks. Their main target is to fine-tune the optimal behaviors, which incur maximal rewards. The attackers pinpoint the optimal attacking actions with reinforcement learning, which is formalized into a Markov decision process. The simulation results show that the rewards of the mixed strategy are much higher than that of honest strategy for the attackers. Therefore, the attackers have enough incentives to adopt the mixed strategy.																	0884-8173	1098-111X				DEC	2020	35	12					2032	2048		10.1002/int.22282		SEP 2020											
J								Automation for the artisanal economy: enhancing the economic and environmental sustainability of crafting professions with human-machine collaboration	AI & SOCIETY										Human-machine collaboration; Artisanal economy; Generative justice; Industrial symbiosis; Ethnocomputing	JOB DECISION LATITUDE; DESIGN; CONSUMPTION; DEMANDS; HEALTH	Artificial intelligence (AI) is poised to eliminate millions of jobs, from finance to truck driving. But artisanal products (e.g., handmade textiles) are valued precisely because of their human origins, and thus have some inherent "immunity" from AI job loss. At the same time, artisanal labor, combined with technology, could potentially help to democratize the economy, allowing independent, small-scale businesses to flourish. Could AI, robotics and related automation technologies enhance the economic viability and environmental sustainability of these beloved crafting professions, perhaps even expanding their niche to replace some job loss in other sectors? In this paper, we compare the problems created by the current mass production economy and potential solutions from an artisanal economy. In doing so, the paper details the possibilities of utilizing AI to support hybrid forms of human-machine production at the microscale; localized and sustainable value chains at the mesoscale; and networks of these localized and sustainable producers at the macroscale. In short, a wide range of automation technologies are potentially available for facilitating and empowering an artisanal economy. Ultimately, it is our hope that this paper will facilitate a discussion on a future vision for more "generative" economic forms in which labor value, ecological value and social value can circulate without extraction or alienation.																	0951-5666	1435-5655				SEP	2020	35	3					595	609		10.1007/s00146-019-00915-w													
J								Indowordnet's help in Indian language machine translation	AI & SOCIETY										Machine translation; Statistical machine translation; Lexical resources; Indowordnet; Indian-Indian language machine translation; English-Indian language machine translation		Languages with insufficient digitally available resources, such as, Indian-Indian and English-Indian language Machine Translation (MT) system developments, faces the difficulty to translate various lexical phenomena. In this paper, we present our work on a comparative study of 440 phrase-based statistical trained models for 110 language pairs across 11 Indian languages. We have developed 110 baseline statistical machine translation systems. Then, we have augmented the training corpus with Indowordnet synset word entries of lexical database and further trained 110 models on top of the baseline system. We have done a detailed performance comparison using various evaluation metrics such as BLEU score, METEOR, and TER. We observed significant improvement in evaluations of translation quality across all the 440 models after using the Indowordnet. These experiments give a detailed insight in two ways: (1) usage of lexical database with synset mapping for resource poor languages and (2) efficient usage of Indowordnet synset mapping. Moreover, synset mapped lexical entries helped the SMT system to handle the ambiguity to a great extent during the translation.																	0951-5666	1435-5655				SEP	2020	35	3					689	698		10.1007/s00146-019-00907-w													
J								Anthropomorphizing AlphaGo: a content analysis of the framing of Google DeepMind's AlphaGo in the Chinese and American press	AI & SOCIETY										AlphaGo; AI; Weiqi; Framing; Media coverage	POLITICS; SCIENCE; ROBOTS; AI	This article conducts a mixed-method content analysis of Chinese and American news media coverage of Google DeepMind's Go playing computer program, AlphaGo. Drawing on humanistic approaches to artificial intelligence, combined with an empirically rigorous content analysis, it examines the differences and overlap in coverage by the Chinese and American press in their accounts of AlphaGo, and its historic match with Korea's Lee Sedol in March, 2016. The event was not only followed intensely in China, but also made the front page ofThe New York Times.This article finds that the Chinese press was more likely than the American press to frame AlphaGo as non-threatening, which the authors attribute to cultural differences and the two countries' different understandings of Go. In addition to quantitatively identifying similarities and differences in the framing of AlphaGo, this paper also investigates the underlying and evolving contestations over what constitutes the "human" and the "machine." It concludes by discussing the implications of the study's findings as well as outlining avenues for further research.																	0951-5666	1435-5655				SEP	2020	35	3					727	735		10.1007/s00146-019-00908-9													
J								The new AI spring: a deflationary view	AI & SOCIETY																													0951-5666	1435-5655				SEP	2020	35	3					747	750		10.1007/s00146-019-00912-z													
J								A possibility of inappropriate use of gender studies in human-robot Interaction	AI & SOCIETY											NEGATIVE ATTITUDES; PERSONALITY																		0951-5666	1435-5655				SEP	2020	35	3					751	754		10.1007/s00146-019-00913-y													
J								DeepEC: An error correction framework for dose prediction and organ segmentation using deep neural networks	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										deep neural networks; dose prediction; error correction; organ segmentation	RISK; CT	Radiotherapy is an indispensable part of adjuvant therapy for cancer that improves local control, overall survival, and the opportunity for good quality of life. Organ delineation and dose plan design are the key steps in the treatment. Organ delineation controls the area of radiotherapy and dose planning controls its intensity. However, both tasks are time-consuming, exhausting, and subjective, and automated methods are desirable. Although automated methods have been studied, the previous studies either focus on organ segmentation or dose prediction, without considering them from a holistic perspective. In this paper, we treat organ segmentation and dose prediction as similar tasks, and propose an error correction framework to improve their performance based on the same mechanism. The proposed error correction framework consists of a prediction network and a calibration network. The biggest difference between our framework and previous studies is that the state-of-the-art networks can be used as a prediction network or calibration network, and then the performance can be improved by the error correction mechanism. To evaluate the framework, we conducted a series of experiments on dose prediction and organ segmentation. These experimental results show that the framework is superior to other state-of-the-art methods in both tasks.																	0884-8173	1098-111X				DEC	2020	35	12					1987	2008		10.1002/int.22280		SEP 2020											
J								CIADL: cloud insider attack detector and locator on multi-tenant network isolation: an OpenStack case study	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Multi-tenant network isolation; Insider attack detection	VIRTUALIZATION	In cloud networks, edging network virtualization technology is widely adopted to protect tenants with isolated networks mainly from threats inside the cloud. However, since tenants completely rely on cloud service provider's service interface to be aware of their current network policy, malicious admin alone or with concluded tenants is/are fully capable of acquiring any target tenant network data by attacking corresponding policies stored and enforced on the edging end hosts without tenants knowing. Therefore, this paper presents cloud insider attack detector and locator (CIADL) on multi-tenant network isolation for OpenStack. We propose an insider attack threat model with attack category. A layered state model based constructing and attack detection methods are also proposed, enabling efficient policy confliction detection between expected policy on central node and enforcing policy on end hosts. Along with a threat locating method with fine granularity of device policy rules for recovery purpose. We implements the proof of concept system of CIADL, and the experiments and analysis show our method can cover all attack types defined in threat model with low overheads, and scales well with network and policy size and attack number increase. Compared to existing work model with VM-VM state, CIADL state model with NET-NET state gets about 8.5% and 92.3% improvement on construction and verification time costs with most hostile environment (AP = 80%) and largest policy scale (PS = 4000), which suggests CIADL is both efficient and scalable.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3473	3495		10.1007/s12652-019-01471-3													
J								Research on the green total factor productivity and its influencing factors based on system GMM model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Pollution comprehensive index; GTFP; Influencing factors; Malmquist index; System GMM	ECONOMIC-EFFICIENCY; ENERGY EFFICIENCY; CHINA; GROWTH; OUTPUT; INPUT	The entropy method is used to calculate pollution comprehensive index reflecting provincial environmental pollution level. On this basis, the Malmquist productivity index is used to study the regional green total factor productivity (GTFP) in China, and the system generalized moment method is used to explore the influencing factors of GTFP. Firstly, the results show that the provinces with lower environmental pollution comprehensive index are mainly in the western and eastern regions, while the provinces with higher pollution comprehensive index are mainly distributed in the central inland areas. Secondly, GTFP shows an N-type upward trend, which is basically consistent with the trend of total factor productivity. The overall GTFP in the east is on the rise, which is promoted by technical efficiency. The GTFP in the central and west has declined, and the main factor restricting their improvement is technological retrogression. The expansion of production scale in the east and west can contribute to the improvement of GTFP. The central region is at a stage where the scale of returns is not economic, and it is more important for promoting technological progress. Lastly, the industrial structure can significantly inhibit the increase of GTFP, however, the energy consumption structure, FDI and pollution control investment have a significant role in improving GTFP.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3497	3508		10.1007/s12652-019-01472-2													
J								Research on intelligent classification of multi-attribute safety information and determination of operating environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Safety management; Safety information classification; Monte Carlo method; Attribute circle; Similarity analysis	SIGNIFICANT PREDICTOR VARIABLES; ROCK BURST PREDICTION; SYSTEM; MODELS	To analyze the system safety through the field system operation data, especially the system operation information provided by the operators, and to obtain the environment scope of the safe operation of the system, the multi-attribute safety information intelligent mining method is adopted. The new definitions and methods of attribute circle classification are proposed. The improvement is using inner attribute polygon and outer attribute polygon to construct the object attribute area. Monte Carlo method is used to calculate the overlap area of the object attribute area, and define overlap and transform with similarity, which is calculated by the overlap degree and the distance between two points in multi-dimensional space. The object classification reasoning method and system adaptive environment analysis method is established. The improved similarity analysis method is more suitable for computer implementation and intelligent analysis. Based on the previous study, this paper also analyzes the similarity of operation safety evaluation given by 30 operators of an electrical system, and obtains the pair wise similarity. The results show that the algorithm classification results have good correspondence with the original data decision level, and the range of the system safe operation environment is obtained, which can lay a foundation for further study of intelligent classification and system safety operation environment optimization.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3509	3520		10.1007/s12652-019-01474-0													
J								Criteria evaluation and selection in non-native language MBA students admission based on machine learning methods	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; MBA admission selection; Non-native language; Performance prediction	ACADEMIC-PERFORMANCE; WORK EXPERIENCE; PROGRAM; SUCCESS; MODEL; PREDICTORS; APPLICANTS; EDUCATION; FAIRNESS; AVERAGE	Although the research on student selection criteria has been very rich up to now, the role of the level of foreign language played in the admission selection of a non-native spoken program is still receiving little attention. This study intends to explore the issue through three research methods: (1) two-sample test of a hypothesis; (2) multiple linear regression analysis; (3) machine learning algorithms (Ridge regression, SVM, Random forest, GBDT). The case about 549 students enrolled in the Shanghai International MBA Program in China from 2007 to 2014 was used as empirical research samples. Through three methods of analysis and comparison, it was found that Oral English fluency played a key role in the admission selection of the English spoken MBA program in China. It is confirmed that the criteria, such as Rank of the graduated university, Company Nature, Latest Highest Degree, Math Exam, Sponsor (Tuition provider) and Stress management, have very good effect in predicting the final grades of students when graduation. This study also shows that the methods based on machine learning algorithm modeling such as ridge regression and SVM are suitable for student selection decision modeling.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3521	3533		10.1007/s12652-019-01490-0													
J								Research on the tenacity survivability of wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network (WSN); Survivability; Tenacity; SAPSO		Nodes of wireless sensor networks due to environmental influences and deliberate attacks by nodes in the network, nodes of wireless sensor network are prone to death. In this paper, based on the idea of algorithm fusion, this paper proposes a research method for survivability of wireless sensor networks based on tenacity. A hybrid algorithm based on simulated annealing algorithm and particle swarm optimization algorithm is used to calculate the value of the tenacity of the network node. The optimized algorithm reduces the time and complexity of calculating tenacity. In this paper, the wireless sensor network is simulated and verified. The experimental results demonstrate the feasibility of evaluating the survivability of wireless sensor networks. The proposed method can effectively improve network lifetime and data transmission capability.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3535	3544		10.1007/s12652-019-01491-z													
J								Construction of ontology for auto-interpretable tolerance semantics in skin model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Skin model; Ontology; Tolerance semantics; GeoSpelling; OWL2 DL; SWRL	GEOMETRIC TOLERANCES; MATHEMATICAL-MODEL; POLYHEDRAL OBJECTS; REPRESENTATION; INFORMATION; DIMENSIONS; FEATURES; DESIGN	Manufacturing and inspection in model-based engineering, achieving computer-readable and -interpretable tolerance specification representations, and maintaining unambiguous and consistent semantics throughout the life cycle of product design are issues that have attracted the attention of researchers and require urgent actions. An ontology method based on skin tolerance model is proposed in this study to address the abovementioned problems. Tolerance specification semantics described by the GeoSpelling language can be consistent at different stages of product development. In the ontology as a formal shared concept model, the knowledge described by the web ontology language and the semantic web rule language can be read and interpreted automatically via computer. The ontology is based on a rigorous description logic and describes tolerance information with clear and unambiguous semantics. The inference algorithm of this ontology can be used for ontology consistency checking, knowledge reasoning, and semantic query. The proposed method utilizes the advantages of ontology artificial intelligence technology and GeoSpelling language to describe tolerance semantics. Thus, this method is suitable for consistent semantic data exchange in different heterogeneous systems and automatic interpretation of tolerance semantics in smart manufacturing.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3545	3558		10.1007/s12652-019-01497-7													
J								A neutral mutated operator applied for DE algorithms	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Differential evolution; Neutral mutation operator; Population diversity; Neutral theory of molecular evolution; IEEE bus 57	DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; GLOBAL OPTIMIZATION; INTELLIGENCE; PARAMETERS; TESTS	As an easily used and powerful heuristic search technique based on population, differential evolution (DE) algorithm has been widely used for many optimization and real engineering projects. Similar to other evolutionary algorithms (EA), DE could not avoid from premature convergence due to over concentrated population, which could be called losing population diversity. To improve the performance, a neutral mutation (NM) operator for DE algorithm is proposed. The proposed operator is inspired by neutral theory of molecular evolution, which claims that most mutations are neutral at the level of molecular. The NM operator maintains slightly deleterious trial vectors with a certain probability in the conventional selection operator of DE. At the same time, two control parameters of Neutral Mutation operator are investigated and a dynamic neutral mutation rate tuning strategy is designed. Besides, some of these trial vectors have a chance to be mutated neutrally within the search domain randomly. As a result, the population is diversified with costing negligible function evaluations. Comprehensive experimental results demonstrate that the presented NM operator could improve population diversity to some extent, especially when the population is not divergent at all. Moreover, a real word problem is used to further evaluate NM operator. Also, this operator can be easily used in other EAs to keep population diversity.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3559	3574		10.1007/s12652-019-01498-6													
J								Research on distribution network reconfiguration based on microgrid	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											GENERATION PLACEMENT; SYSTEMS; ALGORITHM; DESIGN; IMPACT	Micro-grids have been considered as a vital part of power system. The distribution system is gradually showing the characteristics of multi-source initiative. The distribution network reconfiguration with microgrid changes the topology of the network by controlling the state of the switch, and optimizes the predetermined indicators under the premise of safe, economic and stable operation. This paper describes a hierarchical distribution network reconfiguration strategy with microgrid, which can reduce the number of operation of the switch and the network loss. This strategy ensures rapid power supply recovery. In the process of reconstruction, this paper uses the immune clonal selection differential evolution algorithm. The simulation examples are included to display the performance of the proposed method.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3607	3615		10.1007/s12652-019-01542-5													
J								Research on application of athlete gesture tracking algorithms based on deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Football players tracking; Improved neural network; Deep learning	NETWORKS	It is difficult to track the posture of players in the course, mainly because of the changing environment and players. In this paper, the improved neural network is used to extract the trajectory characteristics of the athletes in the football player's game video, and the network is trained on a large number of data objects containing similarity objects, which improves the ability of the algorithm to distinguish the athlete's trajectory. A scheme of soccer attitude tracking based on twin neural network. The experimental results show that the algorithm has a good effect in the field of football and the accuracy is over 90% and the Siamese neural network is better than a traditional convolutional neural network.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3649	3657		10.1007/s12652-019-01575-w													
J								Differential evolution optimization of intertwining logistic map-DNA based image encryption technique	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ILM; DE; DNA; Image encryption	HYBRID GENETIC ALGORITHM; SEQUENCE OPERATION; CHAOTIC MAP; CRYPTANALYSIS; CIPHERS; PERMUTATION; MODEL	Differential evolution (DE) is a powerful evolutionary algorithms, widely applied in different fields of science and engineering for solving the problem of optimization. Since image encryption has been viewed as an interesting research topic by many experts and innumerable methods to encrypt images have emerged, currently, the focus is on obtaining optimized images. The paper presents a novel image encryption scheme that uses intertwining logistic map (ILM), DNA encoding and DE optimization. The proposed approach is based on three phases: permutation involving ILM, diffusion engaging DNA and optimization using DE. Parameters like entropy, key sensitivity, secret key space, unified average change in intensity (UACI), correlation coefficient -vertical, horizontal and diagonal, and number of pixel change rate have been evaluated to test the efficiency of the proposed method. The paper also compares this performance with that of the genetic algorithms (GA), used previously for optimization. The significance of this approach is enhancing entropy, the essential characteristic of randomness, resisting against numerous statistical and differential attacks and generating good experimental results. The main contribution of this paper is to present the efficiency of DE in image optimization and exhibit how DE is better than GA.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3771	3786		10.1007/s12652-019-01580-z													
J								Reconfiguring IVHF-TOPSIS decision making method with parameterized reference solutions and a novel distance for corporate carbon performance evaluation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Interval-valued hesitant fuzzy (IVHF) set; Multi-criteria decision-making (MCDM); Parameterized reference solutions; Modified Wave-Hedges measurement; TOPSIS; Carbon performance evaluation	HESITANT FUZZY-SETS; SIMILARITY MEASURES; PREFERENCE RELATIONS; SUPPLIER SELECTION; MODEL; INFORMATION; INDUSTRY; VIKOR; AHP; ART	The hesitant and imprecise uncertainties widely exist in real decision-making problems. For solving the class of problems, this work is aimed at reconfiguring a novel method under the TOPSIS framework to solve the general uncertain decision problem that with both the ratings of alternatives and weights of criteria represented by interval-valued hesitant fuzzy (IVHF) information. Three novelties are proposed to support the reconfigured method. First, a parameterized approach for generating apt positive and negative IVHF reference solutions is proposed, which permits decision makers (DMs) to express their different aspiration strengths for fitting complex and uncertain decision scenarios and situations. Second, a novel distance for IVHF elements is constructed based on the modification of Wave-Hedges measurement to address the defects of the previous hesitant distances and to measure the separations of alternatives in TOPSIS. Third, a nine-step solution procedure of IVHF-TOSPSIS method is reconfigured to solve effectively the general problem that the ratings and weights are expressed with IVHF information. Finally, the reconfigured method is exploited to settle the carbon performance evaluation of industrial firms and some sensitivity and comparison analyses are conducted to validate the proposed method.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3811	3832		10.1007/s12652-019-01603-9													
J								Intelligent algorithm in a smart wearable device for predicting and alerting in the danger of vehicle collision	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Active protection algorithm; Vehicle impact; Precise alarm; Wearable device	PEDESTRIAN INJURY RISK; TIME; DRIVERS; SAFETY; IDENTIFICATION; SEVERITY; BEHAVIOR; CRASHES; SYSTEM; AGE	We have designed a smart wearable device to protect actively pedestrian from impact of vehicle. This device consists of several modules, including radar sensor, transmission module, alarm module and intelligent security program module. In the dominant program module, the safety intelligent algorithm based on fuzzy comprehensive evaluation and BP neural network is proposed. From the perspective of a pedestrian, the moving data sensed by radar, combining with multiple effects of local surroundings, people and vehicles, road and transportation situation, weather, physiological and psychological situation of the pedestrian, are used as the data source for the algorithm. Based on the weight of the index determined by BP neural network, we use the fuzzy comprehensive evaluation to calculate the vehicle risk index. The smart wearable device can effectively predict and warn the situation of vehicles impacting pedestrians. The simulation has confirmed the accuracy of the prewarning algorithm under various conditions.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3841	3852		10.1007/s12652-019-01609-3													
J								Two level filtering mechanism to detect phishing sites using lightweight visual similarity approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Phishing; Anti-phishing; Visual similarity; Blacklist; Heuristics; Ensemble	WEBPAGES; EFFICIENT; FEATURES	The visual similarity-based techniques detect the phishing sites based on the similarity between the suspicious site and the existing database of resources such as screenshots, styles, logos, favicons etc. These techniques fail to detect phishing sites which target non-whitelisted legitimate domain or when phishing site with manipulated whitelisted legitimate content is encountered. Also, these techniques are not well adaptable at the client-side due to their computation and space complexity. Thus there is a need for light weight visual similarity-based technique detecting phishing sites targeting non-whitelisted legitimate resources. Unlike traditional visual similarity-based techniques using whitelists, in this paper, we employed a light-weight visual similarity based blacklist approach as a first level filter for the detection of near duplicate phishing sites. For the non-blacklisted phishing sites, we have incorporated a heuristic mechanism as a second level filter. We used two fuzzy similarity measures, Simhash and Perceptual hash for calculating the similarity score between the suspicious site and existing blacklisted phishing sites. Each similarity measure generates a unique fingerprint for a given website and also differs with less number of bits with a similar website. All three fingerprints together represent a website which undergoes blacklist filtering for the identification of the target website. The phishing sites which bypassed from the first level filter undergo second level heuristic filtering. We used comprehensive heuristic features including URL and source code based features for the detection of non-blacklisted phishing sites. The experimental results demonstrate that the blacklist filter alone is able to detect 55.58% of phishing sites which are either replicas or near duplicates of existing phishing sites. We also proposed an ensemble model with Random Forest (RF), Extra-Tree and XGBoost to evaluate the contribution of both blacklist and heuristic filters together as an entity and the model achieved a significant accuracy of 98.72% and Matthews Correlation Coefficient (MCC) of 97.39%. The proposed model is deployed as a chrome extension named as BlackPhish to provide real time protection against phishing sites at the client side. We also compared BlackPhish with the existing anti-phishing techniques where it outperformed existing works with a significant difference in accuracy and MCC.																	1868-5137	1868-5145				SEP	2020	11	9			SI		3853	3872		10.1007/s12652-019-01637-z													
J								Dimensionality reduction of multielement glass evidence to calculate likelihood ratios	JOURNAL OF CHEMOMETRICS										dimensionality reduction; forensic glass comparisons; likelihood ratio; multivariate data analysis		Dimensionality reduction of multivariate elemental concentrations of glass is reported for computing likelihood ratios (LRs). TheLRs calculated using principal component analysis (PCA) and a post hoc calibration step result in very low (<1%) false inclusions when comparing glass samples known to originate from different sources and very low (<1%) false exclusions when comparing glass samples known to originate from the same source. TheLRs calculated using the novel PCA approach are compared with previously reportedLRs calculated using a more computationally intensive Multivariate Kernel (MVK) model followed by a calibration step using a pool adjacent violators (PAV) algorithm. In both cases, the calibratedLRs limited the magnitude of the misleading evidence, providing only weak to moderate support for the incorrect hypotheses. Most of the different pairs that were found to be falsely included were explained by chemical relatedness (same manufacturer of the glass sources in very close time interval between manufacture). The computation ofLRs using dimensionality reduction of elemental concentrations using PCA may transfer to other multivariate data-generating evidence types.																	0886-9383	1099-128X														e3298	10.1002/cem.3298		SEP 2020											
J								Freight transportation broker agent based on constraint logic programming	EVOLVING SYSTEMS										Constraint logic programming; Pickup and delivery problem; Search and optimization; Intelligent agent	OPTIMIZATION; SEARCH	We propose an agent-based freight brokering system that provides an intelligent logistics brokerage service focusing on the transport activity for the efficient allocation of transport resources (vehicles or trucks) to the transport applications. The freight broker has the role to coordinate transportation arrangements of transport customers with transport resource providers. In this paper we focus on the fundamental function of this business that aims to find available trucks and to define their feasible routes for transporting requested customer loads. Our main achievement is the proposal of a knowledge-based freight broker based on agents and constraint programming. We describe the multi-agent architecture and the interaction protocol of the freight broker. The brokering function is defined as a special type of vehicle routing with pickup and delivery problem. This function is achieved by proposing an optimization agent based on constraint logic programming. We present in detail our novel declarative optimization model, as well as the components of the optimization agent. This model was implemented and evaluated using the state-of-the-art ECLiPSe-CLP engine. The experimental results that we obtained for sample benchmark problems are promising and they support the effectiveness of our approach in a practical setting.																	1868-6478	1868-6486				SEP	2020	11	3			SI		363	382		10.1007/s12530-018-9230-3													
J								Particle swarm optimization algorithms for autonomous robots with deterministic leaders using space filling movements	EVOLVING SYSTEMS										Autonomous agents; Space filling curves; Particle swarm optimization; Deterministic leaders; Application		In this work the swarm behavior principles of Craig W. Reynolds are combined with deterministic traits. This is done by using leaders with motions based on space filling curves like Peano and Hilbert. Our goal is to evaluate how the swarm of agents works with this approach, supposing the entire swarm will better explore the entire space. Therefore, we examine different combinations of Peano and Hilbert with the already known swarm algorithms and test them in a practical challenge for the harvesting of manganese nodules on the sea ground with the use of autonomous agents. We run experiments with various settings, then evaluate and describe the results. In the last section some further development ideas and thoughts for the expansion of this study are considered.																	1868-6478	1868-6486				SEP	2020	11	3			SI		383	396		10.1007/s12530-018-9245-9													
J								Graph communities in Neo4j Four algorithms at work	EVOLVING SYSTEMS										Community discovery; CNM; Louvain; Edge betweeness; Walktrap; Graph databases; Higher order data; Neo4j; Twitter4j		Community discovery is an essential topic in social network analysis since it provides a way for recursively decomposing a large social graph to easily interpretable subgraphs. The implementation of four major community discovery algorithms, namely the Newman-Girvan or Edge Betweeness, the Walktrap, the Louvain, and the CNM as Java analytics over Neo4j is described. Their correctness was evaluated functionally in two real Twitter graphs with vastly different characteristics. This was done on the grounds that a successful structural graph partitioning should eventually be reflected in the network functionality domain. Additionally, most real world graphs lack a list of ground truth communities, rendering a structural verification difficult, while functionality can be easily observed in most cases. Naturally, this renders the evaluation network-specific, as different social networks have different operational characteristics. The primary algorithmic finding was that the Louvain algorithm yields Twitter communities whose distribution size matches closer, in terms of the Kullback-Leibler divergence, the tweet and retweet distributions, with Newman-Girvan, Walktrap, and CNM following in that order.																	1868-6478	1868-6486				SEP	2020	11	3			SI		397	407		10.1007/s12530-018-9244-x													
J								Weakly supervised multilabel classification for semantic interpretation of endoscopy video frames	EVOLVING SYSTEMS										Endoscopy; Video analysis; Lesion detection; Weakly supervised learning; Multi-label classification; Bag-of-words; Convolutional neural networks	BOWEL CAPSULE ENDOSCOPY; PERFORMANCE	Several studies have addressed the problem of abnormality detection in medical images using computer-based systems. The impact of such systems in clinical practice and in the society can be high, considering that they can contribute to the reduction of medical errors and the associated adverse events. Today, most of these systems are based on binary classification algorithms that are "strongly" supervised, in the sense that the abnormal training images need to be annotated in detail, i.e., with pixel-level annotations indicating the location of the abnormalities. However, this approach usually does not take into account the diversity of the image content, which may include a variety of structures and artifacts. In the context of gastrointestinal video-endoscopy, addressed in this study, the semantics of the normal contents of the endoscopic video frames include normal mucosal tissues, bubbles, debris and the hole of the lumen, whereas the abnormal video frames may include additional semantics corresponding to lesions or blood. This observation motivated us to investigate various multi-label classification methods, aiming to a richer semantic interpretation of the endoscopic images. Among them, an image-saliency enabled bag-of-words approach and a convolutional neural network architecture enabling multi-scale feature extraction (MM-CNN) are presented. Weakly-supervised learning is implemented using only semantic-level annotations, i.e., meaningful keywords, thus, avoiding the need for the resource demanding pixelwise annotation of the training images. Experiments were performed on a diverse set of wireless capsule endoscopy images. The results of the experiments validate that the weakly-supervised multi-label classification can provide enhanced discrimination of the gastrointestinal abnormalities, with MM-CNN method to provide the best performance.																	1868-6478	1868-6486				SEP	2020	11	3			SI		409	421		10.1007/s12530-018-9236-x													
J								A versatile package recommendation framework aiming at preference score maximization	EVOLVING SYSTEMS										Recommendation system; Package recommendations; Top-kpackages; Collaborative filtering		Package recommendation systems have gained in popularity especially in the tourism domain, where they propose combinations of different types of attractions that can be visited by someone during a city tour. These systems can also be applied in suggesting home entertainment, proper nutrition or academic courses. Such systems must optimize multiple user criteria in tandem, such as preference score, package cost or duration. This work proposes a flexible framework for recommending packages that best fit users' preferences while satisfying several constraints on the set of the valid packages. This is achieved by modeling the relation between the items and the categories these items belong to, aiming at recommending to each user the top-kpackages that cover their preferred categories and the restriction of a maximum package cost. Our contribution includes an optimal and a greedy algorithm, that both outperform a state-of-the-art system and a popularity-based baseline solution. The novelty of the optimal algorithm is that it combines the collaborative filtering predictions with a graph-based model to produce package recommendations. The problem is expressed through a minimum cost flow network and is solved by integer linear programming. The greedy algorithm has a low computational complexity and provides recommendations which are close to the optimal one. An extensive evaluation of the proposed framework has been carried out on six popular recommendation datasets. The results obtained using a set of widely accepted metrics show promising performance. Finally, the formulation of the problem for specific domains has also been addressed.																	1868-6478	1868-6486				SEP	2020	11	3			SI		423	441		10.1007/s12530-018-9231-2													
J								Evolving fuzzy-neural paradigm applied to the recognition and removal of artefactual beats in continuous seismocardiogram recordings	EVOLVING SYSTEMS										Seismocardiogram; Evolving fuzzy neural network; Electrocardiogram		Seismocardiogram (SCG) recording is a novel method for the prolonged monitoring of the cardiac mechanical performance during spontaneous behavior. Thousands of beats are recorded during a variety of physical activities so that the automatic analysis and processing of such data is a challenging task due to the presence of artefactual beats and morphological changes over time that currently request the human expertise. We propose the use of the evolving fuzzy neural network (EFuNN) paradigm for the automatic artifact prediction in the SCG signal. The fuzzy logic processing method can be applied to model the human expertise knowledge using the learning capabilities of an artificial neural network. The evolving capability of the EFuNN paradigm has been applied to solve the issue of the physiological variability of the SGC waveform. Tests have been carried out to validate this approach. The obtained results demonstrate that the EFuNN's evolving capabilities are effective to solve most of the issues related to the learning and to the scalability of the method on an off-the shelf computing platform.																	1868-6478	1868-6486				SEP	2020	11	3			SI		443	452		10.1007/s12530-018-9238-8													
J								Self-supervised autoencoders for clustering and classification	EVOLVING SYSTEMS										Autoencoders; Clustering; Classification; Dimensionality reduction; Deep learning	K-MEANS; FACE RECOGNITION; ALGORITHM	Clustering techniques aim at finding meaningful groups of data samples which exhibit similarity with regards to a set of characteristics, typically measured in terms of pairwise distances. Due to the so-called curse of dimensionality, i.e., the observation that high-dimensional spaces are unsuited for measuring distances, distance-based clustering techniques such as the classick-means algorithm fail to uncover meaningful clusters in high-dimensional spaces. Thus, dimensionality reduction techniques can be used to greatly improve the performance of such clustering methods. In this work, we study Autoencoders as Deep Learning tools for dimensionality reduction, and combine them withk-means clustering to learn low-dimensional representations which improve the clustering performance by enhancing intra-cluster relationships and suppressing inter-cluster ones, in a self-supervised manner. In the supervised paradigm, distance-based classifiers may also greatly benefit from robust dimensionality reduction techniques. The proposed method is evaluated via multiple experiments on datasets of handwritten digits, various objects and faces, and is shown to improve external cluster quality measuring criteria. A fully supervised counterpart is also evaluated on two face recognition datasets, and is shown to improve the performance of various lightweight classifiers, allowing their use in real-time applications on devices with limited computational resources, such as Unmanned Aerial Vehicles (UAVs).																	1868-6478	1868-6486				SEP	2020	11	3			SI		453	466		10.1007/s12530-018-9235-y													
J								A method for the detection of the most suitable fuzzy implication for data applications	EVOLVING SYSTEMS										Fuzzy sets; Fuzzy implications; Fuzzy conditional propositions; Fuzzy set distance; Linguistic variables	LOGIC	Fuzzy implications are widely used in applications where propositional logic is applicable. In cases where a variety of fuzzy implications can be used for a specific application, it is important that the optimal candidate to be chosen in order valuable inference to be drawn for a given set of data. This study introduces a method for detecting the most suitable fuzzy implication among others under consideration by evaluating the metric distance between each implication and the ideal implication for a given data application. The ideal implicationIis defined and used as a reference in order to measure the suitability of fuzzy implications. The method incorporates an algorithm which results in two extreme cases of fuzzy implications regarding their suitability for inference making; the most suitable and the least suitable implications. An example involving five fuzzy implications is included to illustrate the procedure of the method. The results obtained verify that the resulting implication is the optimal operator for inference making for the data.																	1868-6478	1868-6486				SEP	2020	11	3			SI		467	477		10.1007/s12530-018-9233-0													
J								From product recommendation to cyber-attack prediction: generating attack graphs and predicting future attacks	EVOLVING SYSTEMS										Recommender systems; Cyber security; Attack graph generation; Attack prediction; Risk management	COLLABORATIVE FILTERING METHOD; USER SIMILARITY; NETWORK; ACCURACY; MODEL	Modern information society depends on reliable functionality of information systems infrastructure, while at the same time the number of cyber-attacks has been increasing over the years and damages have been caused. Furthermore, graphs can be used to show paths than can be exploited by attackers to intrude into systems and gain unauthorized access through vulnerability exploitation. This paper presents a method that builds attack graphs using data supplied from the maritime supply chain infrastructure. The method delivers all possible paths that can be exploited to gain access. Then, a recommendation system is utilized to make predictions about future attack steps within the network. We show that recommender systems can be used in cyber defense by predicting attacks. The goal of this paper is to identify attack paths and show how a recommendation method can be used to classify future cyber-attacks in terms of risk management. The proposed method has been experimentally evaluated and validated, with the results showing that it is both practical and effective.																	1868-6478	1868-6486				SEP	2020	11	3			SI		479	490		10.1007/s12530-018-9234-z													
J								A genetic algorithm for spatiosocial tensor clustering Exploiting tensorflow potential	EVOLVING SYSTEMS										Multilingual social networks; Multimodal social networks; Cross cultural communication; Language variation models; Tensor clustering; Google TensorFlow; Genetic algorithms; Gene variability; Geolocation data; Spatiosocial data; Humanistic data; Higher order data; H; 2; 8; G; 2; 2; G; 3; M; 1	SOCIAL NETWORKS	Tensor clustering is a knowledge management technique which is well known as a major algorithmic and technological driver behind a broad applications spectrum. The latter ranges from multimodal social media analysis and geolocation processing to analytics tailored for large omic data. However, known exact tensor clustering problems when reduced to tensor factorization are provably NP hard. This is attributed in part to the volume of data contained in a tensor, proportional to the product of its dimensions, as well as to the increased interdependency between the tensor entries across its dimensions. One well studied way to circumvent this inherent difficulty is to resort to heuristics. This article presents an enhanced version of a genetic algorithm tailored for community discovery structure in tensors containing spatiosocial data, namely linguistic and geolocation data. The objective function as well as the chromosome fitness functions by design take into account elements of linguistic propagation models. The genetic operators of selection, crossover, and mutation as well as the newly added double mutation operator work directly on the community level. Moreover, various policies for maintaining gene variability across generations are studied in an extensive simulation powered by Google TensorFlow. As with its predecessor, the proposed genetic algorithm has been applied to a dataset consisting of a large number of Tweets and their associated geolocations from the Grand Duchy of Luxembourg, a historically andde factotrilingual country. The results are compared with those obtained from the original genetic algorithm and their differences are interpreted.																	1868-6478	1868-6486				SEP	2020	11	3			SI		491	501		10.1007/s12530-019-09274-9													
J								An efficient classification approach in imbalanced datasets for intrinsic plagiarism detection	EVOLVING SYSTEMS										Intrinsic plagiarism detection; Stylometry; Supervised learning; Unbalanced training data; SMOTE; PAN Webis		The ever increasing volume of information due to the widespread use of computers and the web has made effective plagiarism detection methods a necessity. Plagiarism can be found in many settings and forms, in literature, in academic papers, even in programming code. Intrinsic plagiarism detection is the task that deals with the discovery of plagiarized passages in a text document, by identifying the stylistic changes and inconsistencies within the document itself, given that no reference corpus is available. The main idea consists in profiling the style of the original author and marking the passages that seem to differ significantly. In this work, we follow a supervised machine learning classification approach. We consider, for the first time, the fact of imbalanced data as a crucial parameter of the problem and experiment with various balancing techniques. Apart from this, we propose some novel stylistic features. We combine our features and imbalanced dataset treatment with various classification methods. Our detection system is tested on the data corpora of PAN Webis intrinsic plagiarism detection shared tasks. It is compared to the best performing detection systems on these datasets, and succeeds the best resulting scores.																	1868-6478	1868-6486				SEP	2020	11	3			SI		503	515		10.1007/s12530-018-9232-1													
J								Mutual information algorithms for optimal attribute selection in data driven partitions of databases	EVOLVING SYSTEMS										Mutual information-based feature selection; Dimensionality reduction; Mining; Online analytical processing (OLAP); Classification and Regression Trees (CART)	DEPENDENCY	Clustering algorithms likek-means, BIRCH, CLARANS and DBSCAN are designed to be scalable and they are developed to discover clusters in the full dimensional space of a database. Nevertheless their characteristics depend upon the size of the database. A database/data warehouse may store terabytes of data. Complex data analysis (mining) may take a very long time to run on the complex dataset. One has to obtain a reduced representation of the dataset that is much smaller in volume-but yet produces the same or almost the same analytical results-in order to accelerate information processing. Reduced representations yield simplified models that are easier to interpret, avoid the curse of dimensionality and enhance generalization by reducing overfitting. Data reduction methods include data cube aggregation, attribute subset selection, fitting data into models, dimensionality reduction, hierarchies as well as other approaches. On the other hand, data-dependent partitions-like the Gessaman's partition and tree-quantization partition-allow for processing different partitions of a dataset separately. Hence parallel processing may be used as an option for big data. Online analytical processing is a practical approach that deals with multi-dimensional queries in DB management. Feature selection is considered as a specific case of a more general paradigm which is called Structure Learning in cases of an outcome associated to a set of attributes. Feature selection aims at selecting a minimum set of features such that the probability distribution of different classes given the values of those features is as close as possible to the original distribution given the values of all features. A mutual information approach based upon representing complex datasets in DB as a minimal set of coherent attribute sets of reduced dimensions is herein proposed. The novelty of the proposed approach consists of employing piecewise analysis of compact clusters in order to increase overall Shannon's mutual information-entropy as a variant to conventional Classification and Regression Trees. Numerical data regarding a test-bed system for anomaly detection are provided in order to illustrate the aforementioned approach.																	1868-6478	1868-6486				SEP	2020	11	3			SI		517	529		10.1007/s12530-018-9237-9													
J								Sonar inspired optimization (SIO) in engineering applications	EVOLVING SYSTEMS										Sonar inspired optimization; Nature-Inspired Intelligent (NII) algorithm; Engineering optimization; Tension; compression spring design; Welded beam design; Pressure vessel	OPTIMAL-DESIGN; ALGORITHM; EVOLUTION	Recently, a new Nature Inspired Intelligent scheme has been proposed and presented, named Sonar Inspired Optimization (SIO). This algorithm is inspired by the SONAR mechanism, which is used by Warships to detect targets and avoid mines. In this paper, improvements have been done to the SIO approach in an attempt to increase the performance of the algorithm. Also, results from experiments in known constrained Engineering applications are presented and discussed. SIO tackles with these problems, managing to overcome the performance of other Nature Inspired metaheuristics, heuristics and mathematical approaches in most of the cases.																	1868-6478	1868-6486				SEP	2020	11	3			SI		531	539		10.1007/s12530-018-9250-z													
J								Appellate Court Modifications Extraction for Portuguese	ARTIFICIAL INTELLIGENCE AND LAW										Natural language processing; Deep Learning; Recurrent Neural Networks; Long Short-Term Memory; Gated Recurrent Units; Machine Learning; Conditional Random Fields; Information extraction; Law; Modificatory provisions	CLASSIFICATION	Appellate Court Modifications Extraction consists of, given an Appellate Court decision, identifying the proposed modifications by the upper Court of the lower Court judge's decision. In this work, we propose a system to extract Appellate Court Modifications for Portuguese. Information extraction for legal texts has been previously addressed using different techniques and for several languages. Our proposal differs from previous work in two ways: (1) our corpus is composed of Brazilian Appellate Court decisions, in which we look for a set of modifications provided by the Court; and (2) to automatically extract the modifications, we use a traditional Machine Learning approach and a Deep Learning approach, both as alternative solutions and as a combined solution. We tackle the Appellate Court Modifications Extraction task, experimenting with a wide variety of methods. In order to train and evaluate the system, we have built theKauaneJuniorcorpus, using public data disclosed by the Appellate State Court of Rio de Janeiro jurisprudence database. Our best method, which is a Bidirectional Long Short-Term Memory network combined with Conditional Random Fields, obtained an F-beta=1 score of 94.79%.																	0924-8463	1572-8382				SEP	2020	28	3					327	360		10.1007/s10506-019-09256-x													
J								Is hybrid formal theory of arguments, stories and criminal evidence well suited for negative causation?	ARTIFICIAL INTELLIGENCE AND LAW										Physical causation; Temporal precedence; Contrastive causation in the law; Argument-based; Causal-based story	CAUSALITY	In this paper, I have two primary goals. First, I show that the causal-based story approach in A hybrid formal theory of arguments, stories and criminal evidence (or Hybrid Theory, for short) is ill suited to negative (or absence) causation. In the literature, the causal-based approach requires that hypothetical stories be causally linked to the explanandum. Many take these links to denote physical or psychological causation, or temporal precedence. However, understanding causality in those terms, as I will show, cannot capture cases of negative causation, which are of interest to the Law. In keeping with this, I also discuss some of the difficulties Hybrid Theory invites by remaining silent on the nature of the causal links. In my second aim, I sketch a way for Hybrid Theory to overcome this problem. By replacing the original, underlying causal structure with contrastive causation in the law, Hybrid Theory can represent reasoning in which the evidence that is appealed to is causally linked via negative causation to the explananda.																	0924-8463	1572-8382				SEP	2020	28	3					361	384		10.1007/s10506-019-09258-9													
J								Automatically Designing CNN Architectures Using the Genetic Algorithm for Image Classification	IEEE TRANSACTIONS ON CYBERNETICS										Computer architecture; Tuning; Genetic algorithms; Evolutionary computation; Manuals; Genetics; Evolution (biology); Convolutional neural networks (CNNs); evolutionary deep learning; genetic algorithms (GAs); neural-network architecture optimization	SELECTION; MODEL	Convolutional neural networks (CNNs) have gained remarkable success on many image classification tasks in recent years. However, the performance of CNNs highly relies upon their architectures. For the most state-of-the-art CNNs, their architectures are often manually designed with expertise in both CNNs and the investigated problems. Therefore, it is difficult for users, who have no extended expertise in CNNs, to design optimal CNN architectures for their own image classification problems of interest. In this article, we propose an automatic CNN architecture design method by using genetic algorithms, to effectively address the image classification tasks. The most merit of the proposed algorithm remains in its "automatic" characteristic that users do not need domain knowledge of CNNs when using the proposed algorithm, while they can still obtain a promising CNN architecture for the given images. The proposed algorithm is validated on widely used benchmark image classification datasets, compared to the state-of-the-art peer competitors covering eight manually designed CNNs, seven automatic + manually tuning, and five automatic CNN architecture design algorithms. The experimental results indicate the proposed algorithm outperforms the existing automatic CNN architecture design algorithms in terms of classification accuracy, parameter numbers, and consumed computational resources. The proposed algorithm also shows the very comparable classification accuracy to the best one from manually designed and automatic + manually tuning CNNs, while consuming fewer computational resources.																	2168-2267	2168-2275				SEPT	2020	50	9					3840	3854		10.1109/TCYB.2020.2983860													
J								SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation	IEEE TRANSACTIONS ON CYBERNETICS										Image segmentation; Feature extraction; Testing; Semantics; Training; Task analysis; Dogs; Few-shot learning; image segmentation; neural networks; siamese network		One-shot image semantic segmentation poses a challenging task of recognizing the object regions from unseen categories with only one annotated example as supervision. In this article, we propose a simple yet effective similarity guidance network to tackle the one-shot (SG-One) segmentation problem. We aim at predicting the segmentation mask of a query image with the reference to one densely labeled support image of the same category. To obtain the robust representative feature of the support image, we first adopt a masked average pooling strategy for producing the guidance features by only taking the pixels belonging to the support image into account. We then leverage the cosine similarity to build the relationship between the guidance features and features of pixels from the query image. In this way, the possibilities embedded in the produced similarity maps can be adopted to guide the process of segmenting objects. Furthermore, our SG-One is a unified framework that can efficiently process both support and query images within one network and be learned in an end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In particular, our SG-One achieves the mIoU score of 46.3%, surpassing the baseline methods.																	2168-2267	2168-2275				SEPT	2020	50	9					3855	3865		10.1109/TCYB.2020.2992433													
J								Efficient Representation and Approximation of Model Predictive Control Laws via Deep Learning	IEEE TRANSACTIONS ON CYBERNETICS										Machine learning; neural networks; predictive control	SYSTEMS; REGULATOR; MPC	We show that artificial neural networks with rectifier units as activation functions can exactly represent the piecewise affine function that results from the formulation of model predictive control (MPC) of linear time-invariant systems. The choice of deep neural networks is particularly interesting as they can represent exponentially many more affine regions compared to networks with only one hidden layer. We provide theoretical bounds on the minimum number of hidden layers and neurons per layer that a neural network should have to exactly represent a given MPC law. The proposed approach has a strong potential as an approximation method of predictive control laws, leading to a better approximation quality and significantly smaller memory requirements than previous approaches, as we illustrate via simulation examples. We also suggest different alternatives to correct or quantify the approximation error. Since the online evaluation of neural networks is extremely simple, the approximated controllers can be deployed on low-power embedded devices with small storage capacity, enabling the implementation of advanced decision-making strategies for complex cyber-physical systems with limited computing capabilities.																	2168-2267	2168-2275				SEPT	2020	50	9					3866	3878		10.1109/TCYB.2020.2999556													
J								Prescribed Performance Adaptive Fuzzy Containment Control for Nonlinear Multiagent Systems Using Disturbance Observer	IEEE TRANSACTIONS ON CYBERNETICS										Disturbance observers; Nonlinear systems; Multi-agent systems; Adaptive control; Robust control; Containment control; dead-zone output; prescribed performance; unknown disturbance	DEAD-ZONE; COOPERATIVE CONTROL; TRACKING	This article focuses on the containment control problem for nonlinear multiagent systems (MASs) with unknown disturbance and prescribed performance in the presence of dead-zone output. The fuzzy-logic systems (FLSs) are used to approximate the unknown nonlinear function, and a nonlinear disturbance observer is used to estimate unknown external disturbances. Meanwhile, a new distributed containment control scheme is developed by utilizing the adaptive compensation technique without assumption of the boundary value of unknown disturbance. Furthermore, a Nussbaum function is utilized to cope with the unknown control coefficient, which is caused by the nonlinearity in the output mechanism. Moreover, a second-order tracking differentiator (TD) is introduced to avoid the repeated differentiation of the virtual controller. The outputs of the followers converge to the convex hull spanned by the multiple dynamic leaders. It is shown that all the signals are semiglobally uniformly ultimately bounded (SGUUB), and the local neighborhood containment errors can converge into the prescribed boundary. Finally, the effectiveness of the approach proposed in this article is illustrated by simulation results.																	2168-2267	2168-2275				SEPT	2020	50	9					3879	3891		10.1109/TCYB.2020.2969499													
J								Distributed Sliding-Mode Tracking Control of Second-Order Nonlinear Multiagent Systems: An Event-Triggered Approach	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Sliding mode control; Tracking; Symmetric matrices; Manifolds; Closed loop systems; Dynamical systems; Consensus tracking; distributed sliding-mode control (SMC); event-triggered control; multiagent systems	NETWORKED CONTROL; CONSENSUS	The event-triggered tracking control problem of second-order multiagent systems in consideration of system nonlinearities is investigated by utilizing the distributed sliding-mode control (SMC) approach. An event-triggered strategy is proposed to decrease the controller sampling frequency and save the network communication resources; the triggering condition is then established for leader-following multiagent systems. In this article, by utilizing the distributed event-based sliding-mode controller, the system state of second-order multiagent systems with system nonlinearities is capable of approaching the integral sliding-mode surface in finite time. A novel integral sliding-mode surface is constructed in this article to guarantee the consensus tracking performance in the existence of system nonlinearities as the state trajectories of second-order integrator systems move on the constructed sliding manifold. By employing the Lyapunov approach, sufficient conditions are deduced to ensure that the consensus tracking performance is obtained for the closed-loop system. Furthermore, it is presented that the triggering scheme can effectively reduce state updates and eliminate the Zeno behavior. A simulation example is provided to testify the validity of our proposed methodology.																	2168-2267	2168-2275				SEPT	2020	50	9					3892	3902		10.1109/TCYB.2019.2963087													
J								Observer-Based Adaptive Fuzzy Tracking Control for Strict-Feedback Nonlinear Systems With Unknown Control Gain Functions	IEEE TRANSACTIONS ON CYBERNETICS										Observers; Backstepping; Adaptive systems; Nonlinear systems; Control design; Closed loop systems; Adaptive backstepping output-feedback controller design; fuzzy-logic systems (FLSs); state observer design; uncertain nonlinear systems	NEURAL-NETWORK CONTROL; BACKSTEPPING CONTROL; ROBUST-CONTROL	This article investigates the adaptive fuzzy output-feedback backstepping control design problem for uncertain strict-feedback nonlinear systems in the presence of unknown virtual and actual control gain functions and unmeasurable states. A fuzzy state observer is designed via fuzzy-logic systems, thus the unmeasurable states are estimated based on the designed fuzzy state observer. By constructing the logarithm Lyapunov functions and incorporating the property of the fuzzy basis functions and bounded control design technique into the adaptive backstepping recursive design, a novel observer-based adaptive fuzzy output-feedback control method is developed. The proposed fuzzy adaptive output-feedback backstepping control scheme can remove the restrictive assumptions in the previous literature that the virtual control gains and actual control gain functions must be constants. Furthermore, it can make the control system be semiglobally uniformly ultimately boundedness (SGUUB) and keep the observer and tracking errors to remain in a small neighborhood of the origin. The numerical simulation example is presented to validate the effectiveness of the proposed control scheme and theory.																	2168-2267	2168-2275				SEPT	2020	50	9					3903	3913		10.1109/TCYB.2020.2977175													
J								Discriminative and Geometry-Aware Unsupervised Domain Adaptation	IEEE TRANSACTIONS ON CYBERNETICS										Data models; Manifolds; Task analysis; Training; Benchmark testing; Analytical models; Labeling; Data distribution matching; data manifold geometric structure alignment; discriminative learning; domain adaptation (DA); transfer learning; visual classification	KERNEL	Domain adaptation (DA) aims to generalize a learning model across training and testing data despite the mismatch of their data distributions. In light of a theoretical estimation of the upper error bound, we argue, in this article, that an effective DA method for classification should: 1) search a shared feature subspace where the source and target data are not only aligned in terms of distributions as most state-of-the-art DA methods do but also discriminative in that instances of different classes are well separated and 2) account for the geometric structure of the underlying data manifold when inferring data labels on the target domain. In comparison with a baseline DA method which only cares about data distribution alignment between source and target, we derive three different DA models for classification, namely, close yet discriminative DA (CDDA), geometry-aware DA (GA-DA), and discriminative and GA-DA (DGA-DA), to highlight the contribution of CDDA based on 1), GA-DA based on 2), and, finally, DGA-DA implementing jointly 1) and 2). Using both the synthetic and real data, we show the effectiveness of the proposed approach which consistently outperforms the state-of-the-art DA methods over 49 image classification DA tasks through eight popular benchmarks. We further carry out an in-depth analysis of the proposed DA method in quantifying the contribution of each term of our DA model and provide insights into the proposed DA methods in visualizing both real and synthetic data.																	2168-2267	2168-2275				SEPT	2020	50	9					3914	3927		10.1109/TCYB.2019.2962000													
J								Sampled-Data State-Feedback Stabilization of Probabilistic Boolean Control Networks: A Control Lyapunov Function Approach	IEEE TRANSACTIONS ON CYBERNETICS										Stability analysis; Probabilistic logic; Switches; Lyapunov methods; Matrix converters; Controllability; Control Lyapunov function (CLF); probabilistic Boolean control network (PBCN); sampled-data state-feedback control; semitensor product (STP); set stabilization	SET STABILIZATION; STABILITY; CONTROLLABILITY; CONSENSUS; SYSTEMS	This article investigates the partial stabilization problem of probabilistic Boolean control networks (PBCNs) under sample-data state-feedback control (SDSFC) with a control Lyapunov function (CLF) approach. First, the probability structure matrix of the considered PBCN is represented by a Boolean matrix, based on which, a new algebraic form of the system is obtained. Second, we convert the partial stabilization problem of PBCNs into the global set stabilization one. Third, we define CLF and its structural matrix under SDSFC. It is found that the existence of a CLF is equivalent to that of SDSFC. Then, a necessary and sufficient condition is obtained for the existence of CLF under SDSFC, based on which, all possible sample-data state-feedback controllers and corresponding structural matrices of CLF are designed by two different methods. Finally, examples are given to illustrate the efficiency of the obtained results.																	2168-2267	2168-2275				SEPT	2020	50	9					3928	3937		10.1109/TCYB.2019.2932914													
J								Wavelet Frame-Based Fuzzy C-Means Clustering for Segmenting Images on Graphs	IEEE TRANSACTIONS ON CYBERNETICS										Fuzzy C-means (FCM) algorithm; image on graphs; image segmentation; spatial information; tight wavelet frames	MEANS ALGORITHM; LOCAL INFORMATION; NOISE REMOVAL; SEGMENTATION; RESTORATION; SURFACES; FCM	In recent years, image processing in a Euclidean domain has been well studied. Practical problems in computer vision and geometric modeling involve image data defined in irregular domains, which can be modeled by huge graphs. In this paper, a wavelet frame-based fuzzy C-means (FCM) algorithm for segmenting images on graphs is presented. To enhance its robustness, images on graphs are first filtered by using spatial information. Since a real image usually exhibits sparse approximation under a tight wavelet frame system, feature spaces of images on graphs can be obtained. Combining the original and filtered feature sets, this paper uses the FCM algorithm for segmentation of images on graphs contaminated by noise of different intensities. Finally, some supporting numerical experiments and comparison with other FCM-related algorithms are provided. Experimental results reported for synthetic and real images on graphs demonstrate that the proposed algorithm is effective and efficient, and has a better ability for segmentation of images on graphs than other improved FCM algorithms existing in the literature. The approach can effectively remove noise and retain feature details of images on graphs. It offers a new avenue for segmenting images in irregular domains.																	2168-2267	2168-2275				SEPT	2020	50	9					3938	3949		10.1109/TCYB.2019.2921779													
J								Weakly Supervised Deep Learning for Whole Slide Lung Cancer Image Analysis	IEEE TRANSACTIONS ON CYBERNETICS										Cancer; Lung; Feature extraction; Tumors; Task analysis; Supervised learning; Image analysis; Deep learning; histology image analysis; weakly supervised learning; whole slide images (WSIs)	PATHOLOGY IMAGES; CLASSIFICATION; SEGMENTATION; HISTOLOGY; FEATURES	Histopathology image analysis serves as the gold standard for cancer diagnosis. Efficient and precise diagnosis is quite critical for the subsequent therapeutic treatment of patients. So far, computer-aided diagnosis has not been widely applied in pathological field yet as currently well-addressed tasks are only the tip of the iceberg. Whole slide image (WSI) classification is a quite challenging problem. First, the scarcity of annotations heavily impedes the pace of developing effective approaches. Pixelwise delineated annotations on WSIs are time consuming and tedious, which poses difficulties in building a large-scale training dataset. In addition, a variety of heterogeneous patterns of tumor existing in high magnification field are actually the major obstacle. Furthermore, a gigapixel scale WSI cannot be directly analyzed due to the immeasurable computational cost. How to design the weakly supervised learning methods to maximize the use of available WSI-level labels that can be readily obtained in clinical practice is quite appealing. To overcome these challenges, we present a weakly supervised approach in this article for fast and effective classification on the whole slide lung cancer images. Our method first takes advantage of a patch-based fully convolutional network (FCN) to retrieve discriminative blocks and provides representative deep features with high efficiency. Then, different context-aware block selection and feature aggregation strategies are explored to generate globally holistic WSI descriptor which is ultimately fed into a random forest (RF) classifier for the image-level prediction. To the best of our knowledge, this is the first study to exploit the potential of image-level labels along with some coarse annotations for weakly supervised learning. A large-scale lung cancer WSI dataset is constructed in this article for evaluation, which validates the effectiveness and feasibility of the proposed method. Extensive experiments demonstrate the superior performance of our method that surpasses the state-of-the-art approaches by a significant margin with an accuracy of 97.3%. In addition, our method also achieves the best performance on the public lung cancer WSIs dataset from The Cancer Genome Atlas (TCGA). We highlight that a small number of coarse annotations can contribute to further accuracy improvement. We believe that weakly supervised learning methods have great potential to assist pathologists in histology image diagnosis in the near future.																	2168-2267	2168-2275				SEPT	2020	50	9					3950	3962		10.1109/TCYB.2019.2935141													
J								Optimal Stealthy Deception Attack Against Cyber-Physical Systems	IEEE TRANSACTIONS ON CYBERNETICS										Detectors; Gaussian distribution; Covariance matrices; Symmetric matrices; Upper bound; Technological innovation; Cyber-physical system (CPS) security; deception attack; Kullback-Leibler (K-L) divergence	SECURE STATE ESTIMATION; RESILIENT CONTROL	This paper studies the problem of designing the optimal deception attack to maximize a utility function with the Kullback-Leibler divergence adopted as a detection constraint. The utility function reflects the goal of pulling the state away from the origin, increasing the cost of the controller and decreasing the cost of the attacker. To analyze the stealthiness of the attack, the attack signal is decomposed into two parts, one of which is strict stealthy. The necessary and sufficient condition is derived for the case that the strict stealthy attack cannot lead to an unbounded benefit. In this case, the linear transformation of the optimal attack is proved to be a Gaussian distribution. With the mean value and covariance of the Gaussian distribution as variables, the original problem is transformed into a new problem which may not be convex. A suboptimal attack policy is provided and the upper bound for the loss of benefit when using the suboptimal attack is also given. A numerical example of unmanned ground vehicle is illustrated to verify the effectiveness of the proposed attack policy.																	2168-2267	2168-2275				SEPT	2020	50	9					3963	3972		10.1109/TCYB.2019.2912622													
J								Distributed Adaptive Fuzzy Event-Triggered Containment Control of Nonlinear Strict-Feedback Systems	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Nonlinear systems; Ear; Fuzzy control; Fuzzy logic; Artificial neural networks; Containment control; event-triggered control; fuzzy logic system; strict-feedback system	NEURAL-NETWORK CONTROL; MULTIAGENT SYSTEMS; TRACKING CONTROL; STATE ESTIMATION; CONSENSUS; DESIGN	In this paper, the adaptive fuzzy event-triggered containment control problem is addressed for uncertain nonlinear strict-feedback systems guided by multiple leaders. A novel distributed adaptive fuzzy event-triggered containment control is designed only using the information of the individual follower and its neighbors. Moreover, a distributed event-trigger condition with an adjustable threshold is developed simultaneously. The designed containment control law is updated in an aperiodic manner, only when event-triggered errors exceed tolerable thresholds. It is proved that the uniformly ultimately bounded containment control can be achieved, and there is no Zeno behavior exhibited by applying the proposed control scheme. Simulation studies are outlined to illustrate the effectiveness of the theoretical results and the advantages of the event-triggered containment control proposed in this paper.																	2168-2267	2168-2275				SEPT	2020	50	9					3973	3983		10.1109/TCYB.2019.2917078													
J								Joint Deployment and Task Scheduling Optimization for Large-Scale Mobile Users in Multi-UAV-Enabled Mobile Edge Computing	IEEE TRANSACTIONS ON CYBERNETICS										Task analysis; Resource management; Unmanned aerial vehicles; Optimization; Mobile handsets; Computational modeling; Energy consumption; Deployment; differential evolution (DE); mobile edge computing (MEC); multiunmanned aerial vehicle (multi-UAV); task scheduling; two-layer optimization	UNMANNED AERIAL VEHICLES; RESOURCE-ALLOCATION; DIFFERENTIAL EVOLUTION; PLACEMENT; INTERNET	This article establishes a new multiunmanned aerial vehicle (multi-UAV)-enabled mobile edge computing (MEC) system, where a number of unmanned aerial vehicles (UAVs) are deployed as flying edge clouds for large-scale mobile users. In this system, we need to optimize the deployment of UAVs, by considering their number and locations. At the same time, to provide good services for all mobile users, it is necessary to optimize task scheduling. Specifically, for each mobile user, we need to determine whether its task is executed locally or on a UAV (i.e., offloading decision), and how many resources should be allocated (i.e., resource allocation). This article presents a two-layer optimization method for jointly optimizing the deployment of UAVs and task scheduling, with the aim of minimizing system energy consumption. By analyzing this system, we obtain the following property: the number of UAVs should be as small as possible under the condition that all tasks can be completed. Based on this property, in the upper layer, we propose a differential evolution algorithm with an elimination operator to optimize the deployment of UAVs, in which each individual represents a UAV's location and the entire population represents an entire deployment of UAVs. During the evolution, we first determine the maximum number of UAVs. Subsequently, the elimination operator gradually reduces the number of UAVs until at least one task cannot be executed under delay constraints. This process achieves an adaptive adjustment of the number of UAVs. In the lower layer, based on the given deployment of UAVs, we transform the task scheduling into a 0-1 integer programming problem. Due to the large-scale characteristic of this 0-1 integer programming problem, we propose an efficient greedy algorithm to obtain the near-optimal solution with much less time. The effectiveness of the proposed two-layer optimization method and the established multi-UAV-enabled MEC system is demonstrated on ten instances with up to 1000 mobile users.																	2168-2267	2168-2275				SEPT	2020	50	9					3984	3997		10.1109/TCYB.2019.2935466													
J								Whole Process Monitoring Based on Unstable Neuron Output Information in Hidden Layers of Deep Belief Network	IEEE TRANSACTIONS ON CYBERNETICS										Neurons; Feature extraction; Deep learning; Fault detection; Data mining; Deep belief network (DBN); deep learning; process monitoring; unstable neuron	FAULT-DIAGNOSIS; PLS	Process monitoring based on deep learning has attracted considerable attention. Generally, several hidden layers exist in the deep-learning model, and only the output information of the last hidden layer neurons extracted by deep learning is applied. Considering that each hidden layer is a kind of information representation of the original data, the information of different hidden layers may contain positive elements for process monitoring. In this article, we found that when a fault occurs, there are some neurons in each hidden layer that the information they output are different, compared with the normal condition. These neurons are called unstable neurons. Obviously, the information they output are beneficial for process monitoring. Motivated by theoretical analysis and experimental studies on unstable neurons, a novel method (UN-DBN) based on the unstable neurons in hidden layers is proposed to integrate the useful information for process monitoring, the Euclidean metric, the moving average filter, and the kernel density estimation technique are employed to provide an intuitionistic expression of the working state. The comparable result applied on a mathematic simulation process and the TE process with other advanced monitoring methods confirms the superiority and feasibility of the proposed method UN-DBN in this article.																	2168-2267	2168-2275				SEPT	2020	50	9					3998	4007		10.1109/TCYB.2019.2948202													
J								Coordination Control for Uncertain Networked Systems Using Interval Observers	IEEE TRANSACTIONS ON CYBERNETICS										Observers; Additives; Symmetric matrices; Time-varying systems; Automation; Decentralized control; Cybernetics; Coordination control; distributed interval observer; uncertain networked system	NONLINEAR MULTIAGENT SYSTEMS; LEADER-FOLLOWING CONSENSUS; TRACKING CONTROL; SAMPLED-DATA	In this article, we take the coordination control problem of linear time-invariant networked systems with uncertain additive disturbance and uncertain initial states into consideration. A distributed interval observer is first constructed for uncertain networked systems in which the control algorithm of each agent involves only the upper bound information and the lower bound information of the interval observer associated with itself and its neighbors, respectively. With the help of the cooperativity theory, it is proved that the interval observer can estimate the piecewise state for each agent and the interval-observer-based control algorithm can drive the uncertain system to achieve coordination behavior. Then, time-varying coordinate transformation is introduced to construct a novel interval observer which can eliminate the cooperativity premise on the system matrices and bound the states of all agents in real time. It is shown that the novel interval-observer-based control algorithm can guide the uncertain system to reach coordinated behavior. Finally, the numerical simulations are provided to verify the theoretical results.																	2168-2267	2168-2275				SEPT	2020	50	9					4008	4019		10.1109/TCYB.2019.2945580													
J								Dissipativity-Based Asynchronous Fuzzy Sliding Mode Control for T-S Fuzzy Hidden Markov Jump Systems	IEEE TRANSACTIONS ON CYBERNETICS										Hidden Markov models; Markov processes; Uncertainty; Fuzzy systems; Sliding mode control; Trajectory; Dissipativity-based asynchronous fuzzy integral SMC (AFISMC); hidden Markov model (HMM); Takagi-Sugeno (T-S) fuzzy Markov jump systems	FINITE-TIME STABILIZATION; INFINITY	This paper investigates the problem of dissipativity-based asynchronous fuzzy integral sliding mode control (AFISMC) for nonlinear Markov jump systems represented by Takagi-Sugeno (T-S) models, which are subject to external noise and matched uncertainties. Since modes of original systems cannot be directly obtained, the hidden Markov model is employed to detect mode information. With the detected mode and the parallel distributed compensation approach, a suitable fuzzy integral sliding surface is devised. Then using Lyapunov function, a sufficient condition for the existence of sliding mode controller gains is developed, which can also ensure the stochastic stability of the sliding mode dynamics with a satisfactory dissipative performance. An AFISMC law is proposed to drive system trajectories into the predetermined sliding mode boundary layer in finite time. For the case with unknown bound of uncertainties, an adaptive AFISMC law is developed as well. The studied T-S fuzzy Markov jump systems involve both continuous-time and discrete-time domains. Finally, some simulation results are presented to demonstrate the applicability and effectiveness of the proposed approaches.																	2168-2267	2168-2275				SEPT	2020	50	9					4020	4030		10.1109/TCYB.2019.2919299													
J								Feature Selection Based on Neighborhood Self-Information	IEEE TRANSACTIONS ON CYBERNETICS										Feature extraction; Rough sets; Indexes; Uncertainty; Measurement uncertainty; Machine learning algorithms; Greedy algorithms; Feature selection; neighborhood; rough approximation; rough set; self-information	ATTRIBUTE REDUCTION; UNCERTAINTY MEASURES; ROUGH SETS; CLASSIFICATION; ENTROPY; APPROXIMATION; GRANULATION; ALGORITHM; RULES	The concept of dependency in a neighborhood rough set model is an important evaluation function for the feature selection. This function considers only the classification information contained in the lower approximation of the decision while ignoring the upper approximation. In this paper, we construct a class of uncertainty measures: decision self-information for the feature selection. These measures take into account the uncertainty information in the lower and the upper approximations. The relationships between these measures and their properties are discussed in detail. It is proven that the fourth measure, called relative neighborhood self-information, is better for feature selection than the other measures, because not only does it consider both the lower and the upper approximations but also the change of its magnitude is largest with the variation of feature subsets. This helps to facilitate the selection of optimal feature subsets. Finally, a greedy algorithm for feature selection has been designed and a series of numerical experiments was carried out to verify the effectiveness of the proposed algorithm. The experimental results show that the proposed algorithm often chooses fewer features and improves the classification accuracy in most cases.																	2168-2267	2168-2275				SEPT	2020	50	9					4031	4042		10.1109/TCYB.2019.2923430													
J								Synchronization of Time-Delayed Complex Networks With Switching Topology Via Hybrid Actuator Fault and Impulsive Effects Control	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Synchronization; Switched systems; Topology; Germanium; Actuators; Complex networks (CNs); fault-tolerant control (FTC); impulses; mode-dependent average dwell time (MDADT); switching; synchronization; time delay; transition probability (TP)	COUPLED NEURAL-NETWORKS; EXPONENTIAL SYNCHRONIZATION; DYNAMICAL NETWORKS; MIXED DELAYS; STOCHASTIC PERTURBATION; MULTIAGENT SYSTEMS; VARYING DELAYS; STABILIZATION; CONSENSUS	This article investigates global exponential synchronization almost surely (GES a.s.) of complex networks (CNs) with node delay and switching topology. By introducing transition probability (TP) and mode-dependent average dwell time (MDADT) to the switching signal, the considered model is more practical than the systems with average dwell-time (ADT) switching. Controllers with both impulsive effects and actuator fault feedback are considered. New analytical techniques are developed to obtain sufficient conditions to guarantee the GES a.s. Different from the existing results on the synchronization of switched systems, our results show that the GES a.s. can still be achieved even in the case that the upper bound of the dwell time (DT) of uncontrolled nodes is very large and the lower bound of the DT of controlled nodes is very small. Numerical examples demonstrate the effectiveness and the merits of the theoretical analysis.																	2168-2267	2168-2275				SEPT	2020	50	9					4043	4052		10.1109/TCYB.2019.2938217													
J								Ant Colony Optimization for the Control of Pollutant Spreading on Social Networks	IEEE TRANSACTIONS ON CYBERNETICS										Social networking (online); Optimization; Integrated circuit modeling; Adaptation models; Biological system modeling; Greedy algorithms; Ant colony optimization; Ant colony optimization (ACO); multiobjective optimization; rumor blocking; social networks	INFLUENCE MAXIMIZATION; ALGORITHM; MOEA/D; GENERATION	The rapid development of online social networks not only enables prompt and convenient dissemination of desirable information but also incurs fast and wide propagation of undesirable information. A common way to control the spread of pollutants is to block some nodes, but such a strategy may affect the service quality of a social network and leads to a high control cost if too many nodes are blocked. This paper considers the node selection problem as a biobjective optimization problem to find a subset of nodes to be blocked so that the effect of the control is maximized while the cost of the control is minimized. To solve this problem, we design an ant colony optimization algorithm with an adaptive dimension size selection under the multiobjective evolutionary algorithm framework based on decomposition (MOEA/D-ADACO). The proposed algorithm divides the biobjective problem into a set of single-objective subproblems and each ant takes charge of optimizing one subproblem. Moreover, two types of pheromone and heuristic information are incorporated into MOEA/D-ADACO, that is, pheromone and heuristic information of dimension size selection and that of node selection. While constructing solutions, the ants first determine the dimension size according to the former type of pheromone and heuristic information. Then, the ants select a specific number of nodes to build solutions according to the latter type of pheromone and heuristic information. Experiments conducted on a set of real-world online social networks confirm that the proposed biobjective optimization model and the developed MOEA/D-ADACO are promising for the pollutant spreading control.																	2168-2267	2168-2275				SEPT	2020	50	9					4053	4065		10.1109/TCYB.2019.2922266													
J								Multiple Attacks Detection in Cyber-Physical Systems Using Random Finite Set Theory	IEEE TRANSACTIONS ON CYBERNETICS										Radio frequency; Cyberattack; Bayes methods; Detectors; Cyber-physical systems; Communication channels; Attacks detection; Bayesian filter; cyber-physical systems (CPSs); random finite set (RFS)	TRACKING; IMPLEMENTATION; OPTIMIZATION; ARCHITECTURE; CONSENSUS; FILTERS	To invade a cyber-physical system (CPS) successfully, hackers are prone to simultaneously launching multiple cyber attacks on different sensors in a CPS. However, little attention has been paid to the problem of detecting multiple cyber attacks up to now. Therefore, in this paper, we deal with the problem on how to efficiently detect multiple cyber attacks aiming at different sensors in CPSs. To achieve the goal of simultaneously detecting both the number of attacks and the attacked sensors, we formulate this problem via a random finite set (RFS) theory, and then apply an iterative RFS-based Bayesian filter and its approximation to solve the problem. Four numerical experiments with different attacks are provided, and the results have demonstrated the effectiveness of the RFS-based approach for the problem of multiple attacks detection in CPSs.																	2168-2267	2168-2275				SEPT	2020	50	9					4066	4075		10.1109/TCYB.2019.2912939													
J								Model-Free Cooperative Adaptive Sliding-Mode-Constrained-Control for Multiple Linear Induction Traction Systems	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Adaptation models; Topology; Systematics; Induction motors; Control systems; Aerodynamics; Cooperative adaptive control; model-free constrained control; multiagent systems; multiple linear induction traction systems (multi-LITSs); systematic error	MOTOR; DESIGN	In order to deal with the speed cooperative control problem in the multiple linear induction traction systems consists of multiple linear induction motors, a model-free cooperative adaptive sliding-mode-constrained-control strategy is proposed considering the input magnitude and rate constraints which may cause the problem of actuator and integral saturation. First, the equivalent circuit topology of the single motor in the system is investigated. Besides, the system is considered as the multiagent system with fixed communication topology due to the interaction between adjacent motors. Then, the output observer is presented to estimate the output and the estimation algorithm of pseudo-partial derivative parameter and uncertainties is proposed. Based on the above, the proposed control scheme is presented by designing an integral sliding-mode surface containing the systematic error and an anti-windup compensator is added to eliminate the saturation. Finally, the simulations of the proposed control strategy for multiagent systems are carried out to demonstrate the effectiveness and superiority of the proposed control strategy.																	2168-2267	2168-2275				SEPT	2020	50	9					4076	4086		10.1109/TCYB.2019.2913983													
J								A Scalable Algorithm for Event-Triggered State Estimation With Unknown Parameters and Switching Topologies Over Sensor Networks	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Topology; State estimation; Network topology; Stability criteria; Estimation error; Distributed estimation; nonlinear stochastic systems; sensor networks (SNs); switching topologies; unknown parameters	VARYING NONLINEAR-SYSTEMS; CONSENSUS FILTER; FAULT ESTIMATION; INPUT ESTIMATION; STABILITY; OBSERVERS; SUBJECT	An event-triggered distributed state estimation problem is investigated for a class of discrete-time nonlinear stochastic systems with unknown parameters over sensor networks (SNs) subject to switched topologies. An event-triggered communication strategy is employed to govern the information broadcast and reduce the unnecessary resource consumption. Based on the adopted communication strategy, a distributed state estimator is designed to estimate the plant states and also identify the unknown parameters. In the framework of input-to-state stability, sufficient conditions with an average dwell time are established to ensure the boundedness of estimation errors in mean-square sense. In addition, the gains of the designed estimators are dependent on the solution of a set of matrix inequalities whose dimensions are unrelated to the scale of underlying SNs, thereby fulfill the scalability requirement. Finally, an illustrative simulation is utilized to verify the feasibility of the proposed design scheme.																	2168-2267	2168-2275				SEPT	2020	50	9					4087	4097		10.1109/TCYB.2019.2917543													
J								Finite-Time H-infinity Asynchronous Control for Nonlinear Markov Jump Distributed Parameter Systems via Quantized Fuzzy Output-Feedback Approach	IEEE TRANSACTIONS ON CYBERNETICS										Asynchronous output-feedback control; distributed parameter systems (DPSs); hidden Markov model (HMM); Markov jump; Takagi-Sugeno fuzzy model	EXPONENTIAL STABILITY; CONTROL DESIGN; PDE SYSTEMS; STABILIZATION; ACTUATOR	This article focuses on the asynchronous output-feedback control design for a class of nonlinear Markov jump distributed parameter systems based on a hidden Markov model. Initially, the considered systems are represented by a Takagi-Sugeno fuzzy model via a sector nonlinearity approach. Furthermore, asynchronous quantizers are introduced to save the limited communication resource in engineering applications. Then, based on the Lyapunov direct method and some inequality techniques, a series of novel stability criteria, which guarantee the finite-time boundedness and H-infinity disturbance attenuation performance of the target plants, is established in the form of spatial differential linear matrix inequalities. Finally, a simulation study is provided to verify the viability of the developed approach.																	2168-2267	2168-2275				SEPT	2020	50	9					4098	4109		10.1109/TCYB.2019.2936827													
J								Double-Integrator Dynamics for Multiagent Systems With Antagonistic Reciprocity	IEEE TRANSACTIONS ON CYBERNETICS										Consensus algorithm; Laplace equations; Graph theory; Multi-agent systems; Topology; Eigenvalues and eigenfunctions; Antagonistic reciprocity; consensus; double-integrator dynamics; multiagent systems	BIPARTITE CONSENSUS; NETWORKS; AGENTS	This article is dedicated to the consensus problem for interacting agents of the double-integrator dynamics subject to antagonistic reciprocity, described by negative scalar parameters. To this end, we first show the existence of the weighted gains which play an essential role for solving the consensus problem. Then, we establish the relationship between the weighted gains and scalar parameters to guarantee that the underlying "Laplacian" matrix contains a simple zero eigenvalue and the remaining eigenvalues enjoy positive real parts. Based on the above analysis, we further proceed to solve the considered problem. A major difficulty is that the Laplacian matrices, associated with the position and velocity information, are entirely distinct from each other, leading to the failure of the conventional consensus method for the second-order dynamics. We derive some criteria involving the weighted gains, the scaling parameters, and the real/image parts of the Laplacian matrix of the interaction graph. Moreover, some special frameworks, which have been extensively studied in the literature, are also elaborated on. Compared with the Altafini's model, we do not need to redefine a new Laplacian matrix, and more important, the restriction on the digon sign-symmetry property is removed. It is worth mentioning that the proposed consensus algorithm cannot be deduced by the Altafini's model or its variants. Finally, a wheeled multirobot system is formulated to validate the efficiency of the theoretical results.																	2168-2267	2168-2275				SEPT	2020	50	9					4110	4120		10.1109/TCYB.2019.2939487													
J								Quasi-Synchronization of Time Delay Markovian Jump Neural Networks With Impulsive-Driven Transmission and Fading Channels	IEEE TRANSACTIONS ON CYBERNETICS										Fading channel; impulsive control; master-slave (MS) Markovian jump neural networks (MJNNs); quasi-synchronization (QS)	ADAPTIVE-CONTROL; VARYING DELAYS; SYSTEMS; STABILITY; DESIGN; TELEOPERATION	The problem of quasi-synchronization (QS) for the Markovian jump master-slave neural networks with time-varying delay is studied in this article, where the mismatch parameters and unreliable communication channels are considered as well. A set of stochastic variables with different expectations are used to describe the fading phenomena of parallel communication channels. An impulsive-driven transmission strategy is designed to reduce the communication load, and a corresponding impulsive controller is then designed. A synchronization error system (SES) is obtained, and a convex QS condition is established for the SES. A linear matrix inequality-based iterative algorithm is proposed to reduce the bound of the SES, and the corresponding controller gains are calculated. A numerical example is provided to illustrate the effectiveness of the developed result.																	2168-2267	2168-2275				SEPT	2020	50	9					4121	4131		10.1109/TCYB.2019.2941582													
J								Nonzero-Sum Game Reinforcement Learning for Performance Optimization in Large-Scale Industrial Processes	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Production; Games; Heuristic algorithms; Nash equilibrium; Reinforcement learning; Game theory; Nash equilibrium; plant-wide performance optimization; reinforcement learning	DIFFERENTIAL GRAPHICAL GAMES; MULTIAGENT SYSTEMS; DESIGN; SYNCHRONIZATION	This article presents a novel technique to achieve plant-wide performance optimization for large-scale unknown industrial processes by integrating the reinforcement learning method with the multiagent game theory. A main advantage of this technique is that plant-wide optimal performance is achieved by a distributed approach where multiple agents solve simplified local nonzero-sum optimization problems so that a global Nash equilibrium is reached. To this end, first, the plant-wide performance optimization problem is reformulated by decomposition into local optimization subproblems for each production index in a multiagent framework. Then, the nonzero-sum graphical game theory is utilized to compute the operational indices for each unit process with the purpose of reaching the global Nash equilibrium, resulting in production indices following their prescribed target values. The stability and the global Nash equilibrium of this multiagent graphical game solution are rigorously proved. The reinforcement learning methods are then developed for each agent to solve the nonzero-sum graphical game problem using data measurements available in the system in real time. The plant dynamics do not have to be known. Finally, the emulation results are given to show the effectiveness of the proposed automated decision algorithm by using measured data from a large mineral processing plant in Gansu Province, China.																	2168-2267	2168-2275				SEPT	2020	50	9					4132	4145		10.1109/TCYB.2019.2950262													
J								Distributed Q-Learning-Based Online Optimization Algorithm for Unit Commitment and Dispatch in Smart Grid	IEEE TRANSACTIONS ON CYBERNETICS										Distributed Q-learning; online optimization; smart grid; unit commitment and dispatch (UCD)	ECONOMIC-DISPATCH	Economic dispatch (ED) and unit commitment (UC) problems need to be revisited in order to make a transition from a traditional power system to a smart grid. In this paper, we formulate the ED and UC problems into a unified form, which is also capable of characterizing the infinite horizon UC problem. Based on the formulation, a centralized Q-learning-based optimization algorithm is proposed. The proposed algorithm runs in an online manner and requires no prior information on the mathematical formulation of the actual cost functions, thus being capable of dealing with situations for which such cost functions are too difficult to obtain. Then, the distributed counterpart of the centralized algorithm is developed by relaxing the demand for global information and balancing exploration and exploitation cooperatively in a distributed way. Theoretical analysis of the proposed algorithms is also provided. Finally, several case studies are presented to demonstrate the effectiveness of the proposed algorithms.																	2168-2267	2168-2275				SEPT	2020	50	9					4146	4156		10.1109/TCYB.2019.2921475													
J								Bidirectional Discrete Matrix Factorization Hashing for Image Search	IEEE TRANSACTIONS ON CYBERNETICS										Binary codes; Matrix decomposition; Semantics; Quantization (signal); Image reconstruction; Manifolds; Visualization; Bidirectional; hashing; image retrieval	RETRIEVAL; TREES	Unsupervised image hashing has recently gained significant momentum due to the scarcity of reliable supervision knowledge, such as class labels and pairwise relationship. Previous unsupervised methods heavily rely on constructing sufficiently large affinity matrix for exploring the geometric structure of data. Nevertheless, due to lack of adequately preserving the intrinsic information of original visual data, satisfactory performance can hardly be achieved. In this article, we propose a novel approach, called bidirectional discrete matrix factorization hashing (BDMFH), which alternates two mutually promoted processes of 1) learning binary codes from data and 2) recovering data from the binary codes. In particular, we design the inverse factorization model, which enforces the learned binary codes inheriting intrinsic structure from the original visual data. Moreover, we develop an efficient discrete optimization algorithm for the proposed BDMFH. Comprehensive experimental results on three large-scale benchmark datasets show that the proposed BDMFH not only significantly outperforms the state-of-the-arts but also provides the satisfactory computational efficiency.																	2168-2267	2168-2275				SEPT	2020	50	9					4157	4168		10.1109/TCYB.2019.2941284													
J								Optimal Filtered and Smoothed Estimators for Discrete-Time Linear Systems With Multiple Packet Dropouts Under Markovian Communication Constraints	IEEE TRANSACTIONS ON CYBERNETICS										Markov processes; Kalman filters; Estimation; Linear systems; Protocols; Load modeling; Communication constraints; Kalman filtering; linear stochastic systems; packet dropouts	NETWORKED CONTROL-SYSTEMS; UNSCENTED KALMAN FILTER; JUMP SYSTEMS; STATE ESTIMATION; POWER-CONTROL; STABILITY; DELAYS	This paper concentrates on the linear least mean square (LLMS) filtered and smoothed estimators for networked linear stochastic systems. Multiple packet losses, Markovian communication constraints, and superposed process noise are considered simultaneously. In order to reduce the channel load during communication, at every step, just one transmission node is permitted to send data packets. Hence, a Markovian communication protocol is utilized to arrange the packets of these transmission nodes. Moreover, multiple data packet dropouts occur during transmission due to an imperfect communication channel. Therefore, the global observation information cannot be obtained by the state estimator. The real state of Markov chain is assumed to be unknown to the estimator except the transition probability matrix. By means of the innovation analysis approach and orthogonal projection principle, we design Kalman-like estimators in a recursive form. Finally, through simulation experiments, we verify the effectiveness and superiority of the designed algorithm.																	2168-2267	2168-2275				SEPT	2020	50	9					4169	4181		10.1109/TCYB.2019.2924485													
J								A pattern-driven solution for designing multi-objective evolutionary algorithms	NATURAL COMPUTING										Meta-heuristic design pattern; Multi-objective evolutionary algorithm; Software testing; Hyper-heuristic	GENETIC ALGORITHM; FRAMEWORK	Multi-objective evolutionary algorithms (MOEAs) have been widely studied in the literature, which led to the development of several frameworks and techniques to implement them. Consequently, the reusability, scalability and maintainability became fundamental concerns in the development of such algorithms. To this end, the use of design patterns (DPs) can benefit, ease and improve the design of MOEAs. DPs are reusable solutions for common design problems, which can be applied to almost any context. Despite their advantages to decrease coupling, increase flexibility, and allow an easier design extension, DPs have been underexplored for MOEA design. In order to contribute to this research topic, we propose a pattern-driven solution for the design of MOEAs. The MOEA designed with our solution is compared to another MOEA designed without it. The comparison considered: the Integration and Test Order (ITO) problem and the Traveling Salesman problem (TSP). Obtained results show that the use of this DP-driven solution allows the reuse of MOEA components, without decreasing the quality, in terms of hypervolume. This means that the developer can extend the algorithms to include other components using only object-oriented mechanisms in an easier way, while maintaining the expected results.																	1567-7818	1572-9796				SEP	2020	19	3			SI		481	494		10.1007/s11047-018-9677-y													
J								A novel context-free grammar for the generation of PSO algorithms	NATURAL COMPUTING										Context-free grammar; Generational hyper-heuristics; Particle swarm optimization	LOCAL-SEARCH HEURISTICS; PARTICLE SWARM; FIREWORKS ALGORITHM; CLASSIFICATION; DISCOVERY; EVOLUTION	Particle swarm optimization algorithm (PSO) has been widely studied over the years due to its competitive results in different applications. However, its performance is dependent on some design components (e.g., inertia factor, velocity equation, topology). Thus, to define which is the best algorithm design to solve a given optimization problem is difficult due to the large number of variations and parameters that can be considered. This work proposes a novel context-free grammar for Grammar-Guided Genetic Programming (GGGP) algorithms to guide the creation of Particle Swarm Optimizers. The proposed grammar considers four aspects of the PSO algorithm that may strongly impact on its performance: swarm initialization, neighborhood topology, velocity update equation and mutation operator. To assess the proposal, a GGGP algorithm was set with the proposed grammar and employed to optimize the PSO algorithm in 32 unconstrained continuous optimization problems. In the experiments, we compared the algorithms generated from the proposed grammar with those algorithms produced by two other grammars presented in the literature to automate PSO designs. The results achieved by the proposed grammar were better than the counterparts. Besides, we also compared the generated algorithms to 6 competition algorithms with different strategies. The experiments have shown that the algorithms generated from the grammar reached better results.																	1567-7818	1572-9796				SEP	2020	19	3			SI		495	513		10.1007/s11047-018-9679-9													
J								Population-based bio-inspired algorithms for cluster ensembles optimization	NATURAL COMPUTING										Cluster ensemble; Consensus partition; Population-based bio-inspired optimization	CORAL-REEFS OPTIMIZATION	Clustering algorithms have been applied to different problems in many different real-word applications. Nevertheless, each algorithm has its own advantages and drawbacks, which can result in different solutions for the same problem. Therefore, the combination of different clustering algorithms (cluster ensembles) has emerged as an attempt to overcome the limitations of each clustering technique. The use of cluster ensembles aims to combine multiple partitions generated by different clustering algorithms into a single clustering solution (consensus partition). Recently, several approaches have been proposed in the literature in order to optimize or to improve continuously the solutions found by the cluster ensembles. As a contribution to this important subject, this paper presents an investigation of five bio-inspired techniques in the optimization of cluster ensembles (Genetic Algorithms, Particle Swarm Optimization, Ant Colony Optimization, Coral Reefs Optimization and Bee Colony Optimization). In this investigation, unlike most of the existing work, an evaluation methodology for assessing three important aspects of cluster ensembles will be presented, assessing robustness, novelty and stability of the consensus partition delivered by different optimization algorithms. In order to evaluate the feasibility of the analyzed techniques, an empirical analysis will be conducted using 20 different problems and applying two different indexes in order to examine its efficiency and feasibility. Our findings indicated that the best population-based optimization method was PSO, followed by CRO, AG, BCO and ACO, for providing robust and stable consensus partitions.																	1567-7818	1572-9796				SEP	2020	19	3			SI		515	532		10.1007/s11047-018-9682-1													
J								A proposal for tuning the alpha parameter in C alpha C-integrals for application in fuzzy rule-based classification systems	NATURAL COMPUTING										Aggregation functions; Choquet integral; Fuzzy rule-based classification systems; Fuzzy reasoning method; Genetic algorithms; Evolutionary fuzzy systems	DIMENSIONAL OVERLAP FUNCTIONS; ALGORITHMS; INDEXES	In this paper, we consider the concept of extended Choquet integral generalized by a copula, called CC-integral. In particular, we adopt a CC-integral that uses a copula defined by a parameter alpha, which behavior was tested in a previous work using different fixed values. In this contribution, we propose an extension of this method by learning the best value for the parameter alpha using a genetic algorithm. This new proposal is applied in the fuzzy reasoning method of fuzzy rule-based classification systems in such a way that, for each class, the most suitable value of the parameter alpha is obtained, which can lead to an improvement on the system's performance. In the experimental study, we test the performance of 4 different so called C alpha C-integrals, comparing the results obtained when using fixed values for the parameter alpha against the results provided by our new evolutionary approach. From the obtained results, it is possible to conclude that the genetic learning of the parameter alpha is statistically superior than the fixed one for two copulas. Moreover, in general, the accuracy achieved in test is superior than that of the fixed approach in all functions. We also compare the quality of this approach with related approaches, showing that the methodology proposed in this work provides competitive results. Therefore, we demonstrate that C alpha C-integrals with alpha learned genetically can be considered as a good alternative to be used in fuzzy rule-based classification systems.																	1567-7818	1572-9796				SEP	2020	19	3			SI		533	546		10.1007/s11047-018-9678-x													
J								Swarm optimization clustering methods for opinion mining	NATURAL COMPUTING										Opinion mining; Opinion clustering; Text clustering; Swarm optimization; Twitter		Supervised machine learning and opinion lexicon are the most frequent approaches for opinion mining, but they require considerable effort to prepare the training data and to build the opinion lexicon, respectively. In this paper, a novel unsupervised clustering approach is proposed for opinion mining. Three swarm algorithms based on Particle Swarm Optimization are evaluated using three corpora with different levels of complexity with respect to size, number of opinions, domains, languages, and class balancing. K-means and Agglomerative clustering algorithms, as well as, the Artificial Bee Colony and Cuckoo Search swarm-based algorithms were selected for comparison. The proposed swarm-based algorithms achieved better accuracy using the word bigram feature model as the pre-processing technique, the Global Silhouette as optimization function, and on datasets with two classes: positive and negative. Although the swarm-based algorithms obtained lower result for datasets with three classes, they are still competitive considering that neither labeled data, nor opinion lexicons are required for the opinion clustering approach.																	1567-7818	1572-9796				SEP	2020	19	3			SI		547	575		10.1007/s11047-018-9681-2													
J								A proposal of quantum data representation to improve the discrimination power	NATURAL COMPUTING										Quantum representation; Data discrimination power; Machine learning; Preprocessing data; Classification	DATA COMPLEXITY-MEASURES; CLASSIFICATION	This work proposes a quantum representation for improvement of data discrimination power, transforming a non linearly separable problem into a linearly separable problem. This methodology proposed here can be naturally employed as data preprocessing for classification task. A classical real world system will be viewed as a composition of quantum systems, where any observable measurement process of the real world data are created from an expected value measure of a quantum system state. In this projection measure a quantum phase information is naturally lost, making the inverse mapping from the classical space into quantum space impossible. However, it is possible find an arbitrate quantum state that represents the same classical information originally measured. A genetic algorithm is employed for search this arbitrate quantum state, going back from classical world to quantum world representation. The genetic algorithm searches for a compatible quantum state with the real world data, where the lost quantum phase is adjusted with the constraints to minimize the classes' variance and to maximize the distance between the classes' centroids. Computational simulations shown that the proposed methodology was able to transform a non linearly separable problem in classical representation space into a linearly separable problem in the quantum representation space, demonstrating an enhancement of data discrimination power.																	1567-7818	1572-9796				SEP	2020	19	3			SI		577	591		10.1007/s11047-019-09734-w													
J								Molecular computing for Markov chains	NATURAL COMPUTING										Molecular computing; DNA strand displacement; Markov chain; Mass action kinetics; Gillespie algorithm	COMPUTATION; DNA; EVOLUTION; NETWORKS	In this paper, it is presented a methodology for implementing arbitrarily constructed time-homogenous Markov chains with biochemical systems. Not only discrete but also continuous-time Markov chains are allowed to be computed. By employing chemical reaction networks as a programmable language, molecular concentrations serve to denote both input and output values. One reaction network is elaborately designed for each chain. The evolution of species' concentrations over time well matches the transient solutions of the target continuous-time Markov chain, while equilibrium concentrations can indicate the steady state probabilities. Additionally, second-order Markov chains are considered for implementation, with bimolecular reactions rather than unary ones. An original scheme is put forward to compile unimolecular systems to DNA strand displacement reactions for the sake of future physical implementations. Deterministic, stochastic and DNA simulations are provided to enhance correctness, validity and feasibility.																	1567-7818	1572-9796				SEP	2020	19	3			SI		593	608		10.1007/s11047-019-09736-8													
J								A class of discrete dynamical systems with properties of both cellular automata and L-systems	NATURAL COMPUTING										Complexity; Entropy; Self-reproducing systems; Self-organizing systems; Cellular automata; L-systems	COMPLEXITY	We introduce and explore a type of discrete dynamic system inheriting some properties of both cellular automata (CA) and L-systems. Originally suggested by Jean Della Dora, and thus called DEM-systems after him and the two current authors, these systems can have the structural flexibility of an L-system as well as algebraic properties of CA. They are defined as sequences on a one-dimensional loop with rules governing dynamics in which new sites can be created, depending on the states of a neighbourhood of sites, and complex behaviour can be generated. Although the definition of DEM-systems is quite broad, we define some subclasses, for which more complete results can be obtained. For example, we define an additive subclass, for which algebraic results on asymptotic growth are possible, and an elementary class of particularly simple rules, for which nevertheless impressive complexity is achievable. Unlike for CA, finite initial sequences can produce positive spatial entropy over time. However, even in cases where the entropy is zero, considerable complexity is possible, especially when the sequence length grows to infinity, and we demonstrate and study behaviours of DEM-systems including fragmentation of sequences, self-reproducing patterns, self-similar but irregular patterns, patterns that not only produce new sites but produce producers of new sites, and sequences whose growth rate is sublinear, linear, quadratic, cubic, or exponential. The most complex behaviour from small finite initial conditions and the simplest class of rules appear to have positive entropy, a suggestion for which we have so far only stong numerical evidence, though we present a proof for these 'elementary' DEM-systems that entropy cannot reach the theoretical maximum of 1.																	1567-7818	1572-9796				SEP	2020	19	3			SI		609	641		10.1007/s11047-019-09739-5													
J								Hyperkernel-based intuitionistic fuzzy c-means for denoising color archival document images	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Denoising; Historical documents; Color images; Low-level features; Hyperspace; Kernel intuitionistic fuzzy c-means	CLUSTERING-ALGORITHM; SEGMENTATION; FCM	In this article, we have addressed the problem of denoising and enhancement of color archival handwritten document images by separating noise from text and background. Indeed, archival document images that originated from scanning or photographing paper documents are mainly digitized in full color mode. Thus, it is necessary to preserve and exploit color information when applying an enhancement method or a denoising technique. Thus, the focus of our work has been to model a color image using a hyperspace. The defined hyperspace formed by the image pixels is obtained by using both topological and color spaces. The novelty of our work lies in exploiting the obtained hyperspace to cluster the extracted low-level features (topological and color) and, thereafter, to separate noise from text and background. Indeed, based on combining the obtained hyperspace with an adapted kernel-based intuitionistic fuzzy c-means (KIFCM) algorithm we have proposed a novel hyper-KIFCM (HKIFCM) method for denoising color historical document images. To illustrate the effectiveness of the HKIFCM method, a thorough experimental study has been firstly conducted with qualitative and quantitative observations obtained from color archival handwritten document images collected from both the Tunisian national archives and two datasets provided in the context of open competitions at ICDAR and ICFHR conferences. Then, we have compared the results achieved with those obtained using the state-of-the-art methods.																	1433-2833	1433-2825				SEP	2020	23	3					161	181		10.1007/s10032-020-00352-2													
J								Hard exudate detection in retinal fundus images using supervised learning	NEURAL COMPUTING & APPLICATIONS										Diabetic retinopathy; Hard exudates; Fundus image; Multilayer perceptron network; Support vector machine; Hierarchical adaptive neurofuzzy inference system; Convolutional neural networks	DIABETIC-RETINOPATHY; SEGMENTATION; DIAGNOSIS	The patients with diabetes have a chance to develop diabetic retinopathy (DR) which affects to the eyes. DR can cause blindness if the patients do not control diabetes. The patients with DR will have an impairment of metabolism of glucose causing a high glucose level in blood vessel called hyperglycemia. It leads to abnormal blood vessel and ultimately results in leakage of blood or fluid like lipoproteins, which are deposited under macular edema called hard exudates. They are normally white or yellowish-white with margins. Hard exudates are often arranged in clumps or circinate rings and located in the outer layer of the retina. The aim of this research was to detect hard exudates by applying several image processing techniques and classify them by using supervised learning methods including support vector machines and some neural network approaches, i.e., multilayer perceptron (MLP) network, hierarchical adaptive neurofuzzy inference system (hierarchical ANFIS), and convolutional neuron networks. DIARETDB1 which contains 89 fundus images is exploited as a dataset for evaluation. Hard exudate candidates are extracted by morphological techniques and classified by the classifiers trained by extracted patches with the corresponding ground truths. The tenfold cross-validation is applied to assure the generalization of the results. The proposed method achieves the area under the curve (AUC) of 0.998 when the MLP network is applied. The AUCs for all four classifiers are more than 0.95. This shows that the combination of image processing techniques and suitable classifiers can perform very well in hard exudate detection problem.																	0941-0643	1433-3058				SEP	2020	32	17					13079	13096		10.1007/s00521-019-04402-7													
J								A PKI without TTP based on conditional trust in blockchain	NEURAL COMPUTING & APPLICATIONS										PKI; Blockchain; Trust model; Certificate		Many people have used public keys in various areas based on public key infrastructure (PKI). PKI provides a method to publicize public keys securely. However, existing PKI methods have a problem that they assume trusted third parties. Therefore, the existing PKIs cannot be used when users cannot trust certificate issuers. To solve this problem, we propose a new trust model and describe its implementation based on blockchain. Users can trust the certificate issued by full nodes even if they do not trust the full nodes themselves. We analyze the security of our model and show that its security can be achieved higher than existing models. This new model can be particularly useful in an environment where a third party cannot be easily trusted.																	0941-0643	1433-3058				SEP	2020	32	17					13097	13106		10.1007/s00521-019-04403-6													
J								Performance optimization of QoS-supported dense WLANs using machine-learning-enabled enhanced distributed channel access (MEDCA) mechanism	NEURAL COMPUTING & APPLICATIONS										QoS-supported WLANs; MAC layer channel access; Machine learning; Dense WLANs; EDCA	COLLISION RESOLUTION; MAC PROTOCOL; SERVICE	Quality of service (QoS) implementation in a wireless local area network (WLAN) enables the prediction of network performance and utilization of effective bandwidth for multimedia applications. In QoS-supported WLAN, enhanced distributed channel access (EDCA) adjusts back-off parameters to implement priority-based channel access at the medium access control (MAC) layer. Although conventional QoS-supported EDCA in WLANs can provide a certain degree of QoS guarantee, the performance of best effort data (low-priority) traffic is sacrificed owing to the blind use of a binary exponential back-off (BEB) mechanism for collision avoidance among WLAN stations (STAs). In EDCA, the BEB mechanism exponentially increases the contention window (CW[AC]) for any specific priority access category (AC) when collision occurs and resets it to its initial size after successful data transmission. This increase and reset ofCW[AC] is performed regardless of the network density inference, i.e., a scarce WLAN does not require an unnecessary exponential increase inCW[AC]. Similarly, a dense WLAN causes more collisions ifCW[AC] is reset to its initial minimum size. Machine-learning algorithms can scrutinize an STA's experience for WLAN inference. Therefore, in this study, we propose a machine-learning-enabled EDCA (MEDCA) mechanism for QoS-supported MAC layer channel access in dense WLANs. This mechanism utilizes a Q-learning algorithm, which is one of the prevailing models of machine learning, to infer the network density and adjust its back-offCW[AC] accordingly. Simulation results show that MEDCA performs better as compared to the conventional EDCA mechanism in QoS-supported dense WLANs.																	0941-0643	1433-3058				SEP	2020	32	17					13107	13115		10.1007/s00521-019-04416-1													
J								An efficient public key functional encryption for inner product evaluations	NEURAL COMPUTING & APPLICATIONS										Functional encryption; Pairing-based public key functional encryption; Inner product evaluation; Fully collusion resistance	PREDICATE ENCRYPTION; SCHEME	As many services have changed from offline to online, a lot of personal information including user private data has been collected by and exchanged with various service providers. An issue raised in this process is that personal information can be exploited by multiple unwanted entities without the data owner's knowledge. To solve this problem, functional encryption was proposed. It is suitable for data protection because even if a third-party uses the owner's secret key for a functionf, it cannot retrieve the original messagexfrom the ciphertext. This means that information aboutxcannot be published, but is exposed only asf(x), the result of the functionf. However, previous pairing-based public key functional encryption schemes for inner product evaluations (FE-IPE) cannot be practical solutions yet because they require too much computation, communication and storage overheads. In this paper, we propose an efficient pairing-based public key FE-IPE that requires onlyn(i.e., the dimension of vectors for function and message) exponentiation plustwopairing computations for decryption with smaller sized public parameters, secret keys and ciphertexts. And this scheme supports fully collusion resistance. The proposed scheme is proven selectively secure against chosen-plaintext attacks in the standard model under the external Diffie-Hellman assumption.																	0941-0643	1433-3058				SEP	2020	32	17					13117	13128		10.1007/s00521-019-04440-1													
J								High-performance IoT streaming data prediction system using Spark: a case study of air pollution	NEURAL COMPUTING & APPLICATIONS										Long Short-Term Memory (LSTM); Distributed deep learning; Distributed Keras (Dist-Keras); Apache Spark		Internet-of-Things (IoT) devices are becoming prevalent, and some of them, such as sensors, generate continuous time-series data, i.e., streaming data. These IoT streaming data are one of Big Data sources, and they require careful consideration for efficient data processing and analysis. Deep learning is emerging as a solution to IoT streaming data analytics. However, there is a persistent problem in deep learning that it takes a long time to learn neural networks. In this paper, we propose a high-performance IoT streaming data prediction system to improve the learning speed and to predict in real time. We showed the efficacy of the system through a case study of air pollution. The experimental results show that the modified LSTM autoencoder model shows the best performance compared to a generic LSTM model. We noticed that achieving the best performance requires optimizing many parameters, including learning rate, epoch, memory cell size, input timestep size, and the number of features/predictors. In that regard, we show that the high-performance data learning/prediction frameworks (e.g., Spark, Dist-Keras, and Hadoop) are essential to rapidly fine-tune a model for training and testing before real deployment of the model as data accumulate.																	0941-0643	1433-3058				SEP	2020	32	17					13147	13154		10.1007/s00521-019-04678-9													
J								Pedestrian detection with LeNet-like convolutional networks	NEURAL COMPUTING & APPLICATIONS										Pedestrian detectors; Computer vision; LeNet convolutional networks; Image classification		We present a detection method that is able to detect a learned target and is valid for both static and moving cameras. As an application, we detect pedestrians, but could be anything if there is a large set of images of it. The data set is fed into a number of deep convolutional networks, and then, two of these models are set in cascade in order to filter the cutouts of a multi-resolution window that scans the frames in a video sequence. We demonstrate that the excellent performance of deep convolutional networks is very difficult to match when dealing with real problems, and yet we obtain competitive results.																	0941-0643	1433-3058				SEP	2020	32	17					13175	13181		10.1007/s00521-017-3197-z													
J								Performance analysis of No-Propagation and ELM algorithms in classification	NEURAL COMPUTING & APPLICATIONS										Neural networks; Machine learning; Binary classification; ELM; No-Prop; Noise; Filtering; Overfitting	EXTREME LEARNING-MACHINE; REGRESSION	The growing volume and complexity of data has led to the development of the so-called linear algorithms for neural networks like ELM, which maintain the precision of classic algorithms but with higher training speed. This speed increase is due to a simpler architecture, the random fixing of the input weights without being trained and the analytical calculation of the output weights instead of the slowly classical iterative gradient methods as Backpropagation. However, the random fixing of the input weights increases the sensibility to input perturbations like noise. Recently, No-Propagation (No-Prop) algorithm has been introduced as another linear algorithm, which shares with ELM the architecture and the random input weights (hidden weights) initialization. In this paper, an exhaustive comparison of both algorithms and its regularized versions are presented. The simulations results suggest that No-Prop is a competitive alternative to the ELM algorithm.																	0941-0643	1433-3058				SEP	2020	32	17					13183	13193		10.1007/s00521-018-3353-0													
J								Tackling business intelligence with bioinspired deep learning	NEURAL COMPUTING & APPLICATIONS										Business intelligence; Business data; Bioinspired systems; Metaplasticity; Deep learning; MLP; AMMLP; AMP; Artificial neural network	FEATURE-SELECTION; CREDIT; ENSEMBLE; CLASSIFICATION; PREDICTION; MODEL	To tackle the complex problem of providing business intelligence solutions based on business data, bioinspired deep learning has to be considered. This paper focuses on the application of artificial metaplasticity learning in business intelligence systems as an alternative paradigm of achieving a deeper information extraction and learning from arbitrary size data sets. As a case study, artificial metaplasticity multilayer perceptron applied to the automation of credit approval decision based on collected client data is analyzed, showing its potential and improvements over the state-of-the-art techniques. This paper successfully introduces the relevant novelty that the artificial neural network itself estimates the pdf of the input data to be used in the metaplasticity learning, so it is much closer to the biologic reality than previous implementations of artificial metaplasticity.																	0941-0643	1433-3058				SEP	2020	32	17					13195	13202		10.1007/s00521-018-3377-5													
J								Convolutional neural networks for computer vision-based detection and recognition of dumpsters	NEURAL COMPUTING & APPLICATIONS										Deep learning; Dumpsters; Semi-supervised learning; Transfer learning		In this paper, we propose a twofold methodology for visual detection and recognition of different types of city dumpsters, with minimal human labeling of the image data set. Firstly, we carry out transfer learning by using Google Inception-v3 convolutional neural network, which is retrained with only a small subset of labeled images out of the whole data set. This first classifier is then improved with a semi-supervised learning based on retraining for two more rounds, each one increasing the number of labeled images but without human supervision. We compare our approach against both to a baseline case, with no incremental retraining, and the best case, assuming we had a fully labeled data set. We use a data set of 27,624 labeled images of dumpsters provided by Ecoembes, a Spanish nonprofit organization that cares for the environment through recycling and the eco-design of packaging in Spain. Such a data set presents a number of challenges. As in other outdoor visual tasks, there are occluding objects such as vehicles, pedestrians and street furniture, as well as other dumpsters whenever they are placed in groups. In addition, dumpsters have different degrees of deterioration which may affect their shape and color. Finally, 35% of the images are classified according to the capacity of the container, which contains a feature which is hard to assess in a snapshot. Since the data set is fully labeled, we can compare our approach both against a baseline case, doing only the transfer learning using a minimal set of labeled images, and against the best case, using all the labels. The experiments show that the proposed system provides an accuracy of 88%, whereas in the best case it is 93%. In other words, the method proposed attains 94% of the best performance.																	0941-0643	1433-3058				SEP	2020	32	17					13203	13211		10.1007/s00521-018-3390-8													
J								AMSOM: artificial metaplasticity in SOM neural networks-application to MIT-BIH arrhythmias database	NEURAL COMPUTING & APPLICATIONS										Metaplasticity; AMMLP; AMP; Feature extraction; Artificial neural network; Self-organizing maps; AMSOM		Artificial metaplasticity is the machine learning algorithm inspired in the biological metaplasticity of neural synapses. Metaplasticity stands for plasticity of plasticity, and as long as plasticity is related to memory, metaplasticity is related to learning. Implemented in supervised learning assuming input patterns distribution or a related function, it has proved to be very efficient in performance and in training convergence for multidisciplinary applications. Now, for the first time, this kind of artificial metaplasticity is implemented in an unsupervised neural network, achieving also excellent results that are presented in this paper. To compare results, a modified self-organization map is applied to the classification of MIT-BIH cardiac arrhythmias database.																	0941-0643	1433-3058				SEP	2020	32	17					13213	13220		10.1007/s00521-018-3576-0													
J								Nonlinear predictability analysis of brain dynamics for automatic recognition of negative stress	NEURAL COMPUTING & APPLICATIONS										Distress; EEG; Predictability; Regularity; Symbolization	PHYSIOLOGICAL TIME-SERIES; APPROXIMATE ENTROPY; SYSTEM; CLASSIFICATION; COMPLEXITY; HEART	Negative stress, also named distress, is nowadays one of the most studied emotional states due to its high impact on advanced societies. Its automatic identification from physiological recordings can be extremely useful to prevent concomitant physical health problems as well as other mental disorders. However, this task is still an unresolved challenge. Recently, quadratic sample entropy (QSE) applied to the electroencephalogram signal has proven to be the most promising single metric to discern between emotional states of calm and negative stress. This index estimates repetitive patterns in a time series without considering time data order within each one, thus ignoring some relevant dynamics. Hence, in this work conditional entropy (CEn) and its correction (CCEn) are studied to complement and improve QSE ability in detection of distress. Both CEn and CCEn symbolize original time series to consider ordinal patterns and, thus, quantify accurately data predictability. Results obtained from 279 samples (146 associated with calm and 133 to distress) provide a similar discriminant power, about 65%, both for conditional entropies and QSE. More interestingly, these metrics also reveal the presence of complementary brain dynamics under a emotional state of distress. Thus, CCEn and QSE suggest a synchronization between opposite frontal and parietal brain regions from both hemispheres, such that when the level of negative stress increases, a more irregular activity is found in left frontal and right parietal areas. Simultaneously, more predictable activity is noticed in right frontal and left parietal regions. These findings agree with previous neurophysiological studies and allow to improve the identification of distress. Precisely, a very simple discriminant model based on CCEn and QSE is able to discern more than 80% of samples, thus overcoming slightly the results reported by previous works, where dozens or hundreds of variables have to be combined with advanced classifiers.																	0941-0643	1433-3058				SEP	2020	32	17					13221	13231		10.1007/s00521-018-3620-0													
J								Improving deep learning performance with missing values via deletion and compensation	NEURAL COMPUTING & APPLICATIONS										Missing values; Imputation; Classification; Deep learning	PATTERN-CLASSIFICATION; DATA IMPUTATION; NETWORK	Missing values in a dataset is one of the most common difficulties in real applications. Many different techniques based on machine learning have been proposed in the literature to face this problem. In this work, the great representation capability of the stacked denoising auto-encoders is used to obtain a new method of imputating missing values based on two ideas: deletion and compensation. This method improves imputation performance by artificially deleting values in the input features and using them as targets in the training process. Nevertheless, although the deletion of samples is demonstrated to be really efficient, it may cause an imbalance between the distributions of the training and the test sets. In order to solve this issue, a compensation mechanism is proposed based on a slight modification of the error function to be optimized. Experiments over several datasets show that the deletion and compensation not only involve improvements in imputation but also in classification in comparison with other classical techniques.																	0941-0643	1433-3058				SEP	2020	32	17					13233	13244		10.1007/s00521-019-04013-2													
J								Fixed-time synchronization of competitive neural networks with proportional delays and impulsive effect	NEURAL COMPUTING & APPLICATIONS										Competitive neural networks; Fixed-time synchronization; Proportional delays; Impulse	GLOBAL EXPONENTIAL STABILITY; ALMOST-PERIODIC SOLUTION; LAG SYNCHRONIZATION; NEUTRAL-TYPE; EXISTENCE; STABILIZATION	This paper investigates the fixed-time synchronization problems for competitive neural networks with proportional delays and impulsive effect. The concerned network involves two coupling terms, i.e., long-term memory and short-term memory, which leads to the difficulty to the dynamics analysis. Based on Lyapunov functionals, the differential inequalities and for the objective of making the settling time independent of initial condition, a novel criterion guaranteeing the fixed-time synchronization of addressed system is derived. Finally, two examples and their simulations are given to demonstrate the effectiveness of the obtained results.																	0941-0643	1433-3058				SEP	2020	32	17					13245	13254		10.1007/s00521-019-04654-3													
J								Changing product specification in extractive distillation process using intelligent control system	NEURAL COMPUTING & APPLICATIONS										Extractive distillation process; Intelligent control system; Soft sensor; Artificial neural networks	ETHANOL; DEHYDRATION	Obtaining anhydrous ethanol by extractive distillation has already been the object of several studies in the control literature. However, despite the presence of varying degrees of purity of anhydrous ethanol owing to its applications in industrial and commercial sectors, little attention has been given to dynamic and control for changing the operating conditions to provide anhydrous ethanol with different specifications. Using a soft sensor based on artificial neural network, this work aimed to develop an intelligent control system to contemplate the changes in the specification of anhydrous ethanol, considering the whole process (extractive and recovery columns), and keeping the process operating at an optimal point. Using the developed intelligent control system, the only necessary modification is the new specification and all new set-points values for controllers (temperature and solvent to azeotropic feed ratio) are updated automatically, without human interference, while with a conventional control system, all the new set-points values must be modified manually. The results showed that for the studied anhydrous ethanol specification range (99.1-99.9% mole), the new optimum operating conditions (new steady-state) were reached in a short time (between 1 and 2 h), with no evidence of overflow or emptying of sumps and reflux vessels of the columns. In addition to the easy implementation of the intelligent control system, the existing control structure remains unchanged, not requiring the investment for new instrumentation.																	0941-0643	1433-3058				SEP	2020	32	17					13255	13266		10.1007/s00521-019-04664-1													
J								Bi-objective optimization approaches to many-to-many hub location routing with distance balancing and hard time window	NEURAL COMPUTING & APPLICATIONS										Capacitated hub location routing; Multi-objective optimization; Hard time window; MOICA	EPSILON-CONSTRAINT METHOD; GENETIC ALGORITHM; ALLOCATION; NETWORK; SINGLE; MODEL; DECISIONS; PICKUP	This study addresses a many-to-many hub location-routing problem where the best-found locations of hubs and the best-found tours for each hub are determined with simultaneous pickup and delivery within the hard time window. To find practical solutions, the hubs and transportation fleet have constrained capacity, in which every node can be serviced by multiple allocations with the hard time window and limited tour length. First, a bi-objective optimization model is proposed to balance travel costs among different routes and to minimize the total sum of fixed costs of locating hubs, the costs of handling, traveling, assigning, and transportation costs. The problem is then solved using an augmented epsilon-constraint technique for small to medium size instances of the problem. Due to the NP-hardness nature of the problem, the proposed multi-objective optimization model is solved by a multi-objective imperialist competitive algorithm (MOICA). To show the superior performance of the MOICA, the solutions are compared with those obtained by the non-dominated sorting genetic algorithm (NSGA-II). For the large-scale problem instances, the comparative results indicate that the MOICA can indeed provide better Pareto optimal solutions compared to NSGA-II for the large-scale problem instances.																	0941-0643	1433-3058				SEP	2020	32	17					13267	13288		10.1007/s00521-019-04666-z													
J								Prediction of fluid pattern in a shear flow on intelligent neural nodes using ANFIS and LBM	NEURAL COMPUTING & APPLICATIONS										Modeling; ANFIS; CFD; LBM; Fluid pattern	REACTORS DEVELOPMENT; THERMAL-FLOWS; MODEL; CFD; COMBINATION; DYNAMICS	Prediction of fluid pattern inside chemical mixing tanks and reactors is very challenging, mainly when scale-up and optimization of devices are essential due to enormous computational time and experimental efforts. This work recommends a new prediction tool to understand fluid behavior on the smart neural nodes. The adaptive neuro-fuzzy inference system (ANFIS) is used to learn the lattice Boltzmann method (LBM) data and predict fluid patterns based on its understanding. The anticipated results with the integration of LBM and ANFIS method indicated a good agreement with existing computational fluid dynamics results. The results show that almost similar fluid pattern occurs on neural nodes in the ANFIS method compared by LBM on lattice unit when shear flow applies on the top and bottom of fluid structure. This finding is very promising to avoid substantial computational time or experimental efforts in the optimization of different chemical devices. Prediction of the shear flow and optimization of boundary conditions to get proper droplet size distribution or bubble size distribution requires heavy computational time. Therefore in this work, ANFIS approach besides the LB method was used to replicate the flow between two parallel plates (vortex structure) in a short computational time. The current overview also shows the ability of the ANFIS method as a machine learning tool to learn how the fluid is disturbed by shear flow. The input data are used as big data during the learning process, and the intelligence of the algorithm is examined based on the total percentage of training data.																	0941-0643	1433-3058				SEP	2020	32	17					13313	13321		10.1007/s00521-019-04677-w													
J								A novel singular spectrum analysis-based multi-objective approach for optimal FIR filter design using artificial bee colony algorithm	NEURAL COMPUTING & APPLICATIONS										Filter design; Singular spectrum analysis; SSA; Multi-objective; Artificial bee colony algorithm; FIR filter	PARTICLE SWARM OPTIMIZATION; DIGITAL-FILTERS; EVOLUTIONARY	Effective filter design plays an important role in signal processing applications. Multiple parameters must be considered to control the over-frequency response of the designed filter. In this study, a novel multi-objective approach is proposed for windowing finite impulse response (FIR) filter design. The windowing FIR filters are commonly used due to its linear phase property, frequency stability and easier implementation. However, windowing method can only control the cutting frequency of filter, and it suffers from the problem of insufficient control of the transition bandwidth, pass and stop band cutoff frequencies. Therefore, the window function was optimized using a novel multi-objective artificial bee colony (ABC) algorithm based on singular spectrum analysis (SSA) to eliminate these disadvantages of the windowing method. The proposed method was compared to three other multi-objective ABC variants. Novel SSA-based multi-objective approach yielded the best performance among four approaches. The proposed multi-objective approach that uses the SSA method has a significant advantage since it does not require user experience, it is not dependent on parameters, and there is no weight determination problem. Also, it does not have sorting and pooling stages that increase the cost of calculation. The obtained results were compared with the published literature studies. The SSA-based multi-objective approach offered better alternative to other literature techniques in terms of calculating the fitness function that deals with finding the most reasonable solution considering all error terms. Finally, the performance of the designed filter was tested on electroencephalography (EEG) signal. The EEG signal was decomposed successfully into subbands using proposed filter design approach. Based on numerical results of this study, the proposed filter provided the low-pass band and stop band ripple, and high stop band attenuation value of all, while having well enough performance.																	0941-0643	1433-3058				SEP	2020	32	17					13323	13341		10.1007/s00521-019-04680-1													
J								Active neural learners for text with dual supervision	NEURAL COMPUTING & APPLICATIONS										Active learning; Annotations; Gradient-based attributions; Convolutional neural networks; Recurrent neural networks; Dual supervision		Dual supervision for text classification and information retrieval, which involves training the machine with class labels augmented with text annotations that are indicative of the class, has been shown to provide significant improvements, both in and beyond active learning (AL) settings. Annotations in the simplest form are highlighted portions of the text that are indicative of the class. In this work, we aim to identify and realize the full potential of unsupervised pretrained word embeddings for text-related tasks in AL settings by training neural nets-specifically, convolutional and recurrent neural nets-through dual supervision. We propose an architecture-independent algorithm for training neural networks with human rationales for class assignments and show how unsupervised embeddings can be better leveraged in active learning settings using the said algorithm. The proposed solution involves the use of gradient-based feature attributions for constraining the machine to follow the user annotations; further, we discuss methods for overcoming the architecture-specific challenges in the optimization. Our results on the sentiment classification task show that one annotated and labeled document can be worth up to seven labeled documents, giving accuracies of up to 70% for as few as ten labeled and annotated documents, and shows promise in significantly reducing user effort for total-recall information retrieval task in systematic literature reviews.																	0941-0643	1433-3058				SEP	2020	32	17					13343	13362		10.1007/s00521-019-04681-0													
J								Mutual-manifold regularized robust fast latent LRR for subspace recovery and learning	NEURAL COMPUTING & APPLICATIONS										Robust fast Latent LRR; Subspace recovery and learning; Mutual-manifold regularization; Feature extraction	LOW-RANK REPRESENTATION; SEGMENTATION; RECOGNITION	In this paper, we propose a simple yet effective low-rank representation (LRR) and subspace recovery model called mutual-manifold regularized robust fast latent LRR. Our model improves the representation ability and robustness from twofold. Specifically, our model is built on the Frobenius norm-based fast latent LRR decomposing given data into a principal feature part, a salient feature part and a sparse error, but improves it clearly by designing mutual-manifold regularization to encode, preserve and propagate local information between coefficients and salient features. The mutual-manifold regularization is defined by using the coefficients as the adaptive reconstruction weights for salient features and constructing a Laplacian matrix over salient features for the coefficients. Thus, some important local topology structure information can be propagated between them, which can make the discovered subspace structures and features potentially more accurate for the data representations. Besides, our approach also considers to improve the robust properties of subspace recovery against noise and sparse errors in coefficients, which is realized by decomposing original coefficients matrix into an error-corrected part and a sparse error part fitting noise in coefficients, and the recovered coefficients are then used for robust subspace recovery. Experimental results on several public databases demonstrate that our method can outperform other related algorithms.																	0941-0643	1433-3058				SEP	2020	32	17					13363	13376		10.1007/s00521-019-04688-7													
J								Exploring speed-accuracy tradeoff in reaching movements: a neurocomputational model	NEURAL COMPUTING & APPLICATIONS										Human movement; Speed-accuracy tradeoff; Fitts' law; Neurocomputational model	RAPID HUMAN MOVEMENTS; DIFFERENTIAL EVOLUTION; MATHEMATICAL-MODELS; CORTICAL CONTROL; KINEMATIC THEORY; ELBOW FLEXOR; MOTOR; MUSCLE; PROPRIOCEPTORS; ABNORMALITIES	The tradeoff between speed and accuracy of human movements has been exploited from many different perspectives, such as experimental psychology, workspace design, human-machine interface. This tradeoff is formalized by Fitts' law, which states a linear relationship between the duration and the difficulty of the movement. The bigger is the required accuracy in reaching a target or farther is the target, the slower has to be the movement. A variety of computational models of neuromusculoskeletal systems have been proposed to pinpoint the neurobiological mechanisms that are involved in human movement. We introduce a neurocomputational model of spinal cord to unveil how the tradeoff between speed and accuracy elicits from the interaction between neural and musculoskeletal systems. Model simulations showed that the speed-accuracy tradeoff is not an intrinsic property of the neuromuscular system, but it is a behavioral trait that emerges from the strategy adopted by the central nervous system for executing faster movements. In particular, results suggest that the velocity of a previous learned movement is regulated by the monosynaptic connection between cortical cells and alpha motoneurons.																	0941-0643	1433-3058				SEP	2020	32	17					13377	13403		10.1007/s00521-019-04690-z													
J								A fault mode identification methodology based on self-organizing map	NEURAL COMPUTING & APPLICATIONS										Diagnostic; Fault identification; Predictive maintenance; Self-organizing map	FEATURE-SELECTION TECHNIQUES; DATA-DRIVEN; DIAGNOSIS	One of the main goals of predictive maintenance is to be able to trigger the right maintenance actions at the right moment in time building upon the monitoring of the health status of the concerned systems and their components. As such, it allows identifying incipient faults and forecasting the moment of failure at the earliest stage. Many different data-driven methods are used in such approaches (Naderi and Khorasani in 2017 IEEE 30th Canadian conference on electrical and computer engineering (CCECE), Windsor, ON, IEEE, pp 1-6, 2017. 10.1109/ccece.2017.7946715; Sarkar et al. in J Eng Gas Turbines Power 1338(8):081602, 2011. 10.1115/1.4002877; Svard et al. in Mech Syst Signal Process 45(1):170-192, 2014. 10.1016/j.ymssp.2013.11.002; Pourbabaee et al. Mech Syst Signal Process 76-77:136-156, 2016. 10.1016/j.ymssp.2016.02.023). This work uses the self-organizing maps (SOMs) or Kohonen map, thanks to its ability to emphasize underlying behavior such as fault modes. An automatic fault mode detection is presented based on a SOM network and the kernel density estimation with as less as possible prior knowledge. The different SOM development steps are presented and the suitable solutions proposed to structure the approach are accompanied by mathematical methods. The generated maps are then used with kernel density analysis to isolate fault modes on them. Finally, a methodology is presented to identify the different fault modes. The work is illustrated with an aircraft jet engines case study.																	0941-0643	1433-3058				SEP	2020	32	17					13405	13423		10.1007/s00521-019-04692-x													
J								Distributed fault-tolerant control of modular and reconfigurable robots with consideration of actuator saturation	NEURAL COMPUTING & APPLICATIONS										Modular and reconfigurable robots; Decomposition-based control; Neural network; Distributed fault-tolerant control; Joint torque sensing	CONTROL DESIGN; SYSTEMS	A novel decomposition-based distributed robust fault-tolerant control method is proposed for modular and reconfigurable robots based on joint torque sensing. The designed robust controller compensates for both model uncertainties and a class of actuator faults. In addition, the proposed scheme does not require a fault detection and diagnosis module, avoiding time delay associated with it. Furthermore, a radial basis function neural network-based compensation scheme is proposed to deal with the actuator saturation problem, which is especially critical when actuator fault has to be tolerated by the control system. Simulation results have shown the effectiveness of the presented method.																	0941-0643	1433-3058				SEP	2020	32	17					13591	13604		10.1007/s00521-020-04768-z													
J								Multi-objective orthogonal opposition-based crow search algorithm for large-scale multi-objective optimization	NEURAL COMPUTING & APPLICATIONS										Crow search algorithm; Orthogonal; Opposition; Multi-objective optimization; Metaheuristic; Engineering designs; MOORA	PARTICLE SWARM OPTIMIZER; EVOLUTIONARY ALGORITHMS; DESIGN	Many engineering optimization problems are typically multi-objective in their natures and multidisciplinary with a large number of decision variables. Furthermore, Pareto dominance loses its effectiveness in such situations. Thus, developing a robust optimization algorithm undoubtedly becomes a true challenge. This paper proposes a multi-objective orthogonal opposition-based crow search algorithm (M2O-CSA) for solving large-scale multi-objective optimization problems (LSMOPs). In the M2O-CSA, a multi-orthogonal opposition strategy is employed to mitigate the conflicts among the convergence and distribution of solutions. First, two individuals are randomly chosen to undergo the crossover stage and then orthogonal array is presented to obtain nine individuals. Then individuals are used in the opposition stage to improve the diversity of solutions. The effectiveness of the proposed M2O-CSA is investigated by implementing it on different dimensions of multi-objective optimization problems (MOPs). The Pareto front solutions of these MOPs have various characteristics such as convex, non-convex and discrete. It is also applied to solve multi-objective design applications with distinctive features such as four bar truss (FBT) design, welded beam (WB) deign, disk brake (DB) design, and speed reduced (SR) design, where they involve different characteristics. In this context, a new decision making tool based on multi-objective optimization on the basis of ratio analysis (MOORA) technique is employed to help the designer for extracting the operating point as the best compromise or satisfactory solution to execute the candidate engineering design. Simulation results affirm that the proposed M2O-CSA works efficiently and effectively.																	0941-0643	1433-3058				SEP	2020	32	17					13715	13746		10.1007/s00521-020-04779-w													
J								SulSite-GTB: identification of protein S-sulfenylation sites by fusing multiple feature information and gradient tree boosting	NEURAL COMPUTING & APPLICATIONS										S-sulfenylation sites; Multi-information fusion; SMOTE; LASSO; Gradient tree boosting	SUBCELLULAR-LOCALIZATION; PREDICTION; SELECTION; IDENTIFY; METHYLATION; DATABASE; PLOC	Protein cysteine S-sulfenylation is an essential and reversible post-translational modification that plays a crucial role in transcriptional regulation, stress response, cell signaling and protein function. Studies have shown that S-sulfenylation is involved in many human diseases such as cancer, diabetes and arteriosclerosis. However, experimental identification of protein S-sulfenylation sites is generally expensive and time-consuming. In this study, we proposed a new protein S-sulfenylation sites prediction method SulSite-GTB. First, fusion of amino acid composition, dipeptide composition, encoding based on grouped weight,Knearest neighbors, position-specific amino acid propensity, position-weighted amino acid composition and pseudo-position specific score matrix feature extraction to obtain the initial feature space. Secondly, we use the synthetic minority oversampling technique (SMOTE) algorithm to process the class imbalance data, and the least absolute shrinkage and selection operator (LASSO) are employed to remove the redundant and irrelevant features. Finally, the optimal feature subset is input into the gradient tree boosting classifier to predict the S-sulfenylation sites, and the five-fold cross-validation and independent test set method are used to evaluate the prediction performance of the model. Experimental results showed the overall prediction accuracy is 92.86% and 88.53%, respectively, and the AUC values are 0.9706 and 0.9425, respectively, on the training set and the independent test set. Compared with other prediction methods, the results show that the proposed method SulSite-GTB is significantly superior to other state-of-the-art methods and provides a new idea for the prediction of post-translational modification sites of other proteins. The source code and all datasets are available at.																	0941-0643	1433-3058				SEP	2020	32	17					13843	13862		10.1007/s00521-020-04792-z													
J								Robust features for text-independent speaker recognition with short utterances	NEURAL COMPUTING & APPLICATIONS										Speaker recognition; Speaker identification; i-vector; PLDA; Short utterances; Noise	SPEECH; IDENTIFICATION; VERIFICATION; NOISY; MFCC; DISTRIBUTIONS; COMBINATION; PLDA; GMM	Speaker recognition systems achieve good performance under controlled conditions. However, in real-world conditions, the performance degrades drastically. The principal cause being when limited data are presented. The presence of background noise is another main factor of performance distortion. In spite of the major advances in speaker recognition field, the effect of noise and the limitation of the amount of available speech data are still open problems, and no optimal solution has been found yet to cope with them. In this paper, we propose a new system using new enhanced and reduced gammatone coefficients in order to improve robustness with limited speech data duration. We demonstrate the usefulness of these coefficients compared to the well-known features with speakers taken from different databases recorded under different conditions.																	0941-0643	1433-3058				SEP	2020	32	17					13863	13883		10.1007/s00521-020-04793-y													
J								Coping with opponents: multi-objective evolutionary neural networks for fighting games	NEURAL COMPUTING & APPLICATIONS										Neuroevolution; Evolutionary algorithms; Multi-objective optimization; NEAT; Fighting games	SELECTION	Fighting games represent a challenging problem for computer-controlled characters. Therefore, they have attracted considerable research interest. This paper investigates novel multi-objective neuroevolutionary approaches for fighting games focusing on the Fighting Game AI Competition. Considering several objectives shall improve the AI's generalization capabilities when confronted with new opponents. To this end, novel combinations of neuroevolution and multi-objective evolutionary algorithms are explored. Since the variants proposed employ the well-knownR2 indicator, we derived a novel faster algorithm for determining the exactR2 contribution. An experimental comparison of the novel variants to existing multi-objective neuroevolutionary algorithms demonstrates clear performance benefits on the test case considered. The best performing algorithm is then used to evolve controllers for the fighting game. Comparing the results with state-of-the-art AI opponents shows very promising results; the novel bot is able to outperform several competitors.																	0941-0643	1433-3058				SEP	2020	32	17					13885	13916		10.1007/s00521-020-04794-x													
J								Prediction of convective clouds formation using evolutionary neural computation techniques	NEURAL COMPUTING & APPLICATIONS										Convection initialisation prediction; Machine learning algorithms; Neural networks; Unbalanced databases	SOLAR-RADIATION PREDICTION; LOW-VISIBILITY EVENTS; SPATIAL-DISTRIBUTION; INITIATION; ALGORITHM; CLASSIFICATION; CONFIGURATION; THUNDERSTORMS; OPTIMIZATION; PROJECTION	The prediction of convective clouds formation is a very important problem in different areas such as agriculture, natural hazards prevention or transport-related facilities. In this paper, we evaluate the capacity of different types of evolutionary artificial neural networks to predict the formation of convective clouds, tackling the problem as a classification task. We use data from Madrid-Barajas airport, including variables and indices derived from the Madrid-Barajas airport radiosonde station. As objective variable, we use the cloud information contained in the METAR and SPECI meteorological reports from the same airport and we consider a prediction time horizon of 12 h. The performance of different types of evolutionary artificial neural networks has been discussed and analysed, including three types of basis functions (sigmoidal unit, product unit and radial basis function) and two types of models, a mono-objective evolutionary algorithm with two objective functions and a multi-objective evolutionary algorithm optimised by the two objective functions simultaneously. We show that some of the developed neuro-evolutionary models obtain high quality solutions to this problem, due to its high unbalance characteristic.																	0941-0643	1433-3058				SEP	2020	32	17					13917	13929		10.1007/s00521-020-04795-w													
J								An enhanced sitting-sizing scheme for shunt capacitors in radial distribution systems using improved atom search optimization	NEURAL COMPUTING & APPLICATIONS										Atom search optimization; Optimal sitting and sizing of capacitors; Minimization of power loss; Voltage profile; Radial distribution system	VOLTAGE REGULATORS; OPTIMUM SIZE; ALGORITHM; LOCATION; ALLOCATION; DESIGN	In this paper, an enhanced sitting-sizing scheme for shunt capacitors (4SCs) in a radial distribution system (RDS) based on an improved atom search optimization (IASO) algorithm is proposed. IASO emulates the model of atomic motion in nature based on interaction forces among atoms. The main goal of the 4SCs problem is to reduce the line losses and minimize the capacitor installation cost by searching for the optimal location and sizing of the capacitors. This leads to improvements in the voltage profile and reliability of the system. The IASO algorithm is introduced to achieve the optimal sitting and sizing of capacitors for RDSs. The proposed IASO algorithm is benchmarked and validated on different radial systems, including the IEEE 33-bus, IEEE 34-bus, IEEE 65-bus, IEEE 85-bus and Marsa Matrouh networks, to demonstrate its performance in real-world applications. The results obtained by the proposed IASO algorithm are compared with the standard ASO, PSO, SCA, GWO and SSA algorithms. Furthermore, the significance of the obtained results is confirmed by performing a nonparametric statistical test, i.e., the Wilcoxon's rank-sum at the 5% significance level. The comprehensive results demonstrate that the results obtained by the proposed IASO algorithm denominate the results obtained by the other algorithms and that IASO minimizes the operating costs while achieving better voltage profiles.																	0941-0643	1433-3058				SEP	2020	32	17					13971	13999		10.1007/s00521-020-04799-6													
J								Fuzzy association rule-based set-point adaptive optimization and control for the flotation process	NEURAL COMPUTING & APPLICATIONS										Fuzzy neural network; Fuzzy association rule; Set-point optimization; Predictive control; Flotation process	MODEL-PREDICTIVE CONTROL; IMAGE; PERFORMANCE	Froth flotation is a complicated process which is difficult to establish its first-principle model. Due to the fluctuations in the grade of raw ore, adaptively adjusting the set-points is extremely important in the flotation process. The inappropriate set-points easily lead to the instability of the process. This paper presents a fuzzy association rule-based set-point adaptive optimization and control strategy for the antimony flotation process without knowing the system model. Firstly, a fuzzy neural network is constructed as a soft-sensor to estimate the feed grade online because of the lack of efficient measurement equipment. Then, fuzzy association rule is used to mine the hidden relationship between the feed grade with reagent dosages and the optimal set-points. Through data mining from the quantitative database, the fuzzy inference system generates the optimal set-points. To implement satisfactory tracking performance, predictive controller is used to compute the control inputs. Because the system dynamics is unknown, long short-term memory network model is established to predict the future behaviors of the process. Finally, simulations and experiments are carried out to demonstrate the effectiveness of the proposed strategy. Compared to the manual manipulation, which is widely used in flotation processes, our control strategy achieves a better control performance, and the concentrate grades are more in line with the process requirement.																	0941-0643	1433-3058				SEP	2020	32	17					14019	14029		10.1007/s00521-020-04801-1													
J								An application of parametric approach for interval differential equation in inventory model for deteriorating items with selling-price-dependent demand	NEURAL COMPUTING & APPLICATIONS										EOQ; Interval-valued deterioration; Interval-valued selling-price-dependent demand; Partially backlogged shortages; Interval-valued cost factors	PARTIAL TRADE CREDITS; PERMISSIBLE DELAY; VARIABLE DEMAND; PARTICLE SWARM; SENSITIVE DEMAND; STOCK; SYSTEM; REPLENISHMENT; ALGORITHM; PAYMENT	Due to the uncertainty of market economy as well as fluctuation of customer's demand, appropriate modelling of an inventory problem is a challenging task to the researchers/OR practitioners. In order to overcome this type of challenging situation, in this article, using the parametric approach of interval a non-deterministic inventory model for deteriorating items with partially backlogged shortages and interval-valued deterioration rate has been developed. Here, the demand rate is considered to be interval-valued and dependent upon the selling price which is also interval-valued. Also, all the parameters (like ordering cost, holding cost, etc.) except the backlogging parameter have been considered as interval-valued. The corresponding problem has been formulated as a maximization problem with an interval-valued objective. Then, to solve the said problem, we have used different variants of the QPSO algorithm with interval fitness using interval ranking. Then, the computational results have been illustrated with the help of three numerical examples. Finally, the sensitivity of the solution has been analysed with the changes in the values of different parameters associated with the model and a fruitful conclusion has been drawn.																	0941-0643	1433-3058				SEP	2020	32	17					14069	14085		10.1007/s00521-020-04806-w													
J								A new approach to cubic q-rung orthopair fuzzy multiple attribute group decision-making based on power Muirhead mean	NEURAL COMPUTING & APPLICATIONS										Cubic q-rung orthopair fuzzy sets; Power average operator; Muirhead mean; Cubic q-rung orthopair fuzzy power Muirhead mean; Multiple attribute group decision-making	AGGREGATION OPERATORS; BONFERRONI OPERATORS; SETS	The q-rung orthopair fuzzy sets (q-ROFSs) have been proved to be an efficient tool in expressing decision makers' (DMs) evaluation values in multiple attribute group decision-making (MAGDM) procedure. To more effectively represent DMs' evaluation information in complicated MAGDM process, this paper proposes a new tool, called cubic q-rung orthopair fuzzy sets (Cq-ROFSs), based on the combination of q-ROFSs with interval-valued q-ROFSs. Then, we investigate MAGDM problems in which DMs' preference information is given in terms of cubic q-rung orthopair fuzzy numbers. First, the definition, operations and comparison method of Cq-ROFSs are introduced. Second, to effectively aggregate cubic q-rung orthopair fuzzy information we propose the cubic q-rung orthopair fuzzy power average operator, the cubic q-rung orthopair fuzzy power Muirhead mean operator as well as their weighted forms. We illustrate the powerfulness and flexibility of the proposed operators in fusing cubic q-rung orthopair fuzzy decision-making information. Third, on the basis of the proposed operators we give the main steps of a novel cubic q-rung orthopair fuzzy MAGDM method. We utilize the method to solve real MAGDM problems to prove its effectiveness and validity. Finally, we explain why DMs should choose our proposed method rather than some others through comparison analysis.																	0941-0643	1433-3058				SEP	2020	32	17					14087	14112		10.1007/s00521-020-04807-9													
J								An effective image compression-encryption scheme based on compressive sensing (CS) and game of life (GOL)	NEURAL COMPUTING & APPLICATIONS										Image encryption; Compressive sensing (CS); Chaos; Game of life (GOL)	HYPER-CHAOTIC SYSTEM; CELLULAR-AUTOMATA; ALGORITHM; PERMUTATION; SECURE; DIFFUSION; ROBUST; MAP	At present, information entropies of cipher images gotten by some CS-based image cryptosystems are lower than 7, which make them vulnerable to entropy attack. To cope with this problem, we propose a novel image compression-encryption method based on compressive sensing (CS) and game of life (GOL). Encryption architecture of permutation, compression and diffusion is utilized. Firstly, a plaintext-dependent game-of-life-based scrambling method is presented to shuffle the sparse coefficient matrix of plain image, and the permutation matrix is constructed by rules of GOL, which may effectively reduce the adjacent pixel correlation and enhance the scrambling effect. Secondly, the confused matrix is compressed by CS and diffused using a key matrix to get the cipher image. Additionally, a five-dimensional (5D) memristive hyperchaotic system is used to generate chaotic sequences. They are utilized to construct measurement matrix, to generate initial cell matrix of GOL and to produce key matrix. Information entropy of plain image and external key parameters are combined to compute initial values of the hyperchaotic system. Therefore, our algorithm has high sensitivity to original image and it may resist against known-plaintext attack and chosen-plaintext attack. Experimental results and performance analyses demonstrate that the proposed encryption algorithm is effective to withstand various typical attacks, and it may be applied for image secure communication.																	0941-0643	1433-3058				SEP	2020	32	17					14113	14141		10.1007/s00521-020-04808-8													
J								Semi-supervised person re-identification by similarity-embedded cycle GANs	NEURAL COMPUTING & APPLICATIONS										Person re-identification; Semi-supervised learning; Deep metric learning; Similarity embedded; Cycle GANs	REPRESENTATION	Recently, person re-identification (PR-ID) has attracted numerous of research interest because of its broad applications. However, most of the existing PR-ID models always follow the supervised framework, which requires substantial labeled data. In fact, it is often very hard to get enough labeled training samples in many practical application scenarios. To overcome this limitation, some semi-supervised PR-ID methods have been presented more recently. Although some of these semi-supervised models achieve satisfied results, there is still much space to improve. In this paper, we propose a novel semi-supervised PR-ID by similarity-embedded cycle GANs (SECGAN). Our SECGAN model can learn cross-view features with limited labeled data by using cycle GANs. Simultaneously, to further enhance the ability of cycle GANs so that it can extract more discriminative and robust features, similarity metric subnet and specific features extracting subnet are embedded into cycle GANs. Extensive experiments have been conducted on three public PR-ID benchmark datasets, and the experimental results show that our proposed SECGAN approach outperforms several typical supervised methods and the existing state-of-the-art semi-supervised methods including traditional and deep learning semi-supervised methods.																	0941-0643	1433-3058				SEP	2020	32	17					14143	14152		10.1007/s00521-020-04809-7													
J								Neural network approach for solving nonlocal boundary value problems	NEURAL COMPUTING & APPLICATIONS										RBF neural network; Variable learning rate; Nonlocal nonlinear problem; Two-point step size gradient method	RADIAL BASIS FUNCTION; FINITE-ELEMENT APPROXIMATIONS; LINEAR ELLIPTIC EQUATION; NUMERICAL-SOLUTION	This paper proposes a radial basis function (RBF) network-based method for solving a nonlinear second-order elliptic equation with Dirichlet boundary conditions. The nonlocal term involved in the differential equation needs a completely different approach from the up-to-now-known methods for solving boundary value problems by using neural networks. A numerical integration procedure is developed for computing the local L2-inner product. It is known that the non-variational methods are not effective in solving nonlocal problems. In this paper, the weak formulation of the nonlocal problem is reduced to the minimization of a nonlinear functional. Unlike many previous works, we use an integral objective functional for implementing the learning procedure. Well-distributed nodes are used as the centers of the RBF neural network. The weights of the RBF network are determined by a two-point step size gradient method. The neural network method proposed in this paper is an alternative to the finite-element method (FEM) for solving nonlocal boundary value problems in nonLipschitz domains. A new variable learning rate strategy has been developed and implemented in order to avoid the divergence of the training process. A comparison between the proposed neural network approach and the FEM is illustrated by challenging examples, and the performance of both methods is thoroughly analyzed.																	0941-0643	1433-3058				SEP	2020	32	17					14153	14171		10.1007/s00521-020-04810-0													
J								A chaotic optimization method based on logistic-sine map for numerical function optimization	NEURAL COMPUTING & APPLICATIONS										Chaotic optimization; Logistic-sine map; Swarm-based optimization; Chaos	BEE COLONY ALGORITHM; SIMULATION	Meta-heuristic optimization algorithms have been used to solve mathematically unidentifiable problems. The main purpose of the optimization methods on problem-solving is to choose the best solution in predefined conditions. To increase performance of the optimization methods, chaotic maps for instance Logistic, Singer, Sine, Tent, Chebyshev, Circle have been widely used in the literature. However, hybrid 1D chaotic maps have higher performance than the 1D chaotic maps. The hybrid chaotic maps have not been used in the optimization process. In this article, 1D hybrid chaotic map (logistic-sine map)-based novel swarm optimization method is proposed to achieve higher numerical results than other optimization methods. Logistic-sine map has good statistical result, and this advantage is used directly to calculate global optimum value in this study. The proposed algorithm is a swarm-based optimization algorithm, and the seed value of the logistic-sine map is generated from local best solutions to reach global optimum. In order to test the proposed hybrid chaotic map-based optimization method, widely used numerical benchmark functions are chosen. The proposed chaotic optimization method is also tested on compression spring design problem. Results and comparisons clearly show that the proposed chaotic optimization method is successful.																	0941-0643	1433-3058				SEP	2020	32	17					14227	14239		10.1007/s00521-020-04815-9													
J								HetConv: Beyond Homogeneous Convolution Kernels for Deep CNNs	INTERNATIONAL JOURNAL OF COMPUTER VISION										Efficient convolutional neural networks; Heterogeneous convolution; FLOPs compression; Model compression; Efficient visual recognition		While usage of convolutional neural networks (CNN) is widely prevalent, methods proposed so far always have considered homogeneous kernels for this task. In this paper, we propose a new type of convolution operation using heterogeneous kernels. The proposed Heterogeneous Kernel-Based Convolution (HetConv) reduces the computation (FLOPs) and the number of parameters as compared to standard convolution operation while it maintains representational efficiency. To show the effectiveness of our proposed convolution, we present extensive experimental results on the standard CNN architectures such as VGG, ResNet, Faster-RCNN, MobileNet, and SSD. We observe that after replacing the standard convolutional filters in these architectures with our proposed HetConv filters, we achieve 1.5 x FLOPs based improvement in speed while it maintains (sometimes improves) the accuracy. We also compare our proposed convolution with group/depth wise convolution and show that it achieves more FLOPs reduction with significantly higher accuracy. Moreover, we demonstrate the efficacy of HetConv based CNN by showing that it also generalizes on object detection and is not constrained to image classification tasks. We also empirically show that the proposed HetConv convolution is more robust towards the over-fitting problem as compared to standard convolution.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2068	2088		10.1007/s11263-019-01264-3													
J								Learning an Evolutionary Embedding via Massive Knowledge Distillation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Knowledge distillation; Open-set problem; Face recognition; Vehicle re-identification; Person re-identification		Knowledge distillation methods aim at transferring knowledge from a large powerful teacher network to a small compact student one. These methods often focus on close-set classification problems and matching features between teacher and student networks from a single sample. However, many real-world classification problems are open-set. This paper proposes an Evolutionary Embedding Learning (EEL) framework to learn a fast and accurate student network for open-set problems via massive knowledge distillation. First, we revisit the formulation of canonical knowledge distillation and make it suitable for the open-set problems with massive classes. Second, by introducing an angular constraint, a novel correlated embedding loss (CEL) is proposed to match embedding spaces between the teacher and student network from a global perspective. Lastly, we propose a simple yet effective paradigm towards a fast and accurate student network development for knowledge distillation. We show the possibility to implement an accelerated student network without sacrificing accuracy, compared with its teacher network. The experimental results are quite encouraging. EEL achieves better performance with other state-of-the-art methods for various large-scale open-set problems, including face recognition, vehicle re-identification and person re-identification.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2089	2106		10.1007/s11263-019-01286-x													
J								Rectified Wing Loss for Efficient and Robust Facial Landmark Localisation with Convolutional Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Facial landmark localisation; Deep convolutional neural networks; Rectified Wing Loss; Pose-based data balancing; Coarse-to-fine networks	FACE ALIGNMENT; REGRESSION; CASCADE	Efficient and robust facial landmark localisation is crucial for the deployment of real-time face analysis systems. This paper presents a new loss function, namely Rectified Wing (RWing) loss, for regression-based facial landmark localisation with Convolutional Neural Networks (CNNs). We first systemically analyse different loss functions, including L2, L1 and smooth L1. The analysis suggests that the training of a network should pay more attention to small-medium errors. Motivated by this finding, we design a piece-wise loss that amplifies the impact of the samples with small-medium errors. Besides, we rectify the loss function for very small errors to mitigate the impact of inaccuracy of manual annotation. The use of our RWing loss boosts the performance significantly for regression-based CNNs in facial landmarking, especially for lightweight network architectures. To address the problem of under-representation of samples with large pose variations, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation strategies. Last, the proposed approach is extended to create a coarse-to-fine framework for robust and efficient landmark localisation. Moreover, the proposed coarse-to-fine framework is able to deal with the small sample size problem effectively. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits of our RWing loss and prove the superiority of the proposed method over the state-of-the-art approaches.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2126	2145		10.1007/s11263-019-01275-0													
J								Disentangled Representation Learning of Makeup Portraits in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION										Face verification; Makeup transfer; Disentangled feature; Correspondence field	FACE; NETWORK	Makeup studies have recently caught much attention in computer version. Two of the typical tasks are makeup-invariant face verification and makeup transfer. Although having experienced remarkable progress, both tasks remain challenging, especially encountering data in the wild. In this paper, we propose a disentangled feature learning strategy to fulfil both tasks in a single generative network. Overall, a makeup portrait can be decomposed into three components: makeup, identity and geometry (including expression, pose etc.). We assume that the extracted image representation can be decomposed into a makeup code that captures the makeup style and an identity code to preserve the source identity. As for other variation factors, we consider them as native structures from the source image that should be reserved. Thus a dense correspondence field is integrated in the network to preserve the geometry on a face. To encourage delightful visual results after makeup transfer, we propose a cosmetic loss to learn makeup styles in a delicate way. Finally, a new Cross-Makeup Face (CMF) benchmark dataset () with in-the-wild makeup portraits is built up to push the frontiers of related research. Both visual and quantitative experimental results on four makeup datasets demonstrate the superiority of the proposed method.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2166	2184		10.1007/s11263-019-01267-0													
J								Fine-Grained Multi-human Parsing	INTERNATIONAL JOURNAL OF COMPUTER VISION										Multi-human parsing; Benchmark dataset; Nested adversarial learning; Generative Adversarial Networks		Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification, e-commerce, media editing, video surveillance, autonomous driving and virtual reality, etc. To perform well, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as themulti-human parsingtask. In this paper, we first present a new large-scale database "Multi-human Parsing (MHP v2.0)" for algorithm development and evaluation to advance the research on understanding humans in crowded scenes. MHP v2.0 contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels and 16 dense pose key point labels, involving 2-26 persons per image captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, including MHP v1.0, PASCAL-Person-Part and Buffy. NAN serves as a strong baseline to shed light on generic instance-level semantic part prediction and drive the future research on multi-human parsing. With the above innovations and contributions, we have organized the CVPR 2018 Workshop on Visual Understanding of Humans in Crowd Scene (VUHCS 2018) and the Fine-Grained Multi-human Parsing and Pose Estimation Challenge. These contributions together significantly benefit the community. Code and pre-trained models are available at.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2185	2203		10.1007/s11263-019-01181-5													
J								A Mean-Field Description of Bursting Dynamics in Spiking Neural Networks with Short-Term Adaptation	NEURAL COMPUTATION											SYNAPSES; MODEL; EXCITABILITY; BIFURCATION; DEPRESSION; PLASTICITY; MODULATION; MECHANISMS; PATTERNS; IMPACT	Bursting plays an important role in neural communication. At the population level, macroscopic bursting has been identified in populations of neurons that do not express intrinsic bursting mechanisms. For the analysis of phase transitions between bursting and non-bursting states, mean-field descriptions of macroscopic bursting behavior are a valuable tool. In this article, we derive mean-field descriptions of populations of spiking neurons and examine whether states of collective bursting behavior can arise from short-term adaptation mechanisms. Specifically, we consider synaptic depression and spike-frequency adaptation in networks of quadratic integrate-and-fire neurons. Analyzing the mean-field model via bifurcation analysis, we find that bursting behavior emerges for both types of short-term adaptation. This bursting behavior can coexist with steady-state behavior, providing a bistable regime that allows for transient switches between synchronized and nonsynchronized states of population dynamics. For all of these findings, we demonstrate a close correspondence between the spiking neural network and the mean-field model. Although the mean-field model has been derived under the assumptions of an infinite population size and all-to-all coupling inside the population, we show that this correspondence holds even for small, sparsely coupled networks. In summary, we provide mechanistic descriptions of phase transitions between bursting and steady-state population dynamics, which play important roles in both healthy neural communication and neurological disorders.																	0899-7667	1530-888X				SEP	2020	32	9					1615	1634		10.1162/neco_a_01300													
J								Parallel Neural Multiprocessing with Gamma Frequency Latencies	NEURAL COMPUTATION											BAND ACTIVITY; NETWORK OSCILLATIONS; PHASE PRECESSION; WORKING-MEMORY; ATTENTION; MODULATION; FIELD; SYNCHRONIZATION; COMMUNICATION; PROPAGATION	The Poisson variability in cortical neural responses has been typically modeled using spike averaging techniques, such as trial averaging and rate coding, since such methods can produce reliable correlates of behavior. However, mechanisms that rely on counting spikes could be slow and inefficient and thus might not be useful in the brain for computations at timescales in the 10 millisecond range. This issue has motivated a search for alternative spike codes that take advantage of spike timing and has resulted in many studies that use synchronized neural networks for communication. Here we focus on recent studies that suggest that the gamma frequency may provide a reference that allows local spike phase representations that could result in much faster information transmission. We have developed a unified model (gamma spike multiplexing) that takes advantage of a single cycle of a cell's somatic gamma frequency to modulate the generation of its action potentials. An important consequence of this coding mechanism is that it allows multiple independent neural processes to run in parallel, thereby greatly increasing the processing capability of the cortex. System-level simulations and preliminary analysis of mouse cortical cell data are presented as support for the proposed theoretical model.																	0899-7667	1530-888X				SEP	2020	32	9					1635	1663		10.1162/neco_a_01301													
J								Fine-Grained 3D-Attention Prototypes for Few-Shot Learning	NEURAL COMPUTATION												In the real world, a limited number of labeled finely grained images per class can hardly represent the class distribution effectively. Due to the more subtle visual differences in fine-grained images than simple images with obvious objects, that is, there exist smaller interclass and larger intraclass variations. To solve these issues, we propose an end-to-end attention-based model for fine-grained few-shot image classification (AFG) with the recent episode training strategy. It is composed mainly of a feature learning module, an image reconstruction module, and a label distribution module. The feature learning module mainly devises a 3D-Attention mechanism, which considers both the spatial positions and different channel attentions of the image features, in order to learn more discriminative local features to better represent the class distribution. The image reconstruction module calculates the mappings between local features and the original images. It is constrained by a designed loss function as auxiliary supervised information, so that the learning of each local feature does not need extra annotations. The label distribution module is used to predict the label distribution of a given unlabeled sample, and we use the local features to represent the image features for classification. By conducting comprehensive experiments on Mini-ImageNet and three fine-grained data sets, we demonstrate that the proposed model achieves superior performance over the competitors.																	0899-7667	1530-888X				SEP	2020	32	9					1664	1684		10.1162/neco_a_01302													
J								Hyperbolic-Valued Hopfield Neural Networks in Synchronous Mode	NEURAL COMPUTATION											ASSOCIATIVE MEMORY	For most multistate Hopfield neural networks, the stability conditions in asynchronous mode are known, whereas those in synchronous mode are not. If they were to converge in synchronous mode, recall would be accelerated by parallel processing. Complex-valued Hopfield neural networks (CHNNs) with a projection rule do not converge in synchronous mode. In this work, we provide stability conditions for hyperbolic Hopfield neural networks (HHNNs) in synchronous mode instead of CHNNs. HHNNs provide better noise tolerance than CHNNs. In addition, the stability conditions are applied to the projection rule, and HHNNs with a projection rule converge in synchronous mode. By computer simulations, we find that the projection rule for HHNNs in synchronous mode maintains a high noise tolerance.																	0899-7667	1530-888X				SEP	2020	32	9					1685	1696		10.1162/neco_a_01303													
J								Tensor Least Angle Regression for Sparse Representations of Multidimensional Signals	NEURAL COMPUTATION											DECOMPOSITIONS; ALGORITHMS; SYSTEMS	Sparse signal representations have gained much interest recently in both signal processing and statistical communities. Compared to orthogonal matching pursuit (OMP) and basis pursuit, which solve the L-0 and L-1 constrained sparse least-squares problems, respectively, least angle regression (LARS) is a computationally efficient method to solve both problems for all critical values of the regularization parameter lambda. However, all of these methods are not suitable for solving large multidimensional sparse least-squares problems, as they would require extensive computational power and memory. An earlier generalization of OMP, known as Kronecker-OMP, was developed to solve the L-0 problem for large multidimensional sparse least-squares problems. However, its memory usage and computation time increase quickly with the number of problem dimensions and iterations. In this letter, we develop a generalization of LARS, tensor least anwnsional, sparse, least-squares problemws (underdetermined or overdetermined) for all critical values of the regularization parameter lambda and with lower computational complexity and memory usage than Kronecker-OMP. To demonstrate the validity and performance of our T-LARS algorithm, we used it to successfully obtain different sparse representations of two relatively large 3D brain images, using fixed and learned separable overcomplete dictionaries, by solving both L-0 and L-1 constrained sparse least-squares problems. Our numerical experiments demonstrate that our T-LARS algorithm is significantly faster (46 to 70 times) than Kronecker-OMP in obtainingK-sparse solutions for multilinear leastsquares problems. However, theK-sparse solutions obtained using Kronecker-OMP always have a slightly lower residual error (1.55% to 2.25%) than ones obtained by T-LARS. Therefore, T-LARS could be an important tool for numerous multidimensional biomedical signal processing applications.																	0899-7667	1530-888X				SEP	2020	32	9					1697	1732		10.1162/neco_a_01304													
J								Polynomial-Time Algorithms for Multiple-Arm Identification with Full-Bandit Feedback	NEURAL COMPUTATION											MULTIARMED BANDIT; APPROXIMATION	We study the problem of stochastic multiple-arm identification, where an agent sequentially explores a size-ksubset of arms (also known as asuper arm) from givennarms and tries to identify the best super arm. Most work so far has considered the semi-bandit setting, where the agent can observe the reward of each pulled arm or assumed each arm can be queried at each round. However, in real-world applications, it is costly or sometimes impossible to observe a reward of individual arms. In this study, we tackle the full-bandit setting, where only a noisy observation of the total sum of a super arm is given at each pull. Although our problem can be regarded as an instance of the best arm identification in linear bandits, a naive approach based on linear bandits is computationally infeasible since the number of super armsKis exponential. To cope with this problem, we first design a polynomial-time approximation algorithm for a 0-1 quadratic programming problem arising in confidence ellipsoid maximization. Based on our approximation algorithm, we propose a bandit algorithm whose computation time isO(logK), thereby achieving an exponential speedup over linear bandit algorithms. We provide a sample complexity upper bound that is still worst-case optimal. Finally, we conduct experiments on large-scale data sets with more than 1010super arms, demonstrating the superiority of our algorithms in terms of both the computation time and the sample complexity.																	0899-7667	1530-888X				SEP	2020	32	9					1733	1773		10.1162/neco_a_01299													
